<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:48:27Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|78001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05852</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05852</id><created>2015-05-20</created><authors><author><keyname>Bruner</keyname><forenames>Marie-Louise</forenames></author><author><keyname>Lackner</keyname><forenames>Martin</forenames></author></authors><title>On the Likelihood of Single-Peaked Preferences</title><categories>cs.GT math.CO</categories><msc-class>05A05, 05A99, 91B14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contains an extensive combinatorial analysis of the single-peaked
domain restriction and investigates the likelihood that an election is
single-peaked. We provide a very general upper bound result for domain
restrictions that can be defined by certain forbidden configurations. This
upper bound implies that many domain restrictions (including the single-peaked
restriction) are very unlikely to appear in a random election chosen according
to the Impartial Culture assumption. For single-peaked elections, this upper
bound can be refined and complemented by a lower bound that is asymptotically
tight. In addition, we provide exact results for elections with few voters or
candidates. Moreover, we consider the Polya urn model and the Mallows model and
obtain lower bounds showing that single-peakedness is more likely to appear
under these probability distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05868</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05868</id><created>2015-05-21</created><authors><author><keyname>Alur</keyname><forenames>Rajeev</forenames></author><author><keyname>Cerny</keyname><forenames>Pavol</forenames></author><author><keyname>Radhakrishna</keyname><forenames>Arjun</forenames></author></authors><title>Synthesis through Unification</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a specification and a set of candidate programs (program space), the
program synthesis problem is to find a candidate program that satisfies the
specification. We present the synthesis through unification (STUN) approach,
which is an extension of the counter-example guided inductive synthesis (CEGIS)
approach. In CEGIS, the synthesizer maintains a subset S of inputs and a
candidate program Prog that is correct for S. The synthesizer repeatedly checks
if there exists a counter-example input c such that the execution of Prog is
incorrect on c. If so, the synthesizer enlarges S to include c, and picks a
program from the program space that is correct for the new set S.
  The STUN approach extends CEGIS with the idea that given a program Prog that
is correct for a subset of inputs, the synthesizer can try to find a program
Prog' that is correct for the rest of the inputs. If Prog and Prog' can be
unified into a program in the program space, then a solution has been found. We
present a generic synthesis procedure based on the STUN approach and specialize
it for three different domains by providing the appropriate unification
operators. We implemented these specializations in prototype tools, and we show
that our tools often per- forms significantly better on standard benchmarks
than a tool based on a pure CEGIS approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05899</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05899</id><created>2015-05-21</created><authors><author><keyname>Saon</keyname><forenames>George</forenames></author><author><keyname>Kuo</keyname><forenames>Hong-Kwang J.</forenames></author><author><keyname>Rennie</keyname><forenames>Steven</forenames></author><author><keyname>Picheny</keyname><forenames>Michael</forenames></author></authors><title>The IBM 2015 English Conversational Telephone Speech Recognition System</title><categories>cs.CL</categories><comments>Submitted to Interspeech 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the latest improvements to the IBM English conversational
telephone speech recognition system. Some of the techniques that were found
beneficial are: maxout networks with annealed dropout rates; networks with a
very large number of outputs trained on 2000 hours of data; joint modeling of
partially unfolded recurrent neural networks and convolutional nets by
combining the bottleneck and output layers and retraining the resulting model;
and lastly, sophisticated language model rescoring with exponential and neural
network LMs. These techniques result in an 8.0% word error rate on the
Switchboard part of the Hub5-2000 evaluation test set which is 23% relative
better than our previous best published result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05900</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05900</id><created>2015-05-21</created><updated>2015-07-24</updated><authors><author><keyname>Menon</keyname><forenames>Vijay</forenames></author><author><keyname>Larson</keyname><forenames>Kate</forenames></author></authors><title>Complexity of Manipulation in Elections with Top-truncated Ballots</title><categories>cs.GT cs.CC cs.MA</categories><comments>19 pages; additional results; added references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the computational social choice literature, there has been great interest
in understanding how computational complexity can act as a barrier against
manipulation of elections. Much of this literature, however, makes the
assumption that the voters or agents specify a complete preference ordering
over the set of candidates. There are many multiagent systems applications, and
even real-world elections, where this assumption is not warranted, and this in
turn raises the question &quot;How hard is it to manipulate elections if the agents
reveal only partial preference orderings?&quot; It is this question we try to
address in this paper. In particular, we look at the weighted manipulation
problem -- both constructive and destructive manipulation -- when the voters
are allowed to specify any top-truncated ordering over the set of candidates.
We provide general results for all scoring rules, for elimination versions of
all scoring rules, for the plurality with runoff rule, for a family of election
systems known as Copeland$^{\alpha}$, and for the maximin protocol. Finally, we
also look at the impact on complexity of manipulation when there is uncertainty
about the non-manipulators' votes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05901</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05901</id><created>2015-05-21</created><authors><author><keyname>Rahmani</keyname><forenames>Mostafa</forenames></author><author><keyname>Atia</keyname><forenames>George</forenames></author></authors><title>Randomized Robust Subspace Recovery for High Dimensional Data Matrices</title><categories>stat.ML cs.CV</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal Component Analysis (PCA) is a fundamental mathematical tool with
broad applicability in numerous scientific areas. In this paper, a randomized
PCA approach that is robust to the presence of outliers and whose complexity is
independent of the dimension of the given data matrix is proposed. The proposed
approach is a two-step algorithm. First, the given data matrix is turned into a
small random matrix. Second, the columns subspace of the low rank matrix is
learned and the outlying columns are located. The low-dimensional geometry of
the low rank matrix is exploited to substantially reduce the complexity of the
algorithm. A small random subset of the columns of the given data matrix is
selected, then the selected data is projected into a random low-dimensional
subspace. The subspace learning algorithm works with this compressed small size
data. Two ideas for robust subspace learning are proposed to work under
different model assumptions. The first idea is based on the linear dependence
between the columns of the low rank matrix, and the second idea is based on the
independence between the columns subspace of the low rank matrix and the
subspace of the outlying columns. The proposed subspace learning approach has a
closed-form expression and the outlier detector is a simple subspace projection
operation. We derive sufficient conditions for the proposed method to extract
the true subspace and identify the outlying data. These conditions are less
stringent than those for existing methods. In particular, a remarkable portion
of the given data is allowed to be outlier data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05908</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05908</id><created>2015-05-21</created><updated>2015-10-05</updated><authors><author><keyname>Kia</keyname><forenames>Solmaz S.</forenames></author><author><keyname>Rounds</keyname><forenames>Stephen</forenames></author><author><keyname>Martinez</keyname><forenames>Sonia</forenames></author></authors><title>Cooperative localization for mobile agents: a recursive decentralized
  algorithm based on Kalman filter decoupling</title><categories>cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider cooperative localization technique for mobile agents with
communication and computation capabilities. We start by provide and overview of
different decentralization strategies in the literature, with special focus on
how these algorithms maintain an account of intrinsic correlations between
state estimate of team members. Then, we present a novel decentralized
cooperative localization algorithm that is a decentralized implementation of a
centralized Extended Kalman Filter for cooperative localization. In this
algorithm, instead of propagating cross-covariance terms, each agent propagates
new intermediate local variables that can be used in an update stage to create
the required propagated cross-covariance terms. Whenever there is a relative
measurement in the network, the algorithm declares the agent making this
measurement as the interim master. By acquiring information from the interim
landmark, the agent the relative measurement is taken from, the interim master
can calculate and broadcast a set of intermediate variables which each robot
can then use to update its estimates to match that of a centralized Extended
Kalman Filter for cooperative localization. Once an update is done, no further
communication is needed until the next relative measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05914</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05914</id><created>2015-05-21</created><updated>2015-05-25</updated><authors><author><keyname>Xu</keyname><forenames>Huijuan</forenames></author><author><keyname>Venugopalan</keyname><forenames>Subhashini</forenames></author><author><keyname>Ramanishka</keyname><forenames>Vasili</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>A Multi-scale Multiple Instance Video Description Network</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating natural language descriptions for in-the-wild videos is a
challenging task. Most state-of-the-art methods for solving this problem borrow
existing deep convolutional neural network (CNN) architectures (AlexNet,
GoogLeNet) to extract a visual representation of the input video. However,
these deep CNN architectures are designed for single-label centered-positioned
object classification. While they generate strong semantic features, they have
no inherent structure allowing them to detect multiple objects of different
sizes and locations in the frame. Our paper tries to solve this problem by
integrating the base CNN into several fully convolutional neural networks
(FCNs) to form a multi-scale network that handles multiple receptive field
sizes in the original image. FCNs, previously applied to image segmentation,
can generate class heat-maps efficiently compared to sliding window mechanisms,
and can easily handle multiple scales. To further handle the ambiguity over
multiple objects and locations, we incorporate the Multiple Instance Learning
mechanism (MIL) to consider objects in different positions and at different
scales simultaneously. We integrate our multi-scale multi-instance architecture
with a sequence-to-sequence recurrent neural network to generate sentence
descriptions based on the visual representation. Ours is the first end-to-end
trainable architecture that is capable of multi-scale region processing.
Evaluation on a Youtube video dataset shows the advantage of our approach
compared to the original single-scale whole frame CNN model. Our flexible and
efficient architecture can potentially be extended to support other video
processing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05916</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05916</id><created>2015-05-21</created><authors><author><keyname>Wood</keyname><forenames>Erroll</forenames></author><author><keyname>Baltrusaitis</keyname><forenames>Tadas</forenames></author><author><keyname>Zhang</keyname><forenames>Xucong</forenames></author><author><keyname>Sugano</keyname><forenames>Yusuke</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author><author><keyname>Bulling</keyname><forenames>Andreas</forenames></author></authors><title>Rendering of Eyes for Eye-Shape Registration and Gaze Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Images of the eye are key in several computer vision problems, such as shape
registration and gaze estimation. Recent large-scale supervised methods for
these problems require time-consuming data collection and manual annotation,
which can be unreliable. We propose synthesizing perfectly labelled
photo-realistic training data in a fraction of the time. We used computer
graphics techniques to build a collection of dynamic eye-region models from
head scan geometry. These were randomly posed to synthesize close-up eye images
for a wide range of head poses, gaze directions, and illumination conditions.
We used our model's controllability to verify the importance of realistic
illumination and shape variations in eye-region training data. Finally, we
demonstrate the benefits of our synthesized training data (SynthesEyes) by
out-performing state-of-the-art methods for eye-shape registration as well as
cross-dataset appearance-based gaze estimation in the wild.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05917</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05917</id><created>2015-05-21</created><authors><author><keyname>Li</keyname><forenames>Shang</forenames></author><author><keyname>Li</keyname><forenames>Xiaoou</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Liu</keyname><forenames>Jingchen</forenames></author></authors><title>Decentralized Sequential Composite Hypothesis Test Based on One-Bit
  Communication</title><categories>stat.AP cs.IT math.IT</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the sequential composite hypothesis test with multiple
sensors. The sensors observe random samples in parallel and communicate with a
fusion center, who makes the global decision based on the sensor inputs. On one
hand, in the centralized scenario, where local samples are precisely
transmitted to the fusion center, the generalized sequential likelihood ratio
test (GSPRT) is shown to be asymptotically optimal in terms of the expected
sample size as error rates tend to zero. On the other hand, for systems with
limited power and bandwidth resources, decentralized solutions that only send a
summary of local samples (we particularly focus on a one-bit communication
protocol) to the fusion center is of great importance. To this end, we first
consider a decentralized scheme where sensors send their one-bit quantized
statistics every fixed period of time to the fusion center. We show that such a
uniform sampling and quantization scheme is strictly suboptimal and its
suboptimality can be quantified by the KL divergence of the distributions of
the quantized statistics under both hypotheses. We then propose a decentralized
GSPRT based on level-triggered sampling. That is, each sensor runs its own
GSPRT repeatedly and reports its local decision to the fusion center
asynchronously. We show that this scheme is asymptotically optimal as the local
thresholds and global thresholds grow large at different rates. Lastly, two
particular models and their associated applications are studied to compare the
centralized and decentralized approaches. Numerical results are provided to
demonstrate that the proposed level-triggered sampling based decentralized
scheme aligns closely with the centralized scheme with substantially lower
communication overhead, and significantly outperforms the uniform sampling and
quantization based decentralized scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05921</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05921</id><created>2015-05-21</created><authors><author><keyname>Driggs-Campbell</keyname><forenames>Katherine</forenames></author><author><keyname>Bajcsy</keyname><forenames>Ruzena</forenames></author></authors><title>Identifying Modes of Intent from Driver Behaviors in Dynamic
  Environments</title><categories>cs.SY</categories><comments>Submitted to ITSC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In light of growing attention of intelligent vehicle systems, we propose
developing a driver model that uses a hybrid system formulation to capture the
intent of the driver. This model hopes to capture human driving behavior in a
way that can be utilized by semi- and fully autonomous systems in heterogeneous
environments. We consider a discrete set of high level goals or intent modes,
that is designed to encompass the decision making process of the human. A
driver model is derived using a dataset of lane changes collected in a
realistic driving simulator, in which the driver actively labels data to give
us insight into her intent. By building the labeled dataset, we are able to
utilize classification tools to build the driver model using features of based
on her perception of the environment, and achieve high accuracy in identifying
driver intent. Multiple algorithms are presented and compared on the dataset,
and a comparison of the varying behaviors between drivers is drawn. Using this
modeling methodology, we present a model that can be used to assess driver
behaviors and to develop human-inspired safety metrics that can be utilized in
intelligent vehicular systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05927</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05927</id><created>2015-05-21</created><authors><author><keyname>Postle</keyname><forenames>Luke</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Five-list-coloring graphs on surfaces II. A linear bound for critical
  graphs in a disk</title><categories>math.CO cs.DM</categories><comments>25 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a plane graph with outer cycle $C$ and let $(L(v):v\in V(G))$ be a
family of sets such that $|L(v)|\ge 5$ for every $v\in V(G)$. By an
$L$-coloring of a subgraph $J$ of $G$ we mean a (proper) coloring $\phi$ of $J$
such that $\phi(v)\in L(v)$ for every vertex $v$ of $J$. We prove a conjecture
of Dvorak et al. that if $H$ is a minimal subgraph of $G$ such that $C$ is a
subgraph of $H$ and every $L$-coloring of $C$ that extends to an $L$-coloring
of $H$ also extends to an $L$-coloring of $G$, then $|V(H)|\le 19|V(C)|$.
  This is a lemma that plays an important role in subsequent papers, because it
motivates the study of graphs embedded in surfaces that satisfy an
isoperimetric inequality suggested by this result. Such study turned out to be
quite profitable for the subject of list coloring graphs on surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05932</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05932</id><created>2015-05-21</created><authors><author><keyname>YD</keyname><forenames>Sumith</forenames></author><author><keyname>Maroo</keyname><forenames>Shalabh C.</forenames></author></authors><title>A new algorithm for contact angle estimation in molecular dynamics
  simulations</title><categories>cond-mat.soft cs.CE</categories><report-no>InterPACKICNMM2015-48569</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  It is important to study contact angle of a liquid on a solid surface to
understand its wetting properties, capillarity and surface interaction energy.
While performing transient molecular dynamics (MD) simulations it requires
calculating the time evolution of contact angle. This is a tedious effort to do
manually or with image processing algorithms. In this work we propose a new
algorithm to estimate contact angle from MD simulations directly and in a
computationally efficient way. This algorithm segregates the droplet molecules
from the vapor molecules using Mahalanobis distance (MND) technique. Then the
density is smeared onto a 2D grid using 4th order B-spline interpolation
function. The vapor liquid interface data is estimated from the grid using
density filtering. With the interface data a circle is fitted using Landau
method. The equation of this circle is solved for obtaining the contact angle.
This procedure is repeated by rotating the droplet about the vertical axis. We
have applied this algorithm to a number of studies (different potentials and
thermostat methods) which involves the MD simulation of water.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05935</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05935</id><created>2015-05-21</created><authors><author><keyname>Hyder</keyname><forenames>Md Mashud</forenames></author><author><keyname>Mahata</keyname><forenames>Kaushik</forenames></author></authors><title>A sparse recovery method for initial ranging in IEEE 802.16 OFDMA
  systems</title><categories>cs.IT cs.NI math.IT</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Initial ranging constitutes a part of the synchronization procedure employed
by the wireless communication standards. This allows the base station (BS) to
detect the subscriber stations (SS) that are willing to commence communication.
In addition, the ranging process allows the BS to estimate the uplink channel
parameters of these SSs. Accurate estimation of these parameters are crucial as
they ensure that the uplink signals from all the SSs arrive at the BS
synchronously and approximately at the same power level. However, this
detection and estimation problem turns out to be very challenging when multiple
users initiate the ranging procedure at the same time. We address this issue by
exploiting the underlying sparsity of the estimation problem. We propose a fast
sparse signal recovery approach to improve the ranging performance in
multi-user environment. Compared to the standard correlation based techniques,
our method shows a clear improvement in ranging code detection, timing offset
and channel power estimation. Although this method has been developed around
the WiMAX standard, the underlying principles apply to other OFDM based
standards as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05939</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05939</id><created>2015-05-21</created><authors><author><keyname>Vu</keyname><forenames>Thang X.</forenames></author><author><keyname>Duhamel</keyname><forenames>Pierre</forenames></author></authors><title>Finite-SNR Analysis of Partial Relaying with Relay Selection in
  Channel-coded Cooperative Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the performance of a cooperative network which consists of
two channel-coded sources, multiple relays, and one destination. Due to
spectral efficiency constraint, we assume only one time slot is dedicated for
relaying. Conventional network coding based cooperation (NCC) selects the best
relay which uses network coding to serve two sources simultaneously. The
performance in terms of bit error rate (BER) of NCC, however, is not available
in the literature. In this paper, we first derive the closed-form expression
for the BER of NCC and analytically show that NCC always achieves diversity of
order two regardless the number of available relays and the channel code.
Secondly, motivated by a loss in diversity in NCC, we propose a novel relaying
scheme based on partial relaying cooperation (PARC) in which two best relays
are selected, each forwarding half of the codeword to help one source.
Closed-form expression for BER and system diversity order of the proposed
scheme are derived. Analytical results show that the diversity order of PARC is
a function of the operating signal-to-noise ratio (SNR) and the minimum
distance of the channel code. More importantly, full diversity order in PARC
can be achieved for practically operating finite SNRs with the proper channel
code. Finally, intensive simulations present a huge SNR gain of PARC over NCC
and reference schemes without relay selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05946</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05946</id><created>2015-05-22</created><updated>2015-06-14</updated><authors><author><keyname>Feyzabadi</keyname><forenames>Seyedshams</forenames></author><author><keyname>Carpin</keyname><forenames>Stefano</forenames></author></authors><title>Motion Planning with Safety Constraints and High-Level Task
  Specifications</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to solve planning problems involving sequential decision
making in unpredictable environments while accomplishing a high level task
specification expressed using the formalism of linear temporal logic. Our
method improves the state of the art by introducing a pruning step that
preserves correctness while significantly reducing the time needed to compute
an optimal policy. Our theoretical contribution is coupled with simulations
substantiating the value of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05947</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05947</id><created>2015-05-22</created><authors><author><keyname>Lavin</keyname><forenames>Alexander</forenames></author></authors><title>A Pareto Front-Based Multiobjective Path Planning Algorithm</title><categories>cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path planning is one of the most vital elements of mobile robotics. With a
priori knowledge of the environment, global path planning provides a
collision-free route through the workspace. The global path plan can be
calculated with a variety of informed search algorithms, most notably the A*
search method, guaranteed to deliver a complete and optimal solution that
minimizes the path cost. Path planning optimization typically looks to minimize
the distance traversed from start to goal, yet many mobile robot applications
call for additional path planning objectives, presenting a multiobjective
optimization (MOO) problem. Past studies have applied genetic algorithms to MOO
path planning problems, but these may have the disadvantages of computational
complexity and suboptimal solutions. Alternatively, the algorithm in this paper
approaches MOO path planning with the use of Pareto fronts, or finding
non-dominated solutions. The algorithm presented incorporates Pareto optimality
into every step of A* search, thus it is named A*-PO. Results of simulations
show A*-PO outperformed several variations of the standard A* algorithm for MOO
path planning. A planetary exploration rover case study was added to
demonstrate the viability of A*-PO in a real-world application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05956</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05956</id><created>2015-05-22</created><updated>2015-09-12</updated><authors><author><keyname>Huang</keyname><forenames>Xin</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author><author><keyname>Cheng</keyname><forenames>Hong</forenames></author></authors><title>Approximate Closest Community Search in Networks</title><categories>cs.SI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been significant interest in the study of the community
search problem in social and information networks: given one or more query
nodes, find densely connected communities containing the query nodes. However,
most existing studies do not address the &quot;free rider&quot; issue, that is, nodes far
away from query nodes and irrelevant to them are included in the detected
community. Some state-of-the-art models have attempted to address this issue,
but not only are their formulated problems NP-hard, they do not admit any
approximations without restrictive assumptions, which may not always hold in
practice.
  In this paper, given an undirected graph G and a set of query nodes Q, we
study community search using the k-truss based community model. We formulate
our problem of finding a closest truss community (CTC), as finding a connected
k-truss subgraph with the largest k that contains Q, and has the minimum
diameter among such subgraphs. We prove this problem is NP-hard. Furthermore,
it is NP-hard to approximate the problem within a factor $(2-\varepsilon)$, for
any $\varepsilon &gt;0 $. However, we develop a greedy algorithmic framework,
which first finds a CTC containing Q, and then iteratively removes the furthest
nodes from Q, from the graph. The method achieves 2-approximation to the
optimal solution. To further improve the efficiency, we make use of a compact
truss index and develop efficient algorithms for k-truss identification and
maintenance as nodes get eliminated. In addition, using bulk deletion
optimization and local exploration strategies, we propose two more efficient
algorithms. One of them trades some approximation quality for efficiency while
the other is a very efficient heuristic. Extensive experiments on 6 real-world
networks show the effectiveness and efficiency of our community model and
search algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05957</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05957</id><created>2015-05-22</created><authors><author><keyname>Shu</keyname><forenames>Tianmin</forenames></author><author><keyname>Xie</keyname><forenames>Dan</forenames></author><author><keyname>Rothrock</keyname><forenames>Brandon</forenames></author><author><keyname>Todorovic</keyname><forenames>Sinisa</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author></authors><title>Joint Inference of Groups, Events and Human Roles in Aerial Videos</title><categories>cs.CV</categories><comments>CVPR 2015 Oral Presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of drones, aerial video analysis becomes increasingly
important; yet, it has received scant attention in the literature. This paper
addresses a new problem of parsing low-resolution aerial videos of large
spatial areas, in terms of 1) grouping, 2) recognizing events and 3) assigning
roles to people engaged in events. We propose a novel framework aimed at
conducting joint inference of the above tasks, as reasoning about each in
isolation typically fails in our setting. Given noisy tracklets of people and
detections of large objects and scene surfaces (e.g., building, grass), we use
a spatiotemporal AND-OR graph to drive our joint inference, using Markov Chain
Monte Carlo and dynamic programming. We also introduce a new formalism of
spatiotemporal templates characterizing latent sub-events. For evaluation, we
have collected and released a new aerial videos dataset using a hex-rotor
flying over picnic areas rich with group events. Our results demonstrate that
we successfully address above inference tasks under challenging conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05958</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05958</id><created>2015-05-22</created><authors><author><keyname>Hua</keyname><forenames>Jingyu</forenames></author><author><keyname>Shen</keyname><forenames>Zhenyu</forenames></author><author><keyname>Zhong</keyname><forenames>Sheng</forenames></author></authors><title>We Can Track You If You Take the Metro: Tracking Metro Riders Using
  Accelerometers on Smartphones</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion sensors (e.g., accelerometers) on smartphones have been demonstrated
to be a powerful side channel for attackers to spy on users' inputs on
touchscreen. In this paper, we reveal another motion accelerometer-based attack
which is particularly serious: when a person takes the metro, a malicious
application on her smartphone can easily use accelerator readings to trace her.
We first propose a basic attack that can automatically extract metro-related
data from a large amount of mixed accelerator readings, and then use an
ensemble interval classier built from supervised learning to infer the riding
intervals of the user. While this attack is very effective, the supervised
learning part requires the attacker to collect labeled training data for each
station interval, which is a significant amount of effort. To improve the
efficiency of our attack, we further propose a semi-supervised learning
approach, which only requires the attacker to collect labeled data for a very
small number of station intervals with obvious characteristics. We conduct real
experiments on a metro line in a major city. The results show that the
inferring accuracy could reach 89\% and 92\% if the user takes the metro for 4
and 6 stations, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05960</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05960</id><created>2015-05-22</created><authors><author><keyname>Chen</keyname><forenames>Qingjun</forenames></author><author><keyname>Qian</keyname><forenames>Chen</forenames></author><author><keyname>Zhong</keyname><forenames>Sheng</forenames></author></authors><title>Privacy-preserving Cross-domain Routing Optimization -- A Cryptographic
  Approach</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's large-scale enterprise networks, data center networks, and wide area
networks can be decomposed into multiple administrative or geographical
domains. Domains may be owned by different administrative units or
organizations. Hence protecting domain information is an important concern.
Existing general-purpose Secure Multi-Party Computation (SMPC) methods that
preserves privacy for domains are extremely slow for cross-domain routing
problems. In this paper we present PYCRO, a cryptographic protocol specifically
designed for privacy-preserving cross-domain routing optimization in Software
Defined Networking (SDN) environments. PYCRO provides two fundamental routing
functions, policy-compliant shortest path computing and bandwidth allocation,
while ensuring strong protection for the private information of domains. We
rigorously prove the privacy guarantee of our protocol. We have implemented a
prototype system that runs PYCRO on servers in a campus network. Experimental
results using real ISP network topologies show that PYCRO is very efficient in
computation and communication costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05962</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05962</id><created>2015-05-22</created><authors><author><keyname>Yadav</keyname><forenames>Pankaj Kumar</forenames></author><author><keyname>Pandey</keyname><forenames>Sriniwas</forenames></author><author><keyname>Mohanty</keyname><forenames>Sraban Kumar</forenames></author></authors><title>Nearest Neighbor based Clustering Algorithm for Large Data Sets</title><categories>cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is an unsupervised learning technique in which data or objects are
grouped into sets based on some similarity measure. Most of the clustering
algorithms assume that the main memory is infinite and can accommodate the set
of patterns. In reality many applications give rise to a large set of patterns
which does not fit in the main memory. When the data set is too large, much of
the data is stored in the secondary memory. Input/Outputs (I/O) from the disk
are the major bottleneck in designing efficient clustering algorithms for large
data sets. Different designing techniques have been used to design clustering
algorithms for large data sets. External memory algorithms are one class of
algorithms which can be used for large data sets. These algorithms exploit the
hierarchical memory structure of the computers by incorporating locality of
reference directly in the algorithm. This paper makes some contribution towards
designing clustering algorithms in the external memory model (Proposed by
Aggarwal and Vitter 1988) to make the algorithms scalable. In this paper, it is
shown that the Shared near neighbors algorithm is not very I/O efficient since
the computational complexity is same as the I/O complexity. The algorithm is
designed in the external memory model and I/O complexity is reduced. The
computational complexity remains same. We substantiate the theoretical analysis
by showing the performance of the algorithms with their traditional counterpart
by implementing in STXXL library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05964</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05964</id><created>2015-05-22</created><authors><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author></authors><title>CCS: It's not Fair! Fair Schedulers cannot be implemented in CCS-like
  languages even under progress and certain fairness assumptions</title><categories>cs.LO</categories><acm-class>F.1.2; F.3.2</acm-class><journal-ref>Acta Informatica 52(2-3), 2015, pp. 175-205</journal-ref><doi>10.1007/s00236-015-0221-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the process algebra community it is sometimes suggested that, on some
level of abstraction, any distributed system can be modelled in standard
process-algebraic specification formalisms like CCS. This sentiment is
strengthened by results testifying that CCS, like many similar formalisms, is
Turing powerful and provides a mechanism for interaction. This paper counters
that sentiment by presenting a simple fair scheduler---one that in suitable
variations occurs in many distributed systems---of which no implementation can
be expressed in CCS, unless CCS is enriched with a fairness assumption.
  Since Dekker's and Peterson's mutual exclusion protocols implement fair
schedulers, it follows that these protocols cannot be rendered correctly in CCS
without imposing a fairness assumption. Peterson expressed this algorithm
correctly in pseudocode without resorting to a fairness assumption, so it
furthermore follows that CCS lacks the expressive power to accurately capture
such pseudocode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05969</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05969</id><created>2015-05-22</created><authors><author><keyname>Piech</keyname><forenames>Chris</forenames></author><author><keyname>Huang</keyname><forenames>Jonathan</forenames></author><author><keyname>Nguyen</keyname><forenames>Andy</forenames></author><author><keyname>Phulsuksombati</keyname><forenames>Mike</forenames></author><author><keyname>Sahami</keyname><forenames>Mehran</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas</forenames></author></authors><title>Learning Program Embeddings to Propagate Feedback on Student Code</title><categories>cs.LG cs.NE cs.SE</categories><comments>Accepted to International Conference on Machine Learning (ICML 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing feedback, both assessing final work and giving hints to stuck
students, is difficult for open-ended assignments in massive online classes
which can range from thousands to millions of students. We introduce a neural
network method to encode programs as a linear mapping from an embedded
precondition space to an embedded postcondition space and propose an algorithm
for feedback at scale using these linear maps as features. We apply our
algorithm to assessments from the Code.org Hour of Code and Stanford
University's CS1 course, where we propagate human comments on student
assignments to orders of magnitude more submissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05972</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05972</id><created>2015-05-22</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Instant Learning: Parallel Deep Neural Networks and Convolutional
  Bootstrapping</title><categories>cs.LG</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although deep neural networks (DNN) are able to scale with direct advances in
computational power (e.g., memory and processing speed), they are not well
suited to exploit the recent trends for parallel architectures. In particular,
gradient descent is a sequential process and the resulting serial dependencies
mean that DNN training cannot be parallelized effectively. Here, we show that a
DNN may be replicated over a massive parallel architecture and used to provide
a cumulative sampling of local solution space which results in rapid and robust
learning. We introduce a complimentary convolutional bootstrapping approach
that enhances performance of the parallel architecture further. Our
parallelized convolutional bootstrapping DNN out-performs an identical
fully-trained traditional DNN after only a single iteration of training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05977</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05977</id><created>2015-05-22</created><updated>2015-06-01</updated><authors><author><keyname>Hu</keyname><forenames>Yiqing</forenames></author><author><keyname>Xiong</keyname><forenames>Yan</forenames></author><author><keyname>Huang</keyname><forenames>Wenchao</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Zhang</keyname><forenames>Yanan</forenames></author><author><keyname>Mao</keyname><forenames>Xufei</forenames></author><author><keyname>Yang</keyname><forenames>Panlong</forenames></author><author><keyname>Wang</keyname><forenames>Caimei</forenames></author></authors><title>A Visible Light Based Indoor Positioning System</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel indoor localization scheme that exploits
ubiquitous visible lights, which are necessarily and densely deployed in almost
all indoor environments. We unveil two phenomena of lights available for
positioning: 1) the light strength varies according to different light sources,
which can be easily detected by light sensors embedded in COTS devices (e.g.,
smart-phone, smart-glass and smart-watch); 2) the light strength is stable in
different times of the day thus exploiting it can avoid frequent site-survey
and database maintenance. Hence, a user could locate oneself by differentiating
the light source of received light strength (RLS). However, different from
existing positioning systems that exploit special LEDs, ubiquitous visible
lights lack fingerprints that can uniquely identify the light source, which
results in an ambiguity problem that an RLS may correspond to multiple
positions. Moreover, RLS is not only determined by device's position, but also
seriously affected by its orientation, which causes great complexity in
site-survey. To address these challenges, we first propose and validate a
realistic light strength model that can attributes RLS to arbitrary positions
with heterogenous orientations. This model is further perfected by taking
account of the device diversity, influence of multiple light sources and
shading of obstacles. Then we design a localizing scheme that harness user's
mobility to generate spatial-related RLS to tackle the position-ambiguity
problem of a single RLS, which is robust against sunlight interference, shading
effect of human-body and unpredictable behaviours (e.g., put the device in
pocket) of user. Experiment results show that our scheme achieves mean accuracy
$1.93$m and $1.98$m in office ($720m^2$) and library scenario ($960m^2$)
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05983</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05983</id><created>2015-05-22</created><updated>2016-03-07</updated><authors><author><keyname>Chauve</keyname><forenames>Cedric</forenames><affiliation>LIX, AMIB</affiliation></author><author><keyname>Courtiel</keyname><forenames>Julien</forenames><affiliation>LIX, AMIB</affiliation></author><author><keyname>Ponty</keyname><forenames>Yann</forenames><affiliation>LIX, AMIB</affiliation></author></authors><title>Counting, generating and sampling tree alignments</title><categories>q-bio.QM cs.DS</categories><comments>ALCOB - 3rd International Conference on Algorithms for Computational
  Biology - 2016, Jun 2016, Trujillo, Spain. 2016</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pairwise ordered tree alignment are combinatorial objects that appear in RNA
secondary structure comparison. However, the usual representation of tree
alignments as supertrees is ambiguous, i.e. two distinct supertrees may induce
identical sets of matches between identical pairs of trees. This ambiguity is
uninformative, and detrimental to any probabilistic analysis.In this work, we
consider tree alignments up to equivalence. Our first result is a precise
asymptotic enumeration of tree alignments, obtained from a context-free grammar
by mean of basic analytic combinatorics. Our second result focuses on
alignments between two given ordered trees $S$ and $T$. By refining our grammar
to align specific trees, we obtain a decomposition scheme for the space of
alignments, and use it to design an efficient dynamic programming algorithm for
sampling alignments under the Gibbs-Boltzmann probability distribution. This
generalizes existing tree alignment algorithms, and opens the door for a
probabilistic analysis of the space of suboptimal RNA secondary structures
alignments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06002</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06002</id><created>2015-05-22</created><authors><author><keyname>Natarajan</keyname><forenames>Lakshmi</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Line of Sight 2 x nr MIMO with Random Antenna Orientations</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications. 30 pages,
  12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Line-of-sight (LoS) multiple-input multiple-output (MIMO) gives full
spatial-multiplexing gain when the antenna array geometry and orientation are
designed based on the inter-terminal distance. These known design
methodologies, that hold for antenna arrays with fixed orientation, do not
provide full MIMO gains for arbitrary array orientations. In this paper, we
study LoS MIMO channels with random array orientations when the number of
transmit antennas used for signalling is 2. We study the impact of common array
geometries on error probability, and identify the code design parameter that
describes the high signal-to-noise ratio (SNR) error performance of a
space-time block code. For planar receive arrays, the error rate is shown to
decay only as fast as that of a rank 1 channel, and no better than SNR^-3 for a
class of codes that includes VBLAST. We then show that for the tetrahedral
receive array, which uses the smallest number of antennas among non-planar
arrays, the error rate decays faster than that of rank 1 channels and is
exponential in SNR for every coding scheme. Finally, we design a LoS MIMO
system that guarantees a good error performance for all transmit/receive array
orientations and over a range of inter-terminal distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06003</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06003</id><created>2015-05-22</created><authors><author><keyname>Ponge</keyname><forenames>Julien</forenames><affiliation>CITI</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CITI</affiliation></author><author><keyname>Stouls</keyname><forenames>Nicolas</forenames><affiliation>CITI</affiliation></author><author><keyname>Loiseau</keyname><forenames>Yannick</forenames><affiliation>LIMOS</affiliation></author></authors><title>Opportunities for a Truffle-based Golo Interpreter</title><categories>cs.PL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Golo is a simple dynamically-typed language for the Java Virtual Machine.
Initially implemented as a ahead-of-time compiler to JVM bytecode, it leverages
invokedy-namic and JSR 292 method handles to implement a reasonably efficient
runtime. Truffle is emerging as a framework for building interpreters for JVM
languages with self-specializing AST nodes. Combined with the Graal compiler,
Truffle offers a simple path towards writing efficient interpreters while
keeping the engineering efforts balanced. The Golo project is interested in
experimenting with a Truffle interpreter in the future, as it would provides
interesting comparison elements between invokedynamic versus Truffle for
building a language runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06004</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06004</id><created>2015-05-22</created><updated>2015-05-26</updated><authors><author><keyname>Viriyasitavat</keyname><forenames>Wantanee</forenames></author><author><keyname>Boban</keyname><forenames>Mate</forenames></author><author><keyname>Tsai</keyname><forenames>Hsin-Mu</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios</forenames></author></authors><title>Vehicular Communications: Survey and Challenges of Channel and
  Propagation Models</title><categories>cs.NI</categories><journal-ref>IEEE Vehicular Technology Magazine, vol. 10, no. 2, pp. 55 - 66,
  June 2015</journal-ref><doi>10.1109/MVT.2015.2410341</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular communication is characterized by a dynamic environment, high
mobility, and comparatively low antenna heights on the communicating entities
(vehicles and roadside units). These characteristics make vehicular propagation
and channel modeling particularly challenging. In this article, we classify and
describe the most relevant vehicular propagation and channel models, with a
particular focus on the usability of the models for the evaluation of protocols
and applications. We first classify the models based on the propagation
mechanisms they employ and their implementation approach. We also classify the
models based on the channel properties they implement and pay special attention
to the usability of the models, including the complexity of implementation,
scalability, and the input requirements (e.g., geographical data input). We
also discuss the less-explored aspects in vehicular channel modeling, including
modeling specific environments (e.g., tunnels, overpasses, and parking lots)
and types of communicating vehicles (e.g., scooters and public transportation
vehicles). We conclude by identifying the underresearched aspects of vehicular
propagation and channel modeling that require further modeling and measurement
studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06007</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06007</id><created>2015-05-22</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Scientometrics and Science Studies: From Words and Co-Words to
  Information and Probabilistic Entropy</title><categories>cs.DL</categories><comments>Journal of the International Society for Scientometrics and
  Informetrics JISSI 2 (1996) 33-39</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tension between qualitative theorizing and quantitative methods is
pervasive in the social sciences, and poses a constant challenge to empirical
research. But in science studies as an interdisciplinary specialty, there are
additional reasons why a more reflexive consciousness of the differences among
the relevant disciplines is necessary. How can qualitative insights from the
history of ideas and the sociology of science be combined with the quantitative
perspective? By using the example of the lexical and semantic value of word
occurrences, the issue of qualitatively different meanings of the same
phenomena is discussed as a methodological problem. Nine criteria for methods
which are needed for the development of science studies as an integrated
enterprise can then be specified. Information calculus is suggested as a method
which can comply with these criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06010</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06010</id><created>2015-05-22</created><authors><author><keyname>Aguil&#xf3;</keyname><forenames>F.</forenames></author><author><keyname>Miralles</keyname><forenames>A.</forenames></author><author><keyname>Zaragoz&#xe1;</keyname><forenames>M.</forenames></author></authors><title>Optimal extensions and quotients of 2--Cayley Digraphs</title><categories>math.CO cs.DM</categories><comments>15 pages, 4 tables, 3 figures</comments><msc-class>05012, 05C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite Abelian group $G$ and a generator subset $A\subset G$ of
cardinality two, we consider the Cayley digraph $\Gamma=$Cay$(G,A)$. This
digraph is called $2$--Cayley digraph. An extension of $\Gamma$ is a
$2$--Cayley digraph, $\Gamma'=$Cay$(G',A)$ with $G&lt;G'$, such that there is some
subgroup $H&lt;G'$ satisfying the digraph isomorphism
Cay$(G'/H,A)\cong$Cay$(G,A)$. We also call the digraph $\Gamma$ a quotient of
$\Gamma'$. Notice that the generator set does not change. A $2$--Cayley digraph
is called optimal when its diameter is optimal with respect to its order.
  In this work we define two procedures, E and Q, which generate a particular
type of extensions and quotients of $2$--Cayley digraphs, respectively. These
procedures are used to obtain optimal quotients and extensions. Quotients
obtained by procedure Q of optimal $2$--Cayley digraphs are proved to be also
optimal. The number of tight extensions, generated by procedure E from a given
tight digraph, is characterized. Tight digraphs for which procedure E gives
infinite tight extensions are also characterized. Finally, these two procedures
allow the obtention of new optimal families of $2$--Cayley digraphs and also
the improvement of the diameter of many proposals in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06012</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06012</id><created>2015-05-22</created><authors><author><keyname>Kehoe</keyname><forenames>Joseph</forenames></author></authors><title>The Specification of Sugarscape</title><categories>cs.MA</categories><comments>80 pages of mainly Z</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sugarscape is a well known and influential Agent Based Social Simulation
(ABSS). Various parts of Sugarscape are supplied as examples in almost all
Agent Based Model (ABM) toolkits. It has been used for demonstrating the
applicability of different approaches to ABM. However a lack of agreement on
the precise definition of the rules within Sugarscape has curtailed its
usefulness. We provide a formal specification of Sugarscape using the Z
specification language. This demonstrates the ability of formal specification
to capture the definition of an ABM in a precise manner. It shows that formal
specifications could be used as an approach to tackle the replication problem
in the field of ABM. It also provides the first clear interpretation of
Sugarscape identifying areas where information is missing and/or ambiguous.
This enables researchers to make proper comparisons between different
implementations of this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06022</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06022</id><created>2015-05-22</created><authors><author><keyname>Hachisuka</keyname><forenames>Toshiya</forenames></author></authors><title>Implementing a Photorealistic Rendering System using GLSL</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ray tracing on GPUs is becoming quite common these days. There are many
publicly available documents on how to implement basic ray tracing on GPUs for
spheres and implicit surfaces. We even have some general frameworks for ray
tracing on GPUs. We however hardly find details on how to implement more
complex ray tracing algorithms themselves that are commonly used for
photorealistic rendering. This paper explains an implementation of a
stand-alone rendering system on GPUs which supports the bounding volume
hierarchy and stochastic progressive photon mapping. The key characteristic of
the system is that it uses only GLSL shaders without relying on any platform
dependent feature. The system can thus run on many platforms that support
OpenGL, making photorealistic rendering on GPUs widely accessible. This paper
also sketches practical ideas for stackless traversal and pseudorandom number
generation which both fit well with the limited system configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06024</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06024</id><created>2015-05-22</created><updated>2016-01-13</updated><authors><author><keyname>Pastor-Satorras</keyname><forenames>Romualdo</forenames></author><author><keyname>Castellano</keyname><forenames>Claudio</forenames></author></authors><title>Distinct types of eigenvector localization in networks</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>Final version: 16 pages, 8 figures. Open access article available
  online at http://www.nature.com/articles/srep18847</comments><journal-ref>Scientific Reports 6, 18847 (2016)</journal-ref><doi>10.1038/srep18847</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spectral properties of the adjacency matrix provide a trove of
information about the structure and function of complex networks. In
particular, the largest eigenvalue and its associated principal eigenvector are
crucial in the understanding of nodes centrality and the unfolding of dynamical
processes. Here we show that two distinct types of localization of the
principal eigenvector may occur in heterogeneous networks. For synthetic
networks with degree distribution $P(q) \sim q^{-\gamma}$, localization occurs
on the largest hub if $\gamma&gt;5/2$; for $\gamma&lt;5/2$ a new type of localization
arises on a mesoscopic subgraph associated with the shell with the largest
index in the $K$-core decomposition. Similar evidence for the existence of
distinct localization modes is found in the analysis of real-world networks.
Our results open a new perspective on dynamical processes on networks and on a
recently proposed alternative measure of node centrality based on the
non-backtracking matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06025</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06025</id><created>2015-05-22</created><authors><author><keyname>Andrade</keyname><forenames>Ricardo</forenames><affiliation>LBBE Lyon / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Birmel&#xe9;</keyname><forenames>Etienne</forenames><affiliation>MAP5</affiliation></author><author><keyname>Mary</keyname><forenames>Arnaud</forenames><affiliation>MAP5</affiliation></author><author><keyname>Picchetti</keyname><forenames>Thomas</forenames><affiliation>MAP5</affiliation></author><author><keyname>Sagot</keyname><forenames>Marie-France</forenames><affiliation>LBBE Lyon / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>Incremental complexity of a bi-objective hypergraph transversal problem</title><categories>cs.CC</categories><proxy>ccsd</proxy><report-no>MAP5 2015-16</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hypergraph transversal problem has been intensively studied, from both a
theoretical and a practical point of view. In particular , its incremental
complexity is known to be quasi-polynomial in general and polynomial for
bounded hypergraphs. Recent applications in computational biology however
require to solve a generalization of this problem, that we call bi-objective
transversal problem. The instance is in this case composed of a pair of
hypergraphs (A, B), and the aim is to find minimal sets which hit all the
hyperedges of A while intersecting a minimal set of hyperedges of B. In this
paper, we formalize this problem, link it to a problem on monotone boolean
$\land$ -- $\lor$ formulae of depth 3 and study its incremental complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06027</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06027</id><created>2015-05-22</created><updated>2015-12-21</updated><authors><author><keyname>Bojanowski</keyname><forenames>Piotr</forenames><affiliation>WILLOW, LIENS</affiliation></author><author><keyname>Lajugie</keyname><forenames>R&#xe9;mi</forenames><affiliation>LIENS, SIERRA</affiliation></author><author><keyname>Grave</keyname><forenames>Edouard</forenames><affiliation>APAM</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, SIERRA</affiliation></author><author><keyname>Laptev</keyname><forenames>Ivan</forenames><affiliation>WILLOW, LIENS</affiliation></author><author><keyname>Ponce</keyname><forenames>Jean</forenames><affiliation>WILLOW, LIENS</affiliation></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames><affiliation>LEAR</affiliation></author></authors><title>Weakly-Supervised Alignment of Video With Text</title><categories>cs.CV cs.CL</categories><comments>ICCV 2015 - IEEE International Conference on Computer Vision, Dec
  2015, Santiago, Chile</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that we are given a set of videos, along with natural language
descriptions in the form of multiple sentences (e.g., manual annotations, movie
scripts, sport summaries etc.), and that these sentences appear in the same
temporal order as their visual counterparts. We propose in this paper a method
for aligning the two modalities, i.e., automatically providing a time stamp for
every sentence. Given vectorial features for both video and text, we propose to
cast this task as a temporal assignment problem, with an implicit linear
mapping between the two feature modalities. We formulate this problem as an
integer quadratic program, and solve its continuous convex relaxation using an
efficient conditional gradient algorithm. Several rounding procedures are
proposed to construct the final integer solution. After demonstrating
significant improvements over the state of the art on the related task of
aligning video with symbolic labels [7], we evaluate our method on a
challenging dataset of videos with associated textual descriptions [36], using
both bag-of-words and continuous representations for text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06032</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06032</id><created>2015-05-22</created><authors><author><keyname>Matic</keyname><forenames>Dragan</forenames></author><author><keyname>Kratica</keyname><forenames>Jozef</forenames></author><author><keyname>Filipovic</keyname><forenames>Vladimir</forenames></author></authors><title>Variable Neighborhood Search for solving Bandwidth Coloring Problem</title><categories>cs.DS cs.DM</categories><msc-class>90C59, 68T20, 05C15</msc-class><acm-class>G.1.6; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a variable neighborhood search (VNS) algorithm for
solving bandwidth coloring problem (BCP) and bandwidth multicoloring problem
(BMCP). BCP and BMCP are generalizations of the well known vertex coloring
problem and they are of a great interest from both theoretical and practical
points of view. Presented VNS combines a shaking procedure which perturbs the
colors for an increasing number of vertices and a specific variable
neighborhood descent (VND) procedure, based on the specially designed
arrangement of the vertices which are the subject of re-coloring. By this
approach, local search is split in a series of disjoint procedures, enabling
better choice of the vertices which are addressed to re-color. The experiments
show that proposed method is highly competitive with the state-of-the-art
algorithms and improves 2 out of 33 previous best known solutions for BMCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06036</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06036</id><created>2015-05-22</created><updated>2016-01-04</updated><authors><author><keyname>Francis</keyname><forenames>Mathew C.</forenames></author><author><keyname>Lahiri</keyname><forenames>Abhiruk</forenames></author></authors><title>VPG and EPG bend-numbers of Halin Graphs</title><categories>math.CO cs.DM</categories><comments>11 pages, 3 figures</comments><msc-class>05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A piecewise linear curve in the plane made up of $k+1$ line segments, each of
which is either horizontal or vertical, with consecutive segments being of
different orientation is called a $k$-bend path. Given a graph $G$, a
collection of $k$-bend paths in which each path corresponds to a vertex in $G$
and two paths have a common point if and only if the vertices corresponding to
them are adjacent in $G$ is called a $B_k$-VPG representation of $G$.
Similarly, a collection of $k$-bend paths each of which corresponds to a vertex
in $G$ is called an $B_k$-EPG representation of $G$ if any two paths have a
line segment of non-zero length in common if and only if their corresponding
vertices are adjacent in $G$. The VPG bend-number $b_v(G)$ of a graph $G$ is
the minimum $k$ such that $G$ has a $B_k$-VPG representation. Similarly, the
EPG bend-number $b_e(G)$ of a graph $G$ is the minimum $k$ such that $G$ has a
$B_k$-EPG representation. Halin graphs are the graphs formed by taking a tree
with no degree $2$ vertex and then connecting its leaves to form a cycle in
such a way that the graph has a planar embedding. We prove that if $G$ is a
Halin graph then $b_v(G) \leq 1$ and $b_e(G) \leq 2$. These bounds are tight.
In fact, we prove the stronger result that if $G$ is a planar graph formed by
connecting the leaves of any tree to form a simple cycle, then it has a
VPG-representation using only one type of 1-bend paths and an
EPG-representation using only one type of 2-bend paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06056</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06056</id><created>2015-05-22</created><updated>2015-09-02</updated><authors><author><keyname>Schalk</keyname><forenames>Andrea</forenames><affiliation>The University of Manchester</affiliation></author><author><keyname>Steele</keyname><forenames>Hugh Paul</forenames><affiliation>Universit&#xe9; Paris 13</affiliation></author></authors><title>Constructing Fully Complete Models of Multiplicative Linear Logic</title><categories>cs.LO</categories><comments>72 pages. An extended abstract of this work appeared in the
  proceedings of LICS 2012</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:6) 2015</journal-ref><doi>10.2168/LMCS-11(3:6)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiplicative fragment of Linear Logic is the formal system in this
family with the best understood proof theory, and the categorical models which
best capture this theory are the fully complete ones. We demonstrate how the
Hyland-Tan double glueing construction produces such categories, either with or
without units, when applied to any of a large family of degenerate models. This
process explains as special cases a number of such models from the literature.
In order to achieve this result, we develop a tensor calculus for compact
closed categories with finite biproducts. We show how the combinatorial
properties required for a fully complete model are obtained by this glueing
construction adding to the structure already available from the original
category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06072</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06072</id><created>2015-05-22</created><updated>2015-12-22</updated><authors><author><keyname>Felzenszwalb</keyname><forenames>Pedro F.</forenames></author><author><keyname>Svaiter</keyname><forenames>Benar F.</forenames></author></authors><title>Diffusion Methods for Classification with Pairwise Relationships</title><categories>cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define two algorithms for propagating information in classification
problems with pairwise relationships. The algorithms are based on contraction
maps and are related to non-linear diffusion and random walks on graphs. The
approach is also related to message passing algorithms, including belief
propagation and mean field methods. The algorithms we describe are guaranteed
to converge on graphs with arbitrary topology. Moreover they always converge to
a unique fixed point, independent of initialization. We prove that the fixed
points of the algorithms under consideration define lower-bounds on the energy
function and the max-marginals of a Markov random field. The theoretical
results also illustrate a relationship between message passing algorithms and
value iteration for an infinite horizon Markov decision process. We illustrate
the practical application of the algorithms under study with numerical
experiments in image restoration, stereo depth estimation and binary
classification on a grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06073</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06073</id><created>2015-05-22</created><authors><author><keyname>Gomez</keyname><forenames>Jean-S{&#xe9;}bastien</forenames><affiliation>LTCI</affiliation></author><author><keyname>Vasseur</keyname><forenames>Aur{&#xe9;}lien</forenames><affiliation>LTCI</affiliation></author><author><keyname>Vergne</keyname><forenames>Ana{&#xef;}s</forenames><affiliation>LTCI</affiliation></author><author><keyname>Martins</keyname><forenames>Philippe</forenames><affiliation>LTCI</affiliation></author><author><keyname>Decreusefond</keyname><forenames>Laurent</forenames><affiliation>LTCI</affiliation></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>A case study on regularity in cellular network deployment</title><categories>math.PR cs.NI</categories><proxy>ccsd</proxy><journal-ref>IEEE Wireless Communications Letters, IEEE, 2015, pp.4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to validate the $\beta$-Ginibre point process as a model for
the distribution of base station locations in a cellular network. The
$\beta$-Ginibre is a repulsive point process in which repulsion is controlled
by the $\beta$ parameter. When $\beta$ tends to zero, the point process
converges in law towards a Poisson point process. If $\beta$ equals to one it
becomes a Ginibre point process. Simulations on real data collected in Paris
(France) show that base station locations can be fitted with a $\beta$-Ginibre
point process. Moreover we prove that their superposition tends to a Poisson
point process as it can be seen from real data. Qualitative interpretations on
deployment strategies are derived from the model fitting of the raw data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06079</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06079</id><created>2015-05-22</created><authors><author><keyname>Arrigoni</keyname><forenames>Federica</forenames></author><author><keyname>Fusiello</keyname><forenames>Andrea</forenames></author><author><keyname>Rossi</keyname><forenames>Beatrice</forenames></author><author><keyname>Fragneto</keyname><forenames>Pasqualina</forenames></author></authors><title>Robust Rotation Synchronization via Low-rank and Sparse Matrix
  Decomposition</title><categories>cs.CV</categories><comments>Submitted to IJCV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the rotation synchronization problem, which arises in
global registration of 3D point-sets and in structure from motion. The problem
is formulated in an unprecedented way as a &quot;low-rank and sparse&quot; matrix
decomposition that handles both outliers and missing data. A minimization
strategy, dubbed R-GoDec, is also proposed and evaluated experimentally against
state-of-the-art algorithms on simulated and real data. The results show that
R-GoDec is the fastest among the robust algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06107</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06107</id><created>2015-05-22</created><updated>2015-09-03</updated><authors><author><keyname>Czumaj</keyname><forenames>Artur</forenames></author><author><keyname>Davies</keyname><forenames>Peter</forenames></author></authors><title>Communicating with Beeps</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The beep model is a very weak communications model in which devices in a
network can communicate only via beeps and silence. As a result of its weak
assumptions, it has broad applicability to many different implementations of
communications networks. This comes at the cost of a restrictive environment
for algorithm design.
  Despite being only recently introduced, the beep model has received
considerable attention, in part due to its relationship with other
communication models such as that of ad-hoc radio networks. However, there has
been no definitive published result for several fundamental tasks in the model.
We aim to rectify this with our paper.
  We present algorithms for the tasks of broadcast, gossiping, and
multi-broadcast. Our $O(D+\log M)$-time algorithm for broadcasting is a simple
formalization of a concept known as beep waves, and is asymptotically optimal.
We give an $O(n \log L)$-time depth-first search procedure, and show how this
can be used as the basis for an $O(n \log LM)$-time gossiping algorithm.
Finally, we present almost optimal algorithms for the more general problem of
multi-broadcast. When message provenance is required, we give an $O(k \log
\frac{LM}{k}+D \log L)$-time algorithm and a corresponding $\Omega(k \log
\frac{LM}{k}+D)$ lower bound. When provenance is not required, we give an
algorithm taking $O(k \log \frac {M}{k}+D \log L)$ time when $M&gt;k$ and $O(M+D
\log L)$ otherwise, and a corresponding lower bound of $\Omega(k \log
\frac{M}{k}+D)$ when $M&gt;k$ and $\Omega(M+D)$ otherwise.
  Our algorithms are all explicit, deterministic, and practical, and give
efficient means of communication while making arguably the minimum possible
assumptions about the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06125</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06125</id><created>2015-05-22</created><authors><author><keyname>Mascharka</keyname><forenames>David</forenames></author><author><keyname>Manley</keyname><forenames>Eric</forenames></author></authors><title>Machine Learning for Indoor Localization Using Mobile Phone-Based
  Sensors</title><categories>cs.LG cs.NI</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the problem of localizing a mobile device based
on readings from its embedded sensors utilizing machine learning methodologies.
We consider a real-world environment, collect a large dataset of 3110
datapoints, and examine the performance of a substantial number of machine
learning algorithms in localizing a mobile device. We have found algorithms
that give a mean error as accurate as 0.76 meters, outperforming other indoor
localization systems reported in the literature. We also propose a hybrid
instance-based approach that results in a speed increase by a factor of ten
with no loss of accuracy in a live deployment over standard instance-based
methods, allowing for fast and accurate localization. Further, we determine how
smaller datasets collected with less density affect accuracy of localization,
important for use in real-world environments. Finally, we demonstrate that
these approaches are appropriate for real-world deployment by evaluating their
performance in an online, in-motion experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06130</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06130</id><created>2015-05-22</created><authors><author><keyname>Agarwal</keyname><forenames>Mukul</forenames></author><author><keyname>MItter</keyname><forenames>Sanjoy</forenames></author></authors><title>A randomized covering-packing duality between source-coding and
  channel-coding</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1302.5860</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A randomized covering-packing duality between source and channel coding will
be discussed by considering the source coding problem of coding a source with a
certain distortion level and by considering a channel which communicates the
source within a certain distortion level. An operational view of source-channel
separation for communication with a fidelity criterion will be discussed in
brief.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06146</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06146</id><created>2015-05-22</created><updated>2015-09-22</updated><authors><author><keyname>Galanis</keyname><forenames>Andreas</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author></authors><title>The complexity of approximately counting in 2-spin systems on
  $k$-uniform bounded-degree hypergraphs</title><categories>cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important recent developments in the complexity of
approximate counting is the classification of the complexity of approximating
the partition functions of antiferromagnetic 2-spin systems on bounded-degree
graphs. This classification is based on a beautiful connection to the so-called
uniqueness phase transition from statistical physics on the infinite
$\Delta$-regular tree. Our objective is to study the impact of this
classification on unweighted 2-spin models on $k$-uniform hypergraphs. As has
already been indicated by Yin and Zhao, the connection between the uniqueness
phase transition and the complexity of approximate counting breaks down in the
hypergraph setting. Nevertheless, we show that for every non-trivial symmetric
$k$-ary Boolean function $f$ there exists a degree bound $\Delta_0$ so that for
all $\Delta \geq \Delta_0$ the following problem is NP-hard: given a
$k$-uniform hypergraph with maximum degree at most $\Delta$, approximate the
partition function of the hypergraph 2-spin model associated with $f$. It is
NP-hard to approximate this partition function even within an exponential
factor. By contrast, if $f$ is a trivial symmetric Boolean function (e.g., any
function $f$ that is excluded from our result), then the partition function of
the corresponding hypergraph 2-spin model can be computed exactly in polynomial
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06149</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06149</id><created>2015-05-22</created><authors><author><keyname>Czumaj</keyname><forenames>Artur</forenames></author><author><keyname>Davies</keyname><forenames>Peter</forenames></author></authors><title>Optimal leader election in multi-hop radio networks</title><categories>cs.DC</categories><comments>14 pages, submitted to DISC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two optimal randomized leader election algorithms for multi-hop
radio networks, which run in expected time asymptotically equal to the time
required to broadcast one message to the entire network. We first observe that,
under certain assumptions, a simulation approach of Bar-Yehuda, Golreich and
Itai (1991) can be used to obtain an algorithm that for directed and undirected
networks elects a leader in $O(D \log\frac{n}{D} + \log^2 n)$ expected time,
where $n$ is the number of the nodes and $D$ is the eccentricity or the
diameter of the network. We then extend this approach and present a second
algorithm, which operates on undirected multi-hop radio networks with collision
detection and elects a leader in $O(D + \log n)$ expected run-time. This
algorithm in fact operates on the beep model, a strictly weaker model in which
nodes can only communicate via beeps or silence. Both of these algorithms are
optimal; no optimal expected-time algorithms for these models have been
previously known.
  We further apply our techniques to design an algorithm that is quicker to
achieve leader election with high probability. We give an algorithm for the
model without collision detection which always runs in time $O((D
\log\frac{n}{D} + \log^2 n)\cdot \sqrt{\log n})$, and succeeds with high
probability. While non-optimal, and indeed slightly slower than the algorithm
of Ghaffari and Haeupler (2013), it has the advantage of working in directed
networks; it is the fastest known leader election algorithm to achieve a
high-probability bound in such circumstances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06151</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06151</id><created>2015-05-22</created><authors><author><keyname>Doca</keyname><forenames>Cezar</forenames></author><author><keyname>Paunoiu</keyname><forenames>Constantin</forenames></author></authors><title>Automatic Detection of the Common and Non-common Frequencies in
  Congruent Discrete Spectra. A Theoretical Approach</title><categories>cs.NA</categories><comments>7 Pages; 10 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both sampling a time-varying signal, and its spectral analysis are activities
subjected to theoretically compelling, such as Shannon's theorem and the
objectively limiting of the frequency's resolution. Usually, the signals'
spectra are processed and interpreted by a scientist who, presumably, has
sufficient prior information about the monitored signals to conclude on the
significant frequencies, for example. On the other hand, processing and
interpretation of signals' spectra can be routine tasks that must be automated
using suitable software, i.e. PC application. In the above context, the paper
presents the theoretic bases of an intuitive and practical approach of the
(automatic) detection of the common and non-common frequencies in two or more
congruent spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06161</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06161</id><created>2015-05-22</created><authors><author><keyname>Caputo</keyname><forenames>Pietro</forenames></author><author><keyname>Martinelli</keyname><forenames>Fabio</forenames></author><author><keyname>Sinclair</keyname><forenames>Alistair</forenames></author><author><keyname>Stauffer</keyname><forenames>Alexandre</forenames></author></authors><title>Dynamics of Lattice Triangulations on Thin Rectangles</title><categories>math.PR cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider random lattice triangulations of $n\times k$ rectangular regions
with weight $\lambda^{|\sigma|}$ where $\lambda&gt;0$ is a parameter and
$|\sigma|$ denotes the total edge length of the triangulation. When
$\lambda\in(0,1)$ and $k$ is fixed, we prove a tight upper bound of order $n^2$
for the mixing time of the edge-flip Glauber dynamics. Combined with the
previously known lower bound of order $\exp(\Omega(n^2))$ for $\lambda&gt;1$ [3],
this establishes the existence of a dynamical phase transition for thin
rectangles with critical point at $\lambda=1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06162</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06162</id><created>2015-05-22</created><authors><author><keyname>George</keyname><forenames>Anjith</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>Design and Implementation of Real-time Algorithms for Eye Tracking and
  PERCLOS Measurement for on board Estimation of Alertness of Drivers</title><categories>cs.CV</categories><comments>Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The alertness level of drivers can be estimated with the use of computer
vision based methods. The level of fatigue can be found from the value of
PERCLOS. It is the ratio of closed eye frames to the total frames processed.
The main objective of the thesis is the design and implementation of real-time
algorithms for measurement of PERCLOS. In this work we have developed a
real-time system which is able to process the video onboard and to alarm the
driver in case the driver is in alert. For accurate estimation of PERCLOS the
frame rate should be greater than 4 and accuracy should be greater than 90%.
For eye detection we have used mainly two approaches Haar classifier based
method and Principal Component Analysis (PCA) based method for day time. During
night time active Near Infra Red (NIR) illumination is used. Local Binary
Pattern (LBP) histogram based method is used for the detection of eyes at night
time. The accuracy rate of the algorithms was found to be more than 90% at
frame rates more than 5 fps which was suitable for the application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06163</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06163</id><created>2015-05-22</created><updated>2015-07-13</updated><authors><author><keyname>Ju</keyname><forenames>Yong Chul</forenames></author><author><keyname>Maurer</keyname><forenames>Daniel</forenames></author><author><keyname>Breu&#xdf;</keyname><forenames>Michael</forenames></author><author><keyname>Bruhn</keyname><forenames>Andr&#xe9;s</forenames></author></authors><title>Direct Variational Perspective Shape from Shading with Cartesian Depth
  Parametrisation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of today's state-of-the-art methods for perspective shape from shading
are modelled in terms of partial differential equations (PDEs) of
Hamilton-Jacobi type. To improve the robustness of such methods w.r.t. noise
and missing data, first approaches have recently been proposed that seek to
embed the underlying PDE into a variational framework with data and smoothness
term. So far, however, such methods either make use of a radial depth
parametrisation that makes the regularisation hard to interpret from a
geometrical viewpoint or they consider indirect smoothness terms that require
additional consistency constraints to provide valid solutions. Moreover the
minimisation of such frameworks is an intricate task, since the underlying
energy is typically non-convex. In our paper we address all three of the
aforementioned issues. First, we propose a novel variational model that
operates directly on the Cartesian depth. In this context, we also point out a
common mistake in the derivation of the surface normal. Moreover, we employ a
direct second-order regulariser with edge-preservation property. This direct
regulariser yields by construction valid solutions without requiring additional
consistency constraints. Finally, we also propose a novel coarse-to-fine
minimisation framework based on an alternating explicit scheme. This framework
allows us to avoid local minima during the minimisation and thus to improve the
accuracy of the reconstruction. Experiments show the good quality of our model
as well as the usefulness of the proposed numerical scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06169</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06169</id><created>2015-05-22</created><authors><author><keyname>Strubell</keyname><forenames>Emma</forenames></author><author><keyname>Vilnis</keyname><forenames>Luke</forenames></author><author><keyname>Silverstein</keyname><forenames>Kate</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Learning Dynamic Feature Selection for Fast Sequential Prediction</title><categories>cs.CL cs.LG</categories><comments>Appears in The 53rd Annual Meeting of the Association for
  Computational Linguistics, Beijing, China, July 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present paired learning and inference algorithms for significantly
reducing computation and increasing speed of the vector dot products in the
classifiers that are at the heart of many NLP components. This is accomplished
by partitioning the features into a sequence of templates which are ordered
such that high confidence can often be reached using only a small fraction of
all features. Parameter estimation is arranged to maximize accuracy and early
confidence in this sequence. Our approach is simpler and better suited to NLP
than other related cascade methods. We present experiments in left-to-right
part-of-speech tagging, named entity recognition, and transition-based
dependency parsing. On the typical benchmarking datasets we can preserve POS
tagging accuracy above 97% and parsing LAS above 88.5% both with over a
five-fold reduction in run-time, and NER F1 above 88 with more than 2x increase
in speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06178</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06178</id><created>2015-05-22</created><authors><author><keyname>Paul</keyname><forenames>Thomas</forenames></author><author><keyname>Puscher</keyname><forenames>Daniel</forenames></author><author><keyname>Strufe</keyname><forenames>Thorsten</forenames></author></authors><title>Private Date Exposure in Facebook and the Impact of Comprehensible
  Audience Selection Controls</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy in Online Social Networks (OSNs) evolved from a niche topic to a
broadly discussed issue in a wide variety of media. Nevertheless, OSNs
drastically increase the amount of information that can be found about
individuals on the web. To estimate the dimension of data leakage in OSNs, we
measure the real exposure of user content of 4,182 Facebook users from 102
countries in the most popular OSN, Facebook. We further quantify the impact of
a comprehensible privacy control interface that has been shown to extremely
decrease configuration efforts as well as misconfiguration in audience
selection.
  Our study highlights the importance of usable security. (i) The total amount
of content that is visible to Facebook users does not dramatically decrease by
simplifying the audience selection interface, but the composition of the
visible content changes. (ii) Which information is uploaded to Facebook as well
as which information is shared with whom strongly depends on the user's country
of origin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06195</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06195</id><created>2015-05-21</created><updated>2015-07-10</updated><authors><author><keyname>Liu</keyname><forenames>Dishi</forenames></author><author><keyname>Matthies</keyname><forenames>Hermann G.</forenames></author></authors><title>Pivoted Cholesky decomposition by Cross Approximation for efficient
  solution of kernel systems</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large kernel systems are prone to be ill-conditioned. Pivoted Cholesky
decomposition (PCD) render a stable and efficient solution to the systems
without a perturbation of regularization. This paper proposes a new PCD
algorithm by tuning Cross Approximation (CA) algorithm to kernel matrices which
merges the merits of PCD and CA, and proves as well as numerically exemplifies
that it solves large kernel systems two-order more efficiently than those
resorts to regularization. As a by-product, a diagonal-pivoted CA technique is
also shown efficient in eigen-decomposition of large covariance matrices in an
uncertainty quantification problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06219</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06219</id><created>2015-04-06</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Sabyasachi</forenames></author><author><keyname>Mandal</keyname><forenames>Soham</forenames></author><author><keyname>Pratiher</keyname><forenames>Sawon</forenames></author><author><keyname>Changdar</keyname><forenames>Satyasaran</forenames></author><author><keyname>Burman</keyname><forenames>Ritwik</forenames></author><author><keyname>Ghosh</keyname><forenames>Nirmalya</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K.</forenames></author></authors><title>A comparative study between proposed Hyper Kurtosis based Modified
  Duo-Histogram Equalization (HKMDHE) and Contrast Limited Adaptive Histogram
  Equalization (CLAHE) for Contrast Enhancement Purpose of Low Contrast Human
  Brain CT scan images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a comparative study between proposed hyper kurtosis based
modified duo-histogram equalization (HKMDHE) algorithm and contrast limited
adaptive histogram enhancement (CLAHE) has been presented for the
implementation of contrast enhancement and brightness preservation of low
contrast human brain CT scan images. In HKMDHE algorithm, contrast enhancement
is done on the hyper-kurtosis based application. The results are very promising
of proposed HKMDHE technique with improved PSNR values and lesser AMMBE values
than CLAHE technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06225</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06225</id><created>2015-05-22</created><authors><author><keyname>Jain</keyname><forenames>Himanshu</forenames></author><author><keyname>Parchure</keyname><forenames>Abhineet</forenames></author><author><keyname>Broadwater</keyname><forenames>Robert P.</forenames></author><author><keyname>Dilek</keyname><forenames>Murat</forenames></author><author><keyname>Woyak</keyname><forenames>Jeremy</forenames></author></authors><title>Three-Phase Dynamics Simulation of Power Systems Using Combined
  Transmission and Distribution System Models</title><categories>cs.SY</categories><comments>8 pages, This paper has been submitted for possible publication to
  the IEEE Transactions on Power Systems. If accepted for publication, this
  version will be superseded by IEEE accepted version and the IEEE Copyright
  will apply</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new method for studying the electromechanical
transients in power systems using combined Transmission and Distribution (T&amp;D)
models. Unlike the traditional dynamics analysis techniques which assume a
balanced electric network, the methodology presented in this paper models
individual phases of the power system and associated imbalances in load and
generation. Therefore, the impacts of load imbalance, single phase distributed
generation and line impedance imbalance on electromechanical transients can be
studied. Although other existing software can also be used to study
electromechanical transients in unbalanced networks, such software model
electric networks through differential equations, which makes the study of
large electric networks time consuming. Thus, the methodology presented
includes imbalance modeling and overcomes the system size limitations of
existing software tools, providing the means for studying electromechanical
transients in large, unbalanced T&amp;D systems. The implementation of this
methodology in software is called the Three Phase Dynamics Analyzer (TPDA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06228</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06228</id><created>2015-05-22</created><authors><author><keyname>Elghannam</keyname><forenames>Fatma</forenames></author><author><keyname>El-Shishtawy</keyname><forenames>Tarek</forenames></author></authors><title>Keyphrase Based Evaluation of Automatic Text Summarization</title><categories>cs.CL</categories><comments>4 pages, 1 figure, 3 tables</comments><msc-class>94AXX</msc-class><journal-ref>International Journal of Computer Applications 117(7):5-8, May
  2015. ISBN : 973-93-80886-51-2</journal-ref><doi>10.5120/20564-2953</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of methods to deal with the informative contents of the text
units in the matching process is a major challenge in automatic summary
evaluation systems that use fixed n-gram matching. The limitation causes
inaccurate matching between units in a peer and reference summaries. The
present study introduces a new Keyphrase based Summary Evaluator KpEval for
evaluating automatic summaries. The KpEval relies on the keyphrases since they
convey the most important concepts of a text. In the evaluation process, the
keyphrases are used in their lemma form as the matching text unit. The system
was applied to evaluate different summaries of Arabic multi-document data set
presented at TAC2011. The results showed that the new evaluation technique
correlates well with the known evaluation systems: Rouge1, Rouge2, RougeSU4,
and AutoSummENG MeMoG. KpEval has the strongest correlation with AutoSummENG
MeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06236</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06236</id><created>2015-05-22</created><updated>2016-03-07</updated><authors><author><keyname>Farag</keyname><forenames>Amal</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Liu</keyname><forenames>Jiamin</forenames></author><author><keyname>Turkbey</keyname><forenames>Evrim</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>A Bottom-up Approach for Pancreas Segmentation using Cascaded
  Superpixels and (Deep) Image Patch Labeling</title><categories>cs.CV</categories><comments>14 pages, 14 figures, 2 tables</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Robust automated organ segmentation is a prerequisite for computer-aided
diagnosis (CAD), quantitative imaging analysis and surgical assistance. For
high-variability organs such as the pancreas, previous approaches report
undesirably low accuracies. We present a bottom-up approach for pancreas
segmentation in abdominal CT scans that is based on a hierarchy of information
propagation by classifying image patches at different resolutions; and
cascading superpixels. There are four stages: 1) decomposing CT slice images as
a set of disjoint boundary-preserving superpixels; 2) computing pancreas class
probability maps via dense patch labeling; 3) classifying superpixels by
pooling both intensity and probability features to form empirical statistics in
cascaded random forest frameworks; and 4) simple connectivity based
post-processing. The dense image patch labeling are conducted by: efficient
random forest classifier on image histogram, location and texture features; and
more expensive (but with better specificity) deep convolutional neural network
classification on larger image windows (with more spatial contexts). Evaluation
of the approach is performed on a database of 80 manually segmented CT volumes
in six-fold cross-validation (CV). Our achieved results are comparable, or
better than the state-of-the-art methods (evaluated by
&quot;leave-one-patient-out&quot;), with Dice 70.7% and Jaccard 57.9%. The computational
efficiency has been drastically improved in the order of 6~8 minutes, comparing
with others of ~10 hours per case. Finally, we implement a multi-atlas label
fusion (MALF) approach for pancreas segmentation using the same datasets. Under
six-fold CV, our bottom-up segmentation method significantly outperforms its
MALF counterpart: (70.7 +/- 13.0%) versus (52.5 +/- 20.8%) in Dice. Deep CNN
patch labeling confidences offer more numerical stability, reflected by smaller
standard deviations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06237</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06237</id><created>2015-05-22</created><authors><author><keyname>Bauer</keyname><forenames>Arnold</forenames></author><author><keyname>Gutjahr</keyname><forenames>Karlheinz</forenames></author><author><keyname>Paar</keyname><forenames>Gerhard</forenames></author><author><keyname>Kontrus</keyname><forenames>Heiner</forenames></author><author><keyname>Glatzl</keyname><forenames>Robert</forenames></author></authors><title>Tunnel Surface 3D Reconstruction from Unoriented Image Sequences</title><categories>cs.CV</categories><comments>Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</comments><report-no>OAGM/2015/10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3D documentation of the tunnel surface during construction requires fast
and robust measurement systems. In the solution proposed in this paper, during
tunnel advance a single camera is taking pictures of the tunnel surface from
several positions. The recorded images are automatically processed to gain a 3D
tunnel surface model. Image acquisition is realized by the
tunneling/advance/driving personnel close to the tunnel face (= the front end
of the advance). Based on the following fully automatic analysis/evaluation, a
decision on the quality of the outbreak can be made within a few minutes. This
paper describes the image recording system and conditions as well as the
stereo-photogrammetry based workflow for the continuously merged dense 3D
reconstruction of the entire advance region. Geo-reference is realized by means
of signalized targets that are automatically detected in the images. We report
on the results of recent testing under real construction conditions, and
conclude with prospects for further development in terms of on-site
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06241</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06241</id><created>2015-05-22</created><authors><author><keyname>Fazeli</keyname><forenames>Arman</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author></authors><title>PIR with Low Storage Overhead: Coding instead of Replication</title><categories>cs.IT math.IT</categories><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Private information retrieval (PIR) protocols allow a user to retrieve a data
item from a database without revealing any information about the identity of
the item being retrieved. Specifically, in information-theoretic $k$-server
PIR, the database is replicated among $k$ non-communicating servers, and each
server learns nothing about the item retrieved by the user. The cost of PIR
protocols is usually measured in terms of their communication complexity, which
is the total number of bits exchanged between the user and the servers, and
storage overhead, which is the ratio between the total number of bits stored on
all the servers and the number of bits in the database. Since single-server
information-theoretic PIR is impossible, the storage overhead of all existing
PIR protocols is at least $2$.
  In this work, we show that information-theoretic PIR can be achieved with
storage overhead arbitrarily close to the optimal value of $1$, without
sacrificing the communication complexity. Specifically, we prove that all known
$k$-server PIR protocols can be efficiently emulated, while preserving both
privacy and communication complexity but significantly reducing the storage
overhead. To this end, we distribute the $n$ bits of the database among $s+r$
servers, each storing $n/s$ coded bits (rather than replicas). For every fixed
$k$, the resulting storage overhead $(s+r)/s$ approaches $1$ as $s$ grows;
explicitly we have $r\le k\sqrt{s}(1+o(1))$. Moreover, in the special case $k =
2$, the storage overhead is only $1 + \frac{1}{s}$. In order to achieve these
results, we introduce and study a new kind of binary linear codes, called here
$k$-server PIR codes. We then show how such codes can be constructed, and we
establish several bounds on the parameters of $k$-server PIR codes. Finally, we
briefly discuss extensions of our results to nonbinary alphabets, to robust
PIR, and to $t$-private PIR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06249</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06249</id><created>2015-05-22</created><authors><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author><author><keyname>Gigu&#xe8;re</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>D&#xe9;raspe</keyname><forenames>Maxime</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author><author><keyname>Corbeil</keyname><forenames>Jacques</forenames></author></authors><title>Greedy Biomarker Discovery in the Genome with Applications to
  Antimicrobial Resistance</title><categories>q-bio.GN cs.LG stat.ML</categories><comments>Peer-reviewed and accepted for an oral presentation in the Greed is
  Great workshop at the International Conference on Machine Learning, Lille,
  France, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Set Covering Machine (SCM) is a greedy learning algorithm that produces
sparse classifiers. We extend the SCM for datasets that contain a huge number
of features. The whole genetic material of living organisms is an example of
such a case, where the number of feature exceeds 10^7. Three human pathogens
were used to evaluate the performance of the SCM at predicting antimicrobial
resistance. Our results show that the SCM compares favorably in terms of
sparsity and accuracy against L1 and L2 regularized Support Vector Machines and
CART decision trees. Moreover, the SCM was the only algorithm that could
consider the full feature space. For all other algorithms, the latter had to be
filtered as a preprocessing step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06250</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06250</id><created>2015-05-22</created><authors><author><keyname>Varadarajan</keyname><forenames>Balakrishnan</forenames></author><author><keyname>Toderici</keyname><forenames>George</forenames></author><author><keyname>Vijayanarasimhan</keyname><forenames>Sudheendra</forenames></author><author><keyname>Natsev</keyname><forenames>Apostol</forenames></author></authors><title>Efficient Large Scale Video Classification</title><categories>cs.CV cs.MM cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video classification has advanced tremendously over the recent years. A large
part of the improvements in video classification had to do with the work done
by the image classification community and the use of deep convolutional
networks (CNNs) which produce competitive results with hand- crafted motion
features. These networks were adapted to use video frames in various ways and
have yielded state of the art classification results. We present two methods
that build on this work, and scale it up to work with millions of videos and
hundreds of thousands of classes while maintaining a low computational cost. In
the context of large scale video processing, training CNNs on video frames is
extremely time consuming, due to the large number of frames involved. We
propose to avoid this problem by training CNNs on either YouTube thumbnails or
Flickr images, and then using these networks' outputs as features for other
higher level classifiers. We discuss the challenges of achieving this and
propose two models for frame-level and video-level classification. The first is
a highly efficient mixture of experts while the latter is based on long short
term memory neural networks. We present results on the Sports-1M video dataset
(1 million videos, 487 classes) and on a new dataset which has 12 million
videos and 150,000 labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06252</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06252</id><created>2015-05-22</created><authors><author><keyname>Kumar</keyname><forenames>Akshay</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author></authors><title>On the Tradeoff between Energy Harvesting and Caching in Wireless
  Networks</title><categories>cs.IT math.IT</categories><comments>To be presented at the IEEE International Conference on
  Communications (ICC), London, U.K., 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-powered, energy harvesting small cell base stations (SBS) are expected
to be an integral part of next-generation wireless networks. However, due to
uncertainties in harvested energy, it is necessary to adopt energy efficient
power control schemes to reduce an SBSs' energy consumption and thus ensure
quality-of-service (QoS) for users. Such energy-efficient design can also be
done via the use of content caching which reduces the usage of the
capacity-limited SBS backhaul. of popular content at SBS can also prove
beneficial in this regard by reducing the backhaul usage. In this paper, an
online energy efficient power control scheme is developed for an energy
harvesting SBS equipped with a wireless backhaul and local storage. In our
model, energy arrivals are assumed to be Poisson distributed and the popularity
distribution of requested content is modeled using Zipf's law. The power
control problem is formulated as a (discounted) infinite horizon dynamic
programming problem and solved numerically using the value iteration algorithm.
Using simulations, we provide valuable insights on the impact of energy
harvesting and caching on the energy and sum-throughput performance of the SBS
as the network size is varied. Our results also show that the size of cache and
energy harvesting equipment at the SBS can be traded off, while still meeting
the desired system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06256</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06256</id><created>2015-05-22</created><authors><author><keyname>Li</keyname><forenames>Tong Shu</forenames></author><author><keyname>Good</keyname><forenames>Benjamin M.</forenames></author><author><keyname>Su</keyname><forenames>Andrew I.</forenames></author></authors><title>Exposing ambiguities in a relation-extraction gold standard with
  crowdsourcing</title><categories>cs.CL q-bio.QM</categories><comments>4 pages, 3 figures In: Bio-Ontologies SIG, ISMB: 10 July 2015, Dublin</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Semantic relation extraction is one of the frontiers of biomedical natural
language processing research. Gold standards are key tools for advancing this
research. It is challenging to generate these standards because of the high
cost of expert time and the difficulty in establishing agreement between
annotators. We implemented and evaluated a microtask crowdsourcing approach
that can produce a gold standard for extracting drug-disease relations. The
aggregated crowd judgment agreed with expert annotations from a pre-existing
corpus on 43 of 60 sentences tested. The levels of crowd agreement varied in a
similar manner to the levels of agreement among the original expert annotators.
This work rein-forces the power of crowdsourcing in the process of assembling
gold standards for relation extraction. Further, it high-lights the importance
of exposing the levels of agreement between human annotators, expert or crowd,
in gold standard corpora as these are reproducible signals indicating
ambiguities in the data or in the annotation guidelines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06258</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06258</id><created>2015-05-22</created><authors><author><keyname>Ghali</keyname><forenames>Cesar</forenames></author><author><keyname>Schlosberg</keyname><forenames>Marc A.</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author><author><keyname>Wood</keyname><forenames>Christopher A.</forenames></author></authors><title>Interest-Based Access Control for Content Centric Networks (extended
  version)</title><categories>cs.NI cs.CR</categories><comments>11 pages, 2 figures</comments><doi>10.1145/2810156.2810174</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-Centric Networking (CCN) is an emerging network architecture designed
to overcome limitations of the current IP-based Internet. One of the
fundamental tenets of CCN is that data, or content, is a named and addressable
entity in the network. Consumers request content by issuing interest messages
with the desired content name. These interests are forwarded by routers to
producers, and the resulting content object is returned and optionally cached
at each router along the path. In-network caching makes it difficult to enforce
access control policies on sensitive content outside of the producer since
routers only use interest information for forwarding decisions. To that end, we
propose an Interest-Based Access Control (IBAC) scheme that enables access
control enforcement using only information contained in interest messages,
i.e., by making sensitive content names unpredictable to unauthorized parties.
Our IBAC scheme supports both hash- and encryption-based name obfuscation. We
address the problem of interest replay attacks by formulating a mutual trust
framework between producers and consumers that enables routers to perform
authorization checks when satisfying interests from their cache. We assess the
computational, storage, and bandwidth overhead of each IBAC variant. Our design
is flexible and allows producers to arbitrarily specify and enforce any type of
access control on content, without having to deal with the problems of content
encryption and key distribution. This is the first comprehensive design for CCN
access control using only information contained in interest messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06262</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06262</id><created>2015-05-22</created><authors><author><keyname>Bennenni</keyname><forenames>Nabil</forenames></author><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>Greedy Construction of DNA Codes and New Bounds</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we construct linear codes over $\mathbb{Z}_4$ with bounded
$GC$-content. The codes are obtained using a greedy algorithm over
$\mathbb{Z}_4$. Further, upper and lower bounds are derived for the maximum
size of DNA codes of length $n$ with constant $GC$-content $w$ and edit
distance $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06263</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06263</id><created>2015-05-22</created><authors><author><keyname>Bennenni</keyname><forenames>Nabil</forenames></author><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Mesnager</keyname><forenames>Sihem</forenames></author></authors><title>New DNA Cyclic Codes over Rings</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper is dealing with DNA cyclic codes which play an important role in
DNA computing and have attracted a particular attention in the literature.
Firstly, we introduce a new family of DNA cyclic codes over the ring
$R=\mathbb{F}_2[u]/(u^6)$. Such codes have theoretical advantages as well as
several applications in DNA computing. A direct link between the elements of
such a ring and the $64$ codons used in the amino acids of the living organisms
is established. Such a correspondence allows us to extend the notion of the
edit distance to the ring $R$ which is useful for the correction of the
insertion, deletion and substitution errors. Next, we define the Lee weight,
the Gray map over the ring $R$ as well as the binary image of the cyclic DNA
codes allowing the transfer of studying DNA codes into studying binary codes.
Secondly, we introduce another new family of DNA skew cyclic codes constructed
over the ring $\tilde {R}=\mathbb{F}_2+v\mathbb{F}_2=\{0,1,v,v+1\}$ where
$v^2=v$ and study their property of being reverse-complement. We show that the
obtained code is derived from the cyclic reverse-complement code over the ring
$\tilde {R}$. We shall provide the binary images and present some explicit
examples of such codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06270</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06270</id><created>2015-05-22</created><authors><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Lizao</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author></authors><title>Two-Dimensional Pattern-Coupled Sparse Bayesian Learning via Generalized
  Approximate Message Passing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering two-dimensional (2-D) block-sparse
signals with \emph{unknown} cluster patterns. Two-dimensional block-sparse
patterns arise naturally in many practical applications such as foreground
detection and inverse synthetic aperture radar imaging. To exploit the
block-sparse structure, we introduce a 2-D pattern-coupled hierarchical
Gaussian prior model to characterize the statistical pattern dependencies among
neighboring coefficients. Unlike the conventional hierarchical Gaussian prior
model where each coefficient is associated independently with a unique
hyperparameter, the pattern-coupled prior for each coefficient not only
involves its own hyperparameter, but also its immediate neighboring
hyperparameters. Thus the sparsity patterns of neighboring coefficients are
related to each other and the hierarchical model has the potential to encourage
2-D structured-sparse solutions. An expectation-maximization (EM) strategy is
employed to obtain the maximum a posterior (MAP) estimate of the
hyperparameters, along with the posterior distribution of the sparse signal. In
addition, the generalized approximate message passing (GAMP) algorithm is
embedded into the EM framework to efficiently compute an approximation of the
posterior distribution of hidden variables, which results in a significant
reduction in computational complexity. Numerical results are provided to
illustrate the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06279</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06279</id><created>2015-05-23</created><authors><author><keyname>Maurer</keyname><forenames>Andreas</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author><author><keyname>Romera-Paredes</keyname><forenames>Bernardino</forenames></author></authors><title>The Benefit of Multitask Representation Learning</title><categories>stat.ML cs.LG</categories><comments>28 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a general method to learn data representations from multiple
tasks. We provide a justification for this method in both settings of multitask
learning and learning-to-learn. The method is illustrated in detail in the
special case of linear feature learning. Conditions on the theoretical
advantage offered by multitask representation learning over independent tasks
learning are established. In particular, focusing on the important example of
half-space learning, we derive the regime in which multitask representation
learning is beneficial over independent task learning, as a function of the
sample size, the number of tasks and the intrinsic data dimensionality. Other
potential applications of our results include multitask feature learning in
reproducing kernel Hilbert spaces and multilayer, deep networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06280</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06280</id><created>2015-05-23</created><authors><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author></authors><title>Risk-Sensitive Mean-Field-Type Games with Lp-norm Drifts</title><categories>math.OC cs.GT cs.MA cs.SY</categories><comments>37 pages, 8 figures. to appear in Automatica 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how risk-sensitive players act in situations where the outcome is
influenced not only by the state-action profile but also by the distribution of
it. In such interactive decision-making problems, the classical mean-field game
framework does not apply. We depart from most of the mean-field games
literature by presuming that a decision-maker may include its own-state
distribution in its decision. This leads to the class of mean-field-type games.
In mean-field-type situations, a single decision-maker may have a big impact on
the mean-field terms for which new type of optimality equations are derived. We
establish a finite dimensional stochastic maximum principle for mean-field-type
games where the drift functions have a p-norm structure which weaken the
classical Lipschitz and differentiability assumptions. Sufficient optimality
equations are established via Dynamic Programming Principle but in infinite
dimension. Using de Finetti-Hewitt-Savage theorem, we show that a propagation
of chaos property with 'virtual' particles holds for the non-linear
McKean-Vlasov dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06282</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06282</id><created>2015-05-23</created><authors><author><keyname>Gadaleta</keyname><forenames>Francesco</forenames></author></authors><title>Are we far from correctly inferring gene interaction networks with
  Lasso?</title><categories>cs.CE q-bio.GN</categories><comments>7 pages, 3 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Detecting the interactions of genetic compounds like genes, SNPs, proteins,
metabolites, etc. can potentially unravel the mechanisms behind complex traits
and common genetic disorders. Several methods have been taken into
consideration for the analysis of different types of genetic data, regression
being one of the most widely adopted. Without any doubt, a common data type is
represented by gene expression profiles, from which gene regulatory networks
have been inferred with different approaches. In this work we review nine
penalised regression methods applied to microarray data to infer the topology
of the network of interactions. We evaluate each method with respect to the
complexity of biological data. We analyse the limitations of each of them in
order to suggest a number of precautions that should be considered to make
their predictions more significant and reliable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06284</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06284</id><created>2015-05-23</created><authors><author><keyname>Younes</keyname><forenames>Ahmed</forenames></author></authors><title>A Bounded-error Quantum Polynomial Time Algorithm for Two Graph
  Bisection Problems</title><categories>quant-ph cs.CC cs.DS</categories><comments>17 Pages, 5 figures</comments><doi>10.1007/s11128-015-1069-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the paper is to propose a bounded-error quantum polynomial time
(BQP) algorithm for the max-bisection and the min-bisection problems. The
max-bisection and the min-bisection problems are fundamental NP-hard problems.
Given a graph with even number of vertices, the aim of the max-bisection
problem is to divide the vertices into two subsets of the same size to maximize
the number of edges between the two subsets, while the aim of the min-bisection
problem is to minimize the number of edges between the two subsets. The
proposed algorithm runs in $O(m^2)$ for a graph with $m$ edges and in the worst
case runs in $O(n^4)$ for a dense graph with $n$ vertices. The proposed
algorithm targets a general graph by representing both problems as Boolean
constraint satisfaction problems where the set of satisfied constraints are
simultaneously maximized/minimized using a novel iterative partial negation and
partial measurement technique. The algorithm is shown to achieve an arbitrary
high probability of success of $1-\epsilon$ for small $\epsilon&gt;0$ using a
polynomial space resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06286</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06286</id><created>2015-05-23</created><authors><author><keyname>Teng</keyname><forenames>Ya-Wen</forenames></author><author><keyname>Tai</keyname><forenames>Chih-Hua</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Chen</keyname><forenames>Ming-Syan</forenames></author></authors><title>An Effective Marketing Strategy for Revenue Maximization with a Quantity
  Constraint</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently the influence maximization problem has received much attention for
its applications on viral marketing and product promotions. However, such
influence maximization problems have not taken into account the monetary effect
on the purchasing decision of individuals. To fulfill this gap, in this paper,
we aim for maximizing the revenue by considering the quantity constraint on the
promoted commodity. For this problem, we not only identify a proper small group
of individuals as seeds for promotion but also determine the pricing of the
commodity. To tackle the revenue maximization problem, we first introduce a
strategic searching algorithm, referred to as Algorithm PRUB, which is able to
derive the optimal solutions. After that, we further modify PRUB to propose a
heuristic, Algorithm PRUB+IF, for obtaining feasible solutions more effciently
on larger instances. Experiments on real social networks with different
valuation distributions demonstrate the effectiveness of PRUB and PRUB+IF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06289</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06289</id><created>2015-05-23</created><updated>2015-06-04</updated><authors><author><keyname>Chang</keyname><forenames>Angel</forenames></author><author><keyname>Monroe</keyname><forenames>Will</forenames></author><author><keyname>Savva</keyname><forenames>Manolis</forenames></author><author><keyname>Potts</keyname><forenames>Christopher</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author></authors><title>Text to 3D Scene Generation with Rich Lexical Grounding</title><categories>cs.CL cs.GR</categories><comments>10 pages, 7 figures, 3 tables. To appear in ACL-IJCNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to map descriptions of scenes to 3D geometric representations has
many applications in areas such as art, education, and robotics. However, prior
work on the text to 3D scene generation task has used manually specified object
categories and language that identifies them. We introduce a dataset of 3D
scenes annotated with natural language descriptions and learn from this data
how to ground textual descriptions to physical objects. Our method successfully
grounds a variety of lexical terms to concrete referents, and we show
quantitatively that our method improves 3D scene generation over previous work
using purely rule-based methods. We evaluate the fidelity and plausibility of
3D scenes generated with our grounding approach through human judgments. To
ease evaluation on this task, we also introduce an automated metric that
strongly correlates with human judgments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06292</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06292</id><created>2015-05-23</created><authors><author><keyname>Wagner</keyname><forenames>Avishai</forenames></author><author><keyname>Zuk</keyname><forenames>Or</forenames></author></authors><title>Low-Rank Matrix Recovery from Row-and-Column Affine Measurements</title><categories>cs.LG cs.IT math.IT math.ST stat.CO stat.ML stat.TH</categories><comments>ICML 2015</comments><msc-class>15A83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and study a row-and-column affine measurement scheme for low-rank
matrix recovery. Each measurement is a linear combination of elements in one
row or one column of a matrix $X$. This setting arises naturally in
applications from different domains. However, current algorithms developed for
standard matrix recovery problems do not perform well in our case, hence the
need for developing new algorithms and theory for our problem. We propose a
simple algorithm for the problem based on Singular Value Decomposition ($SVD$)
and least-squares ($LS$), which we term \alg. We prove that (a simplified
version of) our algorithm can recover $X$ exactly with the minimum possible
number of measurements in the noiseless case. In the general noisy case, we
prove performance guarantees on the reconstruction accuracy under the Frobenius
norm. In simulations, our row-and-column design and \alg algorithm show
improved speed, and comparable and in some cases better accuracy compared to
standard measurements designs and algorithms. Our theoretical and experimental
results suggest that the proposed row-and-column affine measurements scheme,
together with our recovery algorithm, may provide a powerful framework for
affine matrix reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06294</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06294</id><created>2015-05-23</created><authors><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author></authors><title>A Frobenius Model of Information Structure in Categorical Compositional
  Distributional Semantics</title><categories>cs.CL cs.AI math.CT math.RA</categories><comments>Accepted for presentation in the 14th Meeting on Mathematics of
  Language (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The categorical compositional distributional model of Coecke, Sadrzadeh and
Clark provides a linguistically motivated procedure for computing the meaning
of a sentence as a function of the distributional meaning of the words therein.
The theoretical framework allows for reasoning about compositional aspects of
language and offers structural ways of studying the underlying relationships.
While the model so far has been applied on the level of syntactic structures, a
sentence can bring extra information conveyed in utterances via intonational
means. In the current paper we extend the framework in order to accommodate
this additional information, using Frobenius algebraic structures canonically
induced over the basis of finite-dimensional vector spaces. We detail the
theory, provide truth-theoretic and distributional semantics for meanings of
intonationally-marked utterances, and present justifications and extensive
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06295</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06295</id><created>2015-05-23</created><authors><author><keyname>Gawronski</keyname><forenames>Przemyslaw</forenames></author><author><keyname>Krawczyk</keyname><forenames>Malgorzata J.</forenames></author><author><keyname>Kulakowski</keyname><forenames>Krzysztof</forenames></author></authors><title>Emerging communities in networks - a flow of ties</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 5 figures</comments><journal-ref>Acta Phys. Pol. B 46 (2015) 911</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for search of communities in networks usually consist discrete
variations of links. Here we discuss a flow method, driven by a set of
differential equations. Two examples are demonstrated in detail. First is a
partition of a signed graph into two parts, where the proposed equations are
interpreted in terms of removal of a cognitive dissonance by agents placed in
the network nodes. There, the signs and values of links refer to positive or
negative interpersonal relationships of different strength. Second is an
application of a method akin to the previous one, dedicated to communities
identification, to the Sierpinski triangle of finite size. During the time
evolution, the related graphs are weighted; yet at the end the discrete
character of links is restored. In the case of the Sierpinski triangle, the
method is supplemented by adding a small noise to the initial connectivity
matrix. By breaking the symmetry of the network, this allows to a successful
handling of overlapping nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06299</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06299</id><created>2015-05-23</created><authors><author><keyname>Perrin</keyname><forenames>Matthieu</forenames></author><author><keyname>Jard</keyname><forenames>Claude</forenames></author><author><keyname>Mostefaoui</keyname><forenames>Achour</forenames></author></authors><title>Tracking Causal Dependencies in Web Services Orchestrations Defined in
  ORC</title><categories>cs.PL cs.DC</categories><comments>NETYS - 3rd International Conference on NETwork sYStems, May 2015,
  Agadir, Morocco. 2015, Proceedings of the third international conference on
  network systems</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article shows how the operational semantics of a language like ORC can
be instrumented so that the execution of a program produces information on the
causal dependencies between events. The concurrent semantics we obtain is based
on asymmetric labeled event structures. The approach is illustrated using a Web
service orchestration instance and the detection of race conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06306</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06306</id><created>2015-05-23</created><authors><author><keyname>Nagpal</keyname><forenames>Akshay</forenames></author><author><keyname>Panda</keyname><forenames>Supriya P.</forenames></author></authors><title>Career Path Suggestion using String Matching and Decision Trees</title><categories>cs.CY</categories><comments>3 pages, 4 figures, 1 table</comments><journal-ref>International Journal of Computer Applications 117(7):32-34, May
  2015</journal-ref><doi>10.5120/20569-2964</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  High school and college graduates seemingly are often battling for the
courses they should major in order to achieve their target career. In this
paper, we worked on suggesting a career path to a graduate to reach his/her
dream career given the current educational status. Firstly, we collected the
career data of professionals and academicians from various career fields and
compiled the data set by using the necessary information from the data.
Further, this was used as the basis to suggest the most appropriate career path
for the person given his/her current educational status. Decision trees and
string matching algorithms were employed to suggest the appropriate career path
for a person. Finally, an analysis of the result has been done directing to
further improvements in the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06307</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06307</id><created>2015-05-23</created><updated>2015-05-27</updated><authors><author><keyname>Akazaki</keyname><forenames>Takumi</forenames></author><author><keyname>Hasuo</keyname><forenames>Ichiro</forenames></author></authors><title>Time Robustness in MTL and Expressivity in Hybrid System Falsification
  (Extended Version)</title><categories>cs.SY cs.LO</categories><comments>22pages, a long version of the paper accepted in 27th International
  Conference on Computer Aided Verification (CAV 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building on the work by Fainekos and Pappas and the one by Donze and Maler,
we introduce AvSTL, an extension of metric interval temporal logic by averaged
temporal operators. Its expressivity in capturing both space and time
robustness helps solving falsification problems, (i.e. searching for a critical
path in hybrid system models); it does so by communicating a designer's
intention more faithfully to the stochastic optimization engine employed in a
falsification solver. We also introduce a sliding window-like algorithm that
keeps the cost of computing truth/robustness values tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06310</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06310</id><created>2015-05-23</created><authors><author><keyname>Ekelin</keyname><forenames>Svante</forenames></author><author><keyname>Johnsson</keyname><forenames>Andreas</forenames></author><author><keyname>Flinta</keyname><forenames>Christofer</forenames></author></authors><title>Scalability and Dimensioning of Network-Capacity Measurement System
  using Reflecting Servers</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a class of methods for measurement of available path capacity and other
capacity-related metrics in a network, trains of probe packets are transmitted
from a sender to a receiver across a network path, and the sequences of time
stamps at sending and reception are analyzed. In large-scale implementations
there may potentially be interference between the probe-packet trains
corresponding to several concurrent measurement sessions, due to congestion in
the network and common measurement end points.
  This paper outlines principles for large-scale deployments of network
capacity measurement methods using standardized network functionality. Further,
the paper provides an in-depth study of dimensioning and scalability challenges
related to the measurement end-points of such systems.
  The main result is a framework for dimensioning of large-scale network
capacity measurement systems based on TWAMP. The framework is based on a method
for explicit calculation of queuelength and waiting-time distributions, where
results from M/G/1 queuing theory are combined with Monte Carlo integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06311</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06311</id><created>2015-05-23</created><authors><author><keyname>Sapiezynski</keyname><forenames>Piotr</forenames></author><author><keyname>Stopczynski</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Gatej</keyname><forenames>Radu</forenames></author><author><keyname>Lehmann</keyname><forenames>Sune</forenames></author></authors><title>Tracking Human Mobility using WiFi signals</title><categories>cs.CY cs.SI</categories><doi>10.1371/journal.pone.0130824</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study six months of human mobility data, including WiFi and GPS traces
recorded with high temporal resolution, and find that time series of WiFi scans
contain a strong latent location signal. In fact, due to inherent stability and
low entropy of human mobility, it is possible to assign location to WiFi access
points based on a very small number of GPS samples and then use these access
points as location beacons. Using just one GPS observation per day per person
allows us to estimate the location of, and subsequently use, WiFi access points
to account for 80\% of mobility across a population. These results reveal a
great opportunity for using ubiquitous WiFi routers for high-resolution outdoor
positioning, but also significant privacy implications of such side-channel
location tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06312</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06312</id><created>2015-05-23</created><authors><author><keyname>Wang</keyname><forenames>Xiangrong</forenames></author><author><keyname>Ko&#xe7;</keyname><forenames>Yakup</forenames></author><author><keyname>Kooij</keyname><forenames>Robert E.</forenames></author><author><keyname>Van Mieghem</keyname><forenames>Piet</forenames></author></authors><title>A network approach for power grid robustness against cascading failures</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 13 figures conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascading failures are one of the main reasons for blackouts in electrical
power grids. Stable power supply requires a robust design of the power grid
topology. Currently, the impact of the grid structure on the grid robustness is
mainly assessed by purely topological metrics, that fail to capture the
fundamental properties of the electrical power grids such as power flow
allocation according to Kirchhoff's laws. This paper deploys the effective
graph resistance as a metric to relate the topology of a grid to its robustness
against cascading failures. Specifically, the effective graph resistance is
deployed as a metric for network expansions (by means of transmission line
additions) of an existing power grid. Four strategies based on network
properties are investigated to optimize the effective graph resistance,
accordingly to improve the robustness, of a given power grid at a low
computational complexity. Experimental results suggest the existence of
Braess's paradox in power grids: bringing an additional line into the system
occasionally results in decrease of the grid robustness. This paper further
investigates the impact of the topology on the Braess's paradox, and identifies
specific sub-structures whose existence results in Braess's paradox. Careful
assessment of the design and expansion choices of grid topologies incorporating
the insights provided by this paper optimizes the robustness of a power grid,
while avoiding the Braess's paradox in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06317</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06317</id><created>2015-05-23</created><authors><author><keyname>Kumar</keyname><forenames>Praneeth</forenames><suffix>V</suffix></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author></authors><title>On the Sum Capacity of the Gaussian X Channel in the Mixed Interference
  Regime</title><categories>cs.IT math.IT</categories><comments>To be presented at ISIT 2015, Hong Kong, China</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the Gaussian X channel in the mixed interference
regime. In this regime, multiple access transmission to one of the receivers is
shown to be close to optimal in terms of sum rate. Three upper bounds are
derived for the sum capacity in the mixed interference regime, and the
subregions where each of these bounds dominate the others are identified. The
genie-aided sum capacity upper bounds derived also show that the gap between
sum capacity and the sum rate of the multiple access transmission scheme is
small for a significant part of the mixed interference region. For any \delta &gt;
0, the region where multiple access transmission to one of the receivers is
within \delta from sum capacity is determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06319</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06319</id><created>2015-05-23</created><authors><author><keyname>de Sousa</keyname><forenames>Samuel</forenames></author><author><keyname>Kropatsch</keyname><forenames>Walter G.</forenames></author></authors><title>The Minimum Spanning Tree of Maximum Entropy</title><categories>cs.CV</categories><comments>Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</comments><report-no>OAGM/2015/15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer vision, we have the problem of creating graphs out of
unstructured point-sets, i.e. the data graph. A common approach for this
problem consists of building a triangulation which might not always lead to the
best solution. Small changes in the location of the points might generate
graphs with unstable configurations and the topology of the graph could change
significantly. After building the data-graph, one could apply Graph Matching
techniques to register the original point-sets. In this paper, we propose a
data graph technique based on the Minimum Spanning Tree of Maximum Entropty
(MSTME). We aim at a data graph construction which could be more stable than
the Delaunay triangulation with respect to small variations in the neighborhood
of points. Our technique aims at creating data graphs which could help the
point-set registration process. We propose an algorithm with a single free
parameter that weighs the importance between the total weight cost and the
entropy of the current spanning tree. We compare our algorithm on a number of
different databases with the Delaunay triangulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06320</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06320</id><created>2015-05-23</created><updated>2015-09-30</updated><authors><author><keyname>Roux</keyname><forenames>Stephane Le</forenames></author></authors><title>Infinite subgame perfect equilibrium in the Hausdorff difference
  hierarchy</title><categories>cs.GT math.LO</categories><comments>The alternative definition of the difference hierarchy has changed
  slightly</comments><msc-class>91A44, 91A18</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subgame perfect equilibria are specific Nash equilibria in perfect
information games in extensive form. They are important because they relate to
the rationality of the players. They always exist in infinite games with
continuous real-valued payoffs, but may fail to exist even in simple games with
slightly discontinuous payoffs. This article considers only games whose outcome
functions are measurable in the Hausdorff difference hierarchy of the open sets
(\textit{i.e.} $\Delta^0_2$ when in the Baire space), and it characterizes the
families of linear preferences such that every game using these preferences has
a subgame perfect equilibrium: the preferences without infinite ascending
chains (of course), and such that for all players $a$ and $b$ and outcomes
$x,y,z$ we have $\neg(z &lt;_a y &lt;_a x \,\wedge\, x &lt;_b z &lt;_b y)$. Moreover at
each node of the game, the equilibrium constructed for the proof is
Pareto-optimal among all the outcomes occurring in the subgame. Additional
results for non-linear preferences are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06324</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06324</id><created>2015-05-23</created><authors><author><keyname>Bekkouche</keyname><forenames>Mohammed</forenames></author><author><keyname>Collavizza</keyname><forenames>H&#xe9;l&#xe8;ne</forenames></author><author><keyname>Rueher</keyname><forenames>Michel</forenames></author></authors><title>Un algorithme incr\'emental dirig\'e par les flots et bas\'e sur les
  contraintes pour l'aide \`a la localisation d'erreurs</title><categories>cs.SE</categories><comments>in French</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this article, we present our improved algorithm for error localization
from counterexamples, LocFaults, flow-driven and constraint-based. This
algorithm analyzes the paths of CFG (Control Flow Graph) of the erroneous
program to calculate the subsets of suspicious instructions to correct the
program. Indeed, we generate a system of constraints for paths of control flow
graph for which at most k conditional statements can be wrong. Then we compute
the MCSs (Minimal Correction Set) of bounded size on each of these paths.
Removal of one of these sets of constraints gives maximal satisfiable subset,
in other words, a maximal satisfiable subset satisfying the postcondition. To
calculate the MCSs, we extend the generic algorithm proposed by Liffiton and
Sakallah in order to deal programs with numerical instructions more
effectively. We are interested to present the incremental aspect of this new
algorithm that is not yet presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06326</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06326</id><created>2015-05-23</created><updated>2015-07-17</updated><authors><author><keyname>Yang</keyname><forenames>Shudi</forenames></author><author><keyname>Yao</keyname><forenames>Zheng-An</forenames></author></authors><title>Complete Weight Enumerators of Some Linear Codes</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><msc-class>11T71, 94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes have been an interesting topic in both theory and practice for
many years. In this paper, for an odd prime $p$, we determine the explicit
complete weight enumerators of two classes of linear codes over $\mathbb{F}_p$
and they may have applications in cryptography and secret sharing schemes.
Moreover, some examples are included to illustrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06345</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06345</id><created>2015-05-23</created><authors><author><keyname>Ariyarathna</keyname><forenames>V.</forenames></author><author><keyname>Kulasekera</keyname><forenames>S.</forenames></author><author><keyname>Madanayake</keyname><forenames>A.</forenames></author><author><keyname>Suarez</keyname><forenames>D.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Bayer</keyname><forenames>F. M.</forenames></author><author><keyname>Belostotski</keyname><forenames>L.</forenames></author></authors><title>Multi-beam 4 GHz Microwave Apertures Using Current-Mode DFT
  Approximation on 65 nm CMOS</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures, In: IEEE International Microwave Symposium 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A current-mode CMOS design is proposed for realizing receive mode multi-beams
in the analog domain using a novel DFT approximation. High-bandwidth CMOS RF
transistors are employed in low-voltage current mirrors to achieve bandwidths
exceeding 4 GHz with good beam fidelity. Current mirrors realize the
coefficients of the considered DFT approximation, which take simple values in
$\{0, \pm1, \pm2\}$ only. This allows high bandwidths realizations using simple
circuitry without needing phase-shifters or delays. The proposed design is used
as a method to efficiently achieve spatial discrete Fourier transform operation
across a ULA to obtain multiple simultaneous RF beams. An example using 1.2 V
current-mode approximate DFT on 65 nm CMOS, with BSIM4 models from the RF kit,
show potential operation up to 4 GHz with eight independent aperture beams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06353</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06353</id><created>2015-05-23</created><authors><author><keyname>Mengistu</keyname><forenames>Henok</forenames></author><author><keyname>Huizinga</keyname><forenames>Joost</forenames></author><author><keyname>Mouret</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Clune</keyname><forenames>Jeff</forenames></author></authors><title>The evolutionary origins of hierarchy</title><categories>cs.NE</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical organization -- the recursive composition of sub-modules -- is
ubiquitous in biological networks, including neural, metabolic, ecological, and
genetic regulatory networks, and in human-made systems, such as large
organizations and the Internet. To date, most research on hierarchy in networks
has been limited to quantifying this property. However, an open, important
question in evolutionary biology is why hierarchical organization evolves in
the first place. It has recently been shown that modularity evolves because of
the presence of a cost for network connections. Here we investigate whether
such connection costs also tend to cause a hierarchical organization of such
modules. In computational simulations, we find that networks without a
connection cost do not evolve to be hierarchical, even when the task has a
hierarchical structure. However, with a connection cost, networks evolve to be
both modular and hierarchical, and these networks exhibit higher overall
performance and evolvability (i.e. faster adaptation to new environments).
Additional analyses confirm that hierarchy independently improves adaptability
after controlling for modularity. Overall, our results suggest that the same
force--the cost of connections--promotes the evolution of both hierarchy and
modularity, and that these properties are important drivers of network
performance and adaptability. In addition to shedding light on the emergence of
hierarchy across the many domains in which it appears, these findings will also
accelerate future research into evolving more complex, intelligent
computational brains in the fields of artificial intelligence and robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06360</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06360</id><created>2015-05-23</created><authors><author><keyname>Onnela</keyname><forenames>Jukka-Pekka</forenames></author><author><keyname>Khanna</keyname><forenames>Tarun</forenames></author></authors><title>Investigating population dynamics of the Kumbh Mela through the lens of
  cell phone data</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kumbh is a religious Hindu festival that has been celebrated for
centuries. The 2013 Kumbh Mela, a grander form of the annual Kumbh, was
purportedly the largest gathering of people in human history. Many of the
participants carried cell phones, making it possible for us to use a
data-driven approach to document this magnificent festival. We used Call Detail
Records (CDRs) from participants attending the event, a total of 390 million
records, to investigate its population dynamics. We report here on some of our
preliminary findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06362</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06362</id><created>2015-05-23</created><authors><author><keyname>Dinur</keyname><forenames>Irit</forenames></author><author><keyname>Harsha</keyname><forenames>Prahladh</forenames></author><author><keyname>Kindler</keyname><forenames>Guy</forenames></author></authors><title>Polynomially Low Error PCPs with polyloglog n Queries via Modular
  Composition</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that every language in NP has a PCP verifier that tosses $O(\log n)$
random coins, has perfect completeness, and a soundness error of at most
$1/\text{poly}(n)$, while making at most $O(\text{poly}\log\log n)$ queries
into a proof over an alphabet of size at most $n^{1/\text{poly}\log\log n}$.
Previous constructions that obtain $1/\text{poly}(n)$ soundness error used
either $\text{poly}\log n $ queries or an exponential sized alphabet, i.e. of
size $2^{n^c}$ for some $c&gt;0$. Our result is an exponential improvement in both
parameters simultaneously.
  Our result can be phrased as a polynomial-gap hardness for approximate CSPs
with arity $\text{poly}\log\log n$ and alphabet size $n^{1/\text{poly}\log n}$.
The ultimate goal, in this direction, would be to prove polynomial hardness for
CSPs with constant arity and polynomial alphabet size (aka the sliding scale
conjecture for inverse polynomial soundness error).
  Our construction is based on a modular generalization of previous PCP
constructions in this parameter regime, which involves a composition theorem
that uses an extra `consistency' query but maintains the inverse polynomial
relation between the soundness error and the alphabet size.
  Our main technical/conceptual contribution is a new notion of soundness,
which we refer to as {\em distributional soundness}, that replaces the previous
notion of &quot;list decoding soundness&quot;, and that allows us to prove a modular
composition theorem with tighter parameters. This new notion of soundness
allows us to invoke composition a super-constant number of times without
incurring a blow-up in the soundness error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06366</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06366</id><created>2015-05-23</created><updated>2015-06-12</updated><authors><author><keyname>Weinbaum</keyname><forenames>David</forenames><affiliation>Weaver</affiliation></author><author><keyname>Veitas</keyname><forenames>Viktoras</forenames></author></authors><title>Open Ended Intelligence: The individuation of Intelligent Agents</title><categories>cs.AI</categories><comments>Preprint; 35 pages, 2 figures; Keywords: intelligence, cognition,
  individuation, assemblage, self-organization, sense-making, coordination,
  enaction; en-US proofreading</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial General Intelligence is a field of research aiming to distill the
principles of intelligence that operate independently of a specific problem
domain or a predefined context and utilize these principles in order to
synthesize systems capable of performing any intellectual task a human being is
capable of and eventually go beyond that. While &quot;narrow&quot; artificial
intelligence which focuses on solving specific problems such as speech
recognition, text comprehension, visual pattern recognition, robotic motion,
etc. has shown quite a few impressive breakthroughs lately, understanding
general intelligence remains elusive. In the paper we offer a novel theoretical
approach to understanding general intelligence. We start with a brief
introduction of the current conceptual approach. Our critique exposes a number
of serious limitations that are traced back to the ontological roots of the
concept of intelligence. We then propose a paradigm shift from intelligence
perceived as a competence of individual agents defined in relation to an a
priori given problem domain or a goal, to intelligence perceived as a formative
process of self-organization by which intelligent agents are individuated. We
call this process open-ended intelligence. Open-ended intelligence is developed
as an abstraction of the process of cognitive development so its application
can be extended to general agents and systems. We introduce and discuss three
facets of the idea: the philosophical concept of individuation, sense-making
and the individuation of general cognitive agents. We further show how
open-ended intelligence can be framed in terms of a distributed,
self-organizing network of interacting elements and how such process is
scalable. The framework highlights an important relation between coordination
and intelligence and a new understanding of values. We conclude with a number
of questions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06376</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06376</id><created>2015-05-23</created><authors><author><keyname>Bonichon</keyname><forenames>Richard</forenames><affiliation>DIMAP - UFRN</affiliation></author><author><keyname>Hermant</keyname><forenames>Olivier</forenames></author></authors><title>A syntactic soundness proof for free-variable tableaux with on-the-fly
  Skolemization</title><categories>cs.LO cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the syntactic soundness of classical tableaux with free variables
and on-the-fly Skolemization. Soundness proofs are usually built from semantic
arguments, and this is to our knowledge, the first proof that appeals to
syntactic means. We actually prove the soundness property with respect to
cut-free sequent calculus. This requires great care because of the additional
liberty in freshness checking allowed by the use of Skolem terms. In contrast
to semantic soundness, we gain the possibility to state a cut elimination
theorem for sequent calculus, under the proviso that completeness of the method
holds. We believe that such techniques can be applied to tableaux in other
logics as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06378</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06378</id><created>2015-05-23</created><updated>2016-01-20</updated><authors><author><keyname>Gupta</keyname><forenames>Maya</forenames></author><author><keyname>Cotter</keyname><forenames>Andrew</forenames></author><author><keyname>Pfeifer</keyname><forenames>Jan</forenames></author><author><keyname>Voevodski</keyname><forenames>Konstantin</forenames></author><author><keyname>Canini</keyname><forenames>Kevin</forenames></author><author><keyname>Mangylov</keyname><forenames>Alexander</forenames></author><author><keyname>Moczydlowski</keyname><forenames>Wojtek</forenames></author><author><keyname>van Esbroeck</keyname><forenames>Alex</forenames></author></authors><title>Monotonic Calibrated Interpolated Look-Up Tables</title><categories>cs.LG</categories><comments>To appear (with minor revisions), Journal Machine Learning Research
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world machine learning applications may require functions that are
fast-to-evaluate and interpretable. In particular, guaranteed monotonicity of
the learned function can be critical to user trust. We propose meeting these
goals for low-dimensional machine learning problems by learning flexible,
monotonic functions using calibrated interpolated look-up tables. We extend the
structural risk minimization framework of lattice regression to train monotonic
look-up tables by solving a convex problem with appropriate linear inequality
constraints. In addition, we propose jointly learning interpretable
calibrations of each feature to normalize continuous features and handle
categorical or missing data, at the cost of making the objective non-convex. We
address large-scale learning through parallelization, mini-batching, and
propose random sampling of additive regularizer terms. Case studies with
real-world problems with five to sixteen features and thousands to millions of
training samples demonstrate the proposed monotonic functions can achieve
state-of-the-art accuracy on practical problems while providing greater
transparency to users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06379</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06379</id><created>2015-05-23</created><authors><author><keyname>Yazicioglu</keyname><forenames>A. Yasin</forenames></author><author><keyname>Egerstedt</keyname><forenames>Magnus</forenames></author><author><keyname>Shamma</keyname><forenames>Jeff S.</forenames></author></authors><title>Communication-Free Distributed Coverage for Networked Systems</title><categories>cs.SY cs.GT cs.MA cs.RO math.OC</categories><doi>10.1109/TCNS.2016.2518083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a communication-free algorithm for distributed
coverage of an arbitrary network by a group of mobile agents with local sensing
capabilities. The network is represented as a graph, and the agents are
arbitrarily deployed on some nodes of the graph. Any node of the graph is
covered if it is within the sensing range of at least one agent. The agents are
mobile devices that aim to explore the graph and to optimize their locations in
a decentralized fashion by relying only on their sensory inputs. We formulate
this problem in a game theoretic setting and propose a communication-free
learning algorithm for maximizing the coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06386</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06386</id><created>2015-05-23</created><authors><author><keyname>Trevisiol</keyname><forenames>Michele</forenames></author><author><keyname>Aiello</keyname><forenames>Luca Maria</forenames></author><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Blanco</keyname><forenames>Roi</forenames></author></authors><title>Local Ranking Problem on the BrowseGraph</title><categories>cs.DS cs.IR</categories><acm-class>H.4; E.1</acm-class><doi>10.1145/2766462.2767704</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;Local Ranking Problem&quot; (LRP) is related to the computation of a
centrality-like rank on a local graph, where the scores of the nodes could
significantly differ from the ones computed on the global graph. Previous work
has studied LRP on the hyperlink graph but never on the BrowseGraph, namely a
graph where nodes are webpages and edges are browsing transitions. Recently,
this graph has received more and more attention in many different tasks such as
ranking, prediction and recommendation. However, a web-server has only the
browsing traffic performed on its pages (local BrowseGraph) and, as a
consequence, the local computation can lead to estimation errors, which hinders
the increasing number of applications in the state of the art. Also, although
the divergence between the local and global ranks has been measured, the
possibility of estimating such divergence using only local knowledge has been
mainly overlooked. These aspects are of great interest for online service
providers who want to: (i) gauge their ability to correctly assess the
importance of their resources only based on their local knowledge, and (ii)
take into account real user browsing fluxes that better capture the actual user
interest than the static hyperlink network. We study the LRP problem on a
BrowseGraph from a large news provider, considering as subgraphs the
aggregations of browsing traces of users coming from different domains. We show
that the distance between rankings can be accurately predicted based only on
structural information of the local graph, being able to achieve an average
rank correlation as high as 0.8.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06389</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06389</id><created>2015-05-23</created><updated>2016-01-16</updated><authors><author><keyname>Liu</keyname><forenames>Ting</forenames></author><author><keyname>Seyedhosseini</keyname><forenames>Mojtaba</forenames></author><author><keyname>Tasdizen</keyname><forenames>Tolga</forenames></author></authors><title>Image Segmentation Using Hierarchical Merge Tree</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates one of the most fundamental computer vision problems:
image segmentation. We propose a supervised hierarchical approach to
object-independent image segmentation. Starting with over-segmenting
superpixels, we use a tree structure to represent the hierarchy of region
merging, by which we reduce the problem of segmenting image regions to finding
a set of label assignment to tree nodes. We formulate the tree structure as a
constrained conditional model to associate region merging with likelihoods
predicted using an ensemble boundary classifier. Final segmentations can then
be inferred by finding globally optimal solutions to the model efficiently. We
also present an iterative training and testing algorithm that generates various
tree structures and combines them to emphasize accurate boundaries by
segmentation accumulation. Experiment results and comparisons with other very
recent methods on six public data sets demonstrate that our approach achieves
the state-of-the-art region accuracy and is very competitive in image
segmentation without semantic priors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06405</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06405</id><created>2015-05-24</created><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>Domain Adaptation Extreme Learning Machines for Drift Compensation in
  E-nose Systems</title><categories>cs.LG</categories><comments>11 pages, 9 figures, to appear in IEEE Transactions on
  Instrumentation and Measurement</comments><doi>10.1109/TIM.2014.2367775</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses an important issue, known as sensor drift that behaves a
nonlinear dynamic property in electronic nose (E-nose), from the viewpoint of
machine learning. Traditional methods for drift compensation are laborious and
costly due to the frequent acquisition and labeling process for gases samples
recalibration. Extreme learning machines (ELMs) have been confirmed to be
efficient and effective learning techniques for pattern recognition and
regression. However, ELMs primarily focus on the supervised, semi-supervised
and unsupervised learning problems in single domain (i.e. source domain). To
our best knowledge, ELM with cross-domain learning capability has never been
studied. This paper proposes a unified framework, referred to as Domain
Adaptation Extreme Learning Machine (DAELM), which learns a robust classifier
by leveraging a limited number of labeled data from target domain for drift
compensation as well as gases recognition in E-nose systems, without loss of
the computational efficiency and learning ability of traditional ELM. In the
unified framework, two algorithms called DAELM-S and DAELM-T are proposed for
the purpose of this paper, respectively. In order to percept the differences
among ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on the
popular sensor drift data with multiple batches collected by E-nose system
clearly demonstrate that the proposed DAELM significantly outperforms existing
drift compensation methods without cumbersome measures, and also bring new
perspectives for ELM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06416</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06416</id><created>2015-05-24</created><authors><author><keyname>Rastgou</keyname><forenames>Mosayeb</forenames></author></authors><title>Robust multiuser detection in impulsive channels based on M-estimation
  using a new penalty function</title><categories>cs.IT math.IT stat.AP</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of multiuser detection in non-Gaussian
channels. We propose a new penalty function for robust multiuser detection. The
proposed detector outperforms other suboptimal detectors in non-Gaussian
environment. Analytical and simulation result shows the performance of the
proposed detector compare to other detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06423</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06423</id><created>2015-05-24</created><authors><author><keyname>Li</keyname><forenames>Mo</forenames></author><author><keyname>Zhang</keyname><forenames>Chun-Mei</forenames></author><author><keyname>Yin</keyname><forenames>Zhen-Qiang</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Chuan</forenames></author><author><keyname>Han</keyname><forenames>Zheng-Fu</forenames></author></authors><title>Simple rate-adaptive LDPC coding for quantum key distribution</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although quantum key distribution (QKD) comes from the development of quantum
theory, the implementation of a practical QKD system does involve a lot of
classical process, such as key reconciliation and privacy amplification, which
is called post-processing. Post-processing has been a crucial element to high
speed QKD systems, even the bottleneck of it because of its relatively high
time consumption. Low density parity check (LDPC) is now becoming a promising
approach of overcoming the bottleneck due to its good performance in processing
throughput. In this article we propose and simulate an easily implemented but
efficiently rate-adaptive LDPC coding approach of reconciliation, different
from the previously proposed puncturing- and shortening-based approach. We also
give a measure for choosing the optimal LDPC parameter for our rate-adaptive
approach according to error rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06425</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06425</id><created>2015-05-24</created><authors><author><keyname>Cariow</keyname><forenames>Aleksandr</forenames></author><author><keyname>Cariowa</keyname><forenames>Galina</forenames></author><author><keyname>&#x141;entek</keyname><forenames>Rafa&#x142;</forenames></author></authors><title>An algorithm for multipication of Kaluza numbers</title><categories>cs.DS</categories><comments>22 pages,3 figures</comments><msc-class>15A23, 15A66, 15A69, 65F30, 65Y20</msc-class><acm-class>F.2.1; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the derivation of a new algorithm for multiplying of two
Kaluza numbers. Performing this operation directly requires 1024 real
multiplications and 992 real additions. The proposed algorithm can compute the
same result with only 512 real multiplications and 576 real additions. The
derivation of our algorithm is based on utilizing the fact that multiplication
of two Kaluza numbers can be expressed as a matrixvector product. The matrix
multiplicand that participates in the product calculating has unique structural
properties. Namely exploitation of these specific properties leads to
significant reducing of the complexity of Kaluza numbers multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06427</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06427</id><created>2015-05-24</created><authors><author><keyname>Li</keyname><forenames>Lantian</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author></authors><title>Deep Speaker Vectors for Semi Text-independent Speaker Verification</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research shows that deep neural networks (DNNs) can be used to extract
deep speaker vectors (d-vectors) that preserve speaker characteristics and can
be used in speaker verification. This new method has been tested on
text-dependent speaker verification tasks, and improvement was reported when
combined with the conventional i-vector method.
  This paper extends the d-vector approach to semi text-independent speaker
verification tasks, i.e., the text of the speech is in a limited set of short
phrases. We explore various settings of the DNN structure used for d-vector
extraction, and present a phone-dependent training which employs the posterior
features obtained from an ASR system. The experimental results show that it is
possible to apply d-vectors on semi text-independent speaker recognition, and
the phone-dependent training improves system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06429</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06429</id><created>2015-05-24</created><authors><author><keyname>Nguyen</keyname><forenames>Phong Q.</forenames></author><author><keyname>Shparlinski</keyname><forenames>Igor E.</forenames></author></authors><title>Counting Co-Cyclic Lattices</title><categories>math.NT cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a well-known asymptotic formula, due to W. M. Schmidt (1968) for the
number of full-rank integer lattices of index at most $V$ in $\mathbb{Z}^n$.
This set of lattices $L$ can naturally be partitioned with respect to the
factor group $\mathbb{Z}^n/L$. Accordingly, we count the number of full-rank
integer lattices $L \subseteq \mathbb{Z}^n$ such that $\mathbb{Z}^n/L$ is
cyclic and of order at most $V$, and deduce that these co-cyclic lattices are
dominant among all integer lattices: their natural density is $\left(\zeta(6)
\prod_{k=4}^n \zeta(k)\right)^{-1} \approx 85\%$. The problem is motivated by
complexity theory, namely worst-case to average-case reductions for lattice
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06430</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06430</id><created>2015-05-24</created><authors><author><keyname>Timany</keyname><forenames>Amin</forenames></author><author><keyname>Jacobs</keyname><forenames>Bart</forenames></author></authors><title>Category Theory in Coq 8.5</title><categories>cs.LO</categories><comments>This is the abstract for a talk accepted for a presentation at the
  7th Coq Workshop, Sophia Antipolis, France on June 26, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on our experience implementing category theory in Coq 8.5. The
repository of this development can be found at
https://bitbucket.org/amintimany/categories/. This implementation most notably
makes use of features, primitive projections for records and universe
polymorphism that are new to Coq 8.5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06443</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06443</id><created>2015-05-24</created><authors><author><keyname>Papadopoulos</keyname><forenames>Timos</forenames></author><author><keyname>Roberts</keyname><forenames>Stephen</forenames></author><author><keyname>Willis</keyname><forenames>Kathy</forenames></author></authors><title>Detecting bird sound in unknown acoustic background using crowdsourced
  training data</title><categories>stat.ML cs.LG cs.SD</categories><comments>Submitted to 'Big Data Sciences for Bioacoustic Environmental
  Survey', 10th Advanced Multimodal Information Retrieval int'l summer school,
  Ermites 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biodiversity monitoring using audio recordings is achievable at a truly
global scale via large-scale deployment of inexpensive, unattended recording
stations or by large-scale crowdsourcing using recording and species
recognition on mobile devices. The ability, however, to reliably identify
vocalising animal species is limited by the fact that acoustic signatures of
interest in such recordings are typically embedded in a diverse and complex
acoustic background. To avoid the problems associated with modelling such
backgrounds, we build generative models of bird sounds and use the concept of
novelty detection to screen recordings to detect sections of data which are
likely bird vocalisations. We present detection results against various
acoustic environments and different signal-to-noise ratios. We discuss the
issues related to selecting the cost function and setting detection thresholds
in such algorithms. Our methods are designed to be scalable and automatically
applicable to arbitrary selections of species depending on the specific
geographic region and time period of deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06449</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06449</id><created>2015-05-24</created><updated>2015-07-02</updated><authors><author><keyname>Lipton</keyname><forenames>Zachary C.</forenames></author><author><keyname>Elkan</keyname><forenames>Charles</forenames></author></authors><title>Efficient Elastic Net Regularization for Sparse Linear Models</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm for efficient training of sparse linear
models with elastic net regularization. Extending previous work on delayed
updates, the new algorithm applies stochastic gradient updates to non-zero
features only, bringing weights current as needed with closed-form updates.
Closed-form delayed updates for the $\ell_1$, $\ell_{\infty}$, and rarely used
$\ell_2$ regularizers have been described previously. This paper provides
closed-form updates for the popular squared norm $\ell^2_2$ and elastic net
regularizers.
  We provide dynamic programming algorithms that perform each delayed update in
constant time. The new $\ell^2_2$ and elastic net methods handle both fixed and
varying learning rates, and both standard {stochastic gradient descent} (SGD)
and {forward backward splitting (FoBoS)}. Experimental results show that on a
bag-of-words dataset with $260,941$ features, but only $88$ nonzero features on
average per training example, the dynamic programming method trains a logistic
regression classifier with elastic net regularization over $2000$ times faster
than otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06450</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06450</id><created>2015-05-24</created><authors><author><keyname>Safdari</keyname><forenames>Hadiseh</forenames></author><author><keyname>Kamali</keyname><forenames>Milad Zare</forenames></author><author><keyname>Shirazi</keyname><forenames>Amir Hossein</forenames></author><author><keyname>Khaliqi</keyname><forenames>Moein</forenames></author><author><keyname>Jafari</keyname><forenames>Gholamreza</forenames></author></authors><title>History effects on network growth</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Growth dynamic of real networks because of emerging complexities is an open
and interesting question. Indeed it is not realistic to ignore history impact
on the current events. The mystery behind that complexity could be in the role
of history in some how. To regard this point, the average effect of history has
been included by a kernel function in differential equation of Barabasi Albert
(BA) model . This approach leads to a fractional order BA differential equation
as a generalization of BA model. As opposed to unlimited growth for degree of
nodes, our results show that over time the memory impact will cause a decay for
degrees. This gives a higher chance to younger members for turning to a hub. In
fact in a real network, there are two competitive processes. On one hand, based
on preferential attachment mechanism nodes with higher degree are more likely
to absorb links. On the other hand, node history through aging process prevents
new connections. Our findings from simulating a network grown by considering
these effects also from studying a real network of collaboration between
Hollywood movie actors conforms the results and significant effects of history
and time on dynamic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06452</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06452</id><created>2015-05-24</created><authors><author><keyname>Wu</keyname><forenames>Shuhang</forenames></author><author><keyname>Wei</keyname><forenames>Shuangqing</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Vaidyanathan</keyname><forenames>Ramachandran</forenames></author><author><keyname>Yuan</keyname><forenames>Jian</forenames></author></authors><title>Asymptotic Error Free Partitioning over Noisy Boolean Multiaccess
  Channels</title><categories>cs.IT math.IT</categories><comments>This paper was submitted in June 2014 to IEEE Transactions on
  Information Theory, and is under review now</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of partitioning active users in a
manner that facilitates multi-access without collision. The setting is of a
noisy, synchronous, Boolean, multi-access channel where $K$ active users (out
of a total of $N$ users) seek to access. A solution to the partition problem
places each of the $N$ users in one of $K$ groups (or blocks) such that no two
active nodes are in the same block. We consider a simple, but non-trivial and
illustrative case of $K=2$ active users and study the number of steps $T$ used
to solve the partition problem. By random coding and a suboptimal decoding
scheme, we show that for any $T\geq (C_1 +\xi_1)\log N$, where $C_1$ and
$\xi_1$ are positive constants (independent of $N$), and $\xi_1$ can be
arbitrary small, the partition problem can be solved with error probability
$P_e^{(N)} \to 0$, for large $N$. Under the same scheme, we also bound $T$ from
the other direction, establishing that, for any $T \leq (C_2 - \xi_2) \log N$,
the error probability $P_e^{(N)} \to 1$ for large $N$; again $C_2$ and $\xi_2$
are constants and $\xi_2$ can be arbitrarily small. These bounds on the number
of steps are lower than the tight achievable lower-bound in terms of $T \geq
(C_g +\xi)\log N $ for group testing (in which all active users are identified,
rather than just partitioned). Thus, partitioning may prove to be a more
efficient approach for multi-access than group testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06454</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06454</id><created>2015-05-24</created><authors><author><keyname>Ke</keyname><forenames>Qing</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>Defining and identifying Sleeping Beauties in science</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>40 pages, Supporting Information included, top examples listed at
  http://qke.github.io/projects/beauty/beauty.html</comments><journal-ref>Proc. Natl. Acad. Sci. USA 112, 7426-7431 (2015)</journal-ref><doi>10.1073/pnas.1424329112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Sleeping Beauty (SB) in science refers to a paper whose importance is not
recognized for several years after publication. Its citation history exhibits a
long hibernation period followed by a sudden spike of popularity. Previous
studies suggest a relative scarcity of SBs. The reliability of this conclusion
is, however, heavily dependent on identification methods based on arbitrary
threshold parameters for sleeping time and number of citations, applied to
small or monodisciplinary bibliographic datasets. Here we present a systematic,
large-scale, and multidisciplinary analysis of the SB phenomenon in science. We
introduce a parameter-free measure that quantifies the extent to which a
specific paper can be considered an SB. We apply our method to 22 million
scientific papers published in all disciplines of natural and social sciences
over a time span longer than a century. Our results reveal that the SB
phenomenon is not exceptional. There is a continuous spectrum of delayed
recognition where both the hibernation period and the awakening intensity are
taken into account. Although many cases of SBs can be identified by looking at
monodisciplinary bibliographic data, the SB phenomenon becomes much more
apparent with the analysis of multidisciplinary datasets, where we can observe
many examples of papers achieving delayed yet exceptional importance in
disciplines different from those where they were originally published. Our
analysis emphasizes a complex feature of citation dynamics that so far has
received little attention, and also provides empirical evidence against the use
of short-term citation metrics in the quantification of scientific impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06457</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06457</id><created>2015-05-24</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author></authors><title>ISS With Respect to Boundary Disturbances for 1-D Parabolic PDES</title><categories>math.OC cs.SY</categories><comments>23 pages, 1 figure, to be submitted to IEEE TAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to unbounded input operators in partial differential equations (PDEs)
with boundary inputs, there has been a long-held intuition that input-to-state
stability (ISS) properties and finite gains cannot be established with respect
to disturbances at the boundary. This intuition has been reinforced by many
unsuccessful attempts, as well as by the success in establishing ISS only with
respect to the derivative of the disturbance. Contrary to this intuition, we
establish such a result for parabolic PDEs. Our methodology does not rely on
the transformation of the boundary disturbance to a distributed input and the
stability analysis is performed in time-varying subsets of the state space. The
obtained results are used for the comparison of the gain coefficients of
transport PDEs with respect to inlet disturbances and for the establishment of
the ISS property with respect to control actuator errors for parabolic systems
under boundary feedback control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06459</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06459</id><created>2015-05-24</created><authors><author><keyname>Yu</keyname><forenames>Xiangyao</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Muralidaran</forenames></author><author><keyname>Devadas</keyname><forenames>Srinivas</forenames></author></authors><title>A Proof of Correctness for the Tardis Cache Coherence Protocol</title><categories>cs.DC</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the correctness of a recently-proposed cache coherence protocol,
Tardis, which is simple, yet scalable to high processor counts, because it only
requires O(logN) storage per cacheline for an N-processor system. We prove that
Tardis follows the sequential consistency model and is both deadlock- and
livelock-free. Our proof is based on simple and intuitive invariants of the
system and thus applies to any system scale and many variants of Tardis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06462</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06462</id><created>2015-05-24</created><authors><author><keyname>Dey</keyname><forenames>Tamal K.</forenames></author><author><keyname>Dong</keyname><forenames>Zhe</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Parameter-free Topology Inference and Sparsification for Data on
  Manifolds</title><categories>cs.CG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In topology inference from data, current approaches face two major problems.
One concerns the selection of a correct parameter to build an appropriate
complex on top of the data points; the other involves with the typical `large'
size of this complex. We address these two issues in the context of inferring
homology from sample points of a smooth manifold of known dimension sitting in
an Euclidean space $\mathbb{R}^k$. We show that, for a sample size of $n$
points, we can identify a set of $O(n^2)$ points (as opposed to $O(n^{\lceil
\frac{k}{2}\rceil})$ Voronoi vertices) approximating a subset of the medial
axis that suffices to compute a distance sandwiched between the well known
local feature size and the local weak feature size (in fact, the approximating
set can be further reduced in size to $O(n)$). This distance, called the lean
feature size, helps pruning the input set at least to the level of local
feature size while making the data locally uniform. The local uniformity in
turn helps in building a complex for homology inference on top of the
sparsified data without requiring any user-supplied distance threshold. Unlike
most topology inference results, ours does not require that the input is dense
relative to a {\em global} feature such as {\em reach} or {\em weak feature
size}; instead it can be adaptive with respect to the local feature size. We
present some empirical evidence in support of our theoretical claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06466</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06466</id><created>2015-05-24</created><authors><author><keyname>Vakilian</keyname><forenames>Vida</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author><author><keyname>Hua</keyname><forenames>Yingbo</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>High-Rate Space Coding for Reconfigurable 2x2 Millimeter-Wave MIMO
  Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter-wave links are of a line-of-sight nature. Hence, multiple-input
multiple-output (MIMO) systems operating in the millimeter-wave band may not
achieve full spatial diversity or multiplexing. In this paper, we utilize
reconfigurable antennas and the high antenna directivity in the millimeter-wave
band to propose a rate-two space coding design for 2x2 MIMO systems. The
proposed scheme can be decoded with a low complexity maximum-likelihood
detector at the receiver and yet it can enhance the bit-error-rate performance
of millimeter-wave systems compared to traditional spatial multiplexing
schemes, such as the Vertical Bell Laboratories Layered Space-Time Architecture
(VBLAST). Using numerical simulations, we demonstrate the efficiency of the
proposed code and show its superiority compared to existing rate-two space-time
block codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06476</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06476</id><created>2015-05-24</created><authors><author><keyname>Jia</keyname><forenames>Tao</forenames></author><author><keyname>Liu</keyname><forenames>Yang-Yu</forenames></author><author><keyname>Cs&#xf3;ka</keyname><forenames>Endre</forenames></author><author><keyname>P&#xf3;sfai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author><author><keyname>Barab&#xe1;si</keyname><forenames>Albert-L&#xe1;szl&#xf3;</forenames></author></authors><title>Emergence of bimodality in controlling complex networks</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Nature Communications 4:2002 (2013)</journal-ref><doi>10.1038/ncomms3002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our ability to control complex systems is a fundamental challenge of
contemporary science. Recently introduced tools to identify the driver nodes,
nodes through which we can achieve full control, predict the existence of
multiple control configurations, prompting us to classify each node in a
network based on their role in control. Accordingly a node is critical,
intermittent or redundant if it acts as a driver node in all, some or none of
the control configurations. Here we develop an analytical framework to identify
the category of each node, leading to the discovery of two distinct control
modes in complex systems: centralized vs distributed control. We predict the
control mode for an arbitrary network and show that one can alter it through
small structural perturbations. The uncovered bimodality has implications from
network security to organizational research and offers new insights into the
dynamics and control of complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06478</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06478</id><created>2015-05-24</created><authors><author><keyname>Rangapuram</keyname><forenames>Syama Sundar</forenames></author><author><keyname>Mudrakarta</keyname><forenames>Pramod Kaushik</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author></authors><title>Tight Continuous Relaxation of the Balanced $k$-Cut Problem</title><categories>stat.ML cs.LG</categories><comments>Long version of paper accepted at NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral Clustering as a relaxation of the normalized/ratio cut has become
one of the standard graph-based clustering methods. Existing methods for the
computation of multiple clusters, corresponding to a balanced $k$-cut of the
graph, are either based on greedy techniques or heuristics which have weak
connection to the original motivation of minimizing the normalized cut. In this
paper we propose a new tight continuous relaxation for any balanced $k$-cut
problem and show that a related recently proposed relaxation is in most cases
loose leading to poor performance in practice. For the optimization of our
tight continuous relaxation we propose a new algorithm for the difficult
sum-of-ratios minimization problem which achieves monotonic descent. Extensive
comparisons show that our method outperforms all existing approaches for ratio
cut and other balanced $k$-cut criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06484</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06484</id><created>2015-05-24</created><authors><author><keyname>Pin</keyname><forenames>Paolo</forenames></author><author><keyname>Rogers</keyname><forenames>Brian</forenames></author></authors><title>Stochastic network formation and homophily</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a chapter of the forthcoming Oxford Handbook on the Economics of
Networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06485</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06485</id><created>2015-05-24</created><authors><author><keyname>Rangapuram</keyname><forenames>Syama Sundar</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author></authors><title>Constrained 1-Spectral Clustering</title><categories>stat.ML cs.LG</categories><comments>Long version of paper accepted at AISTATS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important form of prior information in clustering comes in form of
cannot-link and must-link constraints. We present a generalization of the
popular spectral clustering technique which integrates such constraints.
Motivated by the recently proposed $1$-spectral clustering for the
unconstrained problem, our method is based on a tight relaxation of the
constrained normalized cut into a continuous optimization problem. Opposite to
all other methods which have been suggested for constrained spectral
clustering, we can always guarantee to satisfy all constraints. Moreover, our
soft formulation allows to optimize a trade-off between normalized cut and the
number of violated constraints. An efficient implementation is provided which
scales to large datasets. We outperform consistently all other proposed methods
in the experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06494</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06494</id><created>2015-05-24</created><updated>2015-08-18</updated><authors><author><keyname>Chaudhary</keyname><forenames>Narendra</forenames></author><author><keyname>Luo</keyname><forenames>Yao</forenames></author><author><keyname>Savari</keyname><forenames>Serap A.</forenames></author><author><keyname>McCay</keyname><forenames>Roger</forenames></author></authors><title>Lossless Layout Image Compression Algorithms for Electron-Beam
  Direct-Write Lithography</title><categories>cs.OH</categories><comments>This is the source file for the paper which was published in the
  Journal of Vacuum Science &amp; Technology B (volume 33, 06FD01) on 5 August 2015</comments><journal-ref>Journal of Vacuum Science &amp; Technology B (Vol.33, Issue 6), 06FD01
  (2015)</journal-ref><doi>10.1116/1.4927639</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electron-beam direct-write (EBDW) lithography systems must in the future
transmit terabits of information per second to be viable for commercial
semiconductor manufacturing. Lossless layout image compression algorithms with
high decoding throughputs and modest decoding resources are tools to address
the data transfer portion of the throughput problem. The earlier lossless
layout image compression algorithm Corner2 is designed for binary layout images
on raster-scanning systems. We propose variations of Corner2 collectively
called Corner2-EPC and Paeth-EPC which apply to electron-beam proximity
corrected layout images and offer interesting trade-offs between compression
ratios and decoding speeds. Most of our algorithms achieve better overall
compression performance than PNG, Block C4 and LineDiffEntropy while having low
decoding times and resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06502</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06502</id><created>2015-05-24</created><updated>2015-05-26</updated><authors><author><keyname>Xiang</keyname><forenames>Can</forenames></author><author><keyname>Liu</keyname><forenames>Hao</forenames></author></authors><title>The Complete Weight Enumerator of A Class of Linear Codes</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to some results of us
  are similiar to others</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes can be employed to construct authentication codes, which is an
interesting area of cryptography. The parameters of the authentication codes
depend on the complete weight enumerator of the underlying linear codes. In
order to obtain an authentication code with good parameters, the underlying
linear code must have proper parameters. The first objective of this paper is
to determine the complete weight enumerators of a class of linear codes with
two weights and three weights. The second is to employ these linear codes to
construct authentication codes with new parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06506</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06506</id><created>2015-05-24</created><updated>2015-08-13</updated><authors><author><keyname>Haeusler</keyname><forenames>Edward Hermann</forenames></author></authors><title>Every super-polynomial proof in purely implicational minimal logic has a
  polynomially sized proof in classical implicational propositional logic</title><categories>cs.CC cs.LO</categories><comments>This paper has been withdrawn by the author due to a fatal error in
  the general form of the deduction used for proved the main proposition</comments><acm-class>F.2.2; F.4.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this article we show how any formula A with a proof in minimal
implicational logic that is super-polynomially sized has a polynomially-sized
proof in classical implicational propositional logic . This fact provides an
argument in favor that any classical propositional tautology has short proofs,
i.e., NP=CoNP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06508</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06508</id><created>2015-05-24</created><authors><author><keyname>Garrabrant</keyname><forenames>Scott</forenames></author><author><keyname>Pak</keyname><forenames>Igor</forenames></author></authors><title>Pattern avoidance is not P-recursive</title><categories>math.CO cs.CC cs.FL</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $F \subset S_k$ be a finite set of permutations and let $C_n(F)$ denote
the number of permutations $\sigma$ in $S_n$ avoiding the set of patterns $F$.
The Noonan-Zeilberger conjecture states that the sequence ${C_n(F)}$ is
P-recursive. We use Computability Theory to disprove this conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06524</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06524</id><created>2015-05-24</created><updated>2015-08-07</updated><authors><author><keyname>Xu</keyname><forenames>Liqing</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author></authors><title>New Explicit Binary Constant Weight Codes from Reed-Solomon Codes</title><categories>cs.IT math.IT</categories><comments>15 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary constant weight codes have important applications and have been
studied for many years. Optimal or near-optimal binary constant weight codes of
small lengths have been determined. In this paper we propose a new construction
of explicit binary constant weight codes from $q$-ary Reed-Solomon codes. Some
of our binary constant weight codes are optimal or new. In particular new
binary constant weight codes $A(64, 10, 8) \geq 4108$ and $A(64, 12, 8) \geq
522$ are constructed. We also give explicitly constructed binary constant
weight codes which improve Gilbert and Graham-Sloane lower bounds in some range
of parameters. An extension to algebraic geometric codes is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06529</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06529</id><created>2015-05-24</created><authors><author><keyname>Zhu</keyname><forenames>Daxin</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Wu</keyname><forenames>Yingjie</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>An efficient dynamic programming algorithm for the generalized LCS
  problem with multiple substring inclusive constraints</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.1872</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we consider a generalized longest common subsequence problem
with multiple substring inclusive constraints. For the two input sequences $X$
and $Y$ of lengths $n$ and $m$, and a set of $d$ constraints
$P=\{P_1,\cdots,P_d\}$ of total length $r$, the problem is to find a common
subsequence $Z$ of $X$ and $Y$ including each of constraint string in $P$ as a
substring and the length of $Z$ is maximized. A new dynamic programming
solution to this problem is presented in this paper. The correctness of the new
algorithm is proved. The time complexity of our algorithm is $O(d2^dnmr)$. In
the case of the number of constraint strings is fixed, our new algorithm for
the generalized longest common subsequence problem with multiple substring
inclusive constraints requires $O(nmr)$ time and space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06530</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06530</id><created>2015-05-24</created><updated>2015-12-08</updated><authors><author><keyname>Bi</keyname><forenames>Suzhi</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Placement Optimization of Energy and Information Access Points in
  Wireless Powered Communication Networks</title><categories>cs.NI</categories><comments>This paper is accepted and to appear in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The applications of wireless power transfer technology to wireless
communications can help build a wireless powered communication network (WPCN)
with more reliable and sustainable power supply compared to the conventional
battery-powered network. However, due to the fundamental differences in
wireless information and power transmissions, many important aspects of
conventional battery-powered wireless communication networks need to be
redesigned for efficient operations of WPCNs. In this paper, we study the
placement optimization of energy and information access points in WPCNs, where
the wireless devices (WDs) harvest the radio frequency energy transferred by
dedicated energy nodes (ENs) in the downlink, and use the harvested energy to
transmit data to information access points (APs) in the uplink. In particular,
we are interested in minimizing the network deployment cost with minimum number
of ENs and APs by optimizing their locations, while satisfying the energy
harvesting and communication performance requirements of the WDs. Specifically,
we first study the minimum-cost placement problem when the ENs and APs are
separately located, where an alternating optimization method is proposed to
jointly optimize the locations of ENs and APs. Then, we study the placement
optimization when each pair of EN and AP are co-located and integrated as a
hybrid access point, and propose an efficient algorithm to solve this problem.
Simulation results show that the proposed methods can effectively reduce the
network deployment cost and yet guarantee the given performance requirements,
which is a key consideration in the future applications of WPCNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06531</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06531</id><created>2015-05-24</created><authors><author><keyname>Chen</keyname><forenames>Tsu-Wei</forenames></author><author><keyname>Abdelmaseeh</keyname><forenames>Meena</forenames></author><author><keyname>Stashuk</keyname><forenames>Daniel</forenames></author></authors><title>Affine and Regional Dynamic Time Warpng</title><categories>cs.CV cs.CE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pointwise matches between two time series are of great importance in time
series analysis, and dynamic time warping (DTW) is known to provide generally
reasonable matches. There are situations where time series alignment should be
invariant to scaling and offset in amplitude or where local regions of the
considered time series should be strongly reflected in pointwise matches. Two
different variants of DTW, affine DTW (ADTW) and regional DTW (RDTW), are
proposed to handle scaling and offset in amplitude and provide regional
emphasis respectively. Furthermore, ADTW and RDTW can be combined in two
different ways to generate alignments that incorporate advantages from both
methods, where the affine model can be applied either globally to the entire
time series or locally to each region. The proposed alignment methods
outperform DTW on specific simulated datasets, and one-nearest-neighbor
classifiers using their associated difference measures are competitive with the
difference measures associated with state-of-the-art alignment methods on real
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06532</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06532</id><created>2015-05-24</created><authors><author><keyname>Jahanian</keyname><forenames>Ali</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author><author><keyname>Allebach</keyname><forenames>Jan P.</forenames></author></authors><title>Colors $-$Messengers of Concepts: Visual Design Mining for Learning
  Color Semantics</title><categories>cs.HC</categories><acm-class>H.1.2; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the concept of color semantics by modeling a dataset of
magazine cover designs, evaluating the model via crowdsourcing, and
demonstrating several prototypes that facilitate color-related design tasks. We
investigate a probabilistic generative modeling framework that expresses
semantic concepts as a combination of color and word distributions
$-$color-word topics. We adopt an extension to Latent Dirichlet Allocation
(LDA) topic modeling called LDA-dual to infer a set of color-word topics over a
corpus of 2,654 magazine covers spanning 71 distinct titles and 12 genres.
While LDA models text documents as distributions over word topics, we model
magazine covers as distributions over color-word topics. The results of our
crowdsourced experiments confirm that the model is able to successfully
discover the associations between colors and linguistic concepts. Finally, we
demonstrate several simple prototypes that apply the learned model to color
palette recommendation, design example retrieval, image retrieval, image color
selection, and image recoloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06537</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06537</id><created>2015-05-22</created><authors><author><keyname>Joshi</keyname><forenames>Manish R.</forenames></author><author><keyname>Pathak</keyname><forenames>Varsha M.</forenames></author></authors><title>A survey of SMS based Information Systems</title><categories>cs.AI cs.IR</categories><comments>17 pages 3 Figures 5 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Short Message Service (SMS) based Information Systems (SMSbIS) provide an
excellent alternative to a traditional approach of obtaining specific
information by direct (through phone) or indirect (IVRS, Web, Email) probing.
Information and communication technology and far reaching mobile penetration
has opened this new research trend Number of key players in Search industry
including Microsoft and Google are attracted by the expected increase in volume
of use of such applications. The wide range of applications and their public
acceptance has motivated researchers to work in this research domain. Several
applications such as SMS based information access using database management
services, SMS based information retrieval through internet (search engine), SMS
based information extraction, question answering, image retrieval etc. have
been emerged. With the aim to understand the functionality involved in these
systems, an extensive review of a few of these SMSbISs has been planned and
executed by us. These systems are classified into four categories based on the
objectives and domains of the applications. As a result of this study a well
structured functional model is presented here. The model is evaluated in
different dimensions, which is presented in this paper. In addition to this a
chronological progress with respect to research and development in this
upcoming field is compiled in this paper. Such an extensive review presented in
this paper would definitely help the researchers and developers to understand
the technical aspects of this field. The functional framework presented here
would be useful to the system designers to design and develop an SMS based
Information System of any specific domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06538</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06538</id><created>2015-05-25</created><authors><author><keyname>Cashore</keyname><forenames>J. Massey</forenames></author><author><keyname>Zhao</keyname><forenames>Xiaoting</forenames></author><author><keyname>Alemi</keyname><forenames>Alexander A.</forenames></author><author><keyname>Liu</keyname><forenames>Yujia</forenames></author><author><keyname>Frazier</keyname><forenames>Peter I.</forenames></author></authors><title>Clustering via Content-Augmented Stochastic Blockmodels</title><categories>stat.ML cs.LG cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of the data being created on the web contains interactions between users
and items. Stochastic blockmodels, and other methods for community detection
and clustering of bipartite graphs, can infer latent user communities and
latent item clusters from this interaction data. These methods, however,
typically ignore the items' contents and the information they provide about
item clusters, despite the tendency of items in the same latent cluster to
share commonalities in content. We introduce content-augmented stochastic
blockmodels (CASB), which use item content together with user-item interaction
data to enhance the user communities and item clusters learned. Comparisons to
several state-of-the-art benchmark methods, on datasets arising from scientists
interacting with scientific articles, show that content-augmented stochastic
blockmodels provide highly accurate clusters with respect to metrics
representative of the underlying community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06539</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06539</id><created>2015-05-25</created><authors><author><keyname>Lawson</keyname><forenames>Gary</forenames></author><author><keyname>Sosonkina</keyname><forenames>Masha</forenames></author><author><keyname>Shen</keyname><forenames>Yuzhong</forenames></author></authors><title>Towards Modeling Energy Consumption of Xeon Phi</title><categories>cs.DC</categories><comments>Energy, Intel Xeon Phi, Symmetric Execution, Proxy Applications</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the push for exascale computing, energy efficiency is of utmost concern.
System architectures often adopt accelerators to hasten application execution
at the cost of power. The Intel Xeon Phi co-processor is unique accelerator
that offers application designers high degrees of parallelism, energy-efficient
cores, and various execution modes. To explore the vast number of available
configurations, a model must be developed to predict execution time, power, and
energy for the CPU and Xeon Phi. An experimentation method has been developed
which measures power for the CPU and Xeon Phi separately, as well as total
system power. Execution time and performance are also captured for two
experiments conducted in this work. The experiments, frequency scaling and
strong scaling, will help validate the adopted model and assist in the
development of a model which defines the host and Xeon Phi. The proxy
applications investigated, representative of large-scale real-world
applications, are Co-Design Molecular Dynamics (CoMD) and Livermore
Unstructured Lagrangian Explicit Shock Hydrodynamics (LULESH). The frequency
experiment discussed in this work is used to determine the time on-chip and
off-chip to measure the compute- or latencyboundedness of the application.
Energy savings were not obtained in symmetric mode for either application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06542</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06542</id><created>2015-05-25</created><authors><author><keyname>Annette</keyname><forenames>Ruby</forenames></author><author><keyname>Banu</keyname><forenames>Aisha</forenames></author></authors><title>A Service Broker Model for Cloud based Render Farm Selection</title><categories>cs.DC</categories><journal-ref>International Journal of Computer Applications 96.24 (2014): 11-14</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is gaining popularity in the 3D Animation industry for
rendering the 3D images. Rendering is an inevitable task in creating the 3d
animated scenes. It is a process where the scene files to be animated is read
and converted into 3D photorealistic images automatically. Since it is a
computationally intensive task, this process consumes the majority of the time
taken for 3D images production. As the scene files could be processed in
parallel, clusters of computers called render farms can be used to speed up the
rendering process. The advantage of using Cloud based render farms is that it
is scalable and can be availed on demand. One of the important challenges faced
by the 3D studios is the comparison and selection of the cloud based render
farm service provider who could satisfy their functional and the non functional
Quality of Service (QoS) requirements. In this paper we propose, a frame work
for Cloud Service Broker (CSB) responsible for the selection and provision of
the cloud based render farm. The Cloud Service Broker matches the functional
and the non functional Quality of Service requirements (QoS) of the user with
the service offerings of the render farm service providers and helps the user
in selecting the right service provider using an aggregate utility function.
The CSB also facilitates the process of Service Level Agreement (SLA)
negotiation and monitoring by the third party monitoring services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06543</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06543</id><created>2015-05-25</created><authors><author><keyname>Annette</keyname><forenames>Ruby</forenames></author><author><keyname>Banu</keyname><forenames>Aisha</forenames></author><author><keyname>Chandran</keyname><forenames>Subash</forenames></author></authors><title>Rendering-as-a-Service: Taxonomy and Comparison</title><categories>cs.DC cs.GT</categories><journal-ref>Procedia Computer Science 50 (2015): 276-281</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The movies like the Avatar are a good example of the stunning visual effects
that the animation could bring into a movie.The 3D wire frame models are
converted to 3D photo realistic images using a process called the rendering.
This rendering process is offered as a service in the cloud, where the
animation files to be rendered are split into frames and rendered in the cloud
resources and are popularly known as Rendering as a Service. As this is gaining
high popularity among the animators community, this work intends to enable the
animators to Gain basic knowledge about Rendering as a Service. Understand the
variety in the service models through the taxonomy,Explore, compare and
classify the services quickly using the tree-structured taxonomy of services.
In this paper, the various characteristics of the services are organized in the
form of a tree to enable quick classification and comparison of the services.
To enhance the understandability, three popular services have been classified
and verified according to the proposed tree-structured taxonomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06550</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06550</id><created>2015-05-25</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>XifengYan</keyname></author></authors><title>MSPKmerCounter: A Fast and Memory Efficient Approach for K-mer Counting</title><categories>q-bio.GN cs.CE cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major challenge in next-generation genome sequencing (NGS) is to assemble
massive overlapping short reads that are randomly sampled from DNA fragments.
To complete assembling, one needs to finish a fundamental task in many leading
assembly algorithms: counting the number of occurrences of k-mers (length-k
substrings in sequences). The counting results are critical for many components
in assembly (e.g. variants detection and read error correction). For large
genomes, the k-mer counting task can easily consume a huge amount of memory,
making it impossible for large-scale parallel assembly on commodity servers.
  In this paper, we develop MSPKmerCounter, a disk-based approach, to
efficiently perform k-mer counting for large genomes using a small amount of
memory. Our approach is based on a novel technique called Minimum Substring
Partitioning (MSP). MSP breaks short reads into multiple disjoint partitions
such that each partition can be loaded into memory and processed individually.
By leveraging the overlaps among the k-mers derived from the same short read,
MSP can achieve astonishing compression ratio so that the I/O cost can be
significantly reduced. For the task of k-mer counting, MSPKmerCounter offers a
very fast and memory-efficient solution. Experiment results on large real-life
short reads data sets demonstrate that MSPKmerCounter can achieve better
overall performance than state-of-the-art k-mer counting approaches.
  MSPKmerCounter is available at http://www.cs.ucsb.edu/~yangli/MSPKmerCounter
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06553</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06553</id><created>2015-05-25</created><updated>2015-10-30</updated><authors><author><keyname>Pitarokoilis</keyname><forenames>Antonios</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>ML Detection in Phase Noise Impaired SIMO Channels with Uplink Training</title><categories>cs.IT math.IT</categories><comments>(To appear in IEEE Transactions on Communications, 2015), Contains
  additional material (Appendix B. T-slot Detectors)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maximum likelihood (ML) detection in training-assisted
single-input multiple-output (SIMO) systems with phase noise impairments is
studied for two different scenarios, i.e. the case when the channel is
deterministic and known (constant channel) and the case when the channel is
stochastic and unknown (fading channel). Further, two different operations with
respect to the phase noise sources are considered, namely, the case of
identical phase noise sources and the case of independent phase noise sources
over the antennas. In all scenarios the optimal detector is derived for a very
general parametrization of the phase noise distribution. Further, a high
signal-to-noise-ratio (SNR) analysis is performed to show that
symbol-error-rate (SER) floors appear in all cases. The SER floor in the case
of identical phase noise sources (for both constant and fading channels) is
independent of the number of antenna elements. In contrast, the SER floor in
the case of independent phase noise sources is reduced when increasing the
number of antenna elements (for both constant and fading channels). Finally,
the system model is extended to multiple data channel uses and it is shown that
the conclusions are valid for these setups, as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06556</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06556</id><created>2015-05-25</created><updated>2015-06-23</updated><authors><author><keyname>Li</keyname><forenames>Chencheng</forenames></author><author><keyname>Zhou</keyname><forenames>Pan</forenames></author></authors><title>Differentially Private Distributed Online Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning has been in the spotlight from the machine learning society
for a long time. To handle massive data in Big Data era, one single learner
could never efficiently finish this heavy task. Hence, in this paper, we
propose a novel distributed online learning algorithm to solve the problem.
Comparing to typical centralized online learner, the distributed learners
optimize their own learning parameters based on local data sources and timely
communicate with neighbors. However, communication may lead to a privacy
breach. Thus, we use differential privacy to preserve the privacy of learners,
and study the influence of guaranteeing differential privacy on the utility of
the distributed online learning algorithm. Furthermore, by using the results
from Kakade and Tewari (2009), we use the regret bounds of online learning to
achieve fast convergence rates for offline learning algorithms in distributed
scenarios, which provides tighter utility performance than the existing
state-of-the-art results. In simulation, we demonstrate that the differentially
private offline learning algorithm has high variance, but we can use mini-batch
to improve the performance. Finally, the simulations show that the analytical
results of our proposed theorems are right and our private distributed online
learning algorithm is a general framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06557</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06557</id><created>2015-05-25</created><authors><author><keyname>Nguyen</keyname><forenames>Phu H.</forenames></author><author><keyname>Kramer</keyname><forenames>Max</forenames></author><author><keyname>Klein</keyname><forenames>Jacques</forenames></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames></author></authors><title>An Extensive Systematic Review on Model-Driven Development of Secure
  Systems</title><categories>cs.SE</categories><comments>visible tables, charts, figures, etc</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Model-Driven Security (MDS) is as a specialised Model-Driven
Engineering research area for supporting the development of secure systems.
Over a decade of research on MDS has resulted in a large number of
publications. Objective: To provide a detailed analysis of the state of the art
in MDS, a systematic literature review (SLR) is essential. Method: We conducted
an extensive SLR on MDS. Derived from our research questions, we designed a
rigorous, extensive search and selection process to identify a set of primary
MDS studies that is as complete as possible. Our three-pronged search process
consists of automatic searching, manual searching, and snowballing. After
discovering and considering more than thousand relevant papers, we identified,
strictly selected, and reviewed 108 MDS publications. Results: The results of
our SLR show the overall status of the key artefacts of MDS, and the identified
primary MDS studies. E.g. regarding security modelling artefact, we found that
developing domain-specific languages plays a key role in many MDS approaches.
The current limitations in each MDS artefact are pointed out and corresponding
potential research directions are suggested. Moreover, we categorise the
identified primary MDS studies into 5 principal MDS studies, and other emerging
or less common MDS studies. Finally, some trend analyses of MDS research are
given. Conclusion: Our results suggest the need for addressing multiple
security concerns more systematically and simultaneously, for tool chains
supporting the MDS development cycle, and for more empirical studies on the
application of MDS methodologies. To the best of our knowledge, this SLR is the
first in the field of Software Engineering that combines a snowballing strategy
with database searching. This combination has delivered an extensive literature
study on MDS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06561</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06561</id><created>2015-05-25</created><authors><author><keyname>Wang</keyname><forenames>Mingzhe</forenames></author><author><keyname>Wang</keyname><forenames>Bo</forenames></author><author><keyname>He</keyname><forenames>Qiu</forenames></author><author><keyname>Liu</keyname><forenames>Xiuxiu</forenames></author><author><keyname>Zhu</keyname><forenames>Kunshuai</forenames></author></authors><title>Analysis of GPU Parallel Computing based on Matlab</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matlab is very widely used in scientific computing, but Matlab computational
efficiency is lower than C language program. In order to improve the computing
speed, some toolbox can use GPU to accelerate the computation. This paper
describes GPU working principle, our experiments and results analysis of
parallel computing by using GPU based on Matlab. Experimental results show that
for parallel operations, GPU computing speed is faster than CPU, for the
logical instructions, GPU computing speed is slower than CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06562</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06562</id><created>2015-05-25</created><authors><author><keyname>Formentin</keyname><forenames>Simone</forenames></author><author><keyname>Dabbene</keyname><forenames>Fabrizio</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author><author><keyname>Zaccarian</keyname><forenames>Luca</forenames></author><author><keyname>Savaresi</keyname><forenames>Sergio M.</forenames></author></authors><title>Robust static anti-windup augmentation with probabilistic certificates</title><categories>cs.SY math.OC</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address robust static anti-windup compensator design and
performance analysis for saturated linear closed loops in the presence of
probabilistic parameter uncertainties via randomized techniques. The proposed
static anti-windup analysis and robust performance synthesis correspond to
several optimization goals, ranging from minimization of the nonlinear
input/output gain to maximization of the stability region or minimization of
the region of attraction. We also introduce a novel paradigm accounting for
uncertainties in the energy of the disturbance inputs. Due to the special
structure of static anti-windup design, wherein the design variables are
decoupled from the Lyapunov certificates, we introduce a significant extension,
called scenario with certificates (SwC), of the so-called scenario approach for
uncertain optimization problems. This extension is of independent interest for
similar robust synthesis problems involving parameter-dependent Lyapunov
functions. We demonstrate that the scenario with certificates robust design
formulation is appealing because it provides a way to implicitly design the
parameter-dependent Lyapunov functions and to remove restrictive assumptions
about convexity with respect to the uncertain parameters. Subsequently, to
reduce the computational cost, we present a sequential randomized algorithm for
iteratively solving this problem. The obtained results are illustrated by
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06573</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06573</id><created>2015-05-25</created><updated>2015-09-15</updated><authors><author><keyname>Grzybowski</keyname><forenames>Andrzej Z.</forenames></author></authors><title>New results on inconsistency indices and their relationship with the
  quality of priority vector estimation</title><categories>cs.AI</categories><comments>26 pages, 2 figures, 19 tables</comments><journal-ref>Expert Systems With Applications 43 (2016) 197- 212</journal-ref><doi>10.1016/j.eswa.2015.08.049</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article is devoted to the problem of inconsistency in the pairwise
comparisons based prioritization methodology. The issue of &quot;inconsistency&quot; in
this context has gained much attention in recent years. The literature provides
us with a number of different &quot;inconsistency&quot; indices suggested for measuring
the inconsistency of the pairwise comparison matrix (PCM). The latter is
understood as a deviation of the PCM from the &quot;consistent case&quot; - a notion that
is formally well-defined in this theory. However the usage of the indices is
justified only by some heuristics. It is still unclear what they really
&quot;measure&quot;. What is even more important and still not known is the relationship
between their values and the &quot;consistency&quot; of the decision maker's judgments on
one hand, and the prioritization results upon the other. We provide examples
showing that it is necessary to distinguish between these three following
tasks: the &quot;measuring&quot; of the &quot;PCM inconsistency&quot; and the PCM-based &quot;measuring&quot;
of the consistency of decision maker's judgments and, finally, the &quot;measuring&quot;
of the usefulness of the PCM as a source of information for estimation of the
priority vector (PV). Next we focus on the third task, which seems to be the
most important one in Multi-Criteria Decision Making. With the help of Monte
Carlo experiments, we study the performance of various inconsistency indices as
indicators of the final PV estimation quality. The presented results allow a
deeper understanding of the information contained in these indices and help in
choosing a proper one in a given situation. They also enable us to develop a
new inconsistency characteristic and, based on it, to propose the PCM
acceptance approach that is supported by the classical statistical methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06578</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06578</id><created>2015-05-25</created><authors><author><keyname>Rithwik</keyname><forenames>Kollipara</forenames></author><author><keyname>Chaudhury</keyname><forenames>Kunal Narayan</forenames></author></authors><title>A Simple Yet Effective Improvement to the Bilateral Filter for Image
  Denoising</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bilateral filter has diverse applications in image processing, computer
vision, and computational photography. In particular, this non-linear filter is
quite effective in denoising images corrupted with additive Gaussian noise. The
filter, however, is known to perform poorly at large noise levels. Several
adaptations of the filter have been proposed in the literature to address this
shortcoming, but often at an added computational cost. In this paper, we report
a simple yet effective modification that improves the denoising performance of
the bilateral filter at almost no additional cost. We provide visual and
quantitative results on standard test images which show that this improvement
is significant both visually and in terms of PSNR and SSIM (often as large as 5
dB). We also demonstrate how the proposed filtering can be implemented at
reduced complexity by adapting a recent idea for fast bilateral filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06582</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06582</id><created>2015-05-25</created><authors><author><keyname>Harase</keyname><forenames>Shin</forenames></author><author><keyname>Kimoto</keyname><forenames>Takamitsu</forenames></author></authors><title>Implementing 64-bit Maximally Equidistributed Mersenne Twisters</title><categories>math.NA cs.NA</categories><comments>13 Pages</comments><msc-class>65C10, 65C05, 11K45</msc-class><acm-class>G.4; I.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CPUs and operating systems are moving from 32 to 64 bits, and hence it is
important to have good pseudorandom number generators designed to fully exploit
these word lengths. However, existing 64-bit very long period generators based
on linear recurrences modulo 2 are not completely optimized in terms of the
equidistribution properties. Here we develop 64-bit maximally equidistributed
pseudorandom number generators that are optimal in this respect and have speeds
equivalent to 64-bit Mersenne Twisters. We provide a table of specific
parameters with period lengths from $2^{607}-1$ to $2^{44497}-1$. Further, we
focus on the SIMD-oriented Fast Mersenne Twister generator SFMT19937, and point
out that the use of 64-bit output sequences deteriorates its equidistribution
properties compared with 32-bit output sequences. We also show that this SFMT
generator fails a standard empirical statistical test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06588</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06588</id><created>2015-05-25</created><authors><author><keyname>Durand-Gasselin</keyname><forenames>Antoine</forenames></author><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Ganty</keyname><forenames>Pierre</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author></authors><title>Model Checking Parameterized Asynchronous Shared-Memory Systems</title><categories>cs.DC cs.FL cs.LO</categories><comments>27 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the complexity of liveness verification for parameterized
systems consisting of a leader process and arbitrarily many anonymous and
identical contributor processes. Processes communicate through a shared,
bounded-value register. While each operation on the register is atomic, there
is no synchronization primitive to execute a sequence of operations atomically.
  We analyze the case in which processes are modeled by finite-state machines
or pushdown machines and the property is given by a B\&quot;uchi automaton over the
alphabet of read and write actions of the leader. We show that the problem is
decidable, and has a surprisingly low complexity: it is NP-complete when all
processes are finite-state machines, and is PSPACE-hard and in NEXPTIME when
they are pushdown machines. This complexity is lower than for the
non-parameterized case: liveness verification of finitely many finite-state
machines is PSPACE-complete, and undecidable for two pushdown machines.
  For finite-state machines, our proofs characterize infinite behaviors using
existential abstraction and semilinear constraints. For pushdown machines, we
show how contributor computations of high stack height can be simulated by
computations of many contributors, each with low stack height. Together, our
results characterize the complexity of verification for parameterized systems
under the assumptions of anonymity and asynchrony.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06595</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06595</id><created>2015-05-25</created><authors><author><keyname>Fish</keyname><forenames>Andrew</forenames></author><author><keyname>Lisitsa</keyname><forenames>Alexei</forenames></author><author><keyname>Stanovsk&#xfd;</keyname><forenames>David</forenames></author></authors><title>A combinatorial approach to knot recognition</title><categories>math.GT cs.CC</categories><msc-class>57M25, 57M27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a report on our ongoing research on a combinatorial approach to knot
recognition, using coloring of knots by certain algebraic objects called
quandles. The aim of the paper is to summarize the mathematical theory of knot
coloring in a compact, accessible manner, and to show how to use it for
computational purposes. In particular, we address how to determine colorability
of a knot, and propose to use SAT solving to search for colorings. The
computational complexity of the problem, both in theory and in our
implementation, is discussed. In the last part, we explain how coloring can be
utilized in knot recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06596</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06596</id><created>2015-05-25</created><updated>2015-09-10</updated><authors><author><keyname>Shibata</keyname><forenames>Masahiro</forenames></author><author><keyname>Kawai</keyname><forenames>Shinji</forenames></author><author><keyname>Ooshita</keyname><forenames>Fukuhito</forenames></author><author><keyname>Kakugawa</keyname><forenames>Hirotsugu</forenames></author><author><keyname>Masuzawa</keyname><forenames>Toshimitsu</forenames></author></authors><title>Partial Gathering of Mobile Agents in Asynchronous Rings</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the partial gathering problem of mobile agents in
asynchronous unidirectional rings equipped with whiteboards on nodes. The
partial gathering problem is a new generalization of the total gathering
problem. The partial gathering problem requires, for a given integer $g$, that
each agent should move to a node and terminate so that at least $g$ agents
should meet at the same node. The requirement for the partial gathering problem
is weaker than that for the (well-investigated) total gathering problem, and
thus, we have interests in clarifying the difference on the move complexity
between them. We propose three algorithms to solve the partial gathering
problem. The first algorithm is deterministic but requires unique ID of each
agent. This algorithm achieves the partial gathering in $O(gn)$ total moves,
where $n$ is the number of nodes. The second algorithm is randomized and
requires no unique ID of each agent (i.e., anonymous). This algorithm achieves
the partial gathering in expected $O(gn)$ total moves. The third algorithm is
deterministic and requires no unique ID of each agent. For this case, we show
that there exist initial configurations in which no algorithm can solve the
problem and agents can achieve the partial gathering in $O(kn)$ total moves for
solvable initial configurations, where $k$ is the number of agents. Note that
the total gathering problem requires $\Omega (kn)$ total moves, while the
partial gathering problem requires $\Omega (gn)$ total moves in each model.
Hence, we show that the move complexity of the first and second algorithms is
asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06600</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06600</id><created>2015-05-25</created><authors><author><keyname>Ofir</keyname><forenames>Nati</forenames></author><author><keyname>Galun</keyname><forenames>Meirav</forenames></author><author><keyname>Nadler</keyname><forenames>Boaz</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author></authors><title>Fast Detection of Curved Edges at Low SNR</title><categories>cs.CV</categories><comments>9 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting edges is a fundamental problem in computer vision with many
applications, some involving very noisy images. While most edge detection
methods are fast, they perform well only on relatively clean images. Indeed,
edges in such images can be reliably detected using only local filters.
Detecting faint edges under high levels of noise cannot be done locally at the
individual pixel level, and requires more sophisticated global processing.
Unfortunately, existing methods that achieve this goal are quite slow. In this
paper we develop a novel multiscale method to detect curved edges in noisy
images. While our algorithm searches for edges over a huge set of candidate
curves, it does so in a practical runtime, nearly linear in the total number of
image pixels. As we demonstrate experimentally, our algorithm is orders of
magnitude faster than previous methods designed to deal with high noise levels.
Nevertheless, it obtains comparable, if not better, edge detection quality on a
variety of challenging noisy images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06605</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06605</id><created>2015-05-25</created><updated>2015-10-18</updated><authors><author><keyname>Sarvadevabhatla</keyname><forenames>Ravi Kiran</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>Expresso : A user-friendly GUI for Designing, Training and Exploring
  Convolutional Neural Networks</title><categories>cs.CV cs.NE</categories><comments>Project page : http://val.serc.iisc.ernet.in/expresso/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With a view to provide a user-friendly interface for designing, training and
developing deep learning frameworks, we have developed Expresso, a GUI tool
written in Python. Expresso is built atop Caffe, the open-source, prize-winning
framework popularly used to develop Convolutional Neural Networks. Expresso
provides a convenient wizard-like graphical interface which guides the user
through various common scenarios -- data import, construction and training of
deep networks, performing various experiments, analyzing and visualizing the
results of these experiments. The multi-threaded nature of Expresso enables
concurrent execution and notification of events related to the aforementioned
scenarios. The GUI sub-components and inter-component interfaces in Expresso
have been designed with extensibility in mind. We believe Expresso's
flexibility and ease of use will come in handy to researchers, newcomers and
seasoned alike, in their explorations related to deep learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06606</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06606</id><created>2015-05-25</created><updated>2015-09-22</updated><authors><author><keyname>Belagiannis</keyname><forenames>Vasileios</forenames></author><author><keyname>Rupprecht</keyname><forenames>Christian</forenames></author><author><keyname>Carneiro</keyname><forenames>Gustavo</forenames></author><author><keyname>Navab</keyname><forenames>Nassir</forenames></author></authors><title>Robust Optimization for Deep Regression</title><categories>cs.CV</categories><comments>Accepted for publication at the International Conference on Computer
  Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (ConvNets) have successfully contributed to
improve the accuracy of regression-based methods for computer vision tasks such
as human pose estimation, landmark localization, and object detection. The
network optimization has been usually performed with L2 loss and without
considering the impact of outliers on the training process, where an outlier in
this context is defined by a sample estimation that lies at an abnormal
distance from the other training sample estimations in the objective space. In
this work, we propose a regression model with ConvNets that achieves robustness
to such outliers by minimizing Tukey's biweight function, an M-estimator robust
to outliers, as the loss function for the ConvNet. In addition to the robust
loss, we introduce a coarse-to-fine model, which processes input images of
progressively higher resolutions for improving the accuracy of the regressed
values. In our experiments, we demonstrate faster convergence and better
generalization of our robust loss function for the tasks of human pose
estimation and age estimation from face images. We also show that the
combination of the robust loss function with the coarse-to-fine model produces
comparable or better results than current state-of-the-art approaches in four
publicly available human pose estimation datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06607</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06607</id><created>2015-05-25</created><updated>2015-05-26</updated><authors><author><keyname>Wang</keyname><forenames>Yijie</forenames></author><author><keyname>Qian</keyname><forenames>Xiaoning</forenames></author></authors><title>Stochastic Block Coordinate Frank-Wolfe Algorithm for Large-Scale
  Biological Network Alignment</title><categories>cs.CE q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increasingly &quot;big&quot; data available in biomedical research, deriving
accurate and reproducible biology knowledge from such big data imposes enormous
computational challenges. In this paper, motivated by recently developed
stochastic block coordinate algorithms, we propose a highly scalable randomized
block coordinate Frank-Wolfe algorithm for convex optimization with general
compact convex constraints, which has diverse applications in analyzing
biomedical data for better understanding cellular and disease mechanisms. We
focus on implementing the derived stochastic block coordinate algorithm to
align protein-protein interaction networks for identifying conserved functional
pathways based on the IsoRank framework. Our derived stochastic block
coordinate Frank-Wolfe (SBCFW) algorithm has the convergence guarantee and
naturally leads to the decreased computational cost (time and space) for each
iteration. Our experiments for querying conserved functional protein complexes
in yeast networks confirm the effectiveness of this technique for analyzing
large-scale biological networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06608</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06608</id><created>2015-05-25</created><updated>2016-01-30</updated><authors><author><keyname>Zhou</keyname><forenames>Pan</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author></authors><title>Towards Optimal Adaptive Wireless Communications in Unknown Environments</title><categories>cs.NI cs.IT math.IT</categories><comments>accepted, and to appear in IEEE transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing efficient channel access schemes for wireless communications
without any prior knowledge about the nature of environments has been a very
challenging issue, especially when the channel states distribution of all
spectrum resources could be entirely or partially stochastic and/or adversarial
at different time and locations. In this paper, we propose an adaptive channel
access algorithm for wireless communications in unknown environments based on
the theory of multi-armed bandits (MAB) problems. By automatically tuning two
control parameters, i.e., learning rate and exploration probability, our
algorithms are capable of finding the optimal channel access strategies and
achieving the almost optimal learning performance over time under our defined
four typical regimes for general unknown environments, e.g., the stochastic
regime where channels follow some unknown i.i.d process, the adversarial regime
where all channels are suffered by adversarial jamming attack, the mixed
stochastic and adversarial regime where a subset of channels are attacked and
the contaminated stochastic regime where occasionally adversarial events
contaminate the stochastic channel process, etc. To reduce the implementation
time and space complexity, we further develop an enhanced algorithm by
exploiting the internal structure of the selection of channel access strategy.
We conduct extensive simulations in all these regimes to validate our
theoretical analysis. The quantitative performance studies indicate the
superior throughput gain and the flexibility of our algorithm in practice,
which is resilient to both oblivious and adaptive jamming attacks with
different intelligence and any attacking strength that ranges from no-attack to
the full-attack of all spectrum resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06611</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06611</id><created>2015-05-25</created><updated>2016-01-25</updated><authors><author><keyname>Yokota</keyname><forenames>Tatsuya</forenames></author><author><keyname>Zhao</keyname><forenames>Qibin</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Smooth PARAFAC Decomposition for Tensor Completion</title><categories>cs.CV</categories><comments>13 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, low-rank based tensor completion, which is a higher-order
extension of matrix completion, has received considerable attention. However,
the low-rank assumption is not sufficient for the recovery of visual data, such
as color and 3D images, where the ratio of missing data is extremely high. In
this paper, we consider &quot;smoothness&quot; constraints as well as low-rank
approximations, and propose an efficient algorithm for performing tensor
completion that is particularly powerful regarding visual data. The proposed
method admits significant advantages, owing to the integration of smooth
PARAFAC decomposition for incomplete tensors and the efficient selection of
models in order to minimize the tensor rank. Thus, our proposed method is
termed as &quot;smooth PARAFAC tensor completion (SPC).&quot; In order to impose the
smoothness constraints, we employ two strategies, total variation (SPC-TV) and
quadratic variation (SPC-QV), and invoke the corresponding algorithms for model
learning. Extensive experimental evaluations on both synthetic and real-world
visual data illustrate the significant improvements of our method, in terms of
both prediction performance and efficiency, compared with many state-of-the-art
tensor completion methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06614</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06614</id><created>2015-05-25</created><authors><author><keyname>De Leone</keyname><forenames>Renato</forenames></author><author><keyname>Minnetti</keyname><forenames>Valentina</forenames></author></authors><title>Electre Tri-Machine Learning Approach to the Record Linkage Problem</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short paper, the Electre Tri-Machine Learning Method, generally used
to solve ordinal classification problems, is proposed for solving the Record
Linkage problem. Preliminary experimental results show that, using the Electre
Tri method, high accuracy can be achieved and more than 99% of the matches and
nonmatches were correctly identified by the procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06615</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06615</id><created>2015-05-25</created><authors><author><keyname>Liu</keyname><forenames>Dong</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author></authors><title>Energy Efficiency of Downlink Networks with Caching at Base Stations</title><categories>cs.IT cs.NI math.IT</categories><comments>A part of this work was published in IEEE GlobeSIP 2014 with title
  &quot;Will Caching at Base Station Improve Energy Efficiency of Downlink
  Transmission?&quot;. This work was supported in part by National Natural Science
  Foundation of China (NSFC) under Grant 61120106002 and 973 Program
  2012CB316003</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching popular contents at the base station (BS) can reduce the backhaul
cost and improve the network throughput. Yet whether locally caching at the BSs
can improve the energy efficiency (EE), a major goal for 5th-generation
cellular networks, remains unclear. Due to the entangled impact of various
factors on EE such as interference level, backhaul capacity, BS density, power
consumption parameters, BS sleeping, content popularity and cache capacity,
another important question is what are the key factors that contribute more to
the EE gain from caching. In this paper, we attempt to explore the potential of
EE of the cache-enabled wireless access networks and identify the key factors.
By deriving the closed-form expression of the EE, we provide the condition when
the EE can benefit from caching, find the optimal cache capacity that maximizes
the network EE, and analyze the maximal EE gain brought by caching. We show
that caching at the BSs can improve the network EE when power efficient cache
hardware is used. When local caching has EE gain over not caching, caching more
files at the BSs may not provide higher EE. Numerical and simulation results
validate our analysis and show that the caching EE gain is large when the
backhaul capacity is stringent, interference level is low, cached files are
popular, and when caching at pico BSs instead of macro BSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06617</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06617</id><created>2015-05-25</created><authors><author><keyname>Kotek</keyname><forenames>Tomer</forenames></author><author><keyname>Makowsky</keyname><forenames>Johann A.</forenames></author></authors><title>Efficient computation of generalized Ising polynomials on graphs with
  fixed clique-width</title><categories>cs.LO</categories><comments>12 pages, 1 figure</comments><msc-class>05C31</msc-class><acm-class>F.4.1; F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph polynomials which are definable in Monadic Second Order Logic (MSOL) on
the vocabulary of graphs are Fixed-Parameter Tractable (FPT) with respect to
clique-width. In contrast, graph polynomials which are definable in MSOL on the
vocabulary of hypergraphs are fixed-parameter tractable with respect to
tree-width, but not necessarily with respect to clique width. No algorithmic
meta-theorem is known for the computation of graph polynomials definable in
MSOL on the vocabulary of hypergraphs with respect to clique-width. We define
an infinite class of such graph polynomials extending the class of graph
polynomials definable in MSOL on the vocabulary of graphs and prove that they
are Fixed-Parameter Polynomial Time (FPPT) computable, i.e. that they can be
computed in time $O(n^{f(k)})$, where $n$ is the number of vertices and $k$ is
the clique-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06621</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06621</id><created>2015-05-25</created><authors><author><keyname>Riccio</keyname><forenames>Giuseppe</forenames></author><author><keyname>Cavuoti</keyname><forenames>Stefano</forenames></author><author><keyname>Schisano</keyname><forenames>Eugenio</forenames></author><author><keyname>Brescia</keyname><forenames>Massimo</forenames></author><author><keyname>Mercurio</keyname><forenames>Amata</forenames></author><author><keyname>Elia</keyname><forenames>Davide</forenames></author><author><keyname>Benedettini</keyname><forenames>Milena</forenames></author><author><keyname>Pezzuto</keyname><forenames>Stefano</forenames></author><author><keyname>Molinari</keyname><forenames>Sergio</forenames></author><author><keyname>Di Giorgio</keyname><forenames>Anna Maria</forenames></author></authors><title>Machine learning based data mining for Milky Way filamentary structures
  reconstruction</title><categories>astro-ph.IM cs.CV</categories><comments>Accepted by peer reviewed WIRN 2015 Conference, to appear on Smart
  Innovation, Systems and Technology, Springer, ISSN 2190-3018, 9 pages, 4
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an innovative method called FilExSeC (Filaments Extraction,
Selection and Classification), a data mining tool developed to investigate the
possibility to refine and optimize the shape reconstruction of filamentary
structures detected with a consolidated method based on the flux derivative
analysis, through the column-density maps computed from Herschel infrared
Galactic Plane Survey (Hi-GAL) observations of the Galactic plane. The present
methodology is based on a feature extraction module followed by a machine
learning model (Random Forest) dedicated to select features and to classify the
pixels of the input images. From tests on both simulations and real
observations the method appears reliable and robust with respect to the
variability of shape and distribution of filaments. In the cases of highly
defined filament structures, the presented method is able to bridge the gaps
among the detected fragments, thus improving their shape reconstruction. From a
preliminary &quot;a posteriori&quot; analysis of derived filament physical parameters,
the method appears potentially able to add a sufficient contribution to
complete and refine the filament reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06622</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06622</id><created>2015-05-25</created><authors><author><keyname>Kotek</keyname><forenames>Tomer</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author><author><keyname>Zuleger</keyname><forenames>Florian</forenames></author></authors><title>Monadic second order finite satisfiability and unbounded tree-width</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The finite satisfiability problem of monadic second order logic is decidable
only on classes of structures of bounded tree-width by the classic result of
Seese (1991). We prove the following problem is decidable:
  Input: (i) A monadic second order logic sentence $\alpha$, and (ii) a
sentence $\beta$ in the two-variable fragment of first order logic extended
with counting quantifiers. The vocabularies of $\alpha$ and $\beta$ may
intersect.
  Output: Is there a finite structure which satisfies $\alpha\land\beta$ such
that the restriction of the structure to the vocabulary of $\alpha$ has bounded
tree-width? (The tree-width of the desired structure is not bounded.)
  As a consequence, we prove the decidability of the satisfiability problem by
a finite structure of bounded tree-width of a logic extending monadic second
order logic with linear cardinality constraints of the form
$|X_{1}|+\cdots+|X_{r}|&lt;|Y_{1}|+\cdots+|Y_{s}|$, where the $X_{i}$ and $Y_{j}$
are monadic second order variables. We prove the decidability of a similar
extension of WS1S.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06623</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06623</id><created>2015-05-25</created><authors><author><keyname>He</keyname><forenames>Meijun</forenames></author><author><keyname>Zhang</keyname><forenames>Shuye</forenames></author><author><keyname>Mao</keyname><forenames>Huiyun</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author></authors><title>Recognition Confidence Analysis of Handwritten Chinese Character with
  CNN</title><categories>cs.CV</categories><comments>5 pages, 8 figures, 4 tables. Accepted to appear at ICDAR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an effective method to analyze the recognition
confidence of handwritten Chinese character, based on the softmax regression
score of a high performance convolutional neural networks (CNN). Through
careful and thorough statistics of 827,685 testing samples that randomly
selected from total 8836 different classes of Chinese characters, we find that
the confidence measurement based on CNN is an useful metric to know how
reliable the recognition results are. Furthermore, we find by experiments that
the recognition confidence can be used to find out similar and confusable
character-pairs, to check wrongly or cursively written samples, and even to
discover and correct mis-labelled samples. Many interesting observations and
statistics are given and analyzed in this study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06630</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06630</id><created>2015-05-25</created><authors><author><keyname>Chang</keyname><forenames>Michael Alan</forenames></author><author><keyname>Holterbach</keyname><forenames>Thomas</forenames></author><author><keyname>Happe</keyname><forenames>Markus</forenames></author><author><keyname>Vanbever</keyname><forenames>Laurent</forenames></author></authors><title>Supercharge me: Boost Router Convergence with SDN</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Defined Networking (SDN) is a promising approach for improving the
performance and manageability of future network architectures. However, little
work has gone into using SDN to improve the performance and manageability of
existing networks without requiring a major overhaul of the existing network
infrastructure.
  In this paper, we show how we can dramatically improve, or supercharge, the
performance of existing IP routers by combining them with SDN-enabled equipment
in a novel way. More particularly, our supercharged solution substantially
reduces the convergence time of an IP router upon link or node failure without
inducing any reconfiguration of the IP router itself. Our key insight is to use
the SDN controller to precompute backup forwarding entries and immediately
activate them upon failure, enabling almost immediate data-plane recovery,
while letting the router converge at its typical slow pace. By boosting
existing equipment's performance, we not only increase their lifetime but also
provide new incentives for network operators to kickstart SDN deployment.
  We implemented a fully functional &quot;supercharger&quot; and use it to boost the
convergence performance of a Cisco Nexus 7k router. Using a FPGA-based traffic
generator, we show that our supercharged router systematically converges within
~150ms, a 900x reduction with respect to its normal convergence time under
similar conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06640</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06640</id><created>2015-04-24</created><authors><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author><author><keyname>Poppi</keyname><forenames>Ricardo</forenames></author></authors><title>Continuous voting by approval and participation</title><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In finding the adequate way to prioritize proposals, the Brazilian
participation community agreed about the measurement of two indexes, one of
approval and one of participation. Both practice and literature is constantly
handled by the experts involved, and the formalization of such model and
metrics seems novel. Also, the relevance of this report is strengthened by the
nearby use of these indexes by the Brazilian General Secretariat of the
Republic to raise and prioritize proposals about public health care in open
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06646</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06646</id><created>2015-05-25</created><updated>2015-06-28</updated><authors><author><keyname>Guidi</keyname><forenames>F.</forenames></author><author><keyname>Coen</keyname><forenames>C. Sacerdoti</forenames></author></authors><title>A Survey on Retrieval of Mathematical Knowledge</title><categories>cs.IR cs.DL</categories><comments>CICM 2015, 20 pages</comments><acm-class>A.1; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a short survey of the literature on indexing and retrieval of
mathematical knowledge, with pointers to 72 papers and tentative taxonomies of
both retrieval problems and recurring techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06651</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06651</id><created>2015-05-25</created><updated>2015-07-09</updated><authors><author><keyname>Wang</keyname><forenames>Yanjing</forenames></author></authors><title>A Logic of Knowing How</title><categories>cs.AI cs.LO</categories><comments>14 pages, a 12-page version accepted by LORI V</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a single-agent modal logic framework for reasoning
about goal-direct &quot;knowing how&quot; based on ideas from linguistics, philosophy,
modal logic and automated planning. We first define a modal language to express
&quot;I know how to guarantee phi given psi&quot; with a semantics not based on standard
epistemic models but labelled transition systems that represent the agent's
knowledge of his own abilities. A sound and complete proof system is given to
capture the valid reasoning patterns about &quot;knowing how&quot; where the most
important axiom suggests its compositional nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06661</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06661</id><created>2015-05-25</created><authors><author><keyname>Rangapuram</keyname><forenames>Syama Sundar</forenames></author><author><keyname>B&#xfc;hler</keyname><forenames>Thomas</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author></authors><title>Towards Realistic Team Formation in Social Networks based on Densest
  Subgraphs</title><categories>cs.SI cs.DS</categories><comments>Technical report of paper accepted at WWW 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a task $\mathcal{T}$, a set of experts $V$ with multiple skills and a
social network $G(V, W)$ reflecting the compatibility among the experts, team
formation is the problem of identifying a team $C \subseteq V$ that is both
competent in performing the task $\mathcal{T}$ and compatible in working
together. Existing methods for this problem make too restrictive assumptions
and thus cannot model practical scenarios. The goal of this paper is to
consider the team formation problem in a realistic setting and present a novel
formulation based on densest subgraphs. Our formulation allows modeling of many
natural requirements such as (i) inclusion of a designated team leader and/or a
group of given experts, (ii) restriction of the size or more generally cost of
the team (iii) enforcing locality of the team, e.g., in a geographical sense or
social sense, etc.
  The proposed formulation leads to a generalized version of the classical
densest subgraph problem with cardinality constraints (DSP), which is an NP
hard problem and has many applications in social network analysis. In this
paper, we present a new method for (approximately) solving the generalized DSP
(GDSP). Our method, FORTE, is based on solving an equivalent continuous
relaxation of GDSP. The solution found by our method has a quality guarantee
and always satisfies the constraints of GDSP. Experiments show that the
proposed formulation (GDSP) is useful in modeling a broader range of team
formation problems and that our method produces more coherent and compact teams
of high quality. We also show, with the help of an LP relaxation of GDSP, that
our method gives close to optimal solutions to GDSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06664</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06664</id><created>2015-05-25</created><updated>2015-05-26</updated><authors><author><keyname>Wang</keyname><forenames>Xiangrong</forenames></author><author><keyname>Ko&#xe7;</keyname><forenames>Yakup</forenames></author><author><keyname>Derrible</keyname><forenames>Sybil</forenames></author><author><keyname>Ahmad</keyname><forenames>Sk Nasir</forenames></author><author><keyname>Kooij</keyname><forenames>Robert E.</forenames></author></authors><title>Quantifying the robustness of metro networks</title><categories>physics.soc-ph cs.SY</categories><comments>13 pages, 4 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metros (heavy rail transit systems) are integral parts of urban
transportation systems. Failures in their operations can have serious impacts
on urban mobility, and measuring their robustness is therefore critical.
Moreover, as physical networks, metros can be viewed as network topological
entities, and as such they possess measurable network properties. In this
paper, by using network science and graph theoretical concepts, we investigate
both theoretical and experimental robustness metrics (i.e., the robustness
indicator, the effective graph conductance, and the critical thresholds) and
their performance in quantifying the robustness of metro networks under random
failures or targeted attacks. We find that the theoretical metrics quantify
different aspects of the robustness of metro networks. In particular, the
robustness indicator captures the number of alternative paths and the effective
graph conductance focuses on the length of each path. Moreover, the high
positive correlation between the theoretical metrics and experimental metrics
and the negative correlation within the theoretical metrics provide significant
insights for planners to design more robust system while accommodating for
transit specificities (e.g., alternative paths, fast transferring).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06685</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06685</id><created>2015-05-25</created><authors><author><keyname>Zhang</keyname><forenames>Jiayi</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author></authors><title>On the Multivariate Gamma-Gamma ($\Gamma \Gamma$) Distribution with
  Arbitrary Correlation and Applications in Wireless Communications</title><categories>cs.IT math.IT</categories><comments>7 pages, 6 figures, accepted by IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The statistical properties of the multivariate Gamma-Gamma ($\Gamma \Gamma$)
distribution with arbitrary correlation have remained unknown. In this paper,
we provide analytical expressions for the joint probability density function
(PDF), cumulative distribution function (CDF) and moment generation function of
the multivariate $\Gamma \Gamma$ distribution with arbitrary correlation.
Furthermore, we present novel approximating expressions for the PDF and CDF of
the sum of $\Gamma \Gamma$ random variables with arbitrary correlation. Based
on this statistical analysis, we investigate the performance of radio frequency
and optical wireless communication systems. It is noteworthy that the presented
expressions include several previous results in the literature as special
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06699</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06699</id><created>2015-05-25</created><updated>2016-02-01</updated><authors><author><keyname>Zhuang</keyname><forenames>Hao</forenames></author><author><keyname>Yu</keyname><forenames>Wenjian</forenames></author><author><keyname>Weng</keyname><forenames>Shih-Hung</forenames></author><author><keyname>Kang</keyname><forenames>Ilgweon</forenames></author><author><keyname>Lin</keyname><forenames>Jeng-Hau</forenames></author><author><keyname>Zhang</keyname><forenames>Xiang</forenames></author><author><keyname>Coutts</keyname><forenames>Ryan</forenames></author><author><keyname>Cheng</keyname><forenames>Chung-Kuan</forenames></author></authors><title>Simulation Algorithms with Exponential Integration for Time-Domain
  Analysis of Large-Scale Power Delivery Networks</title><categories>cs.CE cs.DC cs.NA math.DS</categories><comments>Accepted by IEEE Transactions on Computer Aided Design of Integrated
  Circuits and Systems (TCAD)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design an algorithmic framework using matrix exponentials for time-domain
simulation of power delivery network (PDN). Our framework can reuse factorized
matrices to simulate the large-scale linear PDN system with variable stepsizes.
In contrast, current conventional PDN simulation solvers have to use fixed
step-size approach in order to reuse factorized matrices generated by the
expensive matrix decomposition. Based on the proposed exponential integration
framework, we design a PDN solver R-MATEX with the flexible time-stepping
capability. The key operation of matrix exponential and vector product (MEVP)
is computed by the rational Krylov subspace method.
  To further improve the runtime, we also propose a distributed computing
framework DR-MATEX. DR-MATEX reduces Krylov subspace generations caused by
frequent breakpoints from a large number of current sources during simulation.
By virtue of the superposition property of linear system and scaling invariance
property of Krylov subspace, DR-MATEX can divide the whole simulation task into
subtasks based on the alignments of breakpoints among those sources. The
subtasks are processed in parallel at different computing nodes without any
communication during the computation of transient simulation. The final result
is obtained by summing up the partial results among all the computing nodes
after they finish the assigned subtasks. Therefore, our computation model
belongs to the category known as Embarrassingly Parallel model.
  Experimental results show R-MATEX and DR-MATEX can achieve up to around 14.4X
and 98.0X runtime speedups over traditional trapezoidal integration based
solver with fixed timestep approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06701</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06701</id><created>2015-05-25</created><authors><author><keyname>Luna</keyname><forenames>Alexandre J. H. de O.</forenames></author><author><keyname>Kruchten</keyname><forenames>Philippe</forenames></author><author><keyname>de Moura</keyname><forenames>Hermano P.</forenames></author></authors><title>Agile Governance Theory: conceptual development</title><categories>cs.SE</categories><comments>Conference Proceedings of 12th International Conference on Management
  of Technology and Information Systems, 22 pages, 8 figures. In 12th
  International Conference on Management of Technology and Information Systems.
  Sao Paulo: FEA-USP</comments><report-no>2423-14033-1-PB, FEA-USP, Sao Paulo, SP, Brazil</report-no><acm-class>K.6; K.6.1; K.6.4; D.2; D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Competitiveness is the key to a sustainable development and it
demands agility at the business and organizational levels, which in turn
requires a flexible and customizable IT environment and effective and
responsive governance in order to deliver value to the business. Objective:
This paper describes the conceptual development of a theory for analyze and
describe agile governance in order to increasing the success rate of their
practice, achieving organizational performance and business competitiveness.
Method: We adopt a multi-method research, framing the theory conceptual
development using Dubin's method of theory building. Results: We have developed
a conceptual framework of the theory encompassing its constructs, laws of
interaction, boundaries and system states. Conclusion: This theory can provide
a better understanding of the nature of agile governance, by mapping of its
constructs, mediators, moderators and disturbing factors, in order to help
organizations reach better results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06702</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06702</id><created>2015-05-25</created><authors><author><keyname>Kniefacz</keyname><forenames>Philipp</forenames></author><author><keyname>Kropatsch</keyname><forenames>Walter</forenames></author></authors><title>Smooth and iteratively Restore: A simple and fast edge-preserving
  smoothing model</title><categories>cs.CV</categories><comments>Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</comments><report-no>OAGM/2015/16</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In image processing, it can be a useful pre-processing step to smooth away
small structures, such as noise or unimportant details, while retaining the
overall structure of the image by keeping edges, which separate objects, sharp.
Typically this edge-preserving smoothing process is achieved using edge-aware
filters. However such filters may preserve unwanted small structures as well if
they contain edges. In this work we present a novel framework for
edge-preserving smoothing which separates the process into two different steps:
First the image is smoothed using a blurring filter and in the second step the
important edges are restored using a guided edge-aware filter. The presented
method proves to deliver very good results, compared to state-of-the-art
edge-preserving smoothing filters, especially at removing unwanted small
structures. Furthermore it is very versatile and can easily be adapted to
different fields of applications while at the same time being very fast to
compute and therefore well-suited for real time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06713</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06713</id><created>2015-05-25</created><authors><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author></authors><title>On Web-based Domain-Specific Language for Internet of Things</title><categories>cs.NI</categories><comments>submitted to ICUMT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the challenges of the Internet of Things programming.
Sensing and data gathering from the various sources are often the key elements
of applications for Smart Cities. So, the effective programming models for them
are very important. In this article, we discuss system software models and
solutions, rather than network related aspects. In our paper, we present the
web-based domain-specific language for Internet of Things applications. Our
goal is to present the modern models for data processing in Internet of Things
and Smart Cities applications. In our view, the use of this kind of tools
should seriously reduce the time to develop new applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06729</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06729</id><created>2015-05-24</created><authors><author><keyname>Vakilian</keyname><forenames>Vida</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author><author><keyname>Hua</keyname><forenames>Yingbo</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>High Rate/Low Complexity Space-Time Block Codes for 2x2 Reconfigurable
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1505.06466</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a full-rate full-diversity space-time block code
(STBC) for 2x2 reconfigurable multiple-input multiple-output (MIMO) systems
that require a low complexity maximum likelihood (ML) detector. We consider a
transmitter equipped with a linear antenna array where each antenna element can
be independently configured to create a directive radiation pattern toward a
selected direction. This property of transmit antennas allow us to increase the
data rate of the system, while reducing the computational complexity of the
receiver. The proposed STBC achieves a coding rate of two in a 2x2 MIMO system
and can be decoded via an ML detector with a complexity of order M, where M is
the cardinality of the transmitted symbol constellation. Our simulations
demonstrate the efficiency of the proposed code compared to existing STBCs in
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06747</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06747</id><created>2015-05-25</created><updated>2015-11-06</updated><authors><author><keyname>Gimenes</keyname><forenames>Gabriel</forenames></author><author><keyname>Rodrigues</keyname><forenames>Jose F.</forenames><suffix>Jr</suffix></author><author><keyname>Cordeiro</keyname><forenames>Robson</forenames></author></authors><title>ORFEL: super-fast detection of defamation and illegitimate promotion in
  online recommendation</title><categories>cs.SI</categories><comments>10 pages, Draft submitted to Elsevier Information Sciences Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What if a successful company starts to receive a torrent of low-valued (one
or two stars) recommendations in its mobile apps from multiple users within a
short (say one month) period? Is it legitimate evidence that the apps have lost
quality, or an intentional plan (via lockstep behavior) to steal market share
through defamation? In case of a systematic attack to one's reputation, it
might not be possible to manually discern between legitimate and fraudulent
interaction in the immense universe of possibilities of user-product
recommendation. Previous works have focused on this issue, but none of them has
considered the context, modeling, and scale that we work with in this paper. We
propose one novel method named Online-Recommendation Fraud ExcLuder (\ORFEL) to
detect defamation and/or illegitimate promotion of online products using
vertex-centric asynchronous parallel processing of bipartite (users-products)
graphs. With an innovative algorithm, our results demonstrate efficacy --
detecting over $95\%$ of potential attacks; and efficiency -- at least two
orders of magnitude faster than the state-of-the-art. Over our new methodology,
we introduce three contributions: (1) a new algorithmic solution; (2) a
scalable approach; and (3) a novel context and modeling of the problem, which
now addresses both defamation and illegitimate promotion. Our work deals with
relevant issues of the Web 2.0, potentially augmenting the credibility of
online recommendation to prevent losses to both customers and vendors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06750</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06750</id><created>2015-05-25</created><updated>2015-05-28</updated><authors><author><keyname>Dodds</keyname><forenames>P. S.</forenames></author><author><keyname>Clark</keyname><forenames>E. M.</forenames></author><author><keyname>Desu</keyname><forenames>S.</forenames></author><author><keyname>Frank</keyname><forenames>M. R.</forenames></author><author><keyname>Reagan</keyname><forenames>A. J.</forenames></author><author><keyname>Williams</keyname><forenames>J. R.</forenames></author><author><keyname>Mitchell</keyname><forenames>L.</forenames></author><author><keyname>Harris</keyname><forenames>K. D.</forenames></author><author><keyname>Kloumann</keyname><forenames>I. M.</forenames></author><author><keyname>Bagrow</keyname><forenames>J. P.</forenames></author><author><keyname>Megerdoomian</keyname><forenames>K.</forenames></author><author><keyname>McMahon</keyname><forenames>M. T.</forenames></author><author><keyname>Tivnan</keyname><forenames>B. F.</forenames></author><author><keyname>Danforth</keyname><forenames>C. M.</forenames></author></authors><title>Reply to Garcia et al.: Common mistakes in measuring frequency dependent
  word characteristics</title><categories>physics.soc-ph cs.CL</categories><comments>5 pages, 2 figures, 1 table. Expanded version of reply appearing in
  PNAS 2015</comments><doi>10.1073/pnas.1505647112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that the concerns expressed by Garcia et al. are misplaced,
due to (1) a misreading of our findings in [1]; (2) a widespread failure to
examine and present words in support of asserted summary quantities based on
word usage frequencies; and (3) a range of misconceptions about word usage
frequency, word rank, and expert-constructed word lists. In particular, we show
that the English component of our study compares well statistically with two
related surveys, that no survey design influence is apparent, and that
estimates of measurement error do not explain the positivity biases reported in
our work and that of others. We further demonstrate that for the frequency
dependence of positivity---of which we explored the nuances in great detail in
[1]---Garcia et al. did not perform a reanalysis of our data---they instead
carried out an analysis of a different, statistically improper data set and
introduced a nonlinearity before performing linear regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06751</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06751</id><created>2015-05-20</created><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author><author><keyname>Yousif</keyname><forenames>Raghad Zuhair</forenames></author></authors><title>Novel Mining of Cancer via Mutation in Tumor Protein P53 using Quick
  Propagation Network</title><categories>cs.CE</categories><comments>6 Pages, 9 figures, 2 Table</comments><journal-ref>International Journal of Computer Science and Electronics
  Engineering IJCSEE, Volume 3, Issue 2, 2015</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There is multiple databases contain datasets of TP53 gene and its tumor
protein P53 which believed to be involved in over 50% of human cancers cases,
these databases are rich as datasets covered all mutations caused diseases
(cancers), but they haven't efficient mining method can classify and diagnosis
mutations patient's then predict the cancer of that patient. This paper
proposed a novel mining of cancer via mutations because there is no mining
method before offers friendly, effective and flexible predict or diagnosis of
cancers via using whole common database of TP53 gene (tumor protein P53) as
dataset and selecting a minimum number of fields in training and testing quick
propagation algorithm which supporting this miming method. Simulating quick
propagation network for the train dataset shows results the Correlation
(0.9999), R-squared (0.9998) and mean of Absolute Relative Error (0.0029),
while the training for the ALL datasets (train, test and validation dataset)
have results the Correlation (0.9993), R-squared (0.9987) and mean of Absolute
Relative Error (0.0057).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06765</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06765</id><created>2015-05-25</created><authors><author><keyname>Skorski</keyname><forenames>Maciej</forenames></author></authors><title>A Time-Success Ratio Analysis of wPRF-based Leakage-Resilient Stream
  Ciphers</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak pseudorandom functions (wPRFs) found an important application as main
building blocks for leakage-resilient ciphers (EUROCRYPT'09). Several security
bounds, based on different techniques, were given to these stream ciphers. The
security loss in these reduction-based proofs is always polynomial, but has not
been studied in detail. The aim of this paper is twofold. First, we present a
clear comparison of quantitatively different security bounds in the literature.
Second, we revisit the current proof techniques and answer the natural question
of how far we are from meaningful and provable security guarantees, when
instantiating weak PRFs with standard primitives (block ciphers or hash
functions). In particular, we demonstrate a flaw in the recent (TCC'14)
analysis of the EUROCRYPT'09 stream cipher, which means that we still don't
know if it offers provable security when instantiated with a standard block
cipher. Our approach is a \emph{time-to-success Ratio} analysis, a universal
measure introduced by Luby, which allow us to compare different security
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06769</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06769</id><created>2015-05-25</created><authors><author><keyname>Gruschina</keyname><forenames>Alexander</forenames></author></authors><title>VeinPLUS: A Transillumination and Reflection-based Hand Vein Database</title><categories>cs.CV</categories><comments>Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</comments><report-no>OAGM/2015/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a short summary of work related to the creation of a
department-hosted hand vein database. After the introducing section, special
properties of the hand vein acquisition are explained, followed by a comparison
table, which shows key differences to existing well-known hand vein databases.
At the end, the ROI extraction process is described and sample images and ROIs
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06770</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06770</id><created>2015-05-25</created><authors><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Thompson</keyname><forenames>Andrew</forenames></author></authors><title>Sketching for Sequential Change-Point Detection</title><categories>cs.LG stat.ML</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study sequential change-point detection using sketches (or linear
projections) of the high-dimensional data vectors, and present a new sketching
procedure, which is based on the generalized likelihood ratio statistic. We
derive theoretical approximations to two fundamental performance metrics for
the sketching procedures: the average run length (ARL) and the expected
detection delay (EDD), and these approximations are shown to be highly accurate
by numerical simulations. We also analyze the ratio of EDD between the
sketching procedure and a procedure using the original data, when the sketching
matrix $A$ is a random Gaussian matrix and a sparse 0-1 matrix (in particular,
an expander graph), respectively. Finally, numerical examples demonstrate that
the sketching procedure can approach the performance of the procedure using the
original data, even when the post-change mean vector is not sparse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06784</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06784</id><created>2015-05-25</created><updated>2015-06-02</updated><authors><author><keyname>Wang</keyname><forenames>Xinhua</forenames></author><author><keyname>Cai</keyname><forenames>Lilong</forenames></author></authors><title>Mathematical modeling and control of a tilt-rotor aircraft</title><categories>cs.SY</categories><journal-ref>Aerospace Science and Technology, vol. 47, no. 12, 2015, 473-492</journal-ref><doi>10.1016/j.ast.2015.10.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel model of large-size tilt-rotor aircraft, which
can operate as a helicopter as well as being capable of transition to
fixed-wing flight. Aerodynamics of the dynamic large-size tilt-rotors based on
blade element method is analyzed during mode transition. For the large-size
aircraft with turboshaft engines, the blade pitch angles of the rotors are
regulated to vary according to the desired level of thrust, and the following
expressions are formulated explicitly: rotor thrust and blade pitch angle, drag
torque and blade pitch angle. A finite-time convergent observer based on
Lyapunov function is developed to reconstruct the unknown variables and
uncertainties during mode transitions. The merits of this design include the
modeling of dynamic large-size tilt-rotor, ease of the uncertainties estimation
during the tilting and the widely applications. Moreover, a switched logic
controller based on the finite-time convergent observer is proposed to drive
the aircraft to implement the mode transition with invariant flying height.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06786</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06786</id><created>2015-05-25</created><authors><author><keyname>Croft</keyname><forenames>William Lee</forenames></author><author><keyname>Shi</keyname><forenames>Wei</forenames></author><author><keyname>Sack</keyname><forenames>Jorg-Rudiger</forenames></author><author><keyname>Corriveau</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Geographic Partitioning Techniques for the Anonymization of Health Care
  Data</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hospitals and health care organizations collect large amounts of detailed
health care data that is in high demand by researchers. Thus, the possessors of
such data are in need of methods that allow for this data to be released
without compromising the confidentiality of the individuals to whom it
pertains. As the geographic aspect of this data is becoming increasingly
relevant for research being conducted, it is important for an
\emph{anonymization} process to pay due attention to the geographic attributes
of such data. In this paper, a novel system for health care data anonymization
is presented. At the core of the system is the aggregation of an initial
regionalization guided by the use of a Voronoi diagram. We conduct a comparison
with another geographic-based system of anonymization, GeoLeader. We show that
our system is capable of producing results of a comparable quality with a much
faster running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06788</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06788</id><created>2015-05-25</created><authors><author><keyname>Ruan</keyname><forenames>H.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Low-Complexity Robust Adaptive Beamforming Algorithms Based on Shrinkage
  for Mismatch Estimation</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures, WSA. arXiv admin note: text overlap with
  arXiv:1311.2331</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose low-complexity robust adaptive beamforming (RAB)
techniques that based on shrinkage methods. The only prior knowledge required
by the proposed algorithms are the angular sector in which the actual steering
vector is located and the antenna array geometry. We firstly present a
Low-Complexity Shrinkage-Based Mismatch Estimation (LOCSME) algorithm to
estimate the desired signal steering vector mismatch, in which the
interference-plus-noise covariance (INC) matrix is estimated with Oracle
Approximating Shrinkage (OAS) method and the weights are computed with matrix
inversions. We then develop low-cost stochastic gradient (SG) recursions to
estimate the INC matrix and update the beamforming weights, resulting in the
proposed LOCSME-SG algorithm. Simulation results show that both LOCSME and
LOCSME-SG achieve very good output signal-to-interference-plus-noise ratio
(SINR) compared to previously reported adaptive RAB algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06791</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06791</id><created>2015-05-25</created><authors><author><keyname>Toole</keyname><forenames>Jameson L.</forenames></author><author><keyname>Lin</keyname><forenames>Yu-Ru</forenames></author><author><keyname>Muehlegger</keyname><forenames>Erich</forenames></author><author><keyname>Shoag</keyname><forenames>Daniel</forenames></author><author><keyname>Gonzalez</keyname><forenames>Marta C.</forenames></author><author><keyname>Lazer</keyname><forenames>David</forenames></author></authors><title>Tracking Employment Shocks Using Mobile Phone Data</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can data from mobile phones be used to observe economic shocks and their
consequences at multiple scales? Here we present novel methods to detect mass
layoffs, identify individuals affected by them, and predict changes in
aggregate unemployment rates using call detail records (CDRs) from mobile
phones. Using the closure of a large manufacturing plant as a case study, we
first describe a structural break model to correctly detect the date of a mass
layoff and estimate its size. We then use a Bayesian classification model to
identify affected individuals by observing changes in calling behavior
following the plant's closure. For these affected individuals, we observe
significant declines in social behavior and mobility following job loss. Using
the features identified at the micro level, we show that the same changes in
these calling behaviors, aggregated at the regional level, can improve
forecasts of macro unemployment rates. These methods and results highlight
promise of new data resources to measure micro economic behavior and improve
estimates of critical economic indicators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06792</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06792</id><created>2015-05-25</created><authors><author><keyname>Pienta</keyname><forenames>Robert</forenames></author><author><keyname>Lin</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Kahng</keyname><forenames>Minsuk</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author><author><keyname>Talukdar</keyname><forenames>Partha P.</forenames></author><author><keyname>Abello</keyname><forenames>James</forenames></author><author><keyname>Parameswaran</keyname><forenames>Ganesh</forenames></author><author><keyname>Chau</keyname><forenames>Duen Horng</forenames></author></authors><title>Seeing the Forest through the Trees: Adaptive Local Exploration of Large
  Graphs</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visualization is a powerful paradigm for exploratory data analysis.
Visualizing large graphs, however, often results in a meaningless hairball. In
this paper, we propose a different approach that helps the user adaptively
explore large million-node graphs from a local perspective. For nodes that the
user investigates, we propose to only show the neighbors with the most
subjectively interesting neighborhoods. We contribute novel ideas to measure
this interestingness in terms of how surprising a neighborhood is given the
background distribution, as well as how well it fits the nodes the user chose
to explore. We introduce FACETS, a fast and scalable method for visually
exploring large graphs. By implementing our above ideas, it allows users to
look into the forest through its trees. Empirical evaluation shows that our
method works very well in practice, providing rankings of nodes that match
interests of users. Moreover, as it scales linearly, FACETS is suited for the
exploration of very large graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06795</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06795</id><created>2015-05-25</created><authors><author><keyname>Karianakis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Dong</keyname><forenames>Jingming</forenames></author><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author></authors><title>How Well Can a CNN Marginalize Simple Nuisances It is Designed for?</title><categories>cs.CV cs.LG cs.NE</categories><comments>19 pages, 12 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct an empirical study to test the ability of convolutional neural
networks (CNNs) to reduce the effects of nuisance transformations of the input
data, such as location, scale and aspect ratio. We isolate factors by adopting
a common convolutional architecture either deployed globally on the image to
compute class posterior distributions, or restricted locally to compute class
conditional distributions given location, scale and aspect ratios of bounding
boxes determined by proposal heuristics. As explained in the paper, averaging
the latter should in principle yield performance inferior to proper
marginalization. Empirical tests yield the converse, however, leading us to
conclude that - at the current level of complexity of convolutional
architectures and scale of the data sets used to train them - CNNs are not very
effective at marginalizing nuisance variability. We also quantify the effects
of context on the overall classification task and its impact on the performance
of CNNs, and propose improved sampling techniques for heuristic proposal
schemes that improve end-to-end performance to state-of-the-art levels. We test
our hypothesis on classification task using the ImageNet Challenge benchmark,
on detection and on wide-baseline matching using the Oxford and Fischer
matching datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06798</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06798</id><created>2015-05-25</created><updated>2015-11-18</updated><authors><author><keyname>Zhang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Zou</keyname><forenames>Jianhua</forenames></author><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Accelerating Very Deep Convolutional Networks for Classification and
  Detection</title><categories>cs.CV cs.LG cs.NE</categories><comments>TPAMI, accepted. arXiv admin note: substantial text overlap with
  arXiv:1411.4229</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to accelerate the test-time computation of convolutional
neural networks (CNNs), especially very deep CNNs that have substantially
impacted the computer vision community. Unlike previous methods that are
designed for approximating linear filters or linear responses, our method takes
the nonlinear units into account. We develop an effective solution to the
resulting nonlinear optimization problem without the need of stochastic
gradient descent (SGD). More importantly, while previous methods mainly focus
on optimizing one or two layers, our nonlinear method enables an asymmetric
reconstruction that reduces the rapidly accumulated error when multiple (e.g.,
&gt;=10) layers are approximated. For the widely used very deep VGG-16 model, our
method achieves a whole-model speedup of 4x with merely a 0.3% increase of
top-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also
shows a graceful accuracy degradation for object detection when plugged into
the Fast R-CNN detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06800</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06800</id><created>2015-05-25</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Baochang</forenames></author></authors><title>Boosting-like Deep Learning For Pedestrian Detection</title><categories>cs.CV cs.LG cs.NE</categories><comments>9 pages,7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper proposes boosting-like deep learning (BDL) framework for
pedestrian detection. Due to overtraining on the limited training samples,
overfitting is a major problem of deep learning. We incorporate a boosting-like
technique into deep learning to weigh the training samples, and thus prevent
overtraining in the iterative process. We theoretically give the details of
derivation of our algorithm, and report the experimental results on open data
sets showing that BDL achieves a better stable performance than the
state-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the
average miss rate compared with ACF and JointDeep on the largest Caltech
benchmark dataset, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06802</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06802</id><created>2015-05-26</created><updated>2015-08-12</updated><authors><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Pan</forenames></author><author><keyname>Liu</keyname><forenames>Tian</forenames></author><author><keyname>Gong</keyname><forenames>Fuzhou</forenames></author></authors><title>Solution space structure of random constraint satisfaction problems with
  growing domains</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>8 pages, 1 figures</comments><doi>10.1088/1742-5468/2015/12/P12006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the solution space structure of model RB, a standard
prototype of Constraint Satisfaction Problem (CSPs) with growing domains. Using
rigorous the first and the second moment method, we show that in the solvable
phase close to the satisfiability transition, solutions are clustered into
exponential number of well-separated clusters, with each cluster contains
sub-exponential number of solutions. As a consequence, the system has a
clustering (dynamical) transition but no condensation transition. This picture
of phase diagram is different from other classic random CSPs with fixed domain
size, such as random K-Satisfiability (K-SAT) and graph coloring problems,
where condensation transition exists and is distinct from satisfiability
transition. Our result verifies the non-rigorous results obtained using cavity
method from spin glass theory, and sheds light on the structures of solution
spaces of problems with a large number of states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06807</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06807</id><created>2015-05-26</created><authors><author><keyname>Meng</keyname><forenames>Xiangrui</forenames></author><author><keyname>Bradley</keyname><forenames>Joseph</forenames></author><author><keyname>Yavuz</keyname><forenames>Burak</forenames></author><author><keyname>Sparks</keyname><forenames>Evan</forenames></author><author><keyname>Venkataraman</keyname><forenames>Shivaram</forenames></author><author><keyname>Liu</keyname><forenames>Davies</forenames></author><author><keyname>Freeman</keyname><forenames>Jeremy</forenames></author><author><keyname>Tsai</keyname><forenames>DB</forenames></author><author><keyname>Amde</keyname><forenames>Manish</forenames></author><author><keyname>Owen</keyname><forenames>Sean</forenames></author><author><keyname>Xin</keyname><forenames>Doris</forenames></author><author><keyname>Xin</keyname><forenames>Reynold</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Zadeh</keyname><forenames>Reza</forenames></author><author><keyname>Zaharia</keyname><forenames>Matei</forenames></author><author><keyname>Talwalkar</keyname><forenames>Ameet</forenames></author></authors><title>MLlib: Machine Learning in Apache Spark</title><categories>cs.LG cs.DC cs.MS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Apache Spark is a popular open-source platform for large-scale data
processing that is well-suited for iterative machine learning tasks. In this
paper we present MLlib, Spark's open-source distributed machine learning
library. MLlib provides efficient functionality for a wide range of learning
settings and includes several underlying statistical, optimization, and linear
algebra primitives. Shipped with Spark, MLlib supports several languages and
provides a high-level API that leverages Spark's rich ecosystem to simplify the
development of end-to-end machine learning pipelines. MLlib has experienced a
rapid growth due to its vibrant open-source community of over 140 contributors,
and includes extensive documentation to support further growth and to let users
quickly get up to speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06810</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06810</id><created>2015-05-26</created><authors><author><keyname>Zamani</keyname><forenames>Mohsen</forenames></author><author><keyname>Ninness</keyname><forenames>Brett</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel</forenames></author></authors><title>On the Reachability of Networked Systems</title><categories>cs.SY</categories><comments>The paper is submitted for possible publication in a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study networks of discrete-time linear time-invariant
subsystems. Our focus is on situations where subsystems are connected to each
other through a time-invariant topology and where there exists a base-station
whose aim is to control the subsystems into any desired destinations. However,
the base-station can only communicate with some of the subsystems that we refer
to as leaders. There are no direct links between the base-station and the rest
of subsystems, known as followers, as they are only able to liaise among
themselves and with some of the leaders.
  The current paper formulates this framework as the well-known reachability
problem for linear systems. Then to address this problem, we introduce notions
of leader-reachability and base-reachability. We present algebraic conditions
under which these notions hold. It turns out that if subsystems are represented
by minimal state space representations, then base-reachability always holds.
Hence, we focus on leader-reachability and investigate the corresponding
conditions in detail. We further demonstrate that when the networked system
parameters i.e. subsystems' parameters and interconnection matrices, assume
generic values then the whole network is both leader-reachable and
base-reachable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06811</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06811</id><created>2015-05-26</created><authors><author><keyname>Yu</keyname><forenames>Huacheng</forenames></author></authors><title>An Improved Combinatorial Algorithm for Boolean Matrix Multiplication</title><categories>cs.DS</categories><comments>A preliminary version of this paper appeared in ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new combinatorial algorithm for triangle finding and Boolean
matrix multiplication that runs in $\hat{O}(n^3/\log^4 n)$ time, where the
$\hat{O}$ notation suppresses poly(loglog) factors. This improves the previous
best combinatorial algorithm by Chan that runs in $\hat{O}(n^3/\log^3 n)$ time.
Our algorithm generalizes the divide-and-conquer strategy of Chan's algorithm.
Moreover, we propose a general framework for detecting triangles in graphs and
computing Boolean matrix multiplication. Roughly speaking, if we can find the
&quot;easy parts&quot; of a given instance efficiently, we can solve the whole problem
faster than $n^3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06812</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06812</id><created>2015-05-26</created><authors><author><keyname>Narasimhan</keyname><forenames>Harikrishna</forenames></author><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author></authors><title>Optimizing Non-decomposable Performance Measures: A Tale of Two Classes</title><categories>stat.ML cs.LG</categories><comments>To appear in proceedings of the 32nd International Conference on
  Machine Learning (ICML 2015)</comments><journal-ref>Journal of Machine Learning Research, W&amp;CP 37 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern classification problems frequently present mild to severe label
imbalance as well as specific requirements on classification characteristics,
and require optimizing performance measures that are non-decomposable over the
dataset, such as F-measure. Such measures have spurred much interest and pose
specific challenges to learning algorithms since their non-additive nature
precludes a direct application of well-studied large scale optimization methods
such as stochastic gradient descent.
  In this paper we reveal that for two large families of performance measures
that can be expressed as functions of true positive/negative rates, it is
indeed possible to implement point stochastic updates. The families we consider
are concave and pseudo-linear functions of TPR, TNR which cover several
popularly used performance measures such as F-measure, G-mean and H-mean.
  Our core contribution is an adaptive linearization scheme for these families,
using which we develop optimization techniques that enable truly point-based
stochastic updates. For concave performance measures we propose SPADE, a
stochastic primal dual solver; for pseudo-linear measures we propose STAMP, a
stochastic alternate maximization procedure. Both methods have crisp
convergence guarantees, demonstrate significant speedups over existing methods
- often by an order of magnitude or more, and give similar or more accurate
predictions on test data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06813</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06813</id><created>2015-05-26</created><authors><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author><author><keyname>Narasimhan</keyname><forenames>Harikrishna</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author></authors><title>Surrogate Functions for Maximizing Precision at the Top</title><categories>stat.ML cs.LG</categories><comments>To appear in the the proceedings of the 32nd International Conference
  on Machine Learning (ICML 2015)</comments><journal-ref>Journal of Machine Learning Research, W&amp;CP 37 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maximizing precision at the top of a ranked list, often dubbed
Precision@k (prec@k), finds relevance in myriad learning applications such as
ranking, multi-label classification, and learning with severe label imbalance.
However, despite its popularity, there exist significant gaps in our
understanding of this problem and its associated performance measure.
  The most notable of these is the lack of a convex upper bounding surrogate
for prec@k. We also lack scalable perceptron and stochastic gradient descent
algorithms for optimizing this performance measure. In this paper we make key
contributions in these directions. At the heart of our results is a family of
truly upper bounding surrogates for prec@k. These surrogates are motivated in a
principled manner and enjoy attractive properties such as consistency to prec@k
under various natural margin/noise conditions.
  These surrogates are then used to design a class of novel perceptron
algorithms for optimizing prec@k with provable mistake bounds. We also devise
scalable stochastic gradient descent style methods for this problem with
provable convergence bounds. Our proofs rely on novel uniform convergence
bounds which require an in-depth analysis of the structural properties of
prec@k and its surrogates. We conclude with experimental results comparing our
algorithms with state-of-the-art cutting plane and stochastic gradient
algorithms for maximizing prec@k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06814</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06814</id><created>2015-05-26</created><authors><author><keyname>Palmieri</keyname><forenames>Francesco A. N.</forenames></author><author><keyname>Buonanno</keyname><forenames>Amedeo</forenames></author></authors><title>Discrete Independent Component Analysis (DICA) with Belief Propagation</title><categories>cs.CV cs.LG stat.ML</categories><comments>Sumbitted for publication (May 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply belief propagation to a Bayesian bipartite graph composed of
discrete independent hidden variables and discrete visible variables. The
network is the Discrete counterpart of Independent Component Analysis (DICA)
and it is manipulated in a factor graph form for inference and learning. A full
set of simulations is reported for character images from the MNIST dataset. The
results show that the factorial code implemented by the sources contributes to
build a good generative model for the data that can be used in various
inference modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06815</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06815</id><created>2015-05-26</created><authors><author><keyname>Talla</keyname><forenames>Vamsi</forenames></author><author><keyname>Kellogg</keyname><forenames>Bryce</forenames></author><author><keyname>Ransford</keyname><forenames>Benjamin</forenames></author><author><keyname>Naderiparizi</keyname><forenames>Saman</forenames></author><author><keyname>Gollakota</keyname><forenames>Shyamnath</forenames></author><author><keyname>Smith</keyname><forenames>Joshua R.</forenames></author></authors><title>Powering the Next Billion Devices with Wi-Fi</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first power over Wi-Fi system that delivers power and works
with existing Wi-Fi chipsets. Specifically, we show that a ubiquitous piece of
wireless communication infrastructure, the Wi-Fi router, can provide far field
wireless power without compromising the network's communication performance.
Building on our design we prototype, for the first time, battery-free
temperature and camera sensors that are powered using Wi-Fi chipsets with
ranges of 20 and 17 feet respectively. We also demonstrate the ability to
wirelessly recharge nickel-metal hydride and lithium-ion coin-cell batteries at
distances of up to 28 feet. Finally, we deploy our system in six homes in a
metropolitan area and show that our design can successfully deliver power via
Wi-Fi in real-world network conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06816</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06816</id><created>2015-05-26</created><updated>2016-02-22</updated><authors><author><keyname>Beltagy</keyname><forenames>I.</forenames></author><author><keyname>Roller</keyname><forenames>Stephen</forenames></author><author><keyname>Cheng</keyname><forenames>Pengxiang</forenames></author><author><keyname>Erk</keyname><forenames>Katrin</forenames></author><author><keyname>Mooney</keyname><forenames>Raymond J.</forenames></author></authors><title>Representing Meaning with a Combination of Logical Form and Vectors</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NLP tasks differ in the semantic information they require, and at this time
no single se- mantic representation fulfills all requirements. Logic-based
representations characterize sentence structure, but do not capture the graded
aspect of meaning. Distributional models give graded similarity ratings for
words and phrases, but do not capture sentence structure in the same detail as
logic-based approaches. So it has been argued that the two are complementary.
We adopt a hybrid approach that combines logic-based and distributional
semantics through probabilistic logic inference in Markov Logic Networks
(MLNs). In this paper, we focus on the three components of a practical system
integrating logical and distributional models: 1) Parsing and task
representation is the logic-based part where input problems are represented in
probabilistic logic. This is quite different from representing them in standard
first-order logic. 2) For knowledge base construction we form weighted
inference rules. We integrate and compare distributional information with other
sources, notably WordNet and an existing paraphrase collection. In particular,
we use our system to evaluate distributional lexical entailment approaches. We
use a variant of Robinson resolution to determine the necessary inference
rules. More sources can easily be added by mapping them to logical rules; our
system learns a resource-specific weight that corrects for scaling differences
between resources. 3) In discussing probabilistic inference, we show how to
solve the inference problems efficiently. To evaluate our approach, we use the
task of textual entailment (RTE), which can utilize the strengths of both
logic-based and distributional representations. In particular we focus on the
SICK dataset, where we achieve state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06818</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06818</id><created>2015-05-26</created><authors><author><keyname>Middeldorp</keyname><forenames>Aart</forenames><affiliation>University of Innsbruck</affiliation></author><author><keyname>van Raamsdonk</keyname><forenames>Femke</forenames><affiliation>VU University Amsterdam</affiliation></author></authors><title>Proceedings 8th International Workshop on Computing with Terms and
  Graphs</title><categories>cs.LO cs.CC</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 183, 2015</journal-ref><doi>10.4204/EPTCS.183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the post-proceedings of the 8th International Workshop
on Computing with Terms and Graphs (TERMGRAPH 2014). The workshop took place in
Vienna on July 13, 2014 and was affiliated with the joint RTA and TLCA
conference, which was part of the Federated Logic Conference (FLoC), which in
turn participated in the Vienna Summer of Logic (VSL) 2014.
  The four regular papers in these proceedings are significantly extended
versions of their pre-proceedings version. They were subjected to an additional
round of reviewing. The paper by Samuel Mimram is an invited contribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06819</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06819</id><created>2015-05-26</created><updated>2015-05-27</updated><authors><author><keyname>Urabe</keyname><forenames>Natsuki</forenames></author><author><keyname>Hasuo</keyname><forenames>Ichiro</forenames></author></authors><title>Coalgebraic Infinite Traces and Kleisli Simulations</title><categories>cs.LO</categories><comments>35 pages, 1 figure; corrected a typo in page 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kleisli simulation is a categorical notion introduced by Hasuo to verify
finite trace inclusion. They allow us to give definitions of forward and
backward simulation for various types of systems. A generic categorical theory
behind Kleisli simulation has been developed and it guarantees the soundness of
those simulations wrt. finite trace semantics. Moreover, those simulations can
be aided by forward partial execution (FPE)---a categorical transformation of
systems previously introduced by the authors.
  In this paper, we give Kleisli simulation a theoretical foundation that
assures its soundness also wrt. infinite trace. There, following Jacobs' work,
infinite trace semantics is characterized as the &quot;largest homomorphism.&quot; It
turns out that soundness of forward simulations is rather straightforward; that
of backward simulation holds too, although it requires certain additional
conditions and its proof is more involved. We also show that FPE can be
successfully employed in the infinite trace setting to enhance the
applicability of Kleisli simulations as witnesses of trace inclusion. Our
framework is parameterized in the monad for branching as well as in the functor
for linear-time behaviors; for the former we use the powerset monad (for
nondeterminism) as well as the sub-Giry monad (for probability).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06821</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06821</id><created>2015-05-26</created><authors><author><keyname>Chen</keyname><forenames>Shi-Zhe</forenames></author><author><keyname>Guo</keyname><forenames>Chun-Chao</forenames></author><author><keyname>Lai</keyname><forenames>Jian-Huang</forenames></author></authors><title>Deep Ranking for Person Re-identification via Joint Representation
  Learning</title><categories>cs.CV</categories><comments>13 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel approach to person re-identification, a
fundamental task in distributed multi-camera surveillance systems. Although a
variety of powerful algorithms have been presented in the past few years, most
of them usually focus on designing hand-crafted features and learning metrics
either individually or sequentially. Different from previous works, we
formulate a unified deep ranking framework that jointly tackles both of these
key components to maximize their strengths. We start from the principle that
the correct match of the probe image should be positioned in the top rank
within the whole gallery set. An effective learning-to-rank algorithm is
proposed to minimize the cost corresponding to the ranking disorders of the
gallery. The ranking model is solved with a deep Convolutional Neural Network
(CNN) that builds the relation between input image pairs and their similarity
scores through joint representation learning directly from raw image pixels.
The proposed framework allows us to get rid of feature engineering and does not
rely on any assumption. An extensive comparative evaluation is given,
demonstrating that our approach significantly outperforms all state-of-the-art
approaches, including both traditional and CNN-based methods on the challenging
VIPeR and CUHK-01 datasets. Additionally, our approach has better ability to
generalize across datasets without fine-tuning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06828</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06828</id><created>2015-05-26</created><authors><author><keyname>Geitner</keyname><forenames>Gert-Helge</forenames></author><author><keyname>Komurgoz</keyname><forenames>Guven</forenames></author></authors><title>Power Flow Modelling of Dynamic Systems - Introduction to Modern
  Teaching Tools</title><categories>cs.SY math.DS physics.ed-ph</categories><comments>12 pages, 9 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As tools for dynamic system modelling both conventional methods such as
transfer function or state space representation and modern power flow based
methods are available. The latter methods do not depend on energy domain, are
able to preserve physical system structures, visualize power conversion or
coupling or split, identify power losses or storage, run on conventional
software and emphasize the relevance of energy as basic principle of known
physical domains. Nevertheless common control structures as well as analysis
and design tools may still be applied. Furthermore the generalization of power
flow methods as pseudo-power flow provides with a universal tool for any
dynamic modelling. The phenomenon of power flow constitutes an up to date
education methodology. Thus the paper summarizes fundamentals of selected power
flow oriented modelling methods, presents a Bond Graph block library for
teaching power oriented modelling as compact menu-driven freeware, introduces
selected examples and discusses special features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06836</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06836</id><created>2015-05-26</created><authors><author><keyname>Xing</keyname><forenames>Luyi</forenames></author><author><keyname>Bai</keyname><forenames>Xiaolong</forenames></author><author><keyname>Li</keyname><forenames>Tongxin</forenames></author><author><keyname>Wang</keyname><forenames>XiaoFeng</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Liao</keyname><forenames>Xiaojing</forenames></author><author><keyname>Hu</keyname><forenames>Shi-Min</forenames></author><author><keyname>Han</keyname><forenames>Xinhui</forenames></author></authors><title>Unauthorized Cross-App Resource Access on MAC OS X and iOS</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On modern operating systems, applications under the same user are separated
from each other, for the purpose of protecting them against malware and
compromised programs. Given the complexity of today's OSes, less clear is
whether such isolation is effective against different kind of cross-app
resource access attacks (called XARA in our research). To better understand the
problem, on the less-studied Apple platforms, we conducted a systematic
security analysis on MAC OS~X and iOS. Our research leads to the discovery of a
series of high-impact security weaknesses, which enable a sandboxed malicious
app, approved by the Apple Stores, to gain unauthorized access to other apps'
sensitive data. More specifically, we found that the inter-app interaction
services, including the keychain, WebSocket and NSConnection on OS~X and URL
Scheme on the MAC OS and iOS, can all be exploited by the malware to steal such
confidential information as the passwords for iCloud, email and bank, and the
secret token of Evernote. Further, the design of the app sandbox on OS~X was
found to be vulnerable, exposing an app's private directory to the sandboxed
malware that hijacks its Apple Bundle ID. As a result, sensitive user data,
like the notes and user contacts under Evernote and photos under WeChat, have
all been disclosed. Fundamentally, these problems are caused by the lack of
app-to-app and app-to-OS authentications. To better understand their impacts,
we developed a scanner that automatically analyzes the binaries of MAC OS and
iOS apps to determine whether proper protection is missing in their code.
Running it on hundreds of binaries, we confirmed the pervasiveness of the
weaknesses among high-impact Apple apps. Since the issues may not be easily
fixed, we built a simple program that detects exploit attempts on OS~X, helping
protect vulnerable apps before the problems can be fully addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06841</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06841</id><created>2015-05-26</created><authors><author><keyname>Malek-Mohammadi</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Koochakzadeh</keyname><forenames>Ali</forenames></author><author><keyname>Babaie-Zadeh</keyname><forenames>Massoud</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author></authors><title>Successive Concave Sparsity Approximation: Near-Oracle Performance in a
  Wide Range of Sparsity Levels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, based on a successively accuracy-increasing approximation of
the $\ell_0$ norm, we propose a new algorithm for recovery of sparse vectors
from underdetermined measurements. The approximations are realized with a
certain class of concave functions that aggressively induce sparsity and their
closeness to the $\ell_0$ norm can be controlled. We prove that the series of
the approximations asymptotically coincides with the $\ell_1$ and $\ell_0$
norms when the approximation accuracy changes from the worst fitting to the
best fitting. When measurements are noise-free, an optimization scheme is
proposed which leads to a number of weighted $\ell_1$ minimization programs,
whereas, in the presence of noise, we propose two iterative thresholding
methods that are computationally appealing. A convergence guarantee for the
iterative thresholding method is provided, and, for a particular function in
the class of the approximating functions, we derive the closed-form
thresholding operator. We further present some theoretical analyses via the
restricted isometry, null space, and spherical section properties. Our
extensive numerical simulations indicate that the proposed algorithm closely
follows the performance of the oracle estimator for a range of sparsity levels
wider than those of the state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06842</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06842</id><created>2015-05-26</created><authors><author><keyname>Jha</keyname><forenames>Ranjan</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Rouillier</keyname><forenames>Fabrice</forenames><affiliation>LIP6</affiliation></author><author><keyname>Moroz</keyname><forenames>Guillaume</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>An algebraic method to check the singularity-free paths for parallel
  robots</title><categories>cs.RO</categories><comments>Appears in International Design Engineering Technical Conferences &amp;
  Computers and Information in Engineering Conference , Aug 2015, Boston,
  United States. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trajectory planning is a critical step while programming the parallel
manipulators in a robotic cell. The main problem arises when there exists a
singular configuration between the two poses of the end-effectors while
discretizing the path with a classical approach. This paper presents an
algebraic method to check the feasibility of any given trajectories in the
workspace. The solutions of the polynomial equations associated with the
tra-jectories are projected in the joint space using Gr{\&quot;o}bner based
elimination methods and the remaining equations are expressed in a parametric
form where the articular variables are functions of time t unlike any numerical
or discretization method. These formal computations allow to write the Jacobian
of the manip-ulator as a function of time and to check if its determinant can
vanish between two poses. Another benefit of this approach is to use a largest
workspace with a more complex shape than a cube, cylinder or sphere. For the
Orthoglide, a three degrees of freedom parallel robot, three different
trajectories are used to illustrate this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06845</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06845</id><created>2015-05-26</created><authors><author><keyname>Caro</keyname><forenames>S</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Lemoine</keyname><forenames>P</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Wenger</keyname><forenames>P</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Kinematic Analysis and Trajectory Planning of the Orthoglide 5-axis</title><categories>cs.RO</categories><comments>Appears in International Design Engineering Technical Conferences \&amp;
  Computers and Information in Engineering Conference, Aug 2015, Boston, United
  States. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The subject of this paper is about the kinematic analysis and the trajectory
planning of the Orthoglide 5-axis. The Orthoglide 5-axis a five degrees of
freedom parallel kinematic machine developed at IRCCyN and is made up of a
hybrid architecture, namely, a three degrees of freedom translational parallel
manip-ulator mounted in series with a two degrees of freedom parallel spherical
wrist. The simpler the kinematic modeling of the Or-thoglide 5-axis, the higher
the maximum frequency of its control loop. Indeed, the control loop of a
parallel kinematic machine should be computed with a high frequency, i.e.,
higher than 1.5 MHz, in order the manipulator to be able to reach high speed
motions with a good accuracy. Accordingly, the direct and inverse kinematic
models of the Orthoglide 5-axis, its inverse kine-matic Jacobian matrix and the
first derivative of the latter with respect to time are expressed in this
paper. It appears that the kinematic model of the manipulator under study can
be written in a quadratic form due to the hybrid architecture of the Orthoglide
5-axis. As illustrative examples, the profiles of the actuated joint angles
(lengths), velocities and accelerations that are used in the control loop of
the robot are traced for two test trajectories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06848</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06848</id><created>2015-05-26</created><authors><author><keyname>Zaghloul</keyname><forenames>Mofreh R.</forenames></author></authors><title>Remark on &quot;Algorithm 916: Computing the Faddeyeva and Voigt functions&quot;:
  Efficiency Improvements and Fortran Translation</title><categories>astro-ph.IM cs.MS</categories><comments>11 pages, 5 tables, Under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This remark describes efficiency improvements to Algorithm 916 [Zaghloul and
Ali 2011]. It is shown that the execution time required by the algorithm, when
run at its highest accuracy, may be improved by more than a factor of two. A
better accuracy vs efficiency trade off scheme is also implemented; this
requires the user to supply the number of significant figures desired in the
computed values as an extra input argument to the function. Using this
trade-off, it is shown that the efficiency of the algorithm may be further
improved significantly while maintaining reasonably accurate and safe results
that are free of the pitfalls and complete loss of accuracy seen in other
competitive techniques. The current version of the code is provided in Matlab
and Scilab in addition to a Fortran translation prepared to meet the needs of
real-world problems where very large numbers of function evaluations would
require the use of a compiled language. To fulfill this last requirement, a
recently proposed reformed version of Humlicek's w4 routine, shown to maintain
the claimed accuracy of the algorithm over a wide and fine grid is implemented
in the present Fortran translation for the case of 4 significant figures. This
latter modification assures the reliability of the code to be employed in the
solution of practical problems requiring numerous evaluation of the function
for applications tolerating low accuracy computations (&lt;10-4).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06850</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06850</id><created>2015-05-26</created><authors><author><keyname>Corneli</keyname><forenames>Joseph</forenames></author><author><keyname>Jordanous</keyname><forenames>Anna</forenames></author></authors><title>Implementing feedback in creative systems: A workshop approach</title><categories>cs.AI</categories><comments>8 pp., submitted to IJCAI 2015 Workshop 42, &quot;AI and Feedback&quot;</comments><acm-class>I.2.11</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  One particular challenge in AI is the computational modelling and simulation
of creativity. Feedback and learning from experience are key aspects of the
creative process. Here we investigate how we could implement feedback in
creative systems using a social model. From the field of creative writing we
borrow the concept of a Writers Workshop as a model for learning through
feedback. The Writers Workshop encourages examination, discussion and debates
of a piece of creative work using a prescribed format of activities. We propose
a computational model of the Writers Workshop as a roadmap for incorporation of
feedback in artificial creativity systems. We argue that the Writers Workshop
setting describes the anatomy of the creative process. We support our claim
with a case study that describes how to implement the Writers Workshop model in
a computational creativity system. We present this work using patterns other
people can follow to implement similar designs in their own systems. We
conclude by discussing the broader relevance of this model to other aspects of
AI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06851</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06851</id><created>2015-05-26</created><authors><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Schifanella</keyname><forenames>Rossano</forenames></author><author><keyname>Aiello</keyname><forenames>Luca Maria</forenames></author><author><keyname>McLean</keyname><forenames>Kate</forenames></author></authors><title>Smelly Maps: The Digital Life of Urban Smellscapes</title><categories>cs.SI cs.CY</categories><comments>11 pages, 7 figures, Proceedings of 9th International AAAI Conference
  on Web and Social Media (ICWSM2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smell has a huge influence over how we perceive places. Despite its
importance, smell has been crucially overlooked by urban planners and
scientists alike, not least because it is difficult to record and analyze at
scale. One of the authors of this paper has ventured out in the urban world and
conducted smellwalks in a variety of cities: participants were exposed to a
range of different smellscapes and asked to record their experiences. As a
result, smell-related words have been collected and classified, creating the
first dictionary for urban smell. Here we explore the possibility of using
social media data to reliably map the smells of entire cities. To this end, for
both Barcelona and London, we collect geo-referenced picture tags from Flickr
and Instagram, and geo-referenced tweets from Twitter. We match those tags and
tweets with the words in the smell dictionary. We find that smell-related words
are best classified in ten categories. We also find that specific categories
(e.g., industry, transport, cleaning) correlate with governmental air quality
indicators, adding validity to our study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06855</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06855</id><created>2015-05-26</created><authors><author><keyname>Cho</keyname><forenames>Byungjin</forenames></author><author><keyname>Koufos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Ruttik</keyname><forenames>Kalle</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author></authors><title>Modeling the Interference Generated from Car Base Stations towards
  Indoor Femto-cells</title><categories>cs.SY cs.NI</categories><comments>6 pages, 4 figures, 3rd IFAC Symposium on Telematics Applications</comments><journal-ref>Proc. of the 3rd IFAC Symposium on Telematics Applications, pp.
  96-101, Nov, 2013</journal-ref><doi>10.3182/20131111-3-KR-2043.00030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In future wireless networks, a significant number of users will be vehicular.
One promising solution to improve the capacity for these vehicular users is to
employ moving relays or car base stations. The system forms cell inside the
vehicle and then uses rooftop antenna for back-hauling to overcome the
vehicular penetration loss. In this paper, we develop a model for aggregate
interference distribution generated from moving/parked cars to indoor users in
order to study whether indoor femto-cells can coexist on the same spectrum with
vehicular communications. Since spectrum authorization for vehicular
communications is open at moment, we consider two spectrum sharing scenarios
(i) communication from mounted antennas on the roof of the vehicles to the
infrastructure network utilizes same spectrum with indoor femto-cells (ii)
in-vehicle communication utilizes same spectrum with indoor femto-cells while
vehicular to infrastructure (V2I) communication is allocated at different
spectrum. Based on our findings we suggest that V2I and indoor femto-cells
should be allocated at different spectrum. The reason being that mounted
roof-top antennas facing the indoor cells generate unacceptable interference
levels. On the other hand, in-vehicle communication and indoor cells can share
the spectrum thanks to the vehicle body isolation and the lower transmit power
levels that can be used inside the vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06856</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06856</id><created>2015-05-26</created><authors><author><keyname>Bethanabhotla</keyname><forenames>Dilip</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>WiFlix: Adaptive Video Streaming in Massive MU-MIMO Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>30 pages. arXiv admin note: text overlap with arXiv:1304.8083</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of simultaneous on-demand streaming of stored video
to multiple users in a multi-cell wireless network where multiple unicast
streaming sessions are run in parallel and share the same frequency band. Each
streaming session is formed by the sequential transmission of video &quot;chunks,&quot;
such that each chunk arrives into the corresponding user playback buffer within
its playback deadline. We formulate the problem as a Network Utility
Maximization (NUM) where the objective is to fairly maximize users' video
streaming Quality of Experience (QoE) and then derive an iterative control
policy using Lyapunov Optimization, which solves the NUM problem up to any
level of accuracy and yields an online protocol with control actions at every
iteration decomposing into two layers interconnected by the users' request
queues : i) a video streaming adaptation layer reminiscent of DASH, implemented
at each user node; ii) a transmission scheduling layer where a max-weight
scheduler is implemented at each base station. The proposed chunk request
scheme is a pull strategy where every user opportunistically requests video
chunks from the neighboring base stations and dynamically adapts the quality of
its requests based on the current size of the request queue. For the
transmission scheduling component, we first describe the general max-weight
scheduler and then particularize it to a wireless network where the base
stations have multiuser MIMO (MU-MIMO) beamforming capabilities. We exploit the
channel hardening effect of large-dimensional MIMO channels (massive MIMO) and
devise a low complexity user selection scheme to solve the underlying
combinatorial problem of selecting user subsets for downlink beamforming, which
can be easily implemented and run independently at each base station.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06862</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06862</id><created>2015-05-26</created><updated>2015-09-16</updated><authors><author><keyname>Finkbeiner</keyname><forenames>Bernd</forenames><affiliation>Saarland University</affiliation></author><author><keyname>Tentrup</keyname><forenames>Leander</forenames><affiliation>Saarland University</affiliation></author></authors><title>Detecting Unrealizability of Distributed Fault-tolerant Systems</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:12) 2015</journal-ref><doi>10.2168/LMCS-11(3:12)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Writing formal specifications for distributed systems is difficult. Even
simple consistency requirements often turn out to be unrealizable because of
the complicated information flow in the distributed system: not all information
is available in every component, and information transmitted from other
components may arrive with a delay or not at all, especially in the presence of
faults. The problem of checking the distributed realizability of a temporal
specification is, in general, undecidable. Semi-algorithms for synthesis, such
as bounded synthesis, are only useful in the positive case, where they
construct an implementation for a realizable specification, but not in the
negative case: if the specification is unrealizable, the search for the
implementation never terminates. In this paper, we introduce counterexamples to
distributed realizability and present a method for the detection of such
counterexamples for specifications given in linear-time temporal logic (LTL). A
counterexample consists of a set of paths, each representing a different
sequence of inputs from the environment, such that, no matter how the
components are implemented, the specification is violated on at least one of
these paths. We present a method for finding such counterexamples both for the
classic distributed realizability problem and for the fault-tolerant
realizability problem. Our method considers, incrementally, larger and larger
sets of paths until a counterexample is found. For safety specifications in
weakly ordered architectures we obtain a decision procedure, while
counterexamples for full LTL and arbitrary architectures may consist of
infinitely many paths. Experimental results, obtained with a QBF-based
prototype implementation, show that our method finds simple errors very
quickly, and even problems with high combinatorial complexity, like the
Byzantine Generals' Problem, are tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06865</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06865</id><created>2015-05-26</created><authors><author><keyname>Bonomi</keyname><forenames>Silvia</forenames><affiliation>MIDLAB</affiliation></author><author><keyname>Del Pozzo</keyname><forenames>Antonella</forenames><affiliation>MIDLAB, NPA</affiliation></author><author><keyname>Potop-Butucaru</keyname><forenames>Maria</forenames><affiliation>NPA</affiliation></author></authors><title>Tight Mobile Byzantine Tolerant Atomic Storage</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the first implementation of an atomic storage tolerant to
mobile Byzantine agents. Our implementation is designed for the round-based
synchronous model where the set of Byzantine nodes changes from round to round.
In this model we explore the feasibility of multi-writer multi-reader atomic
register prone to various mobile Byzantine behaviors. We prove upper and lower
bounds for solving the atomic storage in all the explored models. Our results,
significantly different from the static case, advocate for a deeper study of
the main building blocks of distributed computing while the system is prone to
mobile Byzantine failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06872</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06872</id><created>2015-05-26</created><authors><author><keyname>Gao</keyname><forenames>Wanling</forenames></author><author><keyname>Luo</keyname><forenames>Chunjie</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Ye</keyname><forenames>Hainan</forenames></author><author><keyname>He</keyname><forenames>Xiwen</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhu</keyname><forenames>Yuqing</forenames></author><author><keyname>Tian</keyname><forenames>Xinhui</forenames></author></authors><title>Identifying Dwarfs Workloads in Big Data Analytics</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data benchmarking is particularly important and provides applicable
yardsticks for evaluating booming big data systems. However, wide coverage and
great complexity of big data computing impose big challenges on big data
benchmarking. How can we construct a benchmark suite using a minimum set of
units of computation to represent diversity of big data analytics workloads?
Big data dwarfs are abstractions of extracting frequently appearing operations
in big data computing. One dwarf represents one unit of computation, and big
data workloads are decomposed into one or more dwarfs. Furthermore, dwarfs
workloads rather than vast real workloads are more cost-efficient and
representative to evaluate big data systems. In this paper, we extensively
investigate six most important or emerging application domains i.e. search
engine, social network, e-commerce, multimedia, bioinformatics and astronomy.
After analyzing forty representative algorithms, we single out eight dwarfs
workloads in big data analytics other than OLAP, which are linear algebra,
sampling, logic operations, transform operations, set operations, graph
operations, statistic operations and sort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06877</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06877</id><created>2015-05-26</created><updated>2015-11-10</updated><authors><author><keyname>Tan</keyname><forenames>Onur</forenames></author><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author><author><keyname>Vilardebo</keyname><forenames>Jesus Gomez</forenames></author></authors><title>Linear Transmission of Composite Gaussian Measurements over a Fading
  Channel under Delay Constraints</title><categories>cs.IT math.IT</categories><comments>31 pages, 7 figures. Revised for IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay constrained linear transmission (LT) strategies are considered for the
transmission of composite Gaussian measurements over an additive white Gaussian
noise fading channel under an average power constraint. If the channel state
information (CSI) is known by both the encoder and decoder, the optimal LT
scheme in terms of the average mean-square error distortion is characterized
under a strict delay constraint, and a graphical interpretation of the optimal
power allocation strategy is presented. Then, for general delay constraints,
two LT strategies are proposed based on the solution to a particular multiple
measurements-parallel channels scenario. It is shown that the distortion
decreases as the delay constraint is relaxed, and when the delay constraint is
completely removed, both strategies achieve the optimal performance under
certain matching conditions. If the CSI is known only by the decoder, the
optimal LT strategy is derived under a strict delay constraint. The extension
for general delay constraints is shown to be hard. As a first step towards
understanding the structure of the optimal scheme in this case, it is shown
that for the multiple measurements-parallel channels scenario, any LT scheme
that uses only a one-to-one linear mapping between measurements and channels is
suboptimal in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06878</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06878</id><created>2015-05-26</created><authors><author><keyname>Fatimah</keyname><forenames>Binish</forenames></author><author><keyname>Joshi</keyname><forenames>Shiv Dutt</forenames></author></authors><title>Computationally efficient MIMO system identification using Signal
  Matched Synthesis Filter Bank</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a multi input multi output(MIMO) system identification framework
by interpreting the MIMO system in terms of a multirate synthesis filter bank.
The proposed methodology is discussed in two steps: in the first step the MIMO
system is interpreted as a synthesis filter bank and the second step is to
convert the MIMO system into a SISO system &quot;without any loss of information&quot;,
which re-structures the system identification problem into a SISO form. The
system identification problem, in its new form, is identical to the problem of
obtaining the signal matched synthesis filter bank (SMSFB) as proposed in Part
II. Since we have developed fast algorithms to obtain the filter bank
coefficients in Part II, for &quot;the given data case&quot; as well as &quot;the given
statistics case&quot;, we can use these algorithm for the MIMO system identification
as well. This framework can have an adaptive as well as block processing
implementation. The algorithms, used here, involve only scalar computations,
unlike the conventional MIMO system identification algorithms where one
requires matrix computations. These order recursive algorithm can also be used
to obtain approximate smaller order model for large order systems without using
any model order reduction algorithm. The proposed identification framework can
also be used for SISO LPTV system identification and also for a SIMO or MISO
system. The efficacy of the proposed scheme is validated and its performance in
the presence of measurement noise is illustrated using simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06882</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06882</id><created>2015-05-26</created><authors><author><keyname>Chen</keyname><forenames>Yi</forenames></author><author><keyname>Sung</keyname><forenames>Chi Wan</forenames></author></authors><title>Characterization of SINR Region for Multiple Interfering Multicast in
  Power-Controlled Systems</title><categories>cs.IT math.IT</categories><comments>25 pages, 4 figures, submitted to IEEE Trans. Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a wireless communication network consisting of multiple
interfering multicast sessions. Different from a unicast system where each
transmitter has only one receiver, in a multicast system, each transmitter has
multiple receivers. It is a well known result for wireless unicast systems that
the feasibility of an signal-to-interference-plus-noise power ratio (SINR)
without power constraint is decided by the Perron-Frobenius eigenvalue of a
nonnegative matrix. We generalize this result and propose necessary and
sufficient conditions for the feasibility of an SINR in a wireless multicast
system with and without power constraint. The feasible SINR region as well as
its geometric properties are studied. Besides, an iterative algorithm is
proposed which can efficiently check the feasibility condition and compute the
boundary points of the feasible SINR region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06893</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06893</id><created>2015-05-26</created><updated>2016-02-24</updated><authors><author><keyname>Kasperski</keyname><forenames>Adam</forenames></author><author><keyname>Zielinski</keyname><forenames>Pawel</forenames></author></authors><title>Robust recoverable and two-stage selection problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the following selection problem is discussed. A set of $n$
items is given and we wish to choose a subset of exactly $p$ items of the
minimum total cost. This problem is a special case of 0-1 knapsack in which all
the item weights are equal to~1. Its deterministic version has a trivial
$O(n)$-time algorithm, which consists in choosing $p$ items of the smallest
costs. In this paper it is assumed that the item costs are uncertain. Two
robust models, namely two-stage and recoverable ones, under discrete and
interval uncertainty representations, are discussed. Several positive and
negative complexity results for both of them are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06895</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06895</id><created>2015-05-26</created><updated>2015-12-18</updated><authors><author><keyname>Danezis</keyname><forenames>George</forenames></author><author><keyname>Meiklejohn</keyname><forenames>Sarah</forenames></author></authors><title>Centrally Banked Cryptocurrencies</title><categories>cs.CR</categories><comments>15 pages, 4 figures, 2 tables in Proceedings of NDSS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current cryptocurrencies, starting with Bitcoin, build a decentralized
blockchain-based transaction ledger, maintained through proofs-of-work that
also generate a monetary supply. Such decentralization has benefits, such as
independence from national political control, but also significant limitations
in terms of scalability and computational cost. We introduce RSCoin, a
cryptocurrency framework in which central banks maintain complete control over
the monetary supply, but rely on a distributed set of authorities, or
mintettes, to prevent double-spending. While monetary policy is centralized,
RSCoin still provides strong transparency and auditability guarantees. We
demonstrate, both theoretically and experimentally, the benefits of a modest
degree of centralization, such as the elimination of wasteful hashing and a
scalable system for avoiding double-spending attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06896</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06896</id><created>2015-05-26</created><updated>2015-09-02</updated><authors><author><keyname>Stra&#xc3;?burger</keyname><forenames>Lutz</forenames><affiliation>INRIA</affiliation></author><author><keyname>Das</keyname><forenames>Anupam</forenames><affiliation>INRIA</affiliation></author><author><keyname>Arisaka</keyname><forenames>Ryuta</forenames><affiliation>INRIA</affiliation></author></authors><title>On Nested Sequents for Constructive Modal Logics</title><categories>cs.LO</categories><comments>33 pages</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:7) 2015</journal-ref><doi>10.2168/LMCS-11(3:7)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present deductive systems for various modal logics that can be obtained
from the constructive variant of the normal modal logic CK by adding
combinations of the axioms d, t, b, 4, and 5. This includes the constructive
variants of the standard modal logics K4, S4, and S5. We use for our
presentation the formalism of nested sequents and give a syntactic proof of cut
elimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06897</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06897</id><created>2015-05-26</created><updated>2015-06-09</updated><authors><author><keyname>Marteau</keyname><forenames>Pierre-Fran&#xe7;ois</forenames><affiliation>IRISA</affiliation></author></authors><title>Times series averaging from a probabilistic interpretation of
  time-elastic kernel</title><categories>cs.LG cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the light of regularized dynamic time warping kernels, this paper
reconsider the concept of time elastic centroid (TEC) for a set of time series.
From this perspective, we show first how TEC can easily be addressed as a
preimage problem. Unfortunately this preimage problem is ill-posed, may suffer
from over-fitting especially for long time series and getting a sub-optimal
solution involves heavy computational costs. We then derive two new algorithms
based on a probabilistic interpretation of kernel alignment matrices that
expresses in terms of probabilistic distributions over sets of alignment paths.
The first algorithm is an iterative agglomerative heuristics inspired from the
state of the art DTW barycenter averaging (DBA) algorithm proposed specifically
for the Dynamic Time Warping measure. The second proposed algorithm achieves a
classical averaging of the aligned samples but also implements an averaging of
the time of occurrences of the aligned samples. It exploits a straightforward
progressive agglomerative heuristics. An experimentation that compares for 45
time series datasets classification error rates obtained by first near
neighbors classifiers exploiting a single medoid or centroid estimate to
represent each categories show that: i) centroids based approaches
significantly outperform medoids based approaches, ii) on the considered
experience, the two proposed algorithms outperform the state of the art DBA
algorithm, and iii) the second proposed algorithm that implements an averaging
jointly in the sample space and along the time axes emerges as the most
significantly robust time elastic averaging heuristic with an interesting noise
reduction capability. Index Terms-Time series averaging Time elastic kernel
Dynamic Time Warping Time series clustering and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06907</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06907</id><created>2015-05-26</created><authors><author><keyname>Gr&#xfc;nauer</keyname><forenames>Andreas</forenames></author><author><keyname>Vincze</keyname><forenames>Markus</forenames></author></authors><title>Using Dimension Reduction to Improve the Classification of
  High-dimensional Data</title><categories>cs.LG cs.CV</categories><comments>Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</comments><report-no>OAGM/2015/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we show that the classification performance of high-dimensional
structural MRI data with only a small set of training examples is improved by
the usage of dimension reduction methods. We assessed two different dimension
reduction variants: feature selection by ANOVA F-test and feature
transformation by PCA. On the reduced datasets, we applied common learning
algorithms using 5-fold cross-validation. Training, tuning of the
hyperparameters, as well as the performance evaluation of the classifiers was
conducted using two different performance measures: Accuracy, and Receiver
Operating Characteristic curve (AUC). Our hypothesis is supported by
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06915</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06915</id><created>2015-05-26</created><authors><author><keyname>Vervier</keyname><forenames>K&#xe9;vin</forenames><affiliation>CBIO</affiliation></author><author><keyname>Mah&#xe9;</keyname><forenames>Pierre</forenames><affiliation>CBIO</affiliation></author><author><keyname>Tournoud</keyname><forenames>Maud</forenames><affiliation>CBIO</affiliation></author><author><keyname>Veyrieras</keyname><forenames>Jean-Baptiste</forenames><affiliation>CBIO</affiliation></author><author><keyname>Vert</keyname><forenames>Jean-Philippe</forenames><affiliation>CBIO</affiliation></author></authors><title>Large-scale Machine Learning for Metagenomics Sequence Classification</title><categories>q-bio.QM cs.CE cs.LG q-bio.GN stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metagenomics characterizes the taxonomic diversity of microbial communities
by sequencing DNA directly from an environmental sample. One of the main
challenges in metagenomics data analysis is the binning step, where each
sequenced read is assigned to a taxonomic clade. Due to the large volume of
metagenomics datasets, binning methods need fast and accurate algorithms that
can operate with reasonable computing requirements. While standard
alignment-based methods provide state-of-the-art performance, compositional
approaches that assign a taxonomic class to a DNA read based on the k-mers it
contains have the potential to provide faster solutions. In this work, we
investigate the potential of modern, large-scale machine learning
implementations for taxonomic affectation of next-generation sequencing reads
based on their k-mers profile. We show that machine learning-based
compositional approaches benefit from increasing the number of fragments
sampled from reference genome to tune their parameters, up to a coverage of
about 10, and from increasing the k-mer size to about 12. Tuning these models
involves training a machine learning model on about 10 8 samples in 10 7
dimensions, which is out of reach of standard soft-wares but can be done
efficiently with modern implementations for large-scale machine learning. The
resulting models are competitive in terms of accuracy with well-established
alignment tools for problems involving a small to moderate number of candidate
species, and for reasonable amounts of sequencing errors. We show, however,
that compositional approaches are still limited in their ability to deal with
problems involving a greater number of species, and more sensitive to
sequencing errors. We finally confirm that compositional approach achieve
faster prediction times, with a gain of 3 to 15 times with respect to the
BWA-MEM short read mapper, depending on the number of candidate species and the
level of sequencing noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06918</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06918</id><created>2015-05-26</created><authors><author><keyname>Lutz</keyname><forenames>Roman</forenames></author></authors><title>Fantasy Football Prediction</title><categories>cs.LG</categories><comments>class project, 7 pages (1 sources, 1 appendix)</comments><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The ubiquity of professional sports and specifically the NFL have lead to an
increase in popularity for Fantasy Football. Users have many tools at their
disposal: statistics, predictions, rankings of experts and even recommendations
of peers. There are issues with all of these, though. Especially since many
people pay money to play, the prediction tools should be enhanced as they
provide unbiased and easy-to-use assistance for users. This paper provides and
discusses approaches to predict Fantasy Football scores of Quarterbacks with
relatively limited data. In addition to that, it includes several suggestions
on how the data could be enhanced to achieve better results. The dataset
consists only of game data from the last six NFL seasons. I used two different
methods to predict the Fantasy Football scores of NFL players: Support Vector
Regression (SVR) and Neural Networks. The results of both are promising given
the limited data that was used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06926</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06926</id><created>2015-05-26</created><authors><author><keyname>Gliwa</keyname><forenames>Bogdan</forenames></author><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author></authors><title>Finding Influential Bloggers</title><categories>cs.SI</categories><journal-ref>International Journal of Machine Learning and Computing, Vol.5,
  No.2, April 2015</journal-ref><doi>10.7763/IJMLC.2015.V5.495</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blogging is a popular way of expressing opinions and discussing topics.
Bloggers demonstrate different levels of commitment and most interesting are
influential bloggers. Around such bloggers, the groups are forming, which
concentrate users sharing similar interests. Finding such bloggers is an
important task and has many applications e.g. marketing, business, politics.
Influential ones affect others which is related to the process of diffusion.
However, there is no objective way to telling which blogger is more
influential. Therefore, researchers take into consideration different criteria
to assess bloggers (e.g. SNA centrality measures). In this paper we propose
new, efficient method for influential bloggers discovery which is based on
relation of commenting in blogger's thread and is defined on bloggers level.
Next, we compare results with other, comparative method proposed by Agarwal et
al. called iFinder which is based on links between posts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06929</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06929</id><created>2015-05-26</created><authors><author><keyname>Chach&#xf3;lski</keyname><forenames>Wojciech</forenames></author><author><keyname>Lundman</keyname><forenames>Anders</forenames></author><author><keyname>Ramanujam</keyname><forenames>Ryan</forenames></author><author><keyname>Scolamiero</keyname><forenames>Martina</forenames></author><author><keyname>&#xd6;berg</keyname><forenames>Sebastian</forenames></author></authors><title>Multidimensional Persistence and Noise</title><categories>math.AT cs.CG math.AC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a notion of noise in the category of tame and
compact functors. Noise induces a pseudo-metric topology on such functors,
which can be used to identify persistent features in the context of
multidimensional persistence. We also define the basic barcode: an invariant
for tame and compact functors. When comparing basic barcodes through
interleavings, the association of a basic barcode to a tame and compact functor
is proven to be $1$-Lipschitz. For standard noise and $1$-dimensional
persistence, the basic barcode identifies the persistent features of the
classical barcode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06939</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06939</id><created>2015-05-26</created><authors><author><keyname>Croft</keyname><forenames>William Lee</forenames></author><author><keyname>Shi</keyname><forenames>Wei</forenames></author><author><keyname>Sack</keyname><forenames>Jorg-Rudiger</forenames></author><author><keyname>Corriveau</keyname><forenames>Jean-Pierre</forenames></author></authors><title>A Novel Geographic Partitioning System for Anonymizing Health Care Data</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With large volumes of detailed health care data being collected, there is a
high demand for the release of this data for research purposes. Hospitals and
organizations are faced with conflicting interests of releasing this data and
protecting the confidentiality of the individuals to whom the data pertains.
Similarly, there is a conflict in the need to release precise geographic
information for certain research applications and the requirement to censor or
generalize the same information for the sake of confidentiality. Ultimately the
challenge is to anonymize data in order to comply with government privacy
policies while reducing the loss in geographic information as much as possible.
In this paper, we present a novel geographic-based system for the anonymization
of health care data. This system is broken up into major components for which
different approaches may be supplied. We compare such approaches in order to
make recommendations on which of them to select to best match user
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06953</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06953</id><created>2015-05-26</created><updated>2016-01-14</updated><authors><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Parameterized Linear Temporal Logics Meet Costs: Still not Costlier than
  LTL (full version)</title><categories>cs.LO cs.FL</categories><comments>A short version appears in Proceedings of GandALF 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the investigation of parameterized extensions of Linear Temporal
Logic (LTL) that retain the attractive algorithmic properties of LTL: a
polynomial space model checking algorithm and a doubly-exponential time
algorithm for solving games. Alur et al. and Kupferman et al. showed that this
is the case for Parametric LTL (PLTL) and PROMPT-LTL respectively, which have
temporal operators equipped with variables that bound their scope in time.
Later, this was also shown to be true for Parametric LDL (PLDL), which extends
PLTL to be able to express all omega-regular properties.
  Here, we generalize PLTL to systems with costs, i.e., we do not bound the
scope of operators in time, but bound the scope in terms of the cost
accumulated during time. Again, we show that model checking and solving games
for specifications in PLTL with costs is not harder than the corresponding
problems for LTL. Finally, we discuss PLDL with costs and extensions to
multiple cost functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06955</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06955</id><created>2015-05-22</created><authors><author><keyname>Wei</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Yunjia</forenames></author><author><keyname>Wang</keyname><forenames>Ting</forenames></author><author><keyname>Li</keyname><forenames>Baifeng</forenames></author><author><keyname>Niu</keyname><forenames>Baolong</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiming</forenames></author></authors><title>Research on Solution Space of Bipartite Graph Vertex-Cover by Maximum
  Matchings</title><categories>cs.DS</categories><comments>19 pages, 8 figures</comments><doi>10.1088/1742-5468/2015/11/P11027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some rigorous results and statistics of the solution space of Vertex-Covers
on bipartite graphs are given in this paper. Based on the $K\ddot{o}nig$'s
theorem, an exact solution space expression algorithm is proposed and
statistical analysis of the nodes' states is provided. The statistical results
fit well with the algorithmic results until the emergence of the unfrozen core,
which makes the fluctuation of statistical quantities and causes the replica
symmetric breaking in the solutions. Besides, the entropy of bipartite
Vertex-Cover solutions is calculated with the clustering entropy using a cycle
simplification technique for the unfrozen core. Furthermore, as generalization
of bipartite graphs, bipartite core graph is proposed, the solution space of
which can also be easily determined; and based on these results, how to
generate a $K\ddot{o}nig-Egerv\acute{a}ry$ subgraph is studied by a growth
process of adding edges. The investigation of solution space of bipartite graph
Vertex-Cover provides intensive understanding and some insights on the solution
space complexity, and will produce benefit for finding maximal
$K\ddot{o}nig-Egerv\acute{a}ry$ subgraphs, solving general graph Vertex-Cover
and recognizing the intrinsic hardness of NP-complete problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06957</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06957</id><created>2015-05-26</created><authors><author><keyname>Casalino</keyname><forenames>Gabriella</forenames></author><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author></authors><title>Sequential Dimensionality Reduction for Extracting Localized Features</title><categories>cs.CV cs.LG cs.NA math.NA stat.ML</categories><comments>21 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear dimensionality reduction techniques are powerful tools for image
analysis as they allow the identification of important features in a data set.
In particular, nonnegative matrix factorization (NMF) has become very popular
as it is able to extract sparse, localized and easily interpretable features by
imposing an additive combination of nonnegative basis elements. Nonnegative
matrix underapproximation (NMU) is a closely related technique that has the
advantage to identify features sequentially. In this paper, we propose a
variant of NMU that is particularly well suited for image analysis as it
incorporates the spatial information, that is, it takes into account the fact
that neighboring pixels are more likely to be contained in the same features,
and favors the extraction of localized features by looking for sparse basis
elements. We show that our new approach competes favorably with comparable
state-of-the-art techniques on several facial and hyperspectral image data
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06967</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06967</id><created>2015-05-14</created><authors><author><keyname>Coakley</keyname><forenames>Michael</forenames></author><author><keyname>Crocetti</keyname><forenames>Giancarlo</forenames></author><author><keyname>Dressner</keyname><forenames>Phil</forenames></author><author><keyname>Kellum</keyname><forenames>Wanda</forenames></author><author><keyname>Lamin</keyname><forenames>Tamba</forenames></author></authors><title>Transforming Telemedicine Through Big Data Analytics</title><categories>cs.CY</categories><comments>2 pages. In the Proceedings of 12th Annual Research Day, 2014 - Pace
  University</comments><acm-class>K.4.1; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A look at how big data is transforming telemedicine to provide better care by
tapping into a larger source of patient information. Telemedicine will have a
profound impact on patient care, increase access and quality, and represent an
opportunity to keep health care costs down. Data generated by smart devices
will enable the real-time monitoring of chronic diseases, allowing optimal
dosage of drugs and improve patient outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06973</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06973</id><created>2015-05-26</created><updated>2015-09-11</updated><authors><author><keyname>Keuper</keyname><forenames>Margret</forenames></author><author><keyname>Levinkov</keyname><forenames>Evgeny</forenames></author><author><keyname>Bonneel</keyname><forenames>Nicolas</forenames></author><author><keyname>Lavou&#xe9;</keyname><forenames>Guillaume</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author><author><keyname>Andres</keyname><forenames>Bjoern</forenames></author></authors><title>Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formulations of the Image Decomposition Problem as a Multicut Problem (MP)
w.r.t. a superpixel graph have received considerable attention. In contrast,
instances of the MP w.r.t. a pixel grid graph have received little attention,
firstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph are
hard to solve in practice, and, secondly, due to the lack of long-range terms
in the objective function of the MP. We propose a generalization of the MP with
long-range terms (LMP). We design and implement two efficient algorithms
(primal feasible heuristics) for the MP and LMP which allow us to study
instances of both problems w.r.t. the pixel grid graphs of the images in the
BSDS-500 benchmark. The decompositions we obtain do not differ significantly
from the state of the art, suggesting that the LMP is a competitive formulation
of the Image Decomposition Problem. To demonstrate the generality of the LMP,
we apply it also to the Mesh Decomposition Problem posed by the Princeton
benchmark, obtaining state-of-the-art decompositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06979</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06979</id><created>2015-05-26</created><authors><author><keyname>Yerokhin</keyname><forenames>Vadim</forenames></author><author><keyname>Shehu</keyname><forenames>Andi</forenames></author><author><keyname>Feldman</keyname><forenames>Edgar</forenames></author><author><keyname>Bagan</keyname><forenames>Emilio</forenames></author><author><keyname>Bergou</keyname><forenames>Janos A.</forenames></author></authors><title>Probabilistically Perfect Cloning of Two Pure States: A Geometric
  Approach</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the long-standing problem of making n perfect clones from m copies
of one of two known pure states with minimum failure probability in the general
case where the known states have arbitrary a priori probabilities. The solution
emerges from a geometric formulation of the problem. This formulation also
reveals a deeper connection between cloning and state discrimination. The
convergence of cloning to state discrimination as the number of clones goes to
infinity exhibits a phenomenon analogous to a second order symmetry breaking
phase transition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06982</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06982</id><created>2015-05-25</created><authors><author><keyname>Clearwater</keyname><forenames>Adam</forenames></author><author><keyname>Puppe</keyname><forenames>Clemens</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>Generalizing the Single-Crossing Property on Lines and Trees to
  Intermediate Preferences on Median Graphs</title><categories>cs.GT math.CO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1410.2272</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demange (2012) generalized the classical single-crossing property to the
intermediate property on median graphs and proved that the representative voter
theorem still holds for this more general framework. We complement her result
with proving that the linear orders of any profile which is intermediate on a
median graph form a Condorcet domain. We prove that for any median graph there
exists a profile that is intermediate with respect to that graph and that one
may need at least as many alternatives as vertices to construct such a profile.
We provide a polynomial-time algorithm to recognize whether or not a given
profile is intermediate with respect to some median graph. Finally, we show
that finding winners for the Chamberlin-Courant rule is polynomial-time
solvable for profiles that are single-crossing on a tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06984</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06984</id><created>2015-05-25</created><authors><author><keyname>Cs&#xf3;ka</keyname><forenames>Endre</forenames></author></authors><title>Limits of some combinatorial problems</title><categories>cs.GT cs.DM math.CO math.PR</categories><comments>Extended abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show some new examples how limit theory can help understanding other
combinatorial structures. First, we generalize the Manickam--Mikl\'os--Singhi
Conjecture, using limit theory. Then we introduce two limit problems of
Alpern's Caching Game, which are good approximations of the finite game when
some parameters tend to infinity. With the use of these limit problems, we show
a surprising result which disproves some conjectures about the finite problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.06999</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.06999</id><created>2015-05-26</created><authors><author><keyname>Belanich</keyname><forenames>Joshua</forenames></author><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author></authors><title>Some Open Problems in Optimal AdaBoost and Decision Stumps</title><categories>cs.LG stat.ML</categories><comments>4 pages, rejected from COLT15 Open Problems May 19, 2015 (submitted
  April 21, 2015; original 3 pages in COLT-conference format)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The significance of the study of the theoretical and practical properties of
AdaBoost is unquestionable, given its simplicity, wide practical use, and
effectiveness on real-world datasets. Here we present a few open problems
regarding the behavior of &quot;Optimal AdaBoost,&quot; a term coined by Rudin,
Daubechies, and Schapire in 2004 to label the simple version of the standard
AdaBoost algorithm in which the weak learner that AdaBoost uses always outputs
the weak classifier with lowest weighted error among the respective hypothesis
class of weak classifiers implicit in the weak learner. We concentrate on the
standard, &quot;vanilla&quot; version of Optimal AdaBoost for binary classification that
results from using an exponential-loss upper bound on the misclassification
training error. We present two types of open problems. One deals with general
weak hypotheses. The other deals with the particular case of decision stumps,
as often and commonly used in practice. Answers to the open problems can have
immediate significant impact to (1) cementing previously established results on
asymptotic convergence properties of Optimal AdaBoost, for finite datasets,
which in turn can be the start to any convergence-rate analysis; (2)
understanding the weak-hypotheses class of effective decision stumps generated
from data, which we have empirically observed to be significantly smaller than
the typically obtained class, as well as the effect on the weak learner's
running time and previously established improved bounds on the generalization
performance of Optimal AdaBoost classifiers; and (3) shedding some light on the
&quot;self control&quot; that AdaBoost tends to exhibit in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07002</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07002</id><created>2015-05-26</created><updated>2015-12-23</updated><authors><author><keyname>Martinez</keyname><forenames>Matias</forenames></author><author><keyname>Durieux</keyname><forenames>Thomas</forenames></author><author><keyname>Xuan</keyname><forenames>Jifeng</forenames></author><author><keyname>Sommerard</keyname><forenames>Romain</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>Automatic Repair of Real Bugs: An Experience Report on the Defects4J
  Dataset</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defects4J is a large, peer-reviewed, structured dataset of real-world Java
bugs. Each bug in Defects4J is provided with a test suite and at least one
failing test case that triggers the bug. In this paper, we report on an
experiment to explore the effectiveness of automatic repair on Defects4J. The
result of our experiment shows that 47 bugs of the Defects4J dataset can be
automatically repaired by state-of- the-art repair. This sets a baseline for
future research on automatic repair for Java. We have manually analyzed 84
different patches to assess their real correctness. In total, 9 real Java bugs
can be correctly fixed with test-suite based repair. This analysis shows that
test-suite based repair suffers from under-specified bugs, for which trivial
and incorrect patches still pass the test suite. With respect to practical
applicability, it takes in average 14.8 minutes to find a patch. The experiment
was done on a scientific grid, totaling 17.6 days of computation time. All
their systems and experimental results are publicly available on Github in
order to facilitate future research on automatic repair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07008</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07008</id><created>2015-05-26</created><authors><author><keyname>Wei</keyname><forenames>Tianwen</forenames></author></authors><title>An Overview of the Asymptotic Performance of the Family of the FastICA
  Algorithms</title><categories>stat.ML cs.LG</categories><comments>To appear in the 12th International Conference on Latent Variable
  Analysis and Source Separation (LVA/ICA 2015), Liberec, Czech Republic</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution summarizes the results on the asymptotic performance of
several variants of the FastICA algorithm. A number of new closed-form
expressions are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07020</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07020</id><created>2015-05-26</created><updated>2016-03-08</updated><authors><author><keyname>Wong</keyname><forenames>Vincent</forenames></author><author><keyname>Cooney</keyname><forenames>Daniel</forenames></author><author><keyname>Bar-Yam</keyname><forenames>Yaneer</forenames></author></authors><title>Beyond Contact Tracing: Community-Based Early Detection for Ebola
  Response</title><categories>physics.soc-ph cs.CY cs.SI physics.med-ph q-bio.PE</categories><comments>29 pages, 9 figures</comments><report-no>New England Complex Systems Institute (NECSI) Report 2016-03-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 2014 Ebola outbreak in west Africa raised many questions about the
control of infectious disease in an increasingly connected global society.
Limited availability of contact information made contact tracing difficult or
impractical in combating the outbreak. We consider the development of
multi-scale public health strategies and simulate policies for community-level
response aimed at early screening of communities rather than individuals, as
well as travel restrictions to prevent community cross-contamination. Our
analysis shows the policies to be effective even at a relatively low level of
compliance. In our simulations, 40% of individuals conforming to these policies
is enough to stop the outbreak. Simulations with a 50% compliance rate are
consistent with the case counts in Liberia during the period of rapid decline
after mid September, 2014. We also find the travel restriction to be effective
at reducing the risks associated with compliance substantially below the 40%
level, shortening the outbreak and enabling efforts to be focused on affected
areas. Our results suggest that the multi-scale approach can be used to further
evolve public health strategy for defeating emerging epidemics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07032</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07032</id><created>2015-05-26</created><updated>2015-06-25</updated><authors><author><keyname>Pandolfi</keyname><forenames>Luciano</forenames></author><author><keyname>Triulzi</keyname><forenames>Daniele</forenames></author></authors><title>Regularity of the steering control for systems with persistent memory</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The following fact is known for large classes of distributed control systems:
when the target is regular, there exists a regular steering control. This fact
is important to prove convergence estimates of numerical algorithms for the
approximate computation of the steering control.
  In this paper we extend this property to a class of systems with persistent
memory (of Maxwell/Boltzmann type) and we give a variational characterization
of the smooth steering control which may open the way to an extension of the
numerical approach proposed by Ervedoza and Zuazua.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07038</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07038</id><created>2015-05-26</created><updated>2015-09-15</updated><authors><author><keyname>Deng</keyname><forenames>Zhun</forenames></author><author><keyname>Ding</keyname><forenames>Jie</forenames></author><author><keyname>Noshad</keyname><forenames>Mohammad</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>The Number of Independent Sets in Hexagonal Graphs</title><categories>cs.IT math-ph math.IT math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method is proposed to derive rigorous bounds on {\eta}, the growth rate
of the logarithm of the number of independent sets on a hexagonal lattice.
Specifically, we prove that 1.546440708536001 &lt;= {\eta} &lt;= 1.5513, which
improves upon the best known 1.5463 &lt;= {\eta} &lt;= 1.5527 due to Nagy and Zeger.
Our lower bound matches the numerical estimate of Baxter up to 9 digits after
the decimal point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07050</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07050</id><created>2015-05-26</created><authors><author><keyname>Mathews</keyname><forenames>Nithin</forenames></author><author><keyname>Christensen</keyname><forenames>Anders Lyhne</forenames></author><author><keyname>O'Grady</keyname><forenames>Rehan</forenames></author><author><keyname>Dorigo</keyname><forenames>Marco</forenames></author></authors><title>Virtual Nervous Systems for Self-Assembling Robots - A preliminary
  report</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the nervous system of a robot as the processing unit responsible
for controlling the robot body, together with the links between the processing
unit and the sensorimotor hardware of the robot - i.e., the equivalent of the
central nervous system in biological organisms. We present autonomous robots
that can merge their nervous systems when they physically connect to each
other, creating a &quot;virtual nervous system&quot; (VNS). We show that robots with a
VNS have capabilities beyond those found in any existing robotic system or
biological organism: they can merge into larger bodies with a single brain
(i.e., processing unit), split into separate bodies with independent brains,
and temporarily acquire sensing and actuating capabilities of specialized peer
robots. VNS-based robots can also self-heal by removing or replacing
malfunctioning body parts, including the brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07054</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07054</id><created>2015-04-30</created><authors><author><keyname>Galeazzi</keyname><forenames>Paolo</forenames></author><author><keyname>Franke</keyname><forenames>Michael</forenames></author></authors><title>Smart Transformations: The Evolution of Choice Principles</title><categories>cs.GT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Evolutionary game theory classically investigates which behavioral patterns
are evolutionarily successful in a single game. More recently, a number of
contributions have studied the evolution of preferences instead: which
subjective conceptualizations of a game's payoffs give rise to evolutionarily
successful behavior in a single game. Here, we want to extend this existing
approach even further by asking: which general patterns of subjective
conceptualizations of payoff functions are evolutionarily successful across a
class of games. In other words, we will look at evolutionary competition of
payoff transformations in &quot;meta-games&quot;, obtained from averaging over payoffs of
single games. Focusing for a start on the class of 2x2 symmetric games, we show
that regret minimization can outperform payoff maximization if agents resort to
a security strategy in case of radical uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07056</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07056</id><created>2015-05-20</created><updated>2015-08-18</updated><authors><author><keyname>Duda</keyname><forenames>Jarek</forenames></author></authors><title>Joint error correction enhancement of the fountain codes concept</title><categories>cs.IT math.IT</categories><comments>14 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fountain codes like LT or Raptor codes, also known as rateless erasure codes,
allow to encode a message as some number of packets, such that any large enough
subset of these packets is sufficient to fully reconstruct the message. It
requires undamaged packets, while the packets which were not lost are usually
damaged in real scenarios. Hence, an additional error correction layer is often
required: adding some level of redundancy to each packet to be able to repair
eventual damages. This approach requires a priori knowledge of the final damage
level of every packet - insufficient redundancy leads to packet loss,
overprotection means suboptimal channel rate. However, the sender may have
inaccurate or even no a priori information about the final damage levels, for
example in applications like broadcasting, degradation of a storage medium or
damage of picture watermarking.
  Joint Reconstruction Codes (JRC) setting is introduced and discussed in this
paper for the purpose of removing the need of a priori knowledge of damage
level and sub-optimality caused by overprotection and discarding underprotected
packets. It is obtained by combining both processes: reconstruction from
multiple packets and forward error correction. The decoder combines the
resultant informational content of all received packets accordingly to their
actual noise level, which can be estimated a posteriori individually for each
packet. Assuming binary symmetric channel (BSC) of $\epsilon$ bit-flip
probability, every potentially damaged bit carries
$R_0(\epsilon)=1-h_1(\epsilon)$ bits of information, where $h_1$ is the Shannon
entropy. The minimal requirement to fully reconstruct the message is that the
sum of rate $R_0(\epsilon)$ over all bits is at least the size of the message.
We will discuss sequential decoding for the reconstruction purpose, which
statistical behavior can be estimated using Renyi entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07062</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07062</id><created>2015-05-26</created><authors><author><keyname>Braham</keyname><forenames>Hajer</forenames></author><author><keyname>Jemaa</keyname><forenames>Sana Ben</forenames></author><author><keyname>Fort</keyname><forenames>Gersende</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author><author><keyname>Sayrac</keyname><forenames>Berna</forenames></author></authors><title>Fixed Rank Kriging for Cellular Coverage Analysis</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coverage planning and optimization is one of the most crucial tasks for a
radio network operator. Efficient coverage optimization requires accurate
coverage estimation which relies on geo-located field measurements. These
measurements are gathered today during highly expensive drive tests and will be
reported in the near future by users equipments thanks to the 3GPP MDT feature
(still costly in terms of battery consumption and signaling overhead). In both
cases, predicting the coverage on a location where no measurements are
available remains a key and challenging task. This paper describes a powerful
tool that gives an accurate coverage prediction on the whole area of interest,
i.e. a coverage map, by spatially interpolating geo-located measurements using
Kriging technique. The paper focuses on the reduction of the computational
complexity of the kriging algorithm by applying Fixed Rank Kriging (FRK). The
performance evaluation of the FRK algorithm both on simulated measurements and
real field measurements shows a good trade-off between prediction efficiency
and computational complexity. In order to go a step further towards operational
application of the proposed algorithm, a scenario with multiple cells is
studied. Simulation results show a good performance in terms of coverage
prediction and detection of best serving cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07067</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07067</id><created>2015-05-26</created><authors><author><keyname>Ortega</keyname><forenames>Pedro A.</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author><author><keyname>Lee</keyname><forenames>Daniel D.</forenames></author></authors><title>Belief Flows of Robust Online Learning</title><categories>stat.ML cs.LG</categories><comments>Appears in Workshop on Information Theory and Applications (ITA),
  February 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new probabilistic model for online learning which
dynamically incorporates information from stochastic gradients of an arbitrary
loss function. Similar to probabilistic filtering, the model maintains a
Gaussian belief over the optimal weight parameters. Unlike traditional Bayesian
updates, the model incorporates a small number of gradient evaluations at
locations chosen using Thompson sampling, making it computationally tractable.
The belief is then transformed via a linear flow field which optimally updates
the belief distribution using rules derived from information theoretic
principles. Several versions of the algorithm are shown using different
constraints on the flow field and compared with conventional online learning
algorithms. Results are given for several classification tasks including
logistic regression and multilayer neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07079</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07079</id><created>2015-05-26</created><authors><author><keyname>Rodrigues</keyname><forenames>Jose F.</forenames><suffix>Jr</suffix></author><author><keyname>Zaina</keyname><forenames>Luciana A. M.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Maria C. F.</forenames></author><author><keyname>Traina</keyname><forenames>Agma J. M.</forenames></author></authors><title>A survey on Information Visualization in light of Vision and Cognitive
  sciences</title><categories>cs.GR</categories><comments>10 pages, Elsevier Journal preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information visualization techniques are built on a context with too many
factors, making it difficult to systematically deal with their underlying
bases. In the intent of promoting a better comprehension, here, we survey
concepts on vision, cognition, and Information Visualization organized in a
theorization named Visual Expression Model. With a reduced level of complexity,
our model organizes the bases of visualization techniques, nevertheless, it is
complete enough to discuss guidelines related to design and analytical tasks.
Organized in a coherent account, our work introduces the following
contributions: (1) Theoretical compilation of vision, cognition, and
Information Visualization, (2) Meticulous discussions supported by vast
literature, and (3) Recommendations to have visualizations satisfy
visual-cognitive aspects. We expect our contributions will improve the practice
of InfoVis by promoting comprehension and by proposing the use of simple
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07096</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07096</id><created>2015-05-26</created><authors><author><keyname>Michelucci</keyname><forenames>Pietro</forenames></author><author><keyname>Shanley</keyname><forenames>Lea</forenames></author><author><keyname>Dickinson</keyname><forenames>Janis</forenames></author><author><keyname>Hirsh</keyname><forenames>Haym</forenames></author></authors><title>A U.S. Research Roadmap for Human Computation</title><categories>cs.HC cs.AI cs.CY</categories><comments>32 pages, 25 figures, Workshop report from the CRA-sponsored Human
  Computation Roadmap Summit: P. Michelucci, L. Shanley, J. Dickinson, and H.
  Hirsh, A U.S. Research Roadmap for Human Computation, Computing Community
  Consortium Technical Report, 2015</comments><doi>10.13140/RG.2.1.4517.2648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web has made it possible to harness human cognition en masse to achieve
new capabilities. Some of these successes are well known; for example Wikipedia
has become the go-to place for basic information on all things; Duolingo
engages millions of people in real-life translation of text, while
simultaneously teaching them to speak foreign languages; and fold.it has
enabled public-driven scientific discoveries by recasting complex biomedical
challenges into popular online puzzle games. These and other early successes
hint at the tremendous potential for future crowd-powered capabilities for the
benefit of health, education, science, and society. In the process, a new field
called Human Computation has emerged to better understand, replicate, and
improve upon these successes through scientific research. Human Computation
refers to the science that underlies online crowd-powered systems and was the
topic of a recent visioning activity in which a representative cross-section of
researchers, industry practitioners, visionaries, funding agency
representatives, and policy makers came together to understand what makes
crowd-powered systems successful. Teams of experts considered past, present,
and future human computation systems to explore which kinds of crowd-powered
systems have the greatest potential for societal impact and which kinds of
research will best enable the efficient development of new crowd-powered
systems to achieve this impact. This report summarize the products and findings
of those activities as well as the unconventional process and activities
employed by the workshop, which were informed by human computation research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07130</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07130</id><created>2015-05-26</created><authors><author><keyname>Endris</keyname><forenames>Kemele M.</forenames></author><author><keyname>Faisal</keyname><forenames>Sidra</forenames></author><author><keyname>Orlandi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author><author><keyname>Scerri</keyname><forenames>Simon</forenames></author></authors><title>Interest-based RDF Update Propagation</title><categories>cs.DC cs.DB cs.IR</categories><comments>16 pages, Keywords: Change Propagation, Dataset Dynamics, Linked
  Data, Replication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many LOD datasets, such as DBpedia and LinkedGeoData, are voluminous and
process large amounts of requests from diverse applications. Many data products
and services rely on full or partial local LOD replications to ensure faster
querying and processing. While such replicas enhance the flexibility of
information sharing and integration infrastructures, they also introduce data
duplication with all the associated undesirable consequences. Given the
evolving nature of the original and authoritative datasets, to ensure
consistent and up-to-date replicas frequent replacements are required at a
great cost. In this paper, we introduce an approach for interest-based RDF
update propagation, which propagates only interesting parts of updates from the
source to the target dataset. Effectively, this enables remote applications to
`subscribe' to relevant datasets and consistently reflect the necessary changes
locally without the need to frequently replace the entire dataset (or a
relevant subset). Our approach is based on a formal definition for
graph-pattern-based interest expressions that is used to filter interesting
parts of updates from the source. We implement the approach in the iRap
framework and perform a comprehensive evaluation based on DBpedia Live updates,
to confirm the validity and value of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07140</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07140</id><created>2015-05-26</created><authors><author><keyname>Panaitopol</keyname><forenames>Dorin</forenames></author><author><keyname>Mouton</keyname><forenames>Christian</forenames></author><author><keyname>Lecroart</keyname><forenames>Benoit</forenames></author><author><keyname>Lair</keyname><forenames>Yannick</forenames></author><author><keyname>Delahaye</keyname><forenames>Philippe</forenames></author></authors><title>Recent Advances in 3GPP Rel-12 Standardization related to D2D and Public
  Safety Communications</title><categories>cs.NI</categories><comments>This work has been initially submitted to WCNC 2014 Workshop on
  Device-to-Device and Public Safety Communications (WDPC)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The goal of this paper is to present advances on recent 3GPP standardization
activities related to Device-to-Device (D2D) and public safety. The paper
provides a clear 3GPP state of the art, including when the 3GPP work on D2D and
public safety communications started. Finally, it presents important
conclusions with respect to further 3GPP work on this topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07158</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07158</id><created>2015-05-26</created><authors><author><keyname>Chen</keyname><forenames>Juntao</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>Resilient and Decentralized Control of Multi-level Cooperative Robotic
  Networks to Maintain Connectivity under Adversarial Attacks</title><categories>cs.SY cs.DC cs.RO</categories><comments>15 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Network connectivity plays an important role in the information exchange
between different agents in the multi-level networks. In this paper, we
establish a game-theoretic framework to capture the uncoordinated nature of the
decision making at different layers of the multi-level networks. To study the
network resiliency, we introduce two adversarial attack models and quantify
their impacts, and design a decentralized and resilient alternating-play
algorithm that aims to maximize the algebraic connectivity of the global
network under attack. We show that the designed algorithm converges to a Nash
equilibrium in a finite number of steps, and yields an equilibrium network.
Moreover, simulation results of a two-layer mobile robotic networks corroborate
and show the interdependency between different layers of networks in the
recovery process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07161</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07161</id><created>2015-05-26</created><authors><author><keyname>Mimram</keyname><forenames>Samuel</forenames><affiliation>LIX, &#xc9;cole Polytechnique</affiliation></author></authors><title>Presenting Finite Posets</title><categories>cs.LO</categories><comments>In Proceedings TERMGRAPH 2014, arXiv:1505.06818</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 183, 2015, pp. 1-17</journal-ref><doi>10.4204/EPTCS.183.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a monoidal category whose morphisms are finite partial orders,
with chosen minimal and maximal elements as source and target respectively.
After recalling the notion of presentation of a monoidal category by the means
of generators and relations, we construct a presentation of our category, which
corresponds to a variant of the notion of bialgebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07162</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07162</id><created>2015-05-26</created><authors><author><keyname>Antoy</keyname><forenames>Sergio</forenames></author><author><keyname>Johannsen</keyname><forenames>Jacob</forenames></author><author><keyname>Libby</keyname><forenames>Steven</forenames></author></authors><title>Needed Computations Shortcutting Needed Steps</title><categories>cs.PL cs.LO</categories><comments>In Proceedings TERMGRAPH 2014, arXiv:1505.06818</comments><proxy>EPTCS</proxy><acm-class>D.3.3;D.3.4;F.4.2;G.2.2</acm-class><journal-ref>EPTCS 183, 2015, pp. 18-32</journal-ref><doi>10.4204/EPTCS.183.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a compilation scheme for a constructor-based, strongly-sequential,
graph rewriting system which shortcuts some needed steps. The object code is
another constructor-based graph rewriting system. This system is normalizing
for the original system when using an innermost strategy. Consequently, the
object code can be easily implemented by eager functions in a variety of
programming languages. We modify this object code in a way that avoids total or
partial construction of the contracta of some needed steps of a computation.
When computing normal forms in this way, both memory consumption and execution
time are reduced compared to ordinary rewriting computations in the original
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07163</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07163</id><created>2015-05-26</created><authors><author><keyname>Eguchi</keyname><forenames>Naohi</forenames><affiliation>Chiba University</affiliation></author></authors><title>Complexity Analysis of Precedence Terminating Infinite Graph Rewrite
  Systems</title><categories>cs.CC cs.LO</categories><comments>In Proceedings TERMGRAPH 2014, arXiv:1505.06818. arXiv admin note:
  text overlap with arXiv:1404.6196</comments><proxy>EPTCS</proxy><acm-class>I.2.2;F.1.1;F.1.3;F.4.1;</acm-class><journal-ref>EPTCS 183, 2015, pp. 33-47</journal-ref><doi>10.4204/EPTCS.183.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The general form of safe recursion (or ramified recurrence) can be expressed
by an infinite graph rewrite system including unfolding graph rewrite rules
introduced by Dal Lago, Martini and Zorzi, in which the size of every normal
form by innermost rewriting is polynomially bounded. Every unfolding graph
rewrite rule is precedence terminating in the sense of Middeldorp, Ohsaki and
Zantema. Although precedence terminating infinite rewrite systems cover all the
primitive recursive functions, in this paper we consider graph rewrite systems
precedence terminating with argument separation, which form a subclass of
precedence terminating graph rewrite systems. We show that for any precedence
terminating infinite graph rewrite system G with a specific argument
separation, both the runtime complexity of G and the size of every normal form
in G can be polynomially bounded. As a corollary, we obtain an alternative
proof of the original result by Dal Lago et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07164</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07164</id><created>2015-05-26</created><authors><author><keyname>Hassan</keyname><forenames>Abubakar</forenames><affiliation>Theory and Practice of Software Ltd</affiliation></author><author><keyname>Mackie</keyname><forenames>Ian</forenames><affiliation>LIX, Ecole Polytechnique</affiliation></author><author><keyname>Sato</keyname><forenames>Shinya</forenames><affiliation>University of Sussex</affiliation></author></authors><title>An Implementation Model for Interaction Nets</title><categories>cs.LO cs.PL</categories><comments>In Proceedings TERMGRAPH 2014, arXiv:1505.06818</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 183, 2015, pp. 66-80</journal-ref><doi>10.4204/EPTCS.183.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To study implementations and optimisations of interaction net systems we
propose a calculus to allow us to reason about nets, a concrete data-structure
that is in close correspondence with the calculus, and a low-level language to
create and manipulate this data structure. These work together so that we can
describe the compilation process for interaction nets, reason about the
behaviours of the implementation, and study the efficiency and properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07168</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07168</id><created>2015-05-26</created><authors><author><keyname>Pagli</keyname><forenames>Linda</forenames></author><author><keyname>Prencipe</keyname><forenames>Giuseppe</forenames></author><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>Getting Close Without Touching: Near-Gathering for Autonomous Mobile
  Robots</title><categories>cs.DC cs.CG cs.RO</categories><comments>25 pages, 8 fiugres</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the Near-Gathering problem for a finite set of
dimensionless, deterministic, asynchronous, anonymous, oblivious and autonomous
mobile robots with limited visibility moving in the Euclidean plane in
Look-Compute-Move (LCM) cycles. In this problem, the robots have to get close
enough to each other, so that every robot can see all the others, without
touching (i.e., colliding with) any other robot. The importance of solving the
Near-Gathering problem is that it makes it possible to overcome the restriction
of having robots with limited visibility. Hence it allows to exploit all the
studies (the majority, actually) done on this topic in the unlimited visibility
setting. Indeed, after the robots get close enough to each other, they are able
to see all the robots in the system, a scenario that is similar to the one
where the robots have unlimited visibility.
  We present the first (deterministic) algorithm for the Near-Gathering
problem, to the best of our knowledge, which allows a set of autonomous mobile
robots to nearly gather within finite time without ever colliding. Our
algorithm assumes some reasonable conditions on the input configuration (the
Near-Gathering problem is easily seen to be unsolvable in general). Further,
all the robots are assumed to have a compass (hence they agree on the &quot;North&quot;
direction), but they do not necessarily have the same handedness (hence they
may disagree on the clockwise direction).
  We also show how the robots can detect termination, i.e., detect when the
Near-Gathering problem has been solved. This is crucial when the robots have to
perform a generic task after having nearly gathered. We show that termination
detection can be obtained even if the total number of robots is unknown to the
robots themselves (i.e., it is not a parameter of the algorithm), and robots
have no way to explicitly communicate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07184</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07184</id><created>2015-05-27</created><authors><author><keyname>Bollegala</keyname><forenames>Danushka</forenames></author><author><keyname>Maehara</keyname><forenames>Takanori</forenames></author><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>Unsupervised Cross-Domain Word Representation Learning</title><categories>cs.CL</categories><comments>53rd Annual Meeting of the Association for Computational Linguistics
  and the 7th International Joint Conferences on Natural Language Processing of
  the Asian Federation of Natural Language Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Meaning of a word varies from one domain to another. Despite this important
domain dependence in word semantics, existing word representation learning
methods are bound to a single domain. Given a pair of
\emph{source}-\emph{target} domains, we propose an unsupervised method for
learning domain-specific word representations that accurately capture the
domain-specific aspects of word semantics. First, we select a subset of
frequent words that occur in both domains as \emph{pivots}. Next, we optimize
an objective function that enforces two constraints: (a) for both source and
target domain documents, pivots that appear in a document must accurately
predict the co-occurring non-pivots, and (b) word representations learnt for
pivots must be similar in the two domains. Moreover, we propose a method to
perform domain adaptation using the learnt word representations. Our proposed
method significantly outperforms competitive baselines including the
state-of-the-art domain-insensitive word representations, and reports best
sentiment classification accuracies for all domain-pairs in a benchmark
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07186</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07186</id><created>2015-05-27</created><updated>2016-03-01</updated><authors><author><keyname>Kuznetsov</keyname><forenames>Eugene</forenames></author></authors><title>Computational lower limits on small Ramsey numbers</title><categories>cs.DM math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer-based attempts to construct lower bounds for small Ramsey numbers
are discussed. A systematic review of cyclic Ramsey graphs is attempted. Many
known lower bounds are reproduced. Several new bounds are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07187</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07187</id><created>2015-05-27</created><authors><author><keyname>Liu</keyname><forenames>Wenjia</forenames></author><author><keyname>Han</keyname><forenames>Shengqian</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author></authors><title>Is Massive MIMO Energy Efficient?</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multi-input multi-output (MIMO) can support high spectral efficiency
(SE) with simple linear transceivers, and is expected to provide high energy
efficiency (EE). In this paper, we analyze the EE of downlink multi-cell
massive MIMO systems under spatially correlated channel model, where both
transmit and circuit power consumptions, training overhead, channel estimation
and pilot contamination (PC) are taken into account. We obtain the maximal EE
for the systems with maximum-ratio transmission (MRT) and zero-forcing
beamforming (ZFBF) for given number of antennas and users by optimizing the
transmit power. The closed-form expressions of approximated optimal transmit
power and maximal EE, and their scaling laws with the number of antennas M are
derived for the systems with MRT and ZFBF. Our analysis shows that the maximal
EE decreases with M for both systems with and without PC, but with different
descending speeds. For the system without PC, the optimal transmit power should
be configured to increases with M, while for the system with PC, the optimal
transmit power should be configured as a constant independent from M. The
analytical results are validated by simulations under a more realistic
three-dimensional channel model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07188</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07188</id><created>2015-05-27</created><authors><author><keyname>Liu</keyname><forenames>Peng</forenames></author><author><keyname>Gazor</keyname><forenames>Saeed</forenames></author><author><keyname>Kim</keyname><forenames>Il-Min</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author></authors><title>Energy Harvesting Noncoherent Cooperative Communications</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper investigates simultaneous wireless information and power transfer
(SWIPT) in energy harvesting (EH) relay systems. Unlike existing SWIPT schemes
requiring the channel state information (CSI) for coherent information
delivery, we propose a noncoherent SWIPT framework for decode-and-forward (DF)
relay systems bypassing the need for CSI and consequently saving energy in the
network. The proposed SWIPT framework embraces power-splitting noncoherent DF
(PS-NcDF) and time-switching noncoherent DF (TS-NcDF) in a unified form, and
supports arbitrary M-ary noncoherent frequency-shift keying (FSK) and
differential phase-shift keying (DPSK). Exact (noncoherent) maximum-likelihood
detectors (MLDs) for PS-NcDF and TS-NcDF are developed in a unified form, which
involves integral evaluations yet serves as the optimum performance benchmark
for noncoherent SWIPT. To reduce the computational cost of the exact MLDs, we
also propose closed-form approximate MLDs achieving near-optimum performance,
thus serving as a practical solution for noncoherent SWIPT. Numerical results
demonstrate a performance tradeoff between the first and second hops through
the adjustment of time switching or power splitting parameters, whose optimal
values minimizing the symbol-error rate (SER) are strictly between 0 and 1. We
demonstrate that M-FSK results in a significant energy saving over M-DPSK for M
&gt;= 8; thus M-FSK may be more suitable for EH relay systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07192</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07192</id><created>2015-05-27</created><authors><author><keyname>Li</keyname><forenames>Hongyang</forenames></author><author><keyname>Lu</keyname><forenames>Huchuan</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author><author><keyname>Shen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Price</keyname><forenames>Brian</forenames></author></authors><title>Inner and Inter Label Propagation: Salient Object Detection in the Wild</title><categories>cs.CV</categories><comments>The full version of the TIP 2015 publication</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose a novel label propagation based method for saliency
detection. A key observation is that saliency in an image can be estimated by
propagating the labels extracted from the most certain background and object
regions. For most natural images, some boundary superpixels serve as the
background labels and the saliency of other superpixels are determined by
ranking their similarities to the boundary labels based on an inner propagation
scheme. For images of complex scenes, we further deploy a 3-cue-center-biased
objectness measure to pick out and propagate foreground labels. A
co-transduction algorithm is devised to fuse both boundary and objectness
labels based on an inter propagation scheme. The compactness criterion decides
whether the incorporation of objectness labels is necessary, thus greatly
enhancing computational efficiency. Results on five benchmark datasets with
pixel-wise accurate annotations show that the proposed method achieves superior
performance compared with the newest state-of-the-arts in terms of different
evaluation metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07193</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07193</id><created>2015-05-27</created><authors><author><keyname>Yu</keyname><forenames>Linyun</forenames></author><author><keyname>Cui</keyname><forenames>Peng</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Song</keyname><forenames>Chaoming</forenames></author><author><keyname>Yang</keyname><forenames>Shiqiang</forenames></author></authors><title>From Micro to Macro: Uncovering and Predicting Information Cascading
  Process with Behavioral Dynamics</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascades are ubiquitous in various network environments. How to predict these
cascades is highly nontrivial in several vital applications, such as viral
marketing, epidemic prevention and traffic management. Most previous works
mainly focus on predicting the final cascade sizes. As cascades are typical
dynamic processes, it is always interesting and important to predict the
cascade size at any time, or predict the time when a cascade will reach a
certain size (e.g. an threshold for outbreak). In this paper, we unify all
these tasks into a fundamental problem: cascading process prediction. That is,
given the early stage of a cascade, how to predict its cumulative cascade size
of any later time? For such a challenging problem, how to understand the micro
mechanism that drives and generates the macro phenomenons (i.e. cascading
proceese) is essential. Here we introduce behavioral dynamics as the micro
mechanism to describe the dynamic process of a node's neighbors get infected by
a cascade after this node get infected (i.e. one-hop subcascades). Through
data-driven analysis, we find out the common principles and patterns lying in
behavioral dynamics and propose a novel Networked Weibull Regression model for
behavioral dynamics modeling. After that we propose a novel method for
predicting cascading processes by effectively aggregating behavioral dynamics,
and propose a scalable solution to approximate the cascading process with a
theoretical guarantee. We extensively evaluate the proposed method on a large
scale social network dataset. The results demonstrate that the proposed method
can significantly outperform other state-of-the-art baselines in multiple tasks
including cascade size prediction, outbreak time prediction and cascading
process prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07194</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07194</id><created>2015-05-27</created><updated>2015-06-12</updated><authors><author><keyname>Liu</keyname><forenames>Peng</forenames></author><author><keyname>Gazor</keyname><forenames>Saeed</forenames></author><author><keyname>Kim</keyname><forenames>Il-Min</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author></authors><title>Noncoherent Relaying in Energy Harvesting Communication Systems</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In energy harvesting (EH) relay networks, the coherent communication requires
accurate estima- tion/tracking of the instantaneous channel state information
(CSI) which consumes extra power. As a remedy, we propose two noncoherent EH
relaying protocols based on the amplify-and-forward (AF) relaying, namely,
power splitting noncoherent AF (PS-NcAF) and time switching noncoherent AF
(TS-NcAF), which do not require any instantaneous CSI. We develop a noncoherent
framework of simultaneous wireless information and power transfer (SWIPT),
embracing PS-NcAF and TS-NcAF in a unified form. For arbitrary M-ary
noncoherent frequency-shift keying (FSK) and differential phase- shift keying
(DPSK), we derive maximum-likelihood detectors (MLDs) for PS-NcAF and TS-NcAF
in a unified form, which involves integral evaluations yet serves as the
optimum performance benchmark. To avoid expensive integral computations, we
propose a closed-form detector using the Gauss-Legendre approximation, which
achieves almost identical performance as the MLD but at substantially lower
complexity. These EH-based noncoherent detectors achieve full diversity in
Rayleigh fading. Numerical results demonstrate that our proposed PS-NcAF and
TS-NcAF may outperform the conventional grid- powered relay system under the
same total power constraint. Various insights which are useful for the design
of practical SWIPT relaying systems are obtained. Interestingly, PS-NcAF
outperforms TS-NcAF in the single-relay case, whereas TS-NcAF outperforms
PS-NcAF in the multi-relay case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07203</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07203</id><created>2015-05-27</created><authors><author><keyname>Cousty</keyname><forenames>Jean</forenames><affiliation>LIGM</affiliation></author><author><keyname>Najman</keyname><forenames>Laurent</forenames><affiliation>LIGM</affiliation></author><author><keyname>Kenmochi</keyname><forenames>Yukiko</forenames><affiliation>LIGM</affiliation></author><author><keyname>Guimar&#xc3;&#xa3;es</keyname><forenames>Silvio</forenames><affiliation>VIPLAB, LIGM</affiliation></author></authors><title>New characterizations of minimum spanning trees and of saliency maps
  based on quasi-flat zones</title><categories>cs.CV cs.DS</categories><proxy>ccsd</proxy><journal-ref>12th International Symposium on Mathematical Morphology (ISMM),
  May 2015, Reykjavik, Iceland. Lecture Notes in Computer Science (LNCS), 9082,
  pp.205-216, Mathematical Morphology and Its Applications to Signal and Image
  Processing</journal-ref><doi>10.1007/978-3-319-18720-4_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study three representations of hierarchies of partitions: dendrograms
(direct representations), saliency maps, and minimum spanning trees. We provide
a new bijection between saliency maps and hierarchies based on quasi-flat zones
as used in image processing and characterize saliency maps and minimum spanning
trees as solutions to constrained minimization problems where the constraint is
quasi-flat zones preservation. In practice, these results form a toolkit for
new hierarchical methods where one can choose the most convenient
representation. They also invite us to process non-image data with
morphological hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07204</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07204</id><created>2015-05-27</created><authors><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>The minimal measurement number for low-rank matrices recovery</title><categories>math.NA cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents several results that address a fundamental question in
low-rank matrices recovery: how many measurements are needed to recover low
rank matrices? We begin by investigating the complex matrices case and show
that $4nr-4r^2$ generic measurements are both necessary and sufficient for the
recovery of rank-$r$ matrices in $\C^{n\times n}$ by algebraic tools. Thus, we
confirm a conjecture which is raised by Eldar, Needell and Plan for the complex
case. We next consider the real case and prove that the bound $4nr-4r^2$ is
tight provided $n=2^k+r, k\in \Z_+$. Motivated by Vinzant's work, we construct
$11$ matrices in $\R^{4\times 4}$ by computer random search and prove they
define injective measurements on rank-$1$ matrices in $\R^{4\times 4}$. This
disproves the conjecture raised by Eldar, Needell and Plan for the real case.
Finally, we use the results in this paper to investigate the phase retrieval by
projection and show fewer than $2n-1$ orthogonal projections are possible for
the recovery of $x\in \R^n$ from the norm of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07206</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07206</id><created>2015-05-27</created><updated>2015-07-14</updated><authors><author><keyname>Bergel</keyname><forenames>Itsik</forenames></author><author><keyname>Perets</keyname><forenames>Yona</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>Uplink Downlink Rate Balancing in Cooperating Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadcast MIMO techniques can significantly increase the throughput in the
downlink of cellular networks, at the price of channel state information (CSI)
feedback from the mobiles, sent over the uplink. Thus, it creates a mechanism
that can tradeoff some uplink capacity for increased downlink capacity. In this
work we quantify this tradeoff and study the exchange ratio between the
feedback rate (over the uplink) and the downlink rate. We study both finite and
infinite networks, and show that for high enough (but finite) SNR, the uplink
rate can be exchanged for increased downlink rate with a favorable exchange
ratio. This exchange ratio is an increasing function of the channel coherence
time, and a decreasing function of the number of measured base stations. We
also show that devoting a constant fraction of the uplink to CSI feedback can
increase the downlink multiplexing gain continuously from 0 to 1, in finite
networks. On the other hand, in infinite networks (with infinite connectivity)
our bounds can only show doubly logarithmic scaling of the rate with SNR. The
presented results prove that the adaptation of the feedback rate can control
the balance between the uplink and downlink rates. This capability is very
important in modern cellular networks, where the operators need to respond to
continuously changing user demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07212</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07212</id><created>2015-05-27</created><authors><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author></authors><title>Intelligent escalation and the principle of relativity</title><categories>cs.GT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.2284</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Escalation is the fact that in a game (for instance in an auction), the
agents play forever. The $0,1$-game is an extremely simple infinite game with
intelligent agents in which escalation arises. It shows at the light of
research on cognitive psychology the difference between intelligence
(algorithmic mind) and rationality (algorithmic and reflective mind) in
decision processes. It also shows that depending on the point of view (inside
or outside) the rationality of the agent may change which is proposed to be
called the principle of relativity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07237</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07237</id><created>2015-05-27</created><updated>2015-09-24</updated><authors><author><keyname>Nebe</keyname><forenames>Gabriele</forenames></author><author><keyname>Willems</keyname><forenames>Wolfgang</forenames></author></authors><title>On self-dual MRD codes</title><categories>cs.IT math.IT</categories><comments>Improved exposition according to the referees' comments</comments><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the automorphism group of Gabidulin codes of full length and
characterise when these codes are equivalent to self-dual codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07239</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07239</id><created>2015-05-27</created><authors><author><keyname>Copigneaux</keyname><forenames>Bertrand</forenames></author></authors><title>Semi-autonomous, context-aware, agent using behaviour modelling and
  reputation systems to authorize data operation in the Internet of Things</title><categories>cs.CY cs.CR</categories><comments>This work is currently supported by the BUTLER Project co-financed
  under the 7th framework program of the European Commission. published in
  Internet of Things (WF-IoT), 2014 IEEE World Forum, 6-8 March 2014, Seoul,
  P411-416, DOI: 10.1109/WF-IoT.2014.6803201, INSPEC: 14255656</comments><doi>10.1109/WF-IoT.2014.6803201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the issue of gathering the &quot;informed consent&quot; of an
end user in the Internet of Things. We start by evaluating the legal importance
and some of the problems linked with this notion of informed consent in the
specific context of the Internet of Things. From this assessment we propose an
approach based on a semi-autonomous, rule based agent that centralize all
authorization decisions on the personal data of a user and that is able to take
decision on his behalf. We complete this initial agent by integrating
context-awareness, behavior modeling and community based reputation system in
the algorithm of the agent. The resulting system is a &quot;smart&quot; application, the
&quot;privacy butler&quot; that can handle data operations on behalf of the end-user
while keeping the user in control. We finally discuss some of the potential
problems and improvements of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07240</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07240</id><created>2015-05-27</created><updated>2015-09-17</updated><authors><author><keyname>Graziotin</keyname><forenames>Daniel</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames></author></authors><title>How Do You Feel, Developer? An Explanatory Theory of the Impact of
  Affects on Programming Performance</title><categories>cs.SE cs.CY cs.HC</categories><comments>24 pages, 2 figures. Postprint</comments><journal-ref>PeerJ Computer Science 1:e18</journal-ref><doi>10.7717/peerj-cs.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Affects---emotions and moods---have an impact on cognitive activities and the
working performance of individuals. Development tasks are undertaken through
cognitive processes, yet software engineering research lacks theory on affects
and their impact on software development activities. In this paper, we report
on an interpretive study aimed at broadening our understanding of the
psychology of programming in terms of the experience of affects while
programming, and the impact of affects on programming performance. We conducted
a qualitative interpretive study based on: face-to-face open-ended interviews,
in-field observations, and e-mail exchanges. This enabled us to construct a
novel explanatory theory of the impact of affects on development performance.
The theory is explicated using an established taxonomy framework. The proposed
theory builds upon the concepts of events, affects, attractors, focus, goals,
and performance. Theoretical and practical implications are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07254</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07254</id><created>2015-05-27</created><authors><author><keyname>Holohan</keyname><forenames>Naoise</forenames></author><author><keyname>Leith</keyname><forenames>Doug</forenames></author><author><keyname>Mason</keyname><forenames>Oliver</forenames></author></authors><title>Differentially Private Response Mechanisms on Categorical Data</title><categories>cs.DM cs.CR math.CO</categories><msc-class>68R01, 68R05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study mechanisms for differential privacy on finite datasets. By deriving
\emph{sufficient sets} for differential privacy we obtain necessary and
sufficient conditions for differential privacy, a tight lower bound on the
maximal expected error of a discrete mechanism and a characterisation of the
optimal mechanism which minimises the maximal expected error within the class
of mechanisms considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07257</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07257</id><created>2015-05-27</created><authors><author><keyname>Bouha</keyname><forenames>Najia</forenames></author><author><keyname>Morvan</keyname><forenames>Gildas</forenames></author><author><keyname>Aboua&#xef;ssa</keyname><forenames>Hassane</forenames></author><author><keyname>Kubera</keyname><forenames>Yoann</forenames></author></authors><title>A First Step Towards Dynamic Hybrid Traffic Modeling</title><categories>cs.MA</categories><comments>in Proceedings of 29th European Conf. on modelling and simulation
  (ECMS), p. 64-70, 2015. arXiv admin note: text overlap with arXiv:1401.6773</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid traffic modeling and simulation provide an important way to represent
and evaluate large-scale traffic networks at different levels of details. The
first level, called &quot;microscopic&quot; allows the description of individual vehicles
and their interactions as well as the study of driver's individual behavior.
The second, based on the analogy with fluidic dynamic, is the &quot;macroscopic&quot; one
and provides an efficient way to represent traffic flow behavior in large
traffic infrastructures, using three aggregated variables: traffic density,
mean speed and traffic volume. An intermediate level called &quot;mesoscopic&quot;
considers a group of vehicles sharing common properties such as a same origin
and destination. The work conducted in this paper presents a first step
allowing simulation of wide area traffic network on the basis of dynamic hybrid
modeling, where the representation associated to a network section can change
at runtime. The proposed approach is implemented in a simulation platform,
called Jam-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07263</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07263</id><created>2015-05-27</created><authors><author><keyname>Padovani</keyname><forenames>Luca</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author></authors><title>Qsmodels: ASP Planning in Interactive Gaming Environment</title><categories>cs.AI</categories><comments>Proceedings of Logics in Artificial Intelligence, 9th European
  Conference, {JELIA} 2004, pp. 689-692. Lisbon, Portugal, September 27-30,
  2004</comments><msc-class>68T27, 68T30, 68T42, 91A43</msc-class><acm-class>I.2.1; D.1.6</acm-class><doi>10.1007/978-3-540-30227-8_58</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qsmodels is a novel application of Answer Set Programming to interactive
gaming environment. We describe a software architecture by which the behavior
of a bot acting inside the Quake 3 Arena can be controlled by a planner. The
planner is written as an Answer Set Program and is interpreted by the Smodels
solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07267</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07267</id><created>2015-05-27</created><authors><author><keyname>M&#xe9;tral</keyname><forenames>Claudine</forenames></author><author><keyname>Falquet</keyname><forenames>Gilles</forenames></author></authors><title>Prototyping Information Visualization in 3D City Models: a Model-based
  Approach</title><categories>cs.HC cs.GR</categories><comments>Proc. of 3DGeoInfo 2014 Conference, Dubai, November 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When creating 3D city models, selecting relevant visualization techniques is
a particularly difficult user interface design task. A first obstacle is that
current geodata-oriented tools, e.g. ArcGIS, have limited 3D capabilities and
limited sets of visualization techniques. Another important obstacle is the
lack of unified description of information visualization techniques for 3D city
models. If many techniques have been devised for different types of data or
information (wind flows, air quality fields, historic or legal texts, etc.)
they are generally described in articles, and not really formalized. In this
paper we address the problem of visualizing information in (rich) 3D city
models by presenting a model-based approach for the rapid prototyping of
visualization techniques. We propose to represent visualization techniques as
the composition of graph transformations. We show that these transformations
can be specified with SPARQL construction operations over RDF graphs. These
specifications can then be used in a prototype generator to produce 3D scenes
that contain the 3D city model augmented with data represented using the
desired technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07277</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07277</id><created>2015-05-27</created><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author></authors><title>Relative Generalized Hamming Weights of Cyclic Codes</title><categories>cs.IT math.IT</categories><comments>14 pages</comments><msc-class>94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relative generalized Hamming weights (RGHWs) of a linear code respect to a
linear subcode determine the security of the linear ramp secret sharing scheme
based on the code. They can be used to express the information leakage of the
secret when some keepers of shares are corrupted. Cyclic codes are an
interesting type of linear codes and have wide applications in communication
and storage systems. In this paper, we investigate the RGHWs of cyclic codes
with two nonzeros respect to any of its irreducible cyclic subcodes. Applying
the method in the paper [arxiv.org/abs/1410.2702], we give two formulae for
RGHWs of the cyclic codes. As applications of the formulae, explicit examples
are computed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07278</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07278</id><created>2015-05-27</created><authors><author><keyname>Yan</keyname><forenames>Haode</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>Chunlei</forenames></author></authors><title>Higher weight distribution of linearized Reed-Solomon codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linearized Reed-Solomon codes are defined. Higher weight distribution of
those codes are determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07279</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07279</id><created>2015-05-27</created><authors><author><keyname>Kempton</keyname><forenames>Louis</forenames></author><author><keyname>Herrmann</keyname><forenames>Guido</forenames></author><author><keyname>di Bernardo</keyname><forenames>Mario</forenames></author></authors><title>Self-organization of weighted networks for optimal synchronizability</title><categories>nlin.AO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a network can self-organize its structure in a completely
distributed manner in order to optimize its synchronizability whilst satisfying
the local constraints: non-negativity of edge weights, and maximum weighted
degree of nodes. A novel multilayer approach is presented which uses a
distributed strategy to estimate two spectral functions of the graph Laplacian,
the algebraic connectivity $\lambda_2$ and the eigenratio $r = \lambda_n /
\lambda_2$ . These local estimates are then used to evolve the edge weights so
as to maximize $\lambda_2$, or minimize $r$ and, hence, achieve an optimal
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07283</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07283</id><created>2015-05-27</created><authors><author><keyname>Natarajan</keyname><forenames>Lakshmi</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Index Codes for the Gaussian Broadcast Channel using Quadrature
  Amplitude Modulation</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Communications Letters. 4 pages, 2
  figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose index codes, based on multidimensional QAM constellations, for the
Gaussian broadcast channel, where every receiver demands all the messages from
the source. The efficiency with which an index code exploits receiver side
information in this broadcast channel is characterised by a code design metric
called &quot;side information gain&quot;. The known index codes for this broadcast
channel enjoy large side information gains, but do not encode all the source
messages at the same rate, and do not admit message sizes that are powers of
two. The index codes proposed in this letter, which are based on linear codes
over integer rings, overcome both these drawbacks and yet provide large values
of side information gain. With the aid of a computer search, we obtain QAM
index codes for encoding up to 5 messages with message sizes 2^m, m &lt;= 6. We
also present the simulated performance of a new 16-QAM index code, concatenated
with an off-the-shelf LDPC code, which is observed to operate within 4.3 dB of
the broadcast channel capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07293</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07293</id><created>2015-05-27</created><authors><author><keyname>Badrinarayanan</keyname><forenames>Vijay</forenames></author><author><keyname>Handa</keyname><forenames>Ankur</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author></authors><title>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust
  Semantic Pixel-Wise Labelling</title><categories>cs.CV</categories><comments>This version was first submitted to CVPR' 15 on November 14, 2014
  with paper Id 1468. A similar architecture was proposed more recently on May
  17, 2015, see http://arxiv.org/pdf/1505.04366.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel deep architecture, SegNet, for semantic pixel wise image
labelling. SegNet has several attractive properties; (i) it only requires
forward evaluation of a fully learnt function to obtain smooth label
predictions, (ii) with increasing depth, a larger context is considered for
pixel labelling which improves accuracy, and (iii) it is easy to visualise the
effect of feature activation(s) in the pixel label space at any depth. SegNet
is composed of a stack of encoders followed by a corresponding decoder stack
which feeds into a soft-max classification layer. The decoders help map low
resolution feature maps at the output of the encoder stack to full input image
size feature maps. This addresses an important drawback of recent deep learning
approaches which have adopted networks designed for object categorization for
pixel wise labelling. These methods lack a mechanism to map deep layer feature
maps to input dimensions. They resort to ad hoc methods to upsample features,
e.g. by replication. This results in noisy predictions and also restricts the
number of pooling layers in order to avoid too much upsampling and thus reduces
spatial context. SegNet overcomes these problems by learning to map encoder
outputs to image pixel labels. We test the performance of SegNet on outdoor RGB
scenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our results
show that SegNet achieves state-of-the-art performance even without use of
additional cues such as depth, video frames or post-processing with CRF models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07302</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07302</id><created>2015-05-27</created><updated>2015-07-07</updated><authors><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Cross</keyname><forenames>James P.</forenames></author></authors><title>Unveiling the Political Agenda of the European Parliament Plenary: A
  Topical Analysis</title><categories>cs.CL cs.CY</categories><comments>Add link to implementation code on Github</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study analyzes political interactions in the European Parliament (EP) by
considering how the political agenda of the plenary sessions has evolved over
time and the manner in which Members of the European Parliament (MEPs) have
reacted to external and internal stimuli when making Parliamentary speeches. It
does so by considering the context in which speeches are made, and the content
of those speeches. To detect latent themes in legislative speeches over time,
speech content is analyzed using a new dynamic topic modeling method, based on
two layers of matrix factorization. This method is applied to a new corpus of
all English language legislative speeches in the EP plenary from the period
1999-2014. Our findings suggest that the political agenda of the EP has evolved
significantly over time, is impacted upon by the committee structure of the
Parliament, and reacts to exogenous events such as EU Treaty referenda and the
emergence of the Euro-crisis have a significant impact on what is being
discussed in Parliament.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07310</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07310</id><created>2015-05-26</created><authors><author><keyname>Tanveer</keyname><forenames>M. Iftekhar</forenames></author></authors><title>Use of Laplacian Projection Technique for Summarizing Likert Scale
  Annotations</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Summarizing Likert scale ratings from human annotators is an important step
for collecting human judgments. In this project we study a novel, graph
theoretic method for this purpose. We also analyze a few interesting properties
for this approach using real annotation datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07321</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07321</id><created>2015-05-27</created><authors><author><keyname>Leckey</keyname><forenames>Kevin</forenames></author><author><keyname>Neininger</keyname><forenames>Ralph</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author></authors><title>A Limit Theorem for Radix Sort and Tries with Markovian Input</title><categories>math.PR cs.DS</categories><msc-class>60F05, 60C05, 68P10, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tries are among the most versatile and widely used data structures on words.
In particular, they are used in fundamental sorting algorithms such as radix
sort which we study in this paper. While the performance of radix sort and
tries under a realistic probabilistic model for the generation of words is of
significant importance, its analysis, even for simplest memoryless sources, has
proved difficult. In this paper we consider a more realistic model where words
are generated by a Markov source. By a novel use of the contraction method
combined with moment transfer techniques we prove a central limit theorem for
the complexity of radix sort and for the external path length in a trie. This
is the first application of the contraction method to the analysis of
algorithms and data structures with Markovian inputs; it relies on the use of
systems of stochastic recurrences combined with a product version of the
Zolotarev metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07335</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07335</id><created>2015-05-25</created><authors><author><keyname>Karlebach</keyname><forenames>Guy</forenames></author></authors><title>A Novel Algorithm for the Maximal Fit Problem in Boolean Networks</title><categories>q-bio.MN cs.CE cs.SI</categories><comments>submitted to EURASIP Journal on Systems Biology and Bioinformatics on
  23/05/2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A gene regulatory network is a central concept in Systems Biology. It links
the expression levels of a set of genes via regulatory controls that gene
products exert on one another. There have been numerous suggestions for models
of gene regulatory networks, with varying degrees of expressivity and ease of
analysis. Perhaps the simplest model is the Boolean network, introduced by
Kauffman several decades ago: expression levels take a Boolean value, and
regulation of expression is expressed by Boolean functions. Even for this
simple formulation, the problem of fitting a given model to an expression
dataset is NP-Complete. In this paper we introduce a novel algorithm for this
problem that makes use of sampling in order to handle large datasets. In order
to demonstrate its performance we test it on multiple large in-silico datasets
with several levels and types of noise. Our results support the notion that
network analysis is applicable to large datasets, and that the production of
such datasets is desirable for the study of gene regulatory networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07354</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07354</id><created>2015-05-27</created><authors><author><keyname>Liu</keyname><forenames>Ying</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Do</keyname><forenames>Younghae</forenames></author></authors><title>Improving the accuracy of the k-shell method by removing redundant
  links-from a perspective of spreading dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent study shows that the accuracy of the k-shell method in determining
node coreness in a spreading process is largely impacted due to the existence
of core-like group, which has a large k-shell index but a low spreading
efficiency. Based on analysis of the structure of core-like groups in
real-world networks, we discover that nodes in the core-like group are mutually
densely connected with very few out-leaving links from the group. By defining a
measure of diffusion importance for each edge based on the number of
out-leaving links of its both ends, we are able to identify redundant links in
the spreading process, which have a relatively low diffusion importance but
lead to form the locally densely connected core-like group. After filtering out
the redundant links and applying the k-shell method to the residual network, we
obtain a renewed coreness for each node which is a more accurate index to
indicate its location importance and spreading influence in the original
network. Moreover, we find that the performance of the ranking algorithms based
on the renewed coreness are also greatly enhanced. Our findings help to more
accurately decompose the network core structure and identify influential nodes
in spreading processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07363</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07363</id><created>2015-05-27</created><updated>2015-07-20</updated><authors><author><keyname>Morrison</keyname><forenames>Katherine</forenames></author></authors><title>An Enumeration of the Equivalence Classes of Self-Dual Matrix Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a result of their applications in network coding, space-time coding, and
coding for criss-cross errors, matrix codes have garnered significant
attention; in various contexts, these codes have also been termed rank-metric
codes, space-time codes over finite fields, and array codes. We focus on
characterizing matrix codes that are both efficient (have high rate) and
effective at error correction (have high minimum rank-distance). It is well
known that the inherent trade-off between dimension and minimum distance for a
matrix code is reversed for its dual code; specifically, if a matrix code has
high dimension and low minimum distance, then its dual code will have low
dimension and high minimum distance. With an aim towards finding codes with a
perfectly balanced trade-off, we study self-dual matrix codes. In this work, we
develop a framework based on double cosets of the matrix-equivalence maps to
provide a complete classification of the equivalence classes of self-dual
matrix codes, and we employ this method to enumerate the equivalence classes of
these codes for small parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07368</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07368</id><created>2015-05-27</created><authors><author><keyname>Charousset</keyname><forenames>Dominik</forenames></author><author><keyname>Hiesgen</keyname><forenames>Raphael</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author></authors><title>Revisiting Actor Programming in C++</title><categories>cs.PL</categories><comments>33 pages</comments><doi>10.1016/j.cl.2016.01.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The actor model of computation has gained significant popularity over the
last decade. Its high level of abstraction makes it appealing for concurrent
applications in parallel and distributed systems. However, designing a
real-world actor framework that subsumes full scalability, strong reliability,
and high resource efficiency requires many conceptual and algorithmic additives
to the original model.
  In this paper, we report on designing and building CAF, the &quot;C++ Actor
Framework&quot;. CAF targets at providing a concurrent and distributed native
environment for scaling up to very large, high-performance applications, and
equally well down to small constrained systems. We present the key
specifications and design concepts---in particular a message-transparent
architecture, type-safe message interfaces, and pattern matching
facilities---that make native actors a viable approach for many robust,
elastic, and highly distributed developments. We demonstrate the feasibility of
CAF in three scenarios: first for elastic, upscaling environments, second for
including heterogeneous hardware like GPGPUs, and third for distributed runtime
systems. Extensive performance evaluations indicate ideal runtime behaviour for
up to 64 cores at very low memory footprint, or in the presence of GPUs. In
these tests, CAF continuously outperforms the competing actor environments
Erlang, Charm++, SalsaLite, Scala, ActorFoundry, and even the OpenMPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07370</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07370</id><created>2015-05-27</created><authors><author><keyname>Van Mieghem</keyname><forenames>Vincent</forenames></author><author><keyname>Pouwelse</keyname><forenames>Johan</forenames></author></authors><title>Anonymous online purchases with exhaustive operational security</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the process of remaining anonymous online and its
concurrent operational security that has to be performed. It focusses
particularly on remaining anonymous while purchasing online goods, resulting in
anonymously bought items. Different aspects of the operational security process
as well as anonymously funding with cryptocurrencies are described. Eventually
it is shown how to anonymously purchase items and services from the hidden web,
as well as the delivery. It is shown that, while becoming increasingly
difficult, it is still possible to make anonymous purchases. Our presented work
combines existing best-practices and deliberately avoids untested novel
approaches when possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07375</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07375</id><created>2015-05-26</created><authors><author><keyname>Dai</keyname><forenames>Hong-Yi</forenames></author></authors><title>The Mysteries of Lisp -- I: The Way to S-expression Lisp</title><categories>cs.PL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Despite its old age, Lisp remains mysterious to many of its admirers. The
mysteries on one hand fascinate the language, on the other hand also obscure
it. Following Stoyan but paying attention to what he has neglected or omitted,
in this first essay of a series intended to unravel these mysteries, we trace
the development of Lisp back to its origin, revealing how the language has
evolved into its nowadays look and feel. The insights thus gained will not only
enhance existent understanding of the language but also inspires further
improvement of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07376</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07376</id><created>2015-05-27</created><updated>2015-11-06</updated><authors><author><keyname>Gatys</keyname><forenames>Leon A.</forenames></author><author><keyname>Ecker</keyname><forenames>Alexander S.</forenames></author><author><keyname>Bethge</keyname><forenames>Matthias</forenames></author></authors><title>Texture Synthesis Using Convolutional Neural Networks</title><categories>cs.CV cs.NE q-bio.NC</categories><comments>Revision for NIPS 2015 Camera Ready. In line with reviewer's comments
  we now focus on the texture model and texture synthesis performance. We limit
  the relationship of our texture model to the ventral stream and its potential
  use for neuroscience to the discussion of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we introduce a new model of natural textures based on the feature spaces
of convolutional neural networks optimised for object recognition. Samples from
the model are of high perceptual quality demonstrating the generative power of
neural networks trained in a purely discriminative fashion. Within the model,
textures are represented by the correlations between feature maps in several
layers of the network. We show that across layers the texture representations
increasingly capture the statistical properties of natural images while making
object information more and more explicit. The model provides a new tool to
generate stimuli for neuroscience and might offer insights into the deep
representations learned by convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07383</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07383</id><created>2015-05-26</created><authors><author><keyname>Anderson</keyname><forenames>Brian</forenames></author><author><keyname>Bergstrom</keyname><forenames>Lars</forenames></author><author><keyname>Herman</keyname><forenames>David</forenames></author><author><keyname>Matthews</keyname><forenames>Josh</forenames></author><author><keyname>McAllister</keyname><forenames>Keegan</forenames></author><author><keyname>Goregaokar</keyname><forenames>Manish</forenames></author><author><keyname>Moffitt</keyname><forenames>Jack</forenames></author><author><keyname>Sapin</keyname><forenames>Simon</forenames></author></authors><title>Experience Report: Developing the Servo Web Browser Engine using Rust</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All modern web browsers - Internet Explorer, Firefox, Chrome, Opera, and
Safari - have a core rendering engine written in C++. This language choice was
made because it affords the systems programmer complete control of the
underlying hardware features and memory in use, and it provides a transparent
compilation model.
  Servo is a project started at Mozilla Research to build a new web browser
engine that preserves the capabilities of these other browser engines but also
both takes advantage of the recent trends in parallel hardware and is more
memory-safe. We use a new language, Rust, that provides us a similar level of
control of the underlying system to C++ but which builds on many concepts
familiar to the functional programming community, forming a novelty - a useful,
safe systems programming language.
  In this paper, we show how a language with an affine type system, regions,
and many syntactic features familiar to functional language programmers can be
successfully used to build state-of-the-art systems software. We also outline
several pitfalls encountered along the way and describe some potential areas
for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07391</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07391</id><created>2015-05-27</created><updated>2015-11-20</updated><authors><author><keyname>Roche</keyname><forenames>Daniel S.</forenames></author><author><keyname>Aviv</keyname><forenames>Adam J.</forenames></author><author><keyname>Choi</keyname><forenames>Seung Geol</forenames></author></authors><title>A Practical Oblivious Map Data Structure with Secure Deletion and
  History Independence</title><categories>cs.CR cs.DS</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We present a new oblivious RAM that supports variable-sized storage blocks
(vORAM), which is the first ORAM to allow varying block sizes without trivial
padding. We also present a new history-independent data structure (a HIRB tree)
that can be stored within a vORAM. Together, this construction provides an
efficient and practical oblivious data structure (ODS) for a key/value map, and
goes further to provide an additional privacy guarantee as compared to prior
ODS maps: even upon client compromise, deleted data and the history of old
operations remain hidden to the attacker.
  We implement and measure the performance of our system using Amazon Web
Services, and the single-operation time for a realistic database (up to
$2^{18}$ entries) is less than 1 second. This represents a 100x speed-up
compared to the current best oblivious map data structure (which provides
neither secure deletion nor history independence) by Wang et al. (CCS 14).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07395</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07395</id><created>2015-05-27</created><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Duvnjak</keyname><forenames>Dujo</forenames></author><author><keyname>Jug</keyname><forenames>Davor</forenames></author></authors><title>GWAT: The Geneva Affective Picture Database WordNet Annotation Tool</title><categories>cs.HC cs.MM</categories><comments>5 pages, 3 figures. In the Proceedings of 38th International
  Convention on Information and Communication Technology, Electronics and
  Microelectronics MIPRO 2015 (pp. 1403-1407)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Geneva Affective Picture Database WordNet Annotation Tool (GWAT) is a
user-friendly web application for manual annotation of pictures in Geneva
Affective Picture Database (GAPED) with WordNet. The annotation tool has an
intuitive interface which can be efficiently used with very little technical
training. A single picture may be labeled with many synsets allowing experts to
describe semantics with different levels of detail. Noun, verb, adjective and
adverb synsets can be keyword-searched and attached to a specific GAPED picture
with their unique identification numbers. Changes are saved automatically in
the tool's relational database. The attached synsets can be reviewed, changed
or deleted later. Additionally, GAPED pictures may be browsed in the tool's
user interface using simple commands where previously attached WordNet synsets
are displayed alongside the pictures. Stored annotations can be exported from
the tool's database to different data formats and used in 3rd party
applications if needed. Since GAPED does not define keywords of individual
pictures but only a general category of picture groups, GWAT represents a
significant improvement towards development of comprehensive picture semantics.
The tool was developed with open technologies WordNet API, Apache, PHP5 and
MySQL. It is freely available for scientific and non-commercial use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07396</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07396</id><created>2015-05-27</created><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Kukolja</keyname><forenames>Davor</forenames></author><author><keyname>Ivanec</keyname><forenames>Dragutin</forenames></author></authors><title>Retrieval of multimedia stimuli with semantic and emotional cues:
  Suggestions from a controlled study</title><categories>cs.HC cs.IR</categories><comments>4 pages, 3 figures, 1 table. In the Proceedings of 38th International
  Convention on Information and Communication Technology, Electronics and
  Microelectronics MIPRO 2015 (pp. 1399-1402)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to efficiently search pictures with annotated semantics and
emotion is an important problem for Human-Computer Interaction with
considerable interdisciplinary significance. Accuracy and speed of the
multimedia retrieval process depends on the chosen metadata annotation model.
The quality of such multifaceted retrieval is opposed to the potential
complexity of data setup procedures and development of multimedia annotations.
Additionally, a recent study has shown that databases of emotionally annotated
multimedia are still being predominately searched manually which highlights the
need to study this retrieval modality. To this regard we present a study with N
= 75 participants aimed to evaluate the influence of keywords and dimensional
emotions in manual retrieval of pictures. The study showed that if the
multimedia database is comparatively small emotional annotations are sufficient
to achieve a fast retrieval despite comparatively lesser overall accuracy. In a
larger dataset semantic annotations became necessary for efficient retrieval
although they contributed to a slower beginning of the search process. The
experiment was performed in a controlled environment with a team of psychology
experts. The results were statistically consistent with validates measures of
the participants' perceptual speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07398</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07398</id><created>2015-05-27</created><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Kukolja</keyname><forenames>Davor</forenames></author><author><keyname>Ivanec</keyname><forenames>Dragutin</forenames></author></authors><title>Comparing affective responses to standardized pictures and videos: A
  study report</title><categories>cs.HC cs.MM</categories><comments>5 pages, 4 figures. In the Proceedings of 38th International
  Convention on Information and Communication Technology, Electronics and
  Microelectronics MIPRO 2015 (pp. 1394-1398)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimedia documents such as text, images, sounds or videos elicit emotional
responses of different polarity and intensity in exposed human subjects. These
stimuli are stored in affective multimedia databases. The problem of emotion
processing is an important issue in Human-Computer Interaction and different
interdisciplinary studies particularly those related to psychology and
neuroscience. Accurate prediction of users' attention and emotion has many
practical applications such as the development of affective computer
interfaces, multifaceted search engines, video-on-demand, Internet
communication and video games. To this regard we present results of a study
with N=10 participants to investigate the capability of standardized affective
multimedia databases in stimulation of emotion. Each participant was exposed to
picture and video stimuli with previously determined semantics and emotion.
During exposure participants' physiological signals were recorded and estimated
for emotion in an off-line analysis. Participants reported their emotion states
after each exposure session. The a posteriori and a priori emotion values were
compared. The experiment showed, among other reported results, that carefully
designed video sequences induce a stronger and more accurate emotional reaction
than pictures. Individual participants' differences greatly influence the
intensity and polarity of experienced emotion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07409</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07409</id><created>2015-05-27</created><authors><author><keyname>Ventura</keyname><forenames>Carles</forenames></author><author><keyname>Gir&#xf3;-i-Nieto</keyname><forenames>Xavier</forenames></author><author><keyname>Vilaplana</keyname><forenames>Ver&#xf3;nica</forenames></author><author><keyname>McGuinness</keyname><forenames>Kevin</forenames></author><author><keyname>Marqu&#xe9;s</keyname><forenames>Ferran</forenames></author><author><keyname>O'Connor</keyname><forenames>Noel E.</forenames></author></authors><title>Improving Spatial Codification in Semantic Segmentation</title><categories>cs.CV</categories><comments>Paper accepted at the IEEE International Conference on Image
  Processing, ICIP 2015. Quebec City, 27-30 September. Project page:
  https://imatge.upc.edu/web/publications/improving-spatial-codification-semantic-segmentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores novel approaches for improving the spatial codification
for the pooling of local descriptors to solve the semantic segmentation
problem. We propose to partition the image into three regions for each object
to be described: Figure, Border and Ground. This partition aims at minimizing
the influence of the image context on the object description and vice versa by
introducing an intermediate zone around the object contour. Furthermore, we
also propose a richer visual descriptor of the object by applying a Spatial
Pyramid over the Figure region. Two novel Spatial Pyramid configurations are
explored: Cartesian-based and crown-based Spatial Pyramids. We test these
approaches with state-of-the-art techniques and show that they improve the
Figure-Ground based pooling in the Pascal VOC 2011 and 2012 semantic
segmentation challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07416</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07416</id><created>2015-05-27</created><updated>2015-06-24</updated><authors><author><keyname>Fenner</keyname><forenames>Stephen A.</forenames></author><author><keyname>Rogers</keyname><forenames>John</forenames></author></authors><title>Combinatorial Game Complexity: An Introduction with Poset Games</title><categories>cs.CC</categories><comments>48 pages, 8 figures. This is the extended version of an article
  appearing in the Bulletin of the EATCS, 2015. New results (Lemma 2.21 and
  Proposition 2.30) and reference added</comments><report-no>CSE-TR-2015-001</report-no><acm-class>F.1.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poset games have been the object of mathematical study for over a century,
but little has been written on the computational complexity of determining
important properties of these games. In this introduction we develop the
fundamentals of combinatorial game theory and focus for the most part on poset
games, of which Nim is perhaps the best-known example. We present the
complexity results known to date, some discovered very recently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07417</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07417</id><created>2015-05-27</created><updated>2015-10-26</updated><authors><author><keyname>Ustyuzhanin</keyname><forenames>Andrey</forenames></author><author><keyname>Artemov</keyname><forenames>Alexey</forenames></author><author><keyname>Kazeev</keyname><forenames>Nikita</forenames></author><author><keyname>Redkin</keyname><forenames>Artem</forenames></author></authors><title>Event Index - an LHCb Event Search System</title><categories>hep-ex cs.DC</categories><comments>Report for the proceedings of the CHEP-2015 conference</comments><journal-ref>Journal of Physics: Conference Series, vol. 664, num 3, pages
  032019, 2015</journal-ref><doi>10.1088/1742-6596/664/3/032019</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  During LHC Run 1, the LHCb experiment recorded around $10^{11}$ collision
events. This paper describes Event Index - an event search system. Its primary
function is to quickly select subsets of events from a combination of
conditions, such as the estimated decay channel or number of hits in a
subdetector. Event Index is essentially Apache Lucene optimized for read-only
indexes distributed over independent shards on independent nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07427</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07427</id><created>2015-05-27</created><updated>2016-02-18</updated><authors><author><keyname>Kendall</keyname><forenames>Alex</forenames></author><author><keyname>Grimes</keyname><forenames>Matthew</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author></authors><title>PoseNet: A Convolutional Network for Real-Time 6-DOF Camera
  Relocalization</title><categories>cs.CV cs.NE cs.RO</categories><comments>9 pages, 13 figures; Corrected numerical error in orientation results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a robust and real-time monocular six degree of freedom
relocalization system. Our system trains a convolutional neural network to
regress the 6-DOF camera pose from a single RGB image in an end-to-end manner
with no need of additional engineering or graph optimisation. The algorithm can
operate indoors and outdoors in real time, taking 5ms per frame to compute. It
obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes
and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23
layer deep convnet, demonstrating that convnets can be used to solve
complicated out of image plane regression problems. This was made possible by
leveraging transfer learning from large scale classification data. We show the
convnet localizes from high level features and is robust to difficult lighting,
motion blur and different camera intrinsics where point based SIFT registration
fails. Furthermore we show how the pose feature that is produced generalizes to
other scenes allowing us to regress pose with only a few dozen training
examples. PoseNet code, dataset and an online demonstration is available on our
project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07428</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07428</id><created>2015-05-27</created><authors><author><keyname>Gomez-Ojeda</keyname><forenames>Ruben</forenames></author><author><keyname>Lopez-Antequera</keyname><forenames>Manuel</forenames></author><author><keyname>Petkov</keyname><forenames>Nicolai</forenames></author><author><keyname>Gonzalez-Jimenez</keyname><forenames>Javier</forenames></author></authors><title>Training a Convolutional Neural Network for Appearance-Invariant Place
  Recognition</title><categories>cs.CV cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Place recognition is one of the most challenging problems in computer vision,
and has become a key part in mobile robotics and autonomous driving
applications for performing loop closure in visual SLAM systems. Moreover, the
difficulty of recognizing a revisited location increases with appearance
changes caused, for instance, by weather or illumination variations, which
hinders the long-term application of such algorithms in real environments. In
this paper we present a convolutional neural network (CNN), trained for the
first time with the purpose of recognizing revisited locations under severe
appearance changes, which maps images to a low dimensional space where
Euclidean distances represent place dissimilarity. In order for the network to
learn the desired invariances, we train it with triplets of images selected
from datasets which present a challenging variability in visual appearance. The
triplets are selected in such way that two samples are from the same location
and the third one is taken from a different place. We validate our system
through extensive experimentation, where we demonstrate better performance than
state-of-art algorithms in a number of popular datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07429</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07429</id><created>2015-05-27</created><authors><author><keyname>Fox</keyname><forenames>Jacob</forenames></author><author><keyname>Pach</keyname><forenames>Janos</forenames></author><author><keyname>Suk</keyname><forenames>Andrew</forenames></author></authors><title>Semi-algebraic colorings of complete graphs</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider edge colorings of the complete graph, where the
vertices are points in $\mathbb{R}^d$, and each color class $E_i$ is defined by
a semi-algebraic relation of constant complexity on the point set. One of our
main results is a multicolor regularity lemma: For any $0&lt;\varepsilon&lt;1$, the
vertex set of any such edge colored complete graph with $m$ colors can be
equitably partitioned into at most $(m/\varepsilon)^c$ parts, such that all but
at most an $\varepsilon$-fraction of the pairs of parts are monochromatic
between them. Here $c&gt;0$ is a constant that depends on the dimension $d$ and
the complexity of the semi-algebraic relations. This generalizes a theorem of
Alon, Pach, Pinchasi, Radoi\v{c}i\'c and Sharir, and Fox, Pach, and Suk. As an
application, we prove the following result.
  For fixed integers $p$ and $q$ with $2\leq q \leq {p\choose 2}$, a
$(p,q)$-coloring is an edge-coloring of a complete graph in which every $p$
vertices induce at least $q$ distinct colors. The function $f^{\ast}(n,p,q)$ is
the minimum integer $m$ such that there is a $(p,q)$-coloring of $K_n$ with at
most $m$ colors, where the vertices of $K_n$ are points in $\mathbb{R}^d$, and
each color class can be defined as a semi-algebraic relation of constant
complexity. Here we show that $f^{\ast}(n,p,\lceil\log p\rceil + 1) \geq
\Omega\left(n^{\frac{1}{c\log^2p}}\right),$ and $f^{\ast}(n,p, \lceil\log
p\rceil) \leq O(\log n)$, thus determining the exact value of $q$ at which
$f^{\ast}(n,p,q)$ changes from logarithmic to polynomial in $n$.
  We also show the following result on a distinct distances problem of
Erd\H{o}s. Let $V$ be an $n$-element planar point set such that any $p$ members
of $V$ determine at least ${p\choose 2} - p + 6$ distinct distances. Then $V$
determines at least $ n^{\frac{8}{7} - o(1)}$ distinct distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07431</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07431</id><created>2015-05-27</created><authors><author><keyname>Pakrooh</keyname><forenames>Pooria</forenames></author><author><keyname>Scharf</keyname><forenames>Louis L.</forenames></author><author><keyname>Pezeshki</keyname><forenames>Ali</forenames></author></authors><title>Threshold Effects in Parameter Estimation from Compressed Data</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate threshold effects associated with swapping of
signal and noise subspaces in estimating signal parameters from compressed
noisy data. The term threshold effect refers to a sharp departure of
mean-squared error from the Cramer-Rao bound when the signal-to-noise ratio
falls below a threshold SNR. In many cases, the threshold effect is caused by a
subspace swap event, when the measured data (or its sample covariance) is
better approximated by a subset of components of an orthogonal subspace than by
the components of a signal subspace. We derive analytical lower bounds on the
probability of a subspace swap in compressively measured noisy data. These
bounds guide our understanding of threshold effects and performance breakdown
for parameter estimation using compression. As a case study, we investigate
threshold effects in maximum likelihood (ML) estimation of directions of
arrival of two closely-spaced sources using co-prime subsampling. Our results
show the impact of compression on threshold SNR. A rule of thumb is that every
doubling of compression ratio brings a penalty in threshold SNR of 3 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07432</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07432</id><created>2015-05-27</created><authors><author><keyname>Ji</keyname><forenames>Zhengfeng</forenames></author></authors><title>Classical Verification of Quantum Proofs</title><categories>quant-ph cs.CC</categories><comments>36 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a classical interactive protocol that verifies the validity of a
quantum witness state for the local Hamiltonian problem. It follows from this
protocol that approximating the non-local value of a multi-player one-round
game to inverse polynomial precision is QMA-hard. Our work makes an interesting
connection between the theory of QMA-completeness and Hamiltonian complexity on
one hand and the study of non-local games and Bell inequalities on the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07434</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07434</id><created>2015-05-27</created><authors><author><keyname>Ye</keyname><forenames>Qing Chuan</forenames></author><author><keyname>Zhang</keyname><forenames>Yingqian</forenames></author><author><keyname>Dekker</keyname><forenames>Rommert</forenames></author></authors><title>Fair task allocation in transportation</title><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task allocation problems have traditionally focused on cost optimization.
However, more and more attention is being given to cases in which cost should
not always be the sole or major consideration. In this paper, we study a fair
task allocation problem where an optimal allocation not only has low cost but
more importantly, it is max-lexmin fair to all participants. To tackle this
max-lexmin fair minimum cost allocation problem, we analyze and solve it in two
parts. We first aim to determine the maximum number of jobs that can be
feasibly done and the fairest distribution thereof. Since there may be many
allocations that are considered equally fair, we then find the allocation with
the minimum costs. We propose two novel polynomial-time algorithms that make
use of the network flow structure of the problem to obtain the optimal
solution. Furthermore, we conduct extensive experiments to investigate the
trade-off between cost minimization and fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07478</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07478</id><created>2015-05-27</created><authors><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author><author><keyname>Peixoto</keyname><forenames>Tiago P.</forenames></author></authors><title>Generalized communities in networks</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><comments>5 pages, 1 figure</comments><journal-ref>Phys. Rev. Lett. 115, 088701 (2015)</journal-ref><doi>10.1103/PhysRevLett.115.088701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A substantial volume of research has been devoted to studies of community
structure in networks, but communities are not the only possible form of
large-scale network structure. Here we describe a broad extension of community
structure that encompasses traditional communities but includes a wide range of
generalized structural patterns as well. We describe a principled method for
detecting this generalized structure in empirical network data and demonstrate
with real-world examples how it can be used to learn new things about the shape
and meaning of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07487</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07487</id><created>2015-05-27</created><authors><author><keyname>Chung</keyname><forenames>Eric</forenames></author><author><keyname>Joy</keyname><forenames>Joshua</forenames></author><author><keyname>Gerla</keyname><forenames>Mario</forenames></author></authors><title>DiscoverFriends: Secure Social Network Communication in Mobile Ad Hoc
  Networks</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a secure communication application called
DiscoverFriends. Its purpose is to communicate to a group of online friends
while bypassing their respective social networking servers under a mobile ad
hoc network environment. DiscoverFriends leverages Bloom filters and a hybrid
encryption technique with a self-organized public-key management scheme to
securely identify friends and provide authentication. Firstly, Bloom filters
provide a space-efficient means of security for friend discovery. Secondly, a
combination of asymmetric and symmetric encryptions algorithms utilizes both
benefits to provide increased security at lower computational cost. Thirdly, a
self-organized public-key management scheme helps authenticate users using a
trust graph in an infrastructureless setting. With the use of Wi-Fi Direct
technology, an initiator is able to establish an ad hoc network where friends
can connect to within the application. DiscoverFriends was analyzed under two
threat models: replay attacks and eavesdropping by a common friend. Finally,
the paper evaluates the application based on storage usage and processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07490</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07490</id><created>2015-05-27</created><authors><author><keyname>Chen</keyname><forenames>Hao</forenames></author></authors><title>Explicit RIP Matrices in Compressed Sensing from Algebraic Geometry</title><categories>cs.IT math.IT</categories><comments>21 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing was proposed by E. J. Cand\'es, J. Romberg, T. Tao, and D.
Donoho for efficient sampling of sparse signals in 2006 and has vast
applications in signal processing. The expicit restricted isometry property
(RIP) measurement matrices are needed in practice. Since 2007 R. DeVore, J.
Bourgain et al and R. Calderbank et al have given several deterministic
cosntrcutions of RIP matrices from various mathematical objects. On the other
hand the strong coherence property of a measurement matrix was introduced by
Bajwa and Calderbank et al for the recovery of signals under the noisy
measuremnt. In this paper we propose new explicit construction of real valued
RIP measurement matrices in compressed sensing from algebraic geometry. Our
construction indicates that using more general algebraic-geometric objects
rather than curves (AG codes), RIP measurement matrices in compressed sensing
can be constructed with much smaller coherence and much bigger sparsity orders.
The RIP matrices from algebraic geometry also have a nice asymptotic bound
matching the bound from the previous constructions of Bourgain et al and the
small-bias sets. On the negative side, we prove that the RIP matrices from
DeVore's construction, its direct algebraic geometric generalization and one of
our new construction do not satisfy the strong coherence property. However we
give a modified version of AG-RIP matrices which satisfies the strong coherence
property. Therefore the new RIP matrices in compressed sensing from our
modified algebraic geometric construction can be used for the recovery of
signals from the noisy measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07493</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07493</id><created>2015-05-27</created><authors><author><keyname>Szabo</keyname><forenames>Steve</forenames></author><author><keyname>Ulmer</keyname><forenames>Felix</forenames></author></authors><title>Dualilty Preserving Gray Maps: (Pseudo) Self-dual Bases and Symmetric
  Bases</title><categories>cs.IT math.IT math.RA</categories><msc-class>94B05, 94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite ring $A$ which is a free left module over a subring $R$ of
$A$, two types of $R$-bases are defined which in turn are used to define
duality preserving maps from codes over $A$ to codes over $R$. The first type,
pseudo-self-dual bases, are a generalization of trace orthogonal bases for
fields. The second are called symmetric bases. Both types are illustrated with
skew cyclic codes which are codes that are $A$-submodules of the skew
polynomial ring $A[X;\theta]/\langle X^n-1\rangle$ (the classical cyclic codes
are the case when $\theta=id$). When $A$ is commutative, there exists criteria
for a skew cyclic code over $A$ to be self-dual. With this criteria and a
duality preserving map, many self-dual codes over the subring $R$ can easily be
found. In this fashion, numerous examples are given, some of which are not
chain or serial rings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07499</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07499</id><created>2015-05-27</created><authors><author><keyname>Bindschaedler</keyname><forenames>Vincent</forenames></author><author><keyname>Shokri</keyname><forenames>Reza</forenames></author></authors><title>Privacy through Fake yet Semantically Real Traces</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Camouflaging data by generating fake information is a well-known obfuscation
technique for protecting data privacy. In this paper, we focus on a very
sensitive and increasingly exposed type of data: location data. There are two
main scenarios in which fake traces are of extreme value to preserve location
privacy: publishing datasets of location trajectories, and using location-based
services. Despite advances in protecting (location) data privacy, there is no
quantitative method to evaluate how realistic a synthetic trace is, and how
much utility and privacy it provides in each scenario. Also, the lack of a
methodology to generate privacy-preserving fake traces is evident. In this
paper, we fill this gap and propose the first statistical metric and model to
generate fake location traces such that both the utility of data and the
privacy of users are preserved. We build upon the fact that, although
geographically they visit distinct locations, people have strongly semantically
similar mobility patterns, for example, their transition pattern across
activities (e.g., working, driving, staying at home) is similar. We define a
statistical metric and propose an algorithm that automatically discovers the
hidden semantic similarities between locations from a bag of real location
traces as seeds, without requiring any initial semantic annotations. We
guarantee that fake traces are geographically dissimilar to their seeds, so
they do not leak sensitive location information. We also protect contributors
to seed traces against membership attacks. Interleaving fake traces with mobile
users' traces is a prominent location privacy defense mechanism. We
quantitatively show the effectiveness of our methodology in protecting against
localization inference attacks while preserving utility of sharing/publishing
traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07502</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07502</id><created>2015-05-27</created><authors><author><keyname>Usui</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Subramanian</keyname><forenames>Lavanya</forenames></author><author><keyname>Chang</keyname><forenames>Kevin</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>SQUASH: Simple QoS-Aware High-Performance Memory Scheduler for
  Heterogeneous Systems with Hardware Accelerators</title><categories>cs.AR</categories><report-no>SAFARI Technical Report No. 2015-003</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern SoCs integrate multiple CPU cores and Hardware Accelerators (HWAs)
that share the same main memory system, causing interference among memory
requests from different agents. The result of this interference, if not
controlled well, is missed deadlines for HWAs and low CPU performance.
State-of-the-art mechanisms designed for CPU-GPU systems strive to meet a
target frame rate for GPUs by prioritizing the GPU close to the time when it
has to complete a frame. We observe two major problems when such an approach is
adapted to a heterogeneous CPU-HWA system. First, HWAs miss deadlines because
they are prioritized only close to their deadlines. Second, such an approach
does not consider the diverse memory access characteristics of different
applications running on CPUs and HWAs, leading to low performance for
latency-sensitive CPU applications and deadline misses for some HWAs, including
GPUs.
  In this paper, we propose a Simple Quality of service Aware memory Scheduler
for Heterogeneous systems (SQUASH), that overcomes these problems using three
key ideas, with the goal of meeting deadlines of HWAs while providing high CPU
performance. First, SQUASH prioritizes a HWA when it is not on track to meet
its deadline any time during a deadline period. Second, SQUASH prioritizes HWAs
over memory-intensive CPU applications based on the observation that the
performance of memory-intensive applications is not sensitive to memory
latency. Third, SQUASH treats short-deadline HWAs differently as they are more
likely to miss their deadlines and schedules their requests based on worst-case
memory access time estimates.
  Extensive evaluations across a wide variety of different workloads and
systems show that SQUASH achieves significantly better CPU performance than the
best previous scheduler while always meeting the deadlines for all HWAs,
including GPUs, thereby largely improving frame rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07503</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07503</id><created>2015-05-27</created><updated>2015-12-02</updated><authors><author><keyname>Orsini</keyname><forenames>Chiara</forenames></author><author><keyname>Dankulov</keyname><forenames>Marija Mitrovi&#x107;</forenames></author><author><keyname>Jamakovic</keyname><forenames>Almerima</forenames></author><author><keyname>Mahadevan</keyname><forenames>Priya</forenames></author><author><keyname>Colomer-de-Sim&#xf3;n</keyname><forenames>Pol</forenames></author><author><keyname>Vahdat</keyname><forenames>Amin</forenames></author><author><keyname>Bassler</keyname><forenames>Kevin E.</forenames></author><author><keyname>Toroczkai</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Bogu&#xf1;&#xe1;</keyname><forenames>Mari&#xe1;n</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author></authors><title>Quantifying randomness in real networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.NI</categories><journal-ref>Nature Communications, v.6, p.8627, 2015</journal-ref><doi>10.1038/ncomms9627</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Represented as graphs, real networks are intricate combinations of order and
disorder. Fixing some of the structural properties of network models to their
values observed in real networks, many other properties appear as statistical
consequences of these fixed observables, plus randomness in other respects.
Here we employ the $dk$-series, a complete set of basic characteristics of the
network structure, to study the statistical dependencies between different
network properties. We consider six real networks---the Internet, US airport
network, human protein interactions, technosocial web of trust, English word
network, and an fMRI map of the human brain---and find that many important
local and global structural properties of these networks are closely reproduced
by $dk$-random graphs whose degree distributions, degree correlations, and
clustering are as in the corresponding real network. We discuss important
conceptual, methodological, and practical implications of this evaluation of
network randomness, and release software to generate $dk$-random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07508</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07508</id><created>2015-05-27</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>Valota</keyname><forenames>Diego</forenames></author></authors><title>Valuations in Nilpotent Minimum Logic</title><categories>cs.LO math.CO math.LO</categories><journal-ref>IEEE International Symposium on Multiple-Valued Logic (ISMVL), pp.
  90-95, 2015</journal-ref><doi>10.1109/ISMVL.2015.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Euler characteristic can be defined as a special kind of valuation on
finite distributive lattices. This work begins with some brief consideration on
the role of the Euler characteristic on NM algebras, the algebraic counterpart
of Nilpotent Minimum logic. Then, we introduce a new valuation, a modified
version of the Euler characteristic we call idempotent Euler characteristic. We
show that the new valuation encodes information about the formul{\ae} in NM
propositional logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07509</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07509</id><created>2015-05-27</created><authors><author><keyname>Arrazola</keyname><forenames>Juan Miguel</forenames></author><author><keyname>Wallden</keyname><forenames>Petros</forenames></author><author><keyname>Andersson</keyname><forenames>Erika</forenames></author></authors><title>Multiparty Quantum Signature Schemes</title><categories>quant-ph cs.CR</categories><comments>22 pages, 4 figures</comments><journal-ref>Quantum Inf. Comput. 6, 0435 (2016)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital signatures are widely used in electronic communications to secure
important tasks such as financial transactions, software updates, and legal
contracts. The signature schemes that are in use today are based on public-key
cryptography and derive their security from computational assumptions. However,
it is possible to construct unconditionally secure signature protocols. In
particular, using quantum communication, it is possible to construct signature
schemes with security based on fundamental principles of quantum mechanics.
Several quantum signature protocols have been proposed, but none of them has
been explicitly generalized to more than three participants, and their security
goals have not been formally defined. Here, we first extend the security
definitions of Swanson and Stinson (2011) so that they can apply also to the
quantum case, and introduce a formal definition of transferability based on
different verification levels. We then prove several properties that multiparty
signature protocols with information-theoretic security -- quantum or classical
-- must satisfy in order to achieve their security goals. We also express two
existing quantum signature protocols with three parties in the security
framework we have introduced. Finally, we generalize a quantum signature
protocol given in Wallden-Dunjko-Kent-Andersson (2015) to the multiparty case,
proving its security against forging, repudiation and non-transferability.
Notably, this protocol can be implemented using any point-to-point quantum key
distribution network and therefore is ready to be experimentally demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07515</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07515</id><created>2015-05-27</created><authors><author><keyname>Huang</keyname><forenames>Wentao</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Communication Efficient Secret Sharing</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A secret sharing scheme is a method to store information securely and
reliably. Particularly, in the threshold secret sharing scheme (due to Shamir),
a secret is divided into shares, encoded and distributed to parties, such that
any large enough collection of parties can decode the secret, and a smaller
(then threshold) set of parties cannot collude to deduce any information about
the secret. While Shamir's scheme was studied for more than 35 years, the
question of minimizing its communication bandwidth was not considered.
Specifically, assume that a user (or a collection of parties) wishes to decode
the secret by receiving information from a set of parties; the question we
study is how to minimize the total amount of communication between the user and
the parties. We prove a tight lower bound on the amount of communication
necessary for decoding, and construct secret sharing schemes achieving the
bound. The key idea for achieving optimal communication bandwidth is to let the
user receive information from more than the necessary number of parties. In
contrast, the current paradigm in secret sharing schemes is to decode from a
minimum set of parties. Hence, existing secret sharing schemes are not optimal
in terms of communication bandwidth. In addition, we consider secure
distributed storage where our proposed communication efficient secret sharing
schemes improve disk access complexity during decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07518</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07518</id><created>2015-05-27</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>The Kuenneth formula for graphs</title><categories>math.CO cs.CG cs.DM math.AT</categories><comments>60 pages 56 figures</comments><msc-class>05Cxx, 57M15, 55U10, 55N10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a Cartesian product G x H for finite simple graphs. It satisfies
the Kuenneth formula: H^k(G x H) is a direct sum of tensor products H^i(G) x
H^j(G) with i+j=k and so p(G x H,x) = p(G,x) p(H,y) for the Poincare polynomial
p(G,x) and X(G x H) = X(G) X(H) for the Euler characteristic X(G)=p(G,-1). G1=G
x K1 has as vertices the simplices of G and a natural digraph structure. We
show that dim(G1) is larger or equal than dim(G) and G1 is homotopic to G. The
Kuenneth identity is proven using Hodge describing the harmonic forms by the
product f g of harmonic forms of G and H and uses a discrete de Rham theorem
given by a combinatorial chain homotopy between simplicial and de Rham
cohomology. We show dim(G x H) = dim(G1) + dim(H1) implying that dim(G x H) is
larger or equal than dim(G) + dim(H) as for Hausdorff dimension in the
continuum. The chromatic number c(G1) is smaller or equal than c(G) and c(G x
H) is bounded above by c(G)+c(H)-1. The automorphism group of G x H contains
Aut(G) x Aut(H). If G~H and U~V then (G x U) ~ (H x V) if ~ means homotopic:
homotopy classes can be multiplided. If G is k-dimensional geometric meaning
that all unit spheres S(x) in G are (k-1)-discrete homotopy spheres, then G1 is
k-dimensional geometric. If G is k-dimensional geometric and H is l-dimensional
geometric, then G x H is geometric of dimension (l+k). The product extends to a
ring of chains which unlike the category of graphs is closed under boundary
operation taking quotients G/A with A subset Aut(G). As we can glue graphs or
chains, joins or fibre bundles can be defined with the same features as in the
continuum, allowing to build isomorphism classes of bundles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07519</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07519</id><created>2015-05-27</created><updated>2015-10-21</updated><authors><author><keyname>Pfeuffer</keyname><forenames>Julianus</forenames></author><author><keyname>Serang</keyname><forenames>Oliver</forenames></author></authors><title>A Bounded $p$-norm Approximation of Max-Convolution for Sub-Quadratic
  Bayesian Inference on Additive Factors</title><categories>stat.CO cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max-convolution is an important problem closely resembling standard
convolution; as such, max-convolution occurs frequently across many fields.
Here we extend the method with fastest known worst-case runtime, which can be
applied to nonnegative vectors by numerically approximating the Chebyshev norm
$\| \cdot \|_\infty$, and use this approach to derive two numerically stable
methods based on the idea of computing $p$-norms via fast convolution: The
first method proposed, with runtime in $O( k \log(k) \log(\log(k)) )$ (which is
less than $18 k \log(k)$ for any vectors that can be practically realized),
uses the $p$-norm as a direct approximation of the Chebyshev norm. The second
approach proposed, with runtime in $O( k \log(k) )$ (although in practice both
perform similarly), uses a novel null space projection method, which extracts
information from a sequence of $p$-norms to estimate the maximum value in the
vector (this is equivalent to querying a small number of moments from a
distribution of bounded support in order to estimate the maximum). The $p$-norm
approaches are compared to one another and are shown to compute an
approximation of the Viterbi path in a hidden Markov model where the transition
matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is
thus reduced from $O( n k^2 )$ steps to $O( n $k \log(k))$ steps in practice,
and is demonstrated by inferring the U.S. unemployment rate from the S&amp;P 500
stock index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07522</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07522</id><created>2015-05-27</created><authors><author><keyname>Redi</keyname><forenames>Miriam</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Graham</keyname><forenames>Lindsay T.</forenames></author><author><keyname>Gosling</keyname><forenames>Samuel D.</forenames></author></authors><title>Like Partying? Your Face Says It All. Predicting the Ambiance of Places
  with Profile Pictures</title><categories>cs.HC cs.CV cs.CY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To choose restaurants and coffee shops, people are increasingly relying on
social-networking sites. In a popular site such as Foursquare or Yelp, a place
comes with descriptions and reviews, and with profile pictures of people who
frequent them. Descriptions and reviews have been widely explored in the
research area of data mining. By contrast, profile pictures have received
little attention. Previous work showed that people are able to partly guess a
place's ambiance, clientele, and activities not only by observing the place
itself but also by observing the profile pictures of its visitors. Here we
further that work by determining which visual cues people may have relied upon
to make their guesses; showing that a state-of-the-art algorithm could make
predictions more accurately than humans at times; and demonstrating that the
visual cues people relied upon partly differ from those of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07534</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07534</id><created>2015-05-27</created><authors><author><keyname>Gu</keyname><forenames>Wenjun</forenames></author><author><keyname>Aminikashani</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Kavehrad</keyname><forenames>Mohsen</forenames></author></authors><title>Impact of Multipath Reflections on the Performance of Indoor Visible
  Light Positioning Systems</title><categories>cs.IT math.IT</categories><comments>Journal paper, 10 pages, 21 figures. arXiv admin note: text overlap
  with arXiv:1504.01192</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible light communication (VLC) using light-emitting-diodes (LEDs) has been
a popular research area recently. VLC can provide a practical solution for
indoor positioning. In this paper, the impact of multipath reflections on
indoor VLC positioning is investigated, considering a complex indoor
environment with walls, floor and ceiling. For the proposed positioning system,
an LED bulb is the transmitter and a photo-diode (PD) is the receiver to detect
received signal strength (RSS) information. Combined deterministic and modified
Monte Carlo (CDMMC) method is applied to compute the impulse response of the
optical channel. Since power attenuation is applied to calculate the distance
between the transmitter and receiver, the received power from each reflection
order is analyzed. Finally, the positioning errors are estimated for all the
locations over the room and compared with the previous works where no
reflections considered. Three calibration approaches are proposed to decrease
the effect of multipath reflections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07543</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07543</id><created>2015-05-28</created><updated>2016-02-08</updated><authors><author><keyname>Kawamoto</keyname><forenames>Tatsuro</forenames></author></authors><title>Localized eigenvectors of the non-backtracking matrix</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 5 figures, to be published from JSTAT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the case of graph partitioning, the emergence of localized eigenvectors
can cause the standard spectral method to fail. To overcome this problem, the
spectral method using a non-backtracking matrix was proposed. Based on
numerical experiments on several examples of real networks, it is clear that
the non-backtracking matrix does not exhibit localization of eigenvectors.
However, we show that localized eigenvectors of the non-backtracking matrix can
exist outside the spectral band, which may lead to deterioration in the
performance of graph partitioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07547</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07547</id><created>2015-05-28</created><authors><author><keyname>Kopparty</keyname><forenames>Swastik</forenames></author></authors><title>Some remarks on multiplicity codes</title><categories>cs.IT math.IT</categories><comments>21 pages in Discrete Geometry and Algebraic Combinatorics, AMS
  Contemporary Mathematics Series, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplicity codes are algebraic error-correcting codes generalizing
classical polynomial evaluation codes, and are based on evaluating polynomials
and their derivatives. This small augmentation confers upon them better local
decoding, list-decoding and local list-decoding algorithms than their classical
counterparts. We survey what is known about these codes, present some
variations and improvements, and finally list some interesting open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07548</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07548</id><created>2015-05-28</created><authors><author><keyname>Lou</keyname><forenames>Jian</forenames></author><author><keyname>Smith</keyname><forenames>Andrew M.</forenames></author><author><keyname>Vorobeychik</keyname><forenames>Yevgeniy</forenames></author></authors><title>Multidefender Security Games</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stackelberg security game models and associated computational tools have seen
deployment in a number of high-consequence security settings, such as LAX
canine patrols and Federal Air Marshal Service. These models focus on isolated
systems with only one defender, despite being part of a more complex system
with multiple players. Furthermore, many real systems such as transportation
networks and the power grid exhibit interdependencies between targets and,
consequently, between decision makers jointly charged with protecting them. To
understand such multidefender strategic interactions present in security, we
investigate game theoretic models of security games with multiple defenders.
Unlike most prior analysis, we focus on the situations in which each defender
must protect multiple targets, so that even a single defender's best response
decision is, in general, highly non-trivial. We start with an analytical
investigation of multidefender security games with independent targets,
offering an equilibrium and price-of-anarchy analysis of three models with
increasing generality. In all models, we find that defenders have the incentive
to over-protect targets, at times significantly. Additionally, in the simpler
models, we find that the price of anarchy is unbounded, linearly increasing
both in the number of defenders and the number of targets per defender.
Considering interdependencies among targets, we develop a novel mixed-integer
linear programming formulation to compute a defender's best response, and make
use of this formulation in approximating Nash equilibria of the game. We apply
this approach towards computational strategic analysis of several models of
networks representing interdependencies, including real-world power networks.
Our analysis shows how network structure and the probability of failure spread
determine the propensity of defenders to over- or under-invest in security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07553</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07553</id><created>2015-05-28</created><updated>2015-09-22</updated><authors><author><keyname>Guillevic</keyname><forenames>Aurore</forenames><affiliation>LIX, GRACE</affiliation></author></authors><title>Computing Individual Discrete Logarithms Faster in GF(p^n) with the
  NFS-DL Algorithm</title><categories>cs.CR math.NT</categories><proxy>ccsd</proxy><journal-ref>Tetsu Iwata; Jung Hee Cheon. Asiacrypt 2015, Nov 2015, Auckland,
  New Zealand. Springer, Asiacrypt 2015, 21st Annual International Conference
  on the Theory and Application of Cryptology and Information Security, 2015,
  \&amp;lt;https://www.math.auckland.ac.nz/~sgal018/AC2015/index.html\&amp;gt;</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Number Field Sieve (NFS) algorithm is the best known method tocompute
discrete logarithms (DL) in finite fields$\mathbf{F}\_{p^n}$, with $p$ medium
to large and $n \geq 1$ small. This algorithmcomprises four steps: polynomial
selection, relation collection,linear algebra and finally, individual logarithm
computation. Thefirst step outputs two polynomials defining two number fields,
and amap from the polynomial ring over the integers modulo each of
thesepolynomials to $\mathbf{F}\_{p^n}$. After the relation collection and
linear algebraphases, the (virtual) logarithm of a subset of elements in each
numberfield is known. Given the target element in $\mathbf{F}\_{p^n}$, the
fourthstep computes a preimage in one number field. If one can write thetarget
preimage as a product of elements of known (virtual) logarithm, then one can
deduce the discrete logarithm of the target. As recently shown by the Logjam
attack, this final step can becritical when it can be computed very quickly.But
we realized that computing an individual DL is much slower in medium-and
large-characteristic non-prime fields $\mathbf{F}\_{p^n}$ with $n \geq
3$,compared to prime fields and quadratic fields $\mathbf{F}\_{p^2}$. We
optimizethe first part of individual DL: the booting step, by
reducingdramatically the size of the preimage norm. Its smoothness probability
is higher, hence the running-time of thebooting step is much improved. Our
method is very efficient for small extension fields with $2 \leqn \leq 6$ and
applies to any $n \textgreater{} 1$, in medium and large characteristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07569</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07569</id><created>2015-05-28</created><updated>2015-12-07</updated><authors><author><keyname>Lu</keyname><forenames>Lu</forenames></author><author><keyname>Zhao</keyname><forenames>Haiquan</forenames></author><author><keyname>Li</keyname><forenames>Kan</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author></authors><title>A novel normalized sign algorithm for system identification under
  impulsive noise interference</title><categories>cs.SY</categories><comments>This paper has been accepted by Circuits, Systems, and Signal
  Processing. pp 1-22, First online: 17 November 2015</comments><doi>10.1007/s00034-015-0195-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To overcome the performance degradation of adaptive filtering algorithms in
the presence of impulsive noise, a novel normalized sign algorithm (NSA) based
on a convex combination strategy, called NSA-NSA, is proposed in this paper.
The proposed algorithm is capable of solving the conflicting requirement of
fast convergence rate and low steady-state error for an individual NSA filter.
To further improve the robustness to impulsive noises, a mixing parameter
updating formula based on a sign cost function is derived. Moreover, a tracking
weight transfer scheme of coefficients from a fast NSA filter to a slow NSA
filter is proposed to speed up the convergence rate. The convergence behavior
and performance of the new algorithm are verified by theoretical analysis and
simulation studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07570</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07570</id><created>2015-05-28</created><updated>2015-11-02</updated><authors><author><keyname>Wang</keyname><forenames>Shusen</forenames></author></authors><title>A Practical Guide to Randomized Matrix Computations with MATLAB
  Implementations</title><categories>cs.MS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix operations such as matrix inversion, eigenvalue decomposition,
singular value decomposition are ubiquitous in real-world applications.
Unfortunately, many of these matrix operations so time and memory expensive
that they are prohibitive when the scale of data is large. In real-world
applications, since the data themselves are noisy, machine-precision matrix
operations are not necessary at all, and one can sacrifice a reasonable amount
of accuracy for computational efficiency.
  In recent years, a bunch of randomized algorithms have been devised to make
matrix computations more scalable. Mahoney (2011) and Woodruff (2014) have
written excellent but very technical reviews of the randomized algorithms.
Differently, the focus of this manuscript is on intuition, algorithm
derivation, and implementation. This manuscript should be accessible to people
with knowledge in elementary matrix algebra but unfamiliar with randomized
matrix computations. The algorithms introduced in this manuscript are all
summarized in a user-friendly way, and they can be implemented in lines of
MATLAB code. The readers can easily follow the implementations even if they do
not understand the maths and algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07578</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07578</id><created>2015-05-28</created><updated>2015-05-29</updated><authors><author><keyname>Jeandel</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Enumeration in Closure Spaces with Applications to Algebra</title><categories>cs.DM cs.LO math.RA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how the concept of enumeration reducibility from comput-ability
theory may be applied to obtain computability conditions on descriptions of
substructures of algebras, more prominently on the language of forbidden words
of a subshift, and on the word problem for finitely generated groups. Many of
the results here are already known for recursively presented groups and
effectively closed subshifts, but using enumeration reducibility we are able to
generalize them to arbitrary objects. The proof is based on a topological
framework similar to universal algebra, and generalizes in particular work by
Kuznetsov and others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07589</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07589</id><created>2015-05-28</created><authors><author><keyname>Greif</keyname><forenames>Chen</forenames></author><author><keyname>He</keyname><forenames>Shiwen</forenames></author><author><keyname>Liu</keyname><forenames>Paul</forenames></author></authors><title>SYM-ILDL: Incomplete $LDL^{T}$ Factorization of Symmetric Indefinite and
  Skew-Symmetric Matrices</title><categories>cs.MS cs.NA math.NA</categories><comments>14 pages, 3 figures</comments><acm-class>F.2.1; G.1.0; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SYM-ILDL is a numerical software package that computes incomplete $LDL^{T}$
(or `ILDL') factorizations of symmetric indefinite and skew-symmetric matrices.
The core of the algorithm is a Crout variant of incomplete LU (ILU), originally
introduced and implemented for symmetric matrices by [Li and Saad, Crout
versions of ILU factorization with pivoting for sparse symmetric matrices,
Transactions on Numerical Analysis 20, pp. 75--85, 2005]. Our code is
economical in terms of storage and it deals with skew-symmetric matrices as
well, in addition to symmetric ones.
  The package is written in C++ and it is templated, open source, and includes
a Matlab interface. The code includes built-in RCM and AMD reordering, two
equilibration strategies, threshold Bunch-Kaufman pivoting and rook pivoting,
among other features. We also include an efficient MINRES implementation,
applied with a specialized symmetric positive definite preconditioning
technique based on the ILDL factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07593</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07593</id><created>2015-05-28</created><authors><author><keyname>Dutta</keyname><forenames>Vibekananda</forenames></author><author><keyname>Zielinska</keyname><forenames>Teresa</forenames></author></authors><title>Networking technologies for robotic applications</title><categories>cs.CY cs.CR cs.RO</categories><comments>Conference</comments><journal-ref>IJASCSE Volume 4, Issue 5 (May 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ongoing progress in networking security, together with the growing range
of robot applications in many fields of everyday life, makes robotics tangible
reality in our near future. Accordingly, new advanced services, depends on the
interplay between the robotics and cyber security, are being an important role
in robotics world. This paper addresses technological implications of security
enhancement to the Internet of Thing (IoT)-aided robotics domain, where
networked robots are expected to work in complex environments. The security
enhancement suggested by the NIST (National Institute of Standards and
Technology) creates a security template for secure communications over the
network are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07599</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07599</id><created>2015-05-28</created><updated>2015-06-30</updated><authors><author><keyname>Qiu</keyname><forenames>Xipeng</forenames></author><author><keyname>Qian</keyname><forenames>Peng</forenames></author><author><keyname>Yin</keyname><forenames>Liusong</forenames></author><author><keyname>Wu</keyname><forenames>Shiyu</forenames></author><author><keyname>Huang</keyname><forenames>Xuanjing</forenames></author></authors><title>Overview of the NLPCC 2015 Shared Task: Chinese Word Segmentation and
  POS Tagging for Micro-blog Texts</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give an overview for the shared task at the 4th CCF
Conference on Natural Language Processing \&amp; Chinese Computing (NLPCC 2015):
Chinese word segmentation and part-of-speech (POS) tagging for micro-blog
texts. Different with the popular used newswire datasets, the dataset of this
shared task consists of the relatively informal micro-texts. The shared task
has two sub-tasks: (1) individual Chinese word segmentation and (2) joint
Chinese word segmentation and POS Tagging. Each subtask has three tracks to
distinguish the systems with different resources. We first introduce the
dataset and task, then we characterize the different approaches of the
participating systems, report the test results, and provide a overview analysis
of these results. An online system is available for open registration and
evaluation at http://nlp.fudan.edu.cn/nlpcc2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07602</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07602</id><created>2015-05-28</created><authors><author><keyname>Chazal</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Massart</keyname><forenames>Pascal</forenames></author><author><keyname>Michel</keyname><forenames>Bertrand</forenames></author></authors><title>Rates of convergence for robust geometric inference</title><categories>math.ST cs.CG stat.TH</categories><msc-class>62G05, 62G30, 62-07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distances to compact sets are widely used in the field of Topological Data
Analysis for inferring geometric and topological features from point clouds. In
this context, the distance to a probability measure (DTM) has been introduced
by Chazal et al. (2011) as a robust alternative to the distance a compact set.
In practice, the DTM can be estimated by its empirical counterpart, that is the
distance to the empirical measure (DTEM). In this paper we give a tight control
of the deviation of the DTEM. Our analysis relies on a local analysis of
empirical processes. In particular, we show that the rates of convergence of
the DTEM directly depends on the regularity at zero of a particular quantile
fonction which contains some local information about the geometry of the
support. This quantile function is the relevant quantity to describe precisely
how difficult is a geometric inference problem. Several numerical experiments
illustrate the convergence of the DTEM and also confirm that our bounds are
tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07605</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07605</id><created>2015-05-28</created><authors><author><keyname>Meng</keyname><forenames>Hongyu</forenames></author><author><keyname>Guo</keyname><forenames>Fangjin</forenames></author></authors><title>Simple sorting algorithm test based on CUDA</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of computing technology, CUDA has become a very
important tool. In computer programming, sorting algorithm is widely used.
There are many simple sorting algorithms such as enumeration sort, bubble sort
and merge sort. In this paper, we test some simple sorting algorithm based on
CUDA and draw some useful conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07630</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07630</id><created>2015-05-28</created><authors><author><keyname>AlOnazi</keyname><forenames>Amani</forenames></author><author><keyname>Keyes</keyname><forenames>David</forenames></author><author><keyname>Lastovetsky</keyname><forenames>Alexey</forenames></author><author><keyname>Rychkov</keyname><forenames>Vladimir</forenames></author></authors><title>Design and Optimization of OpenFOAM-based CFD Applications for Hybrid
  and Heterogeneous HPC Platforms</title><categories>cs.DC</categories><comments>Presented at ParCFD 2014, prepared for submission to Computer and
  Fluids. 12 pages, 9 figures, 2 tables</comments><msc-class>68W10</msc-class><acm-class>G.1.8; C.1.4; I.3.1; G.4; G.1.0; F.1.2; D.1.3; C.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardware-aware design and optimization is crucial in exploiting emerging
architectures for PDE-based computational fluid dynamics applications. In this
work, we study optimizations aimed at acceleration of OpenFOAM-based
applications on emerging hybrid heterogeneous platforms. OpenFOAM uses MPI to
provide parallel multi-processor functionality, which scales well on
homogeneous systems but does not fully utilize the potential per-node
performance on hybrid heterogeneous platforms. In our study, we use two
OpenFOAM applications, icoFoam and laplacianFoam, both based on Krylov
iterative methods. We propose a number of optimizations of the dominant kernel
of the Krylov solver, aimed at acceleration of the overall execution of the
applications on modern GPU-accelerated heterogeneous platforms. Experimental
results show that the proposed hybrid implementation significantly outperforms
the state-of-the-art implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07634</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07634</id><created>2015-05-28</created><authors><author><keyname>van Rooyen</keyname><forenames>Brendan</forenames></author><author><keyname>Menon</keyname><forenames>Aditya Krishna</forenames></author><author><keyname>Williamson</keyname><forenames>Robert C.</forenames></author></authors><title>Learning with Symmetric Label Noise: The Importance of Being Unhinged</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex potential minimisation is the de facto approach to binary
classification. However, Long and Servedio [2010] proved that under symmetric
label noise (SLN), minimisation of any convex potential over a linear function
class can result in classification performance equivalent to random guessing.
This ostensibly shows that convex losses are not SLN-robust. In this paper, we
propose a convex, classification-calibrated loss and prove that it is
SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of
being negatively unbounded. The loss is a modification of the hinge loss, where
one does not clamp at zero; hence, we call it the unhinged loss. We show that
the optimal unhinged solution is equivalent to that of a strongly regularised
SVM, and is the limiting solution for any convex potential; this implies that
strong l2 regularisation makes most standard learners SLN-robust. Experiments
confirm the SLN-robustness of the unhinged loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07635</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07635</id><created>2015-05-28</created><authors><author><keyname>An</keyname><forenames>Xiaojing</forenames></author><author><keyname>Zhao</keyname><forenames>Haojun</forenames></author><author><keyname>Ding</keyname><forenames>Lulu</forenames></author><author><keyname>Fan</keyname><forenames>Zhongrui</forenames></author><author><keyname>Wang</keyname><forenames>Hanyue</forenames></author></authors><title>Optimized Password Recovery for Encrypted RAR on GPUs</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RAR uses classic symmetric encryption algorithm SHA-1 hashing and AES
algorithm for encryption, and the only method of password recovery is brute
force, which is very time-consuming. In this paper, we present an approach
using GPUs to speed up the password recovery process. However, because the
major calculation and time-consuming part, SHA-1 hashing, is hard to be
parallelized, so this paper adopts coarse granularity parallel. That is, one
GPU thread is responsible for the validation of one password. We mainly use
three optimization methods to optimize this parallel version: asynchronous
parallel between CPU and GPU, redundant calculations and conditional statements
reduction, and the usage of registers optimization. Experiment result shows
that the final version reaches 43~57 times speedup on an AMD FirePro W8000 GPU,
compared to a well-optimized serial version on Intel Core i5 CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07647</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07647</id><created>2015-05-28</created><updated>2015-10-13</updated><authors><author><keyname>Jing</keyname><forenames>Yushi</forenames></author><author><keyname>Liu</keyname><forenames>David</forenames></author><author><keyname>Kislyuk</keyname><forenames>Dmitry</forenames></author><author><keyname>Zhai</keyname><forenames>Andrew</forenames></author><author><keyname>Xu</keyname><forenames>Jiajing</forenames></author><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Tavel</keyname><forenames>Sarah</forenames></author></authors><title>Visual Search at Pinterest</title><categories>cs.CV</categories><comments>in Proceedings of the 21th ACM SIGKDD International Conference on
  Knowledge and Discovery and Data Mining, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that, with the availability of distributed computation
platforms such as Amazon Web Services and open-source tools, it is possible for
a small engineering team to build, launch and maintain a cost-effective,
large-scale visual search system with widely available tools. We also
demonstrate, through a comprehensive set of live experiments at Pinterest, that
content recommendation powered by visual search improve user engagement. By
sharing our implementation details and the experiences learned from launching a
commercial visual search engines from scratch, we hope visual search are more
widely incorporated into today's commercial applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07648</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07648</id><created>2015-05-28</created><authors><author><keyname>Tsitsiklis</keyname><forenames>John N.</forenames></author><author><keyname>Xu</keyname><forenames>Kuang</forenames></author></authors><title>Flexible Queueing Architectures</title><categories>math.PR cs.NI cs.PF</categories><comments>A preliminary version of this paper appeared at the 2013 ACM
  Sigmetrics conference; the performance of the architectures proposed in the
  current paper is significantly better than the one in the conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a multi-server model with $n$ flexible servers and $n$ queues,
connected through a bipartite graph, where the level of flexibility is captured
by the graph's average degree, $d_n$. Applications in content replication in
data centers, skill-based routing in call centers, and flexible supply chains
are among our main motivations.
  We focus on the scaling regime where the system size $n$ tends to infinity,
while the overall traffic intensity stays fixed. We show that a large capacity
region and an asymptotically vanishing queueing delay are simultaneously
achievable even under limited flexibility ($d_n \ll n$). Our main results
demonstrate that, when $d_n\gg \ln n$, a family of expander-graph-based
flexibility architectures has a capacity region that is within a constant
factor of the maximum possible, while simultaneously ensuring a diminishing
queueing delay for all arrival rate vectors in the capacity region. Our
analysis is centered around a new class of virtual-queue-based scheduling
policies that rely on dynamically constructed job-to-server assignments on the
connectivity graph. For comparison, we also analyze a natural family of modular
architectures, which is simpler but has provably weaker performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07672</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07672</id><created>2015-05-28</created><authors><author><keyname>Ludtke</keyname><forenames>Niklas</forenames></author><author><keyname>Das</keyname><forenames>Debapriya</forenames></author><author><keyname>Theis</keyname><forenames>Lucas</forenames></author><author><keyname>Bethge</keyname><forenames>Matthias</forenames></author></authors><title>A Generative Model of Natural Texture Surrogates</title><categories>cs.CV</categories><comments>34 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural images can be viewed as patchworks of different textures, where the
local image statistics is roughly stationary within a small neighborhood but
otherwise varies from region to region. In order to model this variability, we
first applied the parametric texture algorithm of Portilla and Simoncelli to
image patches of 64X64 pixels in a large database of natural images such that
each image patch is then described by 655 texture parameters which specify
certain statistics, such as variances and covariances of wavelet coefficients
or coefficient magnitudes within that patch.
  To model the statistics of these texture parameters, we then developed
suitable nonlinear transformations of the parameters that allowed us to fit
their joint statistics with a multivariate Gaussian distribution. We find that
the first 200 principal components contain more than 99% of the variance and
are sufficient to generate textures that are perceptually extremely close to
those generated with all 655 components. We demonstrate the usefulness of the
model in several ways: (1) We sample ensembles of texture patches that can be
directly compared to samples of patches from the natural image database and can
to a high degree reproduce their perceptual appearance. (2) We further
developed an image compression algorithm which generates surprisingly accurate
images at bit rates as low as 0.14 bits/pixel. Finally, (3) We demonstrate how
our approach can be used for an efficient and objective evaluation of samples
generated with probabilistic models of natural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07673</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07673</id><created>2015-05-28</created><authors><author><keyname>Ba&#xf1;os</keyname><forenames>Alfonso</forenames></author><author><keyname>Mulero</keyname><forenames>Juan I.</forenames></author><author><keyname>Barreiro</keyname><forenames>Antonio</forenames></author><author><keyname>Dav&#xf3;</keyname><forenames>Miguel A.</forenames></author></authors><title>An impulsive dynamical systems framework for reset control systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Impulsive dynamical systems is a well-established area of dynamical systems
theory, and it is used in this work to analyze several basic properties of
reset control systems: existence and uniqueness of solutions, and continuous
dependence on the initial condition (well-posedness). The work scope is about
reset control systems with a linear and time-invariant base system, and a
zero-crossing resetting law. A necessary and sufficient condition for existence
and uniqueness of solutions, based on the well-posedness of reset instants, is
developed. As a result, it is shown that reset control systems (with strictly
proper plants) do no have Zeno solutions. It is also shown that full reset and
partial reset (with a special structure) always produce well-posed reset
instants. Moreover, a definition of continuous dependence on the initial
condition is developed, and also a sufficient condition for reset control
systems to satisfy that property. Finally, this property is used to analyze
sensitivity of reset control systems to sensor noise. This work also includes a
number of illustrative examples motivating the key concepts and main results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07675</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07675</id><created>2015-05-28</created><authors><author><keyname>Yang</keyname><forenames>Weixin</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author><author><keyname>Xie</keyname><forenames>Zecheng</forenames></author><author><keyname>Feng</keyname><forenames>Ziyong</forenames></author></authors><title>Improved Deep Convolutional Neural Network For Online Handwritten
  Chinese Character Recognition using Domain-Specific Knowledge</title><categories>cs.CV</categories><comments>5 pages, 4 figures, 3 tables. Accepted to appear at ICDAR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks (DCNNs) have achieved great success in
various computer vision and pattern recognition applications, including those
for handwritten Chinese character recognition (HCCR). However, most current
DCNN-based HCCR approaches treat the handwritten sample simply as an image
bitmap, ignoring some vital domain-specific information that may be useful but
that cannot be learnt by traditional networks. In this paper, we propose an
enhancement of the DCNN approach to online HCCR by incorporating a variety of
domain-specific knowledge, including deformation, non-linear normalization,
imaginary strokes, path signature, and 8-directional features. Our contribution
is twofold. First, these domain-specific technologies are investigated and
integrated with a DCNN to form a composite network to achieve improved
performance. Second, the resulting DCNNs with diversity in their domain
knowledge are combined using a hybrid serial-parallel (HSP) strategy.
Consequently, we achieve a promising accuracy of 97.20% and 96.87% on
CASIA-OLHWDB1.0 and CASIA-OLHWDB1.1, respectively, outperforming the best
results previously reported in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07676</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07676</id><created>2015-05-28</created><authors><author><keyname>Kovnatsky</keyname><forenames>Artiom</forenames></author><author><keyname>Glashoff</keyname><forenames>Klaus</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author></authors><title>MADMM: a generic algorithm for non-smooth optimization on manifolds</title><categories>math.OC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous problems in machine learning are formulated as optimization with
manifold constraints. In this paper, we propose the Manifold alternating
directions method of multipliers (MADMM), an extension of the classical ADMM
scheme for manifold-constrained non-smooth optimization problems and show its
application to several challenging problems in dimensionality reduction, data
analysis, and manifold learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07683</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07683</id><created>2015-05-28</created><authors><author><keyname>Ziegeldorf</keyname><forenames>Jan Henrik</forenames></author><author><keyname>Morchon</keyname><forenames>Oscar Garcia</forenames></author><author><keyname>Wehrle</keyname><forenames>Klaus</forenames></author></authors><title>Privacy in the Internet of Things: Threats and Challenges</title><categories>cs.CR cs.CY</categories><comments>Security &amp; Communications Networks 2014</comments><doi>10.1002/sec.795</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things paradigm envisions the pervasive interconnection and
cooperation of smart things over the current and future Internet
infrastructure. The Internet of Things is, thus, the evolution of the Internet
to cover the real-world, enabling many new services that will improve people's
everyday lives, spawn new businesses and make buildings, cities and transport
smarter. Smart things allow indeed for ubiquitous data collection or tracking,
but these useful features are also examples of privacy threats that are already
now limiting the success of the Internet of Things vision when not implemented
correctly. These threats involve new challenges such as the pervasive
privacy-aware management of personal data or methods to control or avoid
ubiquitous tracking and profiling. This paper analyzes the privacy issues in
the Internet of Things in detail. To this end, we first discuss the evolving
features and trends in the Internet of Things with the goal of scrutinizing
their privacy implications. Second, we classify and examine privacy threats in
this new setting, pointing out the challenges that need to be overcome to
ensure that the Internet of Things becomes a reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07690</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07690</id><created>2015-05-28</created><authors><author><keyname>Janssen</keyname><forenames>Michiel</forenames></author><author><keyname>Duits</keyname><forenames>Remco</forenames></author><author><keyname>Breeuwer</keyname><forenames>Marcel</forenames></author></authors><title>Invertible Orientation Scores of 3D Images</title><categories>math.NA cs.CV</categories><comments>ssvm 2015 published version in LNCS contains a mistake (a switch
  notation spherical angles) that is corrected in this arxiv version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enhancement and detection of elongated structures in noisy image data is
relevant for many biomedical applications. To handle complex crossing
structures in 2D images, 2D orientation scores were introduced, which already
showed their use in a variety of applications. Here we extend this work to 3D
orientation scores. First, we construct the orientation score from a given
dataset, which is achieved by an invertible coherent state type of transform.
For this transformation we introduce 3D versions of the 2D cake-wavelets, which
are complex wavelets that can simultaneously detect oriented structures and
oriented edges. For efficient implementation of the different steps in the
wavelet creation we use a spherical harmonic transform. Finally, we show some
first results of practical applications of 3D orientation scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07702</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07702</id><created>2015-05-28</created><authors><author><keyname>Mouatadid</keyname><forenames>Lalla</forenames></author><author><keyname>Robere</keyname><forenames>Robert</forenames></author></authors><title>Path Graphs, Clique Trees, and Flowers</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An \emph{asteroidal triple} is a set of three independent vertices in a graph
such that any two vertices in the set are connected by a path which avoids the
neighbourhood of the third.
  A classical result by Lekkerkerker and Boland \cite{6} showed that interval
graphs are precisely the chordal graphs that do not have asteroidal triples.
  Interval graphs are chordal, as are the \emph{directed path graphs} and the
\emph{path graphs}.
  Similar to Lekkerkerker and Boland, Cameron, Ho\'{a}ng, and L\'{e}v\^{e}que
\cite{4} gave a characterization of directed path graphs by a &quot;special type&quot; of
asteroidal triple, and asked whether or not there was such a characterization
for path graphs.
  We give strong evidence that asteroidal triples alone are insufficient to
characterize the family of path graphs, and give a new characterization of path
graphs via a forbidden induced subgraph family that we call \emph{sun systems}.
  Key to our new characterization is the study of \emph{asteroidal sets} in sun
systems, which are a natural generalization of asteroidal triples.
  Our characterization of path graphs by forbidding sun systems also
generalizes a characterization of directed path graphs by forbidding odd suns
that was given by Chaplick et al.~\cite{9}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07712</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07712</id><created>2015-05-28</created><authors><author><keyname>Werner</keyname><forenames>Eric</forenames></author></authors><title>A Category Theory of Communication Theory</title><categories>cs.IT cs.CL cs.LO math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A theory of how agents can come to understand a language is presented. If
understanding a sentence $\alpha$ is to associate an operator with $\alpha$
that transforms the representational state of the agent as intended by the
sender, then coming to know a language involves coming to know the operators
that correspond to the meaning of any sentence. This involves a higher order
operator that operates on the possible transformations that operate on the
representational capacity of the agent. We formalize these constructs using
concepts and diagrams analogous to category theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07716</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07716</id><created>2015-05-28</created><authors><author><keyname>Doerfert</keyname><forenames>Johannes</forenames></author><author><keyname>Streit</keyname><forenames>Kevin</forenames></author><author><keyname>Hack</keyname><forenames>Sebastian</forenames></author><author><keyname>Benaissa</keyname><forenames>Zino</forenames></author></authors><title>Polly's Polyhedral Scheduling in the Presence of Reductions</title><categories>cs.PL cs.DC</categories><comments>Presented at the IMPACT15 workshop</comments><acm-class>D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The polyhedral model provides a powerful mathematical abstraction to enable
effective optimization of loop nests with respect to a given optimization goal,
e.g., exploiting parallelism. Unexploited reduction properties are a frequent
reason for polyhedral optimizers to assume parallelism prohibiting dependences.
To our knowledge, no polyhedral loop optimizer available in any production
compiler provides support for reductions. In this paper, we show that
leveraging the parallelism of reductions can lead to a significant performance
increase. We give a precise, dependence based, definition of reductions and
discuss ways to extend polyhedral optimization to exploit the associativity and
commutativity of reduction computations. We have implemented a
reduction-enabled scheduling approach in the Polly polyhedral optimizer and
evaluate it on the standard Polybench 3.2 benchmark suite. We were able to
detect and model all 52 arithmetic reductions and achieve speedups up to
2.21$\times$ on a quad core machine by exploiting the multidimensional
reduction in the BiCG benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07717</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07717</id><created>2015-05-28</created><updated>2015-10-08</updated><authors><author><keyname>Farias</keyname><forenames>Rodrigo Cabral</forenames></author><author><keyname>Cohen</keyname><forenames>Jeremy Emile</forenames></author><author><keyname>Comon</keyname><forenames>Pierre</forenames></author></authors><title>Exploring multimodal data fusion through joint decompositions with
  flexible couplings</title><categories>cs.IT math.IT</categories><comments>13 pages, 6 figures, revised version</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A Bayesian framework is proposed to define flexible coupling models for joint
tensor decompositions of multiple data sets. Under this framework, a natural
formulation of the data fusion problem is to cast it in terms of a joint
maximum a posteriori (MAP) estimator. Data driven scenarios of joint posterior
distributions are provided, including general Gaussian priors and non Gaussian
coupling priors. We present and discuss implementation issues of algorithms
used to obtain the joint MAP estimator. We also show how this framework can be
adapted to tackle the problem of joint decompositions of large datasets. In the
case of a conditional Gaussian coupling with a linear transformation, we give
theoretical bounds on the data fusion performance using the Bayesian Cramer-Rao
bound. Simulations are reported for hybrid coupling models ranging from simple
additive Gaussian models, to Gamma-type models with positive variables and to
the coupling of data sets which are inherently of different size due to
different resolution of the measurement devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07725</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07725</id><created>2015-05-28</created><updated>2015-06-02</updated><authors><author><keyname>Hsiao-feng</keyname><affiliation>Francis</affiliation></author><author><keyname>Lu</keyname></author><author><keyname>Elia</keyname><forenames>Petros</forenames></author><author><keyname>Singh</keyname><forenames>Arun</forenames></author></authors><title>Performance-Complexity Analysis for MAC ML-based Decoding with User
  Selection</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Signal Processing for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work explores the rate-reliability-complexity limits of the quasi-static
K-user multiple access channel (MAC), with or without feedback. Using high-SNR
asymptotics, the work first derives bounds on the computational resources
required to achieve near-optimal (ML-based) decoding performance. It then
bounds the (reduced) complexity needed to achieve any (including suboptimal)
diversity-multiplexing performance tradeoff (DMT) performance, and finally
bounds the same complexity, in the presence of feedback-aided user selection.
This latter effort reveals the ability of a few bits of feedback not only to
improve performance, but also to reduce complexity. In this context, the
analysis reveals the interesting finding that proper calibration of user
selection can allow for near-optimal ML-based decoding, with complexity that
need not scale exponentially in the total number of codeword bits. The derived
bounds constitute the best known performance-vs-complexity behavior to date for
ML-based MAC decoding, as well as a first exploration of the
complexity-feedback-performance interdependencies in multiuser settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07726</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07726</id><created>2015-05-28</created><updated>2015-12-22</updated><authors><author><keyname>Xiang</keyname><forenames>Can</forenames></author></authors><title>Linear Codes from a Generic Construction</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1503.06511,
  arXiv:1503.06512 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generic construction of linear codes over finite fields has recently
received a lot of attention, and many one-weight, two-weight and three-weight
codes with good error correcting capability have been produced with this
generic approach. The first objective of this paper is to establish
relationships among some classes of linear codes obtained with this approach,
so that the parameters of some classes of linear codes can be derived from
those of other classes with known parameters. In this way, linear codes with
new parameters will be derived. The second is to present a class of
three-weight binary codes and consider their applications in secret sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07734</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07734</id><created>2015-05-28</created><updated>2015-12-04</updated><authors><author><keyname>Hunold</keyname><forenames>Sascha</forenames></author><author><keyname>Carpen-Amarie</keyname><forenames>Alexandra</forenames></author></authors><title>MPI Benchmarking Revisited: Experimental Design and Reproducibility</title><categories>cs.DC</categories><comments>40 pages, 45 figures</comments><acm-class>D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Message Passing Interface (MPI) is the prevalent programming model used
on today's supercomputers. Therefore, MPI library developers are looking for
the best possible performance (shortest run-time) of individual MPI functions
across many different supercomputer architectures. Several MPI benchmark suites
have been developed to assess the performance of MPI implementations.
Unfortunately, the outcome of these benchmarks is often neither reproducible
nor statistically sound. To overcome these issues, we show which experimental
factors have an impact on the run-time of blocking collective MPI operations
and how to control them. We address the problem of process and clock
synchronization in MPI benchmarks. Finally, we present a new experimental
method that allows us to obtain reproducible and statistically sound MPI
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07736</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07736</id><created>2015-05-28</created><updated>2015-05-29</updated><authors><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Wi&#xdf;mann</keyname><forenames>Thorsten</forenames></author></authors><title>Finitary Corecursion for the Infinitary Lambda Calculus</title><categories>math.CT cs.LO</categories><acm-class>F.3.2; F.4.1; D.3.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Kurz et al. have recently shown that infinite $\lambda$-trees with finitely
many free variables modulo $\alpha$-equivalence form a final coalgebra for a
functor on the category of nominal sets. Here we investigate the rational
fixpoint of that functor. We prove that it is formed by all rational
$\lambda$-trees, i.e. those $\lambda$-trees which have only finitely many
subtrees (up to isomorphism). This yields a corecursion principle that allows
the definition of operations such as substitution on rational $\lambda$-trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07737</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07737</id><created>2015-05-28</created><authors><author><keyname>Kirousis</keyname><forenames>Lefteris</forenames></author><author><keyname>Kolaitis</keyname><forenames>Phokion G.</forenames></author></authors><title>Aggregation of Votes with Multiple Positions on Each Issue</title><categories>math.CO cs.DM cs.GT math.LO</categories><msc-class>91B12, 91B14</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of aggregating votes cast by a society on a fixed set
of issues, where each member of the society may vote for one of several
positions on each issue, but the combination of votes on the various issues is
restricted to a feasible set of voting patterns. Our main result asserts that,
in such a set-up, non-dictatorial aggregation of votes in a society of
arbitrary size is possible if and only non-dictatorial aggregation is possible
in a society with just three members.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07751</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07751</id><created>2015-05-27</created><authors><author><keyname>Sudano</keyname><forenames>John J.</forenames></author></authors><title>Pignistic Probability Transforms for Mixes of Low- and High-Probability
  Events</title><categories>cs.AI</categories><comments>7 pages, International Society of Information Fusion Conference
  Proceedings Fusion 2001 at Montreal, Quebec, Canada</comments><journal-ref>Fourth International Conference on Information Fusion, August
  2001, Montreal. Pages TPUB3 23-27</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some real world information fusion situations, time critical decisions
must be made with an incomplete information set. Belief function theories
(e.g., Dempster-Shafer theory of evidence, Transferable Belief Model) have been
shown to provide a reasonable methodology for processing or fusing the
quantitative clues or information measurements that form the incomplete
information set. For decision making, the pignistic (from the Latin pignus, a
bet) probability transform has been shown to be a good method of using Beliefs
or basic belief assignments (BBAs) to make decisions. For many systems, one
need only address the most-probable elements in the set. For some critical
systems, one must evaluate the risk of wrong decisions and establish safe
probability thresholds for decision making. This adds a greater complexity to
decision making, since one must address all elements in the set that are above
the risk decision threshold. The problem is greatly simplified if most of the
probabilities fall below this threshold. Finding a probability transform that
properly represents mixes of low- and high-probability events is essential.
This article introduces four new pignistic probability transforms with an
implementation that uses the latest values of Beliefs, Plausibilities, or BBAs
to improve the pignistic probability estimates. Some of them assign smaller
values of probabilities for smaller values of Beliefs or BBAs than the Smets
pignistic transform. They also assign higher probability values for larger
values of Beliefs or BBAs than the Smets pignistic transform. These probability
transforms will assign a value of probability that converges faster to the
values below the risk threshold. A probability information content (PIC)
variable is also introduced that assigns an information content value to any
set of probability. Four operators are defined to help simplify the
derivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07757</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07757</id><created>2015-05-28</created><authors><author><keyname>Naumann</keyname><forenames>Matthias</forenames></author><author><keyname>Wendzel</keyname><forenames>Steffen</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Keller</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Micro protocol engineering for unstructured carriers: On the embedding
  of steganographic control protocols into audio transmissions</title><categories>cs.MM cs.CY</categories><comments>20 pages, 7 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network steganography conceals the transfer of sensitive information within
unobtrusive data in computer networks. So-called micro protocols are
communication protocols placed within the payload of a network steganographic
transfer. They enrich this transfer with features such as reliability, dynamic
overlay routing, or performance optimization --- just to mention a few. We
present different design approaches for the embedding of hidden channels with
micro protocols in digitized audio signals under consideration of different
requirements. On the basis of experimental results, our design approaches are
compared, and introduced into a protocol engineering approach for micro
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07766</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07766</id><created>2015-05-28</created><updated>2015-06-09</updated><authors><author><keyname>Usevich</keyname><forenames>Konstantin</forenames></author><author><keyname>Comon</keyname><forenames>Pierre</forenames></author></authors><title>Quasi-Hankel low-rank matrix completion: a convex relaxation</title><categories>math.OC cs.IT math.IT math.NA</categories><comments>28 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The completion of matrices with missing values under the rank constraint is a
non-convex optimization problem. A popular convex relaxation is based on
minimization of the nuclear norm (sum of singular values) of the matrix. For
this relaxation, an important question is whether the two optimization problems
lead to the same solution. This question was addressed in the literature mostly
in the case of random positions of missing elements and random known elements.
In this contribution, we analyze the case of structured matrices with fixed
pattern of missing values, in particular, the case of Hankel and quasi-Hankel
matrix completion, which appears as a subproblem in the computation of
symmetric tensor canonical polyadic decomposition. We extend existing results
on completion of rank-one real Hankel matrices to completion of rank-r complex
Hankel and quasi-Hankel matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07772</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07772</id><created>2015-05-28</created><authors><author><keyname>Jarczyk</keyname><forenames>Oskar</forenames></author></authors><title>Mobile crowdsourcing - activation of smartphones users to elicit
  specialized knowledge through worker profile match</title><categories>cs.SI cs.CY</categories><comments>PJIIT WDSIT 2014 - Workshop for doctoral students and young
  researchers in Information Technology, Kazimierz Dolny, Poland</comments><acm-class>H.5.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Crowdsourcing models applied to work on mobile devices continuously reach new
ways of solving sophisticated problems, now with a use of portable advanced
devices, where users are not limited to a stationary use. There exists an open
problem of quality in crowdsourcing models due the inexperienced or malicious
workers. In this paper, we propose a model and a short specification of a
platform for a bundled widely available crowdsourcing mechanism, which tries to
utilize workers individual characteristics to maximum. Analyzed solution relies
on geographical data classified by localization category. Secondly, we profile
mobile workers by precisely analyzing their activity history. Results of this
research will make an impact on better understanding the latent potential of
mobile devices users. It makes for not only better quality in results, but also
opens a possibility of implementing a &quot;twitch crowdsourcing&quot; or emergency
relief systems. Special tasks assigned to owners of mobile devices can help
those, which are in need of help, making them the task creators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07774</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07774</id><created>2015-05-28</created><updated>2015-09-04</updated><authors><author><keyname>Ateniese</keyname><forenames>Giuseppe</forenames></author><author><keyname>Hitaj</keyname><forenames>Briland</forenames></author><author><keyname>Mancini</keyname><forenames>Luigi V.</forenames></author><author><keyname>Verde</keyname><forenames>Nino V.</forenames></author><author><keyname>Villani</keyname><forenames>Antonio</forenames></author></authors><title>No Place to Hide that Bytes won't Reveal: Sniffing Location-Based
  Encrypted Traffic to Track a User's Position</title><categories>cs.CR</categories><comments>14 pages, 9th International Conference on Network and System Security
  (NSS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  News reports of the last few years indicated that several intelligence
agencies are able to monitor large networks or entire portions of the Internet
backbone. Such a powerful adversary has only recently been considered by the
academic literature. In this paper, we propose a new adversary model for
Location Based Services (LBSs). The model takes into account an unauthorized
third party, different from the LBS provider itself, that wants to infer the
location and monitor the movements of a LBS user. We show that such an
adversary can extrapolate the position of a target user by just analyzing the
size and the timing of the encrypted traffic exchanged between that user and
the LBS provider. We performed a thorough analysis of a widely deployed
location based app that comes pre-installed with many Android devices:
GoogleNow. The results are encouraging and highlight the importance of devising
more effective countermeasures against powerful adversaries to preserve the
privacy of LBS users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07777</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07777</id><created>2015-05-28</created><authors><author><keyname>Rodrigues</keyname><forenames>Jose F.</forenames><suffix>Jr.</suffix></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author><author><keyname>Pan</keyname><forenames>Jia-Yu</forenames></author><author><keyname>Traina</keyname><forenames>Agma J. M.</forenames></author><author><keyname>Traina</keyname><forenames>Caetano</forenames><suffix>Jr.</suffix></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>Large Graph Analysis in the GMine System</title><categories>cs.SI</categories><journal-ref>Large Graph Analysis in the GMine System IEEE Transactions on
  Knowledge and Data Engineering 1: 25. 106 - 118 (2013)</journal-ref><doi>10.1109/TKDE.2011.199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current applications have produced graphs on the order of hundreds of
thousands of nodes and millions of edges. To take advantage of such graphs, one
must be able to find patterns, outliers and communities. These tasks are better
performed in an interactive environment, where human expertise can guide the
process. For large graphs, though, there are some challenges: the excessive
processing requirements are prohibitive, and drawing hundred-thousand nodes
results in cluttered images hard to comprehend. To cope with these problems, we
propose an innovative framework suited for any kind of tree-like graph visual
design. GMine integrates (a) a representation for graphs organized as
hierarchies of partitions - the concepts of SuperGraph and Graph-Tree; and (b)
a graph summarization methodology - CEPS. Our graph representation deals with
the problem of tracing the connection aspects of a graph hierarchy with sub
linear complexity, allowing one to grasp the neighborhood of a single node or
of a group of nodes in a single click. As a proof of concept, the visual
environment of GMine is instantiated as a system in which large graphs can be
investigated globally and locally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07778</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07778</id><created>2015-05-28</created><authors><author><keyname>Ghosh</keyname><forenames>Suman K.</forenames></author><author><keyname>Valveny</keyname><forenames>Ernest</forenames></author></authors><title>Query by String word spotting based on character bi-gram indexing</title><categories>cs.CV</categories><comments>To be published in ICDAR2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a segmentation-free query by string word spotting
method. Both the documents and query strings are encoded using a recently
proposed word representa- tion that projects images and strings into a common
atribute space based on a pyramidal histogram of characters(PHOC). These
attribute models are learned using linear SVMs over the Fisher Vector
representation of the images along with the PHOC labels of the corresponding
strings. In order to search through the whole page, document regions are
indexed per character bi- gram using a similar attribute representation. On top
of that, we propose an integral image representation of the document using a
simplified version of the attribute model for efficient computation. Finally we
introduce a re-ranking step in order to boost retrieval performance. We show
state-of-the-art results for segmentation-free query by string word spotting in
single-writer and multi-writer standard datasets
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07780</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07780</id><created>2015-05-28</created><authors><author><keyname>Effantin</keyname><forenames>Brice</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Gastineau</keyname><forenames>Nicolas</forenames><affiliation>Le2i</affiliation></author><author><keyname>Togni</keyname><forenames>Olivier</forenames><affiliation>Le2i</affiliation></author></authors><title>A characterization of b-chromatic and partial Grundy numbers by induced
  subgraphs</title><categories>cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gy{\'a}rf{\'a}s et al. and Zaker have proven that the Grundy number of a
graph $G$ satisfies $\Gamma(G)\ge t$ if and only if $G$ contains an induced
subgraph called a $t$-atom. The family of $t$-atoms has bounded order and
contains a finite number of graphs. In this article, we introduce equivalents
of $t$-atoms for b-coloring and partial Grundy coloring. This concept is used
to prove that determining if $\varphi(G)\ge t$ and $\partial\Gamma(G)\ge t$
(under conditions for the b-coloring), for a graph $G$, is in XP with parameter
$t$. We illustrate the utility of the concept of $t$-atoms by giving results on
b-critical vertices and edges, on b-perfect graphs and on graphs of girth at
least $7$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07781</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07781</id><created>2015-05-28</created><authors><author><keyname>Gastineau</keyname><forenames>Nicolas</forenames><affiliation>Le2i</affiliation></author><author><keyname>Kheddouci</keyname><forenames>Hamamache</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Togni</keyname><forenames>Olivier</forenames><affiliation>Le2i</affiliation></author></authors><title>Subdivision into i-packings and S-packing chromatic number of some
  lattices</title><categories>cs.DM math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $i$-packing in a graph $G$ is a set of vertices at pairwise distance
greater than $i$. For a nondecreasing sequence of integers
$S=(s\_{1},s\_{2},\ldots)$, the $S$-packing chromatic number of a graph $G$ is
the least integer $k$ such that there exists a coloring of $G$ into $k$ colors
where each set of vertices colored $i$, $i=1,\ldots, k$, is an $s\_i$-packing.
This paper describes various subdivisions of an $i$-packing into $j$-packings
($j\textgreater{}i$) for the hexagonal, square and triangular lattices. These
results allow us to bound the $S$-packing chromatic number for these graphs,
with more precise bounds and exact values for sequences $S=(s\_{i},
i\in\mathbb{N}^{*})$, $s\_{i}=d+ \lfloor (i-1)/n \rfloor$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07783</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07783</id><created>2015-05-28</created><authors><author><keyname>Frey</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>UB, LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Appriou</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, UB</affiliation></author><author><keyname>Lotte</keyname><forenames>Fabien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Hachet</keyname><forenames>Martin</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author></authors><title>Estimating Visual Comfort in Stereoscopic Displays Using
  Electroencephalography: A Proof-of-Concept</title><categories>cs.HC</categories><comments>INTERACT, Sep 2015, Bamberg, Germany</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With stereoscopic displays, a depth sensation that is too strong could impede
visual comfort and result in fatigue or pain. Electroencephalography (EEG) is a
technology which records brain activity. We used it to develop a novel
brain-computer interface that monitors users' states in order to reduce visual
strain. We present the first proof-of-concept system that discriminates
comfortable conditions from uncomfortable ones during stereoscopic vision using
EEG. It reacts within 1s to depth variations, achieving 63% accuracy on average
and 74% when 7 consecutive variations are measured. This study could lead to
adaptive systems that automatically suit stereoscopic displays to users and
viewing conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07794</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07794</id><created>2015-05-28</created><authors><author><keyname>Beffara</keyname><forenames>Emmanuel</forenames><affiliation>I2M</affiliation></author></authors><title>Unifying type systems for mobile processes</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a unifying framework for type systems for process calculi. The
core of the system provides an accurate correspondence between essentially
functional processes and linear logic proofs; fragments of this system
correspond to previously known connections between proofs and processes. We
show how the addition of extra logical axioms can widen the class of typeable
processes in exchange for the loss of some computational properties like
lock-freeness or termination, allowing us to see various well studied systems
(like i/o types, linearity, control) as instances of a general pattern. This
suggests unified methods for extending existing type systems with new features
while staying in a well structured environment and constitutes a step towards
the study of denotational semantics of processes using proof-theoretical
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07804</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07804</id><created>2015-05-28</created><authors><author><keyname>Rodrigues</keyname><forenames>Jose F.</forenames><suffix>Jr</suffix></author><author><keyname>Traina</keyname><forenames>Agma J. M.</forenames></author><author><keyname>Oliveira</keyname><forenames>Maria C. F.</forenames></author><author><keyname>Traina</keyname><forenames>Caetano</forenames><suffix>Jr</suffix></author></authors><title>The Spatial-Perceptual Design Space: a new comprehension for Data
  Visualization</title><categories>cs.GR</categories><journal-ref>Information Visualization 6: 4. 261-279 (2007)</journal-ref><doi>10.1057/palgrave.ivs.9500161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the design space of visualizations aiming at identifying and
relating its components. In this sense, we establish a model to examine the
process through which visualizations become expressive for users. This model
has leaded us to a taxonomy oriented to the human visual perception, a
conceptualization that provides natural criteria in order to delineate a novel
understanding for the visualization design space. The new organization of
concepts that we introduce is our main contribution: a grammar for the
visualization design based on the review of former works and of classical and
state-of-the-art techniques. Like so, the paper is presented as a survey whose
structure introduces a new conceptualization for the space of techniques
concerning visual analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07814</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07814</id><created>2015-05-28</created><updated>2015-11-24</updated><authors><author><keyname>Wu</keyname><forenames>Xinyu</forenames></author><author><keyname>Saxena</keyname><forenames>Vishal</forenames></author><author><keyname>Zhu</keyname><forenames>Kehan</forenames></author><author><keyname>Balagopal</keyname><forenames>Sakkarapani</forenames></author></authors><title>A CMOS Spiking Neuron for Brain-Inspired Neural Networks with Resistive
  Synapses and In-Situ Learning</title><categories>cs.NE</categories><journal-ref>IEEE Transactions on Circuits and Systems II: Express Briefs,
  62(11), 1088-1092, 2015</journal-ref><doi>10.1109/TCSII.2015.2456372</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanoscale resistive memories are expected to fuel dense integration of
electronic synapses for large-scale neuromorphic system. To realize such a
brain-inspired computing chip, a compact CMOS spiking neuron that performs
in-situ learning and computing while driving a large number of resistive
synapses is desired. This work presents a novel leaky integrate-and-fire neuron
design which implements the dual-mode operation of current integration and
synaptic drive, with a single opamp and enables in-situ learning with crossbar
resistive synapses. The proposed design was implemented in a 0.18 $\mu$m CMOS
technology. Measurements show neuron's ability to drive a thousand resistive
synapses, and demonstrate an in-situ associative learning. The neuron circuit
occupies a small area of 0.01 mm$^2$ and has an energy-efficiency of 9.3
pJ$/$spike$/$synapse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07818</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07818</id><created>2015-05-28</created><updated>2015-11-27</updated><authors><author><keyname>Ganin</keyname><forenames>Yaroslav</forenames></author><author><keyname>Ustinova</keyname><forenames>Evgeniya</forenames></author><author><keyname>Ajakan</keyname><forenames>Hana</forenames></author><author><keyname>Germain</keyname><forenames>Pascal</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author><author><keyname>Lempitsky</keyname><forenames>Victor</forenames></author></authors><title>Domain-Adversarial Training of Neural Networks</title><categories>stat.ML cs.LG cs.NE</categories><comments>To appear in JMLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new representation learning approach for domain adaptation, in
which data at training and test time come from similar but different
distributions. Our approach is directlyinspired by the theory on domain
adaptation suggesting that, for effective domain transfer to be achieved,
predictions must be made based on features that cannot discriminate between the
training (source) and test (target) domains.
  The approach implements this idea in the context of neural network
architectures that are trained on labeled data from the source domain and
unlabeled data from the target domain (no labeled target-domain data is
necessary). As the training progresses, the approach promotes the emergence of
features that are (i) discriminative for the main learning task on the source
domain and (ii) indiscriminate with respect to the shift between the domains.
We show that this adaptation behaviour can be achieved in almost any
feed-forward model by augmenting it with few standard layers and a new gradient
reversal layer. The resulting augmented architecture can be trained using
standard backpropagation and stochastic gradient descent, and can thus be
implemented with little effort using any of the deep learning packages.
  We demonstrate the success of our approach for two distinct classification
problems (document sentiment analysis and image classification), where
state-of-the-art domain adaptation performance on standard benchmarks is
achieved. We also validate the approach for descriptor learning task in the
context of person re-identification application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07862</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07862</id><created>2015-05-28</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Scheffer</keyname><forenames>Christian</forenames></author><author><keyname>Schmidt</keyname><forenames>Arne</forenames></author></authors><title>New Geometric Algorithms for Fully Connected Staged Self-Assembly</title><categories>cs.DS cs.CG</categories><comments>16 pages, 11 figures; to appear in DNA21</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider staged self-assembly systems, in which square-shaped tiles can be
added to bins in several stages. Within these bins, the tiles may connect to
each other, depending on the glue types of their edges. Previous work by
Demaine et al. showed that a relatively small number of tile types suffices to
produce arbitrary shapes in this model. However, these constructions were only
based on a spanning tree of the geometric shape, so they did not produce full
connectivity of the underlying grid graph in the case of shapes with holes;
designing fully connected assemblies with a polylogarithmic number of stages
was left as a major open problem. We resolve this challenge by presenting new
systems for staged assembly that produce fully connected polyominoes in O(log^2
n) stages, for various scale factors and temperature {\tau} = 2 as well as
{\tau} = 1. Our constructions work even for shapes with holes and uses only a
constant number of glues and tiles. Moreover, the underlying approach is more
geometric in nature, implying that it promised to be more feasible for shapes
with compact geometric description.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07872</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07872</id><created>2015-05-28</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Towards combinatorial clustering: preliminary research survey</title><categories>cs.AI cs.SY math.OC</categories><comments>102 pages, 66 figures, 67 tables</comments><msc-class>90B50, 90B51, 90C27, 90C59</msc-class><acm-class>G.1.6; G.2.1; G.2.3; H.3.3; I.2.8; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes clustering problems from the combinatorial viewpoint. A
brief systemic survey is presented including the following: (i) basic
clustering problems (e.g., classification, clustering, sorting, clustering with
an order over cluster), (ii) basic approaches to assessment of objects and
object proximities (i.e., scales, comparison, aggregation issues), (iii) basic
approaches to evaluation of local quality characteristics for clusters and
total quality characteristics for clustering solutions, (iv) clustering as
multicriteria optimization problem, (v) generalized modular clustering
framework, (vi) basic clustering models/methods (e.g., hierarchical clustering,
k-means clustering, minimum spanning tree based clustering, clustering as
assignment, detection of clisue/quasi-clique based clustering, correlation
clustering, network communities based clustering), Special attention is
targeted to formulation of clustering as multicriteria optimization models.
Combinatorial optimization models are used as auxiliary problems (e.g.,
assignment, partitioning, knapsack problem, multiple choice problem,
morphological clique problem, searching for consensus/median for structures).
Numerical examples illustrate problem formulations, solving methods, and
applications. The material can be used as follows: (a) a research survey, (b) a
fundamental for designing the structure/architecture of composite modular
clustering software, (c) a bibliography reference collection, and (d) a
tutorial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07873</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07873</id><created>2015-05-28</created><authors><author><keyname>Linahan</keyname><forenames>Jeff</forenames></author></authors><title>A Geometric Interpretation of the Boolean Gilbert-Johnson-Keerthi
  Algorithm</title><categories>cs.CG</categories><comments>10 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gilbert-Johnson-Keerthi (GJK) algorithm is an iterative improvement
technique for finding the minimum distance between two convex objects. It can
easily be extended to work with concave objects and return the pair of closest
points. [4] The key operation of GJK is testing whether a Voronoi region of a
simplex contains the origin or not. In this paper we show that, in the context
where one is interested only in the Boolean value of whether two convex objects
intersect, and not in the actual distance between them, the number of test
cases in GJK can be significantly reduced. This results in a simpler and more
efficient algorithm that can be used in many computational geometry
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07892</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07892</id><created>2015-05-28</created><authors><author><keyname>Ryba</keyname><forenames>Fabrice J.</forenames></author><author><keyname>Orlinski</keyname><forenames>Matthew</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author><author><keyname>Rossow</keyname><forenames>Christian</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author></authors><title>Amplification and DRDoS Attack Defense -- A Survey and New Perspectives</title><categories>cs.NI cs.CR</categories><proxy>Matthias W\&quot;ahlisch</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The severity of amplification attacks has grown in recent years. Since 2013
there have been at least two attacks which involved over 300Gbps of attack
traffic. This paper offers an analysis of these and many other amplification
attacks. We compare a wide selection of different proposals for detecting and
preventing amplification attacks, as well as proposals for tracing the
attackers. Since source IP spoofing plays an important part in almost all of
the attacks mentioned, a survey on the state of the art in spoofing defenses is
also presented. This work acts as an introduction into amplification attacks
and source IP address spoofing. By combining previous works into a single
comprehensive bibliography, and with our concise discussion, we hope to prevent
redundant work and encourage others to find practical solutions for defending
against future amplification attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07897</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07897</id><created>2015-05-28</created><updated>2015-06-03</updated><authors><author><keyname>Lu</keyname><forenames>Zhigang</forenames></author><author><keyname>Shen</keyname><forenames>Hong</forenames></author></authors><title>An Accuracy-Assured Privacy-Preserving Recommender System for Internet
  Commerce</title><categories>cs.CR cs.IR</categories><comments>replacement for the previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems, tool for predicting users' potential preferences by
computing history data and users' interests, show an increasing importance in
various Internet applications such as online shopping. As a well-known
recommendation method, neighbourhood-based collaborative filtering has
attracted considerable attention recently. The risk of revealing users' private
information during the process of filtering has attracted noticeable research
interests. Among the current solutions, the probabilistic techniques have shown
a powerful privacy preserving effect. When facing $k$ Nearest Neighbour attack,
all the existing methods provide no data utility guarantee, for the
introduction of global randomness. In this paper, to overcome the problem of
recommendation accuracy loss, we propose a novel approach, Partitioned
Probabilistic Neighbour Selection, to ensure a required prediction accuracy
while maintaining high security against $k$NN attack. We define the sum of $k$
neighbours' similarity as the accuracy metric alpha, the number of user
partitions, across which we select the $k$ neighbours, as the security metric
beta. We generalise the $k$ Nearest Neighbour attack to beta k Nearest
Neighbours attack. Differing from the existing approach that selects neighbours
across the entire candidate list randomly, our method selects neighbours from
each exclusive partition of size $k$ with a decreasing probability. Theoretical
and experimental analysis show that to provide an accuracy-assured
recommendation, our Partitioned Probabilistic Neighbour Selection method yields
a better trade-off between the recommendation accuracy and system security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07900</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07900</id><created>2015-05-28</created><authors><author><keyname>Lu</keyname><forenames>Zhigang</forenames></author><author><keyname>Shen</keyname><forenames>Hong</forenames></author></authors><title>A Faster Algorithm to Build New Users Similarity List in
  Neighbourhood-based Collaborative Filtering</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neighbourhood-based Collaborative Filtering (CF) has been applied in the
industry for several decades, because of the easy implementation and high
recommendation accuracy. As the core of neighbourhood-based CF, the task of
dynamically maintaining users' similarity list is challenged by cold-start
problem and scalability problem. Recently, several methods are presented on
solving the two problems. However, these methods applied an $O(n^2)$ algorithm
to compute the similarity list in a special case, where the new users, with
enough recommendation data, have the same rating list. To address the problem
of large computational cost caused by the special case, we design a faster
($O(\frac{1}{125}n^2)$) algorithm, TwinSearch Algorithm, to avoid computing and
sorting the similarity list for the new users repeatedly to save the
computational resources. Both theoretical and experimental results show that
the TwinSearch Algorithm achieves better running time than the traditional
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07909</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07909</id><created>2015-05-28</created><updated>2015-07-06</updated><authors><author><keyname>Wang</keyname><forenames>Huazheng</forenames></author><author><keyname>Gao</keyname><forenames>Bin</forenames></author><author><keyname>Bian</keyname><forenames>Jiang</forenames></author><author><keyname>Tian</keyname><forenames>Fei</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered
  Word Embedding</title><categories>cs.CL cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligence Quotient (IQ) Test is a set of standardized questions designed
to evaluate human intelligence. Verbal comprehension questions appear very
frequently in IQ tests, which measure human's verbal ability including the
understanding of the words with multiple senses, the synonyms and antonyms, and
the analogies among words. In this work, we explore whether such tests can be
solved automatically by artificial intelligence technologies, especially the
deep learning technologies that are recently developed and successfully applied
in a number of fields. However, we found that the task was quite challenging,
and simply applying existing technologies (e.g., word embedding) could not
achieve a good performance, mainly due to the multiple senses of words and the
complex relations among words. To tackle these challenges, we propose a novel
framework consisting of three components. First, we build a classifier to
recognize the specific type of a verbal question (e.g., analogy,
classification, synonym, or antonym). Second, we obtain distributed
representations of words and relations by leveraging a novel word embedding
method that considers the multi-sense nature of words and the relational
knowledge among words (or their senses) contained in dictionaries. Third, for
each type of questions, we propose a specific solver based on the obtained
distributed word representations and relation representations. Experimental
results have shown that the proposed framework can not only outperform existing
methods for solving verbal comprehension questions but also exceed the average
performance of the Amazon Mechanical Turk workers involved in the study. The
results indicate that with appropriate uses of the deep learning technologies
we might be a further step closer to the human intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07911</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07911</id><created>2015-05-28</created><updated>2015-07-01</updated><authors><author><keyname>Goel</keyname><forenames>Gagan</forenames></author><author><keyname>Khani</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author></authors><title>Core-competitive Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major drawbacks of the celebrated VCG auction is its low (or zero)
revenue even when the agents have high value for the goods and a {\em
competitive} outcome could have generated a significant revenue. A competitive
outcome is one for which it is impossible for the seller and a subset of buyers
to `block' the auction by defecting and negotiating an outcome with higher
payoffs for themselves. This corresponds to the well-known concept of {\em
core} in cooperative game theory.
  In particular, VCG revenue is known to be not competitive when the goods
being sold have complementarities. A bottleneck here is an impossibility result
showing that there is no auction that simultaneously achieves competitive
prices (a core outcome) and incentive-compatibility.
  In this paper we try to overcome the above impossibility result by asking the
following natural question: is it possible to design an incentive-compatible
auction whose revenue is comparable (even if less) to a competitive outcome?
Towards this, we define a notion of {\em core-competitive} auctions. We say
that an incentive-compatible auction is $\alpha$-core-competitive if its
revenue is at least $1/\alpha$ fraction of the minimum revenue of a
core-outcome. We study the Text-and-Image setting. In this setting, there is an
ad slot which can be filled with either a single image ad or $k$ text ads. We
design an $O(\ln \ln k)$ core-competitive randomized auction and an
$O(\sqrt{\ln(k)})$ competitive deterministic auction for the Text-and-Image
setting. We also show that both factors are tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07916</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07916</id><created>2015-05-29</created><authors><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author><author><keyname>Khasidashvili</keyname><forenames>Zurab</forenames></author><author><keyname>Seger</keyname><forenames>Carl-Johan H.</forenames></author><author><keyname>Gajavelly</keyname><forenames>Rajkumar</forenames></author><author><keyname>Haldankar</keyname><forenames>Tanmay</forenames></author><author><keyname>Chhatani</keyname><forenames>Dinesh</forenames></author><author><keyname>Mistry</keyname><forenames>Rakesh</forenames></author></authors><title>Word-level Symbolic Trajectory Evaluation</title><categories>cs.LO</categories><comments>19 pages, 3 figures, 2 tables, full version of paper in International
  Conference on Computer-Aided Verification (CAV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic trajectory evaluation (STE) is a model checking technique that has
been successfully used to verify industrial designs. Existing implementations
of STE, however, reason at the level of bits, allowing signals to take values
in {0, 1, X}. This limits the amount of abstraction that can be achieved, and
presents inherent limitations to scaling. The main contribution of this paper
is to show how much more abstract lattices can be derived automatically from
RTL descriptions, and how a model checker for the general theory of STE
instantiated with such abstract lattices can be implemented in practice. This
gives us the first practical word-level STE engine, called STEWord. Experiments
on a set of designs similar to those used in industry show that STEWord scales
better than word-level BMC and also bit-level STE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07919</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07919</id><created>2015-05-29</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Wang</keyname><forenames>Xianbin</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>A Survey on Wireless Security: Technical Challenges, Recent Advances and
  Future Trends</title><categories>cs.IT cs.CR math.IT</categories><comments>31 pages. Appears in Proceedings of the IEEE, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is motivated to examine the security vulnerabilities and threats
imposed by the inherent open nature of wireless communications and to devise
efficient defense mechanisms for improving the wireless network security. We
first summarize the security requirements of wireless networks, including their
authenticity, confidentiality, integrity and availability issues. Next, a
comprehensive overview of security attacks encountered in wireless networks is
presented in view of the network protocol architecture, where the potential
security threats are discussed at each protocol layer. We also provide a survey
of the existing security protocols and algorithms that are adopted in the
existing wireless network standards, such as the Bluetooth, Wi-Fi, WiMAX, and
the long-term evolution (LTE) systems. Then, we discuss the state-of-the-art in
physical-layer security, which is an emerging technique of securing the open
communications environment against eavesdropping attacks at the physical layer.
Several physical-layer security techniques are reviewed and compared, including
information-theoretic security, artificial noise aided security,
security-oriented beamforming, diversity assisted security, and physical-layer
key generation approaches. Additionally, since a jammer emitting radio signals
can readily interfere with the legitimate wireless users, we introduce the
family of various jamming attacks and their counter-measures, including the
constant jammer, intermittent jammer, reactive jammer, adaptive jammer and
intelligent jammer. Finally, some technical challenges which remain unresolved
at the time of writing are summarized and the future trends in wireless
security are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07922</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07922</id><created>2015-05-29</created><authors><author><keyname>Huang</keyname><forenames>Junshi</forenames></author><author><keyname>Feris</keyname><forenames>Rogerio S.</forenames></author><author><keyname>Chen</keyname><forenames>Qiang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of cross-domain image retrieval, considering the
following practical application: given a user photo depicting a clothing image,
our goal is to retrieve the same or attribute-similar clothing items from
online shopping stores. This is a challenging problem due to the large
discrepancy between online shopping images, usually taken in ideal
lighting/pose/background conditions, and user photos captured in uncontrolled
conditions. To address this problem, we propose a Dual Attribute-aware Ranking
Network (DARN) for retrieval feature learning. More specifically, DARN consists
of two sub-networks, one for each domain, whose retrieval feature
representations are driven by semantic attribute learning. We show that this
attribute-guided learning is a key factor for retrieval accuracy improvement.
In addition, to further align with the nature of the retrieval problem, we
impose a triplet visual similarity constraint for learning to rank across the
two sub-networks. Another contribution of our work is a large-scale dataset
which makes the network learning feasible. We exploit customer review websites
to crawl a large set of online shopping images and corresponding offline user
photos with fine-grained clothing attributes, i.e., around 450,000 online
shopping images and about 90,000 exact offline counterpart images of those
online ones. All these images are collected from real-world consumer websites
reflecting the diversity of the data modality, which makes this dataset unique
and rare in the academic community. We extensively evaluate the retrieval
performance of networks in different configurations. The top-20 retrieval
accuracy is doubled when using the proposed DARN other than the current popular
solution using pre-trained CNN features only (0.570 vs. 0.268).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07923</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07923</id><created>2015-05-29</created><authors><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>Fast Computation of PERCLOS and Saccadic Ratio</title><categories>cs.CV</categories><comments>MS Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis describes the development of fast algorithms for the computation
of PERcentage CLOSure of eyes (PERCLOS) and Saccadic Ratio (SR). PERCLOS and SR
are two ocular parameters reported to be measures of alertness levels in human
beings. PERCLOS is the percentage of time in which at least 80% of the eyelid
remains closed over the pupil. Saccades are fast and simultaneous movement of
both the eyes in the same direction. SR is the ratio of peak saccadic velocity
to the saccadic duration. This thesis addresses the issues of image based
estimation of PERCLOS and SR, prevailing in the literature such as illumination
variation, poor illumination conditions, head rotations etc. In this work,
algorithms for real-time PERCLOS computation has been developed and implemented
on an embedded platform. The platform has been used as a case study for
assessment of loss of attention in automotive drivers. The SR estimation has
been carried out offline as real-time implementation requires high frame rates
of processing which is difficult to achieve due to hardware limitations. The
accuracy in estimation of the loss of attention using PERCLOS and SR has been
validated using brain signals, which are reported to be an authentic cue for
estimating the state of alertness in human beings. The major contributions of
this thesis include database creation, design and implementation of fast
algorithms for estimating PERCLOS and SR on embedded computing platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07925</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07925</id><created>2015-05-29</created><authors><author><keyname>Yang</keyname><forenames>Yun</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>On the Computational Complexity of High-Dimensional Bayesian Variable
  Selection</title><categories>math.ST cs.LG stat.CO stat.ME stat.ML stat.TH</categories><comments>42 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of Markov chain Monte Carlo (MCMC)
methods for high-dimensional Bayesian linear regression under sparsity
constraints. We first show that a Bayesian approach can achieve
variable-selection consistency under relatively mild conditions on the design
matrix. We then demonstrate that the statistical criterion of posterior
concentration need not imply the computational desideratum of rapid mixing of
the MCMC algorithm. By introducing a truncated sparsity prior for variable
selection, we provide a set of conditions that guarantee both
variable-selection consistency and rapid mixing of a particular
Metropolis-Hastings algorithm. The mixing time is linear in the number of
covariates up to a logarithmic factor. Our proof controls the spectral gap of
the Markov chain by constructing a canonical path ensemble that is inspired by
the steps taken by greedy algorithms for variable selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07928</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07928</id><created>2015-05-29</created><updated>2015-07-05</updated><authors><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author><author><keyname>Zhu</keyname><forenames>Wei-Ping</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Security-Reliability Trade-off Analysis of Multi-Relay Aided
  Decode-and-Forward Cooperation Systems</title><categories>cs.IT math.IT</categories><comments>8 pages, IEEE Transactions on Vehicular Technology, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cooperative wireless network comprised of a source, a
destination and multiple relays operating in the presence of an eavesdropper,
which attempts to tap the source-destination transmission. We propose
multi-relay selection scheme for protecting the source against eavesdropping.
More specifically, multi-relay selection allows multiple relays to
simultaneously forward the source's transmission to the destination, differing
from the conventional single-relay selection where only the best relay is
chosen to assist the transmission from the source to destination. For the
purpose of comparison, we consider the classic direct transmission and
single-relay selection as benchmark schemes. We derive closed-form expressions
of the intercept probability and outage probability for the direct transmission
as well as for the single-relay and multi-relay selection schemes over Rayleigh
fading channels. It is demonstrated that as the outage requirement is relaxed,
the intercept performance of the three schemes improves and vice versa,
implying that there is a \emph{security versus reliability trade-off} (SRT). We
also show that both the single-relay and multi-relay selection schemes
outperform the direct transmission in terms of SRT, demonstrating the advantage
of the relay selection schemes for protecting the source's transmission against
the eavesdropping attacks. Finally, upon increasing the number of relays, the
SRTs of both the single-relay and multi-relay selection schemes improve
significantly and as expected, multi-relay selection outperforms single-relay
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07929</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07929</id><created>2015-05-29</created><updated>2015-09-02</updated><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Relay Selection for Wireless Communications Against Eavesdropping: A
  Security-Reliability Tradeoff Perspective</title><categories>cs.IT cs.CR math.IT</categories><comments>16 pages, IEEE Network, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article examines the secrecy coding aided wireless communications from a
source to a destination in the presence of an eavesdropper from a
security-reliability tradeoff (SRT) perspective. Explicitly, the security is
quantified in terms of the intercept probability experienced at the
eavesdropper, while the outage probability encountered at the destination is
used to measure the transmission reliability. We characterize the SRT of
conventional direct transmission from the source to the destination and show
that if the outage probability is increased, the intercept probability
decreases, and vice versa. We first demonstrate that the employment of relay
nodes for assisting the source-destination transmissions is capable of
defending against eavesdropping, followed by quantifying the benefits of
single-relay selection (SRS) as well as of multi-relay selection (MRS) schemes.
More specifically, in the SRS scheme, only the single &quot;best&quot; relay is selected
for forwarding the source signal to the destination, whereas the MRS scheme
allows multiple relays to participate in this process. It is illustrated that
both the SRS and MRS schemes achieve a better SRT than the conventional direct
transmission, especially upon increasing the number of relays. Numerical
results also show that as expected, the MRS outperforms the SRS in terms of its
SRT. Additionally, we present some open challenges and future directions for
the wireless relay aided physical-layer security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07930</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07930</id><created>2015-05-29</created><authors><author><keyname>Nguyen</keyname><forenames>Tam V.</forenames></author><author><keyname>Sepulveda</keyname><forenames>Jose</forenames></author></authors><title>Salient Object Detection via Augmented Hypotheses</title><categories>cs.CV</categories><comments>IJCAI 2015 paper</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we propose using \textit{augmented hypotheses} which consider
objectness, foreground and compactness for salient object detection. Our
algorithm consists of four basic steps. First, our method generates the
objectness map via objectness hypotheses. Based on the objectness map, we
estimate the foreground margin and compute the corresponding foreground map
which prefers the foreground objects. From the objectness map and the
foreground map, the compactness map is formed to favor the compact objects. We
then derive a saliency measure that produces a pixel-accurate saliency map
which uniformly covers the objects of interest and consistently separates fore-
and background. We finally evaluate the proposed framework on two challenging
datasets, MSRA-1000 and iCoSeg. Our extensive experimental results show that
our method outperforms state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07931</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07931</id><created>2015-05-29</created><authors><author><keyname>Yang</keyname><forenames>Xuefeng</forenames></author><author><keyname>Mao</keyname><forenames>Kezhi</forenames></author></authors><title>Supervised Fine Tuning for Word Embedding with Integrated Knowledge</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning vector representation for words is an important research field which
may benefit many natural language processing tasks. Two limitations exist in
nearly all available models, which are the bias caused by the context
definition and the lack of knowledge utilization. They are difficult to tackle
because these algorithms are essentially unsupervised learning approaches.
Inspired by deep learning, the authors propose a supervised framework for
learning vector representation of words to provide additional supervised fine
tuning after unsupervised learning. The framework is knowledge rich approacher
and compatible with any numerical vectors word representation. The authors
perform both intrinsic evaluation like attributional and relational similarity
prediction and extrinsic evaluations like the sentence completion and sentiment
analysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show
that the proposed fine tuning framework may significantly improve the quality
of the vector representation of words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07934</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07934</id><created>2015-05-29</created><authors><author><keyname>Lukac</keyname><forenames>Martin</forenames></author><author><keyname>Abdiyeva</keyname><forenames>Kamila</forenames></author><author><keyname>Kameyama</keyname><forenames>Michitaka</forenames></author></authors><title>Symbolic Segmentation Using Algorithm Selection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an alternative approach to symbolic segmentation;
instead of implementing a new method we approach symbolic segmentation as an
algorithm selection problem. That is, let there be $n$ available algorithms for
symbolic segmentation, a selection mechanism forms a set of input features and
image attributes and selects on a case by case basis the best algorithm. The
selection mechanism is demonstrated from within an algorithm framework where
the selection is done in a set of various algorithm networks. Two sets of
experiments are performed and in both cases we demonstrate that the algorithm
selection allows to increase the result of the symbolic segmentation by a
considerable amount.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07940</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07940</id><created>2015-05-29</created><authors><author><keyname>Wobrock</keyname><forenames>Dennis</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Frey</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>UB, LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Graeff</keyname><forenames>Delphine</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>De La Rivi&#xe8;re</keyname><forenames>Jean-Baptiste</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Castet</keyname><forenames>Julien</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Lotte</keyname><forenames>Fabien</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Continuous Mental Effort Evaluation during 3D Object Manipulation Tasks
  based on Brain and Physiological Signals</title><categories>cs.HC cs.GR</categories><comments>Published in INTERACT, Sep 2015, Bamberg, Germany</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing 3D User Interfaces (UI) requires adequate evaluation tools to
ensure good usability and user experience. While many evaluation tools are
already available and widely used, existing approaches generally cannot provide
continuous and objective measures of usa-bility qualities during interaction
without interrupting the user. In this paper, we propose to use brain (with
ElectroEncephaloGraphy) and physiological (ElectroCardioGraphy, Galvanic Skin
Response) signals to continuously assess the mental effort made by the user to
perform 3D object manipulation tasks. We first show how this mental effort
(a.k.a., mental workload) can be estimated from such signals, and then measure
it on 8 participants during an actual 3D object manipulation task with an input
device known as the CubTile. Our results suggest that monitoring workload
enables us to continuously assess the 3DUI and/or interaction technique
ease-of-use. Overall, this suggests that this new measure could become a useful
addition to the repertoire of available evaluation tools, enabling a finer
grain assessment of the ergonomic qualities of a given 3D user interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07945</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07945</id><created>2015-05-29</created><updated>2015-09-19</updated><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Sellwood</keyname><forenames>James</forenames></author></authors><title>Relationships, Paths and Principal Matching: A New Approach to Access
  Control</title><categories>cs.CR</categories><acm-class>D.4.6; H.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on relationship-based access control has begun to show how it can
be applied to general computing systems, as opposed to simply being employed
for social networking applications. The use of relationships to determine
authorization policies enables more powerful policies to be defined than those
based solely on the commonly used concept of role membership. The
relationships, paths and principal matching (RPPM) model described here is a
formal access control model using relationships and a two-stage request
evaluation process. We make use of path conditions, which are similar to
regular expressions, to define policies. We then employ non-deterministic
finite automata to determine which policies are applicable to a request. The
power and robustness of the RPPM model allows us to include contextual
information in the authorization process (through the inclusion of logical
entities) and allows us to support desirable policy foundations such as
separation of duty and Chinese Wall. Additionally, the RPPM model naturally
supports a caching mechanism which has significant impact on request evaluation
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07954</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07954</id><created>2015-05-29</created><authors><author><keyname>Toranzo</keyname><forenames>I. V.</forenames></author><author><keyname>L&#xf3;pez-Rosa</keyname><forenames>S.</forenames></author><author><keyname>Esquivel</keyname><forenames>R. O.</forenames></author><author><keyname>Dehesa</keyname><forenames>J. S.</forenames></author></authors><title>Heisenberg-like and Fisher-information-based uncertainty relations for
  $N$-electron $d$-dimensional systems</title><categories>cs.IT math.IT quant-ph</categories><comments>9 pages, 6 figures</comments><journal-ref>Phys. Rev. A 91, 062122 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heisenberg-like and Fisher-information-based uncertainty relations which
extend and generalize previous similar expressions are obtained for $N$-fermion
$d$-dimensional systems. The contributions of both spatial and spin degrees of
freedom are taken into account. The accuracy of some of these generalized
spinned uncertainty-like relations is numerically examined for a large number
of atomic and molecular systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07958</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07958</id><created>2015-05-29</created><authors><author><keyname>Flaut</keyname><forenames>Cristina</forenames></author></authors><title>Codes over subsets of algebras obtained by the Cayley-Dickson process</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define binary block codes over subsets of real algebras
obtained by the Cayley-Dickson process and we provide an algorithm to obtain
codes with a better rate. This algorithm offers more flexibility than other
methods known until now, similar to Lenstra algorithm on elliptic curves
compared with Pollard algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.07987</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.07987</id><created>2015-05-29</created><authors><author><keyname>Gransden</keyname><forenames>Thomas</forenames></author><author><keyname>Walkinshaw</keyname><forenames>Neil</forenames></author><author><keyname>Raman</keyname><forenames>Rajeev</forenames></author></authors><title>SEPIA: Search for Proofs Using Inferred Automata</title><categories>cs.LO</categories><comments>To appear at 25th International Conference on Automated Deduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes SEPIA, a tool for automated proof generation in Coq.
SEPIA combines model inference with interactive theorem proving. Existing proof
corpora are modelled using state-based models inferred from tactic sequences.
These can then be traversed automatically to identify proofs. The SEPIA system
is described and its performance evaluated on three Coq datasets. Our results
show that SEPIA provides a useful complement to existing automated tactics in
Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08001</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08001</id><created>2015-05-29</created><authors><author><keyname>Zollo</keyname><forenames>Fabiana</forenames></author><author><keyname>Novak</keyname><forenames>Petra Kralj</forenames></author><author><keyname>Del Vicario</keyname><forenames>Michela</forenames></author><author><keyname>Bessi</keyname><forenames>Alessandro</forenames></author><author><keyname>Mozetic</keyname><forenames>Igor</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Emotional Dynamics in the Age of Misinformation</title><categories>cs.SI cs.CY physics.soc-ph</categories><journal-ref>PLoS ONE, 10(9): e0138740 (2015)</journal-ref><doi>10.1371/journal.pone.0138740</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the World Economic Forum, the diffusion of unsubstantiated
rumors on online social media is one of the main threats for our society.
  The disintermediated paradigm of content production and consumption on online
social media might foster the formation of homophile communities
(echo-chambers) around specific worldviews. Such a scenario has been shown to
be a vivid environment for the diffusion of false claims, in particular with
respect to conspiracy theories. Not rarely, viral phenomena trigger naive (and
funny) social responses -- e.g., the recent case of Jade Helm 15 where a simple
military exercise turned out to be perceived as the beginning of the civil war
in the US. In this work, we address the emotional dynamics of collective
debates around distinct kind of news -- i.e., science and conspiracy news --
and inside and across their respective polarized communities (science and
conspiracy news).
  Our findings show that comments on conspiracy posts tend to be more negative
than on science posts. However, the more the engagement of users, the more they
tend to negative commenting (both on science and conspiracy). Finally, zooming
in at the interaction among polarized communities, we find a general negative
pattern. As the number of comments increases -- i.e., the discussion becomes
longer -- the sentiment of the post is more and more negative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08003</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08003</id><created>2015-05-29</created><authors><author><keyname>Breunig</keyname><forenames>Ulrich</forenames></author><author><keyname>Schmid</keyname><forenames>Verena</forenames></author><author><keyname>Hartl</keyname><forenames>Richard F.</forenames></author><author><keyname>Vidal</keyname><forenames>Thibaut</forenames></author></authors><title>A fast large neighbourhood based heuristic for the Two-Echelon Vehicle
  Routing Problem</title><categories>cs.DS</categories><comments>Working Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address an optimisation problem arising from city logistics.
The focus lies on a two-level transportation system to deliver goods to
customers within densely populated areas. The optimisation problem called the
Two-Echelon Vehicle Routing Problem seeks to produce vehicle itineraries to
deliver goods to customers with transit through intermediate facilities. A
local-search metaheuristic based on the principle of destroy and repair of a
Large Neighbourhood Search is developed and implemented to find high quality
solutions within limited computing time. For future reference we resolve
confusion with inconsistent versions of benchmark instances by explaining their
differences and provide all of them online. The proposed algorithm is tested
with those instances. It is able to find the currently best known solutions or
better ones for 95% of the benchmark instances. The computational experiments
show that this simple method achieves excellent solutions for the problem
within short computing times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08019</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08019</id><created>2015-05-29</created><authors><author><keyname>Shen</keyname><forenames>Feifei</forenames></author><author><keyname>Song</keyname><forenames>Zhenjian</forenames></author><author><keyname>Wu</keyname><forenames>Congrui</forenames></author><author><keyname>Geng</keyname><forenames>Jiaqi</forenames></author><author><keyname>Wang</keyname><forenames>Qingyun</forenames></author></authors><title>Research on the fast Fourier transform of image based on GPU</title><categories>cs.MS cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Study of general purpose computation by GPU (Graphics Processing Unit) can
improve the image processing capability of micro-computer system. This paper
studies the parallelism of the different stages of decimation in time radix 2
FFT algorithm, designs the butterfly and scramble kernels and implements 2D FFT
on GPU. The experiment result demonstrates the validity and advantage over
general CPU, especially in the condition of large input size. The approach can
also be generalized to other transforms alike.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08023</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08023</id><created>2015-05-29</created><authors><author><keyname>Wu</keyname><forenames>Meng</forenames></author><author><keyname>Yang</keyname><forenames>Can</forenames></author><author><keyname>Xiang</keyname><forenames>Taoran</forenames></author><author><keyname>Cheng</keyname><forenames>Daning</forenames></author></authors><title>The Research and Optimization of Parallel Finite Element Algorithm based
  on MiniFE</title><categories>cs.DC cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite element method (FEM) is one of the most important numerical methods in
modern engineering design and analysis. Since traditional serial FEM is
difficult to solve large FE problems efficiently and accurately,
high-performance parallel FEM has become one of the essential way to solve
practical engineering problems. Based on MiniFE program, which is released by
National Energy Research Scientific Computing Center(NERSC), this work analyzes
concrete steps, key computing pattern and parallel mechanism of parallel FEM.
According to experimental results, this work analyzes the proportion of
calculation amount of each module and concludes the main performance bottleneck
of the program. Based on that, we optimize the MiniFE program on a server
platform. The optimization focuses on the bottleneck of the program - SpMV
kernel, and uses an efficient storage format named BCRS. Moreover, an improving
plan of hybrid MPI+OpenMP programming is provided. Experimental results show
that the optimized program performs better in both SpMV kernel and
synchronization. It can increase the performance of the program, on average, by
8.31%. Keywords : finite element, parallel, MiniFE, SpMV, performance
optimization
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08031</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08031</id><created>2015-05-29</created><authors><author><keyname>Vandaele</keyname><forenames>Arnaud</forenames></author><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author><author><keyname>Glineur</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>On the Linear Extension Complexity of Regular n-gons</title><categories>math.OC cs.DM math.CO</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new upper bound on the linear extension
complexity of regular $n$-gons. It is based on the equivalence between the
computation of (i) an extended formulation of size $r$ of a polytope $P$, and
(ii) a rank-$r$ nonnegative factorization of a slack matrix of the polytope
$P$. We provide explicit nonnegative factorizations for the slack matrix of any
regular $n$-gons of size $2 \lceil \log_2(n) \rceil - 1$ if $2^{k-1} &lt; n \leq
2^{k-1}+2^{k-2}$ for some integer $k$, and of size $2 \lceil \log_2(n) \rceil$
if $2^{k-1}+2^{k-2} &lt; n \leq 2^{k}$. For $2^{k-1}+2^{k-2} &lt; n \leq 2^{k}$, our
bound coincides with the best known upper bound of $2 \left\lceil \log_2(n)
\right\rceil$ by Fiorini, Rothvoss and Tiwary [Extended Formulations for
Polygons, Discrete Comput. Geom. 48(3), pp. 658-668, 2012]. We conjecture that
our upper bound is tight, which is suggested by numerical experiments for small
$n$. Moreover, this improved upper bound allows us to close the gap with the
best known lower bound for certain regular $n$-gons (namely, $9 \leq n \leq 14$
and $21 \leq n \leq 24$) hence allowing for the first time to determine their
extension complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08043</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08043</id><created>2015-05-29</created><authors><author><keyname>Rubinchik</keyname><forenames>Mikhail</forenames></author><author><keyname>Shur</keyname><forenames>Arseny M.</forenames></author></authors><title>The Number of Distinct Subpalindromes in Random Words</title><categories>math.CO cs.FL</categories><comments>14 pages, 1 figure; submitted to FI (Special issue of RuFiDiM 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a random word of length $n$ over a $k$-ary fixed alphabet
contains, on expectation, $\Theta(\sqrt{n})$ distinct palindromic factors. We
study this number of factors, $E(n,k)$, in detail, showing that the limit
$\lim_{n\to\infty}E(n,k)/\sqrt{n}$ does not exist for any $k\ge2$,
$\liminf_{n\to\infty}E(n,k)/\sqrt{n}=\Theta(1)$, and
$\limsup_{n\to\infty}E(n,k)/\sqrt{n}=\Theta(\sqrt{k})$. Such a complicated
behaviour stems from the asymmetry between the palindromes of even and odd
length. We show that a similar, but much simpler, result on the expected number
of squares in random words holds. We also provide some experimental data on the
number of palindromic factors in random words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08067</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08067</id><created>2015-05-29</created><authors><author><keyname>Bergach</keyname><forenames>Mohamed Amine</forenames></author><author><keyname>Kofman</keyname><forenames>Emilien</forenames></author><author><keyname>de Simone</keyname><forenames>Robert</forenames></author><author><keyname>Tissot</keyname><forenames>Serge</forenames></author><author><keyname>Syska</keyname><forenames>Michel</forenames></author></authors><title>Efficient FFT mapping on GPU for radar processing application: modeling
  and implementation</title><categories>cs.MS cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  General-purpose multiprocessors (as, in our case, Intel IvyBridge and Intel
Haswell) increasingly add GPU computing power to the former multicore
architectures. When used for embedded applications (for us, Synthetic aperture
radar) with intensive signal processing requirements, they must constantly
compute convolution algorithms, such as the famous Fast Fourier Transform. Due
to its &quot;fractal&quot; nature (the typical butterfly shape, with larger FFTs defined
as combination of smaller ones with auxiliary data array transpose functions),
one can hope to compute analytically the size of the largest FFT that can be
performed locally on an elementary GPU compute block. Then, the full
application must be organized around this given building block size. Now, due
to phenomena involved in the data transfers between various memory levels
across CPUs and GPUs, the optimality of such a scheme is only loosely
predictable (as communications tend to overcome in time the complexity of
computations). Therefore a mix of (theoretical) analytic approach and
(practical) runtime validation is here needed. As we shall illustrate, this
occurs at both stage, first at the level of deciding on a given elementary FFT
block size, then at the full application level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08069</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08069</id><created>2015-05-29</created><updated>2015-06-05</updated><authors><author><keyname>Zhu</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Jun</forenames></author></authors><title>Robust Design of Transmit Waveform and Receive Filter For Colocated MIMO
  Radar</title><categories>cs.IT math.IT</categories><comments>6 pages, 13 figures, part of this work was submitted to IEEE Signal
  Processing Letters; (short introduction; typos corrected; revised statement
  in section III-B and IV; revised figure labels)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of angle-robust joint transmit waveform and receive
filter design for colocated Multiple-Input Multiple-Output (MIMO) radar, in the
presence of signal-dependent interferences. The design problem is cast as a
max-min optimization problem to maximize the worst-case output
signal-to-interference-plus-noise-ratio (SINR) with respect to the unknown
angle of the target of interest. Based on rank-one relaxation and semi-definite
programming (SDP) representation of a nonnegative trigonometric polynomial, a
cyclic optimization algorithm is proposed to tackle this problem. The
effectiveness of the proposed method is illustrated via numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08070</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08070</id><created>2015-05-29</created><authors><author><keyname>Kaminski</keyname><forenames>Yirmeyahu</forenames></author><author><keyname>Werman</keyname><forenames>Mike</forenames></author></authors><title>General Deformations of Point Configurations Viewed By a Pinhole Model
  Camera</title><categories>cs.CV math.AG</categories><msc-class>68W30, 14N99, 68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a theoretical study of the following Non-Rigid Structure from
Motion problem. What can be computed from a monocular view of a parametrically
deforming set of points? We treat various variations of this problem for affine
and polynomial deformations with calibrated and uncalibrated cameras. We show
that in general at least three images with quasi-identical two deformations are
needed in order to have a finite set of solutions of the points' structure and
calculate some simple examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08071</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08071</id><created>2015-05-29</created><authors><author><keyname>Jain</keyname><forenames>Brijnesh J.</forenames></author></authors><title>Geometry of Graph Edit Distance Spaces</title><categories>cs.CV math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the geometry of graph spaces endowed with a special
class of graph edit distances. The focus is on geometrical results useful for
statistical pattern recognition. The main result is the Graph Representation
Theorem. It states that a graph is a point in some geometrical space, called
orbit space. Orbit spaces are well investigated and easier to explore than the
original graph space. We derive a number of geometrical results from the orbit
space representation, translate them to the graph space, and indicate their
significance and usefulness in statistical pattern recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08073</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08073</id><created>2015-05-29</created><authors><author><keyname>Pandolfi</keyname><forenames>Luciano</forenames></author></authors><title>Controllability of isotropic viscoelastic bodies of Maxwell-Boltzmann
  type</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a viscoelastic three dimensional body (of
Maxwell-Boltzmann type) controlled on (part of) the boundary. We assume that
the material is isotropic and homogeneous. If the body is elastic (i.e. no
dissipation due to past memory), controllability has been studied by several
authors. We prove that the viscoelastic body inherits the controllability
properties of the corresponding purely elastic system. The proof relays on
cosine operator methods combined with moment theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08075</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08075</id><created>2015-05-29</created><authors><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Ballesteros</keyname><forenames>Miguel</forenames></author><author><keyname>Ling</keyname><forenames>Wang</forenames></author><author><keyname>Matthews</keyname><forenames>Austin</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Transition-Based Dependency Parsing with Stack Long Short-Term Memory</title><categories>cs.CL cs.LG cs.NE</categories><comments>Proceedings of ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a technique for learning representations of parser states in
transition-based dependency parsers. Our primary innovation is a new control
structure for sequence-to-sequence neural networks---the stack LSTM. Like the
conventional stack data structures used in transition-based parsing, elements
can be pushed to or popped from the top of the stack in constant time, but, in
addition, an LSTM maintains a continuous space embedding of the stack contents.
This lets us formulate an efficient parsing model that captures three facets of
a parser's state: (i) unbounded look-ahead into the buffer of incoming words,
(ii) the complete history of actions taken by the parser, and (iii) the
complete contents of the stack of partially built tree fragments, including
their internal structures. Standard backpropagation techniques are used for
training and yield state-of-the-art parsing performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08082</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08082</id><created>2015-05-29</created><authors><author><keyname>Segu&#xed;</keyname><forenames>Santi</forenames></author><author><keyname>Pujol</keyname><forenames>Oriol</forenames></author><author><keyname>Vitri&#xe0;</keyname><forenames>Jordi</forenames></author></authors><title>Learning to count with deep object features</title><categories>cs.CV</categories><comments>This paper has been accepted at Deep Vision Workshop at CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning to count is a learning strategy that has been recently proposed in
the literature for dealing with problems where estimating the number of object
instances in a scene is the final objective. In this framework, the task of
learning to detect and localize individual object instances is seen as a harder
task that can be evaded by casting the problem as that of computing a
regression value from hand-crafted image features. In this paper we explore the
features that are learned when training a counting convolutional neural network
in order to understand their underlying representation. To this end we define a
counting problem for MNIST data and show that the internal representation of
the network is able to classify digits in spite of the fact that no direct
supervision was provided for them during training. We also present preliminary
results about a deep network that is able to count the number of pedestrians in
a scene.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08088</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08088</id><created>2015-05-27</created><authors><author><keyname>Straight</keyname><forenames>Kevin A.</forenames></author></authors><title>An Analogy Based Method for Freight Forwarding Cost Estimation</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The author explored estimation by analogy (EBA) as a means of estimating the
cost of international freight consignment. A version of the k-Nearest Neighbors
algorithm (k-NN) was tested by predicting job costs from a database of over
5000 actual jobs booked by an Irish freight forwarding firm over a seven year
period. The effect of a computer intensive training process on overall accuracy
of the method was found to be insignificant when the method was implemented
with four or fewer neighbors. Overall, the accuracy of the analogy based
method, while still significantly less accurate than manually working up
estimates, might be worthwhile to implement in practice, depending labor costs
in an adopting firm. A simulation model was used to compare manual versus
analytical estimation methods. The point of indifference occurs when it takes a
firm more than 1.5 worker hours to prepare a manual estimate (at current Irish
labor costs). Suggestions are given for future experiments to improve the
sampling policy of the method to improve accuracy and to improve overall
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08097</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08097</id><created>2015-05-29</created><updated>2015-07-19</updated><authors><author><keyname>McGilvary</keyname><forenames>Gary Andrew</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author><author><keyname>Atkinson</keyname><forenames>Malcolm</forenames></author></authors><title>Ad hoc Cloud Computing: From Concept to Realization</title><categories>cs.DC</categories><comments>Published in IEEE Cloud 2015 named &quot;Ad hoc Cloud Computing&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first complete, integrated and end-to-end solution
for ad hoc cloud computing environments. Ad hoc clouds harvest resources from
existing sporadically available, non-exclusive (i.e. primarily used for some
other purpose) and unreliable infrastructures. In this paper we discuss the
problems ad hoc cloud computing solves and outline our architecture which is
based on BOINC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08098</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08098</id><created>2015-05-29</created><updated>2015-09-11</updated><authors><author><keyname>Bianco</keyname><forenames>Simone</forenames></author><author><keyname>Ciocca</keyname><forenames>Gianluigi</forenames></author><author><keyname>Cusano</keyname><forenames>Claudio</forenames></author></authors><title>CURL: Co-trained Unsupervised Representation Learning for Image
  Classification</title><categories>cs.LG cs.CV stat.ML</categories><comments>Submitted</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a strategy for semi-supervised image classification
that leverages unsupervised representation learning and co-training. The
strategy, that is called CURL from Co-trained Unsupervised Representation
Learning, iteratively builds two classifiers on two different views of the
data. The two views correspond to different representations learned from both
labeled and unlabeled data and differ in the fusion scheme used to combine the
image features. To assess the performance of our proposal, we conducted several
experiments on widely used data sets for scene and object recognition. We
considered three scenarios (inductive, transductive and self-taught learning)
that differ in the strategy followed to exploit the unlabeled data. As image
features we considered a combination of GIST, PHOG, and LBP as well as features
extracted from a Convolutional Neural Network. Moreover, two embodiments of
CURL are investigated: one using Ensemble Projection as unsupervised
representation learning coupled with Logistic Regression, and one based on
LapSVM. The results show that CURL clearly outperforms other supervised and
semi-supervised learning methods in the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08105</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08105</id><created>2015-05-29</created><authors><author><keyname>Baldan</keyname><forenames>Paolo</forenames></author><author><keyname>Bonchi</keyname><forenames>Filippo</forenames></author><author><keyname>Kerstan</keyname><forenames>Henning</forenames></author><author><keyname>K&#xf6;nig</keyname><forenames>Barbara</forenames></author></authors><title>Towards Trace Metrics via Functor Lifting</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We investigate the possibility of deriving metric trace semantics in a
coalgebraic framework. First, we generalize a technique for systematically
lifting functors from the category Set of sets to the category PMet of
pseudometric spaces, showing under which conditions also natural
transformations, monads and distributive laws can be lifted. By exploiting some
recent work on an abstract determinization, these results enable the derivation
of trace metrics starting from coalgebras in Set. More precisely, for a
coalgebra on Set we determinize it, thus obtaining a coalgebra in the
Eilenberg-Moore category of a monad. When the monad can be lifted to PMet, we
can equip the final coalgebra with a behavioral distance. The trace distance
between two states of the original coalgebra is the distance between their
images in the determinized coalgebra through the unit of the monad. We show how
our framework applies to nondeterministic automata and probabilistic automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08107</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08107</id><created>2015-05-29</created><authors><author><keyname>Herrera</keyname><forenames>Roberto H.</forenames></author><author><keyname>Liu</keyname><forenames>Zhaorui</forenames></author><author><keyname>Raffa</keyname><forenames>Natasha</forenames></author><author><keyname>Christensen</keyname><forenames>Paul</forenames></author><author><keyname>Elvers</keyname><forenames>Adrianus</forenames></author></authors><title>Improving Time Estimation by Blind Deconvolution: with Applications to
  TOFD and Backscatter Sizing</title><categories>cs.OH</categories><comments>10 pages, 10 figures, Conference Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a blind deconvolution scheme based on statistical
wavelet estimation. We assume no prior knowledge of the wavelet, and do not
select a reflector from the signal. Instead, the wavelet (ultrasound pulse) is
statistically estimated from the signal itself by a kurtosis-based metric. This
wavelet is then used to deconvolve the RF (radiofrequency) signal through
Wiener filtering, and the resultant zero phase trace is subjected to spectral
broadening by Autoregressive Spectral Extrapolation (ASE). These steps increase
the time resolution of diffraction techniques. Results on synthetic and real
cases show the robustness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08115</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08115</id><created>2015-05-29</created><authors><author><keyname>Martinsson</keyname><forenames>P. G.</forenames></author></authors><title>Blocked rank-revealing QR factorizations: How randomized sampling can be
  used to avoid single-vector pivoting</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a matrix $A$ of size $m\times n$, the manuscript describes a algorithm
for computing a QR factorization $AP=QR$ where $P$ is a permutation matrix, $Q$
is orthonormal, and $R$ is upper triangular. The algorithm is blocked, to allow
it to be implemented efficiently. The need for single vector pivoting in
classical algorithms for computing QR factorizations is avoided by the use of
randomized sampling to find blocks of pivot vectors at once. The advantage of
blocking becomes particularly pronounced when $A$ is very large, and possibly
stored out-of-core, or on a distributed memory machine. The manuscript also
describes a generalization of the QR factorization that allows $P$ to be a
general orthonormal matrix. In this setting, one can at moderate cost compute a
\textit{rank-revealing} factorization where the mass of $R$ is concentrated to
the diagonal entries. Moreover, the diagonal entries of $R$ closely approximate
the singular values of $A$. The algorithms described have asymptotic flop count
$O(m\,n\,\min(m,n))$, just like classical deterministic methods. The scaling
constant is slightly higher than those of classical techniques, but this is
more than made up for by reduced communication and the ability to block the
computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08126</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08126</id><created>2015-05-29</created><updated>2015-11-03</updated><authors><author><keyname>Ben-Or</keyname><forenames>Michael</forenames></author><author><keyname>Eldar</keyname><forenames>Lior</forenames></author></authors><title>The Quasi-Random Perspective on Matrix Spectral Analysis with
  Applications</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the eigenvectors and eigenvalues of a given Hermitian matrix is
arguably one of the most well-studied computational problems. Yet despite its
immense importance, and a vast array of heuristic techniques, there is no
algorithm that can provably approximate the spectral decomposition of any
Hermitian matrix in asymptotic bit-complexity o(n^3). Inspired by the quantum
computing paradigm, we introduce a new perspective on this problem that draws
from the theory of quasi-random, or low-discrepancy sequences - a theory which
has been unconnected to linear-algebra problems thus far. We analyze the
discrepancy of an n-dimensional sequence formed by taking the fractional part
of integer multiples of the vector of eigenvalues of the input matrix. This
analysis gives rise to a conceptually new algorithm to compute an approximate
spectral decomposition of any n x n Hermitian matrix. This algorithm can be
implemented by (randomized) circuits of bit-complexity at most
O(n^{\omega+\nu}) for any \nu&gt; 0. Because it is disjoint from previous
algorithms, we hope it sheds new light on the complexity of approximate
spectral decomposition, in terms of run-time, space complexity, and the number
of random bits required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08128</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08128</id><created>2015-05-29</created><updated>2015-06-01</updated><authors><author><keyname>Sarkar</keyname><forenames>Soumic</forenames></author><author><keyname>Kar</keyname><forenames>Indra Narayan</forenames></author></authors><title>Formation Stabilization with Collision Avoidance of Complex Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two different aspects of formation control of multiple agents subjected to
linear transformation have been addressed in this paper. We consider a set of
complex single integrator systems so that the dimension of the system reduces
to half as opposed to the vector representation in Cartesian coordinate system.
We first design a stable formation controller in an attempt to solve the
formation control turned to stabilization problem and then find a collision
avoidance controller in the transformed domain, respectively. Different linear
transformations are used to facilitate the formation control task in a
different way. For example Jacobi transformation is used to separate the shape
control and trajectory control. The inverse of the transformation must have
nonzero eigenvalues with both positive and negative real parts which may lead
the system to instability. If the inverse of the transformation appears in
closed loop then a diagonal stabilizing matrix is required to reassign the
eigenvalues of the inverse of transformation in the right half of complex
plane. The algorithm to find such stabilizing matrix is provided. We then
define a matrix of potential in the actual domain which is the stepping stone
to find a matrix of potential in the transformed domain. Thus collision
avoidance controller can be designed directly in the transformed domain. The
mathematical proof is given that both the actual and transformed system behaves
identically. Simulation results are provided to support our claim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08138</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08138</id><created>2014-12-24</created><authors><author><keyname>Xu</keyname><forenames>Ziyun</forenames></author></authors><title>Chinese Interpreting Studies: Genesis of a Discipline</title><categories>cs.DL</categories><journal-ref>Forum: International Journal of Interpretation and Translation 12,
  no. 2 (October 2014): 159-90</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of Chinese Interpreting Studies (CIS) has been robust over the
past two decades; this is reflected in the total number of research papers
produced. This paper takes a scientometric approach to assessing the
production, themes and theoretical influences of those papers over time. The
most productive authors, universities, and regions, as well as patterns of
research collaboration, were analyzed to gain a deeper understanding of the CIS
landscape. This study reveals that the general culture of the discipline
remained constant throughout the period, none of its theoretical influences or
topics having gained significantly in popularity. However, certain limitations
in the way research is conducted (lack of collaboration, inadequate academic
policies, etc.) hinder its potential for future growth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08149</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08149</id><created>2015-05-29</created><authors><author><keyname>Kapustin</keyname><forenames>Michael</forenames></author><author><keyname>Kapustin</keyname><forenames>Pavlo</forenames></author></authors><title>Modeling of the meaning: computational interpreting and understanding of
  natural language fragments</title><categories>cs.CL</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this introductory article we present the basics of an approach to
implementing computational interpreting of natural language aiming to model the
meanings of words and phrases. Unlike other approaches, we attempt to define
the meanings of text fragments in a composable and computer interpretable way.
We discuss models and ideas for detecting different types of semantic
incomprehension and choosing the interpretation that makes most sense in a
given context. Knowledge representation is designed for handling
context-sensitive and uncertain / imprecise knowledge, and for easy
accommodation of new information. It stores quantitative information capturing
the essence of the concepts, because it is crucial for working with natural
language understanding and reasoning. Still, the representation is general
enough to allow for new knowledge to be learned, and even generated by the
system. The article concludes by discussing some reasoning-related topics:
possible approaches to generation of new abstract concepts, and describing
situations and concepts in words (e.g. for specifying interpretation
difficulties).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08153</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08153</id><created>2015-05-29</created><authors><author><keyname>Fayyaz</keyname><forenames>Mohsen</forenames></author><author><keyname>Hajizadeh_Saffar</keyname><forenames>Mohammad</forenames></author><author><keyname>Sabokrou</keyname><forenames>Mohammad</forenames></author><author><keyname>Fathy</keyname><forenames>Mahmood</forenames></author></authors><title>Feature Representation for Online Signature Verification</title><categories>cs.CV cs.AI</categories><comments>10 pages, 10 figures, Submitted to IEEE Transactions on Information
  Forensics and Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometrics systems have been used in a wide range of applications and have
improved people authentication. Signature verification is one of the most
common biometric methods with techniques that employ various specifications of
a signature. Recently, deep learning has achieved great success in many fields,
such as image, sounds and text processing. In this paper, deep learning method
has been used for feature extraction and feature selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08154</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08154</id><created>2015-05-29</created><authors><author><keyname>Ghazinour</keyname><forenames>Kambiz</forenames></author><author><keyname>Ghayoumi</keyname><forenames>Mehdi</forenames></author></authors><title>Dynamic Modeling for Representing Access Control Policies Effect</title><categories>cs.CR</categories><comments>6 Pages, ICCS , 2015</comments><msc-class>94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In large databases, creating user interface for browsing or performing
insertion, deletion or modification of data is very costly in terms of
programming. In addition, each modification of an access control policy causes
many potential and unpredictable side effects which cause rule conflicts or
security breaches that affect the corresponding user interfaces as well. While
changes to access control policies in databases are inevitable, having a
dynamic system that generates interface according to the latest access control
policies become increasingly valuable. Lack of such a system leads to
unauthorized access to data and eventually violates the privacy of data owners.
In this work, we discuss a dynamic interface that applies Role Based Access
Control (RBAC) policies as the output of policy analysis and limits the amount
of information that users have access according to the policies defined for
roles. This interface also shows security administrators the effect of their
changes from the user's point of view while minimizing the cost by generating
the interface automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08155</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08155</id><created>2015-05-29</created><authors><author><keyname>Zhang</keyname><forenames>Qun</forenames></author><author><keyname>Youssef</keyname><forenames>Abdou</forenames></author></authors><title>Performance Evaluation and Optimization of Math-Similarity Search</title><categories>cs.IR</categories><comments>15 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Similarity search in math is to find mathematical expressions that are
similar to a user's query. We conceptualized the similarity factors between
mathematical expressions, and proposed an approach to math similarity search
(MSS) by defining metrics based on those similarity factors [11]. Our
preliminary implementation indicated the advantage of MSS compared to
non-similarity based search. In order to more effectively and efficiently
search similar math expressions, MSS is further optimized. This paper focuses
on performance evaluation and optimization of MSS. Our results show that the
proposed optimization process significantly improved the performance of MSS
with respect to both relevance ranking and recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08159</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08159</id><created>2015-04-25</created><authors><author><keyname>Zhao</keyname><forenames>Zhi-Dan</forenames></author><author><keyname>Gao</keyname><forenames>Ya-Chun</forenames></author><author><keyname>Cai</keyname><forenames>Shi-Min</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Dynamic Patterns of Academic Forum Activities</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>10 pages, 7 figures and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mass of traces of human activities show rich dynamic patterns. In this
article, we comprehensively investigate the dynamic patterns of 50 thousands of
researchers' activities in Sciencenet, the largest multi-disciplinary academic
community in China. Through statistical analyses, we found that (i) there
exists a power-law scaling between the frequency of visits to an academic forum
and the number of corresponding visitors, with the exponent being about 1.33;
(ii) the expansion process of academic forums obeys the Heaps' law, namely the
number of distinct visited forums to the number of visits grows in a power-law
form with exponent being about 0.54; (iii) the probability distributions of
time interval and the number of visits taken to revisit the same academic forum
both follow power-laws, indicating the existence of memory effect in academic
forum activities. On the basis of these empirical results, we propose a dynamic
model that incorporates the exploration, preferential return and memory effect,
which can well reproduce the observed scaling laws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.08162</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.08162</id><created>2015-05-29</created><authors><author><keyname>Trotter</keyname><forenames>William T.</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author><author><keyname>Wang</keyname><forenames>Ruidong</forenames></author></authors><title>Dimension and cut vertices: an application of Ramsey theory</title><categories>math.CO cs.DM</categories><msc-class>06A07, 05C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by quite recent research involving the relationship between the
dimension of a poset and graph theoretic properties of its cover graph, we show
that for every $d\geq 1$, if $P$ is a poset and the dimension of a subposet $B$
of $P$ is at most $d$ whenever the cover graph of $B$ is a block of the cover
graph of $P$, then the dimension of $P$ is at most $d+2$. We also construct
examples which show that this inequality is best possible. We consider the
proof of the upper bound to be fairly elegant and relatively compact. However,
we know of no simple proof for the lower bound, and our argument requires a
powerful tool known as the Product Ramsey Theorem. As a consequence, our
constructions involve posets of enormous size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00001</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00001</id><created>2015-05-28</created><updated>2015-06-03</updated><authors><author><keyname>Lu</keyname><forenames>Zhigang</forenames></author><author><keyname>Shen</keyname><forenames>Hong</forenames></author></authors><title>A Security-assured Accuracy-maximised Privacy Preserving Collaborative
  Filtering Recommendation Algorithm</title><categories>cs.CR</categories><comments>arXiv admin note: text overlap with arXiv:1505.07897</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The neighbourhood-based Collaborative Filtering is a widely used method in
recommender systems. However, the risks of revealing customers' privacy during
the process of filtering have attracted noticeable public concern recently.
Specifically, $k$NN attack discloses the target user's sensitive information by
creating $k$ fake nearest neighbours by non-sensitive information. Among the
current solutions against $k$NN attack, the probabilistic methods showed a
powerful privacy preserving effect. However, the existing probabilistic methods
neither guarantee enough prediction accuracy due to the global randomness, nor
provide assured security enforcement against $k$NN attack. To overcome the
problems of current probabilistic methods, we propose a novel approach,
Partitioned Probabilistic Neighbour Selection, to ensure a required security
guarantee while achieving the optimal prediction accuracy against $k$NN attack.
In this paper, we define the sum of $k$ neighbours' similarity as the accuracy
metric $\alpha$, the number of user partitions, across which we select the $k$
neighbours, as the security metric $\beta$. Differing from the present methods
that globally selected neighbours, our method selects neighbours from each
group with exponential differential privacy to decrease the magnitude of noise.
Theoretical and experimental analysis show that to achieve the same security
guarantee against $k$NN attack, our approach ensures the optimal prediction
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00011</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00011</id><created>2015-05-29</created><authors><author><keyname>Logan</keyname><forenames>Brooke</forenames></author><author><keyname>Nguyen</keyname><forenames>Hieu D.</forenames></author></authors><title>Group Symmetries of Complementary Code Matrices</title><categories>cs.IT math.IT</categories><comments>14 pages, 1 figure</comments><msc-class>94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize group symmetries of poly-phase complementary code matrices
(CCMs), which we use to classify CCMs in terms of their equivalence classes. We
also present classification results for CCMs of dimension $N\times 4$ where
$N=2,3,4,5,6$. Finally, we present a new construction to generate quad-phase
CCMs from ternary CCMs and compare this to other existing constructions that
focus on generating CCMs from those of smaller dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00012</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00012</id><created>2015-04-29</created><authors><author><keyname>Dong</keyname><forenames>Jie</forenames></author><author><keyname>Sawyer</keyname><forenames>Nicole</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author></authors><title>The Application of Non-Cooperative Stackelberg Game Theory in Behavioral
  Science: Social Optimality with any Number of Players</title><categories>cs.GT</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we present a ground-breaking new postulate for game theory. The first
part of this postulate contains the axiomatic observation that all games are
created by a designer, whether they are: e.g., (dynamic/static) or
(stationary/non-stationary) or (sequential/one-shot) non-cooperative games, and
importantly, whether or not they are intended to represent a non-cooperative
Stackelberg game, they can be mapped to a Stackelberg game. I.e., the game
designer is the leader who is totally rational and honest, and the followers
are mapped to the players of the designed game. If now the game designer, or
&quot;the leader&quot; in the Stackelberg context, adopts a pure strategy, we postulate
the following second part following from axiomatic observation of ultimate game
leadership, where empirical insight leads to the second part of this postulate.
Importantly, implementing a non-cooperative Stackelberg game, with a very
honest and rational leader results in social optimality for all players
(followers), assuming pure strategy across all followers and leader, and that
the leader is totally rational, honest, and is able to achieve a minimum amount
of competency in leading this game, with any finite number of iterations of
leading this finite game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00019</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00019</id><created>2015-05-29</created><updated>2015-10-17</updated><authors><author><keyname>Lipton</keyname><forenames>Zachary C.</forenames></author><author><keyname>Berkowitz</keyname><forenames>John</forenames></author><author><keyname>Elkan</keyname><forenames>Charles</forenames></author></authors><title>A Critical Review of Recurrent Neural Networks for Sequence Learning</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Countless learning tasks require dealing with sequential data. Image
captioning, speech synthesis, and music generation all require that a model
produce outputs that are sequences. In other domains, such as time series
prediction, video analysis, and musical information retrieval, a model must
learn from inputs that are sequences. Interactive tasks, such as translating
natural language, engaging in dialogue, and controlling a robot, often demand
both capabilities. Recurrent neural networks (RNNs) are connectionist models
that capture the dynamics of sequences via cycles in the network of nodes.
Unlike standard feedforward neural networks, recurrent networks retain a state
that can represent information from an arbitrarily long context window.
Although recurrent neural networks have traditionally been difficult to train,
and often contain millions of parameters, recent advances in network
architectures, optimization techniques, and parallel computation have enabled
successful large-scale learning with them. In recent years, systems based on
long short-term memory (LSTM) and bidirectional (BRNN) architectures have
demonstrated ground-breaking performance on tasks as varied as image
captioning, language translation, and handwriting recognition. In this survey,
we review and synthesize the research that over the past three decades first
yielded and then made practical these powerful learning models. When
appropriate, we reconcile conflicting notation and nomenclature. Our goal is to
provide a self-contained explication of the state of the art together with a
historical perspective and references to primary research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00021</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00021</id><created>2015-05-29</created><authors><author><keyname>Kazhdan</keyname><forenames>Michael</forenames></author><author><keyname>Singh</keyname><forenames>Gurprit</forenames></author><author><keyname>Pilleboue</keyname><forenames>Adrien</forenames></author><author><keyname>Coeurjolly</keyname><forenames>David</forenames></author><author><keyname>Ostromoukhov</keyname><forenames>Victor</forenames></author></authors><title>Variance Analysis for Monte Carlo Integration: A
  Representation-Theoretic Perspective</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we revisit the work of Pilleboue et al. [2015], providing a
representation-theoretic derivation of the closed-form expression for the
expected value and variance in homogeneous Monte Carlo integration. We show
that the results obtained for the variance estimation of Monte Carlo
integration on the torus, the sphere, and Euclidean space can be formulated as
specific instances of a more general theory. We review the related
representation theory and show how it can be used to derive a closed-form
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00022</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00022</id><created>2015-05-29</created><authors><author><keyname>Zhao</keyname><forenames>Xiaohan</forenames></author><author><keyname>Liu</keyname><forenames>Qingyun</forenames></author><author><keyname>Zhou</keyname><forenames>Lin</forenames></author><author><keyname>Zheng</keyname><forenames>Haitao</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author></authors><title>Graph Watermarks</title><categories>cs.CR cs.SI</categories><comments>16 pages, 14 figures, full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From network topologies to online social networks, many of today's most
sensitive datasets are captured in large graphs. A significant challenge facing
owners of these datasets is how to share sensitive graphs with collaborators
and authorized users, e.g. network topologies with network equipment vendors or
Facebook's social graphs with academic collaborators. Current tools can provide
limited node or edge privacy, but require modifications to the graph that
significantly reduce its utility.
  In this work, we propose a new alternative in the form of graph watermarks.
Graph watermarks are small graphs tailor-made for a given graph dataset, a
secure graph key, and a secure user key. To share a sensitive graph G with a
collaborator C, the owner generates a watermark graph W using G, the graph key,
and C's key as input, and embeds W into G to form G'. If G' is leaked by C,its
owner can reliably determine if the watermark W generated for C does in fact
reside inside G', thereby proving C is responsible for the leak. Graph
watermarks serve both as a deterrent against data leakage and a method of
recourse after a leak. We provide robust schemes for creating, embedding and
extracting watermarks, and use analysis and experiments on large, real graphs
to show that they are unique and difficult to forge. We study the robustness of
graph watermarks against both single and powerful colluding attacker models,
then propose and empirically evaluate mechanisms to dramatically improve
resilience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00029</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00029</id><created>2015-05-29</created><authors><author><keyname>Jia</keyname><forenames>Tao</forenames></author><author><keyname>Spivey</keyname><forenames>Robert F.</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw</forenames></author><author><keyname>Korniss</keyname><forenames>Gyorgy</forenames></author></authors><title>An Analysis of the Matching Hypothesis in Networks</title><categories>physics.soc-ph cs.SI</categories><journal-ref>PLoS ONE 10(6): e0129804 (2015)</journal-ref><doi>10.1371/journal.pone.0129804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The matching hypothesis in social psychology claims that people are more
likely to form a committed relationship with someone equally attractive.
Previous works on stochastic models of human mate choice process indicate that
patterns supporting the matching hypothesis could occur even when similarity is
not the primary consideration in seeking partners. Yet, most if not all of
these works concentrate on fully-connected systems. Here we extend the analysis
to networks. Our results indicate that the correlation of the couple's
attractiveness grows monotonically with the increased average degree and
decreased degree diversity of the network. This correlation is lower in sparse
networks than in fully-connected systems, because in the former less attractive
individuals who find partners are likely to be coupled with ones who are more
attractive than them. The chance of failing to be matched decreases
exponentially with both the attractiveness and the degree. The matching
hypothesis may not hold when the degree-attractiveness correlation is present,
which can give rise to negative attractiveness correlation. Finally, we find
that the ratio between the number of matched couples and the size of the
maximum matching varies non-monotonically with the average degree of the
network. Our results reveal the role of network topology in the process of
human mate choice and bring insights into future investigations of different
matching processes in networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00034</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00034</id><created>2015-05-29</created><authors><author><keyname>Doss</keyname><forenames>Charles R.</forenames></author></authors><title>Bracketing Numbers of Convex Functions on Polytopes</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>19 pages</comments><msc-class>52A41, 41A46, 52A27, 52B11, 52C17, 62G20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study bracketing numbers for spaces of bounded convex functions in the
$L_p$ norms. We impose no Lipschitz constraint. Previous results gave bounds
when the domain of the functions is a hyperrectangle. We extend these results
to the case wherein the domain is a polytope. Bracketing numbers are crucial
quantities for understanding asymptotic behavior for many statistical
nonparametric estimators. Our results are of interest in particular in many
multidimensional estimation problems based on convexity shape constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00036</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00036</id><created>2015-05-29</created><updated>2015-07-11</updated><authors><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Massaro</keyname><forenames>Emanuele</forenames></author><author><keyname>Bojic</keyname><forenames>Iva</forenames></author><author><keyname>Arias</keyname><forenames>Juan Murillo</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Predicting Regional Economic Indices using Big Data of Individual Bank
  Card Transactions</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>12 pages, 6 figures</comments><msc-class>91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For centuries quality of life was a subject of studies across different
disciplines. However, only with the emergence of a digital era, it became
possible to investigate this topic on a larger scale. Over time it became clear
that quality of life not only depends on one, but on three relatively different
parameters: social, economic and well-being measures. In this study we focus
only on the first two, since the last one is often very subjective and
consequently hard to measure. Using a complete set of bank card transactions
recorded by Banco Bilbao Vizcaya Argentaria (BBVA) during 2011 in Spain, we
first create a feature space by defining various meaningful characteristics of
a particular area performance through activity of its businesses, residents and
visitors. We then evaluate those quantities by considering available official
statistics for Spanish provinces (e.g., housing prices, unemployment rate, life
expectancy) and investigate whether they can be predicted based on our feature
space. For the purpose of prediction, our study proposes a supervised machine
learning approach. Our finding is that there is a clear correlation between
individual spending behavior and official socioeconomic indexes denoting
quality of life. Moreover, we believe that this modus operandi is useful to
understand, predict and analyze the impact of human activity on the wellness of
our society on scales for which there is no consistent official statistics
available (e.g., cities and towns, districts or smaller neighborhoods).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00037</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00037</id><created>2015-05-29</created><authors><author><keyname>Park</keyname><forenames>Gilchan</forenames></author><author><keyname>Taylor</keyname><forenames>Julia M.</forenames></author></authors><title>Using Syntactic Features for Phishing Detection</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on the comparison of the subject and object of verbs in
their usage between phishing emails and legitimate emails. The purpose of this
research is to explore whether the syntactic structures and subjects and
objects of verbs can be distinguishable features for phishing detection. To
achieve the objective, we have conducted two series of experiments: the
syntactic similarity for sentences, and the subject and object of verb
comparison. The results of the experiments indicated that both features can be
used for some verbs, but more work has to be done for others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00051</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00051</id><created>2015-05-29</created><authors><author><keyname>Duarte</keyname><forenames>Leonardo A.</forenames></author><author><keyname>Penatti</keyname><forenames>Ot&#xe1;vio A. B.</forenames></author><author><keyname>Almeida</keyname><forenames>Jurandy</forenames></author></authors><title>Bag-of-Genres for Video Genre Retrieval</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a higher level representation for videos aiming at video
genre retrieval. In video genre retrieval, there is a challenge that videos may
comprise multiple categories, for instance, news videos may be composed of
sports, documentary, and action. Therefore, it is interesting to encode the
distribution of such genres in a compact and effective manner. We propose to
create a visual dictionary using a genre classifier. Each visual word in the
proposed model corresponds to a region in the classification space determined
by the classifier's model learned on the training frames. Therefore, the video
feature vector contains a summary of the activations of each genre in its
contents. We evaluate the bag-of-genres model for video genre retrieval, using
the dataset of MediaEval Tagging Task of 2012. Results show that the proposed
model increases the quality of the representation being more compact than
existing features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00059</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00059</id><created>2015-05-29</created><authors><author><keyname>Arjovsky</keyname><forenames>Martin</forenames></author></authors><title>Saddle-free Hessian-free optimization for Deep Learning</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a variant of the Hessian-free optimization method by Martens
(2010) but which implements the saddle-free Newton method (Dauphin et al, 2014)
instead of classical Newton. It does this in linear time in the amount of
parameters in the network, which makes it scalable to very large problems. It
is also easy to use, stable, and does not make any low rank approximation of
any version of the Hessian. Finally, it is memory efficient, since it does not
store any matrix, and uses only matrix-vector products to solve the problem at
hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00060</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00060</id><created>2015-05-29</created><authors><author><keyname>Cai</keyname><forenames>Xiaohao</forenames></author><author><keyname>Chan</keyname><forenames>Raymond</forenames></author><author><keyname>Nikolova</keyname><forenames>Mila</forenames></author><author><keyname>Zeng</keyname><forenames>Tieyong</forenames></author></authors><title>A Three-stage Approach for Segmenting Degraded Color Images: Smoothing,
  Lifting and Thresholding (SLaT)</title><categories>cs.CV math.NA</categories><comments>19 pages</comments><msc-class>65F22</msc-class><acm-class>I.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a SLaT (Smoothing, Lifting and Thresholding) method
with three stages for multiphase segmentation of color images corrupted by
different degradations: noise, information loss, and blur. At the first stage,
a convex variant of the Mumford-Shah model is applied to each channel to obtain
a smooth image. We show that the model has unique solution under the different
degradations. In order to properly handle the color information, the second
stage is dimension lifting where we consider a new vector-valued image composed
of the restored image and its transform in the secondary color space with
additional information. This ensures that even if the first color space has
highly correlated channels, we can still have enough information to give good
segmentation results. In the last stage, we apply multichannel thresholding to
the combined vector-valued image to find the segmentation. The number of phases
is only required in the last stage, so users can choose or change it all
without the need of solving the previous stages again. Experiments demonstrate
that our SLaT method gives excellent results in terms of segmentation quality
and CPU time in comparison with other state-of-the-art segmentation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00063</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00063</id><created>2015-05-29</created><authors><author><keyname>Zhu</keyname><forenames>Daxin</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Wu</keyname><forenames>Yingjie</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>An Efficient Dynamic Programming Algorithm for STR-IC-SEQ-EC-LCS Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a generalized longest common subsequence problem,
in which a constraining sequence of length $s$ must be included as a substring
and the other constraining sequence of length $t$ must be excluded as a
subsequence of two main sequences and the length of the result must be maximal.
For the two input sequences $X$ and $Y$ of lengths $n$ and $m$, and the given
two constraining sequences of length $s$ and $t$, we present an $O(nmst)$ time
dynamic programming algorithm for solving the new generalized longest common
subsequence problem. The time complexity can be reduced further to cubic time
in a more detailed analysis. The correctness of the new algorithm is proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00066</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00066</id><created>2015-05-29</created><authors><author><keyname>Bash</keyname><forenames>Boulat A.</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Hiding Information in Noise: Fundamental Limits of Covert Wireless
  Communication</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures</comments><journal-ref>IEEE Communications Magazine 53.12 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Widely-deployed encryption-based security prevents unauthorized decoding, but
does not ensure undetectability of communication. However, covert, or low
probability of detection/intercept (LPD/LPI) communication is crucial in many
scenarios ranging from covert military operations and the organization of
social unrest, to privacy protection for users of wireless networks. In
addition, encrypted data or even just the transmission of a signal can arouse
suspicion, and even the most theoretically robust encryption can often be
defeated by a determined adversary using non-computational methods such as
side-channel analysis. Various covert communication techniques were developed
to address these concerns, including steganography for finite-alphabet
noiseless applications and spread-spectrum systems for wireless communications.
After reviewing these covert communication systems, this article discusses new
results on the fundamental limits of their capabilities, as well as provides a
vision for the future of such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00071</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00071</id><created>2015-05-30</created><authors><author><keyname>Brittenham</keyname><forenames>Mark</forenames></author><author><keyname>Hermiller</keyname><forenames>Susan</forenames></author><author><keyname>Johnson</keyname><forenames>Ashley</forenames></author></authors><title>Homology and closure properties of autostackable groups</title><categories>math.GR cs.FL</categories><comments>20 pages</comments><msc-class>20F65 (Primary) 20F10, 68Q42 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autostackability for finitely presented groups is a topological property of
the Cayley graph combined with formal language theoretic restrictions, that
implies solvability of the word problem. The class of autostackable groups is
known to include all asynchronously automatic groups with respect to a
prefix-closed normal form set, and all groups admitting finite complete
rewriting systems. Although groups in the latter two classes all satisfy the
homological finiteness condition $FP_\infty$, we show that the class of
autostackable groups includes a group that is not of type $FP_3$. We also show
that the class of autostackable groups is closed under graph products and
extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00074</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00074</id><created>2015-05-30</created><authors><author><keyname>Huang</keyname><forenames>Yi-bin</forenames></author><author><keyname>Li</keyname><forenames>Kang</forenames></author><author><keyname>Wang</keyname><forenames>Ge</forenames></author><author><keyname>Cao</keyname><forenames>Min</forenames></author><author><keyname>Li</keyname><forenames>Pin</forenames></author><author><keyname>Zhang</keyname><forenames>Yu-jia</forenames></author></authors><title>Recognition of convolutional neural network based on CUDA Technology</title><categories>cs.DC cs.NE</categories><comments>5 pages, 3 figures and 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  For the problem whether Graphic Processing Unit(GPU),the stream processor
with high performance of floating-point computing is applicable to neural
networks, this paper proposes the parallel recognition algorithm of
Convolutional Neural Networks(CNNs).It adopts Compute Unified Device
Architecture(CUDA)technology, definite the parallel data structures, and
describes the mapping mechanism for computing tasks on CUDA. It compares the
parallel recognition algorithm achieved on GPU of GTX200 hardware architecture
with the serial algorithm on CPU. It improves speed by nearly 60 times. Result
shows that GPU based the stream processor architecture ate more applicable to
some related applications about neural networks than CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00080</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00080</id><created>2015-05-30</created><updated>2015-07-03</updated><authors><author><keyname>Gadaleta</keyname><forenames>Francesco</forenames></author><author><keyname>Bessonov</keyname><forenames>Kyrylo</forenames></author></authors><title>Integration of Gene Expression Data and Methylation Reveals Genetic
  Networks for Glioblastoma</title><categories>cs.CE q-bio.GN</categories><comments>This paper has been withdrawn by the author due to submission to
  commercial journal</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Motivation: The consistent amount of different types of omics data requires
novel methods of analysis and data integration. In this work we describe
Regression2Net, a computational approach to analyse gene expression and
methylation profiles via regression analysis and network-based techniques.
  Results: We identified 284 and 447 unique candidate genes potentially
associated to the Glioblastoma pathology from two networks inferred from mixed
genetic datasets. In-depth biological analysis of these networks reveals genes
that are related to energy metabolism, cell cycle control (AATF), immune system
response and several types of cancer. Importantly, we observed significant
over- representation of cancer related pathways including glioma especially in
the methylation network. This confirms the strong link between methylation and
glioblastomas. Potential glioma suppressor genes ACCN3 and ACCN4 linked to
NBPF1 neuroblastoma breakpoint family have been identified in our expression
network. Numerous ABC transporter genes (ABCA1, ABCB1) present in the
expression network suggest drug resistance of glioblastoma tumors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00081</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00081</id><created>2015-05-30</created><updated>2015-06-06</updated><authors><author><keyname>Acikmese</keyname><forenames>Behcet</forenames></author></authors><title>Linear Matrix Inequalities for Ultimate Boundedness of Dynamical Systems
  with Conic Uncertain/Nonlinear Terms</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note introduces a sufficient Linear Matrix Inequality (LMI) condition
for the ultimate boundedness of a class of continuous-time dynamical systems
with conic uncertain/nonlinear terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00086</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00086</id><created>2015-05-30</created><authors><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>Simulating a die roll by flipping two coins</title><categories>math.CO cs.DM</categories><comments>2 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to simulate a roll of a fair $n$-sided die by one flip of a
biased coin with probability $1/n$ of coming up heads, followed by
$3\lfloor\log_2 n \rfloor+1$ flips of a fair coin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00091</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00091</id><created>2015-05-30</created><authors><author><keyname>Murti</keyname><forenames>Tri</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Sobri</keyname><forenames>Muhammad</forenames></author></authors><title>Sistem penunjang keputusan kelayakan pemberian pinjaman dengna metode
  fuzzy tsukamoto</title><categories>cs.AI</categories><comments>5 pages, in Indonesian, in Seminar Nasional Inovasi dan Tren 2015
  (SNIT2015), Bekasi, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision support systems (DSS) can be used to help settlement issues or
decisions that are semi-structured or structured. The method used is Fuzzy
Tsukamoto. PT Triprima Finance is a company engaged in the service sector
lending with collateral in the form of Motor Vehicle Owner Book or car (reg).
PT. Triprima Finance should consider borrowing from its customers with the
consent of the head manager. Such approval requires a long time because they
have to pass through many stages of the reporting procedure. Decision-making
activities at PT Triprima Finance carried out by the analysis process manually.
To help overcome these problems, the need for completion method in accuracy and
speed of decision making feasibility of lending. To overcome this need to
develop a new system that is a decision support system Tsukamoto fuzzy method.
is expected to facilitate kaposko to determine the decisions to be taken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00092</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00092</id><created>2015-05-30</created><authors><author><keyname>Nikolakopoulos</keyname><forenames>Athanasios N.</forenames></author><author><keyname>Garofalakis</keyname><forenames>John D.</forenames></author></authors><title>Random Surfing Without Teleportation</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>14 pages, accepted for publication in Kontogiannis et al. (eds),
  Algorithms Probability Networks and Games, Springer</comments><doi>10.1007/978-3-319-24024-4_19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the standard Random Surfer Model, the teleportation matrix is necessary to
ensure that the final PageRank vector is well-defined. The introduction of this
matrix, however, results in serious problems and imposes fundamental
limitations to the quality of the ranking vectors. In this work, building on
the recently proposed NCDawareRank framework, we exploit the decomposition of
the underlying space into blocks, and we derive easy to check necessary and
sufficient conditions for random surfing without teleportation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00093</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00093</id><created>2015-05-30</created><authors><author><keyname>Klets</keyname><forenames>Dmytro</forenames></author></authors><title>Modeling of Mobile Vehicle Skid in Traction Movement Mode</title><categories>cs.SY</categories><comments>6 pages, 2 figures</comments><msc-class>68W30</msc-class><acm-class>I.1.2</acm-class><journal-ref>Motrol 15_7 (2013) 157-162</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, a mathematical model assembling a &quot;driver - mobile vehicle -
road environment&quot; system and capable of simulating the process of mobile
vehicles skid in traction movement mode is proposed. The usage of non-linear
drift models allows the development of efficient algorithms for mobile vehicles
dynamic stabilization systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00095</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00095</id><created>2015-05-30</created><updated>2015-06-15</updated><authors><author><keyname>Avramopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>On the computational complexity of evolution</title><categories>cs.CC cs.GT</categories><comments>I am withdrawing the claim that NP = coNP. The community working on
  the hardness of nonlinear optimization problems uses the term &quot;NP-hard&quot; to
  mean hardness under Turing (rather than Karp) reductions and does not
  distinguish between &quot;NP-hard&quot; and &quot;coNP-hard&quot; problems. Therefore, my proof
  that NP = coNP has a flaw in its argument. The rest of the results are
  correct though</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the problem of recognizing an ESS in a symmetric
bimatrix game is coNP-complete. In this paper, we show that recognizing an ESS
even in doubly symmetric bimatrix games is also coNP-complete. Our result
further implies that recognizing asymptotically stable equilibria of the
replicator dynamic in this class of games is also a coNP-complete problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00097</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00097</id><created>2015-05-30</created><authors><author><keyname>James</keyname><forenames>Alex Pappachen</forenames></author><author><keyname>Dasarathy</keyname><forenames>Belur</forenames></author></authors><title>A Review of Feature and Data Fusion with Medical Images</title><categories>cs.CV</categories><comments>Multisensor Data Fusion: From Algorithm and Architecture Design to
  Applications, CRC Press, 2015. arXiv admin note: substantial text overlap
  with arXiv:1401.0166</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The fusion techniques that utilize multiple feature sets to form new features
that are often more robust and contain useful information for future processing
are referred to as feature fusion. The term data fusion is applied to the class
of techniques used for combining decisions obtained from multiple feature sets
to form global decisions. Feature and data fusion interchangeably represent two
important classes of techniques that have proved to be of practical importance
in a wide range of medical imaging problems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00099</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00099</id><created>2015-05-30</created><authors><author><keyname>Azizi</keyname><forenames>Reza</forenames></author><author><keyname>Sedghi</keyname><forenames>Hasan</forenames></author><author><keyname>Shoja</keyname><forenames>Hamid</forenames></author><author><keyname>Sepas-Moghaddam</keyname><forenames>Alireza</forenames></author></authors><title>A Novel Energy Aware Node Clustering Algorithm for Wireless Sensor
  Networks Using a Modified Artificial Fish Swarm Algorithm</title><categories>cs.AI cs.NI</categories><comments>13 pages, 5 figures, 2 tables, International Journal of Computer
  Networks &amp; Communications(IJCNC) Vol.7, No.3, May 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering problems are considered amongst the prominent challenges in
statistics and computational science. Clustering of nodes in wireless sensor
networks which is used to prolong the life-time of networks is one of the
difficult tasks of clustering procedure. In order to perform nodes clustering,
a number of nodes are determined as cluster heads and other ones are joined to
one of these heads, based on different criteria e.g. Euclidean distance. So
far, different approaches have been proposed for this process, where swarm and
evolutionary algorithms contribute in this regard. In this study, a novel
algorithm is proposed based on Artificial Fish Swarm Algorithm (AFSA) for
clustering procedure. In the proposed method, the performance of the standard
AFSA is improved by increasing balance between local and global searches.
Furthermore, a new mechanism has been added to the base algorithm for improving
convergence speed in clustering problems. Performance of the proposed technique
is compared to a number of state-of-the-art techniques in this field and the
outcomes indicate the supremacy of the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00100</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00100</id><created>2015-05-30</created><authors><author><keyname>Razis</keyname><forenames>Gerasimos</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>Discovering similar Twitter accounts using semantics</title><categories>cs.SI</categories><comments>Draft version of paper. arXiv admin note: substantial text overlap
  with arXiv:1409.3771</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On daily basis, millions of Twitter accounts post a vast number of tweets
including numerous Twitter entities (mentions, replies, hashtags, photos,
URLs). Many of these entities are used in common by many accounts. The more
common entities are found in the messages of two different accounts, the more
similar, in terms of content or interest, they tend to be. Towards this
direction, we introduce a methodology for discovering and suggesting similar
Twitter accounts, based entirely on their disseminated content in terms of
Twitter entities used. The methodology is based exclusively on semantic
representation protocols and related technologies. An ontological schema is
also described towards the semantification of the Twitter accounts and their
entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00102</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00102</id><created>2015-05-30</created><authors><author><keyname>Bellot</keyname><forenames>Pau</forenames></author><author><keyname>Meyer</keyname><forenames>Patrick E.</forenames></author></authors><title>Efficient combination of pairswise feature networks</title><categories>stat.ML cs.LG</categories><comments>JMLR: Workshop and Conference Proceedings, 2014 Connectomics (ECML
  2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel method for the reconstruction of a neural network
connectivity using calcium fluorescence data. We introduce a fast unsupervised
method to integrate different networks that reconstructs structural
connectivity from neuron activity. Our method improves the state-of-the-art
reconstruction method General Transfer Entropy (GTE). We are able to better
eliminate indirect links, improving therefore the quality of the network via a
normalization and ensemble process of GTE and three new informative features.
The approach is based on a simple combination of networks, which is remarkably
fast. The performance of our approach is benchmarked on simulated time series
provided at the connectomics challenge and also submitted at the public
competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00108</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00108</id><created>2015-05-30</created><authors><author><keyname>Rozaki</keyname><forenames>Eleni</forenames></author></authors><title>Design and implementation for automated network troubleshooting using
  data mining</title><categories>cs.NI cs.SE</categories><comments>19 pages, 7 figures, International Journal of Data Mining &amp; Knowledge
  Management Process (IJDKP) Vol.5, No.3, May 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficient and effective monitoring of mobile networks is vital given the
number of users who rely on such networks and the importance of those networks.
The purpose of this paper is to present a monitoring scheme for mobile networks
based on the use of rules and decision tree data mining classifiers to upgrade
fault detection and handling. The goal is to have optimisation rules that
improve anomaly detection. In addition, a monitoring scheme that relies on
Bayesian classifiers was also implemented for the purpose of fault isolation
and localisation. The data mining techniques described in this paper are
intended to allow a system to be trained to actually learn network fault rules.
The results of the tests that were conducted allowed for the conclusion that
the rules were highly effective to improve network troubleshooting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00122</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00122</id><created>2015-05-30</created><authors><author><keyname>Sabokrou</keyname><forenames>Mohammad</forenames></author><author><keyname>Fathy</keyname><forenames>Mahmood</forenames></author><author><keyname>Hoseini</keyname><forenames>Mojtaba</forenames></author></authors><title>IDSA: Intelligent Distributed Sensor Activation Algorithm For Target
  Tracking With Wireless Sensor Network</title><categories>cs.NI cs.NE</categories><comments>18 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One important application of the Wireless Sensor Network(WSN) is target
tracking, the aim of this application is converging to an event or object in an
area. In this paper, we propose an energy-efficient distributed sensor
activation protocol based on predicted location technique, called Intelligent
Distributed Sensor Activation Algorithm (IDSA). The proposed algorithm predicts
the location of target in the next time interval, by analyzing current location
and movement history of the target, this prediction is done by computational
intelligence. The fewest essential number of sensor nodes within the predicted
location will be activated to cover the target. The results show that the
proposed method outperforms the existing methods such as Na\&quot;ive and DSA in
terms of energy consumption and the number of nodes that was involved in
tracking the target.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00128</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00128</id><created>2015-05-30</created><authors><author><keyname>Quaresma</keyname><forenames>Pedro</forenames></author><author><keyname>Santos</keyname><forenames>Vanda</forenames></author><author><keyname>Mari&#x107;</keyname><forenames>Milena</forenames></author></authors><title>A Web Environment for Geometry</title><categories>cs.CY</categories><comments>CICM 2015, Conference on Intelligent Computer Mathematics
  (Work-in-Progress track), July 13-17, 2015, Washington DC, USA</comments><msc-class>97G40</msc-class><acm-class>K.3.1, K.3.2, I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web Geometry Laboratory, (WGL), is a blended-learning, collaborative and
adaptive, Web environment for geometry. It integrates a well known dynamic
geometry system. In a collaborative session, exchange of geometrical and
textual information between the user engaged in the session is possible. In a
normal work session (stand-alone mode), all the geometric steps done by the
students are recorded, alongside the navigation information, allowing, in a
latter stage, their teachers to &quot;play back&quot; the students sessions, using that
info to assert the students level and adjust the teaching strategies to each
individual student. Teachers can register and begin using one of the public
servers, defining students, preparing materials to be released to the students,
open collaborative sessions, etc. Using an action research methodology the WGL
system is being developed, validated through case-studies, and further
improved, in a cycle where the implementation steps are intertwined with case
studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00130</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00130</id><created>2015-05-30</created><authors><author><keyname>Guo</keyname><forenames>Qingpei</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author><author><keyname>Song</keyname><forenames>Yang</forenames></author></authors><title>The Implementation of Hadoop-based Crawler System and Graphlite-based
  PageRank-Calculation In Search Engine</title><categories>cs.DC</categories><comments>8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the size of the Internet is experiencing rapid growth. As of
December 2014, the number of global Internet websites has more than 1 billion
and all kinds of information resources are integrated together on the Internet,
however,the search engine is to be a necessary tool for all users to retrieve
useful information from vast amounts of web data. Generally speaking, a
complete search engine includes the crawler system, index building systems,
sorting systems and retrieval system. At present there are many open source
implementation of search engine, such as lucene, solr, katta, elasticsearch,
solandra and so on. The crawler system and sorting system is indispensable for
any kind of search engine and in order to guarantee its efficiency, the former
needs to update crawled vast amounts of data and the latter requires real-time
to build index on newly crawled web pages and calculae its corresponding
PageRank value. It is unlikely to accomplish such huge computation tasks
depending on a single hardware implementation of the crawler system and sorting
system,from which aspect, the distributed cluster technology is brought to the
front. In this paper, we use the hadoop Map - Reduce computing framework to
implement a distributed crawler system, and use the GraphLite, a distributed
synchronous graph-computing framework, to achieve the real-time computation in
getting the PageRank value of the new crawled web page.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00133</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00133</id><created>2015-05-30</created><authors><author><keyname>Cegielski</keyname><forenames>Patrick</forenames></author><author><keyname>Grigorieff</keyname><forenames>Serge</forenames></author><author><keyname>Guessarian</keyname><forenames>Irene</forenames></author></authors><title>Characterizing congruence preserving functions $Z/nZ\to Z/mZ$ via
  rational polynomials</title><categories>math.NT cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a basis of rational polynomial-like functions
$P_0,\ldots,P_{n-1}$ for the free module of functions $Z/nZ\to Z/mZ$. We then
characterize the subfamily of congruence preserving functions as the set of
linear combinations of the functions $lcm(k)\,P_k$ where $lcm(k)$ is the least
common multiple of $2,\ldots,k$ (viewed in $Z/mZ$). As a consequence, when
$n\geq m$, the number of such functions is independent of $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00147</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00147</id><created>2015-05-30</created><authors><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Raghu</keyname><forenames>Maithra</forenames></author></authors><title>Team Performance with Test Scores</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Team performance is a ubiquitous area of inquiry in the social sciences, and
it motivates the problem of team selection -- choosing the members of a team
for maximum performance. Influential work of Hong and Page has argued that
testing individuals in isolation and then assembling the highest-scoring ones
into a team is not an effective method for team selection. For a broad class of
performance measures, based on the expected maximum of random variables
representing individual candidates, we show that tests directly measuring
individual performance are indeed ineffective, but that a more subtle family of
tests used in isolation can provide a constant-factor approximation for team
performance. These new tests measure the 'potential' of individuals, in a
precise sense, rather than performance; to our knowledge they represent the
first time that individual tests have been shown to produce near-optimal teams
for a non-trivial team performance measure. We also show families of
subdmodular and supermodular team performance functions for which no test
applied to individuals can produce near-optimal teams, and discuss implications
for submodular maximization via hill-climbing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00154</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00154</id><created>2015-05-30</created><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Resolvability in E{\gamma} with Applications to Lossy Compression and
  Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>ISIT 2015</comments><journal-ref>2015 IEEE International Symposium on Information Theory (ISIT),
  14-19 June 2015, pages: 755 - 759, Hong Kong</journal-ref><doi>10.1109/ISIT.2015.7282556</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the amount of randomness needed for an input process to approximate
a given output distribution of a channel in the $E_{\gamma}$ distance. A
general one-shot achievability bound for the precision of such an approximation
is developed. In the i.i.d.~setting where $\gamma=\exp(nE)$, a (nonnegative)
randomness rate above $\inf_{Q_{\sf U}: D(Q_{\sf X}||\pi_{\sf X})\le E}
\{D(Q_{\sf X}||\pi_{\sf X})+I(Q_{\sf U},Q_{\sf X|U})-E\}$ is necessary and
sufficient to asymptotically approximate the output distribution $\pi_{\sf
X}^{\otimes n}$ using the channel $Q_{\sf X|U}^{\otimes n}$, where $Q_{\sf
U}\to Q_{\sf X|U}\to Q_{\sf X}$. The new resolvability result is then used to
derive a one-shot upper bound on the error probability in the rate distortion
problem, and a lower bound on the size of the eavesdropper list to include the
actual message in the wiretap channel problem. Both bounds are asymptotically
tight in i.i.d.~settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00165</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00165</id><created>2015-05-30</created><updated>2015-06-02</updated><authors><author><keyname>Moran</keyname><forenames>Shay</forenames></author><author><keyname>Warmuth</keyname><forenames>Manfred K.</forenames></author></authors><title>Labeled compression schemes for extremal classes</title><categories>cs.LG cs.DM math.CO</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a long-standing open problem whether there always exists a compression
scheme whose size is of the order of the Vapnik-Chervonienkis (VC) dimension
$d$. Recently compression schemes of size exponential in $d$ have been found
for any concept class of VC dimension $d$. Previously size $d$ unlabeled
compression scheme have been given for maximum classes, which are special
concept classes whose size equals an upper bound due to Sauer-Shelah. We
consider a natural generalization of the maximum classes called extremal
classes. Their definition is based on a generalization of the Sauer-Shelah
bound called the Sandwich Theorem which has applications in many areas of
combinatorics. The key result of the paper is the construction of a labeled
compression scheme for extremal classes of size equal to their VC dimension. We
also give a number of open problems concerning the combinatorial structure of
extremal classes and the existence of unlabeled compression schemes for them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00168</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00168</id><created>2015-05-30</created><authors><author><keyname>Eppel</keyname><forenames>Sagi</forenames></author></authors><title>Using curvature to distinguish between surface reflections and vessel
  contents in computer vision based recognition of materials in transparent
  vessels</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recognition of materials and objects inside transparent containers using
computer vision has a wide range of applications, ranging from industrial
bottles filling to the automation of chemistry laboratory. One of the main
challenges in such recognition is the ability to distinguish between image
features resulting from the vessels surface and image features resulting from
the material inside the vessel. Reflections and the functional parts of a
vessels surface can create strong edges that can be mistakenly identified as
corresponding to the vessel contents, and cause recognition errors. The ability
to evaluate whether a specific edge in an image stems from the vessels surface
or from its contents can considerably improve the ability to identify materials
inside transparent vessels. This work will suggest a method for such
evaluation, based on the following two assumptions: 1) Areas of high curvature
on the vessel surface are likely to cause strong edges due to changes in
reflectivity, as is the appearance of functional parts (e.g. corks or valves).
2) Most transparent vessels (bottles, glasses) have high symmetry
(cylindrical). As a result the curvature angle of the vessels surface at each
point of the image is similar to the curvature angle of the contour line of the
vessel in the same row in the image. These assumptions, allow the
identification of image regions with strong edges corresponding to the vessel
surface reflections. Combining this method with existing image analysis methods
for detecting materials inside transparent containers allows considerable
improvement in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00176</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00176</id><created>2015-05-30</created><authors><author><keyname>Qiu</keyname><forenames>Liquan</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author><author><keyname>Dai</keyname><forenames>Ruifen</forenames></author><author><keyname>Zhang</keyname><forenames>Yuxiang</forenames></author><author><keyname>Li</keyname><forenames>Lei</forenames></author></authors><title>An Open Source Testing Tool for Evaluating Handwriting Input Methods</title><categories>cs.HC cs.CV</categories><comments>5 pages, 3 figures, 11 tables. Accepted to appear at ICDAR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an open source tool for testing the recognition accuracy
of Chinese handwriting input methods. The tool consists of two modules, namely
the PC and Android mobile client. The PC client reads handwritten samples in
the computer, and transfers them individually to the Android client in
accordance with the socket communication protocol. After the Android client
receives the data, it simulates the handwriting on screen of client device, and
triggers the corresponding handwriting recognition method. The recognition
accuracy is recorded by the Android client. We present the design principles
and describe the implementation of the test platform. We construct several test
datasets for evaluating different handwriting recognition systems, and conduct
an objective and comprehensive test using six Chinese handwriting input methods
with five datasets. The test results for the recognition accuracy are then
compared and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00178</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00178</id><created>2015-05-30</created><updated>2015-07-30</updated><authors><author><keyname>Richier</keyname><forenames>Cedric</forenames></author><author><keyname>Elazouzi</keyname><forenames>Rachid</forenames></author><author><keyname>Jimenez</keyname><forenames>Tania</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author><author><keyname>Linares</keyname><forenames>Georges</forenames></author></authors><title>Forecasting popularity of videos in YouTube</title><categories>cs.SI</categories><comments>Paper withdrawn due to conflict of interest in conference call for
  paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new prediction process to explain and predict
popularity evolution of YouTube videos. We exploit our recent study on the
classification of YouTube videos in order to predict the evolution of videos'
view-count. This classification allows to identify important factors of the
observed popularity dynamics. Our experimental results show that our prediction
process is able to reduce the average prediction errors compared to a
state-of-the-art baseline model. We also evaluate the impact of adding
popularity criteria in our classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00179</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00179</id><created>2015-05-30</created><authors><author><keyname>Daubechies</keyname><forenames>Ingrid</forenames></author><author><keyname>Saab</keyname><forenames>Rayan</forenames></author></authors><title>A Deterministic Analysis of Decimation for Sigma-Delta Quantization of
  Bandlimited Functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Sigma-Delta ($\Sigma\Delta$) quantization of oversampled bandlimited
functions. We prove that digitally integrating blocks of bits and then
down-sampling, a process known as decimation, can efficiently encode the
associated $\Sigma\Delta$ bit-stream. It allows a large reduction in the
bit-rate while still permitting good approximation of the underlying
bandlimited function via an appropriate reconstruction kernel. Specifically, in
the case of stable $r$th order $\Sigma\Delta$ schemes we show that the
reconstruction error decays exponentially in the bit-rate. For example, this
result applies to the 1-bit, greedy, first-order $\Sigma\Delta$ scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00189</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00189</id><created>2015-05-30</created><updated>2015-08-31</updated><authors><author><keyname>Braun</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Jana</keyname><forenames>Suman</forenames></author><author><keyname>Boneh</keyname><forenames>Dan</forenames></author></authors><title>Robust and Efficient Elimination of Cache and Timing Side Channels</title><categories>cs.CR</categories><comments>15 pages, 10 figures, submitted to NDSS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Timing and cache side channels provide powerful attacks against many
sensitive operations including cryptographic implementations. Existing defenses
cannot protect against all classes of such attacks without incurring
prohibitive performance overhead. A popular strategy for defending against all
classes of these attacks is to modify the implementation so that the timing and
cache access patterns of every hardware instruction is independent of the
secret inputs. However, this solution is architecture-specific, brittle, and
difficult to get right. In this paper, we propose and evaluate a robust
low-overhead technique for mitigating timing and cache channels. Our solution
requires only minimal source code changes and works across multiple
languages/platforms. We report the experimental results of applying our
solution to protect several C, C++, and Java programs. Our results demonstrate
that our solution successfully eliminates the timing and cache side-channel
leaks while incurring significantly lower performance overhead than existing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00190</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00190</id><created>2015-05-30</created><authors><author><keyname>Hung</keyname><forenames>Ruo-Wei</forenames></author></authors><title>Hamiltonian Cycles in Linear-Convex Supergrid Graphs</title><categories>cs.DM math.CO</categories><comments>17 pages, 24 figurs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A supergrid graph is a finite induced subgraph of the infinite graph
associated with the two-dimensional supergrid. The supergrid graphs contain
grid graphs and triangular grid graphs as subgraphs. The Hamiltonian cycle
problem for grid and triangular grid graphs was known to be NP-complete. In the
past, we have shown that the Hamiltonian cycle problem for supergrid graphs is
also NP-complete. The Hamiltonian cycle problem on supergrid graphs can be
applied to control the stitching trace of computerized sewing machines. In this
paper, we will study the Hamiltonian cycle property of linear-convex supergrid
graphs which form a subclass of supergrid graphs. A connected graph is called
$k$-connected if there are $k$ vertex-disjoint paths between every pair of
vertices, and is called locally connected if the neighbors of each vertex in it
form a connected subgraph. In this paper, we first show that any 2-connected,
linear-convex supergrid graph is locally connected. We then prove that any
2-connected, linear-convex supergrid graph contains a Hamiltonian cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00193</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00193</id><created>2015-05-31</created><updated>2015-06-14</updated><authors><author><keyname>Satpathy</keyname><forenames>Sanket</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>Gaussian Secure Source Coding and Wyner's Common Information</title><categories>cs.IT math.IT</categories><comments>ISIT 2015, 5 pages, uses IEEEtran.cls</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study secure source-coding with causal disclosure, under the Gaussian
distribution. The optimality of Gaussian auxiliary random variables is shown in
various scenarios. We explicitly characterize the tradeoff between the rates of
communication and secret key. This tradeoff is the result of a mutual
information optimization under Markov constraints. As a corollary, we deduce a
general formula for Wyner's Common Information in the Gaussian setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00194</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00194</id><created>2015-05-31</created><authors><author><keyname>Satpathy</keyname><forenames>Sanket</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>Secure Cascade Channel Synthesis</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of generating correlated random variables in a
distributed fashion, where communication is constrained to a cascade network.
The first node in the cascade observes an i.i.d. sequence X^n locally before
initiating communication along the cascade. All nodes share bits of common
randomness that are independent of X^n. We consider secure synthesis - random
variables produced by the system appear to be appropriately correlated and
i.i.d. even to an eavesdropper who is cognizant of the communication
transmissions. We characterize the optimal tradeoff between the amount of
common randomness used and the required rates of communication. We find that
not only does common randomness help, its usage exceeds the communication rate
requirements. The most efficient scheme is based on a superposition codebook,
with the first node selecting messages for all downstream nodes. We also
provide a fleeting view of related problems, demonstrating how the optimal rate
region may shrink or expand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00195</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00195</id><created>2015-05-31</created><authors><author><keyname>Peng</keyname><forenames>Baolin</forenames></author><author><keyname>Yao</keyname><forenames>Kaisheng</forenames></author></authors><title>Recurrent Neural Networks with External Memory for Language
  Understanding</title><categories>cs.CL cs.AI cs.LG cs.NE</categories><comments>submitted to Interspeech 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNNs) have become increasingly popular for the
task of language understanding. In this task, a semantic tagger is deployed to
associate a semantic label to each word in an input sequence. The success of
RNN may be attributed to its ability to memorize long-term dependence that
relates the current-time semantic label prediction to the observations many
time instances away. However, the memory capacity of simple RNNs is limited
because of the gradient vanishing and exploding problem. We propose to use an
external memory to improve memorization capability of RNNs. We conducted
experiments on the ATIS dataset, and observed that the proposed model was able
to achieve the state-of-the-art results. We compare our proposed model with
alternative models and report analysis results that may provide insights for
future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00196</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00196</id><created>2015-05-31</created><updated>2015-08-20</updated><authors><author><keyname>Yao</keyname><forenames>Kaisheng</forenames></author><author><keyname>Zweig</keyname><forenames>Geoffrey</forenames></author></authors><title>Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme
  Conversion</title><categories>cs.CL</categories><comments>Published in INTERSPEECH 2015, Dresden, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence-to-sequence translation methods based on generation with a
side-conditioned language model have recently shown promising results in
several tasks. In machine translation, models conditioned on source side words
have been used to produce target-language text, and in image captioning, models
conditioned images have been used to generate caption text. Past work with this
approach has focused on large vocabulary tasks, and measured quality in terms
of BLEU. In this paper, we explore the applicability of such models to the
qualitatively different grapheme-to-phoneme task. Here, the input and output
side vocabularies are small, plain n-gram models do well, and credit is only
given when the output is exactly correct. We find that the simple
side-conditioned generation approach is able to rival the state-of-the-art, and
we are able to significantly advance the stat-of-the-art with bi-directional
long short-term memory (LSTM) neural networks that use the same alignment
information that is used in conventional approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00198</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00198</id><created>2015-05-31</created><updated>2015-06-01</updated><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author><author><keyname>Lee</keyname><forenames>Sanghyuk</forenames></author><author><keyname>Ting</keyname><forenames>Tiew On</forenames></author><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author></authors><title>Atomic Scheduling of Appliance Energy Consumption in Residential Smart
  Grid</title><categories>cs.NI cs.SY</categories><comments>8 pages; 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current formulation of the optimal scheduling of appliance energy
consumption uses as optimization variables the vectors of appliances' scheduled
energy consumption over equally-divided time slots of a day, which does not
take into account the atomicity of appliances' operations (i.e., the
unsplittable nature of appliances' operations and resulting energy
consumption). In this paper, we provide a new formulation of atomic scheduling
of energy consumption based on the optimal routing framework; the flow
configurations of users over multiple paths between the common source and
destination nodes of a ring network are used as optimization variables, which
indicate the starting times of scheduled energy consumption, and optimal
scheduling problems are now formulated in terms of the user flow
configurations. Because the atomic optimal scheduling results in a
Boolean-convex problem for a convex objective function, we propose a successive
convex relaxation technique for efficient calculation of an approximate
solution, where we iteratively drop fractional-valued elements and apply convex
relaxation to the resulting problem until we find a feasible suboptimal
solution. Numerical results for the cost and peak-to-average ratio minimization
problems demonstrate that the successive convex relaxation technique can
provide solutions close to, often identical to, global optimal solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00202</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00202</id><created>2015-05-31</created><authors><author><keyname>Malyshev</keyname><forenames>D. S.</forenames></author></authors><title>A complexity dichotomy for the dominating set problem</title><categories>cs.DM</categories><msc-class>05C69, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We completely determine the complexity status of the dominating set problem
for hereditary graph classes defined by forbidden induced subgraphs with at
most five vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00204</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00204</id><created>2015-05-31</created><authors><author><keyname>Wang</keyname><forenames>Zhuang</forenames></author><author><keyname>Lv</keyname><forenames>Xiao</forenames></author><author><keyname>Yan</keyname><forenames>Mingyu</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author></authors><title>Fair Packet Scheduling in Network on Chip</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interconnection networks of parallel systems are used for servicing traf- fic
generated by different applications, often belonging to different users. When
multiple traffic flows contend for channel bandwidth, the scheduling algorithm
regulating the access to that channel plays a key role in ensur- ing that each
flow obtains the required quality of service. Fairness is a highly desirable
property for a scheduling algorithm. We show that using the Relative Fairness
Bound as a fairness measure may lead to decrease in throughput and increase in
latency. We propose an alternative metric to evaluate the fairness and avoid
the drawback of Relative Fairness Bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00213</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00213</id><created>2015-05-31</created><authors><author><keyname>Tandon</keyname><forenames>Anshoo</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author></authors><title>Subblock-Constrained Codes for Real-Time Simultaneous Energy and
  Information Transfer</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an energy-harvesting receiver that uses the same received signal
both for decoding information and for harvesting energy, which is employed to
power its circuitry. In the scenario where the receiver has limited battery
size, a signal with bursty energy content may cause power outage at the
receiver since the battery will drain during intervals with low signal energy.
In this paper, we consider a discrete memoryless channel and characterize
achievable information rates when the energy content in each codeword is
regularized by ensuring that sufficient energy is carried within every subblock
duration. In particular, we study constant subblock-composition codes (CSCCs)
where all subblocks in every codeword have the same fixed composition, and this
subblock-composition is chosen to maximize the rate of information transfer
while meeting the energy requirement. Compared to constant composition codes
(CCCs), we show that CSCCs incur a rate loss and that the error exponent for
CSCCs is also related to the error exponent for CCCs by the same rate loss
term. We show that CSCC capacity can be improved by allowing different
subblocks to have different composition while still meeting the subblock energy
constraint. We provide numerical examples highlighting the tradeoff between
delivery of sufficient energy to the receiver and achieving high information
transfer rates. It is observed that the ability to use energy in real-time
imposes less of penalty than the ability to use information in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00227</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00227</id><created>2015-05-31</created><authors><author><keyname>Cui</keyname><forenames>Yajun</forenames></author><author><keyname>Zhao</keyname><forenames>Yang</forenames></author><author><keyname>Xiao</keyname><forenames>Kafei</forenames></author><author><keyname>Zhang</keyname><forenames>Chenglong</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author></authors><title>Parallel Spectral Clustering Algorithm Based on Hadoop</title><categories>cs.DC cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering and cloud computing is emerging branch of computer
science or related discipline. It overcome the shortcomings of some traditional
clustering algorithm and guarantee the convergence to the optimal solution,
thus have to the widespread attention. This article first introduced the
parallel spectral clustering algorithm research background and significance,
and then to Hadoop the cloud computing Framework has carried on the detailed
introduction, then has carried on the related to spectral clustering is
introduced, then introduces the spectral clustering arithmetic Method of
parallel and relevant steps, finally made the related experiments, and the
experiment are summarized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00231</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00231</id><created>2015-05-31</created><authors><author><keyname>Liu</keyname><forenames>Wei</forenames></author></authors><title>Channel Equalization and Beamforming for Quaternion-Valued Wireless
  Communication Systems</title><categories>cs.IT math.IT math.NA</categories><comments>11 pages, 10 figures, accepted by the Journal of The Franklin
  Institute subject to minor revisions in March 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quaternion-valued wireless communication systems have been studied in the
past. Although progress has been made in this promising area, a crucial missing
link is lack of effective and efficient quaternion-valued signal processing
algorithms for channel equalization and beamforming. With most recent
developments in quaternion-valued signal processing, in this work, we fill the
gap to solve the problem by studying two quaternion-valued adaptive algorithms:
one is the reference signal based quaternion-valued least mean square (QLMS)
algorithm and the other one is the quaternion-valued constant modulus algorithm
(QCMA). The quaternion-valued Wiener solution for possible block-based
calculation is also derived. Simulation results are provided to show the
working of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00238</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00238</id><created>2015-05-31</created><authors><author><keyname>Kailkhura</keyname><forenames>Bhavya</forenames></author><author><keyname>Liu</keyname><forenames>Sijia</forenames></author><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Measurement Matrix Design for Compressive Detection with Secrecy
  Guarantees</title><categories>cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we consider the problem of detecting a high dimensional
signal based on compressed measurements with physical layer secrecy guarantees.
We assume that the network operates in the presence of an eavesdropper who
intends to discover the state of the nature being monitored by the system. We
design measurement matrices which maximize the detection performance of the
network while guaranteeing a certain level of secrecy. We solve the measurement
matrix design problem under three different scenarios: $a)$ signal is known,
$b)$ signal lies in a low dimensional subspace, and $c)$ signal is sparse. It
is shown that the security performance of the system can be improved by using
optimized measurement matrices along with artificial noise injection based
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00242</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00242</id><created>2015-05-31</created><authors><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author><author><keyname>Yaroslavtsev</keyname><forenames>Grigory</forenames></author></authors><title>Privacy for the Protected (Only)</title><categories>cs.DS cs.CR cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by tensions between data privacy for individual citizens, and
societal priorities such as counterterrorism and the containment of infectious
disease, we introduce a computational model that distinguishes between parties
for whom privacy is explicitly protected, and those for whom it is not (the
targeted subpopulation). The goal is the development of algorithms that can
effectively identify and take action upon members of the targeted subpopulation
in a way that minimally compromises the privacy of the protected, while
simultaneously limiting the expense of distinguishing members of the two groups
via costly mechanisms such as surveillance, background checks, or medical
testing. Within this framework, we provide provably privacy-preserving
algorithms for targeted search in social networks. These algorithms are natural
variants of common graph search methods, and ensure privacy for the protected
by the careful injection of noise in the prioritization of potential targets.
We validate the utility of our algorithms with extensive computational
experiments on two large-scale social network datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00243</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00243</id><created>2015-05-31</created><updated>2015-06-05</updated><authors><author><keyname>Wang</keyname><forenames>Hui</forenames></author><author><keyname>Ho</keyname><forenames>Anthony TS</forenames></author><author><keyname>Li</keyname><forenames>Shujun</forenames></author></authors><title>OR-Benchmark: An Open and Reconfigurable Digital Watermarking
  Benchmarking Framework</title><categories>cs.MM cs.CR cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Benchmarking digital watermarking algorithms is not an easy task because
different applications of digital watermarking often have very different sets
of requirements and trade-offs between conflicting requirements. While there
have been some general-purpose digital watermarking benchmarking systems
available, they normally do not support complicated benchmarking tasks and
cannot be easily reconfigured to work with different watermarking algorithms
and testing conditions. In this paper, we propose OR-Benchmark, an open and
highly reconfigurable general-purpose digital watermarking benchmarking
framework, which has the following two key features: 1) all the interfaces are
public and general enough to support all watermarking applications and
benchmarking tasks we can think of; 2) end users can easily extend the
functionalities and freely configure what watermarking algorithms are tested,
what system components are used, how the benchmarking process runs, and what
results should be produced. We implemented a prototype of this framework as a
MATLAB software package and used it to benchmark a number of digital
watermarking algorithms involving two types of watermarks for content
authentication and self-restoration purposes. The benchmarking results
demonstrated the advantages of the proposed benchmarking framework, and also
gave us some useful insights about existing image authentication and
self-restoration watermarking algorithms which are an important but less
studied topic in digital watermarking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00246</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00246</id><created>2015-05-31</created><authors><author><keyname>Beykikhoshk</keyname><forenames>Adham</forenames></author><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author><author><keyname>Caelli</keyname><forenames>Terry</forenames></author></authors><title>Using Twitter to learn about the autism community</title><categories>cs.SI cs.CY</categories><comments>Social Network Analysis and Mining, 2015</comments><doi>10.1007/s13278-015-0261-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering the raising socio-economic burden of autism spectrum disorder
(ASD), timely and evidence-driven public policy decision making and
communication of the latest guidelines pertaining to the treatment and
management of the disorder is crucial. Yet evidence suggests that policy makers
and medical practitioners do not always have a good understanding of the
practices and relevant beliefs of ASD-afflicted individuals' carers who often
follow questionable recommendations and adopt advice poorly supported by
scientific data. The key goal of the present work is to explore the idea that
Twitter, as a highly popular platform for information exchange, could be used
as a data-mining source to learn about the population affected by ASD -- their
behaviour, concerns, needs etc. To this end, using a large data set of over 11
million harvested tweets as the basis for our investigation, we describe a
series of experiments which examine a range of linguistic and semantic aspects
of messages posted by individuals interested in ASD. Our findings, the first of
their nature in the published scientific literature, strongly motivate
additional research on this topic and present a methodological basis for
further work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00249</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00249</id><created>2015-05-31</created><authors><author><keyname>Jarden</keyname><forenames>Adi</forenames></author><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author><author><keyname>Mandrescu</keyname><forenames>Eugen</forenames></author></authors><title>Monotonic Properties of Collections of Maximum Independent Sets of a
  Graph</title><categories>cs.DM math.CO</categories><comments>15 pages, 7 figures</comments><msc-class>05C69, 05C70 (Primary) 05A20(Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a simple graph with vertex set V(G). A subset S of V(G) is
independent if no two vertices from S are adjacent. The graph G is known to be
a Konig-Egervary if alpha(G) + mu(G)= |V(G)|, where alpha(G) denotes the size
of a maximum independent set and mu(G) is the cardinality of a maximum
matching. Let Omega(G) denote the family of all maximum independent sets, and f
be the function from the set of subcollections Gamma of Omega(G) such that
f(Gamma) = (the cardinality of the union of elements of Gamma) + (the
cardinality of the intersection of elements of Gamma). Our main finding claims
that f is &quot;&lt;&lt;&quot;-increasing, where the preorder {Gamma1} &lt;&lt; {Gamma2} means that
the union of all elements of {Gamma1} is a subset of the union of all elements
of {Gamma2}, while the intersection of all elements of {Gamma2} is a subset of
the intersection of all elements of {Gamma1}. Let us say that a family {Gamma}
is a Konig-Egervary collection if f(Gamma) = 2*alpha(G). We conclude with the
observation that for every graph G each subcollection of a Konig-Egervary
collection is Konig-Egervary as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00251</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00251</id><created>2015-05-31</created><updated>2015-10-30</updated><authors><author><keyname>Ruan</keyname><forenames>Zhongyuan</forenames></author><author><keyname>Iniguez</keyname><forenames>Gerardo</forenames></author><author><keyname>Karsai</keyname><forenames>Marton</forenames></author><author><keyname>Kertesz</keyname><forenames>Janos</forenames></author></authors><title>Kinetics of Social Contagion</title><categories>physics.soc-ph cs.SI</categories><comments>Accepted by PRL; 8 pages, 4 figures + 8 pages Supplemental Material</comments><journal-ref>Phys. Rev. Lett. 115, 218702 (2015)</journal-ref><doi>10.1103/PhysRevLett.115.218702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion of information, behavioral patterns or innovations follows diverse
pathways depending on a number of conditions, including the structure of the
underlying social network, the sensitivity to peer pressure and the influence
of media. Here we study analytically and by simulations a general model that
incorporates threshold mechanism capturing sensitivity to peer pressure, the
effect of `immune' nodes who never adopt, and a perpetual flow of external
information. While any constant, non-zero rate of dynamically-introduced
spontaneous adopters leads to global spreading, the kinetics by which the
asymptotic state is approached shows rich behavior. In particular we find that,
as a function of the immune node density, there is a transition from fast to
slow spreading governed by entirely different mechanisms. This transition
happens below the percolation threshold of network fragmentation, and has its
origin in the competition between cascading behavior induced by adopters and
blocking due to immune nodes. This change is accompanied by a percolation
transition of the induced clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00253</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00253</id><created>2015-05-31</created><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author></authors><title>Minimum MS. E. Gerber's Lemma</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mrs. Gerber's Lemma lower bounds the entropy at the output of a binary
symmetric channel in terms of the entropy of the input process. In this paper,
we lower bound the output entropy via a different measure of input uncertainty,
pertaining to the minimum mean squared error (MMSE) prediction cost of the
input process. We show that in many cases our bound is tighter than the one
obtained from Mrs. Gerber's Lemma. As an application, we evaluate the bound for
binary hidden Markov processes, and obtain new estimates for the entropy rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00255</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00255</id><created>2015-05-31</created><authors><author><keyname>Jarden</keyname><forenames>Adi</forenames></author><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author><author><keyname>Mandrescu</keyname><forenames>Eugen</forenames></author></authors><title>Critical and Maximum Independent Sets of a Graph</title><categories>cs.DM math.CO</categories><comments>12 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:1407.7368</comments><msc-class>05C69 (Primary) 05C70 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a simple graph with vertex set V(G). A subset S of V(G) is
independent if no two vertices from S are adjacent. By Ind(G) we mean the
family of all independent sets of G while core(G) and corona(G) denote the
intersection and the union of all maximum independent sets, respectively. The
number d(X)= |X|-|N(X)| is the difference of the set of vertices X, and an
independent set A is critical if d(A)=max{d(I):I belongs to Ind(G)} (Zhang,
1990). Let ker(G) and diadem(G) be the intersection and union, respectively, of
all critical independent sets of G (Levit and Mandrescu, 2012). In this paper,
we present various connections between critical unions and intersections of
maximum independent sets of a graph. These relations give birth to new
characterizations of Koenig-Egervary graphs, some of them involving ker(G),
core(G), corona(G), and diadem(G).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00272</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00272</id><created>2015-05-31</created><updated>2016-02-15</updated><authors><author><keyname>Merzky</keyname><forenames>Andre</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author></authors><title>Synapse: Synthetic Application Profiler and Emulator</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Synapse motivated by the needs to estimate and emulate workload
execution characteristics on high-performance and distributed heterogeneous
resources. Synapse has a platform independent application profiler, and the
ability to emulate profiled workloads on a variety of heterogeneous resources.
Synapse is used as a proxy application (or &quot;representative application&quot;) for
real workloads, with the added advantage that it can be tuned at arbitrary
levels of granularity in ways that are simply not possible using real
applications. Experiments show that automated profiling using Synapse
represents application characteristics with high fidelity. Emulation using
Synapse can reproduce the application behavior in the original runtime
environment, as well as reproducing properties when used in a different
run-time environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00273</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00273</id><created>2015-05-31</created><authors><author><keyname>Ghazi</keyname><forenames>Badih</forenames></author><author><keyname>Kamath</keyname><forenames>Pritish</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Communication Complexity of Permutation-Invariant Functions</title><categories>cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the quest for a broader understanding of communication
complexity of simple functions, we introduce the class of
&quot;permutation-invariant&quot; functions. A partial function $f:\{0,1\}^n \times
\{0,1\}^n\to \{0,1,?\}$ is permutation-invariant if for every bijection
$\pi:\{1,\ldots,n\} \to \{1,\ldots,n\}$ and every $\mathbf{x}, \mathbf{y} \in
\{0,1\}^n$, it is the case that $f(\mathbf{x}, \mathbf{y}) =
f(\mathbf{x}^{\pi}, \mathbf{y}^{\pi})$. Most of the commonly studied functions
in communication complexity are permutation-invariant. For such functions, we
present a simple complexity measure (computable in time polynomial in $n$ given
an implicit description of $f$) that describes their communication complexity
up to polynomial factors and up to an additive error that is logarithmic in the
input size. This gives a coarse taxonomy of the communication complexity of
simple functions. Our work highlights the role of the well-known lower bounds
of functions such as 'Set-Disjointness' and 'Indexing', while complementing
them with the relatively lesser-known upper bounds for 'Gap-Inner-Product'
(from the sketching literature) and 'Sparse-Gap-Inner-Product' (from the recent
work of Canonne et al. [ITCS 2015]). We also present consequences to the study
of communication complexity with imperfectly shared randomness where we show
that for total permutation-invariant functions, imperfectly shared randomness
results in only a polynomial blow-up in communication complexity after an
additive $O(\log \log n)$ overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00275</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00275</id><created>2015-05-31</created><updated>2015-08-15</updated><authors><author><keyname>Narayan</keyname><forenames>Shashi</forenames></author><author><keyname>Cohen</keyname><forenames>Shay B.</forenames></author></authors><title>Diversity in Spectral Learning for Natural Language Parsing</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an approach to create a diverse set of predictions with spectral
learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating
multiple spectral models where noise is added to the underlying features in the
training set before the estimation of each model. We describe three ways to
decode with multiple models. In addition, we describe a simple variant of the
spectral algorithm for L-PCFGs that is fast and leads to compact models. Our
experiments for natural language parsing, for English and German, show that we
get a significant improvement over baselines comparable to state of the art.
For English, we achieve the $F_1$ score of 90.18, and for German we achieve the
$F_1$ score of 83.38.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00277</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00277</id><created>2015-05-31</created><authors><author><keyname>Andrecut</keyname><forenames>M.</forenames></author></authors><title>A Matrix Public Key Cryptosystem</title><categories>cs.CR</categories><comments>18 pages, C code included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a matrix public key cryptosystem and its numerical implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00278</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00278</id><created>2015-05-31</created><authors><author><keyname>Yu</keyname><forenames>Licheng</forenames></author><author><keyname>Park</keyname><forenames>Eunbyung</forenames></author><author><keyname>Berg</keyname><forenames>Alexander C.</forenames></author><author><keyname>Berg</keyname><forenames>Tamara L.</forenames></author></authors><title>Visual Madlibs: Fill in the blank Image Generation and Question
  Answering</title><categories>cs.CV cs.CL</categories><comments>10 pages; 8 figures; 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new dataset consisting of 360,001 focused
natural language descriptions for 10,738 images. This dataset, the Visual
Madlibs dataset, is collected using automatically produced fill-in-the-blank
templates designed to gather targeted descriptions about: people and objects,
their appearances, activities, and interactions, as well as inferences about
the general scene or its broader context. We provide several analyses of the
Visual Madlibs dataset and demonstrate its applicability to two new description
generation tasks: focused description generation, and multiple-choice
question-answering for images. Experiments using joint-embedding and deep
learning methods show promising results on these tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00286</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00286</id><created>2015-05-31</created><authors><author><keyname>Rahman</keyname><forenames>Mostafizur</forenames></author><author><keyname>Shi</keyname><forenames>Jiajun</forenames></author><author><keyname>Li</keyname><forenames>Mingyu</forenames></author><author><keyname>Khasanvis</keyname><forenames>Santosh</forenames></author><author><keyname>Moritz</keyname><forenames>Csaba Andras</forenames></author></authors><title>Manufacturing Pathway and Experimental Demonstration for Nanoscale
  Fine-Grained 3-D Integrated Circuit Fabric</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At sub-20nm technologies CMOS scaling faces severe challenges primarily due
to fundamental device scaling limitations, interconnection overhead and complex
manufacturing. Migration to 3D has been long sought as a possible pathway to
continue scaling, however, intrinsic requirements of CMOS are not compatible
for fine-grained 3D integration. We proposed a truly fine-grained 3D integrated
circuit fabric called Skybridge that solves nanoscale challenges and achieves
orders of magnitude benefits over CMOS. In Skybridge, device, circuit,
connectivity, thermal management and manufacturing issues are addressed in an
integrated 3D compatible manner. At the core of Skybridge assembly are uniform
vertical nanowires, which are functionalized with architected features for
fabric integration. All active components are created primarily using
sequential material deposition steps on these nanowires. Lithography and doping
are performed prior to any functionalization and their precision requirements
are significantly reduced. This paper introduces Skybridge manufacturing
pathway that is developed based on extensive process, device simulations and
experimental metrology, and uses established processes. Experimental
demonstrations of key process steps are also shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00289</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00289</id><created>2015-05-31</created><authors><author><keyname>Ladeia</keyname><forenames>Cibele A.</forenames></author><author><keyname>Romeiro</keyname><forenames>Neyva M. L.</forenames></author><author><keyname>Natti</keyname><forenames>Paulo L.</forenames></author><author><keyname>Cirilo</keyname><forenames>Eliandro R.</forenames></author></authors><title>Semi-Discrete Formulations for 1D Burgers Equation</title><categories>math.NA cs.NA</categories><comments>14 pages, 2 figures, in Portuguese</comments><journal-ref>TEMA - Tend\^encias em Matem\'atica Aplicada e Computacional 14,
  N.3, 319-331, 2013</journal-ref><doi>10.5540/tema.2013.014.03.0319</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we compare semi-discrete formulations to obtain numerical
solutions for the 1D Burgers equation. The formulations consist in the
discretization of the time-domain via multi-stage methods of second and fourth
order: R_{11} and R_{22} Pad\'e approximants, and of the spatial-domain via
finite element methods: least-squares (MEFMQ), Galerkin (MEFG) and
Streamline-Upwind Petrov-Galerkin (SUPG). Knowing the analytical solutions of
the 1D Burgues equation, for different initial and boundary conditions,
analyzes were performed for numerical errors from L_{2} and L_{\infinity} norm.
We found that the R_{22} Pad\'e approximants, added to the MEFMQ, MEFG, and
SUPG formulations, increased the region of convergence of the numerical
solutions, and showed greater accuracy when compared to the solutions obtained
by the R_{11} Pad\'e approximants. We note that the R_{22} Pad\'e approximants
softened the oscillations of the numerical solutions associated to the MEFG and
SUPG formulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00290</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00290</id><created>2015-05-31</created><updated>2015-08-04</updated><authors><author><keyname>Kalai</keyname><forenames>Yael Tauman</forenames></author><author><keyname>Komargodski</keyname><forenames>Ilan</forenames></author></authors><title>Compressing Communication in Distributed Protocols</title><categories>cs.DC</categories><comments>23 pages + 1 title page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to compress communication in distributed protocols in which
parties do not have private inputs. More specifically, we present a generic
method for converting any protocol in which parties do not have private inputs,
into another protocol where each message is {\em &quot;short&quot;} while preserving the
same number of rounds, the same communication pattern, the same output
distribution, and the same resilience to error. Assuming that the output lies
in some universe of size $M$, in our resulting protocol each message consists
of only $\mathsf{poly}{\log}(M,n,d)$ many bits, where $n$ is the number of
parties and $d$ is the number of rounds. Our transformation works in the full
information model, in the presence of either static or adaptive Byzantine
faults.
  In particular, our result implies that for any such $\mathsf{poly}(n)$-round
distributed protocol which generates outputs in a universe of size
$\mathsf{poly}(n)$, long messages are not needed, and messages of length
$\mathsf{poly}{\log}(n)$ suffice. In other words, in this regime, any
distributed task that can be solved in the $\mathcal{LOCAL}$ model, can also be
solved in the $\mathcal{CONGEST}$ model with the \emph{same} round complexity
and security guarantees.
  As a corollary, we conclude that for any $\mathsf{poly}(n)$-round collective
coin-flipping protocol, leader election protocol, or selection protocols,
messages of length $\mathsf{poly}{\log}(n)$ suffice (in the presence of either
static or adaptive Byzantine faults).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00300</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00300</id><created>2015-05-31</created><authors><author><keyname>Lopez</keyname><forenames>Jose A.</forenames></author></authors><title>How To Tame Your Sparsity Constraints</title><categories>math.OC cs.SY</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that designing sparse $H_\infty$ controllers, in a discrete (LTI)
setting, is easy when the controller is assumed to be an FIR filter. In this
case, the problem reduces to a static output feedback problem with equality
constraints. We show how to obtain an initial guess, for the controller, and
then provide a simple algorithm that alternates between two (convex)
feasibility programs until converging, when the problem is feasible, to a
suboptimal $H_\infty$ controller that is automatically stable. As FIR filters
contain the information of their impulse response in their coefficients, it is
easy to see that our results provide a path of least resistance to designing
sparse robust controllers for continuous-time plants, via system identification
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00301</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00301</id><created>2015-05-31</created><authors><author><keyname>Wolfe</keyname><forenames>Travis</forenames></author><author><keyname>Dredze</keyname><forenames>Mark</forenames></author><author><keyname>Mayfield</keyname><forenames>James</forenames></author><author><keyname>McNamee</keyname><forenames>Paul</forenames></author><author><keyname>Harman</keyname><forenames>Craig</forenames></author><author><keyname>Finin</keyname><forenames>Tim</forenames></author><author><keyname>Van Durme</keyname><forenames>Benjamin</forenames></author></authors><title>Interactive Knowledge Base Population</title><categories>cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most work on building knowledge bases has focused on collecting entities and
facts from as large a collection of documents as possible. We argue for and
describe a new paradigm where the focus is on a high-recall extraction over a
small collection of documents under the supervision of a human expert, that we
call Interactive Knowledge Base Population (IKBP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00307</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00307</id><created>2015-05-31</created><authors><author><keyname>Soroush</keyname><forenames>Emad</forenames></author><author><keyname>Balazinska</keyname><forenames>Magdalena</forenames></author><author><keyname>Krughoff</keyname><forenames>Simon</forenames></author><author><keyname>Connolly</keyname><forenames>Andrew</forenames></author></authors><title>Efficient Iterative Processing in the SciDB Parallel Array Engine</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many scientific data-intensive applications perform iterative computations on
array data. There exist multiple engines specialized for array processing.
These engines efficiently support various types of operations, but none
includes native support for iterative processing. In this paper, we develop a
model for iterative array computations and a series of optimizations. We
evaluate the benefits of an optimized, native support for iterative array
processing on the SciDB engine and real workloads from the astronomy domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00312</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00312</id><created>2015-05-31</created><authors><author><keyname>Zoghi</keyname><forenames>Masrour</forenames></author><author><keyname>Karnin</keyname><forenames>Zohar</forenames></author><author><keyname>Whiteson</keyname><forenames>Shimon</forenames></author><author><keyname>de Rijke</keyname><forenames>Maarten</forenames></author></authors><title>Copeland Dueling Bandits</title><categories>cs.LG</categories><comments>33 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A version of the dueling bandit problem is addressed in which a Condorcet
winner may not exist. Two algorithms are proposed that instead seek to minimize
regret with respect to the Copeland winner, which, unlike the Condorcet winner,
is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed
for small numbers of arms, while the second, Scalable Copeland Bandits (SCB),
works better for large-scale problems. We provide theoretical results bounding
the regret accumulated by CCB and SCB, both substantially improving existing
results. Such existing results either offer bounds of the form $O(K \log T)$
but require restrictive assumptions, or offer bounds of the form $O(K^2 \log
T)$ without requiring such assumptions. Our results offer the best of both
worlds: $O(K \log T)$ bounds without restrictive assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00323</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00323</id><created>2015-05-31</created><authors><author><keyname>Podosinnikova</keyname><forenames>Anastasia</forenames></author><author><keyname>Setzer</keyname><forenames>Simon</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author></authors><title>Robust PCA: Optimization of the Robust Reconstruction Error over the
  Stiefel Manifold</title><categories>stat.ML cs.LG</categories><comments>long version of GCPR 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that Principal Component Analysis (PCA) is strongly affected
by outliers and a lot of effort has been put into robustification of PCA. In
this paper we present a new algorithm for robust PCA minimizing the trimmed
reconstruction error. By directly minimizing over the Stiefel manifold, we
avoid deflation as often used by projection pursuit methods. In distinction to
other methods for robust PCA, our method has no free parameter and is
computationally very efficient. We illustrate the performance on various
datasets including an application to background modeling and subtraction. Our
method performs better or similar to current state-of-the-art methods while
being faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00326</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00326</id><created>2015-05-31</created><authors><author><keyname>Lee</keyname><forenames>Uichin</forenames></author><author><keyname>Joy</keyname><forenames>Joshua</forenames></author><author><keyname>Noh</keyname><forenames>Youngtae</forenames></author></authors><title>Secure Personal Content Networking over Untrusted Devices</title><categories>cs.CR cs.NI</categories><doi>10.1007/s11277-014-2093-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Securely sharing and managing personal content is a challenging task in
multi-device environments. In this paper, we design and implement a new
platform called Personal Content Networking (PCN). Our work is inspired by
Content-Centric Networking (CCN) because we aim to enable access to personal
content using its name instead of its location. The unique challenge of PCN is
to support secure file operations such as replication, updates, and access
control over distributed untrusted devices. The primary contribution of this
work is the design and implementation of a secure content management platform
that supports secure updates, replications, and fine-grained content-centric
access control of files. Furthermore, we demonstrate its feasibility through a
prototype implementation on the CCNx skeleton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00327</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00327</id><created>2015-05-31</created><authors><author><keyname>Wang</keyname><forenames>Zhiguang</forenames></author><author><keyname>Oates</keyname><forenames>Tim</forenames></author></authors><title>Imaging Time-Series to Improve Classification and Imputation</title><categories>cs.LG cs.NE stat.ML</categories><comments>Accepted by IJCAI-2015 ML track</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Inspired by recent successes of deep learning in computer vision, we propose
a novel framework for encoding time series as different types of images,
namely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov
Transition Fields (MTF). This enables the use of techniques from computer
vision for time series classification and imputation. We used Tiled
Convolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn
high-level features from the individual and compound GASF-GADF-MTF images. Our
approaches achieve highly competitive results when compared to nine of the
current best time series classification approaches. Inspired by the bijection
property of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on
the GASF images of four standard and one synthesized compound dataset. The
imputation MSE on test data is reduced by 12.18%-48.02% when compared to using
the raw data. An analysis of the features and weights learned via tiled CNNs
and DAs explains why the approaches work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00330</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00330</id><created>2015-05-31</created><authors><author><keyname>Jia</keyname><forenames>Shuqiao</forenames></author><author><keyname>Aazhang</keyname><forenames>Behnaam</forenames></author></authors><title>Signaling Design of Two-Way MIMO Full-Duplex Channel: Optimality Under
  Imperfect Transmit Front-End Chain</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE transactions on wireless communications</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We derive the optimal signaling for a multiple input multiple output (MIMO)
full-duplex two-way channel under the imperfect transmit front-end chain. We
characterize the two-way rates of the channel by using a game-theoretical
approach, where we focus on the Pareto boundary of the achievable rate region
and Nash equilibia (NE). For a MISO full-duplex two-way channel, we prove that
beamforming is an optimal transmission strategy which can achieve any point on
the Pareto boundary. Furthermore, we present a closed-form expression for the
optimal beamforming weights. In our numerical examples we quantify gains in the
achievable rates of the proposed beamforming over the zero-forcing beamforming.
For a general MIMO full-duplex channel, we establish the existence of NE and
present a condition for the uniqueness of NE. We then propose an iterative
water-filling algorithm which is capable of reaching NE. Through simulations
the threshold of the self-interference level is found, below which the
full-duplex NE outperforms the half-duplex TDMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00333</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00333</id><created>2015-05-31</created><updated>2015-11-13</updated><authors><author><keyname>Ma</keyname><forenames>Lin</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author></authors><title>Learning to Answer Questions From Image Using Convolutional Neural
  Network</title><categories>cs.CL cs.CV cs.LG cs.NE</categories><comments>7 pages, 4 figures. Accepted by AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to employ the convolutional neural network (CNN)
for the image question answering (QA). Our proposed CNN provides an end-to-end
framework with convolutional architectures for learning not only the image and
question representations, but also their inter-modal interactions to produce
the answer. More specifically, our model consists of three CNNs: one image CNN
to encode the image content, one sentence CNN to compose the words of the
question, and one multimodal convolution layer to learn their joint
representation for the classification in the space of candidate answer words.
We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA
datasets, which are two benchmark datasets for the image QA, with the
performances significantly outperforming the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00337</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00337</id><created>2015-05-31</created><authors><author><keyname>Long</keyname><forenames>Zhiguo</forenames></author><author><keyname>Li</keyname><forenames>Sanjiang</forenames></author></authors><title>On Distributive Subalgebras of Qualitative Spatial and Temporal Calculi</title><categories>cs.AI</categories><comments>Adding proof of Theorem 2 to appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qualitative calculi play a central role in representing and reasoning about
qualitative spatial and temporal knowledge. This paper studies distributive
subalgebras of qualitative calculi, which are subalgebras in which (weak)
composition distributives over nonempty intersections. It has been proven for
RCC5 and RCC8 that path consistent constraint network over a distributive
subalgebra is always minimal and globally consistent (in the sense of strong
$n$-consistency) in a qualitative sense. The well-known subclass of convex
interval relations provides one such an example of distributive subalgebras.
This paper first gives a characterisation of distributive subalgebras, which
states that the intersection of a set of $n\geq 3$ relations in the subalgebra
is nonempty if and only if the intersection of every two of these relations is
nonempty. We further compute and generate all maximal distributive subalgebras
for Point Algebra, Interval Algebra, RCC5 and RCC8, Cardinal Relation Algebra,
and Rectangle Algebra. Lastly, we establish two nice properties which will play
an important role in efficient reasoning with constraint networks involving a
large number of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00354</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00354</id><created>2015-06-01</created><updated>2015-07-24</updated><authors><author><keyname>Roudi</keyname><forenames>Yasser</forenames></author><author><keyname>Taylor</keyname><forenames>Graham</forenames></author></authors><title>Learning with hidden variables</title><categories>q-bio.NC cond-mat.dis-nn cs.LG cs.NE stat.ML</categories><comments>revised version accepted in Current Opinion in Neurobiology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning and inferring features that generate sensory input is a task
continuously performed by cortex. In recent years, novel algorithms and
learning rules have been proposed that allow neural network models to learn
such features from natural images, written text, audio signals, etc. These
networks usually involve deep architectures with many layers of hidden neurons.
Here we review recent advancements in this area emphasizing, amongst other
things, the processing of dynamical inputs by networks with hidden nodes and
the role of single neuron models. These points and the questions they arise can
provide conceptual advancements in understanding of learning in the cortex and
the relationship between machine learning approaches to learning with hidden
nodes and those in cortical circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00366</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00366</id><created>2015-06-01</created><authors><author><keyname>Raza</keyname><forenames>Khalid</forenames></author></authors><title>Formal Concept Analysis for Knowledge Discovery from Biological Data</title><categories>cs.AI cs.CE q-bio.GN</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to rapid advancement in high-throughput techniques, such as microarrays
and next generation sequencing technologies, biological data are increasing
exponentially. The current challenge in computational biology and
bioinformatics research is how to analyze these huge raw biological data to
extract biologically meaningful knowledge. This review paper presents the
applications of formal concept analysis for the analysis and knowledge
discovery from biological data, including gene expression discretization, gene
co-expression mining, gene expression clustering, finding genes in gene
regulatory networks, enzyme/protein classifications, binding site
classifications, and so on. It also presents a list of FCA-based software tools
applied in biological domain and covers the challenges faced so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00368</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00368</id><created>2015-06-01</created><authors><author><keyname>Van</keyname><forenames>Thanh The</forenames></author><author><keyname>Le</keyname><forenames>Thanh Manh</forenames></author></authors><title>RBIR using Interest Regions and Binary Signatures</title><categories>cs.CV</categories><comments>14 pages, 8 figures</comments><acm-class>H.2.8; H.3.3</acm-class><journal-ref>Annales Univ. Sci. Budapest, Sect. Comp. 43 (2014), pp. 89-103</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an approach to overcome the low accuracy of the
Content-Based Image Retrieval (CBIR) (when using the global features). To
increase the accuracy, we use Harris-Laplace detector to identify the interest
regions of image. Then, we build the Region-Based Image Retrieval (RBIR). For
the efficient image storage and retrieval, we encode images into binary
signatures. The binary signature of a image is created from its interest
regions. Furthermore, this paper also provides an algorithm for image retrieval
on S-tree by comparing the images' signatures on a metric similarly to EMD
(earth mover's distance). Finally, we evaluate the created models on COREL's
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00379</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00379</id><created>2015-06-01</created><updated>2015-08-15</updated><authors><author><keyname>Lin</keyname><forenames>Yankai</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Luan</keyname><forenames>Huanbo</forenames></author><author><keyname>Sun</keyname><forenames>Maosong</forenames></author><author><keyname>Rao</keyname><forenames>Siwei</forenames></author><author><keyname>Liu</keyname><forenames>Song</forenames></author></authors><title>Modeling Relation Paths for Representation Learning of Knowledge Bases</title><categories>cs.CL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation learning of knowledge bases (KBs) aims to embed both entities
and relations into a low-dimensional space. Most existing methods only consider
direct relations in representation learning. We argue that multiple-step
relation paths also contain rich inference patterns between entities, and
propose a path-based representation learning model. This model considers
relation paths as translations between entities for representation learning,
and addresses two key challenges: (1) Since not all relation paths are
reliable, we design a path-constraint resource allocation algorithm to measure
the reliability of relation paths. (2) We represent relation paths via semantic
composition of relation embeddings. Experimental results on real-world datasets
show that, as compared with baselines, our model achieves significant and
consistent improvements on knowledge base completion and relation extraction
from text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00380</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00380</id><created>2015-06-01</created><updated>2015-11-04</updated><authors><author><keyname>Chiribella</keyname><forenames>Giulio</forenames><affiliation>Department of Computer Science, University of Hong Kong, Hong Kong</affiliation></author><author><keyname>Scandolo</keyname><forenames>Carlo Maria</forenames><affiliation>Department of Computer Science, University of Oxford, Oxford, UK</affiliation></author></authors><title>Operational axioms for diagonalizing states</title><categories>quant-ph cs.IT math.IT</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 96-115</journal-ref><doi>10.4204/EPTCS.195.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In quantum theory every state can be diagonalized, i.e. decomposed as a
convex combination of perfectly distinguishable pure states. This elementary
structure plays an ubiquitous role in quantum mechanics, quantum information
theory, and quantum statistical mechanics, where it provides the foundation for
the notions of majorization and entropy. A natural question then arises: can we
reconstruct these notions from purely operational axioms? We address this
question in the framework of general probabilistic theories, presenting a set
of axioms that guarantee that every state can be diagonalized. The first axiom
is Causality, which ensures that the marginal of a bipartite state is well
defined. Then, Purity Preservation states that the set of pure transformations
is closed under composition. The third axiom is Purification, which allows to
assign a pure state to the composition of a system with its environment.
Finally, we introduce the axiom of Pure Sharpness, stating that for every
system there exists at least one pure effect occurring with unit probability on
some state. For theories satisfying our four axioms, we show a constructive
algorithm for diagonalizing every given state. The diagonalization result
allows us to formulate a majorization criterion that captures the
convertibility of states in the operational resource theory of purity, where
random reversible transformations are regarded as free operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00391</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00391</id><created>2015-06-01</created><authors><author><keyname>Mohan</keyname><forenames>Nitinder</forenames></author><author><keyname>Singh</keyname><forenames>Pushpendra</forenames></author></authors><title>CCNCheck: Enabling Checkpointed Distributed Applications in Content
  Centric Networks</title><categories>cs.DC cs.NI</categories><comments>2 pages technical talk abstract, CCNxCon-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of checkpointing a distributed application
efficiently in Content Centric Networks so that it can withstand transient
failures. We present CCNCheck, a system which enables a sender optimized way of
checkpointing distributed applications in CCN's and provides an efficient
mechanism for failure recovery in such applications. CCNCheck's checkpointing
mechanism is a fork of DMTCP repository CCNCheck is capable of running any
distributed application written in C/C++ language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00393</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00393</id><created>2015-06-01</created><authors><author><keyname>Laurent</keyname><forenames>Guillaume</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author><author><keyname>Karsai</keyname><forenames>M&#xe1;rton</forenames></author></authors><title>From calls to communities: a model for time varying social networks</title><categories>physics.soc-ph cs.CY cs.SI physics.comp-ph physics.data-an</categories><comments>10 pages, 5 figures</comments><doi>10.1140/epjb/e2015-60481-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social interactions vary in time and appear to be driven by intrinsic
mechanisms, which in turn shape the emerging structure of the social network.
Large-scale empirical observations of social interaction structure have become
possible only recently, and modelling their dynamics is an actual challenge.
Here we propose a temporal network model which builds on the framework of
activity-driven time-varying networks with memory. The model also integrates
key mechanisms that drive the formation of social ties - social reinforcement,
focal closure and cyclic closure, which have been shown to give rise to
community structure and the global connectedness of the network. We compare the
proposed model with a real-world time-varying network of mobile phone
communication and show that they share several characteristics from
heterogeneous degrees and weights to rich community structure. Further, the
strong and weak ties that emerge from the model follow similar weight-topology
correlations as real-world social networks, including the role of weak ties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00394</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00394</id><created>2015-06-01</created><updated>2015-07-09</updated><authors><author><keyname>Paradies</keyname><forenames>Marcus</forenames></author><author><keyname>Rudolf</keyname><forenames>Michael</forenames></author><author><keyname>Lehner</keyname><forenames>Wolfgang</forenames></author></authors><title>GraphVista: Interactive Exploration Of Large Graphs</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential to gain business insights from graph-structured data through
graph analytics is increasingly attracting companies from a variety of
industries, ranging from web companies to traditional enterprise businesses. To
analyze a graph, a user often executes isolated graph queries using a dedicated
interface---a procedural graph programming interface or a declarative graph
query language. The results are then returned and displayed using a specific
visualization technique. This follows the classical ad-hoc
Query$\rightarrow$Result interaction paradigm and often requires multiple query
iterations until an interesting aspect in the graph data is identified. This is
caused on the one hand by the schema flexibility of graph data and on the other
hand by the intricacies of declarative graph query languages. To lower the
burden for the user to explore an unknown graph without prior knowledge of a
graph query language, visual graph exploration provides an effective and
intuitive query interface to navigate through the graph interactively.
  We demonstrate GRAPHVISTA, a graph visualization and exploration tool that
can seamlessly combine ad-hoc querying and interactive graph exploration within
the same query session. In our demonstration, conference attendees will see
GRAPHVISTA running against a large real-world graph data set. They will start
by identifying entry points of interest with the help of ad-hoc queries and
will then discover the graph interactively through visual graph exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00395</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00395</id><created>2015-06-01</created><authors><author><keyname>Toldo</keyname><forenames>Roberto</forenames></author><author><keyname>Gherardi</keyname><forenames>Riccardo</forenames></author><author><keyname>Farenzena</keyname><forenames>Michela</forenames></author><author><keyname>Fusiello</keyname><forenames>Andrea</forenames></author></authors><title>Hierarchical structure-and-motion recovery from uncalibrated images</title><categories>cs.CV</categories><comments>Accepted for publication in CVIU</comments><journal-ref>Computer Vision and Image Understanding, Volume 140, November
  2015, Pages 127-143</journal-ref><doi>10.1016/j.cviu.2015.05.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the structure-and-motion problem, that requires to find
camera motion and 3D struc- ture from point matches. A new pipeline, dubbed
Samantha, is presented, that departs from the prevailing sequential paradigm
and embraces instead a hierarchical approach. This method has several
advantages, like a provably lower computational complexity, which is necessary
to achieve true scalability, and better error containment, leading to more
stability and less drift. Moreover, a practical autocalibration procedure
allows to process images without ancillary information. Experiments with real
data assess the accuracy and the computational efficiency of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00398</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00398</id><created>2015-06-01</created><updated>2015-09-10</updated><authors><author><keyname>Chiribella</keyname><forenames>Giulio</forenames></author><author><keyname>D'Ariano</keyname><forenames>Giacomo Mauro</forenames></author><author><keyname>Perinotti</keyname><forenames>Paolo</forenames></author></authors><title>Quantum from principles</title><categories>quant-ph cs.IT cs.LO math-ph math.CT math.IT math.MP</categories><comments>49 pages, no figures, few typos corrected. Summarizes the framework
  and the results of arXiv:0908.1583 and arXiv:1011.6451 Contribution to the
  book &quot;Quantum Theory: Informational Foundations and Foils&quot;, Springer Verlag
  (http://www.springer.com/us/book/9789401773027)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum theory was discovered in an adventurous way, under the urge to solve
puzzles-like the spectrum of the blackbody radiation-that haunted the physics
community at the beginning of the 20th century. It soon became clear, though,
that quantum theory was not just a theory of specific physical systems, but
rather a new language of universal applicability. Can this language be
reconstructed from first principles? Can we arrive at it from logical
reasoning, instead of ad hoc guesswork? A positive answer was provided in Refs.
[1, 2], where we put forward six principles that identify quantum theory
uniquely in a broad class of theories. We first defined a class of &quot;theories of
information&quot;, constructed as extensions of probability theory in which events
can be connected into networks. In this framework, we formulated the six
principles as rules governing the control and the accessibility of information.
Directly from these rules, we reconstructed a number of quantum information
features, and eventually, the whole Hilbert space framework. In short, our
principles characterize quantum theory as the theory of information that allows
for maximal control of randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00406</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00406</id><created>2015-06-01</created><updated>2015-09-03</updated><authors><author><keyname>Aghasadeghi</keyname><forenames>Amir Pouya</forenames></author><author><keyname>Bastan</keyname><forenames>Mohadeseh</forenames></author><author><keyname>Khadivi</keyname><forenames>Shahram</forenames></author></authors><title>Monolingually Derived Phrase Scores for Phrase Based SMT Using Neural
  Networks Vector Representations</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose two new features for estimating phrase-based
machine translation parameters from mainly monolingual data. Our method is
based on two recently introduced neural network vector representation models
for words and sentences. It is the first time that these models have been used
in an end to end phrase-based machine translation system. Scores obtained from
our method can recover more than 80% of BLEU loss caused by removing phrase
table probabilities. We also show that our features combined with the phrase
table probabilities improve the BLEU score by absolute 0.74 points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00412</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00412</id><created>2015-06-01</created><authors><author><keyname>Della Penda</keyname><forenames>Demia</forenames></author><author><keyname>Fu</keyname><forenames>Liqun</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Energy efficient D2D communications in dynamic TDD systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Journal of Selected Areas in Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network-assisted device-to-device communication is a promising technology for
improving the performance of proximity-based services. This paper demonstrates
how the integration of device-to-device communications and dynamic
time-division duplex can improve the energy efficiency of future cellular
networks, leading to a greener system operation and a prolonged battery
lifetime of mobile devices. We jointly optimize the mode selection,
transmission period and power allocation to minimize the energy consumption
(from both a system and a device perspective) while satisfying a certain rate
requirement. The radio resource management problems are formulated as
mixed-integer nonlinear programming problems. Although they are known to be
NP-hard in general, we exploit the problem structure to design efficient
algorithms that optimally solve several problem cases. For the remaining cases,
a heuristic algorithm that computes near-optimal solutions while respecting
practical constraints on execution times and signaling overhead is also
proposed. Simulation results confirm that the combination of device-to-device
and flexible time-division-duplex technologies can significantly enhance
spectrum and energy-efficiency of next generation cellular systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00419</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00419</id><created>2015-06-01</created><updated>2016-01-26</updated><authors><author><keyname>Cheng</keyname><forenames>Shantian</forenames></author></authors><title>Dense Packings from Algebraic Number Fields and Codes</title><categories>math.NT cs.IT math.AC math.IT math.MG</categories><msc-class>52C17, 11R04, 94B65 (Primary), 94B05, 11H31 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new method from number fields and codes to construct dense
packings in the Euclidean spaces. Via the canonical $\mathbb{Q}$-embedding of
arbitrary number field $K$ into $\mathbb{R}^{[K:\mathbb{Q}]}$, both the prime
ideal $\mathfrak{p}$ and its residue field $\kappa$ can be embedded as discrete
subsets in $\mathbb{R}^{[K:\mathbb{Q}]}$. Thus we can concatenate the embedding
image of the Cartesian product of $n$ copies of $\mathfrak{p}$ together with
the image of a length $n$ code over $\kappa$. This concatenation leads to a
packing in Euclidean space $\mathbb{R}^{n[K:\mathbb{Q}]}$. Moreover, we extend
the single concatenation to multiple concatenation to obtain dense packings and
asymptotically good packing families. For instance, with the help of \Magma{},
we construct one $256$-dimension packing denser than the Barnes-Wall lattice
BW$_{256}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00425</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00425</id><created>2015-06-01</created><authors><author><keyname>Jiang</keyname><forenames>Bo</forenames></author><author><keyname>Wu</keyname><forenames>Jiaying</forenames></author><author><keyname>Shi</keyname><forenames>Xiuyu</forenames></author><author><keyname>Huang</keyname><forenames>Ruhuan</forenames></author></authors><title>Hadoop Scheduling Base On Data Locality</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In hadoop, the job scheduling is an independent module, users can design
their own job scheduler based on their actual application requirements, thereby
meet their specific business needs. Currently, hadoop has three schedulers:
FIFO, computing capacity scheduling and fair scheduling policy, all of them are
take task allocation strategy that considerate data locality simply. They
neither support data locality well nor fully apply to all cases of jobs
scheduling. In this paper, we took the concept of resources-prefetch into
consideration, and proposed a job scheduling algorithm based on data locality.
By estimate the remaining time to complete a task, compared with the time to
transfer a resources block, to preselect candidate nodes for task allocation.
Then we preselect a non-local map tasks from the unfinished job queue as
resources-prefetch tasks. Getting information of resources blocks of
preselected map task, select a nearest resources blocks from the candidate node
and transferred to local through network. Thus we would ensure data locality
good enough. Eventually, we design a experiment and proved resources-prefetch
method can guarantee good job data locality and reduce the time to complete the
job to a certain extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00432</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00432</id><created>2015-06-01</created><updated>2015-08-11</updated><authors><author><keyname>Cheng</keyname><forenames>Shantian</forenames></author></authors><title>Improvement on Asymptotic Density of Packing Families Derived from
  Multiplicative Lattices</title><categories>math.NT cs.IT math.AG math.IT math.MG</categories><msc-class>52C17, 11R58, 94B65, 14H05</msc-class><journal-ref>Finite Fields and Their Applications, vol. 36 (2015), 133-150</journal-ref><doi>10.1016/j.ffa.2015.07.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\omega=(-1+\sqrt{-3})/2$. For any lattice $P\subseteq \mathbb{Z}^n$,
$\mathcal{P}=P+\omega P$ is a subgroup of $\mathcal{O}_K^n$, where
$\mathcal{O}_K=\mathbb{Z}[\omega]\subseteq \mathbb{C}$. As $\mathbb{C}$ is
naturally isomorphic to $\mathbb{R}^2$, $\mathcal{P}$ can be regarded as a
lattice in $\mathbb{R}^{2n}$. Let $P$ be a multiplicative lattice (principal
lattice or congruence lattice) introduced by Rosenbloom and Tsfasman. We
concatenate a family of special codes with $t_{\mathfrak{P}}^\ell\cdot(P+\omega
P)$, where $t_{\mathfrak{P}}$ is the generator of a prime ideal $\mathfrak{P}$
of $\mathcal{O}_K$. Applying this concatenation to a family of principal
lattices, we obtain a new family with asymptotic density exponent
$\lambda\geqslant-1.26532182283$, which is better than $-1.87$ given by
Rosenbloom and Tsfasman considering only principal lattice families. For a new
family based on congruence lattices, the result is $\lambda\geqslant
-1.26532181404$, which is better than $-1.39$ by considering only congruence
lattice families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00438</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00438</id><created>2015-06-01</created><updated>2016-01-21</updated><authors><author><keyname>Rajeswaran</keyname><forenames>Aravind</forenames></author><author><keyname>Narasimhan</keyname><forenames>Shankar</forenames></author></authors><title>Network Topology Identification using PCA and its Graph Theoretic
  Interpretations</title><categories>cs.LG cs.DM cs.SY stat.ME</categories><comments>Structure of paper is changed to improve presentation. Methods and
  results are unchanged. A more detailed literature survey has been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the problem of identifying (reconstructing) network topology from
steady state network measurements. Concretely, given only a data matrix
$\mathbf{X}$ where the $X_{ij}$ entry corresponds to flow in edge $i$ in
configuration (steady-state) $j$, we wish to find a network structure for which
flow conservation is obeyed at all the nodes. This models many network problems
involving conserved quantities like water, power, and metabolic networks. We
show that identification is equivalent to learning a model $\mathbf{A_n}$ which
captures the approximate linear relationships between the different variables
comprising $\mathbf{X}$ (i.e. of the form $\mathbf{A_n X \approx 0}$) such that
$\mathbf{A_n}$ is full rank (highest possible) and consistent with a network
node-edge incidence structure. The problem is solved through a sequence of
steps like estimating approximate linear relationships using Principal
Component Analysis, obtaining f-cut-sets from these approximate relationships,
and graph realization from f-cut-sets (or equivalently f-circuits). Each step
and the overall process is polynomial time. The method is illustrated by
identifying topology of a water distribution network. We also study the extent
of identifiability from steady-state data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00449</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00449</id><created>2015-06-01</created><authors><author><keyname>Wang</keyname><forenames>Zhuo</forenames></author><author><keyname>Tian</keyname><forenames>Longlong</forenames></author><author><keyname>Guo</keyname><forenames>Dianjie</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaoming</forenames></author></authors><title>Optimization and analysis of large scale data sorting algorithm based on
  Hadoop</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dealing with massive data sorting, we usually use Hadoop which is a
framework that allows for the distributed processing of large data sets across
clusters of computers using simple programming models. A common approach in
implement of big data sorting is to use shuffle and sort phase in MapReduce
based on Hadoop. However, if we use it directly, the efficiency could be very
low and the load imbalance can be a big problem. In this paper we carry out an
experimental study of an optimization and analysis of large scale data sorting
algorithm based on hadoop. In order to reach optimization, we use more than 2
rounds MapReduce. In the first round, we use a MapReduce to take sample
randomly. Then we use another MapReduce to order the data uniformly, according
to the results of the first round. If the data is also too big, it will turn
back to the first round and keep on. The experiments show that, it is better to
use the optimized algorithm than shuffle of MapReduce to sort large scale data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00462</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00462</id><created>2015-06-01</created><authors><author><keyname>Darmann</keyname><forenames>Andreas</forenames></author><author><keyname>Pferschy</keyname><forenames>Ulrich</forenames></author><author><keyname>Schauer</keyname><forenames>Joachim</forenames></author></authors><title>On the shortest path game: extended version</title><categories>cs.DM cs.GT</categories><comments>Extended version contains the full description of the dynamic
  programming arrays in Section 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address a game theoretic variant of the shortest path
problem, in which two decision makers (players) move together along the edges
of a graph from a given starting vertex to a given destination. The two players
take turns in deciding in each vertex which edge to traverse next. The decider
in each vertex also has to pay the cost of the chosen edge. We want to
determine the path where each player minimizes its costs taking into account
that also the other player acts in a selfish and rational way. Such a solution
is a subgame perfect equilibrium and can be determined by backward induction in
the game tree of the associated finite game in extensive form.
  We show that the decision problem associated with such a path is
PSPACE-complete even for bipartite graphs both for the directed and the
undirected version. The latter result is a surprising deviation from the
complexity status of the closely related game Geography.
  On the other hand, we can give polynomial time algorithms for directed
acyclic graphs and for cactus graphs even in the undirected case. The latter is
based on a decomposition of the graph into components and their resolution by a
number of fairly involved dynamic programming arrays. Finally, we give some
arguments about closing the gap of the complexity status for graphs of bounded
treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00468</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00468</id><created>2015-06-01</created><updated>2015-09-10</updated><authors><author><keyname>Lukasik</keyname><forenames>Michal</forenames></author><author><keyname>Cohn</keyname><forenames>Trevor</forenames></author><author><keyname>Bontcheva</keyname><forenames>Kalina</forenames></author></authors><title>Classifying Tweet Level Judgements of Rumours in Social Media</title><categories>cs.SI cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media is a rich source of rumours and corresponding community
reactions. Rumours reflect different characteristics, some shared and some
individual. We formulate the problem of classifying tweet level judgements of
rumours as a supervised learning task. Both supervised and unsupervised domain
adaptation are considered, in which tweets from a rumour are classified on the
basis of other annotated rumours. We demonstrate how multi-task learning helps
achieve good results on rumours from the 2011 England riots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00473</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00473</id><created>2015-06-01</created><updated>2016-02-15</updated><authors><author><keyname>H&#xe9;as</keyname><forenames>Patrick</forenames></author><author><keyname>Dr&#xe9;meau</keyname><forenames>Ang&#xe9;lique</forenames></author><author><keyname>Herzet</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>An Efficient Algorithm for Video Super-Resolution Based On a Sequential
  Model</title><categories>cs.CV</categories><comments>37 pages, SIAM Journal on Imaging Sciences, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel procedure for video super-resolution, that
is the recovery of a sequence of high-resolution images from its low-resolution
counterpart. Our approach is based on a &quot;sequential&quot; model (i.e., each
high-resolution frame is supposed to be a displaced version of the preceding
one) and considers the use of sparsity-enforcing priors. Both the recovery of
the high-resolution images and the motion fields relating them is tackled. This
leads to a large-dimensional, non-convex and non-smooth problem. We propose an
algorithmic framework to address the latter. Our approach relies on fast
gradient evaluation methods and modern optimization techniques for
non-differentiable/non-convex problems. Unlike some other previous works, we
show that there exists a provably-convergent method with a complexity linear in
the problem dimensions. We assess the proposed optimization method on {several
video benchmarks and emphasize its good performance with respect to the state
of the art.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00479</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00479</id><created>2015-06-01</created><authors><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>Thapper</keyname><forenames>Johan</forenames></author></authors><title>Constraint Satisfaction and Semilinear Expansions of Addition over the
  Rationals and the Reals</title><categories>cs.CC</categories><comments>22 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A semilinear relation is a finite union of finite intersections of open and
closed half-spaces over, for instance, the reals, the rationals, or the
integers. Semilinear relations have been studied in connection with algebraic
geometry, automata theory, and spatiotemporal reasoning. We consider semilinear
relations over the rationals and the reals. Under this assumption, the
computational complexity of the constraint satisfaction problem (CSP) is known
for all finite sets containing R+={(x,y,z) | x+y=z}, &lt;=, and {1}. These
problems correspond to expansions of the linear programming feasibility
problem. We generalise this result and fully determine the complexity for all
finite sets of semilinear relations containing R+. This is accomplished in part
by introducing an algorithm, based on computing affine hulls, which solves a
new class of semilinear CSPs in polynomial time. We further analyse the
complexity of linear optimisation over the solution set and the existence of
integer solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00481</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00481</id><created>2015-06-01</created><authors><author><keyname>Huang</keyname><forenames>Weilin</forenames></author><author><keyname>Yin</keyname><forenames>Hujun</forenames></author></authors><title>Robust Face Recognition with Structural Binary Gradient Patterns</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a computationally efficient yet powerful binary framework
for robust facial representation based on image gradients. It is termed as
structural binary gradient patterns (SBGP). To discover underlying local
structures in the gradient domain, we compute image gradients from multiple
directions and simplify them into a set of binary strings. The SBGP is derived
from certain types of these binary strings that have meaningful local
structures and are capable of resembling fundamental textural information. They
detect micro orientational edges and possess strong orientation and locality
capabilities, thus enabling great discrimination. The SBGP also benefits from
the advantages of the gradient domain and exhibits profound robustness against
illumination variations. The binary strategy realized by pixel correlations in
a small neighborhood substantially simplifies the computational complexity and
achieves extremely efficient processing with only 0.0032s in Matlab for a
typical face image. Furthermore, the discrimination power of the SBGP can be
enhanced on a set of defined orientational image gradient magnitudes, further
enforcing locality and orientation. Results of extensive experiments on various
benchmark databases illustrate significant improvements of the SBGP based
representations over the existing state-of-the-art local descriptors in the
terms of discrimination, robustness and complexity. Codes for the SBGP methods
will be available at
http://www.eee.manchester.ac.uk/research/groups/sisp/software/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00482</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00482</id><created>2015-06-01</created><updated>2015-09-16</updated><authors><author><keyname>Mens</keyname><forenames>Irini-Eleftheria</forenames><affiliation>CNRS-VERIMAG</affiliation></author><author><keyname>Maler</keyname><forenames>Oded</forenames><affiliation>CNRS-VERIMAG</affiliation></author></authors><title>Learning Regular Languages over Large Ordered Alphabets</title><categories>cs.LO cs.AI cs.FL</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:13) 2015</journal-ref><doi>10.2168/LMCS-11(3:13)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is concerned with regular languages defined over large alphabets,
either infinite or just too large to be expressed enumeratively. We define a
generic model where transitions are labeled by elements of a finite partition
of the alphabet. We then extend Angluin's L* algorithm for learning regular
languages from examples for such automata. We have implemented this algorithm
and we demonstrate its behavior where the alphabet is a subset of the natural
or real numbers. We sketch the extension of the algorithm to a class of
languages over partially ordered alphabets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00484</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00484</id><created>2015-06-01</created><authors><author><keyname>Gattami</keyname><forenames>Ather</forenames></author></authors><title>Optimal Communication of States of Dynamical Systems over Gaussian
  Channels with Noisy Feedback: The Scalar Case</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.4350</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of communicating the state of a dynamical system via
a Shannon Gaussian channel. The receiver, which acts as both a decoder and
estimator, observes the noisy measurement of the channel output and makes an
optimal estimate of the state of the dynamical system in the minimum mean
square sense. Noisy feedback from the receiver to the transmitter is present.
The transmitter observes the noise-corrupted feedback message from the receiver
together with a possibly noisy measurement of the state the dynamical system.
These measurements are then used to encode the message to be transmitted over a
noisy Gaussian channel, where a per symbol power constraint is imposed on the
transmitted message. Thus, we get a mixed problem of Shannon's source-channel
coding problem and a sort of Kalman filtering problem. In particular, we
consider two feedback instances, one being feedback of receiver measurements
and the second being the receiver's state estimates. We show that optimal
encoders and decoders are linear filters with a finite memory and we give
explicitly the state space realizations of the optimal filters. For the case
where the transmitter has access to noisy measurements of the state, we derive
a separation principle for the optimal communication scheme. Furthermore, we
investigate the presence of noiseless feedback or no feedback from the receiver
to the transmitter. Necessary and sufficient conditions for the existence of a
stationary solution are also given for the feedback cases considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00485</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00485</id><created>2015-06-01</created><authors><author><keyname>Barker</keyname><forenames>Adam</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Thai</keyname><forenames>Long</forenames></author></authors><title>Cloud Services Brokerage: A Survey and Research Roadmap</title><categories>cs.DC cs.SE</categories><comments>Paper published in the 8th IEEE International Conference on Cloud
  Computing (CLOUD 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Cloud Services Brokerage (CSB) acts as an intermediary between cloud
service providers (e.g., Amazon and Google) and cloud service end users,
providing a number of value adding services. CSBs as a research topic are in
there infancy. The goal of this paper is to provide a concise survey of
existing CSB technologies in a variety of areas and highlight a roadmap, which
details five future opportunities for research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00490</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00490</id><created>2015-06-01</created><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author></authors><title>Cut-Set Bounds for Multimessage Multicast Networks with Independent
  Channels and Zero-Delay Edges</title><categories>cs.IT math.IT</categories><comments>will be presented in part at 2015 IEEE International Symposium of
  Information Theory, Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a communication network consisting of nodes and directed edges
that connect the nodes. Each edge receives a symbol from a node and outputs a
symbol to a node in each time slot. An edge (l,i) terminating at node i is said
to incur zero delay on an edge (i,j) originating from node i if the following
holds in each time slot: Node i receives the symbol output from (l,i) before
encoding the symbol to be transmitted on (i,j). Otherwise, (l,i) is said to
incur a delay on (i,j). In the classical model, every edge incurs a unit delay
on every other edge and the cut-set bound is a well-known outer bound on the
capacity region. In this paper, we investigate the multimessage multicast
network (MMN) consisting of independent channels under an edge-delay model
where an edge may incur zero delay on some other edges. Our result reveals that
the capacity region of the MMN with independent channels and zero-delay edges
lies within the classical cut-set bound despite a violation of the unit-delay
assumption. In addition, we fully characterize under our edge-delay model the
capacity regions of the MMN with independent DMCs and the MMN with independent
AWGNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00506</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00506</id><created>2015-06-01</created><updated>2016-03-02</updated><authors><author><keyname>Ikram</keyname><forenames>Muhammad</forenames></author><author><keyname>Onwuzurike</keyname><forenames>Lucky</forenames></author><author><keyname>Farooqi</keyname><forenames>Shehroze</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Friedman</keyname><forenames>Arik</forenames></author><author><keyname>Jourjon</keyname><forenames>Guillaume</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Shafiq</keyname><forenames>M. Zubair</forenames></author></authors><title>Combating Fraud in Online Social Networks: Characterizing and Detecting
  Facebook Like Farms</title><categories>cs.SI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As businesses increasingly rely on social networking sites to engage with
their customers, it is crucial to understand and counter reputation
manipulation activities, including fraudulently boosting the number of Facebook
page likes using so-called like farms. Thus, social network operators have
started to deploy various fraud detection algorithms such as graph clustering
methods, however, with limited efficacy. In fact, this paper presents a
comprehensive analysis and evaluation of existing graph-based fraud detection
algorithms for detecting like farm accounts. Our results show that more
sophisticated and stealthy farms can successfully evade detection by spreading
likes over longer timespans and by liking many popular pages to mimic normal
users.
  Next, we analyze a wide range of features extracted from users' timeline
posts, which we group into two main classes: lexical and interaction-based. We
find that like farm accounts tend to more often re-share content, use fewer
words and poorer vocabulary, target fewer topics, and generate more (often
duplicate) comments and likes compared to normal users. Using these
timeline-based features, we experiment with machine learning algorithms to
detect like farms accounts, obtaining appreciably high accuracy (as high as 99%
precision and 97% recall).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00508</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00508</id><created>2015-06-01</created><updated>2015-09-01</updated><authors><author><keyname>Kivel&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Isomorphisms in Multilayer Networks</title><categories>physics.soc-ph cs.DM cs.SI math.CO</categories><comments>Working paper; 15 pages, 6 figures</comments><msc-class>05C82, 68R10, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the concept of graph isomorphisms to multilayer networks with any
number of &quot;aspects&quot; (i.e., types of layering), and we identify multiple types
of isomorphisms. For example, in multilayer networks with a single aspect,
permuting vertex labels, layer labels, and both vertex labels and layer labels
each yield different isomorphism relations between multilayer networks.
Multilayer network isomorphisms lead naturally to defining isomorphisms in any
of the numerous types of network that can be represented as a multilayer
network, and we thereby obtain isomorphisms for multiplex networks, temporal
networks, networks with both of these features, and more. We reduce each of the
multilayer network isomorphism problems to a graph isomorphism problem such
that the size of the graph isomorphism problem grows linearly with the size of
the multilayer network isomorphism problem. One can thus use software that has
been developed to solve graph isomorphism problems as a practical means for
solving multilayer network isomorphism problems. Our theory lays a foundation
for extending many network analysis methods such as motifs, graphlets,
structural roles, and network alignment to any multilayer network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00511</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00511</id><created>2015-06-01</created><updated>2015-09-25</updated><authors><author><keyname>Ba</keyname><forenames>Jimmy</forenames></author><author><keyname>Swersky</keyname><forenames>Kevin</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Predicting Deep Zero-Shot Convolutional Neural Networks using Textual
  Descriptions</title><categories>cs.LG cs.CV cs.NE</categories><comments>Correct the typos in table 1 regarding [5]. To appear in ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges in Zero-Shot Learning of visual categories is
gathering semantic attributes to accompany images. Recent work has shown that
learning from textual descriptions, such as Wikipedia articles, avoids the
problem of having to explicitly define these attributes. We present a new model
that can classify unseen categories from their textual description.
Specifically, we use text features to predict the output weights of both the
convolutional and the fully connected layers in a deep convolutional neural
network (CNN). We take advantage of the architecture of CNNs and learn features
at different layers, rather than just learning an embedding space for both
modalities, as is common with existing approaches. The proposed model also
allows us to automatically generate a list of pseudo- attributes for each
visual category consisting of words from Wikipedia articles. We train our
models end-to-end us- ing the Caltech-UCSD bird and flower datasets and
evaluate both ROC and Precision-Recall curves. Our empirical results show that
the proposed model significantly outperforms previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00522</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00522</id><created>2015-06-01</created><authors><author><keyname>Jetchev</keyname><forenames>Dimitar</forenames></author><author><keyname>Wesolowski</keyname><forenames>Benjamin</forenames></author></authors><title>On Graphs of Isogenies of Principally Polarizable Abelian Surfaces and
  the Discrete Logarithm Problem</title><categories>math.NT cs.CR</categories><comments>22 pages</comments><msc-class>11G20, 11G25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct certain isogeny graphs of principally polarized ordinary abelian
surfaces over finite fields and prove (under the Generalized Riemann
Hypothesis) rapid mixing properties for these graphs. We use these graphs,
together with a recent algorithm of Dudeanu, Jetchev and Robert for computing
explicit isogenies in genus 2, to prove ran- dom self-reducibility of the
discrete logarithm problem within the subclass of principally polarizable
ordinary abelian surfaces with maximal endomorphism ring. In addition, we
remove the heuristics in the complexity analysis of an algorithm of Galbraith
for explicitly computing isogenies between two elliptic curves in the same
isogeny class and generalize it in genus 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00527</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00527</id><created>2015-06-01</created><authors><author><keyname>Bianco</keyname><forenames>Simone</forenames></author><author><keyname>Ciocca</keyname><forenames>Gianluigi</forenames></author></authors><title>User Preferences Modeling and Learning for Pleasing Photo Collage
  Generation</title><categories>cs.MM cs.CV cs.HC</categories><comments>To be published in ACM Transactions on Multimedia Computing,
  Communications, and Applications (TOMM)</comments><acm-class>H.1.2; I.4.0; G.1.6; I.2.6; I.4.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider how to automatically create pleasing photo collages
created by placing a set of images on a limited canvas area. The task is
formulated as an optimization problem. Differently from existing
state-of-the-art approaches, we here exploit subjective experiments to model
and learn pleasantness from user preferences. To this end, we design an
experimental framework for the identification of the criteria that need to be
taken into account to generate a pleasing photo collage. Five different
thematic photo datasets are used to create collages using state-of-the-art
criteria. A first subjective experiment where several subjects evaluated the
collages, emphasizes that different criteria are involved in the subjective
definition of pleasantness. We then identify new global and local criteria and
design algorithms to quantify them. The relative importance of these criteria
are automatically learned by exploiting the user preferences, and new collages
are generated. To validate our framework, we performed several psycho-visual
experiments involving different users. The results shows that the proposed
framework allows to learn a novel computational model which effectively encodes
an inter-user definition of pleasantness. The learned definition of
pleasantness generalizes well to new photo datasets of different themes and
sizes not used in the learning. Moreover, compared with two state of the art
approaches, the collages created using our framework are preferred by the
majority of the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00528</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00528</id><created>2015-06-01</created><authors><author><keyname>Wang</keyname><forenames>Chang</forenames></author><author><keyname>Cao</keyname><forenames>Liangliang</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Medical Synonym Extraction with Concept Space Models</title><categories>cs.CL</categories><comments>7 pages, to appear in IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel approach for medical synonym extraction. We
aim to integrate the term embedding with the medical domain knowledge for
healthcare applications. One advantage of our method is that it is very
scalable. Experiments on a dataset with more than 1M term pairs show that the
proposed approach outperforms the baseline approaches by a large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00529</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00529</id><created>2015-06-01</created><authors><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author><author><keyname>Miranda</keyname><forenames>Enrique</forenames></author></authors><title>Desirability and the birth of incomplete preferences</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish an equivalence between two seemingly different theories: one is
the traditional axiomatisation of incomplete preferences on horse lotteries
based on the mixture independence axiom; the other is the theory of desirable
gambles developed in the context of imprecise probability. The equivalence
allows us to revisit incomplete preferences from the viewpoint of desirability
and through the derived notion of coherent lower previsions. On this basis, we
obtain new results and insights: in particular, we show that the theory of
incomplete preferences can be developed assuming only the existence of a worst
act---no best act is needed---, and that a weakened Archimedean axiom suffices
too; this axiom allows us also to address some controversy about the regularity
assumption (that probabilities should be positive---they need not), which
enables us also to deal with uncountable possibility spaces; we show that it is
always possible to extend in a minimal way a preference relation to one with a
worst act, and yet the resulting relation is never Archimedean, except in a
trivial case; we show that the traditional notion of state independence
coincides with the notion called strong independence in imprecise
probability---this leads us to give much a weaker definition of state
independence than the traditional one; we rework and uniform the notions of
complete preferences, beliefs, values; we argue that Archimedeanity does not
capture all the problems that can be modelled with sets of expected utilities
and we provide a new notion that does precisely that. Perhaps most importantly,
we argue throughout that desirability is a powerful and natural setting to
model, and work with, incomplete preferences, even in case of non-Archimedean
problems. This leads us to suggest that desirability, rather than preference,
should be the primitive notion at the basis of decision-theoretic
axiomatisations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00540</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00540</id><created>2015-06-01</created><authors><author><keyname>Gupta</keyname><forenames>Vipul</forenames></author><author><keyname>Kailkhura</keyname><forenames>Bhavya</forenames></author><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Joint Sparsity Pattern Recovery with 1-bit Compressive Sensing in Sensor
  Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, submitted in Asilomar Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of jointly sparse support recovery with 1-bit
compressive measurements in a sensor network. Sensors are assumed to observe
sparse signals having the same but unknown sparse support. Each sensor
quantizes its measurement vector element-wise to 1-bit and transmits the
quantized observations to a fusion center. We develop a computationally
tractable support recovery algorithm which minimizes a cost function defined in
terms of the likelihood function and the $l_{1,\infty}$ norm. We observe that
even with noisy 1-bit measurements, jointly sparse support can be recovered
accurately with multiple sensors each collecting only a small number of
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00547</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00547</id><created>2015-06-01</created><authors><author><keyname>Zlotnik</keyname><forenames>David Evan</forenames></author><author><keyname>Forbes</keyname><forenames>James Richard</forenames></author></authors><title>Differential Geometric SLAM</title><categories>cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simultaneous localization and mapping (SLAM) problem is considered in
three dimensions. The proposed algorithm, differential geometric SLAM
(DG-SLAM), employs methods from differential geometry to propagate the state
and map estimates. Unlike EKF SLAM, the proposed filter is provably
asymptotically stable under the assumption of no measurement noise or biases.
The robustness of the DG-SLAM algorithm is assessed in simulation with
measurement noise. The simulation demonstrates successful localization and
mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00548</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00548</id><created>2015-06-01</created><updated>2015-06-02</updated><authors><author><keyname>Junghanns</keyname><forenames>Martin</forenames></author><author><keyname>Petermann</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Kevin</forenames></author><author><keyname>Rahm</keyname><forenames>Erhard</forenames></author></authors><title>GRADOOP: Scalable Graph Data Management and Analytics with Hadoop</title><categories>cs.DB</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many Big Data applications in business and science require the management and
analysis of huge amounts of graph data. Previous approaches for graph analytics
such as graph databases and parallel graph processing systems (e.g., Pregel)
either lack sufficient scalability or flexibility and expressiveness. We are
therefore developing a new end-to-end approach for graph data management and
analysis based on the Hadoop ecosystem, called Gradoop (Graph analytics on
Hadoop). Gradoop is designed around the so-called Extended Property Graph Data
Model (EPGM) supporting semantically rich, schema-free graph data within many
distinct graphs. A set of high-level operators is provided for analyzing both
single graphs and collections of graphs. Based on these operators, we propose a
domain-specific language to define analytical workflows. The Gradoop graph
store is currently utilizing HBase for distributed storage of graph data in
Hadoop clusters. An initial version of Gradoop has been used to analyze graph
data for business intelligence and social network analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00552</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00552</id><created>2015-06-01</created><authors><author><keyname>Nutini</keyname><forenames>Julie</forenames></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author><author><keyname>Laradji</keyname><forenames>Issam H.</forenames></author><author><keyname>Friedlander</keyname><forenames>Michael</forenames></author><author><keyname>Koepke</keyname><forenames>Hoyt</forenames></author></authors><title>Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than
  Random Selection</title><categories>math.OC cs.LG stat.CO stat.ML</categories><comments>ICML 2015, 34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been significant recent work on the theory and application of
randomized coordinate descent algorithms, beginning with the work of Nesterov
[SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection
rule achieves the same convergence rate as the Gauss-Southwell selection rule.
This result suggests that we should never use the Gauss-Southwell rule, as it
is typically much more expensive than random selection. However, the empirical
behaviours of these algorithms contradict this theoretical result: in
applications where the computational costs of the selection rules are
comparable, the Gauss-Southwell selection rule tends to perform substantially
better than random coordinate selection. We give a simple analysis of the
Gauss-Southwell rule showing that---except in extreme cases---it's convergence
rate is faster than choosing random coordinates. Further, in this work we (i)
show that exact coordinate optimization improves the convergence rate for
certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that
gives an even faster convergence rate given knowledge of the Lipschitz
constants of the partial derivatives, (iii) analyze the effect of approximate
Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the
Gauss-Southwell rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00555</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00555</id><created>2015-06-01</created><authors><author><keyname>Brand&#xe3;o</keyname><forenames>Wladmir Cardoso</forenames></author></authors><title>Writing and Publishing Scientific Articles in Computer Science</title><categories>cs.GL</categories><comments>2 pages, 2 figures, Brazilian Portuguese</comments><acm-class>A.1; K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over 15 years of teaching, advising students and coordinating scientific
research activities and projects in computer science, we have observed the
difficulties of students to write scientific papers to present the results of
their research practices. In addition, they repeatedly have doubts about the
publishing process. In this article we propose a conceptual framework to
support the writing and publishing of scientific papers in computer science,
providing a kind of guide for computer science students to effectively present
the results of their research practices, particularly for experimental
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00568</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00568</id><created>2015-06-01</created><authors><author><keyname>Yang</keyname><forenames>Yanpeng</forenames></author><author><keyname>Sung</keyname><forenames>Ki Won</forenames></author></authors><title>Technical Rate of Substitution of Spectrum in Future Mobile Broadband
  Provisioning</title><categories>cs.NI</categories><comments>5 pages, 5 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dense deployment of base stations (BSs) and multi-antenna techniques are
considered key enablers for future mobile networks. Meanwhile, spectrum sharing
techniques and utilization of higher frequency bands make more bandwidth
available. An important question for future system design is which element is
more effective than others. In this paper, we introduce the concept of
technical rate of substitution (TRS) from microeconomics and study the TRS of
spectrum in terms of BS density and antenna number per BS. Numerical results
show that TRS becomes higher with increasing user data rate requirement,
suggesting that spectrum is the most effective means of provisioning extremely
fast mobile broadband.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00571</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00571</id><created>2015-06-01</created><updated>2015-06-04</updated><authors><author><keyname>Hatjimihail</keyname><forenames>Aristides T.</forenames></author></authors><title>Calculation of the confidence bounds for the fraction nonconforming of
  normal populations of measurements in clinical laboratory medicine</title><categories>cs.CE</categories><comments>26 pages, 5 tables, 8 figures</comments><report-no>hcsltr04</report-no><msc-class>6804</msc-class><acm-class>J.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The fraction nonconforming is a key quality measure used in statistical
quality control design in clinical laboratory medicine. The confidence bounds
of normal populations of measurements for the fraction nonconforming each of
the lower and upper quality specification limits when both the random and the
systematic error are unknown can be calculated using the noncentral
t-distribution, as it is described in detail and illustrated with examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00572</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00572</id><created>2015-06-01</created><updated>2015-06-13</updated><authors><author><keyname>Liao</keyname><forenames>Han-Teng</forenames></author><author><keyname>Fu</keyname><forenames>King-wa</forenames></author><author><keyname>Hale</keyname><forenames>Scott A.</forenames></author></authors><title>How much is said in a microblog? A multilingual inquiry based on Weibo
  and Twitter</title><categories>cs.SI cs.CL cs.CY</categories><comments>9 pages, 4 figures WebSci 2015</comments><acm-class>H.5.3, H.5.4</acm-class><doi>10.1145/2786451.2786486</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a multilingual study on, per single post of microblog
text, (a) how much can be said, (b) how much is written in terms of characters
and bytes, and (c) how much is said in terms of information content in posts by
different organizations in different languages. Focusing on three different
languages (English, Chinese, and Japanese), this research analyses Weibo and
Twitter accounts of major embassies and news agencies. We first establish our
criterion for quantifying &quot;how much can be said&quot; in a digital text based on the
openly available Universal Declaration of Human Rights and the translated
subtitles from TED talks. These parallel corpora allow us to determine the
number of characters and bits needed to represent the same content in different
languages and character encodings. We then derive the amount of information
that is actually contained in microblog posts authored by selected accounts on
Weibo and Twitter. Our results confirm that languages with larger character
sets such as Chinese and Japanese contain more information per character than
English, but the actual information content contained within a microblog text
varies depending on both the type of organization and the language of the post.
We conclude with a discussion on the design implications of microblog text
limits for different languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00573</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00573</id><created>2015-06-01</created><authors><author><keyname>Obukhov</keyname><forenames>Yuri</forenames></author><author><keyname>Jubert</keyname><forenames>Pierre-Olivier</forenames></author><author><keyname>Bedau</keyname><forenames>Daniel</forenames></author><author><keyname>Grobis</keyname><forenames>Michael</forenames></author></authors><title>Two-dimensional Decoding Algorithms and Recording Techniques for Bit
  Patterned Media Feasibility Demonstrations</title><categories>cs.IT math.IT</categories><comments>9 pages, 9 figures</comments><msc-class>94A40 (Primary), 94B10, 82D40, 68P30 (Secondary)</msc-class><acm-class>B.4.2; B.3.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recording experiments and decoding algorithms are presented for evaluating
the bit-error-rate of state-of-the-art magnetic bitpatterned media. The
recording experiments are performed with a static tester and conventional
hard-disk-drive heads. As the reader dimensions are larger than the bit
dimensions in both the down-track and the cross-track directions, a
two-dimensional bit decoding algorithm is required. Two such algorithms are
presented in details together with the methodology implemented to accurately
retrieve island positions during recording. Using these techniques, a 1.6
Td/in$^2$ magnetic bit pattern media is demonstrated to support 2D bit error
rates below 1e-2 under shingled magnetic recording conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00575</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00575</id><created>2015-06-01</created><updated>2016-01-06</updated><authors><author><keyname>Boumal</keyname><forenames>Nicolas</forenames></author></authors><title>A Riemannian low-rank method for optimization over semidefinite matrices
  with block-diagonal constraints</title><categories>math.OC cs.CV stat.CO</categories><comments>37 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm to solve optimization problems of the form $\min
f(X)$ for a smooth function $f$ under the constraints that $X$ is positive
semidefinite and the diagonal blocks of $X$ are small identity matrices. Such
problems often arise as the result of relaxing a rank constraint (lifting). In
particular, many estimation tasks involving phases, rotations, orthonormal
bases or permutations fit in this framework, and so do certain relaxations of
combinatorial problems such as Max-Cut. The proposed algorithm exploits the
facts that (1) such formulations admit low-rank solutions, and (2) their
rank-restricted versions are smooth optimization problems on a Riemannian
manifold. Combining insights from both the Riemannian and the convex geometries
of the problem, we characterize when second-order critical points of the smooth
problem reveal KKT points of the semidefinite problem. We compare against state
of the art, mature software and find that, on certain interesting problem
instances, what we call the staircase method is orders of magnitude faster, is
more accurate and scales better. Code is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00578</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00578</id><created>2015-06-01</created><authors><author><keyname>Blacoe</keyname><forenames>William</forenames></author></authors><title>On Quantum Generalizations of Information-Theoretic Measures and their
  Contribution to Distributional Semantics</title><categories>cs.IT cs.CL math.IT</categories><comments>Presented at Quantum Interaction Conference 2015, Filzbach,
  Switzerland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-theoretic measures such as relative entropy and correlation are
extremely useful when modeling or analyzing the interaction of probabilistic
systems. We survey the quantum generalization of 5 such measures and point out
some of their commonalities and interpretations. In particular we find the
application of information theory to distributional semantics useful. By
modeling the distributional meaning of words as density operators rather than
vectors, more of their semantic structure may be exploited. Furthermore,
properties of and interactions between words such as ambiguity, similarity and
entailment can be simulated more richly and intuitively when using methods from
quantum information theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00590</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00590</id><created>2015-06-01</created><authors><author><keyname>Thai</keyname><forenames>Long</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>Executing Bag of Distributed Tasks on Virtually Unlimited Cloud
  Resources</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bag-of-Distributed-Tasks (BoDT) application is the collection of identical
and independent tasks each of which requires a piece of input data located
around the world. As a result, Cloud computing offers an ef- fective way to
execute BoT application as it not only consists of multiple geographically
distributed data centres but also allows a user to pay for what she actually
uses only. In this paper, BoDT on the Cloud using virtually unlimited cloud
resources. A heuristic algorithm is proposed to find an execution plan that
takes budget constraints into account. Compared with other approaches, with the
same given budget, our algorithm is able to reduce the overall execution time
up to 50%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00598</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00598</id><created>2015-06-01</created><authors><author><keyname>Shalmashi</keyname><forenames>Serveh</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Sung</keyname><forenames>Ki Won</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Energy Efficiency and Sum Rate Tradeoffs for Massive MIMO Systems with
  Underlaid Device-to-Device Communications</title><categories>cs.IT math.IT</categories><comments>30 pages, 10 figures, Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the coexistence of two technologies that have
been put forward for the fifth generation (5G) of cellular networks, namely,
network-assisted device-to-device (D2D) communications and massive MIMO
(multiple-input multiple-output). Potential benefits of both technologies are
known individually, but the tradeoffs resulting from their coexistence have not
been adequately addressed. To this end, we assume that D2D users reuse the
downlink resources of cellular networks in an underlay fashion. In addition,
multiple antennas at the BS are used in order to obtain precoding gains and
simultaneously support multiple cellular users using multiuser or massive MIMO
technique. Two metrics are considered, namely the average sum rate (ASR) and
energy efficiency (EE). We derive tractable and directly computable expressions
and study the tradeoffs between the ASR and EE as functions of the number of BS
antennas, the number of cellular users and the density of D2D users within a
given coverage area. Our results show that both the ASR and EE behave
differently in scenarios with low and high density of D2D users, and that
coexistence of underlay D2D communications and massive MIMO is mainly
beneficial in low densities of D2D users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00617</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00617</id><created>2015-06-01</created><updated>2015-09-08</updated><authors><author><keyname>Kozachinskiy</keyname><forenames>Alexander</forenames></author></authors><title>On Slepian--Wolf Theorem with Interaction</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study interactive &quot;one-shot&quot; analogues of the classical
Slepian-Wolf theorem. Alice receives a value of a random variable $X$, Bob
receives a value of another random variable $Y$ that is jointly distributed
with $X$. Alice's goal is to transmit $X$ to Bob (with some error probability
$\varepsilon$). Instead of one-way transmission, which is studied in the
classical coding theory, we allow them to interact. They may also use shared
randomness.
  We show, that Alice can transmit $X$ to Bob in expected $H(X|Y) +
2\sqrt{H(X|Y)} + O(\log_2\left(\frac{1}{\varepsilon}\right))$ number of bits.
Moreover, we show that every one-round protocol $\pi$ with information
complexity $I$ can be compressed to the (many-round) protocol with expected
communication about $I + 2\sqrt{I}$ bits. This improves a result by Braverman
and Rao \cite{braverman2011information}, where they had $5\sqrt{I}$. Further,
we show how to solve this problem (transmitting $X$) using $3H(X|Y) +
O(\log_2\left(\frac{1}{\varepsilon}\right))$ bits and $4$ rounds on average.
This improves a result of~\cite{brody2013towards}, where they had $4H(X|Y) +
O(\log1/\varepsilon)$ bits and 10 rounds on average.
  In the end of the paper we discuss how many bits Alice and Bob may need to
communicate on average besides $H(X|Y)$. The main question is whether the upper
bounds mentioned above are tight. We provide an example of $(X, Y)$, such that
transmission of $X$ from Alice to Bob with error probability $\varepsilon$
requires $H(X|Y) + \Omega\left(\log_2\left(\frac{1}{\varepsilon}\right)\right)$
bits on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00619</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00619</id><created>2015-06-01</created><authors><author><keyname>van Merri&#xeb;nboer</keyname><forenames>Bart</forenames></author><author><keyname>Bahdanau</keyname><forenames>Dzmitry</forenames></author><author><keyname>Dumoulin</keyname><forenames>Vincent</forenames></author><author><keyname>Serdyuk</keyname><forenames>Dmitriy</forenames></author><author><keyname>Warde-Farley</keyname><forenames>David</forenames></author><author><keyname>Chorowski</keyname><forenames>Jan</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Blocks and Fuel: Frameworks for deep learning</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce two Python frameworks to train neural networks on large
datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler
with CUDA-support. It facilitates the training of complex neural network models
by providing parametrized Theano operations, attaching metadata to Theano's
symbolic computational graph, and providing an extensive set of utilities to
assist training the networks, e.g. training algorithms, logging, monitoring,
visualization, and serialization. Fuel provides a standard format for machine
learning datasets. It allows the user to easily iterate over large datasets,
performing many types of pre-processing on the fly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00627</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00627</id><created>2015-06-01</created><authors><author><keyname>Williams</keyname><forenames>Matthew J.</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Spatio-Temporal Complex Networks: Reachability, Centrality, and
  Robustness</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While recent advances in spatial and temporal networks have enabled
researchers to more-accurately describe many real-world systems, existing
models do not capture the combined constraint that space and time impose on the
relationships and interactions present in a spatio-temporal complex network.
This has important consequences, often resulting in an over-simplification of
the resilience of a system and obscuring the network's true structure. In this
paper, we study the response of spatio-temporal complex networks to random
error and systematic attack. Firstly, we propose a model of spatio-temporal
paths in time-varying spatially embedded networks. This model captures the
property that, in many real-world systems, interaction between nodes is
non-instantaneous and governed by the space in which they are embedded.
Secondly, using numerical experiments on four empirical examples of such
systems, we study the effect of node failure on a network's topological,
temporal, and spatial structure. We find that networks exhibit divergent
behaviour with respect to random error and systematic attack. Finally, to
identify weaknesses specific to the behaviour of a spatio-temporal system, we
introduce centrality measures that evaluate the importance of a node as a
structural bridge and its role in supporting temporally efficient flow through
the network. We explore the disruption to each system caused by attack
strategies based on each of these centrality measures. This exposes the complex
nature of fragility in a spatio-temporal system, showing that there is a
variety of failure modes when a network is subject to systematic attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00632</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00632</id><created>2015-05-29</created><authors><author><keyname>Buu</keyname><forenames>Olivier</forenames></author></authors><title>Sensitivity Analysis of Resonant Circuits</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use first-order perturbation theory to provide a local linear relation
between the circuit parameters and the poles of an RLC network. The sensitivity
matrix, which defines this relationship, is obtained from the systems
eigenvectors and the derivative of its eigenvalues. In general, the sensitivity
matrix is related to the equilibrium fluctuations of the system. In particular,
it may be used as the basis for a statistical model to efficiently predict the
sensitivity of the circuit response to small component variations. The method
is illustrated with a calculation of conditional probabilities by Monte Carlo
Simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00669</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00669</id><created>2015-06-01</created><authors><author><keyname>Le</keyname><forenames>Can M.</forenames></author><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>Concentration and regularization of random graphs</title><categories>math.PR cs.SI math.ST stat.TH</categories><comments>20 pages</comments><msc-class>05C80, 60B20, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies how close random graphs are typically to their
expectations. We interpret this question through the concentration of the
adjacency and Laplacian matrices in the spectral norm. We study inhomogeneous
Erd\&quot;os-R\'enyi random graphs on $n$ vertices, where edges form independently
and possibly with different probabilities $p_{ij}$. Sparse random graphs whose
expected degrees are $o(\log n)$ fail to concentrate. The obstruction is caused
by vertices with abnormally high and low degrees. We show that concentration
can be restored if we regularize the degrees of such vertices, and one can do
this is various ways. As an example, let us reweight or remove enough edges to
make all degrees bounded above by $O(d)$ where $d=\max pn_{ij}$. Then we show
that the resulting adjacency matrix $A'$ concentrates with the optimal rate:
$\|A' - \mathbb{E} A\| = O(\sqrt{d})$. Similarly, if we make all degrees
bounded below by $d$ by adding weight $d/n$ to all edges, then the resulting
Laplacian concentrates with the optimal rate: $\|L(A') - L(\mathbb{E} A')\| =
O(1/\sqrt{d})$. Our approach is based on Grothendieck-Pietsch factorization,
using which we construct a new decomposition of random graphs. These results
improve and simplify the recent work of L. Levina and the authors. We
illustrate the concentration results with an application to the community
detection problem in the analysis of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00671</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00671</id><created>2015-06-01</created><authors><author><keyname>Acharya</keyname><forenames>Jayadev</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Li</keyname><forenames>Jerry</forenames></author><author><keyname>Schmidt</keyname><forenames>Ludwig</forenames></author></authors><title>Sample-Optimal Density Estimation in Nearly-Linear Time</title><categories>cs.DS cs.IT cs.LG math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a new, fast algorithm for agnostically learning univariate
probability distributions whose densities are well approximated by piecewise
polynomial functions. Let $f$ be the density function of an arbitrary
univariate distribution, and suppose that $f$ is $\mathrm{OPT}$-close in
$L_1$-distance to an unknown piecewise polynomial function with $t$ interval
pieces and degree $d$. Our algorithm draws $n = O(t(d+1)/\epsilon^2)$ samples
from $f$, runs in time $\tilde{O}(n \cdot \mathrm{poly}(d))$, and with
probability at least $9/10$ outputs an $O(t)$-piecewise degree-$d$ hypothesis
$h$ that is $4 \cdot \mathrm{OPT} +\epsilon$ close to $f$.
  Our general algorithm yields (nearly) sample-optimal and nearly-linear time
estimators for a wide range of structured distribution families over both
continuous and discrete domains in a unified way. For most of our applications,
these are the first sample-optimal and nearly-linear time estimators in the
literature. As a consequence, our work resolves the sample and computational
complexities of a broad class of inference tasks via a single &quot;meta-algorithm&quot;.
Moreover, we experimentally demonstrate that our algorithm performs very well
in practice.
  Our algorithm consists of three &quot;levels&quot;: (i) At the top level, we employ an
iterative greedy algorithm for finding a good partition of the real line into
the pieces of a piecewise polynomial. (ii) For each piece, we show that the
sub-problem of finding a good polynomial fit on the current interval can be
solved efficiently with a separation oracle method. (iii) We reduce the task of
finding a separating hyperplane to a combinatorial problem and give an
efficient algorithm for this problem. Combining these three procedures gives a
density estimation algorithm with the claimed guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00677</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00677</id><created>2015-06-01</created><authors><author><keyname>Ghosal</keyname><forenames>Pratik</forenames></author><author><keyname>Kunysz</keyname><forenames>Adam</forenames></author><author><keyname>Paluch</keyname><forenames>Katarzyna</forenames></author></authors><title>Characterisation of Strongly Stable Matchings</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An instance of a strongly stable matching problem (SSMP) is an undirected
bipartite graph $G=(A \cup B, E)$, with an adjacency list of each vertex being
a linearly ordered list of ties, which are subsets of vertices equally good for
a given vertex. Ties are disjoint and may contain one vertex. A matching $M$ is
a set of vertex-disjoint edges. An edge $(x,y) \in E \setminus M$ is a {\em
blocking edge} for $M$ if $x$ is either unmatched or strictly prefers $y$ to
its current partner in $M$, and $y$ is either unmatched or strictly prefers $x$
to its current partner in $M$ or is indifferent between them. A matching is
{\em strongly stable} if there is no blocking edge with respect to it. We
present an algorithm for the generation of all strongly stable matchings, thus
solving an open problem already stated in the book by Gusfield and Irving
\cite{GI}. It has previously been shown that strongly stable matchings form a
distributive lattice and although the number of strongly stable matchings can
be exponential in the number of vertices, we show that there exists a partial
order with $O(m)$ elements representing all strongly stable matchings, where
$m$ denotes the number of edges in the graph. We give two algorithms that
construct two such representations: one in $O(nm^2)$ time and the other in
$O(nm)$ time, where $n$ denotes the number of vertices in the graph. Note that
the construction of the second representation has the same time complexity as
that of computing a single strongly stable matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00680</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00680</id><created>2015-06-01</created><authors><author><keyname>Chalk</keyname><forenames>Cameron T.</forenames></author><author><keyname>Fu</keyname><forenames>Bin</forenames></author><author><keyname>Huerta</keyname><forenames>Alejandro</forenames></author><author><keyname>Maldonado</keyname><forenames>Mario A.</forenames></author><author><keyname>Martinez</keyname><forenames>Eric</forenames></author><author><keyname>Schweller</keyname><forenames>Robert T.</forenames></author><author><keyname>Wylie</keyname><forenames>Tim</forenames></author></authors><title>Flipping Tiles: Concentration Independent Coin Flips in Tile
  Self-Assembly</title><categories>cs.FL cs.ET</categories><comments>DNA21 Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the \emph{robust coin flip} problem in which one
must design an abstract tile assembly system (aTAM system) whose terminal
assemblies can be partitioned such that the final assembly lies within either
partition with exactly probability 1/2, regardless of what relative
concentration assignment is given to the tile types of the system. We show that
robust coin flipping is possible within the aTAM, and that such systems can
guarantee a worst case $\mathcal{O}(1)$ space usage. As an application, we then
combine our coin-flip system with the result of Chandran, Gopalkrishnan, and
Reif~\cite{ChaGopRei12} to show that for any positive integer $n$, there exists
a $\mathcal{O}(\log n)$ tile system that assembles a constant-width linear
assembly of expected length $n$ that works for all concentration assignments.
We accompany our primary construction with variants that show trade-offs in
space complexity, initial seed size, temperature, tile complexity, bias, and
extensibility, and also prove some negative results. Further, we consider the
harder scenario in which tile concentrations change arbitrarily at each
assembly step and show that while this is not solvable in the aTAM, this
version of the problem can be solved by more exotic tile assembly models from
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00682</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00682</id><created>2015-06-01</created><updated>2016-02-29</updated><authors><author><keyname>Coviello</keyname><forenames>Lorenzo</forenames></author><author><keyname>Chen</keyname><forenames>Yiling</forenames></author><author><keyname>Franceschetti</keyname><forenames>Massimo</forenames></author></authors><title>Group buying with bundle discounts: computing efficient, stable and fair
  solutions</title><categories>cs.GT</categories><comments>34 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00684</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00684</id><created>2015-06-01</created><updated>2015-10-19</updated><authors><author><keyname>Wang</keyname><forenames>Zhiying</forenames></author><author><keyname>Cadambe</keyname><forenames>Viveck R.</forenames></author></authors><title>Multi-Version Coding - An Information Theoretic Perspective of
  Consistent Distributed Storage</title><categories>cs.IT math.IT</categories><comments>30 Pages. Extended version of conference publications in ISIT 2014
  and Allerton 2014. Revision adds a section, Section VII, and corrects minor
  typographical errors in the rest of the document</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In applications of distributed storage systems to distributed computing and
implementation of key- value stores, the following property, usually referred
to as consistency in computer science and engineering, is an important
requirement: as the data stored changes, the latest version of the data must be
accessible to a client that connects to the storage system. An information
theoretic formulation called multi-version coding is introduced in the paper,
in order to study storage costs of consistent distributed storage systems.
Multi-version coding is characterized by {\nu} totally ordered versions of a
message, and a storage system with n servers. At each server, values
corresponding to an arbitrary subset of the {\nu} versions are received and
encoded. For any subset of c servers in the storage system, the value
corresponding to the latest common version, or a later version as per the total
ordering, among the c servers is required to be decodable. An achievable
multi-version code construction via linear coding and a converse result that
shows that the construction is approximately tight, are provided. An
implication of the converse is that there is an inevitable price, in terms of
storage cost, to ensure consistency in distributed storage systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00685</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00685</id><created>2015-06-01</created><authors><author><keyname>Kamalapurkar</keyname><forenames>Rushikesh</forenames></author><author><keyname>Andrews</keyname><forenames>Lindsey</forenames></author><author><keyname>Walters</keyname><forenames>Patrick</forenames></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Model-based reinforcement learning for infinite-horizon approximate
  optimal tracking</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an approximate online adaptive solution to the
infinite-horizon optimal tracking problem for control-affine continuous-time
nonlinear systems with unknown drift dynamics. Model-based reinforcement
learning is used to relax the persistence of excitation condition. Model-based
reinforcement learning is implemented using a concurrent learning-based system
identifier to simulate experience by evaluating the Bellman error over
unexplored areas of the state space. Tracking of the desired trajectory and
convergence of the developed policy to a neighborhood of the optimal policy are
established via Lyapunov-based stability analysis. Simulation results
demonstrate the effectiveness of the developed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00694</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00694</id><created>2015-06-01</created><authors><author><keyname>Chapman</keyname><forenames>Archie C.</forenames></author><author><keyname>Verbic</keyname><forenames>Gregor</forenames></author></authors><title>An Iterative On-Line Mechanism for Demand-Side Aggregation</title><categories>cs.SY math.OC</categories><comments>11 pages, 7 figures, submitted to IEEE Transactions on Smart Grids</comments><doi>10.1109/TSG.2015.2457905</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a demand-side aggregation scheme specifically for large
numbers of small loads, such as households and small and medium-sized
businesses. We introduce a novel auction format, called a staggered clock-proxy
auction (SCPA), for on-line scheduling of these loads. This is a two phase
format, consisting of: a sequence of overlapping iterative ascending-price
clock auctions, one for each time-slot over a finite decision horizon, and; a
set of proxy auctions that begin at the termination of each individual clock
auction, and which determine the final price and allocation for each time-slot.
The overlapping design of the clock phases grant bidders the ability to
effectively bid on inter-temporal bundles of electricity use, thereby focusing
on the most relevant parts of the price-quantity space. Since electricity is a
divisible good, the proxy auction uses demand-schedule bids, which the
aggregator uses to compute a uniform-price partial competitive equilibrium for
each time slot. We show that, under mild assumptions on the bidders' utilities
functions, the proxy phase implements the Vickrey-Clarke-Groves outcome, which
makes straightforward bidding in the proxy phase a Bayes-Nash equilibrium.
Furthermore, we demonstrate the SCPA in a scenario comprised of household
agents with three different utility function types, and show how the mechanism
enables efficient on-line energy use scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00695</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00695</id><created>2015-06-01</created><updated>2015-07-13</updated><authors><author><keyname>Chonev</keyname><forenames>Ventsislav</forenames></author><author><keyname>Ouaknine</keyname><forenames>Joel</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>On the Decidability of the Bounded Continuous Skolem Problem</title><categories>cs.SY cs.SC</categories><comments>Submitted to SODA'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Continuous Skolem Problem asks whether a real-valued function satisfying
an ordinary linear differential equation has a zero in a given interval of real
numbers. This is a fundamental reachability problem arising in the analysis of
continuous linear dynamical systems, including linear hybrid automata and
continuous-time Markov chains. Not only is decidability of this problem open,
but decidability is open even for the sub-problem in which a zero is sought in
a bounded interval. In this paper we show decidability of the bounded problem
subject to Schanuel's conjecture, a central conjecture in transcendental number
theory. Regarding the unbounded case, by way of hardness we show that
decidability of the Continuous Skolem Problem would entail a major new
effectiveness result in Diophantine approximation, namely computability of the
Diophantine-approximation types of all real algebraic numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00698</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00698</id><created>2015-06-01</created><authors><author><keyname>Setiawan</keyname><forenames>Hendra</forenames></author><author><keyname>Huang</keyname><forenames>Zhongqiang</forenames></author><author><keyname>Devlin</keyname><forenames>Jacob</forenames></author><author><keyname>Lamar</keyname><forenames>Thomas</forenames></author><author><keyname>Zbib</keyname><forenames>Rabih</forenames></author><author><keyname>Schwartz</keyname><forenames>Richard</forenames></author><author><keyname>Makhoul</keyname><forenames>John</forenames></author></authors><title>Statistical Machine Translation Features with Multitask Tensor Networks</title><categories>cs.CL</categories><comments>11 pages (9 content + 2 references), 2 figures, accepted to ACL 2015
  as a long paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a three-pronged approach to improving Statistical Machine
Translation (SMT), building on recent success in the application of neural
networks to SMT. First, we propose new features based on neural networks to
model various non-local translation phenomena. Second, we augment the
architecture of the neural network with tensor layers that capture important
higher-order interaction among the network units. Third, we apply multitask
learning to estimate the neural network parameters jointly. Each of our
proposed methods results in significant improvements that are complementary.
The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and
Chinese-English translation over a state-of-the-art system that already
includes neural network features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00711</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00711</id><created>2015-06-01</created><authors><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author><author><keyname>Saleh</keyname><forenames>Babak</forenames></author></authors><title>Quantifying Creativity in Art Networks</title><categories>cs.AI cs.CV cs.CY cs.MM cs.SI</categories><comments>This paper will be published in the sixth International Conference on
  Computational Creativity (ICCC) June 29-July 2nd 2015, Park City, Utah, USA.
  This arXiv version is an extended version of the conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can we develop a computer algorithm that assesses the creativity of a
painting given its context within art history? This paper proposes a novel
computational framework for assessing the creativity of creative products, such
as paintings, sculptures, poetry, etc. We use the most common definition of
creativity, which emphasizes the originality of the product and its influential
value. The proposed computational framework is based on constructing a network
between creative products and using this network to infer about the originality
and influence of its nodes. Through a series of transformations, we construct a
Creativity Implication Network. We show that inference about creativity in this
network reduces to a variant of network centrality problems which can be solved
efficiently. We apply the proposed framework to the task of quantifying
creativity of paintings (and sculptures). We experimented on two datasets with
over 62K paintings to illustrate the behavior of the proposed framework. We
also propose a methodology for quantitatively validating the results of the
proposed algorithm, which we call the &quot;time machine experiment&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00716</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00716</id><created>2015-06-01</created><authors><author><keyname>Szil&#xe1;rd</keyname><forenames>P&#xe1;ll</forenames></author><author><keyname>Abraham</keyname><forenames>Mark James</forenames></author><author><keyname>Kutzner</keyname><forenames>Carsten</forenames></author><author><keyname>Hess</keyname><forenames>Berk</forenames></author><author><keyname>Lindahl</keyname><forenames>Erik</forenames></author></authors><title>Tackling Exascale Software Challenges in Molecular Dynamics Simulations
  with GROMACS</title><categories>cs.CE</categories><comments>EASC 2014 conference proceeding</comments><acm-class>J.2; I.6.8</acm-class><journal-ref>Proc. EASC 2014, 8759 pp. 3-27, Springer LNCS</journal-ref><doi>10.1007/978-3-319-15976-8_1</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  GROMACS is a widely used package for biomolecular simulation, and over the
last two decades it has evolved from small-scale efficiency to advanced
heterogeneous acceleration and multi-level parallelism targeting some of the
largest supercomputers in the world. Here, we describe some of the ways we have
been able to realize this through the use of parallelization on all levels,
combined with a constant focus on absolute performance. Release 4.6 of GROMACS
uses SIMD acceleration on a wide range of architectures, GPU offloading
acceleration, and both OpenMP and MPI parallelism within and between nodes,
respectively. The recent work on acceleration made it necessary to revisit the
fundamental algorithms of molecular simulation, including the concept of
neighborsearching, and we discuss the present and future challenges we see for
exascale simulation - in particular a very fine-grained task parallelism. We
also discuss the software management, code peer review and continuous
integration testing required for a project of this complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00717</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00717</id><created>2015-06-01</created><authors><author><keyname>Clarke</keyname><forenames>Charles L. A.</forenames></author><author><keyname>Culpepper</keyname><forenames>J. Shane</forenames></author><author><keyname>Moffat</keyname><forenames>Alistair</forenames></author></authors><title>Assessing Efficiency-Effectiveness Tradeoffs in Multi-Stage Retrieval
  Systems Without Using Relevance Judgments</title><categories>cs.IR</categories><acm-class>H.3.1; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale retrieval systems are often implemented as a cascading sequence
of phases -- a first filtering step, in which a large set of candidate
documents are extracted using a simple technique such as Boolean matching
and/or static document scores; and then one or more ranking steps, in which the
pool of documents retrieved by the filter is scored more precisely using dozens
or perhaps hundreds of different features. The documents returned to the user
are then taken from the head of the final ranked list. Here we examine methods
for measuring the quality of filtering and preliminary ranking stages, and show
how to use these measurements to tune the overall performance of the system.
Standard top-weighted metrics used for overall system evaluation are not
appropriate for assessing filtering stages, since the output is a set of
documents, rather than an ordered sequence of documents. Instead, we use an
approach in which a quality score is computed based on the discrepancy between
filtered and full evaluation. Unlike previous approaches, our methods do not
require relevance judgments, and thus can be used with virtually any query set.
We show that this quality score directly correlates with actual differences in
measured effectiveness when relevance judgments are available. Since the
quality score does not require relevance judgments, it can be used to identify
queries that perform particularly poorly for a given filter. Using these
methods, we explore a wide range of filtering options using thousands of
queries, categorize the relative merits of the different approaches, and
identify useful parameter combinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00718</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00718</id><created>2015-06-01</created><authors><author><keyname>Mori</keyname><forenames>Ryuhei</forenames></author><author><keyname>Watanabe</keyname><forenames>Osamu</forenames></author></authors><title>Peeling Algorithm on Random Hypergraphs with Superlinear Number of
  Hyperedges</title><categories>cs.DM</categories><comments>13 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When we try to solve a system of linear equations, we can consider a simple
iterative algorithm in which an equation including only one variable is chosen
at each step, and the variable is fixed to the value satisfying the equation.
The dynamics of this algorithm is captured by the peeling algorithm. Analyses
of the peeling algorithm on random hypergraphs are required for many problems,
e.g., the decoding threshold of low-density parity check codes, the inverting
threshold of Goldreich's pseudorandom generator, the load threshold of cuckoo
hashing, etc. In this work, we deal with random hypergraphs including
superlinear number of hyperedges, and derive the tight threshold for the
succeeding of the peeling algorithm. For the analysis, Wormald's method of
differential equations, which is commonly used for analyses of the peeling
algorithm on random hypergraph with linear number of hyperedges, cannot be used
due to the superlinear number of hyperedges. A new method called the evolution
of the moment generating function is proposed in this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00720</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00720</id><created>2015-06-01</created><authors><author><keyname>Nie</keyname><forenames>You-Qi</forenames></author><author><keyname>Huang</keyname><forenames>Leilei</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Payne</keyname><forenames>Frank</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Pan</keyname><forenames>Jian-Wei</forenames></author></authors><title>68 Gbps quantum random number generation by measuring laser phase
  fluctuations</title><categories>quant-ph cs.CR</categories><comments>14 pages, 5 figures. Accepted for publication in Review of Scientific
  Instruments</comments><journal-ref>Rev. Sci. Instrum. 86, 063105 (2015)</journal-ref><doi>10.1063/1.4922417</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The speed of a quantum random number generator is essential for practical
applications, such as high-speed quantum key distribution systems. Here, we
push the speed of a quantum random number generator to 68 Gbps by operating a
laser around its threshold level. To achieve the rate, not only high-speed
photodetector and high sampling rate are needed, but also a very stable
interferometer is required. A practical interferometer with active feedback
instead of common temperature control is developed to meet requirement of
stability. Phase fluctuations of the laser are measured by the interferometer
with a photodetector, and then digitalized to raw random numbers with a rate of
80 Gbps. The min-entropy of the raw data is evaluated by modeling the system
and is used to quantify the quantum randomness of the raw data. The bias of the
raw data caused by other signals, such as classical and detection noises, can
be removed by Toeplitz-matrix hashing randomness extraction. The final random
numbers can pass through the standard randomness tests. Our demonstration shows
that high-speed quantum random number generators are ready for practical usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00740</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00740</id><created>2015-06-01</created><updated>2016-01-25</updated><authors><author><keyname>Gabrys</keyname><forenames>Ryan</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Asymmetric Lee Distance Codes for DNA-Based Storage</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We consider a new family of codes, termed asymmetric Lee distance codes, that
arise in the design and implementation of DNA-based storage systems and systems
with parallel string transmission protocols. The codewords are defined over a
quaternary alphabet, although the results carry over to other alphabet sizes;
furthermore, symbol confusability is dictated by their underlying binary
representation. Our contributions are two-fold. First, we demonstrate that the
new distance represents a linear combination of the Lee and Hamming distance
and derive upper bounds on the size of the codes under this metric based on
linear programming techniques. Second, we propose a number of code
constructions which imply lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00743</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00743</id><created>2015-06-01</created><authors><author><keyname>Zhang</keyname><forenames>Xiaowang</forenames></author><author><keyname>Feng</keyname><forenames>Zhiyong</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Rao</keyname><forenames>Guozheng</forenames></author></authors><title>Context-Free Path Queries on RDF Graphs</title><categories>cs.DB</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A navigational query on a graph database returns binary relations over the
nodes of the graph. Recently, regular expression-based path query languages are
popular in expressing navigational queries on RDF graphs. It is natural to
replace regular expressions by context-free grammar so that the power of
current regular expression-based path query languages would be increased since
regular expressions are strictly subsumed in context-free grammar. In this
paper, we present context-free path queries and context-free SPARQL queries on
RDF graphs and investigate some of their fundamental properties. Our results
show that those proposed context-free path queries have efficient query
evaluation and context-free SPARQL queries have the same complexity of query
evaluation as SPARQL queries. Regarding expressiveness, we prove that those
context-free path queries exactly subsume corresponding fragments of nested
regular expressions and context-free SPARQL is a strict extension of both
SPARQL and nSPARQL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00744</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00744</id><created>2015-06-02</created><authors><author><keyname>Lin</keyname><forenames>Zhiyong</forenames></author><author><keyname>Liu</keyname><forenames>Hai</forenames></author><author><keyname>Yu</keyname><forenames>Lu</forenames></author><author><keyname>Leung</keyname><forenames>Yiu-Wing</forenames></author><author><keyname>Chu</keyname><forenames>Xiaowen</forenames></author></authors><title>ZOS: A Fast Rendezvous Algorithm Based on Set of Available Channels for
  Cognitive Radios</title><categories>cs.NI cs.DC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of existing rendezvous algorithms generate channel-hopping sequences
based on the whole channel set. They are inefficient when the set of available
channels is a small subset of the whole channel set. We propose a new algorithm
called ZOS which uses three types of elementary sequences (namely, Zero-type,
One-type, and S-type) to generate channel-hopping sequences based on the set of
available channels. ZOS provides guaranteed rendezvous without any additional
requirements. The maximum time-to-rendezvous of ZOS is upper-bounded by
O(m1*m2*log2M) where M is the number of all channels and m1 and m2 are the
numbers of available channels of two users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00745</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00745</id><created>2015-06-02</created><updated>2015-06-24</updated><authors><author><keyname>LaMont</keyname><forenames>Colin H.</forenames></author><author><keyname>Wiggins</keyname><forenames>Paul A.</forenames></author></authors><title>An objective prior that unifies objective Bayes and information-based
  inference</title><categories>stat.ML cs.LG physics.data-an</categories><comments>7 pages, 1 figure (+ minor corrections)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are three principle paradigms of statistical inference: (i) Bayesian,
(ii) information-based and (iii) frequentist inference. We describe an
objective prior (the weighting or $w$-prior) which unifies objective Bayes and
information-based inference. The $w$-prior is chosen to make the marginal
probability an unbiased estimator of the predictive performance of the model.
This definition has several other natural interpretations. From the perspective
of the information content of the prior, the $w$-prior is both uniformly and
maximally uninformative. The $w$-prior can also be understood to result in a
uniform density of distinguishable models in parameter space. Finally we
demonstrate the the $w$-prior is equivalent to the Akaike Information Criterion
(AIC) for regular models in the asymptotic limit. The $w$-prior appears to be
generically applicable to statistical inference and is free of {\it ad hoc}
regularization. The mechanism for suppressing complexity is analogous to AIC:
model complexity reduces model predictivity. We expect this new objective-Bayes
approach to inference to be widely-applicable to machine-learning problems
including singular models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00746</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00746</id><created>2015-06-02</created><updated>2015-07-20</updated><authors><author><keyname>Carlone</keyname><forenames>Luca</forenames></author><author><keyname>Rosen</keyname><forenames>David</forenames></author><author><keyname>Calafiore</keyname><forenames>Giuseppe</forenames></author><author><keyname>Leonard</keyname><forenames>John</forenames></author><author><keyname>Dellaert</keyname><forenames>Frank</forenames></author></authors><title>Lagrangian Duality in 3D SLAM: Verification Techniques and Optimal
  Solutions</title><categories>cs.RO math.OC</categories><comments>10 pages, 4 figures</comments><msc-class>68W01, 68W40, 68W25, 49K30</msc-class><acm-class>I.2.9; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art techniques for simultaneous localization and mapping (SLAM)
employ iterative nonlinear optimization methods to compute an estimate for
robot poses. While these techniques often work well in practice, they do not
provide guarantees on the quality of the estimate. This paper shows that
Lagrangian duality is a powerful tool to assess the quality of a given
candidate solution. Our contribution is threefold. First, we discuss a revised
formulation of the SLAM inference problem. We show that this formulation is
probabilistically grounded and has the advantage of leading to an optimization
problem with quadratic objective. The second contribution is the derivation of
the corresponding Lagrangian dual problem. The SLAM dual problem is a (convex)
semidefinite program, which can be solved reliably and globally by
off-the-shelf solvers. The third contribution is to discuss the relation
between the original SLAM problem and its dual. We show that from the dual
problem, one can evaluate the quality (i.e., the suboptimality gap) of a
candidate SLAM solution, and ultimately provide a certificate of optimality.
Moreover, when the duality gap is zero, one can compute a guaranteed optimal
SLAM solution from the dual problem, circumventing non-convex optimization. We
present extensive (real and simulated) experiments supporting our claims and
discuss practical relevance and open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00747</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00747</id><created>2015-06-02</created><authors><author><keyname>Jiang</keyname><forenames>Chaoyang</forenames></author><author><keyname>Soh</keyname><forenames>Yeng Chai</forenames></author><author><keyname>Li</keyname><forenames>Hua</forenames></author></authors><title>Sensor placement by maximal projection on minimum eigenspace for linear
  inverse problem</title><categories>cs.DS cs.IT math.IT</categories><comments>12 pages, 10 figures, and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new greedy sensor placement algorithm, named maximal
projection on minimum eigenspace (MPME), for linear inverse problem. For each
sensing location, the projection of its observation vector onto the eigenspace
associated with the minimum eigenvalue of the dual observation matrix is shown
to be negative correlated with the worst case error variance (WCEV) of the
estimated parameters. We individually select the sensing location whose
observation vector has a maximum projection onto the eigenspace of the minimum
eigenvalue of the current dual observation matrix. The proposed MPME is shown
computationally more efficient than the state-of-the-art. The Monte-Carlo
simulation shows that the MPME outperforms the convex relaxation method [1],
the SparSenSe [2], and the FrameSense [3] in terms of the WCEV and the mean
square error (MSE) of the estimated parameters, especially when the number of
sensor nodes is slightly more than the dimension of the estimated vector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00749</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00749</id><created>2015-06-02</created><authors><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>O'Donoghue</keyname><forenames>Brendan</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Large-Scale Convex Optimization for Dense Wireless Cooperative Networks</title><categories>cs.IT math.IT math.OC</categories><comments>to appear in IEEE Trans. Signal Process., 2015. Simulation code is
  available at https://github.com/SHIYUANMING/large-scale-convex-optimization</comments><doi>10.1109/TSP.2015.2443731</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex optimization is a powerful tool for resource allocation and signal
processing in wireless networks. As the network density is expected to
drastically increase in order to accommodate the exponentially growing mobile
data traffic, performance optimization problems are entering a new era
characterized by a high dimension and/or a large number of constraints, which
poses significant design and computational challenges. In this paper, we
present a novel two-stage approach to solve large-scale convex optimization
problems for dense wireless cooperative networks, which can effectively detect
infeasibility and enjoy modeling flexibility. In the proposed approach, the
original large-scale convex problem is transformed into a standard cone
programming form in the first stage via matrix stuffing, which only needs to
copy the problem parameters such as channel state information (CSI) and
quality-of-service (QoS) requirements to the pre-stored structure of the
standard form. The capability of yielding infeasibility certificates and
enabling parallel computing is achieved by solving the homogeneous self-dual
embedding of the primal-dual pair of the standard form. In the solving stage,
the operator splitting method, namely, the alternating direction method of
multipliers (ADMM), is adopted to solve the large-scale homogeneous self-dual
embedding. Compared with second-order methods, ADMM can solve large-scale
problems in parallel with modest accuracy within a reasonable amount of time.
Simulation results will demonstrate the speedup, scalability, and reliability
of the proposed framework compared with the state-of-the-art modeling
frameworks and solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00752</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00752</id><created>2015-06-02</created><authors><author><keyname>Suwajanakorn</keyname><forenames>Supasorn</forenames></author><author><keyname>Kemelmacher-Shlizerman</keyname><forenames>Ira</forenames></author><author><keyname>Seitz</keyname><forenames>Steve</forenames></author></authors><title>What Makes Kevin Spacey Look Like Kevin Spacey</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reconstruct a controllable model of a person from a large photo collection
that captures his or her {\em persona}, i.e., physical appearance and behavior.
The ability to operate on unstructured photo collections enables modeling a
huge number of people, including celebrities and other well photographed people
without requiring them to be scanned. Moreover, we show the ability to drive or
{\em puppeteer} the captured person B using any other video of a different
person A. In this scenario, B acts out the role of person A, but retains
his/her own personality and character. Our system is based on a novel
combination of 3D face reconstruction, tracking, alignment, and multi-texture
modeling, applied to the puppeteering problem. We demonstrate convincing
results on a large variety of celebrities derived from Internet imagery and
video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00753</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00753</id><created>2015-06-02</created><authors><author><keyname>Chan</keyname><forenames>Chun Lam</forenames></author><author><keyname>Fernandes</keyname><forenames>Winston</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author><author><keyname>Krishnapur</keyname><forenames>Manjunath</forenames></author></authors><title>Phase Transitions for the Uniform Distribution in the PML Problem and
  its Bethe Approximation</title><categories>cs.DM cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pattern maximum likelihood (PML) estimate, introduced by Orlitsky et al.,
is an estimate of the multiset of probabilities in an unknown probability
distribution $\mathbf{p}$, the estimate being obtained from $n$ i.i.d. samples
drawn from $\mathbf{p}$. The PML estimate involves solving a difficult
optimization problem over the set of all probability mass functions (pmfs) of
finite support. In this paper, we describe an interesting phase transition
phenomenon in the PML estimate: at a certain sharp threshold, the uniform
distribution goes from being a local maximum to being a local minimum for the
optimization problem in the estimate. We go on to consider the question of
whether a similar phase transition phenomenon also exists in the Bethe
approximation of the PML estimate, the latter being an approximation method
with origins in statistical physics. We show that the answer to this question
is a qualified &quot;Yes&quot;. Our analysis involves the computation of the mean and
variance of the $(i,j)$th entry, $a_{i,j}$, in a random $k \times k$
non-negative integer matrix $A$ with row and column sums all equal to $M$,
drawn according to a distribution that assigns to $A$ a probability
proportional to $\prod_{i,j} \frac{(M-a_{i,j})!}{a_{i,j}!}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00761</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00761</id><created>2015-06-02</created><authors><author><keyname>Van</keyname><forenames>Thanh The</forenames></author><author><keyname>Le</keyname><forenames>Thanh Manh</forenames></author></authors><title>Image Retrieval Based on Binary Signature ang S-kGraph</title><categories>cs.CV</categories><comments>17 pages, 9 figures</comments><acm-class>H.2.8; H.3.3</acm-class><journal-ref>Annales Univ. Sci. Budapest, Sect. Comp., 43(2014), pp.105-122</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an optimum approach for querying similar images
on large digital-image databases. Our work is based on RBIR (region-based image
retrieval) method which uses multiple regions as the key to retrieval images.
This method significantly improves the accuracy of queries. However, this also
increases the cost of computing. To reduce this expensive computational cost,
we implement binary signature encoder which maps an image to its identification
in binary. In order to fasten the lookup, binary signatures of images are
classified by the help of S-kGraph. Finally, our work is evaluated on COREL's
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00765</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00765</id><created>2015-06-02</created><authors><author><keyname>Cai</keyname><forenames>Zheng</forenames></author><author><keyname>Cao</keyname><forenames>Donglin</forenames></author><author><keyname>Ji</keyname><forenames>Rongrong</forenames></author></authors><title>Video (GIF) Sentiment Analysis using Large-Scale Mid-Level Ontology</title><categories>cs.MM cs.CL cs.IR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With faster connection speed, Internet users are now making social network a
huge reservoir of texts, images and video clips (GIF). Sentiment analysis for
such online platform can be used to predict political elections, evaluates
economic indicators and so on. However, GIF sentiment analysis is quite
challenging, not only because it hinges on spatio-temporal visual
contentabstraction, but also for the relationship between such abstraction and
final sentiment remains unknown.In this paper, we dedicated to find out such
relationship.We proposed a SentiPairSequence basedspatiotemporal visual
sentiment ontology, which forms the midlevel representations for GIFsentiment.
The establishment process of SentiPair contains two steps. First, we construct
the Synset Forest to define the semantic tree structure of visual sentiment
label elements. Then, through theSynset Forest, we organically select and
combine sentiment label elements to form a mid-level visual sentiment
representation. Our experiments indicate that SentiPair outperforms other
competing mid-level attributes. Using SentiPair, our analysis frameworkcan
achieve satisfying prediction accuracy (72.6%). We also opened ourdataset
(GSO-2015) to the research community. GSO-2015 contains more than 6,000
manually annotated GIFs out of more than 40,000 candidates. Each is labeled
with both sentiment and SentiPair Sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00768</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00768</id><created>2015-06-02</created><authors><author><keyname>Khurana</keyname><forenames>Madhu</forenames></author><author><keyname>Saxena</keyname><forenames>Vikas</forenames></author></authors><title>Soft Computing Techniques for Change Detection in remotely sensed images
  : A Review</title><categories>cs.NE cs.CV</categories><comments>9 pages, 1 table, 1 figure</comments><journal-ref>International Journal of Computer Science Issues, Volume 12, Issue
  2, March 2015, pp 245-253</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of remote sensing satellites, a huge repository of remotely
sensed images is available. Change detection in remotely sensed images has been
an active research area as it helps us understand the transitions that are
taking place on the Earths surface. This paper discusses the methods and their
classifications proposed by various researchers for change detection. Since use
of soft computing based techniques are now very popular among research
community, this paper also presents a classification based on learning
techniques used in soft-computing methods for change detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00769</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00769</id><created>2015-06-02</created><authors><author><keyname>Leukfeldt</keyname><forenames>Eric Rutger</forenames></author></authors><title>Comparing Victims of Phishing and Malware Attacks: Unraveling Risk
  Factors and Possibilities for Situational Crime Prevention</title><categories>cs.CY</categories><comments>Paper presented at the International Conference on Cyber Security.
  16-17 May 2015, Redlands, California, USA. (2015) International Journal of
  Advanced Studies in Computer Science and Engineering (IJASCSE) 4(5)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper compares the risk factors for becoming a victim of two types of
phishing: high-tech phishing (using malicious software) and low-tech phishing
(using e-mails and telephone calls). These risk factors are linked to
possibilities for situational crime prevention. Data from a cybercrime victim
survey in the Netherlands (n=10,316) is used. Based on routine activity theory,
the multivariate analyses include thirty variables. The findings show
situational crime prevention has to be aimed at groups other than just the
users themselves. Criminals are primarily interested in popular online places
and the onus is on the owners of these virtual places to protect their visitors
from getting infected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00770</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00770</id><created>2015-06-02</created><authors><author><keyname>Herrera-Yag&#xfc;e</keyname><forenames>C.</forenames></author><author><keyname>Schneider</keyname><forenames>C. M.</forenames></author><author><keyname>Couronn&#xe9;</keyname><forenames>T.</forenames></author><author><keyname>Smoreda</keyname><forenames>Z.</forenames></author><author><keyname>Benito</keyname><forenames>R. M.</forenames></author><author><keyname>Zufiria</keyname><forenames>P. J.</forenames></author><author><keyname>Gonz&#xe1;lez</keyname><forenames>M. C.</forenames></author></authors><title>The anatomy of urban social networks and its implications in the
  searchability problem</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The appearance of large geolocated communication datasets has recently
increased our understanding of how social networks relate to their physical
space. However, many recurrently reported properties, such as the spatial
clustering of network communities, have not yet been systematically tested at
different scales. In this work we analyze the social network structure of over
25 million phone users from three countries at three different scales: country,
provinces and cities. We consistently find that this last urban scenario
presents significant differences to common knowledge about social networks.
First, the emergence of a giant component in the network seems to be controlled
by whether or not the network spans over the entire urban border, almost
independently of the population or geographic extension of the city. Second,
urban communities are much less geographically clustered than expected. These
two findings shed new light on the widely-studied searchability in
self-organized networks. By exhaustive simulation of decentralized search
strategies we conclude that urban networks are searchable not through
geographical proximity as their country-wide counterparts, but through an
homophily-driven community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00779</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00779</id><created>2015-06-02</created><authors><author><keyname>Komiyama</keyname><forenames>Junpei</forenames></author><author><keyname>Honda</keyname><forenames>Junya</forenames></author><author><keyname>Nakagawa</keyname><forenames>Hiroshi</forenames></author></authors><title>Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed
  Bandit Problem with Multiple Plays</title><categories>stat.ML cs.LG</categories><comments>17 pages, 7 figures, to appear in ICML2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a multiple-play multi-armed bandit (MAB) problem in which several
arms are selected at each round. Recently, Thompson sampling (TS), a randomized
algorithm with a Bayesian spirit, has attracted much attention for its
empirically excellent performance, and it is revealed to have an optimal regret
bound in the standard single-play MAB problem. In this paper, we propose the
multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the
multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS
for binary rewards has the optimal regret upper bound that matches the regret
lower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the first
computationally efficient algorithm with optimal regret. A set of computer
simulations was also conducted, which compared MP-TS with state-of-the-art
algorithms. We also propose a modification of MP-TS, which is shown to have
better empirical performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00783</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00783</id><created>2015-06-02</created><updated>2015-11-10</updated><authors><author><keyname>Celledoni</keyname><forenames>Elena</forenames></author><author><keyname>Eslitzbichler</keyname><forenames>Markus</forenames></author><author><keyname>Schmeding</keyname><forenames>Alexander</forenames></author></authors><title>Shape Analysis on Lie Groups with Applications in Computer Animation</title><categories>math.DG cs.GR math.NA</categories><comments>41 pages, 7 figures, v2: Substantial revision, corrected several
  errors, added many new results on Riemannian geometry related to the square
  root velocity transform, main results remain unchanged</comments><msc-class>58D15 (primary), 58D05, 22E65, 58B10, 58B20 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shape analysis methods have in the past few years become very popular, both
for theoretical exploration as well as from an application point of view.
Originally developed for planar curves, these methods have been expanded to
higher dimensional curves, surfaces, activities, character motions and many
other objects. In this paper, we develop a framework for shape analysis of
curves in Lie groups for problems of computer animations. In particular, we
will use these methods to find cyclic approximations of non-cyclic character
animations and interpolate between existing animations to generate new ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00785</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00785</id><created>2015-06-02</created><authors><author><keyname>Byrne</keyname><forenames>Eimear</forenames></author><author><keyname>Calderini</keyname><forenames>Marco</forenames></author></authors><title>Error Correction for Index Coding With Coded Side Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the original index coding problem, each user has a set of uncoded packets
as side information, and wants to decode some other packets from the source
node. Shum et al, (2013) considered the more general problem where side
information can be also coded. We refer to this class of problems as index
coding with coded side-information (ICCSI). We describe a generalized min-rank
for the ICCSI problem, and obtain bounds on this quantity taking an algebraic
approach rather than one from graph theory. We also consider error correction
for the ICCSI problem, both for the Hamming and rank metric and address the
question of the main index coding problem for error correcting index codes
(ECIC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00794</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00794</id><created>2015-06-02</created><updated>2015-06-02</updated><authors><author><keyname>Wang</keyname><forenames>Wenhao</forenames></author></authors><title>Notes on Rainbow Distinguished Point Method</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper clarifies the method described in our previous paper (DOI:
978-3-319-02726-5\_21), namely rainbow distinguished point method, and give
revised theoretical and experimental results which shows rainbow distinguished
point method behaves inferiorly to other time memory tradeoff methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00799</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00799</id><created>2015-06-02</created><authors><author><keyname>Zeng</keyname><forenames>Xiangyu</forenames></author><author><keyname>Yin</keyname><forenames>Shi</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author></authors><title>Learning Speech Rate in Speech Recognition</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant performance reduction is often observed in speech recognition
when the rate of speech (ROS) is too low or too high. Most of present
approaches to addressing the ROS variation focus on the change of speech
signals in dynamic properties caused by ROS, and accordingly modify the dynamic
model, e.g., the transition probabilities of the hidden Markov model (HMM).
However, an abnormal ROS changes not only the dynamic but also the static
property of speech signals, and thus can not be compensated for purely by
modifying the dynamic model. This paper proposes an ROS learning approach based
on deep neural networks (DNN), which involves an ROS feature as the input of
the DNN model and so the spectrum distortion caused by ROS can be learned and
compensated for. The experimental results show that this approach can deliver
better performance for too slow and too fast utterances, demonstrating our
conjecture that ROS impacts both the dynamic and the static property of speech.
In addition, the proposed approach can be combined with the conventional HMM
transition adaptation method, offering additional performance gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00815</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00815</id><created>2015-06-02</created><updated>2015-06-11</updated><authors><author><keyname>Hu</keyname><forenames>Yuhuang</forenames></author><author><keyname>Ishwarya</keyname><forenames>M. S.</forenames></author><author><keyname>Loo</keyname><forenames>Chu Kiong</forenames></author></authors><title>Classify Images with Conceptor Network</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article demonstrates a new conceptor network based classifier in
classifying images. Mathematical descriptions and analysis are presented.
Various tests are experimented using three benchmark datasets: MNIST, CIFAR-10
and CIFAR-100. The experiments displayed that conceptor network can offer
superior results and flexible configurations than conventional classifiers such
as Softmax Regression and Support Vector Machine (SVM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00820</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00820</id><created>2015-06-02</created><authors><author><keyname>Golo</keyname><forenames>Natasa</forenames></author><author><keyname>Galam</keyname><forenames>Serge</forenames></author></authors><title>Conspiratorial beliefs observed through entropy principles</title><categories>physics.soc-ph cs.SI</categories><comments>21 page, 14 figures</comments><doi>10.3390/e17085611</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a novel approach framed in terms of information theory and entropy
to tackle the issue of conspiracy theories propagation. We start with the
report of an event (such as 9/11 terroristic attack) represented as a series of
individual strings of information denoted respectively by two-state variable
Ei=+/-1, i=1,..., N. Assigning Ei value to all strings, the initial order
parameter and entropy are determined. Conspiracy theorists comment on the
report, focusing repeatedly on several strings Ek and changing their meaning
(from -1 to +1). The reading of the event is turned fuzzy with an increased
entropy value. Beyond some threshold value of entropy, chosen by simplicity to
its maximum value, meaning N/2 variables with Ei=1, doubt prevails in the
reading of the event and the chance is created that an alternative theory might
prevail. Therefore, the evolution of the associated entropy is a way to measure
the degree of penetration of a conspiracy theory. Our general framework relies
on online content made voluntarily available by crowds of people, in response
to some news or blog articles published by official news agencies. We apply
different aggregation levels (comment, person, discussion thread) and discuss
the associated patterns of entropy change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00821</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00821</id><created>2015-06-02</created><updated>2015-07-03</updated><authors><author><keyname>Hoang</keyname><forenames>Hung Gia</forenames></author><author><keyname>Vo</keyname><forenames>Ba-Tuong</forenames></author><author><keyname>Vo</keyname><forenames>Ba-Ngu</forenames></author></authors><title>A Generalized Labeled Multi-Bernoulli Filter Implementation using Gibbs
  Sampling</title><categories>stat.CO cs.LG</categories><comments>11 pages, 8 figures. Part of the paper has been accepted for
  presentation at the 18th international conference on Information Fusion
  (FUSION 15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an efficient implementation of the generalized labeled
multi-Bernoulli (GLMB) filter by combining the prediction and update into a
single step. In contrast to the original approach which involves separate
truncations in the prediction and update steps, the proposed implementation
requires only one single truncation for each iteration, which can be performed
using a standard ranked optimal assignment algorithm. Furthermore, we propose a
new truncation technique based on Markov Chain Monte Carlo methods such as
Gibbs sampling, which drastically reduces the complexity of the filter. The
superior performance of the proposed approach is demonstrated through extensive
numerical studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00828</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00828</id><created>2015-06-02</created><updated>2015-09-22</updated><authors><author><keyname>Daum</keyname><forenames>Sebastian</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author><author><keyname>Maus</keyname><forenames>Yannic</forenames></author></authors><title>Rumor Spreading with Bounded In-Degree</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classic gossip-based model of communication for disseminating
information in a network, in each time unit, every node $u$ is allowed to
contact a single random neighbor $v$. If $u$ knows the data (rumor) to be
disseminated, it disperses it to $v$ (known as PUSH) and if it does not, it
requests it from $v$ (known as PULL). While in the classic gossip model, each
node is only allowed to contact a single neighbor in each time unit, each node
can possibly be contacted by many neighboring nodes.
  In the present paper, we consider a restricted model where at each node only
one incoming request can be served. As long as only a single piece of
information needs to be disseminated, this does not make a difference for push
requests. It however has a significant effect on pull requests. In the paper,
we therefore concentrate on this weaker pull version, which we call 'restricted
pull'.
  We distinguish two versions of the restricted pull protocol depending on
whether the request to be served among a set of pull requests at a given node
is chosen adversarially or uniformly at random. As a first result, we prove an
exponential separation between the two variants. We show that there are
instances where if an adversary picks the request to be served, the restricted
pull protocol requires a polynomial number of rounds whereas if the winning
request is chosen uniformly at random, the restricted pull protocol only
requires a polylogarithmic number of rounds to inform the whole network.
Further, as the main technical contribution, we show that if the request to be
served is chosen randomly, the slowdown of using restricted pull versus using
the classic pull protocol can w.h.p. be upper bounded by $O(\Delta / \delta
\log n)$, where $\Delta$ and $\delta$ are the largest and smallest degree of
the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00839</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00839</id><created>2015-06-02</created><authors><author><keyname>Ribeiro</keyname><forenames>Eug&#xe9;nio</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author></authors><title>The Influence of Context on Dialogue Act Recognition</title><categories>cs.CL</categories><comments>32 pages, 9 figures, 6 tables, 1 equation, submitted to Computer
  Speech and Language (Elsevier)</comments><acm-class>H.1.2; H.3.1; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a deep analysis of the influence of context information
on dialogue act recognition. We performed experiments on the annotated subsets
of three different corpora: the widely explored Switchboard Dialog Act Corpus,
as well as the unexplored LEGO and Cambridge Restaurant corpora.
  In contrast with previous work, especially in what concerns the Switchboard
corpus, we used an event-based classification approach, using SVMs, instead of
the more common sequential approaches, such as HMMs. We opted for such an
approach so that we could control the amount of provided context information
and, thus, explore its range of influence. Our base features consist of
n-grams, punctuation, and wh-words. Context information is obtained from
previous utterances and provided in three ways -- n-grams, n-grams tagged with
relative position, and dialogue act classifications. A comparative study was
conducted to evaluate the performance of the three approaches. From it, we were
able to assess the importance of context information on dialogue act
recognition, as well as its range of influence for each of the three selected
representations.
  In addition to the conclusions originated by the analysis, this work also
produced results that advance the state-of-the-art, especially considering
previous work on the Switchboard corpus. Furthermore, since, to our knowledge,
the remaining datasets had not been previously explored for this task, our
experiments can be used as baselines for future work on those corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00842</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00842</id><created>2015-06-02</created><authors><author><keyname>Falch</keyname><forenames>Thomas L.</forenames></author><author><keyname>Elster</keyname><forenames>Anne C.</forenames></author></authors><title>Machine Learning Based Auto-tuning for Enhanced OpenCL Performance
  Portability</title><categories>cs.DC</categories><comments>This is a pre-print version an article to be published in the
  Proceedings of the 2015 IEEE International Parallel and Distributed
  Processing Symposium Workshops (IPDPSW). For personal use only</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous computing, which combines devices with different architectures,
is rising in popularity, and promises increased performance combined with
reduced energy consumption. OpenCL has been proposed as a standard for
programing such systems, and offers functional portability. It does, however,
suffer from poor performance portability, code tuned for one device must be
re-tuned to achieve good performance on another device. In this paper, we use
machine learning-based auto-tuning to address this problem. Benchmarks are run
on a random subset of the entire tuning parameter configuration space, and the
results are used to build an artificial neural network based model. The model
can then be used to find interesting parts of the parameter space for further
search. We evaluate our method with different benchmarks, on several devices,
including an Intel i7 3770 CPU, an Nvidia K40 GPU and an AMD Radeon HD 7970
GPU. Our model achieves a mean relative error as low as 6.1%, and is able to
find configurations as little as 1.3% worse than the global minimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00844</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00844</id><created>2015-06-02</created><authors><author><keyname>Fikadu</keyname><forenames>Mulugeta K.</forenames></author><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Cui</keyname><forenames>Qimei</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Exact Error Analysis and Energy-Efficiency Optimization of Regenerative
  Relay Systems with Spatial Correlation</title><categories>cs.IT math.IT</categories><comments>43 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency and its optimization constitute critical tasks in the
design of low-power wireless networks. The present work is devoted to the error
rate analysis and energy-efficiency optimization of regenerative cooperative
networks in the presence of multipath fading under spatial correlation. To this
end, exact and asymptotic analytic expressions are firstly derived for the
symbol-error-rate of $M{-}$ary quadrature amplitude and $M{-}$ary phase shift
keying modulations assuming a dual-hop decode-and-forward relay system,
spatially correlated Nakagami${-}m$ multipath fading and maximum ratio
combining. The derived expressions are subsequently employed in quantifying the
energy consumption of the considered system, incorporating both transmit energy
and the energy consumed by the transceiver circuits, as well as in deriving the
optimal power allocation formulation for minimizing energy consumption under
certain quality-of-service requirements. A relatively harsh path-loss model,
that also accounts for realistic device-to-device communications, is adopted in
numerical evaluations and various useful insights are provided for the design
of future low-energy wireless networks deployments. Indicatively, it is shown
that depending on the degree of spatial correlation, severity of fading,
transmission distance, relay location and power allocation strategy, target
performance can be achieved with large overall energy reduction compared to
direct transmission reference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00852</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00852</id><created>2015-06-02</created><updated>2016-02-10</updated><authors><author><keyname>Sajjadi</keyname><forenames>Mehdi S. M.</forenames></author><author><keyname>Alamgir</keyname><forenames>Morteza</forenames></author><author><keyname>von Luxburg</keyname><forenames>Ulrike</forenames></author></authors><title>Peer Grading in a Course on Algorithms and Data Structures: Machine
  Learning Algorithms do not Improve over Simple Baselines</title><categories>cs.LG stat.ML</categories><comments>Published at the Third Annual ACM Conference on Learning at Scale L@S</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer grading is the process of students reviewing each others' work, such as
homework submissions, and has lately become a popular mechanism used in massive
open online courses (MOOCs). Intrigued by this idea, we used it in a course on
algorithms and data structures at the University of Hamburg. Throughout the
whole semester, students repeatedly handed in submissions to exercises, which
were then evaluated both by teaching assistants and by a peer grading
mechanism, yielding a large dataset of teacher and peer grades. We applied
different statistical and machine learning methods to aggregate the peer grades
in order to come up with accurate final grades for the submissions (supervised
and unsupervised, methods based on numeric scores and ordinal rankings).
Surprisingly, none of them improves over the baseline of using the mean peer
grade as the final grade. We discuss a number of possible explanations for
these results and present a thorough analysis of the generated dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00853</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00853</id><created>2015-06-02</created><updated>2015-07-10</updated><authors><author><keyname>Czumaj</keyname><forenames>Artur</forenames></author><author><keyname>Davies</keyname><forenames>Peter</forenames></author></authors><title>Almost Optimal Deterministic Broadcast in Radio Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the fastest known algorithm for the fundamental
problem of deterministic broadcast in ad-hoc radio networks with unknown
topology, which completes the task in $O(n \log D \log \log \frac{D
\Delta}{n})$ time-steps, where $n$ is the number of nodes in the network,
$\Delta$ is the maximum in-degree of any node, and $D$ is the eccentricity of
the network. This comes within a log-logarithmic factor of the $\Omega(n \log
D)$ lower bound due to Clementi et. al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00858</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00858</id><created>2015-06-02</created><updated>2015-12-08</updated><authors><author><keyname>Tu</keyname><forenames>Kewei</forenames></author></authors><title>Stochastic And-Or Grammars: A Unified Framework and Logic Perspective</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic And-Or grammars (AOG) extend traditional stochastic grammars of
language to model other types of data such as images and events. In this paper
we propose a representation framework of stochastic AOGs that is agnostic to
the type of the data being modeled and thus unifies various domain-specific
AOGs. Many existing grammar formalisms and probabilistic models in natural
language processing, computer vision, and machine learning can be seen as
special cases of this framework. We also propose a domain-independent inference
algorithm of stochastic context-free AOGs and show its tractability under a
reasonable assumption. Furthermore, we provide an interpretation of stochastic
context-free AOGs as a subset of first-order probabilistic logic, which
connects stochastic AOGs to the field of statistical relational learning. Based
on the interpretation, we clarify the relation between stochastic AOGs and a
few existing statistical relational models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00868</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00868</id><created>2015-06-02</created><authors><author><keyname>Bassino</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author><author><keyname>Bouvel</keyname><forenames>Mathilde</forenames></author><author><keyname>Pierrot</keyname><forenames>Adeline</forenames></author><author><keyname>Pivoteau</keyname><forenames>Carine</forenames></author><author><keyname>Rossin</keyname><forenames>Dominique</forenames></author></authors><title>An algorithm computing combinatorial specifications of permutation
  classes</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a methodology that automatically derives a
combinatorial specification for a permutation class C, given its basis B of
excluded patterns and the set of simple permutations in C, when these sets are
both finite. This is achieved considering both pattern avoidance and pattern
containment constraints in permutations. The obtained specification yields a
system of equations satisfied by the generating function of C, this system
being always positive and algebraic. It also yields a uniform random sampler of
permutations in C. The method presented is fully algorithmic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00880</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00880</id><created>2015-06-02</created><updated>2015-12-09</updated><authors><author><keyname>Lombana</keyname><forenames>Daniel Alberto Burbano</forenames></author><author><keyname>di Bernardo</keyname><forenames>Mario</forenames></author></authors><title>Multiplex PI-Control for Consensus in Networks of Heterogeneous Linear
  Agents</title><categories>cs.SY</categories><comments>13 pages, 6 Figures, Preprint submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a multiplex proportional-integral approach, for
solving consensus problems in networks of heterogeneous nodes dynamics affected
by constant disturbances. The proportional and integral actions are deployed on
two different layers across the network, each with its own topology. Sufficient
conditions for convergence are derived that depend upon the structure of the
network, the parameters characterizing the control layers and the node
dynamics. The effectiveness of the theoretical results is illustrated using a
power network model as a representative example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00884</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00884</id><created>2015-06-02</created><authors><author><keyname>Endrullis</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Grabmayer</keyname><forenames>Clemens</forenames></author><author><keyname>Hendriks</keyname><forenames>Dimitri</forenames></author><author><keyname>Zantema</keyname><forenames>Hans</forenames></author></authors><title>The Degree of Squares is an Atom (Extended Version)</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We answer an open question in the theory of degrees of infinite sequences
with respect to transducibility by finite-state transducers. An initial study
of this partial order of degrees was carried out in (Endrullis, Hendriks, Klop,
2011), but many basic questions remain unanswered. One of the central questions
concerns the existence of atom degrees, other than the degree of the `identity
sequence' 1 0^0 1 0^1 1 0^2 1 0^3 .... A degree is called an `atom' if below it
there is only the bottom degree 0, which consists of the ultimately periodic
sequences. We show that also the degree of the `squares sequence' 1 0^0 1 0^1 1
0^4 1 0^9 1 0^{16} ... is an atom. As the main tool for this result we
characterise the transducts of `spiralling' sequences and their degrees. We use
this to show that every transduct of a `polynomial sequence' either is in 0 or
can be transduced back to a polynomial sequence for a polynomial of the same
order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00893</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00893</id><created>2015-06-02</created><authors><author><keyname>C&#xf4;rte-Real</keyname><forenames>Joana</forenames></author><author><keyname>Mantadelis</keyname><forenames>Theofrastos</forenames></author><author><keyname>Dutra</keyname><forenames>In&#xea;s</forenames></author><author><keyname>Rocha</keyname><forenames>Ricardo</forenames></author></authors><title>SkILL - a Stochastic Inductive Logic Learner</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Inductive Logic Programming (PILP) is a rel- atively unexplored
area of Statistical Relational Learning which extends classic Inductive Logic
Programming (ILP). This work introduces SkILL, a Stochastic Inductive Logic
Learner, which takes probabilistic annotated data and produces First Order
Logic theories. Data in several domains such as medicine and bioinformatics
have an inherent degree of uncer- tainty, that can be used to produce models
closer to reality. SkILL can not only use this type of probabilistic data to
extract non-trivial knowl- edge from databases, but it also addresses
efficiency issues by introducing a novel, efficient and effective search
strategy to guide the search in PILP environments. The capabilities of SkILL
are demonstrated in three dif- ferent datasets: (i) a synthetic toy example
used to validate the system, (ii) a probabilistic adaptation of a well-known
biological metabolism ap- plication, and (iii) a real world medical dataset in
the breast cancer domain. Results show that SkILL can perform as well as a
deterministic ILP learner, while also being able to incorporate probabilistic
knowledge that would otherwise not be considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00894</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00894</id><created>2015-06-02</created><updated>2015-10-12</updated><authors><author><keyname>Strasberg</keyname><forenames>Philipp</forenames></author><author><keyname>Cerrillo</keyname><forenames>Javier</forenames></author><author><keyname>Schaller</keyname><forenames>Gernot</forenames></author><author><keyname>Brandes</keyname><forenames>Tobias</forenames></author></authors><title>Thermodynamics of stochastic Turing machines</title><categories>cond-mat.stat-mech cs.FL</categories><comments>13 pages incl. appendix, 3 figures and 1 table, slightly changed
  version as published in PRE</comments><journal-ref>Phys. Rev. E 92, 042104 (2015)</journal-ref><doi>10.1103/PhysRevE.92.042104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analogy to Brownian computers we explicitly show how to construct
stochastic models, which mimic the behaviour of a general purpose computer (a
Turing machine). Our models are discrete state systems obeying a Markovian
master equation, which are logically reversible and have a well-defined and
consistent thermodynamic interpretation. The resulting master equation, which
describes a simple one-step process on an enormously large state space, allows
us to thoroughly investigate the thermodynamics of computation for this
situation. Especially, in the stationary regime we can well approximate the
master equation by a simple Fokker-Planck equation in one dimension. We then
show that the entropy production rate at steady state can be made arbitrarily
small, but the total (integrated) entropy production is finite and grows
logarithmically with the number of computational steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00898</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00898</id><created>2015-06-02</created><updated>2015-10-28</updated><authors><author><keyname>Azizyan</keyname><forenames>Martin</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Extreme Compressive Sampling for Covariance Estimation</title><categories>stat.ML cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of estimating the covariance of a collection
of vectors using only extremely compressed measurements of each vector. An
estimator based on back-projections of these compressive samples is proposed
and analyzed. A distribution-free analysis shows that by observing just a
single compressive measurement of each vector, one can consistently estimate
the covariance matrix, in both infinity and spectral norm, and this same
analysis leads to precise rates of convergence in both norms. Via
information-theoretic techniques, lower bounds showing that this estimator is
minimax-optimal for both infinity and spectral norm estimation problems are
established. These results are also specialized to give matching upper and
lower bounds for estimating the population covariance of a collection of
Gaussian vectors, again in the compressive measurement model. The analysis
conducted in this paper shows that the effective sample complexity for this
problem is scaled by a factor of $m^2/d^2$ where $m$ is the compression
dimension and $d$ is the ambient dimension. Applications to subspace learning
(Principal Components Analysis) and learning over distributed sensor networks
are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00899</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00899</id><created>2015-06-02</created><authors><author><keyname>Rao</keyname><forenames>Xiongbin</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Compressive Sensing with Prior Support Quality Information and
  Application to Massive MIMO Channel Estimation with Temporal Correlation</title><categories>cs.IT math.IT</categories><comments>14 double-column pages, accepted for publication in IEEE transactions
  on signal processing in May, 2015</comments><doi>10.1109/TSP.2015.2446444</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of compressive sensing (CS) recovery
with a prior support and the prior support quality information available.
Different from classical works which exploit prior support blindly, we shall
propose novel CS recovery algorithms to exploit the prior support adaptively
based on the quality information. We analyze the distortion bound of the
recovered signal from the proposed algorithm and we show that a better quality
prior support can lead to better CS recovery performance. We also show that the
proposed algorithm would converge in $\mathcal{O}\left(\log\mbox{SNR}\right)$
steps. To tolerate possible model mismatch, we further propose some robustness
designs to combat incorrect prior support quality information. Finally, we
apply the proposed framework to sparse channel estimation in massive MIMO
systems with temporal correlation to further reduce the required pilot training
overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00904</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00904</id><created>2015-06-02</created><authors><author><keyname>Kiseleva</keyname><forenames>Julia</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Melanie J. I.</forenames></author><author><keyname>Bernardi</keyname><forenames>Lucas</forenames></author><author><keyname>Davis</keyname><forenames>Chad</forenames></author><author><keyname>Kovacek</keyname><forenames>Ivan</forenames></author><author><keyname>Einarsen</keyname><forenames>Mats Stafseng</forenames></author><author><keyname>Kamps</keyname><forenames>Jaap</forenames></author><author><keyname>Tuzhilin</keyname><forenames>Alexander</forenames></author><author><keyname>Hiemstra</keyname><forenames>Djoerd</forenames></author></authors><title>Where to Go on Your Next Trip? Optimizing Travel Destinations Based on
  User Preferences</title><categories>cs.IR</categories><comments>6 pages, 2 figures in SIGIR 2015, SIRIP Symposium on IR in Practice</comments><doi>10.1145/2766462.2776777</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation based on user preferences is a common task for e-commerce
websites. New recommendation algorithms are often evaluated by offline
comparison to baseline algorithms such as recommending random or the most
popular items. Here, we investigate how these algorithms themselves perform and
compare to the operational production system in large scale online experiments
in a real-world application. Specifically, we focus on recommending travel
destinations at Booking.com, a major online travel site, to users searching for
their preferred vacation activities. To build ranking models we use
multi-criteria rating data provided by previous users after their stay at a
destination. We implement three methods and compare them to the current
baseline in Booking.com: random, most popular, and Naive Bayes. Our general
conclusion is that, in an online A/B test with live users, our Naive-Bayes
based ranker increased user engagement significantly over the current online
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00925</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00925</id><created>2015-06-02</created><authors><author><keyname>Miranda</keyname><forenames>Catarina Runa</forenames></author><author><keyname>Mendes</keyname><forenames>Pedro</forenames></author><author><keyname>Coelho</keyname><forenames>Pedro</forenames></author><author><keyname>Alvarez</keyname><forenames>Xenxo</forenames></author><author><keyname>Freitas</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Dias</keyname><forenames>Miguel Sales</forenames></author><author><keyname>Orvalho</keyname><forenames>Ver&#xf3;nica Costa</forenames></author></authors><title>Facial Expressions Tracking and Recognition: Database Protocols for
  Systems Validation and Evaluation</title><categories>cs.CV</categories><comments>10 pages, 6 images, Computers &amp; Graphics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Each human face is unique. It has its own shape, topology, and distinguishing
features. As such, developing and testing facial tracking systems are
challenging tasks. The existing face recognition and tracking algorithms in
Computer Vision mainly specify concrete situations according to particular
goals and applications, requiring validation methodologies with data that fits
their purposes. However, a database that covers all possible variations of
external and factors does not exist, increasing researchers' work in acquiring
their own data or compiling groups of databases.
  To address this shortcoming, we propose a methodology for facial data
acquisition through definition of fundamental variables, such as subject
characteristics, acquisition hardware, and performance parameters. Following
this methodology, we also propose two protocols that allow the capturing of
facial behaviors under uncontrolled and real-life situations. As validation, we
executed both protocols which lead to creation of two sample databases: FdMiee
(Facial database with Multi input, expressions, and environments) and FACIA
(Facial Multimodal database driven by emotional induced acting).
  Using different types of hardware, FdMiee captures facial information under
environmental and facial behaviors variations. FACIA is an extension of FdMiee
introducing a pipeline to acquire additional facial behaviors and speech using
an emotion-acting method. Therefore, this work eases the creation of adaptable
database according to algorithm's requirements and applications, leading to
simplified validation and testing processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00930</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00930</id><created>2015-06-02</created><authors><author><keyname>Marques</keyname><forenames>Diogo</forenames></author><author><keyname>Carri&#xe7;o</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Guerreiro</keyname><forenames>Tiago</forenames></author></authors><title>Assessing Inconspicuous Smartphone Authentication for Blind People</title><categories>cs.HC cs.CR</categories><comments>4 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As people store more personal data in their smartphones, the consequences of
having it stolen or lost become an increasing concern. A typical
counter-measure to avoid this risk is to set up a secret code that has to be
entered to unlock the device after a period of inactivity. However, for blind
users, PINs and passwords are inadequate, since entry 1) consumes a non-trivial
amount of time, e.g. using screen readers, 2) is susceptible to observation,
where nearby people can see or hear the secret code, and 3) might collide with
social norms, e.g. disrupting personal interactions. Tap-based authentication
methods have been presented and allow unlocking to be performed in a short time
and support naturally occurring inconspicuous behavior (e.g. concealing the
device inside a jacket) by being usable with a single hand. This paper presents
a study with blind users (N = 16) where an authentication method based on tap
phrases is evaluated. Results showed the method to be usable and to support the
desired inconspicuity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00934</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00934</id><created>2015-06-02</created><updated>2015-10-08</updated><authors><author><keyname>Wang</keyname><forenames>Xiaozhe</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Data-Driven Diagnostics of Mechanism and Source of Sustained
  Oscillations</title><categories>cs.SY</categories><comments>The paper has been accepted by IEEE Transactions on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sustained oscillations observed in power systems can damage equipment,
degrade the power quality and increase the risks of cascading blackouts. There
are several mechanisms that can give rise to oscillations, each requiring
different countermeasure to suppress or eliminate the oscillation. This work
develops mathematical framework for analysis of sustained oscillations and
identifies statistical signatures of each mechanism, based on which a novel
oscillation diagnosis method is developed via real-time processing of phasor
measurement units (PMUs) data. Case studies show that the proposed method can
accurately identify the exact mechanism for sustained oscillation, and
meanwhile provide insightful information to locate the oscillation sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00935</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00935</id><created>2015-06-02</created><authors><author><keyname>Vanchinathan</keyname><forenames>Hastagiri P.</forenames></author><author><keyname>Marfurt</keyname><forenames>Andreas</forenames></author><author><keyname>Robelin</keyname><forenames>Charles-Antoine</forenames></author><author><keyname>Kossmann</keyname><forenames>Donald</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Discovering Valuable Items from Massive Data</title><categories>cs.LG cs.AI cs.IT math.IT</categories><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose there is a large collection of items, each with an associated cost
and an inherent utility that is revealed only once we commit to selecting it.
Given a budget on the cumulative cost of the selected items, how can we pick a
subset of maximal value? This task generalizes several important problems such
as multi-arm bandits, active search and the knapsack problem. We present an
algorithm, GP-Select, which utilizes prior knowledge about similarity be- tween
items, expressed as a kernel function. GP-Select uses Gaussian process
prediction to balance exploration (estimating the unknown value of items) and
exploitation (selecting items of high value). We extend GP-Select to be able to
discover sets that simultaneously have high utility and are diverse. Our
preference for diversity can be specified as an arbitrary monotone submodular
function that quantifies the diminishing returns obtained when selecting
similar items. Furthermore, we exploit the structure of the model updates to
achieve an order of magnitude (up to 40X) speedup in our experiments without
resorting to approximations. We provide strong guarantees on the performance of
GP-Select and apply it to three real-world case studies of industrial
relevance: (1) Refreshing a repository of prices in a Global Distribution
System for the travel industry, (2) Identifying diverse, binding-affine
peptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scale
recommender system by recommending items to users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00944</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00944</id><created>2015-06-02</created><authors><author><keyname>da Silva</keyname><forenames>Maise Dantas</forenames></author><author><keyname>Protti</keyname><forenames>F&#xe1;bio</forenames></author><author><keyname>Szwarcfiter</keyname><forenames>Jayme Luiz</forenames></author></authors><title>Parameterized mixed cluster editing via modular decomposition</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a natural generalization of the well-known
problems Cluster Editing and Bicluster Editing, whose parameterized versions
have been intensively investigated in the recent literature. The generalized
problem, called Mixed Cluster Editing or ${\cal M}$-Cluster Editing, is
formulated as follows. Let ${\cal M}$ be a family of graphs. Given a graph $G$
and a nonnegative integer $k$, transform $G$, through a sequence of at most $k$
edge editions, into a target graph $G'$ with the following property: $G'$ is a
vertex-disjoint union of graphs $G_1, G_2, \ldots$ such that every $G_i$ is a
member of ${\cal M}$. The graph $G'$ is called a mixed cluster graph or ${\cal
M}$-cluster graph. Let ${\cal K}$ denote the family of complete graphs, ${\cal
KL}$ the family of complete $l$-partite graphs ($l \geq 2$), and $\L={\cal K}
\cup {\cal KL}$. In this work we focus on the case ${\cal M} = {\cal L}$. Using
modular decomposition techniques previously applied to Cluster/Bicluster
Editing, we present a linear-time algorithm to construct a problem kernel for
the parameterized version of ${\cal L}$-Cluster Editing. Keywords: bicluster
graphs, cluster graphs, edge edition problems, edge modification problems,
fixed-parameter tractability, NP-complete problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00950</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00950</id><created>2015-06-02</created><authors><author><keyname>Vadai</keyname><forenames>Gergely</forenames></author><author><keyname>Mingesz</keyname><forenames>Robert</forenames></author><author><keyname>Gingl</keyname><forenames>Zoltan</forenames></author></authors><title>Generalized Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange
  system using arbitrary resistors</title><categories>cs.CR</categories><journal-ref>Scientific Reports 5, 13653 (2015)</journal-ref><doi>10.1038/srep13653</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange system has been
introduced as a simple, very low cost and efficient classical physical
alternative to quantum key distribution systems. The ideal system uses only a
few electronic components - identical resistor pairs, switches and
interconnecting wires - to guarantee perfectly protected data transmission. We
show that a generalized KLJN system can provide unconditional security even if
it is used with significantly less limitations. The more universal conditions
ease practical realizations considerably and support more robust protection
against attacks. Our theoretical results are confirmed by numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00963</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00963</id><created>2015-06-02</created><updated>2015-07-30</updated><authors><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author><author><keyname>Lalmas</keyname><forenames>Mounia</forenames></author><author><keyname>Baeza-Yates</keyname><forenames>Ricardo</forenames></author></authors><title>Finding Intermediary Topics Between People of Opposing Views: A Case
  Study</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages. Presented at the International Workshop on Social
  Personalisation &amp; Search, SPS2015 (co-located with SIGIR 2015)</comments><acm-class>H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In micro-blogging platforms, people can connect with others and have
conversations on a wide variety of topics. However, because of homophily and
selective exposure, users tend to connect with like-minded people and only read
agreeable information. Motivated by this scenario, in this paper we study the
diversity of intermediary topics, which are latent topics estimated from user
generated content. These topics can be used as features in recommender systems
aimed at introducing people of diverse political viewpoints. We conducted a
case study on Twitter, considering the debate about a sensitive issue in Chile,
where we quantified homophilic behavior in terms of political discussion and
then we evaluated the diversity of intermediary topics in terms of political
stances of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00967</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00967</id><created>2015-06-02</created><updated>2016-01-06</updated><authors><author><keyname>Cant&#xf3;n</keyname><forenames>A.</forenames></author><author><keyname>Fern&#xe1;ndez-Jambrina</keyname><forenames>L.</forenames></author><author><keyname>Mar&#xed;a</keyname><forenames>M. E. Rosado</forenames></author><author><keyname>V&#xe1;zquez-Gallo</keyname><forenames>M. J.</forenames></author></authors><title>Geometric elements and classification of quadrics in rational B\'ezier
  form</title><categories>cs.GR math.NA</categories><comments>14 pages, 12 figures. Minor changes from previous version. To appear
  in Journal of Computational and Applied Mathematics</comments><msc-class>65D17</msc-class><journal-ref>Journal of Computational and Applied Mathematics 300, 400-419
  (2016)</journal-ref><doi>10.1016/j.cam.2016.01.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we classify and derive closed formulas for geometric elements
of quadrics in rational B\'ezier triangular form (such as the center, the conic
at infinity, the vertex and the axis of paraboloids and the principal planes),
using just the control vertices and the weights for the quadric patch. The
results are extended also to quadric tensor product patches. Our results rely
on using techniques from projective algebraic geometry to find suitable
bilinear forms for the quadric in a coordinate-free fashion, considering a
pencil of quadrics that are tangent to the given quadric along a conic. Most of
the information about the quadric is encoded in one coefficient, involving the
weights of the patch, which allows us to tell apart oval from ruled quadrics.
This coefficient is also relevant to determine the affine type of the quadric.
Spheres and quadrics of revolution are characterised within this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00976</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00976</id><created>2015-06-02</created><updated>2015-09-03</updated><authors><author><keyname>Marti</keyname><forenames>Gautier</forenames></author><author><keyname>Very</keyname><forenames>Philippe</forenames></author><author><keyname>Donnat</keyname><forenames>Philippe</forenames></author></authors><title>Toward a generic representation of random variables for machine learning</title><categories>cs.LG stat.ML</categories><comments>submitted to Pattern Recognition Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a pre-processing and a distance which improve the
performance of machine learning algorithms working on independent and
identically distributed stochastic processes. We introduce a novel
non-parametric approach to represent random variables which splits apart
dependency and distribution without losing any information. We also propound an
associated metric leveraging this representation and its statistical estimate.
Besides experiments on synthetic datasets, the benefits of our contribution is
illustrated through the example of clustering financial time series, for
instance prices from the credit default swaps market. Results are available on
the website www.datagrapple.com and an IPython Notebook tutorial is available
at www.datagrapple.com/Tech for reproducible research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00981</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00981</id><created>2015-06-02</created><updated>2016-02-18</updated><authors><author><keyname>Dupuis</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Swiveled R\'enyi entropies</title><categories>quant-ph cond-mat.stat-mech cs.IT hep-th math-ph math.IT math.MP</categories><comments>v4: 33 pages, published in Quantum Information Processing</comments><doi>10.1007/s11128-015-1211-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces &quot;swiveled Renyi entropies&quot; as an alternative to the
Renyi entropic quantities put forward in [Berta et al., Phys. Rev. A 91, 022333
(2015)]. What distinguishes the swiveled Renyi entropies from the prior
proposal of Berta et al. is that there is an extra degree of freedom: an
optimization over unitary rotations with respect to particular fixed bases
(swivels). A consequence of this extra degree of freedom is that the swiveled
Renyi entropies are ordered, which is an important property of the Renyi family
of entropies. The swiveled Renyi entropies are however generally discontinuous
at $\alpha=1$ and do not converge to the von Neumann entropy-based measures in
the limit as $\alpha\rightarrow1$, instead bounding them from above and below.
Particular variants reduce to known Renyi entropies, such as the Renyi relative
entropy or the sandwiched Renyi relative entropy, but also lead to ordered
Renyi conditional mutual informations and ordered Renyi generalizations of a
relative entropy difference. Refinements of entropy inequalities such as
monotonicity of quantum relative entropy and strong subadditivity follow as a
consequence of the aforementioned properties of the swiveled Renyi entropies.
Due to the lack of convergence at $\alpha=1$, it is unclear whether the
swiveled Renyi entropies would be useful in one-shot information theory, so
that the present contribution represents partial progress toward this goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00982</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00982</id><created>2015-06-02</created><updated>2015-07-03</updated><authors><author><keyname>Bacci</keyname><forenames>Giacomo</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author></authors><title>Game Theory for Signal Processing in Networks</title><categories>cs.GT cs.IT cs.NI math.IT</categories><comments>44 pages, 14 figures, To appear in IEEE Signal Processing Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this tutorial, the basics of game theory are introduced along with an
overview of its most recent and emerging applications in signal processing. One
of the main features of this contribution is to gather in a single paper some
fundamental game-theoretic notions and tools which, over the past few years,
have become widely spread over a large number of papers. In particular, both
strategic-form and coalition-form games are described in details while the key
connections and differences between them are outlined. Moreover, a particular
attention is also devoted to clarify the connections between strategic-form
games and distributed optimization and learning algorithms. Beyond an
introduction to the basic concepts and main solution approaches, several
carefully designed examples are provided to allow a better understanding of how
to apply the described tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00986</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00986</id><created>2015-06-02</created><updated>2015-11-19</updated><authors><author><keyname>Karampourniotis</keyname><forenames>Panagiotis D.</forenames></author><author><keyname>Sreenivasan</keyname><forenames>Sameet</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author><author><keyname>Korniss</keyname><forenames>Gyorgy</forenames></author></authors><title>The Impact of Heterogeneous Thresholds on Social Contagion with Multiple
  Initiators</title><categories>physics.soc-ph cs.SI</categories><comments>Final version, reflecting changes in response to referees' comments</comments><journal-ref>PLoS ONE 10(11): e0143020 (2015)</journal-ref><doi>10.1371/journal.pone.0143020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The threshold model is a simple but classic model of contagion spreading in
complex social systems. To capture the complex nature of social influencing we
investigate numerically and analytically the transition in the behavior of
threshold-limited cascades in the presence of multiple initiators as the
distribution of thresholds is varied between the two extreme cases of identical
thresholds and a uniform distribution. We accomplish this by employing a
truncated normal distribution of the nodes' thresholds and observe a
non-monotonic change in the cascade size as we vary the standard deviation.
Further, for a sufficiently large spread in the threshold distribution, the
tipping-point behavior of the social influencing process disappears and is
replaced by a smooth crossover governed by the size of initiator set. We
demonstrate that for a given size of the initiator set, there is a specific
variance of the threshold distribution for which an opinion spreads optimally.
Furthermore, in the case of synthetic graphs we show that the spread
asymptotically becomes independent of the system size, and that global cascades
can arise just by the addition of a single node to the initiator set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00990</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00990</id><created>2015-06-02</created><updated>2016-01-28</updated><authors><author><keyname>Lu</keyname><forenames>Yao</forenames></author></authors><title>Unsupervised Learning on Neural Network Outputs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The outputs of a trained neural network contain much richer information than
just an one-hot classifier. For example, a neural network might give an image
of a dog the probability of one in a million of being a cat but it is still
much larger than the probability of being a car. To reveal the hidden structure
in them, we apply two unsupervised learning algorithms, PCA and ICA, to the
outputs of a deep Convolutional Neural Network trained on the ImageNet of 1000
classes. The PCA/ICA embedding of the object classes reveals their visual
similarity and the PCA/ICA components can be interpreted as common visual
features shared by similar object classes. For an application, we proposed a
new zero-shot learning method, in which the visual features learned by PCA/ICA
are employed. Our zero-shot learning method achieves the state-of-the-art
results on the ImageNet of over 20000 classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00998</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00998</id><created>2015-06-02</created><authors><author><keyname>North</keyname><forenames>Phillip</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author></authors><title>One-Bit Compressive Sensing with Partial Support</title><categories>math.NA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Compressive Sensing framework maintains relevance even when the available
measurements are subject to extreme quantization, as is exemplified by the
so-called one-bit compressed sensing framework which aims to recover a signal
from measurements reduced to only their sign-bit. In applications, it is often
the case that we have some knowledge of the structure of the signal beforehand,
and thus would like to leverage it to attain more accurate and efficient
recovery. This work explores avenues for incorporating such partial-support
information into the one-bit setting. Experimental results demonstrate that
newly proposed methods of this work yield improved signal recovery even for
varying levels of accuracy in the prior information. This work is thus the
first to provide recovery mechanisms that efficiently use prior signal
information in the one-bit reconstruction setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.00999</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.00999</id><created>2015-06-02</created><authors><author><keyname>Garcia-Duran</keyname><forenames>Alberto</forenames></author><author><keyname>Bordes</keyname><forenames>Antoine</forenames></author><author><keyname>Usunier</keyname><forenames>Nicolas</forenames></author><author><keyname>Grandvalet</keyname><forenames>Yves</forenames></author></authors><title>Combining Two And Three-Way Embeddings Models for Link Prediction in
  Knowledge Bases</title><categories>cs.AI cs.CL cs.LG</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tackles the problem of endogenous link prediction for Knowledge
Base completion. Knowledge Bases can be represented as directed graphs whose
nodes correspond to entities and edges to relationships. Previous attempts
either consist of powerful systems with high capacity to model complex
connectivity patterns, which unfortunately usually end up overfitting on rare
relationships, or in approaches that trade capacity for simplicity in order to
fairly model all relationships, frequent or not. In this paper, we propose
Tatec a happy medium obtained by complementing a high-capacity model with a
simpler one, both pre-trained separately and then combined. We present several
variants of this model with different kinds of regularization and combination
strategies and show that this approach outperforms existing methods on
different types of relationships by achieving state-of-the-art results on four
benchmarks of the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01001</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01001</id><created>2015-06-02</created><authors><author><keyname>Meredith</keyname><forenames>Lucius Gregory</forenames></author></authors><title>Linear Types Can Change the Blockchain</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We give an interpretation of full classical linear logic, and linear proofs
in terms of operations on the blockchain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01002</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01002</id><created>2015-06-02</created><updated>2015-06-03</updated><authors><author><keyname>Hedges</keyname><forenames>Jules</forenames></author><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author><author><keyname>Sprits</keyname><forenames>Evguenia</forenames></author><author><keyname>Winschel</keyname><forenames>Viktor</forenames></author><author><keyname>Zahn</keyname><forenames>Philipp</forenames></author></authors><title>Higher-Order Game Theory</title><categories>cs.GT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1409.7411</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In applied game theory the motivation of players is a key element. It is
encoded in the payoffs of the game form and often based on utility functions.
But there are cases were formal descriptions in the form of a utility function
do not exist. In this paper we introduce a representation of games where
players' goals are modeled based on so-called higher-order functions. Our
representation provides a general and powerful way to mathematically summarize
players' intentions. In our framework utility functions as well as preference
relations are special cases to describe players' goals. We show that in
higher-order functions formal descriptions of players may still exist where
utility functions do not using a classical example, a variant of Keynes' beauty
contest. We also show that equilibrium conditions based on Nash can be easily
adapted to our framework. Lastly, this framework serves as a stepping stone to
powerful tools from computer science that can be usefully applied to economic
game theory in the future such as computational and computability aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01003</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01003</id><created>2015-06-02</created><updated>2015-06-03</updated><authors><author><keyname>Hedges</keyname><forenames>Jules</forenames></author><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author><author><keyname>Sprits</keyname><forenames>Evguenia</forenames></author><author><keyname>Winschel</keyname><forenames>Viktor</forenames></author><author><keyname>Zahn</keyname><forenames>Philipp</forenames></author></authors><title>Higher-Order Decision Theory</title><categories>cs.GT</categories><comments>arXiv admin note: text overlap with arXiv:1409.7411</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical decision theory models behaviour in terms of utility maximisation
where utilities represent rational preference relations over outcomes. However,
empirical evidence and theoretical considerations suggest that we need to go
beyond this framework. We propose to represent goals by higher-order functions
or operators that take other functions as arguments where the max and argmax
operators are special cases. Our higher-order functions take a context function
as their argument where a context represents a process from actions to
outcomes. By that we can define goals being dependent on the actions and the
process in addition to outcomes only. This formulation generalises outcome
based preferences to context-dependent goals. We show how to uniformly
represent within our higher-order framework classical utility maximisation but
also various other extensions that have been debated in economics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01047</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01047</id><created>2015-06-02</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Pitarokoilis</keyname><forenames>Antonios</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Distributed Massive MIMO in Cellular Networks: Impact of Imperfect
  Hardware and Number of Oscillators</title><categories>cs.IT math.IT</categories><comments>First published in the Proceedings of the 23rd European Signal
  Processing Conference (EUSIPCO-2015) in 2015, published by EURASIP. 5 pages,
  3, figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed massive multiple-input multiple-output (MIMO) combines the array
gain of coherent MIMO processing with the proximity gains of distributed
antenna setups. In this paper, we analyze how transceiver hardware impairments
affect the downlink with maximum ratio transmission. We derive closed-form
spectral efficiencies expressions and study their asymptotic behavior as the
number of the antennas increases. We prove a scaling law on the hardware
quality, which reveals that massive MIMO is resilient to additive distortions,
while multiplicative phase noise is a limiting factor. It is also better to
have separate oscillators at each antenna than one per BS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01048</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01048</id><created>2015-06-02</created><authors><author><keyname>Speidel</keyname><forenames>Ulrich</forenames></author><author><keyname>Cocker</keyname><forenames>'Etuate</forenames></author><author><keyname>Vingelmann</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Heide</keyname><forenames>Janus</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author></authors><title>Can network coding bridge the digital divide in the Pacific?</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 3 figures, conference (Netcod2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional TCP performance is significantly impaired under long latency
and/or constrained bandwidth. While small Pacific Island states on satellite
links experience this in the extreme, small populations and remoteness often
rule out submarine fibre connections and their communities struggle to reap the
benefits of the Internet. Network-coded TCP (TCP/NC) can increase goodput under
high latency and packet loss, but has not been used to tunnel conventional TCP
and UDP across satellite links before. We report on a feasibility study aimed
at determining expected goodput gain across such TCP/NC tunnels into island
targets on geostationary and medium earth orbit satellite links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01051</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01051</id><created>2015-06-02</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Energy-Efficient Future Wireless Networks: A Marriage between Massive
  MIMO and Small Cells</title><categories>cs.IT math.IT</categories><comments>Published at IEEE Workshop on Signal Processing Advances in Wireless
  Communications (SPAWC 2015), 5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How would a cellular network designed for high energy efficiency look like?
To answer this fundamental question, we model cellular networks using
stochastic geometry and optimize the energy efficiency with respect to the
density of base stations, the number of antennas and users per cell, the
transmit power levels, and the pilot reuse. The highest efficiency is neither
achieved by a pure small-cell approach, nor by a pure massive MIMO solution.
Interestingly, it is the combination of these approaches that provides the
highest energy efficiency; small cells contributes by reducing the propagation
losses while massive MIMO enables multiplexing of users with controlled
interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01054</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01054</id><created>2015-06-02</created><updated>2015-06-25</updated><authors><author><keyname>Ruelens</keyname><forenames>Frederik</forenames></author><author><keyname>Iacovella</keyname><forenames>Sandro</forenames></author><author><keyname>Claessens</keyname><forenames>Bert J.</forenames></author><author><keyname>Belmans</keyname><forenames>Ronnie</forenames></author></authors><title>Learning Agent for a Heat-Pump Thermostat With a Set-Back Strategy Using
  Model-Free Reinforcement Learning</title><categories>cs.SY</categories><comments>Submitted to Energies - MDPI.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional control paradigm for a heat pump with a less efficient
auxiliary heating element is to keep its temperature set point constant during
the day. This constant temperature set point ensures that the heat pump
operates in its more efficient heat-pump mode and minimizes the risk of
activating the less efficient auxiliary heating element. As an alternative to a
constant set-point strategy, this paper proposes a learning agent for a
thermostat with a set-back strategy. This set-back strategy relaxes the
set-point temperature during convenient moments, e.g. when the occupants are
not at home. Finding an optimal set-back strategy requires solving a sequential
decision-making process under uncertainty, which presents two challenges. A
first challenge is that for most residential buildings a description of the
thermal characteristics of the building is unavailable and challenging to
obtain. A second challenge is that the relevant information on the state, i.e.
the building envelope, cannot be measured by the learning agent. In order to
overcome these two challenges, our paper proposes an auto-encoder coupled with
a batch reinforcement learning technique. The proposed approach is validated
for two building types with different thermal characteristics for heating in
the winter and cooling in the summer. The simulation results indicate that the
proposed learning agent can reduce the energy consumption by 4-9% during 100
winter days and by 9-11% during 80 summer days compared to the conventional
constant set-point strategy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01055</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01055</id><created>2015-05-20</created><authors><author><keyname>Blais</keyname><forenames>Eric</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author><author><keyname>Wan</keyname><forenames>Andrew</forenames></author></authors><title>An inequality for the Fourier spectrum of parity decision trees</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new bound on the sum of the linear Fourier coefficients of a
Boolean function in terms of its parity decision tree complexity. This result
generalizes an inequality of O'Donnell and Servedio for regular decision trees.
We use this bound to obtain the first non-trivial lower bound on the parity
decision tree complexity of the recursive majority function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01056</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01056</id><created>2015-06-02</created><authors><author><keyname>Lin</keyname><forenames>Peng</forenames></author></authors><title>Performing Bayesian Risk Aggregation using Discrete Approximation
  Algorithms with Graph Factorization</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risk aggregation is a popular method used to estimate the sum of a collection
of financial assets or events, where each asset or event is modelled as a
random variable. Applications, in the financial services industry, include
insurance, operational risk, stress testing, and sensitivity analysis, but the
problem is widely encountered in many other application domains. This thesis
has contributed two algorithms to perform Bayesian risk aggregation when model
exhibit hybrid dependency and high dimensional inter-dependency. The first
algorithm operates on a subset of the general problem, with an emphasis on
convolution problems, in the presence of continuous and discrete variables (so
called hybrid models) and the second algorithm offer a universal method for
general purpose inference over much wider classes of Bayesian Network models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01057</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01057</id><created>2015-06-02</created><updated>2015-06-05</updated><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Luong</keyname><forenames>Minh-Thang</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author></authors><title>A Hierarchical Neural Autoencoder for Paragraphs and Documents</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language generation of coherent long texts like paragraphs or longer
documents is a challenging problem for recurrent networks models. In this
paper, we explore an important step toward this generation task: training an
LSTM (Long-short term memory) auto-encoder to preserve and reconstruct
multi-sentence paragraphs. We introduce an LSTM model that hierarchically
builds an embedding for a paragraph from embeddings for sentences and words,
then decodes this embedding to reconstruct the original paragraph. We evaluate
the reconstructed paragraph using standard metrics like ROUGE and Entity Grid,
showing that neural models are able to encode texts in a way that preserve
syntactic, semantic, and discourse coherence. While only a first step toward
generating coherent text units from neural models, our work has the potential
to significantly impact natural language generation and
summarization\footnote{Code for the three models described in this paper can be
found at www.stanford.edu/~jiweil/ .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01058</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01058</id><created>2015-06-02</created><authors><author><keyname>Partov</keyname><forenames>Bahar</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>Utility Fair Rate Allocation in LTE/802.11 Networks</title><categories>cs.NI</categories><comments>13 pages, submitted to IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider proportional fair rate allocation in a heterogeneous network with
a mix of LTE and 802.11 cells which supports multipath and multihomed operation
(simultaneous connection of a user device to multiple LTE BSs and 802.11 APs).
We show that the utility fair optimisation problem is non-convex but that a
global optimum can be found by solving a sequence of convex optimisations in a
distributed fashion. The result is a principled approach to offload from LTE to
802.11 and for exploiting LTE/802.11 path diversity to meet user traffic
demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01060</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01060</id><created>2015-06-02</created><updated>2015-10-19</updated><authors><author><keyname>Zhou</keyname><forenames>Nan</forenames></author><author><keyname>Xu</keyname><forenames>Yangyang</forenames></author><author><keyname>Cheng</keyname><forenames>Hong</forenames></author><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Pedrycz</keyname><forenames>Witold</forenames></author></authors><title>Global and Local Structure Preserving Sparse Subspace Learning: An
  Iterative Approach to Unsupervised Feature Selection</title><categories>cs.LG</categories><comments>32 page, 6 figures and 60 references</comments><msc-class>05-04</msc-class><acm-class>E.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As we aim at alleviating the curse of high-dimensionality, subspace learning
is becoming more popular. Existing approaches use either information about
global or local structure of the data, and few studies simultaneously focus on
global and local structures as the both of them contain important information.
In this paper, we propose a global and local structure preserving sparse
subspace learning (GLoSS) model for unsupervised feature selection. The model
can simultaneously realize feature selection and subspace learning. In
addition, we develop a greedy algorithm to establish a generic combinatorial
model, and an iterative strategy based on an accelerated block coordinate
descent is used to solve the GLoSS problem. We also provide whole iterate
sequence convergence analysis of the proposed iterative algorithm. Extensive
experiments are conducted on real-world datasets to show the superiority of the
proposed approach over several state-of-the-art unsupervised feature selection
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01062</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01062</id><created>2015-06-02</created><authors><author><keyname>Ipeirotis</keyname><forenames>Panagiotis G.</forenames></author><author><keyname>Gabrilovich</keyname><forenames>Evgeniy</forenames></author></authors><title>Quizz: Targeted crowdsourcing with a billion (potential) users</title><categories>cs.AI cs.HC</categories><comments>WWW '14 Proceedings of the 23rd international conference on World
  Wide Web. 11 pages</comments><doi>10.1145/2566486.2567988</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Quizz, a gamified crowdsourcing system that simultaneously
assesses the knowledge of users and acquires new knowledge from them. Quizz
operates by asking users to complete short quizzes on specific topics; as a
user answers the quiz questions, Quizz estimates the user's competence. To
acquire new knowledge, Quizz also incorporates questions for which we do not
have a known answer; the answers given by competent users provide useful
signals for selecting the correct answers for these questions. Quizz actively
tries to identify knowledgeable users on the Internet by running advertising
campaigns, effectively leveraging the targeting capabilities of existing,
publicly available, ad placement services. Quizz quantifies the contributions
of the users using information theory and sends feedback to the
advertisingsystem about each user. The feedback allows the ad targeting
mechanism to further optimize ad placement.
  Our experiments, which involve over ten thousand users, confirm that we can
crowdsource knowledge curation for niche and specialized topics, as the
advertising network can automatically identify users with the desired expertise
and interest in the given topic. We present controlled experiments that examine
the effect of various incentive mechanisms, highlighting the need for having
short-term rewards as goals, which incentivize the users to contribute.
Finally, our cost-quality analysis indicates that the cost of our approach is
below that of hiring workers through paid-crowdsourcing platforms, while
offering the additional advantage of giving access to billions of potential
users all over the planet, and being able to reach users with specialized
expertise that is not typically available through existing labor marketplaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01066</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01066</id><created>2015-06-02</created><updated>2016-01-08</updated><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Chen</keyname><forenames>Xinlei</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author></authors><title>Visualizing and Understanding Neural Models in NLP</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While neural networks have been successfully applied to many NLP tasks the
resulting vector-based models are very difficult to interpret. For example it's
not clear how they achieve {\em compositionality}, building sentence meaning
from the meanings of words and phrases. In this paper we describe four
strategies for visualizing compositionality in neural models for NLP, inspired
by similar work in computer vision. We first plot unit values to visualize
compositionality of negation, intensification, and concessive clauses, allow us
to see well-known markedness asymmetries in negation. We then introduce three
simple and straightforward methods for visualizing a unit's {\em salience}, the
amount it contributes to the final composed meaning: (1) gradient
back-propagation, (2) the variance of a token from the average word node, (3)
LSTM-style gates that measure information flow. We test our methods on
sentiment using simple recurrent nets and LSTMs. Our general-purpose methods
may have wide applications for understanding compositionality and other
semantic properties of deep networks , and also shed light on why LSTMs
outperform simple recurrent nets,
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01069</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01069</id><created>2015-06-02</created><updated>2015-06-08</updated><authors><author><keyname>Wu</keyname><forenames>Xinyu</forenames></author><author><keyname>Saxena</keyname><forenames>Vishal</forenames></author><author><keyname>Zhu</keyname><forenames>Kehan</forenames></author></authors><title>A CMOS Spiking Neuron for Dense Memristor-Synapse Connectivity for
  Brain-Inspired Computing</title><categories>cs.NE cs.ET</categories><comments>This is a preprint of an article accepted for publication in
  International Joint Conference on Neural Networks (IJCNN) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuromorphic systems that densely integrate CMOS spiking neurons and
nano-scale memristor synapses open a new avenue of brain-inspired computing.
Existing silicon neurons have molded neural biophysical dynamics but are
incompatible with memristor synapses, or used extra training circuitry thus
eliminating much of the density advantages gained by using memristors, or were
energy inefficient. Here we describe a novel CMOS spiking leaky
integrate-and-fire neuron circuit. Building on a reconfigurable architecture
with a single opamp, the described neuron accommodates a large number of
memristor synapses, and enables online spike timing dependent plasticity (STDP)
learning with optimized power consumption. Simulation results of an 180nm CMOS
design showed 97% power efficiency metric when realizing STDP learning in
10,000 memristor synapses with a nominal 1M{\Omega} memristance, and only
13{\mu}A current consumption when integrating input spikes. Therefore, the
described CMOS neuron contributes a generalized building block for large-scale
brain-inspired neuromorphic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01070</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01070</id><created>2015-06-02</created><updated>2015-11-24</updated><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author></authors><title>Do Multi-Sense Embeddings Improve Natural Language Understanding?</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning a distinct representation for each sense of an ambiguous word could
lead to more powerful and fine-grained models of vector-space representations.
Yet while `multi-sense' methods have been proposed and tested on artificial
word-similarity tasks, we don't know if they improve real natural language
understanding tasks. In this paper we introduce a multi-sense embedding model
based on Chinese Restaurant Processes that achieves state of the art
performance on matching human word similarity judgments, and propose a
pipelined architecture for incorporating multi-sense embeddings into language
understanding.
  We then test the performance of our model on part-of-speech tagging, named
entity recognition, sentiment analysis, semantic relation identification and
semantic relatedness, controlling for embedding dimensionality. We find that
multi-sense embeddings do improve performance on some tasks (part-of-speech
tagging, semantic relation identification, semantic relatedness) but not on
others (named entity recognition, various forms of sentiment analysis). We
discuss how these differences may be caused by the different role of word sense
information in each of the tasks. The results highlight the importance of
testing embedding models in real applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01071</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01071</id><created>2015-06-02</created><updated>2015-06-16</updated><authors><author><keyname>Buzmakov</keyname><forenames>Aleksey</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames></author><author><keyname>Napoli</keyname><forenames>Amedeo</forenames></author></authors><title>Fast Generation of Best Interval Patterns for Nonmonotonic Constraints</title><categories>cs.AI cs.DS</categories><comments>18 pages; 2 figures; 2 tables; 1 algorithm; PKDD 2015 Conference
  Scientific Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In pattern mining, the main challenge is the exponential explosion of the set
of patterns. Typically, to solve this problem, a constraint for pattern
selection is introduced. One of the first constraints proposed in pattern
mining is support (frequency) of a pattern in a dataset. Frequency is an
anti-monotonic function, i.e., given an infrequent pattern, all its
superpatterns are not frequent. However, many other constraints for pattern
selection are not (anti-)monotonic, which makes it difficult to generate
patterns satisfying these constraints. In this paper we introduce the notion of
projection-antimonotonicity and $\theta$-$\Sigma\o\phi\iota\alpha$ algorithm
that allows efficient generation of the best patterns for some nonmonotonic
constraints. In this paper we consider stability and $\Delta$-measure, which
are nonmonotonic constraints, and apply them to interval tuple datasets. In the
experiments, we compute best interval tuple patterns w.r.t. these measures and
show the advantage of our approach over postfiltering approaches.
  KEYWORDS: Pattern mining, nonmonotonic constraints, interval tuple data
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01072</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01072</id><created>2015-06-02</created><updated>2015-06-08</updated><authors><author><keyname>Wu</keyname><forenames>Xinyu</forenames></author><author><keyname>Saxena</keyname><forenames>Vishal</forenames></author><author><keyname>Zhu</keyname><forenames>Kehan</forenames></author></authors><title>Homogeneous Spiking Neuromorphic System for Real-World Pattern
  Recognition</title><categories>cs.NE cs.AI cs.CV cs.ET</categories><comments>This is a preprint of an article accepted for publication in IEEE
  Journal on Emerging and Selected Topics in Circuits and Systems, vol 5, no.
  2, June 2015</comments><journal-ref>IEEE Journal on Emerging and Selected Topics in Circuits and
  Systems, vol 5, no. 2, June 2015</journal-ref><doi>10.1109/JETCAS.2015.2433552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A neuromorphic chip that combines CMOS analog spiking neurons and memristive
synapses offers a promising solution to brain-inspired computing, as it can
provide massive neural network parallelism and density. Previous hybrid analog
CMOS-memristor approaches required extensive CMOS circuitry for training, and
thus eliminated most of the density advantages gained by the adoption of
memristor synapses. Further, they used different waveforms for pre and
post-synaptic spikes that added undesirable circuit overhead. Here we describe
a hardware architecture that can feature a large number of memristor synapses
to learn real-world patterns. We present a versatile CMOS neuron that combines
integrate-and-fire behavior, drives passive memristors and implements
competitive learning in a compact circuit module, and enables in-situ
plasticity in the memristor synapses. We demonstrate handwritten-digits
recognition using the proposed architecture using transistor-level circuit
simulations. As the described neuromorphic architecture is homogeneous, it
realizes a fundamental building block for large-scale energy-efficient
brain-inspired silicon chips that could lead to next-generation cognitive
computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01075</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01075</id><created>2015-06-02</created><authors><author><keyname>Fok</keyname><forenames>C. -L.</forenames></author><author><keyname>Johnson</keyname><forenames>G.</forenames></author><author><keyname>Yamokoski</keyname><forenames>J. D.</forenames></author><author><keyname>Mok</keyname><forenames>A.</forenames></author><author><keyname>Sentis</keyname><forenames>L.</forenames></author></authors><title>ControlIt! - A Software Framework for Whole-Body Operational Space
  Control</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whole Body Operational Space Control (WBOSC) is a pioneering algorithm in the
field of human-centered Whole-Body Control (WBC). It enables floating-base
highly-redundant robots to achieve unified motion/force control of one or more
operational space objectives while adhering to physical constraints. Limited
studies exist on the software architecture and APIs that enable WBOSC to
perform and be integrated into a larger system. In this paper we address this
by presenting ControlIt!, a new open-source software framework for WBOSC.
Unlike previous implementations, ControlIt! is multi-threaded to increase servo
frequencies on standard PC hardware. A new parameter binding mechanism enables
tight integration between ControlIt! and external processes via an extensible
set of transport protocols. To support a new robot, only two plugins and a URDF
model needs to be provided --- the rest of ControlIt! remains unchanged. New
WBC primitives can be added by writing a Task or Constraint plugin.
ControlIt!'s capabilities are demonstrated on Dreamer, a 16-DOF torque
controlled humanoid upper body robot containing both series elastic and
co-actuated joints, and using it to perform a product disassembly task. Using
this testbed, we show that ControlIt! can achieve average servo latencies of
about 0.5ms when configured with two Cartesian position tasks, two orientation
tasks, and a lower priority posture task. This is significantly higher than the
5ms that was achieved using UTA-WBC, the prototype implementation of WBOSC that
is both application and platform-specific. Variations in the product's position
is handled by updating the goal of the Cartesian position task. ControlIt!'s
source code is released under an LGPL license and we hope it will be adopted
and maintained by the WBC community for the long term as a platform for WBC
development and integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01077</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01077</id><created>2015-06-02</created><authors><author><keyname>de Oliveira</keyname><forenames>Saullo Haniell Galv&#xe3;o</forenames></author><author><keyname>Veroneze</keyname><forenames>Rosana</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando Jos&#xe9;</forenames></author></authors><title>On bicluster aggregation and its benefits for enumerative solutions</title><categories>cs.LG</categories><comments>15 pages, will be published by Springer Verlag in the LNAI Series in
  the book Advances in Data Mining</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Biclustering involves the simultaneous clustering of objects and their
attributes, thus defining local two-way clustering models. Recently, efficient
algorithms were conceived to enumerate all biclusters in real-valued datasets.
In this case, the solution composes a complete set of maximal and non-redundant
biclusters. However, the ability to enumerate biclusters revealed a challenging
scenario: in noisy datasets, each true bicluster may become highly fragmented
and with a high degree of overlapping. It prevents a direct analysis of the
obtained results. To revert the fragmentation, we propose here two approaches
for properly aggregating the whole set of enumerated biclusters: one based on
single linkage and the other directly exploring the rate of overlapping. Both
proposals were compared with each other and with the actual state-of-the-art in
several experiments, and they not only significantly reduced the number of
biclusters but also consistently increased the quality of the solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01082</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01082</id><created>2015-06-02</created><updated>2015-08-24</updated><authors><author><keyname>Comin</keyname><forenames>Carlo</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>An Improved Upper Bound on Maximal Clique Listing via Rectangular Fast
  Matrix Multiplication</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first output-sensitive algorithm for the Maximal Clique Listing problem
was given by Tsukiyama et.al. in 1977. As any algorithm falling within the
Reverse Search paradigm, it performs a DFS visit of a directed tree (the
RS-tree) having the objects to be listed (i.e. maximal cliques) as its nodes.
In a recursive implementation, the RS-tree corresponds to the recursion tree of
the algorithm. The time delay is given by the cost of generating the next child
of a node, and Tsukiyama showed it is $O(mn)$. In 2004, Makino and Uno
sharpened the time delay to $O(n^{\omega})$ by generating all the children of a
node in one single shot performed by computing a \emph{square} fast matrix
multiplication. In this paper, we further improve the asymptotics for the
exploration of the same RS-tree by grouping the offsprings' computation even
further. Our idea is to rely on rectangular fast matrix multiplication in order
to compute all children of $n^2$ nodes in one shot. According to the current
upper bounds on fast matrix multiplication, with this the time delay improves
from $O(n^{2.3728639})$ to $O(n^{2.093362})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01083</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01083</id><created>2015-06-02</created><authors><author><keyname>Brody</keyname><forenames>Joshua</forenames></author><author><keyname>Sanchez</keyname><forenames>Mario</forenames></author></authors><title>Dependent Random Graphs and Multiparty Pointer Jumping</title><categories>cs.CC</categories><comments>18 pages</comments><msc-class>05C80</msc-class><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate a study of a relaxed version of the standard Erdos-Renyi random
graph model, where each edge may depend on a few other edges. We call such
graphs &quot;dependent random graphs&quot;. Our main result in this direction is a
thorough understanding of the clique number of dependent random graphs. We also
obtain bounds for the chromatic number. Surprisingly, many of the standard
properties of random graphs also hold in this relaxed setting. We show that
with high probability, a dependent random graph will contain a clique of size
$\frac{(1-o(1))\log n}{\log(1/p)}$, and the chromatic number will be at most
$\frac{n \log(1/1-p)}{\log n}$.
  As an application and second main result, we give a new communication
protocol for the k-player Multiparty Pointer Jumping (MPJ_k) problem in the
number-on-the-forehead (NOF) model. Multiparty Pointer Jumping is one of the
canonical NOF communication problems, yet even for three players, its
communication complexity is not well understood. Our protocol for MPJ_3 costs
$O(\frac{n\log\log n}{\log n})$ communication, improving on a bound of Brody
and Chakrabarti [BC08]. We extend our protocol to the non-Boolean pointer
jumping problem $\widehat{MPJ}_k$, achieving an upper bound which is o(n) for
any $k &gt;= 4$ players. This is the first o(n) bound for $\widehat{MPJ}_k$ and
improves on a bound of Damm, Jukna, and Sgall [DJS98] which has stood for
almost twenty years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01085</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01085</id><created>2015-06-02</created><updated>2015-10-26</updated><authors><author><keyname>Zhu</keyname><forenames>Zhijie</forenames></author><author><keyname>Schmerling</keyname><forenames>Edward</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>A Convex Optimization Approach to Smooth Trajectories for Motion
  Planning with Car-Like Robots</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent past, several sampling-based algorithms have been proposed to
compute trajectories that are collision-free and dynamically-feasible. However,
the outputs of such algorithms are notoriously jagged. In this paper, by
focusing on robots with car-like dynamics, we present a fast and simple
heuristic algorithm, named Convex Elastic Smoothing (CES) algorithm, for
trajectory smoothing and speed optimization. The CES algorithm is inspired by
earlier work on elastic band planning and iteratively performs shape and speed
optimization. The key feature of the algorithm is that both optimization
problems can be solved via convex programming, making CES particularly fast. A
range of numerical experiments show that the CES algorithm returns high-quality
solutions in a matter of a few hundreds of milliseconds and hence appears
amenable to a real-time implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01092</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01092</id><created>2015-06-02</created><authors><author><keyname>Kim</keyname><forenames>Saehoon</forenames></author><author><keyname>Choi</keyname><forenames>Seungjin</forenames></author></authors><title>Bilinear Random Projections for Locality-Sensitive Binary Codes</title><categories>cs.CV</categories><comments>11 pages, 23 figures, CVPR-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locality-sensitive hashing (LSH) is a popular data-independent indexing
method for approximate similarity search, where random projections followed by
quantization hash the points from the database so as to ensure that the
probability of collision is much higher for objects that are close to each
other than for those that are far apart. Most of high-dimensional visual
descriptors for images exhibit a natural matrix structure. When visual
descriptors are represented by high-dimensional feature vectors and long binary
codes are assigned, a random projection matrix requires expensive complexities
in both space and time. In this paper we analyze a bilinear random projection
method where feature matrices are transformed to binary codes by two smaller
random projection matrices. We base our theoretical analysis on extending
Raginsky and Lazebnik's result where random Fourier features are composed with
random binary quantizers to form locality sensitive binary codes. To this end,
we answer the following two questions: (1) whether a bilinear random projection
also yields similarity-preserving binary codes; (2) whether a bilinear random
projection yields performance gain or loss, compared to a large linear
projection. Regarding the first question, we present upper and lower bounds on
the expected Hamming distance between binary codes produced by bilinear random
projections. In regards to the second question, we analyze the upper and lower
bounds on covariance between two bits of binary codes, showing that the
correlation between two bits is small. Numerical experiments on MNIST and
Flickr45K datasets confirm the validity of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01094</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01094</id><created>2015-06-02</created><updated>2015-08-19</updated><authors><author><keyname>Guu</keyname><forenames>Kelvin</forenames></author><author><keyname>Miller</keyname><forenames>John</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Traversing Knowledge Graphs in Vector Space</title><categories>cs.CL cs.AI cs.DB stat.ML</categories><comments>2015 Conference on Empirical Methods on Natural Language Processing
  (EMNLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path queries on a knowledge graph can be used to answer compositional
questions such as &quot;What languages are spoken by people living in Lisbon?&quot;.
However, knowledge graphs often have missing facts (edges) which disrupts path
queries. Recent models for knowledge base completion impute missing facts by
embedding knowledge graphs in vector spaces. We show that these models can be
recursively applied to answer path queries, but that they suffer from cascading
errors. This motivates a new &quot;compositional&quot; training objective, which
dramatically improves all models' ability to answer path queries, in some cases
more than doubling accuracy. On a standard knowledge base completion task, we
also demonstrate that compositional training acts as a novel form of structural
regularization, reliably improving performance across all base models (reducing
errors by up to 43%) and achieving new state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01105</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01105</id><created>2015-06-02</created><authors><author><keyname>Kamath</keyname><forenames>Sudeep</forenames></author><author><keyname>Anantharam</keyname><forenames>Venkat</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author><author><keyname>Wang</keyname><forenames>Chih-Chun</forenames></author></authors><title>The two-unicast problem</title><categories>cs.IT math.IT</categories><comments>23 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the communication capacity of wireline networks for a two-unicast
traffic pattern. The network has two sources and two destinations with each
source communicating a message to its own destination, subject to the capacity
constraints on the directed edges of the network. We propose a simple outer
bound for the problem that we call the Generalized Network Sharing (GNS) bound.
We show this bound is the tightest edge-cut bound for two-unicast networks and
is tight in several bottleneck cases, though it is not tight in general. We
also show that the problem of computing the GNS bound is NP-complete. Finally,
we show that despite its seeming simplicity, the two-unicast problem is as hard
as the most general network coding problem. As a consequence, linear coding is
insufficient to achieve capacity for general two-unicast networks, and
non-Shannon inequalities are necessary for characterizing capacity of general
two-unicast networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01106</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01106</id><created>2015-06-02</created><authors><author><keyname>Tian</keyname><forenames>Wenhong</forenames></author><author><keyname>Xu</keyname><forenames>Minxian</forenames></author><author><keyname>Chen</keyname><forenames>Aiguo</forenames></author><author><keyname>Li</keyname><forenames>Guozhong</forenames></author><author><keyname>Wang</keyname><forenames>Xinyang</forenames></author><author><keyname>Chen</keyname><forenames>Yu</forenames></author></authors><title>Open-Source Simulators for Cloud Computing: Comparative Study and
  Challenging Issues</title><categories>cs.DC</categories><comments>15 pages, 11 figures, accepted for publication in Journal: Simulation
  Modelling Practice and Theory</comments><doi>10.1016/j.simpat.2015.06.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource scheduling in infrastructure as a service (IaaS) is one of the keys
for large-scale Cloud applications. Extensive research on all issues in real
environment is extremely difficult because it requires developers to consider
network infrastructure and the environment, which may be beyond the control. In
addition, the network conditions cannot be controlled or predicted. Performance
evaluations of workload models and Cloud provisioning algorithms in a
repeatable manner under different configurations are difficult. Therefore,
simulators are developed. To understand and apply better the state-of-the-art
of cloud computing simulators, and to improve them, we study four known
open-source simulators. They are compared in terms of architecture, modeling
elements, simulation process, performance metrics and scalability in
performance. Finally, a few challenging issues as future research trends are
outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01110</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01110</id><created>2015-06-02</created><authors><author><keyname>Cao</keyname><forenames>Bokai</forenames></author><author><keyname>Zhou</keyname><forenames>Hucheng</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Multi-view Machines</title><categories>cs.LG stat.ML</categories><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a learning task, data can usually be collected from different sources or
be represented from multiple views. For example, laboratory results from
different medical examinations are available for disease diagnosis, and each of
them can only reflect the health state of a person from a particular
aspect/view. Therefore, different views provide complementary information for
learning tasks. An effective integration of the multi-view information is
expected to facilitate the learning performance. In this paper, we propose a
general predictor, named multi-view machines (MVMs), that can effectively
include all the possible interactions between features from multiple views. A
joint factorization is embedded for the full-order interaction parameters which
allows parameter estimation under sparsity. Moreover, MVMs can work in
conjunction with different loss functions for a variety of machine learning
tasks. A stochastic gradient descent method is presented to learn the MVM
model. We further illustrate the advantages of MVMs through comparison with
other methods for multi-view classification, including support vector machines
(SVMs), support tensor machines (STMs) and factorization machines (FMs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01113</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01113</id><created>2015-06-02</created><updated>2015-07-20</updated><authors><author><keyname>Miranda</keyname><forenames>Conrado Silva</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando Jos&#xe9;</forenames></author></authors><title>Multi-Objective Optimization for Self-Adjusting Weighted Gradient in
  Machine Learning Tasks</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of the focus in machine learning research is placed in creating new
architectures and optimization methods, but the overall loss function is seldom
questioned. This paper interprets machine learning from a multi-objective
optimization perspective, showing the limitations of the default linear
combination of loss functions over a data set and introducing the hypervolume
indicator as an alternative. It is shown that the gradient of the hypervolume
is defined by a self-adjusting weighted mean of the individual loss gradients,
making it similar to the gradient of a weighted mean loss but without requiring
the weights to be defined a priori. This enables an inner boosting-like
behavior, where the current model is used to automatically place higher weights
on samples with higher losses but without requiring the use of multiple models.
Results on a denoising autoencoder show that the new formulation is able to
achieve better mean loss than the direct optimization of the mean loss,
providing evidence to the conjecture that self-adjusting the weights creates a
smoother loss surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01115</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01115</id><created>2015-06-03</created><authors><author><keyname>Iliopoulos</keyname><forenames>Alexandros-Stavros</forenames></author><author><keyname>Liu</keyname><forenames>Tiancheng</forenames></author><author><keyname>Sun</keyname><forenames>Xiaobai</forenames></author></authors><title>Hyperspectral Image Classification and Clutter Detection via Multiple
  Structural Embeddings and Dimension Reductions</title><categories>cs.CV</categories><comments>13 pages, 6 figures (30 images), submitted to International
  Conference on Computer Vision (ICCV) 2015</comments><acm-class>I.4.6; I.4.8; I.4.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new and effective approach for Hyperspectral Image (HSI)
classification and clutter detection, overcoming a few long-standing challenges
presented by HSI data characteristics. Residing in a high-dimensional spectral
attribute space, HSI data samples are known to be strongly correlated in their
spectral signatures, exhibit nonlinear structure due to several physical laws,
and contain uncertainty and noise from multiple sources. In the presented
approach, we generate an adaptive, structurally enriched representation
environment, and employ the locally linear embedding (LLE) in it. There are two
structure layers external to LLE. One is feature space embedding: the HSI data
attributes are embedded into a discriminatory feature space where
spatio-spectral coherence and distinctive structures are distilled and
exploited to mitigate various difficulties encountered in the native
hyperspectral attribute space. The other structure layer encloses the ranges of
algorithmic parameters for LLE and feature embedding, and supports a
multiplexing and integrating scheme for contending with multi-source
uncertainty. Experiments on two commonly used HSI datasets with a small number
of learning samples have rendered remarkably high-accuracy classification
results, as well as distinctive maps of detected clutter regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01125</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01125</id><created>2015-06-03</created><authors><author><keyname>Zhong</keyname><forenames>Zhun</forenames></author><author><keyname>Li</keyname><forenames>Zongmin</forenames></author><author><keyname>Li</keyname><forenames>Runlin</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoxia</forenames></author></authors><title>Unsupervised domain adaption dictionary learning for visual recognition</title><categories>cs.CV</categories><comments>5 pages, 3 figures, ICIP 2015</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Over the last years, dictionary learning method has been extensively applied
to deal with various computer vision recognition applications, and produced
state-of-the-art results. However, when the data instances of a target domain
have a different distribution than that of a source domain, the dictionary
learning method may fail to perform well. In this paper, we address the
cross-domain visual recognition problem and propose a simple but effective
unsupervised domain adaption approach, where labeled data are only from source
domain. In order to bring the original data in source and target domain into
the same distribution, the proposed method forcing nearest coupled data between
source and target domain to have identical sparse representations while jointly
learning dictionaries for each domain, where the learned dictionaries can
reconstruct original data in source and target domain respectively. So that
sparse representations of original data can be used to perform visual
recognition tasks. We demonstrate the effectiveness of our approach on standard
datasets. Our method performs on par or better than competitive
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01136</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01136</id><created>2015-06-03</created><authors><author><keyname>Yu</keyname><forenames>Lu</forenames></author><author><keyname>Liu</keyname><forenames>Hai</forenames></author><author><keyname>Leung</keyname><forenames>Yiu-Wing</forenames></author><author><keyname>Chu</keyname><forenames>Xiaowen</forenames></author><author><keyname>Lin</keyname><forenames>Zhiyong</forenames></author></authors><title>Efficient Channel-Hopping Rendezvous Algorithm Based on Available
  Channel Set</title><categories>cs.NI</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio networks, rendezvous is a fundamental operation by which
two cognitive users establish a communication link on a commonly-available
channel for communications. Some existing rendezvous algorithms can guarantee
that rendezvous can be completed within finite time and they generate
channel-hopping (CH) sequences based on the whole channel set. However, some
channels may not be available (e.g., they are being used by the licensed users)
and these existing algorithms would randomly replace the unavailable channels
in the CH sequence. This random replacement is not effective, especially when
the number of unavailable channels is large. In this paper, we design a new
rendezvous algorithm that attempts rendezvous on the available channels only
for faster rendezvous. This new algorithm, called Interleaved Sequences based
on Available Channel set (ISAC), constructs an odd sub-sequence and an even
sub-sequence and interleaves these two sub-sequences to compose a CH sequence.
We prove that ISAC provides guaranteed rendezvous (i.e., rendezvous can be
achieved within finite time). We derive the upper bound on the maximum
time-to-rendezvous (MTTR) to be O(m) (m is not greater than Q) under the
symmetric model and O(mn) (n is not greater than Q) under the asymmetric model,
where m and n are the number of available channels of two users and Q is the
total number of channels (i.e., all potentially available channels). We conduct
extensive computer simulation to demonstrate that ISAC gives significantly
smaller MTTR than the existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01138</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01138</id><created>2015-06-03</created><authors><author><keyname>Mukherjee</keyname><forenames>Sudarshan</forenames></author><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author></authors><title>CFO Estimation for Massive MU-MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-complexity carrier frequency offset (CFO) estimation/compensation in
massive multi-user (MU) multiple-input multiple-output (MIMO) systems is a
challenging problem. The existing CFO estimation/compensation strategies for
conventional small MIMO systems experience tremendous increase in complexity
with increasing number of the user terminals (UTs), $K$ and the number of base
station (BS) antennas, $M$ (i.e. in massive MIMO regime). In this paper, we
devise a \textit{low-complexity} algorithm for CFO estimation using the pilots
received at the BS during a special uplink slot. The total per-channel use
complexity of the proposed algorithm increases only linearly with increasing
$M$ and is independent of $K$. Analytical expression is derived for the mean
square error (MSE) of the proposed CFO estimator. Further analysis reveals that
the MSE of the proposed estimator decreases with increasing $M$ (fixed $K$) and
increasing $K$ (fixed $M$ and CFO pilot sequence length $\gg K$). We also
derive an achievable information sum-rate expression of the time-reversal
maximal-ratio-combining (TR-MRC) detector in the presence of residual CFO
errors, resulting from the proposed CFO estimation/compensation algorithm.
Analysis of this information rate expression reveals that an
$\mathcal{O}(\sqrt{M})$ array gain is achievable. This new result is
interesting since the best possible array gain under an ideal no CFO condition
is also known to be $\mathcal{O}(\sqrt{M})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01144</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01144</id><created>2015-06-03</created><updated>2015-10-08</updated><authors><author><keyname>Wu</keyname><forenames>Qi</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Dick</keyname><forenames>Anthony</forenames></author></authors><title>What value high level concepts in vision to language problems?</title><categories>cs.CV</categories><comments>17 pages. Fixed typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of the recent progress in Vision-to-Language (V2L) problems has been
achieved through a combination of Convolutional Neural Networks (CNNs) and
Recurrent Neural Networks (RNNs). This approach does not explicitly represent
high-level semantic concepts, but rather seeks to progress directly from image
features to text. We propose here a method of incorporating high-level concepts
into the very successful CNN-RNN approach, and show that it achieves a
significant improvement on the state-of-the-art performance in both image
captioning and visual question answering. We also show that the same mechanism
can be used to introduce external semantic information and that doing so
further improves performance. In doing so we provide an analysis of the value
of high level semantic information in V2L problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01150</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01150</id><created>2015-06-03</created><authors><author><keyname>Yu</keyname><forenames>Mingchao</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author></authors><title>The Benefit of Limited Feedback to Generation-Based Random Linear
  Network Coding in Wireless Broadcast</title><categories>cs.IT math.IT</categories><comments>Original work submitted to IEEE Wireless Comm. Letters. 10 pages, 5
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random linear network coding (RLNC) is asymptotically throughput optimal in
the wireless broadcast of a block of packets from a sender to a set of
receivers, but suffers from heavy computational load and packet decoding delay.
To mitigate this problem while maintaining good throughput, we partition the
packet block into disjoint generations after broadcasting the packets uncoded
once and collecting one round of feedback about receivers' packet reception
state. We prove the NP-hardness of the optimal partitioning problem by using a
hypergraph coloring approach, and develop an efficient heuristic algorithm for
its solution. Simulations show that our algorithm outperforms existing
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01151</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01151</id><created>2015-06-03</created><authors><author><keyname>Aubry</keyname><forenames>Mathieu</forenames></author><author><keyname>Russell</keyname><forenames>Bryan</forenames></author></authors><title>Understanding deep features with computer-generated imagery</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an approach for analyzing the variation of features generated by
convolutional neural networks (CNNs) with respect to scene factors that occur
in natural images. Such factors may include object style, 3D viewpoint, color,
and scene lighting configuration. Our approach analyzes CNN feature responses
corresponding to different scene factors by controlling for them via rendering
using a large database of 3D CAD models. The rendered images are presented to a
trained CNN and responses for different layers are studied with respect to the
input scene factors. We perform a decomposition of the responses based on
knowledge of the input scene factors and analyze the resulting components. In
particular, we quantify their relative importance in the CNN responses and
visualize them using principal component analysis. We show qualitative and
quantitative results of our study on three CNNs trained on large image
datasets: AlexNet, Places, and Oxford VGG. We observe important differences
across the networks and CNN layers for different scene factors and object
categories. Finally, we demonstrate that our analysis based on
computer-generated imagery translates to the network representation of natural
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01153</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01153</id><created>2015-06-03</created><authors><author><keyname>de Croon</keyname><forenames>G. C. H. E.</forenames></author></authors><title>Distance estimation with efference copies and optical flow maneuvers: a
  stability-based strategy</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The visual cue of optical flow plays a major role in the navigation of flying
insects, and is increasingly studied for use by small flying robots as well. A
major problem is that successful optical flow control seems to require distance
estimates, while optical flow is known to provide only the ratio of velocity to
distance. In this article, a novel, stability-based strategy is proposed to
estimate distances with monocular optical flow and knowledge of the control
inputs (efference copies). It is shown analytically that given a fixed control
gain, the stability of a constant divergence control loop only depends on the
distance to the approached surface. At close distances, the control loop first
starts to exhibit self-induced oscillations, eventually leading to instability.
The proposed stability-based strategy for estimating distances has two major
attractive characteristics. First, self-induced oscillations are easy for the
robot to detect and are hardly influenced by wind. Second, the distance can be
estimated during a zero divergence maneuver, i.e., around hover. The
stability-based strategy is implemented and tested both in simulation and with
a Parrot AR drone 2.0. It is shown that it can be used to: (1) trigger a final
approach response during a constant divergence landing with fixed gain, (2)
estimate the distance in hover, and (3) estimate distances during an entire
landing if the robot uses adaptive gain control to continuously stay on the
'edge of oscillation'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01154</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01154</id><created>2015-06-03</created><authors><author><keyname>Yu</keyname><forenames>Mingchao</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author></authors><title>Performance Characterization and Transmission Schemes for Instantly
  Decodable Network Coding in Wireless Broadcast</title><categories>cs.IT math.IT</categories><comments>Original work submitted to IEEE Trans. Comm. 30 pages</comments><doi>10.1186/s13634-015-0279-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider broadcasting a block of packets to multiple wireless receivers
under random packet erasures using instantly decodable network coding (IDNC).
The sender first broadcasts each packet uncoded once, then generates coded
packets according to receivers' feedback about their missing packets. We focus
on strict IDNC (S-IDNC), where each coded packet includes at most one missing
packet of every receiver. But we will also compare it with general IDNC
(G-IDNC), where this condition is relaxed. We characterize two fundamental
performance limits of S-IDNC: 1) the number of transmissions to complete the
broadcast, and 2) the average delay for a receiver to decode a packet. We
derive a closed-form expression for the expected minimum number of
transmissions in terms of the number of packets and receivers and the erasure
probability. We prove that it is NP-hard to minimize the decoding delay of
S-IDNC. We also derive achievable upper bounds on the above two performance
limits. We show that G-IDNC can outperform S-IDNC %in terms of the number of
transmissions without packet erasures, but not necessarily with packet
erasures. Next, we design optimal and heuristic S-IDNC transmission schemes and
coding algorithms with full/intermittent receiver feedback. We present
simulation results to corroborate the developed theory and compare with
existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01159</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01159</id><created>2015-06-03</created><updated>2015-09-10</updated><authors><author><keyname>Ray</keyname><forenames>Baishakhi</forenames></author><author><keyname>Hellendoorn</keyname><forenames>Vincent</forenames></author><author><keyname>Godhane</keyname><forenames>Saheel</forenames></author><author><keyname>Tu</keyname><forenames>Zhaopeng</forenames></author><author><keyname>Bacchelli</keyname><forenames>Alberto</forenames></author><author><keyname>Devanbu</keyname><forenames>Premkumar</forenames></author></authors><title>On the &quot;Naturalness&quot; of Buggy Code</title><categories>cs.SE</categories><comments>12 pages</comments><msc-class>68N30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real software, the kind working programmers produce by the kLOC to solve
real-world problems, tends to be &quot;natural&quot;, like speech or natural language; it
tends to be highly repetitive and predictable. Researchers have captured this
naturalness of software through statistical models and used them to good effect
in suggestion engines, porting tools, coding standards checkers, and idiom
miners. This suggests that code that appears improbable, or surprising, to a
good statistical language model is &quot;unnatural&quot; in some sense, and thus possibly
suspicious. In this paper, we investigate this hypothesis. We consider a large
corpus of bug fix commits (ca.~8,296), from 10 different Java projects, and we
focus on its language statistics, evaluating the naturalness of buggy code and
the corresponding fixes. We find that code with bugs tends to be more entropic
(i.e., unnatural), becoming less so as bugs are fixed. Focusing on highly
entropic lines is similar in cost-effectiveness to some well-known static bug
finders (PMD, FindBugs) and ordering warnings from these bug finders using an
entropy measure improves the cost-effectiveness of inspecting code implicated
in warnings. This suggests that entropy may be a valid language-independent and
simple way to complement the effectiveness of PMD or FindBugs, and that
search-based bug-fixing methods may benefit from using entropy both for
fault-localization and searching for fixes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01163</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01163</id><created>2015-06-03</created><authors><author><keyname>Liao</keyname><forenames>Yi-Hsiu</forenames></author><author><keyname>Lee</keyname><forenames>Hung-Yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>Towards Structured Deep Neural Network for Automatic Speech Recognition</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose the Structured Deep Neural Network (Structured DNN)
as a structured and deep learning algorithm, learning to find the best
structured object (such as a label sequence) given a structured input (such as
a vector sequence) by globally considering the mapping relationships between
the structure rather than item by item.
  When automatic speech recognition is viewed as a special case of such a
structured learning problem, where we have the acoustic vector sequence as the
input and the phoneme label sequence as the output, it becomes possible to
comprehensively learned utterance by utterance as a whole, rather than frame by
frame.
  Structured Support Vector Machine (structured SVM) was proposed to perform
ASR with structured learning previously, but limited by the linear nature of
SVM. Here we propose structured DNN to use nonlinear transformations in
multi-layers as a structured and deep learning algorithm. It was shown to beat
structured SVM in preliminary experiments on TIMIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01165</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01165</id><created>2015-06-03</created><authors><author><keyname>Le</keyname><forenames>Thanh Manh</forenames></author><author><keyname>Van</keyname><forenames>Thanh The</forenames></author></authors><title>Image Retrieval System Base on EMD Similarity Measure and S-Tree</title><categories>cs.CV cs.IR</categories><comments>14 pages, 3 figures, Appendix</comments><acm-class>H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper approaches the binary signature for each image based on the
percentage of the pixels in each color images, at the same time the paper
builds a similar measure between images based on EMD (Earth Mover's Distance).
Besides, the paper proceeded to create the S-tree based on the similar measure
EMD to store the image's binary signatures to quickly query image signature
data. From there, the paper build an image retrieval algorithm and CBIR
(Content-Based Image Retrieval) based on a similar measure EMD and S-tree.
Based on this theory, the paper proceeded to build application and experimental
assessment of the process of querying image on the database system which have
over 10,000 images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01166</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01166</id><created>2015-06-03</created><authors><author><keyname>Van</keyname><forenames>Thanh The</forenames></author><author><keyname>Le</keyname><forenames>Thanh Manh</forenames></author></authors><title>Color Image Retrieval Using Fuzzy Measure Hamming and S-Tree</title><categories>cs.CV</categories><comments>9 pages, 4 figures</comments><acm-class>H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter approaches the image retrieval system on the base of the colors
of image. It creates fuzzy signature to describe the color of image on color
space HSV and builds fuzzy Hamming distance (FHD) to evaluate the similarity
between the images. In order to reduce the storage space and speed up the
search of similar images, it aims to create S-tree to store fuzzy signature
relies on FHD and builds image retrieval algorithm on S-tree. Then, it provides
the content-based image retrieval (CBIR) and an image retrieval method on FHD
and S-tree. Last but not least, based on this theory, it also presents an
application and experimental assessment of the process of querying similar
image on the database system over 10,000 images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01170</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01170</id><created>2015-06-03</created><authors><author><keyname>Albrecht</keyname><forenames>Stefano V.</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Subramanian</forenames></author></authors><title>A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc
  Coordination in Multiagent Systems</title><categories>cs.GT cs.AI cs.MA</categories><comments>Technical Report, The University of Edinburgh, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ad hoc coordination problem is to design an autonomous agent which is
able to achieve optimal flexibility and efficiency in a multiagent system with
no mechanisms for prior coordination. We conceptualise this problem formally
using a game-theoretic model, called the stochastic Bayesian game, in which the
behaviour of a player is determined by its private information, or type. Based
on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc
Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in
a planning procedure to find optimal actions in the sense of Bellman optimal
control. We evaluate HBA in a multiagent logistics domain called level-based
foraging, showing that it achieves higher flexibility and efficiency than
several alternative algorithms. We also report on a human-machine experiment at
a public science exhibition in which the human participants played repeated
Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative
algorithms, showing that HBA achieves equal efficiency and a significantly
higher welfare and winning rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01171</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01171</id><created>2015-06-03</created><authors><author><keyname>ElSayed</keyname><forenames>Ahmed G. M.</forenames></author><author><keyname>Salama</keyname><forenames>Ahmed S.</forenames></author><author><keyname>El-Ghazali</keyname><forenames>Alaa El-Din M.</forenames></author></authors><title>A Hybrid Model for Enhancing Lexical Statistical Machine Translation
  (SMT)</title><categories>cs.CL</categories><comments>9 pages, 10 figures</comments><journal-ref>International Journal of Computer Science Issues Volume 12, Issue
  2, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interest in statistical machine translation systems increases currently
due to political and social events in the world. A proposed Statistical Machine
Translation (SMT) based model that can be used to translate a sentence from the
source Language (English) to the target language (Arabic) automatically through
efficiently incorporating different statistical and Natural Language Processing
(NLP) models such as language model, alignment model, phrase based model,
reordering model, and translation model. These models are combined to enhance
the performance of statistical machine translation (SMT). Many implementation
tools have been used in this work such as Moses, Gizaa++, IRSTLM, KenLM, and
BLEU. Based on the implementation, evaluation of this model, and comparing the
generated translation with other implemented machine translation systems like
Google Translate, it was proved that this proposed model has enhanced the
results of the statistical machine translation, and forms a reliable and
efficient model in this field of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01186</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01186</id><created>2015-06-03</created><updated>2015-06-05</updated><authors><author><keyname>Smith</keyname><forenames>Leslie N.</forenames></author></authors><title>No More Pesky Learning Rate Guessing Games</title><categories>cs.CV cs.LG cs.NE</categories><comments>11 pages, 12 figures, 4 tables</comments><report-no>US Naval Research Laboratory Technical Report (NRL/MR/5510--15-9631)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the learning rate is the most important hyper-parameter to
tune for training deep convolutional neural networks (i.e., a &quot;guessing game&quot;).
This report describes a new method for setting the learning rate, named
cyclical learning rates, that eliminates the need to experimentally find the
best values and schedule for the learning rates. Instead of setting the
learning rate to fixed values, this method lets the learning rate cyclically
vary within reasonable boundary values. This report shows that training with
cyclical learning rates achieves near optimal classification accuracy without
tuning and often in many fewer iterations. This report also describes a simple
way to estimate &quot;reasonable bounds&quot; - by linearly increasing the learning rate
in one training run of the network for only a few epochs. In addition, cyclical
learning rates are demonstrated on training with the CIFAR-10 dataset and the
AlexNet and GoogLeNet architectures on the ImageNet dataset. These methods are
practical tools for everyone who trains convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01188</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01188</id><created>2015-06-03</created><authors><author><keyname>Lei</keyname><forenames>Siyu</forenames></author><author><keyname>Maniu</keyname><forenames>Silviu</forenames></author><author><keyname>Mo</keyname><forenames>Luyi</forenames></author><author><keyname>Cheng</keyname><forenames>Reynold</forenames></author><author><keyname>Senellart</keyname><forenames>Pierre</forenames></author></authors><title>Online Influence Maximization (Extended Version)</title><categories>cs.SI cs.DB physics.soc-ph</categories><comments>13 pages. To appear in KDD 2015. Extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks are commonly used for marketing purposes. For example, free
samples of a product can be given to a few influential social network users (or
&quot;seed nodes&quot;), with the hope that they will convince their friends to buy it.
One way to formalize marketers' objective is through influence maximization (or
IM), whose goal is to find the best seed nodes to activate under a fixed
budget, so that the number of people who get influenced in the end is
maximized. Recent solutions to IM rely on the influence probability that a user
influences another one. However, this probability information may be
unavailable or incomplete. In this paper, we study IM in the absence of
complete information on influence probability. We call this problem Online
Influence Maximization (OIM) since we learn influence probabilities at the same
time we run influence campaigns. To solve OIM, we propose a multiple-trial
approach, where (1) some seed nodes are selected based on existing influence
information; (2) an influence campaign is started with these seed nodes; and
(3) users' feedback is used to update influence information. We adopt the
Explore-Exploit strategy, which can select seed nodes using either the current
influence probability estimation (exploit), or the confidence bound on the
estimation (explore). Any existing IM algorithm can be used in this framework.
We also develop an incremental algorithm that can significantly reduce the
overhead of handling users' feedback information. Our experiments show that our
solution is more effective than traditional IM methods on the partial
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01190</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01190</id><created>2015-06-03</created><authors><author><keyname>Brener</keyname><forenames>A. M.</forenames></author><author><keyname>Musabekova</keyname><forenames>L. M.</forenames></author></authors><title>Modeling of through-reactors with allowance of Large-Scale Effect on
  Heat and Mass Efficiency of Chemical Apparatuses</title><categories>cs.CE</categories><comments>17 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals also with a problem of gas absorption accompanied by an
instantaneous, irreversible reaction in the liquid layer. The well-known
methods for calculating such processes are based usually on the certain
amendments to solutions, which are obtained disregarding the chemical reaction.
Unlike the known work (1. D. Baetens, R. Van Keer, L.H. Hosten. Gas-liquid
reaction: absorption accompanied by an instantaneous, irreversible reaction//
Moving Boundaries IV, Southampton, Boston, 1997) the approach we used takes
into account the influence of reaction resulting product on the arising and
velocity of a moving reaction plane. The known results in the theory of
chemical apparatuses scaling are devoted to apparatuses with non-regular
packings mainly. However how the phases distribution over the regular packings
of chemical columns effects the heat and mass efficiency is studied lesser.
This paper deals with the methods of simulation the scaling effects applying to
chemical towers with regular packings of various types. The models for
describing the influence of initial liquid and gas distribution in chemical
columns with regular packing on the mass transfer efficiency have been
submitted. The sufficiently simple methods for evaluating the influence of
large-scale factor on the efficiency of mass transfer have been obtained. These
methods are suitable for use in engineering calculation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01192</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01192</id><created>2015-06-03</created><authors><author><keyname>Tseng</keyname><forenames>Bo-Hsiang</forenames></author><author><keyname>Lee</keyname><forenames>Hung-Yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-Shan</forenames></author></authors><title>Personalizing a Universal Recurrent Neural Network Language Model with
  User Characteristic Features by Crowdsouring over Social Networks</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the popularity of mobile devices, personalized speech recognizer becomes
more realizable today and highly attractive. Each mobile device is primarily
used by a single user, so it's possible to have a personalized recognizer well
matching to the characteristics of individual user. Although acoustic model
personalization has been investigated for decades, much less work have been
reported on personalizing language model, probably because of the difficulties
in collecting enough personalized corpora. Previous work used the corpora
collected from social networks to solve the problem, but constructing a
personalized model for each user is troublesome. In this paper, we propose a
universal recurrent neural network language model with user characteristic
features, so all users share the same model, except each with different user
characteristic features. These user characteristic features can be obtained by
crowdsouring over social networks, which include huge quantity of texts posted
by users with known friend relationships, who may share some subject topics and
wording patterns. The preliminary experiments on Facebook corpus showed that
this proposed approach not only drastically reduced the model perplexity, but
offered very good improvement in recognition accuracy in n-best rescoring
tests. This approach also mitigated the data sparseness problem for
personalized language models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01195</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01195</id><created>2015-06-03</created><updated>2015-06-03</updated><authors><author><keyname>Liu</keyname><forenames>Tianyi</forenames></author><author><keyname>Fang</keyname><forenames>Shuangsang</forenames></author><author><keyname>Zhao</keyname><forenames>Yuehui</forenames></author><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author></authors><title>Implementation of Training Convolutional Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning refers to the shining branch of machine learning that is based
on learning levels of representations. Convolutional Neural Networks (CNN) is
one kind of deep neural network. It can study concurrently. In this article, we
gave a detailed analysis of the process of CNN algorithm both the forward
process and back propagation. Then we applied the particular convolutional
neural network to implement the typical face recognition problem by java. Then,
a parallel strategy was proposed in section4. In addition, by measuring the
actual time of forward and backward computing, we analysed the maximal speed up
and parallel efficiency theoretically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01204</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01204</id><created>2015-06-03</created><authors><author><keyname>Nurellari</keyname><forenames>Edmond</forenames></author><author><keyname>McLernon</keyname><forenames>Des</forenames></author><author><keyname>Ghogho</keyname><forenames>Mounir</forenames></author><author><keyname>Zaidi</keyname><forenames>Syed Ali Raza</forenames></author></authors><title>Distributed Optimal Quantization and Power Allocation for Sensor
  Detection Via Consensus</title><categories>cs.SY</categories><comments>5 pages, 5 figures, proceeding VTC Spring,2015, Glasgow, United
  Kingdom, vtc15spr, (Vehicular Technology Conference Spring 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the optimal transmit power allocation problem (from the sensor
nodes (SNs) to the fusion center (FC)) for the decentralized detection of an
unknown deterministic spatially uncorrelated signal which is being observed by
a distributed wireless sensor network. We propose a novel fully distributed
algorithm, in order to calculate the optimal transmit power allocation for each
sensor node (SN) and the optimal number of quantization bits for the test
statistic in order to match the channel capacity. The SNs send their quantized
information over orthogonal uncorrelated channels to the FC which linearly
combines them and makes a final decision. What makes this scheme attractive is
that the SNs share with their neighbours just their individual transmit powers
at the current states. As a result, the SN processing complexity is further
reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01210</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01210</id><created>2015-06-03</created><authors><author><keyname>Nurellari</keyname><forenames>Edmond</forenames></author><author><keyname>Aldalahmeh</keyname><forenames>Sami</forenames></author><author><keyname>Ghogho</keyname><forenames>Mounir</forenames></author><author><keyname>McLernon</keyname><forenames>Des</forenames></author></authors><title>Quantized Fusion Rules for Energy-Based Distributed Detection in
  Wireless Sensor Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 6 figures, Proceeding SSPD 2014, Edinburgh, United Kingdom.
  Sensor Signal Processing for Defence, SSPD, 2014</comments><doi>10.1109/SSPD.2014.6943313</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of soft decision fusion in a bandwidth-constrained
wireless sensor network (WSN). The WSN is tasked with the detection of an
intruder transmitting an unknown signal over a fading channel. A binary
hypothesis testing is performed using the soft decision of the sensor nodes
(SNs). Using the likelihood ratio test, the optimal soft fusion rule at the
fusion center (FC) has been shown to be the weighted distance from the soft
decision mean under the null hypothesis. But as the optimal rule requires
a-priori knowledge that is difficult to attain in practice, suboptimal fusion
rules are proposed that are realizable in practice. We show how the effect of
quantizing the test statistic can be mitigated by increasing the number of SN
samples, i.e., bandwidth can be traded off against increased latency. The
optimal power and bit allocation for the WSN is also derived. Simulation
results show that SNs with good channels are allocated more bits, while SNs
with poor channels are censored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01217</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01217</id><created>2015-06-03</created><authors><author><keyname>Masood</keyname><forenames>Tehreem</forenames></author><author><keyname>Nadeem</keyname><forenames>Aamer</forenames></author><author><keyname>Lee</keyname><forenames>Gang-soo</forenames></author></authors><title>A Safe Regression Testing Technique for Web Services based on WSDL
  Specification</title><categories>cs.SE</categories><comments>9 figures, 2 tables, 11 pages, ASEA 2011</comments><journal-ref>Communications in Computer and Information Science Volume 257,
  2011, pp 108-119</journal-ref><doi>10.1007/978-3-642-27207-3_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Specification-based regression testing of web services is an important
activity which verifies the quality of web services. A major problem in web
services is that only provider has the source code and both user and broker
only have the XML based specification. So from the perspective of user and
broker, specification based regression testing of web services is needed. The
existing techniques are code based. Due to the dynamic behavior of web
services, web services undergo maintenance and evolution process rapidly.
Retesting of web services is required in order to verify the impact of changes.
In this paper, we present an automated safe specification based regression
testing approach that uses original and modified WSDL specifications for change
identification. All the relevant test cases are selected as reusable hence our
regression test selection approach is safe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01227</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01227</id><created>2015-06-03</created><authors><author><keyname>Zhao</keyname><forenames>Zhiwei</forenames></author><author><keyname>Dong</keyname><forenames>Wei</forenames></author><author><keyname>Yu</keyname><forenames>Jie</forenames></author><author><keyname>Gu</keyname><forenames>Tao</forenames></author><author><keyname>Bu</keyname><forenames>Jiajun</forenames></author></authors><title>3D Wireless: Modeling Wireless Performance by Combining Spatial and
  Temporal Behaviors</title><categories>cs.NI</categories><comments>Submitted to RTSS 2015; Anycast/Broadcast Modeling; Performance
  Modeling; Wireless simulation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance characterization is a fundamental issue in wireless networks for
real time routing, wireless network simulation, and etc. There are four basic
wireless operations that are required to be modeled, i.e., unicast, anycast,
broadcast, and multicast. As observed in many recent works, the temporal and
spatial distribution of packet receptions can have significant impact on
wireless performance involving multiple links (anycast/broadcast/multicast).
However, existing performance models and simulations overlook these two
wireless behaviors, leading to biased performance estimation and simulation
results. In this paper, we first explicitly identify the necessary
&quot;3-Dimension&quot; information for wireless performance modeling, i.e., packet
reception rate (PRR), PRR spatial distribution, and temporal distribution. We
then propose a comprehensive modeling approach considering 3-Dimension Wireless
information (called 3DW model). Further, we demonstrate the generality and wide
applications of 3DW model by two case studies: 3DWbased network simulation and
3DW-based real time routing protocol. Extensive simulation and testbed
experiments have been conducted. The results show that 3DW model achieves much
more accurate performance estimation for both anycast and broadcast/multicast.
3DW-based simulation can effectively reserve the end-to-end performance metric
of the input empirical traces. 3DW-based routing can select more efficient
senders, achieving better transmission efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01233</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01233</id><created>2015-06-03</created><authors><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Otop</keyname><forenames>Jan</forenames></author><author><keyname>Samanta</keyname><forenames>Roopsha</forenames></author></authors><title>Lipschitz Robustness of Timed I/O Systems</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first study of robustness of systems that are both timed as
well as reactive (I/O). We study the behavior of such timed I/O systems in the
presence of &quot;uncertain inputs&quot; and formalize their robustness using the
analytic notion of Lipschitz continuity. Thus, a timed I/O system is
K-(Lipschitz) robust if the perturbation in its output is at most K times the
perturbation in its input. We quantify input and output perturbation using
&quot;similarity functions&quot; over timed words such as the timed version of the
Manhattan distance and the Skorokhod distance. We consider two models of timed
I/O systems --- timed transducers and asynchronous sequential circuits. While
K-robustness is undecidable even for discrete transducers, we identify a class
of timed transducers which admits a polynomial space decision procedure for
K-robustness. For asynchronous sequential circuits, we reduce K-robustness
w.r.t. timed Manhattan distances to K-robusness of discrete letter-to-letter
transducers and show PSPACE-compeleteness of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01245</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01245</id><created>2015-06-03</created><authors><author><keyname>Zhu</keyname><forenames>Xinhua</forenames></author><author><keyname>Li</keyname><forenames>Fei</forenames></author><author><keyname>Chen</keyname><forenames>Hongchao</forenames></author><author><keyname>Peng</keyname><forenames>Qi</forenames></author></authors><title>A density compensation-based path computing model for measuring semantic
  similarity</title><categories>cs.AI</categories><comments>17 pages,11 figures</comments><msc-class>68T50</msc-class><acm-class>I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The shortest path between two concepts in a taxonomic ontology is commonly
used to represent the semantic distance between concepts in the edge-based
semantic similarity measures. In the past, the edge counting is considered to
be the default method for the path computation, which is simple, intuitive and
has low computational complexity. However, a large lexical taxonomy of such as
WordNet has the irregular densities of links between concepts due to its broad
domain but. The edge counting-based path computation is powerless for this
non-uniformity problem. In this paper, we advocate that the path computation is
able to be separated from the edge-based similarity measures and form various
general computing models. Therefore, in order to solve the problem of
non-uniformity of concept density in a large taxonomic ontology, we propose a
new path computing model based on the compensation of local area density of
concepts, which is equal to the number of direct hyponyms of the subsumers of
concepts in their shortest path. This path model considers the local area
density of concepts as an extension of the edge-based path and converts the
local area density divided by their depth into the compensation for edge-based
path with an adjustable parameter, which idea has been proven to be consistent
with the information theory. This model is a general path computing model and
can be applied in various edge-based similarity algorithms. The experiment
results show that the proposed path model improves the average correlation
between edge-based measures with human judgments on Miller and Charles
benchmark from less than 0.8 to more than 0.85, and has a big advantage in
efficiency than information content (IC) computation in a dynamic ontology,
thereby successfully solving the non-uniformity problem of taxonomic ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01256</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01256</id><created>2015-06-03</created><authors><author><keyname>Ndoundam</keyname><forenames>Rene</forenames></author><author><keyname>Ekodeck</keyname><forenames>Stephane Gael Raymond</forenames></author></authors><title>PDF Steganography based on Chinese Remainder Theorem</title><categories>cs.CR</categories><comments>29pages, 5 figures</comments><msc-class>94A60</msc-class><acm-class>D.4.6</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose different approaches of PDF files based steganography, essentially
based on the Chinese Remainder Theorem. Here, after a cover PDF document has
been released from unnecessary A0, a secret message is hidden in it using one
of the proposed approaches, making it invisible to common PDF readers, and the
file is then transmitted through a non-secure communication channel. Where each
of our methods, ensure the condition that the number of inserted A0 is less
than the number of characters of the secret message s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01273</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01273</id><created>2015-06-03</created><updated>2015-06-04</updated><authors><author><keyname>Apar&#xed;cio</keyname><forenames>Marta</forenames></author><author><keyname>Figueiredo</keyname><forenames>Paulo</forenames></author><author><keyname>Raposo</keyname><forenames>Francisco</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author></authors><title>Summarization of Films and Documentaries Based on Subtitles and Scripts</title><categories>cs.CL cs.AI cs.IR</categories><comments>7 pages, 9 tables, 4 figures, submitted to Pattern Recognition
  Letters (Elsevier)</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We assess the performance of generic text summarization algorithms applied to
films and documentaries, using the well-known behavior of summarization of news
articles as reference. We use three datasets: (i) news articles, (ii) film
scripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics
are used for comparing generated summaries against news abstracts, plot
summaries, and synopses. We show that the best performing algorithms are LSA,
for news articles and documentaries, and LexRank and Support Sets, for films.
Despite the different nature of films and documentaries, their relative
behavior is in accordance with that obtained for news articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01280</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01280</id><created>2015-06-03</created><updated>2016-01-28</updated><authors><author><keyname>Battiston</keyname><forenames>Federico</forenames></author><author><keyname>Iacovacci</keyname><forenames>Jacopo</forenames></author><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author></authors><title>Emergence of multiplex communities in collaboration networks</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 6 figures, published in Plos One</comments><doi>10.1371/journal.pone.0147451</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community structures in collaboration networks reflect the natural tendency
of individuals to organize their work in groups in order to better achieve
common goals. In most of the cases, individuals exploit their connections to
introduce themselves to new areas of interests, giving rise to multifaceted
collaborations which span different fields. In this paper, we analyse
collaborations in science and among movie actors as multiplex networks, where
the layers represent respectively research topics and movie genres, and we show
that communities indeed coexist and overlap at the different layers of such
systems. We then propose a model to grow multiplex networks based on two
mechanisms of intra and inter-layer triadic closure which mimic the real
processes by which collaborations evolve. We show that our model is able to
explain the multiplex community structure observed empirically, and we infer
the strength of the two underlying social mechanisms from real-world systems.
Being also able to correctly reproduce the values of intra-layer and
inter-layer assortativity correlations, the model contributes to a better
understanding of the principles driving the evolution of social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01296</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01296</id><created>2015-06-03</created><authors><author><keyname>Podolskii</keyname><forenames>Vladimir V.</forenames></author></authors><title>Circuit Complexity Meets Ontology-Based Data Access</title><categories>cs.LO cs.CC</categories><comments>To appear in proceedings of CSR 2015, LNCS 9139, Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology-based data access is an approach to organizing access to a database
augmented with a logical theory. In this approach query answering proceeds
through a reformulation of a given query into a new one which can be answered
without any use of theory. Thus the problem reduces to the standard database
setting.
  However, the size of the query may increase substantially during the
reformulation. In this survey we review a recently developed framework on
proving lower and upper bounds on the size of this reformulation by employing
methods and results from Boolean circuit complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01310</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01310</id><created>2015-06-03</created><authors><author><keyname>Esch</keyname><forenames>Rodrigo R.</forenames></author><author><keyname>Protti</keyname><forenames>F&#xe1;bio</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author></authors><title>Adaptive event sensing in networks of autonomous mobile agents</title><categories>cs.MA cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a connected region in two-dimensional space where events of a certain
kind occur according to a certain time-varying density, we consider the problem
of setting up a network of autonomous mobile agents to detect the occurrence of
those events and possibly record them in as effective a manner as possible. We
assume that agents can communicate with one another wirelessly within a fixed
communication radius, and moreover that initially no agent has any information
regarding the event density. We introduce a new distributed algorithm for agent
control based on the notion of an execution mode, which essentially lets each
agent roam the target region either at random or following its local view of a
density-dependent gradient. Agents can switch back and forth between the two
modes, and the precise manner of such changes depends on the setting of various
parameters that can be adjusted as a function of the application at hand. We
provide simulation results on some synthetic applications especially designed
to highlight the algorithm's behavior relative to the possible execution modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01315</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01315</id><created>2015-06-01</created><authors><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author><author><keyname>Xu</keyname><forenames>Yixin</forenames></author></authors><title>Impossibility Theorems and the Universal Algebraic Toolkit</title><categories>cs.CC cs.LO math.CO q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We elucidate a close connection between the Theory of Judgment Aggregation
(more generally, Evaluation Aggregation), and a relatively young but rapidly
growing field of universal algebra, that was primarily developed to investigate
constraint satisfaction problems. Our connection yields a full classification
of non-binary evaluations into possibility and impossibility domains both under
the idempotent and the supportive conditions. Prior to the current result E.
Dokow and R. Holzman nearly classified non-binary evaluations in the supportive
case, by combinatorial means. The algebraic approach gives us new insights to
the easier binary case as well, which had been fully classified by the above
authors. Our algebraic view lets us put forth a suggestion about a
strengthening of the Non-dictatorship criterion, that helps us avoid &quot;outliers&quot;
like the affine subspace. Finally, we give upper bounds on the complexity of
computing if a domain is impossible or not (to our best knowledge no finite
time bounds were given earlier).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01326</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01326</id><created>2015-06-03</created><authors><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author><author><keyname>Osborne</keyname><forenames>Michael A</forenames></author><author><keyname>Girolami</keyname><forenames>Mark</forenames></author></authors><title>Probabilistic Numerics and Uncertainty in Computations</title><categories>math.NA cs.AI cs.LG stat.CO stat.ML</categories><comments>Author Generated Postprint. 17 pages, 4 Figures, 1 Table</comments><doi>10.1098/rspa.2015.0142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We deliver a call to arms for probabilistic numerical methods: algorithms for
numerical tasks, including linear algebra, integration, optimization and
solving differential equations, that return uncertainties in their
calculations. Such uncertainties, arising from the loss of precision induced by
numerical calculation with limited time or hardware, are important for much
contemporary science and industry. Within applications such as climate science
and astrophysics, the need to make decisions on the basis of computations with
large and complex data has led to a renewed focus on the management of
numerical uncertainty. We describe how several seminal classic numerical
methods can be interpreted naturally as probabilistic inference. We then show
that the probabilistic view suggests new algorithms that can flexibly be
adapted to suit application specifics, while delivering improved empirical
performance. We provide concrete illustrations of the benefits of probabilistic
numeric algorithms on real scientific problems from astrometry and astronomical
imaging, while highlighting open problems with these new algorithms. Finally,
we describe how probabilistic numerical methods provide a coherent framework
for identifying the uncertainty in calculations performed with a combination of
numerical algorithms (e.g. both numerical optimisers and differential equation
solvers), potentially allowing the diagnosis (and control) of error sources in
computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01330</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01330</id><created>2015-06-03</created><authors><author><keyname>Wang</keyname><forenames>Sen</forenames></author><author><keyname>Nie</keyname><forenames>Feiping</forenames></author><author><keyname>Chang</keyname><forenames>Xiaojun</forenames></author><author><keyname>Yao</keyname><forenames>Lina</forenames></author><author><keyname>Li</keyname><forenames>Xue</forenames></author><author><keyname>Sheng</keyname><forenames>Quan Z.</forenames></author></authors><title>Unsupervised Feature Analysis with Class Margin Optimization</title><categories>cs.LG</categories><comments>Accepted by European Conference on Machine Learning and Principles
  and Practice of Knowledge Discovery in Databases, ECML/PKDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised feature selection has been always attracting research attention
in the communities of machine learning and data mining for decades. In this
paper, we propose an unsupervised feature selection method seeking a feature
coefficient matrix to select the most distinctive features. Specifically, our
proposed algorithm integrates the Maximum Margin Criterion with a
sparsity-based model into a joint framework, where the class margin and feature
correlation are taken into account at the same time. To maximize the total data
separability while preserving minimized within-class scatter simultaneously, we
propose to embed Kmeans into the framework generating pseudo class label
information in a scenario of unsupervised feature selection. Meanwhile, a
sparsity-based model, ` 2 ,p-norm, is imposed to the regularization term to
effectively discover the sparse structures of the feature coefficient matrix.
In this way, noisy and irrelevant features are removed by ruling out those
features whose corresponding coefficients are zeros. To alleviate the local
optimum problem that is caused by random initializations of K-means, a
convergence guaranteed algorithm with an updating strategy for the clustering
indicator matrix, is proposed to iteractively chase the optimal solution.
Performance evaluation is extensively conducted over six benchmark data sets.
From plenty of experimental results, it is demonstrated that our method has
superior performance against all other compared approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01333</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01333</id><created>2015-06-03</created><authors><author><keyname>Slavov</keyname><forenames>Vasil</forenames></author><author><keyname>Katib</keyname><forenames>Anas</forenames></author><author><keyname>Rao</keyname><forenames>Praveen</forenames></author><author><keyname>Paturi</keyname><forenames>Srivenu</forenames></author><author><keyname>Barenkala</keyname><forenames>Dinesh</forenames></author></authors><title>Fast Processing of SPARQL Queries on RDF Quadruples</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose a new approach for fast processing of SPARQL
queries on large RDF datasets containing RDF quadruples (or quads). Our
approach called RIQ employs a decrease-and-conquer strategy: Rather than
indexing the entire RDF dataset, RIQ identifies groups of similar RDF graphs
and indexes each group separately. During query processing, RIQ uses a novel
filtering index to first identify candidate groups that may contain matches for
the query. On these candidates, it executes optimized queries using a
conventional SPARQL processor to produce the final results. Our initial
performance evaluation results are promising: Using a synthetic and a real
dataset, each containing about 1.4 billion quads, we show that RIQ outperforms
RDF-3X and Jena TDB on a variety of SPARQL queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01338</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01338</id><created>2015-06-03</created><authors><author><keyname>Keshavarz</keyname><forenames>Hossein</forenames></author><author><keyname>Scott</keyname><forenames>Clayton</forenames></author><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author></authors><title>Optimal change point detection in Gaussian processes</title><categories>math.ST cs.IT cs.LG math.IT stat.ML stat.TH</categories><comments>44 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of detecting a change in the mean of one-dimensional
Gaussian process data. This problem is investigated in the setting of
increasing domain (customarily employed in time series analysis) and in the
setting of fixed domain (typically arising in spatial data analysis). We
propose a detection method based on generalized likelihood ratio test (GLRT),
and show that our method achieves asymptotically optimal rate in the minimax
sense, in both settings. The salient feature of the proposed method is that it
exploits in an efficient way the data dependence captured by the Gaussian
process covariance structure. When the covariance is not known, we propose
plug-in GLRT method and derive conditions under which the method remains
asymptotically optimal. By contrast, the standard CUSUM method, which does not
account for the covariance structure, is shown to be asymptotically optimal
only in the increasing domain. Our algorithms and accompanying theory are
applicable to a wide variety of covariance structures, including the Matern
class, the powered exponential class, and others. The plug-in GLRT method is
shown to perform well for a number of covariance estimators, including maximum
likelihood estimators with a dense or a tapered covariance matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01339</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01339</id><created>2015-06-03</created><updated>2015-11-13</updated><authors><author><keyname>Whitehill</keyname><forenames>Jacob</forenames></author></authors><title>Exploiting an Oracle that Reports AUC Scores in Machine Learning
  Contests</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In machine learning contests such as the ImageNet Large Scale Visual
Recognition Challenge and the KDD Cup, contestants can submit candidate
solutions and receive from an oracle (typically the organizers of the
competition) the accuracy of their guesses compared to the ground-truth labels.
One of the most commonly used accuracy metrics for binary classification tasks
is the Area Under the Receiver Operating Characteristics Curve (AUC). In this
paper we provide proofs-of-concept of how knowledge of the AUC of a set of
guesses can be used, in two different kinds of attacks, to improve the accuracy
of those guesses. On the other hand, we also demonstrate the intractability of
one kind of AUC exploit by proving that the number of possible binary labelings
of $n$ examples for which a candidate solution obtains a AUC score of $c$ grows
exponentially in $n$, for every $c\in (0,1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01342</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01342</id><created>2015-06-03</created><updated>2015-09-23</updated><authors><author><keyname>RoyChowdhury</keyname><forenames>Aruni</forenames></author><author><keyname>Lin</keyname><forenames>Tsung-Yu</forenames></author><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author><author><keyname>Learned-Miller</keyname><forenames>Erik</forenames></author></authors><title>Face Identification with Bilinear CNNs</title><categories>cs.CV</categories><comments>Updated with CMC and DET curves, open-set results on IJB-A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent explosive growth in convolutional neural network (CNN) research
has produced a variety of new architectures for deep learning. One intriguing
new architecture is the bilinear CNN (\mbox{B-CNN}), which has shown dramatic
performance gains on certain fine-grained recognition
problems~\cite{lin2015bilinear}. We apply this new CNN to the challenging new
face recognition benchmark, the IARPA Janus Benchmark~A~(IJB-A)~\cite{IJBA}. It
features faces from a large number of identities in challenging real-world
conditions. Because the face images were not identified automatically using a
computerized face detection system, it does not have the bias inherent in such
a database. We demonstrate the performance of the \mbox{B-CNN} model beginning
from an AlexNet-style network pre-trained on ImageNet. We then show results for
fine-tuning using a moderate-sized and public external database,
FaceScrub~\cite{ng265data}. We also present results with additional fine-tuning
on the limited training data provided by the protocol. In each case, the
fine-tuned bilinear model shows substantial improvements over the standard CNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01355</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01355</id><created>2015-06-03</created><authors><author><keyname>Yu</keyname><forenames>Qian</forenames></author></authors><title>Finding the Optimal Demodulator Under Implementation Constraints</title><categories>cs.IT math.IT</categories><comments>masters thesis</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The common approach of designing a communication device is to maximize a
well-defined objective function, e.g., the channel capacity and the cut-off
rate. We propose easy-to-implement solutions for Gaussian channels that
approximate the optimal results for these maximization problems. Three topics
are addressed. First, we consider the case where the channel output is
quantized, and we find the quantization thresholds that maximize the mutual
information. The approximation derived from the asymptotic solution has a
negligible loss on the entire range of SNR when 2-PAM modulation is used, and
its quantization thresholds linearly depend on the standard deviation of noise.
We also derive a simple estimator of the relative capacity loss due to
quantization, based on the high-rate limit. Then we consider the integer
constraint on the decoding metric, and maximize the mismatched channel
capacity. We study the asymptotic solution of the optimal metric assignment and
show that the same approximation we derived in the matched decoding case still
holds for the mismatched decoder. Finally, we consider the demodulation problem
for 8PSK bit-interleaved coded modulation(BICM). We derive the approximated
optimal demodulation metrics that maximize the general cut-off rate or the
mismatched capacity using max-log approximation . The error rate performances
of the two metrics' assignments are compared, based on
Reed-Solomon-Viterbi(RSV) code, and the mismatched capacity metric turns out to
be better. The proposed approximation can be computed using an efficient
firmware algorithm, and improves the system performance of commercial chips.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01367</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01367</id><created>2015-06-03</created><authors><author><keyname>Li</keyname><forenames>Jerry</forenames></author><author><keyname>Schmidt</keyname><forenames>Ludwig</forenames></author></authors><title>A Nearly Optimal and Agnostic Algorithm for Properly Learning a Mixture
  of k Gaussians, for any Constant k</title><categories>cs.DS cs.IT cs.LG math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning a Gaussian mixture model (GMM) is a fundamental problem in machine
learning, learning theory, and statistics. One notion of learning a GMM is
proper learning: here, the goal is to find a mixture of $k$ Gaussians
$\mathcal{M}$ that is close to the density $f$ of the unknown distribution from
which we draw samples. The distance between $\mathcal{M}$ and $f$ is typically
measured in the total variation or $L_1$-norm.
  We give an algorithm for learning a mixture of $k$ univariate Gaussians that
is nearly optimal for any fixed $k$. The sample complexity of our algorithm is
$\tilde{O}(\frac{k}{\epsilon^2})$ and the running time is $(k \cdot
\log\frac{1}{\epsilon})^{O(k^4)} + \tilde{O}(\frac{k}{\epsilon^2})$. It is
well-known that this sample complexity is optimal (up to logarithmic factors),
and it was already achieved by prior work. However, the best known time
complexity for proper learning a $k$-GMM was
$\tilde{O}(\frac{1}{\epsilon^{3k-1}})$. In particular, the dependence between
$\frac{1}{\epsilon}$ and $k$ was exponential. We significantly improve this
dependence by replacing the $\frac{1}{\epsilon}$ term with a $\log
\frac{1}{\epsilon}$ while only increasing the exponent moderately. Hence, for
any fixed $k$, the $\tilde{O} (\frac{k}{\epsilon^2})$ term dominates our
running time, and thus our algorithm runs in time which is nearly-linear in the
number of samples drawn. Achieving a running time of $\textrm{poly}(k,
\frac{1}{\epsilon})$ for proper learning of $k$-GMMs has recently been stated
as an open problem by multiple researchers, and we make progress on this
question.
  Moreover, our approach offers an agnostic learning guarantee: our algorithm
returns a good GMM even if the distribution we are sampling from is not a
mixture of Gaussians. To the best of our knowledge, our algorithm is the first
agnostic proper learning algorithm for GMMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01394</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01394</id><created>2015-05-31</created><authors><author><keyname>Ding</keyname><forenames>Guoru</forenames></author><author><keyname>Wang</keyname><forenames>Jinlong</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author><author><keyname>Yao</keyname><forenames>Yu-Dong</forenames></author><author><keyname>Song</keyname><forenames>Fei</forenames></author><author><keyname>Tsiftsis</keyname><forenames>Theodoros A.</forenames></author></authors><title>Cellular-Base-Station Assisted Device-to-Device Communications in TV
  White Space</title><categories>cs.NI</categories><comments>Accepted by IEEE Journal on Selected Areas in Communications, to
  appear, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a systematic approach to exploit TV white space (TVWS)
for device-to-device (D2D) communications with the aid of the existing cellular
infrastructure. The goal is to build a location-specific TVWS database, which
provides a look-up table service for any D2D link to determine its maximum
permitted emission power (MPEP) in an unlicensed digital TV (DTV) band. To
achieve this goal, the idea of mobile crowd sensing is firstly introduced to
collect active spectrum measurements from massive personal mobile devices.
Considering the incompleteness of crowd measurements, we formulate the problem
of unknown measurements recovery as a matrix completion problem and apply a
powerful fixed point continuation algorithm to reconstruct the unknown elements
from the known elements. By joint exploitation of the big spectrum data in its
vicinity, each cellular base station further implements a nonlinear support
vector machine algorithm to perform irregular coverage boundary detection of a
licensed DTV transmitter. With the knowledge of the detected coverage boundary,
an opportunistic spatial reuse algorithm is developed for each D2D link to
determine its MPEP. Simulation results show that the proposed approach can
successfully enable D2D communications in TVWS while satisfying the
interference constraint from the licensed DTV services. In addition, to our
best knowledge, this is the first try to explore and exploit TVWS inside the
DTV protection region resulted from the shadowing effect. Potential application
scenarios include communications between internet of vehicles in the
underground parking, D2D communications in hotspots such as subway, game
stadiums, and airports, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01398</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01398</id><created>2015-06-03</created><authors><author><keyname>Alphonse</keyname><forenames>Jismy</forenames></author><author><keyname>G.</keyname><forenames>Biju V.</forenames></author></authors><title>Recognition of Changes in SAR Images Based on Gauss-Log Ratio and MRFFCM</title><categories>cs.CV</categories><comments>7 pages, 7 figures, 2 tables in International Journal of advanced
  studies in Computer Science and Engineering (IJASCSE), ISSN : 2278 7917,
  Volume 4 Issue 5, 2015, www.ijascse.org</comments><report-no>page 65-71</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A modified version of MRFFCM (Markov Random Field Fuzzy C means) based SAR
(Synthetic aperture Radar) image change detection method is proposed in this
paper. It involves three steps: Difference Image (DI) generation by using
Gauss-log ratio operator, speckle noise reduction by SRAD (Speckle Reducing
Anisotropic Diffusion), and the detection of changed regions by using MRFFCM.
The proposed method is compared with existing methods such as FCM and MRFFCM
using simulated and real SAR images. The measures used for evaluation includes
Overall Error (OE), Percentage Correct Classification (PCC), Kappa Coefficient
(KC), Root Mean Square Error (RMSE), and Peak Signal to Noise Ratio (PSNR). The
results show that the proposed method is better compared to FCM and MRFFCM
based change detection method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01406</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01406</id><created>2015-06-03</created><updated>2016-01-25</updated><authors><author><keyname>Gualdron</keyname><forenames>Hugo</forenames><affiliation>Polo</affiliation></author><author><keyname>Cordeiro</keyname><forenames>Robson</forenames><affiliation>Polo</affiliation></author><author><keyname>Rodrigues</keyname><forenames>Jose F.</forenames><suffix>Jr.</suffix><affiliation>Polo</affiliation></author><author><keyname>Horng</keyname><forenames>Duen</forenames><affiliation>Polo</affiliation></author><author><keyname>Chau</keyname></author><author><keyname>Kahng</keyname><forenames>Minsuk</forenames></author><author><keyname>Kang</keyname><forenames>U</forenames></author></authors><title>M-Flash: Fast Billion-scale Graph Computation Using Block Partition
  Model</title><categories>cs.DB cs.DS cs.SI</categories><comments>10 pages, Draft 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent graph computation approaches such as GraphChi, X-Stream, TurboGraph
and MMap demonstrated that a single PC can perform efficient computation on
billion-scale graphs. While they use different techniques to achieve
scalability through optimizing I/O operations, such optimization often does not
fully exploit the capabilities of modern hard drives. Our main contributions
are:
  (1) we propose a novel and scalable graph computation framework called
M-Flash that uses a new, bimodal block processing strategy (\emph{BBP}) to
boost computation speed by minimizing I/O cost;
  (2) M-Flash includes a flexible and deliberatively simple programming model
that enables us to easily implement popular and essential graph algorithms,
including the \textit{first} single-machine billion-scale eigensolver; and (3)
we performed extensive experiments on real graphs with up to 6.6 billion edges,
demonstrating M-Flash's consistent and significant speed-up over
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01414</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01414</id><created>2015-06-03</created><authors><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author><author><keyname>Farina</keyname><forenames>Jason</forenames></author><author><keyname>Kechadi</keyname><forenames>M-Tahar</forenames></author></authors><title>Network investigation methodology for BitTorrent Sync: A Peer-to-Peer
  based file synchronisation service</title><categories>cs.CR cs.NI</categories><comments>in Computers and Security 2015</comments><acm-class>K.6.m; C.2.1</acm-class><doi>10.1016/j.cose.2015.05.003</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  High availability is no longer just a business continuity concern. Users are
increasingly dependant on devices that consume and produce data in ever
increasing volumes. A popular solution is to have a central repository which
each device accesses after centrally managed authentication. This model of use
is facilitated by cloud based file synchronisation services such as Dropbox,
OneDrive, Google Drive and Apple iCloud. Cloud architecture allows the
provisioning of storage space with &quot;always-on&quot; access. Recent concerns over
unauthorised access to third party systems and large scale exposure of private
data have made an alternative solution desirable. These events have caused
users to assess their own security practices and the level of trust placed in
third party storage services. One option is BitTorrent Sync, a cloudless
synchronisation utility provides data availability and redundancy. This utility
replicates files stored in shares to remote peers with access controlled by
keys and permissions. While lacking the economies brought about by scale,
complete control over data access has made this a popular solution. The ability
to replicate data without oversight introduces risk of abuse by users as well
as difficulties for forensic investigators. This paper suggests a methodology
for investigation and analysis of the protocol to assist in the control of data
flow across security perimeters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01419</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01419</id><created>2015-06-03</created><authors><author><keyname>Cioab&#x103;</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Xu</keyname><forenames>Peng</forenames></author></authors><title>Mixing Rates of Random Walks with Little Backtracking</title><categories>math.CO cs.DM math.PR</categories><comments>31 pages; to appear in the CRM Proceedings Series, published by the
  American Mathematical Society as part of the Contemporary Mathematics Series</comments><msc-class>05C81, 05E30, 15A18, 60B10, 60C05, 60G99, 60J10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many regular graphs admit a natural partition of their edge set into cliques
of the same order such that each vertex is contained in the same number of
cliques. In this paper, we study the mixing rate of certain random walks on
such graphs and we generalize previous results of Alon, Benjamini, Lubetzky and
Sodin regarding the mixing rates of non-backtracking random walks on regular
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01428</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01428</id><created>2015-06-03</created><authors><author><keyname>Di Francescomarino</keyname><forenames>Chiara</forenames></author><author><keyname>Dumas</keyname><forenames>Marlon</forenames></author><author><keyname>Maggi</keyname><forenames>Fabrizio Maria</forenames></author><author><keyname>Teinemaa</keyname><forenames>Irene</forenames></author></authors><title>Clustering-Based Predictive Process Monitoring</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business process enactment is generally supported by information systems that
record data about process executions, which can be extracted as event logs.
Predictive process monitoring is concerned with exploiting such event logs to
predict how running (uncompleted) cases will unfold up to their completion. In
this paper, we propose a predictive process monitoring framework for estimating
the probability that a given predicate will be fulfilled upon completion of a
running case. The predicate can be, for example, a temporal logic constraint or
a time constraint, or any predicate that can be evaluated over a completed
trace. The framework takes into account both the sequence of events observed in
the current trace, as well as data attributes associated to these events. The
prediction problem is approached in two phases. First, prefixes of previous
traces are clustered according to control flow information. Secondly, a
classifier is built for each cluster using event data to discriminate between
fulfillments and violations. At runtime, a prediction is made on a running case
by mapping it to a cluster and applying the corresponding classifier. The
framework has been implemented in the ProM toolset and validated on a log
pertaining to the treatment of cancer patients in a large hospital.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01430</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01430</id><created>2015-06-03</created><authors><author><keyname>Liu</keyname><forenames>Mingming</forenames></author><author><keyname>Wirth</keyname><forenames>Fabian</forenames></author><author><keyname>Corless</keyname><forenames>Martin</forenames></author><author><keyname>Shorten</keyname><forenames>Robert</forenames></author></authors><title>On global convergence of consensus with nonlinear feedback, the Lure
  problem, and some applications</title><categories>cs.SY</categories><comments>This technical note has been submitted to IEEE Transactions on
  Automatic Control for publication in January 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We give a rigorous proof of convergence of a recently proposed consensus
algorithm with output constraint. Examples are presented to illustrate the
efficacy and utility of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01432</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01432</id><created>2015-06-03</created><updated>2015-06-08</updated><authors><author><keyname>Kuzelka</keyname><forenames>Ondrej</forenames></author><author><keyname>Davis</keyname><forenames>Jesse</forenames></author><author><keyname>Schockaert</keyname><forenames>Steven</forenames></author></authors><title>Encoding Markov Logic Networks in Possibilistic Logic</title><categories>cs.AI</categories><comments>Extended version of a paper appearing in UAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov logic uses weighted formulas to compactly encode a probability
distribution over possible worlds. Despite the use of logical formulas, Markov
logic networks (MLNs) can be difficult to interpret, due to the often
counter-intuitive meaning of their weights. To address this issue, we propose a
method to construct a possibilistic logic theory that exactly captures what can
be derived from a given MLN using maximum a posteriori (MAP) inference.
Unfortunately, the size of this theory is exponential in general. We therefore
also propose two methods which can derive compact theories that still capture
MAP inference, but only for specific types of evidence. These theories can be
used, among others, to make explicit the hidden assumptions underlying an MLN
or to explain the predictions it makes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01434</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01434</id><created>2015-06-03</created><updated>2015-11-18</updated><authors><author><keyname>Badkoubeh</keyname><forenames>Amir</forenames></author><author><keyname>Zheng</keyname><forenames>Jun</forenames></author><author><keyname>Zhu</keyname><forenames>Guchuan</forenames></author></authors><title>Flatness-based Deformation Control of an Euler-Bernoulli Beam with
  In-domain Actuation</title><categories>math.OC cs.SY</categories><comments>Preprint of an original research work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of deformation control of an Euler-Bernoulli
beam with in-domain actuation. The proposed control scheme consists in first
relating the system model described by an inhomogeneous partial differential
equation to a target system under a standard boundary control form. Then, a
combination of closed-loop feedback control and flatness-based motion planning
is used for stabilizing the closed-loop system around reference trajectories.
The validity of the proposed method is assessed through well-posedness and
stability analysis of the considered systems. The performance of the developed
control scheme is demonstrated through numerical simulations of a
representative micro-beam.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01436</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01436</id><created>2015-06-03</created><updated>2015-08-20</updated><authors><author><keyname>Liu</keyname><forenames>Mingming</forenames></author><author><keyname>Ord&#xf3;&#xf1;ez-Hurtado</keyname><forenames>Rodrigo H.</forenames></author><author><keyname>Wirth</keyname><forenames>Fabian</forenames></author><author><keyname>Gu</keyname><forenames>Yingqi</forenames></author><author><keyname>Crisostomi</keyname><forenames>Emanuele</forenames></author><author><keyname>Shorten</keyname><forenames>Robert</forenames></author></authors><title>A Distributed and Privacy-Aware Speed Advisory System for Optimising
  Conventional and Electric Vehicles Networks</title><categories>cs.SY</categories><comments>This is a journal paper based on the conference paper &quot;Highway speed
  limits, optimised consensus, and intelligent speed advisory systems&quot;
  presented at the 3rd International Conference on Connected Vehicles and Expo
  (ICCVE 2014) in November 2014. This is the revised version of the paper
  recently submitted to the IEEE Transactions on Intelligent Transportation
  Systems for publication</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  One of the key ideas to make Intelligent Transportation Systems (ITS) work
effectively is to deploy advanced communication and cooperative control
technologies among the vehicles and road infrastructures. In this spirit, we
propose a consensus-based distributed speed advisory system that optimally
determines a recommended common speed for a given area in order that the group
emissions, or group battery consumptions, are minimised. Our algorithms achieve
this in a privacy-aware manner; namely, individual vehicles do not reveal
in-vehicle information to other vehicles or to infrastructure. A mobility
simulator is used to illustrate the efficacy of the algorithm, and
hardware-in-the-loop tests involving a real vehicle are given to illustrate
user acceptability and ease of the deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01437</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01437</id><created>2015-06-03</created><updated>2015-07-04</updated><authors><author><keyname>Hand</keyname><forenames>Paul</forenames></author><author><keyname>Lee</keyname><forenames>Choongbum</forenames></author><author><keyname>Voroninski</keyname><forenames>Vladislav</forenames></author></authors><title>ShapeFit: Exact location recovery from corrupted pairwise directions</title><categories>cs.CV cs.IT math.CO math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $t_1,\ldots,t_n \in \mathbb{R}^d$ and consider the location recovery
problem: given a subset of pairwise direction observations $\{(t_i - t_j) /
\|t_i - t_j\|_2\}_{i&lt;j \in [n] \times [n]}$, where a constant fraction of these
observations are arbitrarily corrupted, find $\{t_i\}_{i=1}^n$ up to a global
translation and scale. We propose a novel algorithm for the location recovery
problem, which consists of a simple convex program over $dn$ real variables. We
prove that this program recovers a set of $n$ i.i.d. Gaussian locations exactly
and with high probability if the observations are given by an \erdosrenyi
graph, $d$ is large enough, and provided that at most a constant fraction of
observations involving any particular location are adversarially corrupted. We
also prove that the program exactly recovers Gaussian locations for $d=3$ if
the fraction of corrupted observations at each location is, up to
poly-logarithmic factors, at most a constant. Both of these recovery theorems
are based on a set of deterministic conditions that we prove are sufficient for
exact recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01442</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01442</id><created>2015-06-03</created><authors><author><keyname>Ganguly</keyname><forenames>Sumit</forenames></author></authors><title>Taylor Polynomial Estimator for Estimating Frequency Moments</title><categories>cs.DS</categories><comments>Supercedes arXiv:1104.4552. Extended Abstract of this paper to appear
  in Proceedings of ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a randomized algorithm for estimating the $p$th moment $F_p$ of
the frequency vector of a data stream in the general update (turnstile) model
to within a multiplicative factor of $1 \pm \epsilon$, for $p &gt; 2$, with high
constant confidence. For $0 &lt; \epsilon \le 1$, the algorithm uses space $O(
n^{1-2/p} \epsilon^{-2} + n^{1-2/p} \epsilon^{-4/p} \log (n))$ words. This
improves over the current bound of $O(n^{1-2/p} \epsilon^{-2-4/p} \log (n))$
words by Andoni et. al. in \cite{ako:arxiv10}. Our space upper bound matches
the lower bound of Li and Woodruff \cite{liwood:random13} for $\epsilon = (\log
(n))^{-\Omega(1)}$ and the lower bound of Andoni et. al. \cite{anpw:icalp13}
for $\epsilon = \Omega(1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01446</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01446</id><created>2015-06-03</created><authors><author><keyname>Mu</keyname><forenames>Qi</forenames></author><author><keyname>Cui</keyname><forenames>Liqing</forenames></author><author><keyname>Song</keyname><forenames>Yufei</forenames></author></authors><title>The implementation and optimization of Bitonic sort algorithm based on
  CUDA</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes in detail the bitonic sort algorithm,and implements the
bitonic sort algorithm based on cuda architecture.At the same time,we conduct
two e?ective optimization of implementation details according to the
characteristics of the GPU,which greatly improve the e?ciency. Finally,we
survey the optimized Bitonic sort algorithm on the GPU with the speedup of
quick sort algorithm on the CPU.Since Quick Sort is not suitable to be
implemented in parallel,but it is more e?cient than other sorting algorithms on
CPU to some extend.Hence,to see the speedup and performance,we compare bitonic
sort on GPU with quick Sort on CPU. For a series of 32-bit random integer,the
experimental results show that the acceleration of our work is nearly 20
times.When array size is about 216,the speedup ratio is even up to 30.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01449</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01449</id><created>2015-06-03</created><updated>2016-03-02</updated><authors><author><keyname>Angel</keyname><forenames>Sebastian</forenames></author><author><keyname>Wahby</keyname><forenames>Riad S.</forenames></author><author><keyname>Howald</keyname><forenames>Max</forenames></author><author><keyname>Leners</keyname><forenames>Joshua B.</forenames></author><author><keyname>Spilo</keyname><forenames>Michael</forenames></author><author><keyname>Sun</keyname><forenames>Zhen</forenames></author><author><keyname>Blumberg</keyname><forenames>Andrew J.</forenames></author><author><keyname>Walfish</keyname><forenames>Michael</forenames></author></authors><title>Defending against malicious peripherals</title><categories>cs.OS cs.CR</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attacks on host computers by malicious peripherals are a growing problem.
Inexpensive and powerful peripherals, which attach to plug-and-play buses, have
made such attacks easy to mount. Making matters worse, commodity operating
systems lack systematic defenses, and users are often not aware of the scope of
the problem. We present Cinch, a pragmatic response to this threat. Cinch uses
virtualization to place the hardware in a logically separate, untrusted
machine, and includes an interposition layer between the untrusted machine and
the protected one. This layer accepts or rejects interaction with devices and
enforces security policies that are easily configured and extended by users. We
show that Cinch integrates with existing OSes, enforces policies that thwart
real world attacks, and has low overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01461</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01461</id><created>2015-06-04</created><authors><author><keyname>Burgess</keyname><forenames>Matthew</forenames></author><author><keyname>Adar</keyname><forenames>Eytan</forenames></author><author><keyname>Cafarella</keyname><forenames>Michael</forenames></author></authors><title>Link-Prediction Enhanced Consensus Clustering for Complex Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real networks that are inferred or collected from data are incomplete
due to missing edges. Missing edges can be inherent to the dataset (Facebook
friend links will never be complete) or the result of sampling (one may only
have access to a portion of the data). The consequence is that downstream
analyses that consume the network will often yield less accurate results than
if the edges were complete. Community detection algorithms, in particular,
often suffer when critical intra-community edges are missing. We propose a
novel consensus clustering algorithm to enhance community detection on
incomplete networks. Our framework utilizes existing community detection
algorithms that process networks imputed by our link prediction based
algorithm. The framework then merges their multiple outputs into a final
consensus output. On average our method boosts performance of existing
algorithms by 7% on artificial data and 17% on ego networks collected from
Facebook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01472</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01472</id><created>2015-06-04</created><authors><author><keyname>Bora</keyname><forenames>Dibya Jyoti</forenames></author><author><keyname>Gupta</keyname><forenames>Anil Kumar</forenames></author><author><keyname>Khan</keyname><forenames>Fayaz Ahmad</forenames></author></authors><title>Comparing the Performance of L*A*B* and HSV Color Spaces with Respect to
  Color Image Segmentation</title><categories>cs.CV</categories><comments>11 pages, 19 figures in International Journal of Emerging Technology
  and Advanced Engineering,Volume 5, Issue 2, February 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Color image segmentation is a very emerging topic for image processing
research. Since it has the ability to present the result in a way that is much
more close to the human yes perceive, so todays more research is going on this
area. Choosing a proper color space is a very important issue for color image
segmentation process. Generally LAB and HSV are the two frequently chosen color
spaces. In this paper a comparative analysis is performed between these two
color spaces with respect to color image segmentation. For measuring their
performance, we consider the parameters: mse and psnr . It is found that HSV
color space is performing better than LAB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01476</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01476</id><created>2015-06-04</created><authors><author><keyname>Cantone</keyname><forenames>Domenico</forenames></author><author><keyname>Nicolosi-Asmundo</keyname><forenames>Marianna</forenames></author></authors><title>The decision problem for a three-sorted fragment of set theory with
  restricted quantification and finite enumerations</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the satisfiability problem for a three-sorted fragment of set theory
(denoted $3LQST_0^R$), which admits a restricted form of quantification over
individual and set variables and the finite enumeration operator $\{\text{-},
\text{-}, \ldots, \text{-}\}$ over individual variables, by showing that it
enjoys a small model property, i.e., any satisfiable formula $\psi$ of
$3LQST_0^R$ has a finite model whose size depends solely on the length of
$\psi$ itself. Several set-theoretic constructs are expressible by
$3LQST_0^R$-formulae, such as some variants of the power set operator and the
unordered Cartesian product. In particular, concerning the unordered Cartesian
product, we show that when finite enumerations are used to represent the
construct, the resulting formula is exponentially shorter than the one that can
be constructed without resorting to such terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01490</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01490</id><created>2015-06-04</created><updated>2015-10-11</updated><authors><author><keyname>Li</keyname><forenames>Chun-Liang</forenames></author><author><keyname>Lin</keyname><forenames>Hsuan-Tien</forenames></author><author><keyname>Lu</keyname><forenames>Chi-Jen</forenames></author></authors><title>Rivalry of Two Families of Algorithms for Memory-Restricted Streaming
  PCA</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of recovering the subspace spanned by the first $k$
principal components of $d$-dimensional data under the streaming setting, with
a memory bound of $O(kd)$. Two families of algorithms are known for this
problem. The first family is based on the framework of stochastic gradient
descent. Nevertheless, the convergence rate of the family can be seriously
affected by the learning rate of the descent steps and deserves more serious
study. The second family is based on the power method over blocks of data, but
setting the block size for its existing algorithms is not an easy task. In this
paper, we analyze the convergence rate of a representative algorithm with
decayed learning rate (Oja and Karhunen, 1985) in the first family for the
general $k&gt;1$ case. Moreover, we propose a novel algorithm for the second
family that sets the block sizes automatically and dynamically with faster
convergence rate. We then conduct empirical studies that fairly compare the two
families on real-world data. The studies reveal the advantages and
disadvantages of these two families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01491</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01491</id><created>2015-06-04</created><authors><author><keyname>Masood</keyname><forenames>Tehreem</forenames></author><author><keyname>Cherifi</keyname><forenames>Chantal Bonner</forenames></author><author><keyname>Moalla</keyname><forenames>N&#xe9;jib</forenames></author></authors><title>Service Networks Monitoring for better Quality of Service</title><categories>cs.SE</categories><comments>6 figures, 2tables, ICIST2015. 5th International Conference on
  Information Society and Techology, ICIST 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, the deployment of Web services in many enterprise applications has
gained much attention. Service network inhibits certain common properties as
they arise spontaneously and are subject to high fluctuation. The objective of
consumer is to compose services for stable business processes in coherence with
their legacy system capabilities and with better quality of services. For this
purpose we have proposed a dynamic decision model that integrates several
performance metrics and attributes to monitor the performance of service
oriented systems in order to ensure their sustainability. Based on the
available metrics, we have identified performance metrics criteria and
classified into categories like time based QoS, size based QoS, combined QoS
and estimated attributes. Then we have designed service network monitoring
ontology (SNM). Our decision model will take user query and SNM as input,
measures the performance capabilities and suggests some new performance
configurations like selected service is not available, physical resource is not
available and no maintenance will be available for the selected service for
composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01494</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01494</id><created>2015-06-04</created><authors><author><keyname>Han</keyname><forenames>Rui</forenames></author><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Gao</keyname><forenames>Wanling</forenames></author><author><keyname>Tian</keyname><forenames>Xinhui</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author></authors><title>Benchmarking Big Data Systems: State-of-the-Art and Future Directions</title><categories>cs.PF</categories><comments>9 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1402.5194</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The great prosperity of big data systems such as Hadoop in recent years makes
the benchmarking of these systems become crucial for both research and industry
communities. The complexity, diversity, and rapid evolution of big data systems
gives rise to various new challenges about how we design generators to produce
data with the 4V properties (i.e. volume, velocity, variety and veracity), as
well as implement application-specific but still comprehensive workloads.
However, most of the existing big data benchmarks can be described as attempts
to solve specific problems in benchmarking systems. This article investigates
the state-of-the-art in benchmarking big data systems along with the future
challenges to be addressed to realize a successful and efficient benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01497</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01497</id><created>2015-06-04</created><updated>2016-01-06</updated><authors><author><keyname>Ren</keyname><forenames>Shaoqing</forenames></author><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal
  Networks</title><categories>cs.CV</categories><comments>Extended tech report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art object detection networks depend on region proposal
algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN
have reduced the running time of these detection networks, exposing region
proposal computation as a bottleneck. In this work, we introduce a Region
Proposal Network (RPN) that shares full-image convolutional features with the
detection network, thus enabling nearly cost-free region proposals. An RPN is a
fully convolutional network that simultaneously predicts object bounds and
objectness scores at each position. The RPN is trained end-to-end to generate
high-quality region proposals, which are used by Fast R-CNN for detection. We
further merge RPN and Fast R-CNN into a single network by sharing their
convolutional features---using the recently popular terminology of neural
networks with 'attention' mechanisms, the RPN component tells the unified
network where to look. For the very deep VGG-16 model, our detection system has
a frame rate of 5fps (including all steps) on a GPU, while achieving
state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS
COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015
competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning
entries in several tracks. Code has been made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01498</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01498</id><created>2015-06-04</created><updated>2015-06-10</updated><authors><author><keyname>Liu</keyname><forenames>Chunlei</forenames></author></authors><title>Gold type codes of higher relative dimension</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some new Gold type codes of higher relative dimension are introduced. Their
weight distribution is determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01499</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01499</id><created>2015-06-04</created><authors><author><keyname>Sureka</keyname><forenames>Ashish</forenames></author><author><keyname>Tripathi</keyname><forenames>Ambika</forenames></author><author><keyname>Dabral</keyname><forenames>Savita</forenames></author></authors><title>Survey Results on Threats To External Validity, Generalizability
  Concerns, Data Sharing and University-Industry Collaboration in Mining
  Software Repository (MSR) Research</title><categories>cs.SE</categories><acm-class>D.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining Software Repositories (MSR) is an applied and practise-oriented field
aimed at solving real problems encountered by practitioners and bringing value
to Industry. Replication of results and findings, generalizability and external
validity, University-Industry collaboration, data sharing and creation dataset
repositories are important issues in MSR research. Research consisting of
bibliometric analysis of MSR paper shows lack of University-Industry
collaboration, deficiency of studies on closed or propriety source dataset and
lack of data as well as tool sharing by researchers. We conduct a survey of
authors of past three years of MSR conference (2012, 2013 and 2014) to collect
data on their views and suggestions to address the stated concerns. We asked 20
questions from more than 100 authors and received a response from 39 authors.
Our results shows that about one-third of the respondents always make their
dataset publicly available and about one-third believe that data sharing should
be a mandatory condition for publication in MSR conferences. Our survey reveals
that more than 50% authors used solely open-source software (OSS) dataset for
their research. More than 50% of the respondents mentioned that difficulty in
sharing Industrial dataset outside the company is one of the major impediments
in University-Industry collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01501</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01501</id><created>2015-06-04</created><authors><author><keyname>Zarmehi</keyname><forenames>Nematollah</forenames></author><author><keyname>Banagar</keyname><forenames>Morteza</forenames></author><author><keyname>Akhaee</keyname><forenames>Mohammad Ali</forenames></author></authors><title>Optimum Decoder for an Additive Video Watermarking with Laplacian Noise
  in H.264</title><categories>cs.MM</categories><journal-ref>2013 10th International ISC Conference on Information Security and
  Cryptology (ISCISC),Aug. 2013, pp. 1-5</journal-ref><doi>10.1109/ISCISC.2013.6767352</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate an additive video watermarking method in H.264
standard in presence of the Laplacian noise. In some applications, due to the
loss of some pixels or a region of a frame, we resort to Laplacian noise rather
than Gaussian one. The embedding is performed in the transform domain; while an
optimum and a sub-optimum decoder are derived for the proposed Laplacian model.
Simulation results show that the proposed watermarking scheme has suitable
performance with enough transparency required for watermarking applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01509</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01509</id><created>2015-06-04</created><authors><author><keyname>Lopez-Pires</keyname><forenames>Fabio</forenames></author><author><keyname>Baran</keyname><forenames>Benjamin</forenames></author></authors><title>Virtual Machine Placement Literature Review</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing Datacenters host millions of virtual machines (VMs) on real
world scenarios. In this context, Virtual Machine Placement (VMP) is one of the
most challenging problems in cloud infrastructure management, considering also
the large number of possible optimization criteria and different formulations
that could be studied. VMP literature include relevant topics such as
energy-efficiency, Service Level Agreements (SLA), cloud service markets,
Quality of Service (QoS) and carbon dioxide emissions, all of them with high
economical and ecological impact. This work presents an extensive up-to-date
review of the most relevant VMP literature in order to identify research
opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01513</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01513</id><created>2015-06-04</created><updated>2015-09-24</updated><authors><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Social signals and algorithmic trading of Bitcoin</title><categories>cs.SI q-fin.TR</categories><comments>http://rsos.royalsocietypublishing.org/content/2/9/150288</comments><journal-ref>Royal Society Open Science, 2:150288, 2015</journal-ref><doi>10.1098/rsos.150288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of data on digital traces is growing to unprecedented sizes,
but inferring actionable knowledge from large-scale data is far from being
trivial. This is especially important for computational finance, where digital
traces of human behavior offer a great potential to drive trading strategies.
We contribute to this by providing a consistent approach that integrates
various datasources in the design of algorithmic traders. This allows us to
derive insights into the principles behind the profitability of our trading
strategies. We illustrate our approach through the analysis of Bitcoin, a
cryptocurrency known for its large price fluctuations. In our analysis, we
include economic signals of volume and price of exchange for USD, adoption of
the Bitcoin technology, and transaction volume of Bitcoin. We add social
signals related to information search, word of mouth volume, emotional valence,
and opinion polarization as expressed in tweets related to Bitcoin for more
than 3 years. Our analysis reveals that increases in opinion polarization and
exchange volume precede rising Bitcoin prices, and that emotional valence
precedes opinion polarization and rising exchange volumes. We apply these
insights to design algorithmic trading strategies for Bitcoin, reaching very
high profits in less than a year. We verify this high profitability with robust
statistical methods that take into account risk and trading costs, confirming
the long-standing hypothesis that trading based social media sentiment has the
potential to yield positive returns on investment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01516</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01516</id><created>2015-06-04</created><updated>2016-01-21</updated><authors><author><keyname>Godoy-Lorite</keyname><forenames>Antonia</forenames></author><author><keyname>Guimera</keyname><forenames>Roger</forenames></author><author><keyname>Sales-Pardo</keyname><forenames>Marta</forenames></author></authors><title>Long-term evolution of email networks: Statistical regularities,
  predictability and stability of social behaviors</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI physics.data-an</categories><journal-ref>PLoS ONE 11(1): e0146113 (2016)</journal-ref><doi>10.1371/journal.pone.0146113</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In social networks, individuals constantly drop ties and replace them by new
ones in a highly unpredictable fashion. This highly dynamical nature of social
ties has important implications for processes such as the spread of information
or of epidemics. Several studies have demonstrated the influence of a number of
factors on the intricate microscopic process of tie replacement, but the
macroscopic long-term effects of such changes remain largely unexplored. Here
we investigate whether, despite the inherent randomness at the microscopic
level, there are macroscopic statistical regularities in the long-term
evolution of social networks. In particular, we analyze the email network of a
large organization with over 1,000 individuals throughout four consecutive
years. We find that, although the evolution of individual ties is highly
unpredictable, the macro-evolution of social communication networks follows
well-defined statistical patterns, characterized by exponentially decaying
log-variations of the weight of social ties and of individuals' social
strength. At the same time, we find that individuals have social signatures and
communication strategies that are remarkably stable over the scale of several
years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01520</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01520</id><created>2015-06-04</created><updated>2015-12-15</updated><authors><author><keyname>van Rooyen</keyname><forenames>Brendan</forenames></author><author><keyname>Menon</keyname><forenames>Aditya Krishna</forenames></author><author><keyname>Williamson</keyname><forenames>Robert C.</forenames></author></authors><title>An Average Classification Algorithm</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many classification algorithms produce a classifier that is a weighted
average of kernel evaluations. When working with a high or infinite dimensional
kernel, it is imperative for speed of evaluation and storage issues that as few
training samples as possible are used in the kernel expansion. Popular existing
approaches focus on altering standard learning algorithms, such as the Support
Vector Machine, to induce sparsity, as well as post-hoc procedures for sparse
approximations. Here we adopt the latter approach. We begin with a very simple
classifier, given by the kernel mean $$ f(x) = \frac{1}{n}
\sum\limits_{i=i}^{n} y_i K(x_i,x) $$ We then find a sparse approximation to
this kernel mean via herding. The result is an accurate, easily parallelized
algorithm for learning classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01530</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01530</id><created>2015-06-04</created><authors><author><keyname>Akin</keyname><forenames>Sami</forenames></author><author><keyname>Fidler</keyname><forenames>Markus</forenames></author></authors><title>Backlog and Delay Reasoning in HARQ Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, hybrid-automatic-repeat-request (HARQ) systems have been favored in
particular state-of-the-art communications systems since they provide the
practicality of error detections and corrections aligned with repeat-requests
when needed at receivers. The queueing characteristics of these systems have
taken considerable focus since the current technology demands data
transmissions with a minimum delay provisioning. In this paper, we investigate
the effects of physical layer characteristics on data link layer performance in
a general class of HARQ systems. Constructing a state transition model that
combines queue activity at a transmitter and decoding efficiency at a receiver,
we identify the probability of clearing the queue at the transmitter and the
packet-loss probability at the receiver. We determine the effective capacity
that yields the maximum feasible data arrival rate at the queue under
quality-of-service constraints. In addition, we put forward non-asymptotic
backlog and delay bounds. Finally, regarding three different HARQ protocols,
namely Type-I HARQ, HARQ-chase combining (HARQ-CC) and HARQ-incremental
redundancy (HARQ-IR), we show the superiority of HARQ-IR in delay robustness
over the others. However, we further observe that the performance gap between
HARQ-CC and HARQ-IR is quite negligible in certain cases. The novelty of our
paper is a general cross-layer analysis of these systems, considering
encoding/decoding in the physical layer and delay aspects in the data-link
layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01544</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01544</id><created>2015-06-04</created><authors><author><keyname>Das</keyname><forenames>Jayanta Kumar</forenames></author><author><keyname>Choudhury</keyname><forenames>Pabitra Pal</forenames></author><author><keyname>Sahoo</keyname><forenames>Sudhakar</forenames></author></authors><title>Carry Value Transformation (CVT) - Exclusive OR (XOR) Tree and Its
  Significant Properties</title><categories>cs.DM</categories><comments>Pages: 5, Figure: 9</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CVT and XOR are two binary operations together used to calculate the sum of
two non-negative integers on using a recursive mechanism. In this present study
the convergence behaviors of this recursive mechanism has been captured through
a tree like structure named as CVT-XOR Tree. We have analyzed how to identify
the parent nodes, leaf nodes and internal nodes in the CVT-XOR Tree. We also
provide the parent information, depth information and the number of children of
a node in different CVT-XOR Trees on defining three different matrices. Lastly,
one observation is made towards very old Mathematical problem of Goldbach
Conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01565</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01565</id><created>2015-06-04</created><authors><author><keyname>Pignolet</keyname><forenames>Yvonne Anne</forenames></author><author><keyname>Roy</keyname><forenames>Matthieu</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author><author><keyname>Tredan</keyname><forenames>Gilles</forenames></author></authors><title>Exploring the Graph of Graphs: Network Evolution and Centrality
  Distances</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topological structure of complex networks has fascinated researchers for
several decades, and today we have a fairly good understanding of the types and
reoccurring characteristics of many different complex networks. However,
surprisingly little is known today about models to compare complex graphs, and
quantitatively measure their similarity and dynamics.
  In the past a variety of node centralities, i.e., functions which assign
values to nodes to represent their importance in the graph. Based on such
centralities we propose a natural similarity measure for complex networks: the
centrality distance $d_C$, the difference between two graphs with respect to a
given node centrality $C$. Centrality distances can take the specific roles of
the different nodes in the network into account, and have many interesting
applications.
  As a case study, we investigate the evolution of networks with respect to
centrality distances and their approximations based on closeness, betweenness,
pagerank, clustering and degree centrality. In particular we study the dynamic
behavior of six real-world social and physical networks and show that
centrality distances can be used to effectively distinguish between randomly
generated and actual evolutionary paths of dynamic complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01573</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01573</id><created>2015-06-04</created><authors><author><keyname>Williams</keyname><forenames>Lance R.</forenames></author></authors><title>Programs as Polypeptides</title><categories>cs.NE cs.ET cs.PL</categories><comments>in European Conference on Artificial Life (ECAL '15), York, UK, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a visual programming language for defining behaviors manifested
by reified actors in a 2D virtual world that can be compiled into programs
comprised of sequences of combinators that are themselves reified as actors.
This makes it possible to build programs that build programs from components of
a few fixed types delivered by diffusion using processes that resemble
chemistry as much as computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01596</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01596</id><created>2015-06-04</created><authors><author><keyname>Rajabi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Ghassemian</keyname><forenames>Hassan</forenames></author></authors><title>Multilayer Structured NMF for Spectral Unmixing of Hyperspectral Images</title><categories>cs.CV</categories><comments>4 pages, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the challenges in hyperspectral data analysis is the presence of mixed
pixels. Mixed pixels are the result of low spatial resolution of hyperspectral
sensors. Spectral unmixing methods decompose a mixed pixel into a set of
endmembers and abundance fractions. Due to nonnegativity constraint on
abundance fraction values, NMF based methods are well suited to this problem.
In this paper multilayer NMF has been used to improve the results of NMF
methods for spectral unmixing of hyperspectral data under the linear mixing
framework. Sparseness constraint on both spectral signatures and abundance
fractions matrices are used in this paper. Evaluation of the proposed algorithm
is done using synthetic and real datasets in terms of spectral angle and
abundance angle distances. Results show that the proposed algorithm outperforms
other previously proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01597</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01597</id><created>2015-06-04</created><updated>2015-06-05</updated><authors><author><keyname>Bing</keyname><forenames>Lidong</forenames></author><author><keyname>Li</keyname><forenames>Piji</forenames></author><author><keyname>Liao</keyname><forenames>Yi</forenames></author><author><keyname>Lam</keyname><forenames>Wai</forenames></author><author><keyname>Guo</keyname><forenames>Weiwei</forenames></author><author><keyname>Passonneau</keyname><forenames>Rebecca J.</forenames></author></authors><title>Abstractive Multi-Document Summarization via Phrase Selection and
  Merging</title><categories>cs.CL cs.AI</categories><comments>11 pages, 1 figure, accepted as a full paper at ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an abstraction-based multi-document summarization framework that
can construct new sentences by exploring more fine-grained syntactic units than
sentences, namely, noun/verb phrases. Different from existing abstraction-based
approaches, our method first constructs a pool of concepts and facts
represented by phrases from the input documents. Then new sentences are
generated by selecting and merging informative phrases to maximize the salience
of phrases and meanwhile satisfy the sentence construction constraints. We
employ integer linear optimization for conducting phrase selection and merging
simultaneously in order to achieve the global optimal solution for a summary.
Experimental results on the benchmark data set TAC 2011 show that our framework
outperforms the state-of-the-art models under automated pyramid evaluation
metric, and achieves reasonably well results on manual linguistic quality
evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01598</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01598</id><created>2015-06-04</created><updated>2015-06-17</updated><authors><author><keyname>Fourny</keyname><forenames>Ghislain</forenames></author></authors><title>decimalInfinite: All Decimals In Bits, No Loss, Same Order, Simple</title><categories>cs.MS</categories><comments>Technical report, 9 pages</comments><msc-class>97R50</msc-class><acm-class>E.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a binary encoding that supports arbitrarily large,
small and precise decimals. It completely preserves information and order. It
does not rely on any arbitrary use-case-based choice of calibration and is
readily implementable and usable, as is. Finally, it is also simple to explain
and understand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01602</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01602</id><created>2015-06-04</created><authors><author><keyname>Hadarean</keyname><forenames>Liana</forenames></author><author><keyname>Horn</keyname><forenames>Alex</forenames></author><author><keyname>King</keyname><forenames>Tim</forenames></author></authors><title>A Concurrency Problem with Exponential DPLL(T) Proofs</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many satisfiability modulo theories solvers implement a variant of the DPLL(T
) framework which separates theory-specific reasoning from reasoning on the
propositional abstraction of the formula. Such solvers conclude that a formula
is unsatisfiable once they have learned enough theory conflicts to derive a
propositional contradiction. However some problems, such as the diamonds
problem, require learning exponentially many conflicts. We give a general
criterion for establishing lower bounds on the number of theory conflicts in
any DPLL(T ) proof for a given problem. We apply our criterion to two different
state-of-the-art symbolic partial-order encodings of a simple, yet
representative concurrency problem. Even though one of the encodings is
asymptotically smaller than the other, we establish the same exponential lower
bound proof complexity for both. Our experiments confirm this theoretical lower
bound across multiple solvers and theory combinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01603</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01603</id><created>2015-06-04</created><authors><author><keyname>Courtieu</keyname><forenames>Pierre</forenames><affiliation>CEDRIC</affiliation></author><author><keyname>Rieg</keyname><forenames>Lionel</forenames><affiliation>NPA, LINCS, IUF, LIP6</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>NPA, LINCS, IUF, LIP6</affiliation></author><author><keyname>Urbain</keyname><forenames>Xavier</forenames><affiliation>ENSIIE, LRI, CEDRIC</affiliation></author></authors><title>A Certified Universal Gathering Algorithm for Oblivious Mobile Robots</title><categories>cs.DC cs.CG cs.DS cs.LO cs.RO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for the problem of universal gathering mobile
oblivious robots (that is, starting from any initial configuration that is not
bivalent, using any number of robots, the robots reach in a finite number of
steps the same position, not known beforehand) without relying on a common
chirality. We give very strong guaranties on the correctness of our algorithm
by proving formally that it is correct, using the COQ proof assistant. To our
knowledge, this is the first certified positive (and constructive) result in
the context of oblivious mobile robots. It demonstrates both the effectiveness
of the approach to obtain new algorithms that are truly generic, and its
managability since the amount of developped code remains human readable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01634</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01634</id><created>2015-06-04</created><authors><author><keyname>Babeanu</keyname><forenames>Alexandru-Ionut</forenames></author><author><keyname>Talman</keyname><forenames>Leandros</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author></authors><title>Structural Properties of Realistic Cultural Space Distributions</title><categories>physics.soc-ph cs.CY</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interesting sociophysical research problem consists of the compatibility
between collective social behavior in the short term and cultural diversity in
the long term. Recently, it has been shown that, when studying a model of short
term collective behavior in parallel with one of long term cultural diversity,
one is lead to the puzzling conclusion that the 2 aspects are mutually
exclusive. However, the compatibility is restored when switching from the
randomly generated cultural space distribution to an empirical one for
specifying the initial conditions in those models. This calls for understanding
the extent to which such a compatibility restoration is independent of the
empirical data set, as well as the relevant structural properties of such data.
Firstly, this work shows that the restoration patterns are largely robust
across data sets. Secondly, it provides a possible mechanism explaining the
restoration, for the special case when the cultural space is formulated only in
terms of nominal variables. The proposed model assumes that a realistic
distribution in cultural space is governed by the existence of several
&quot;cultural prototype&quot;, a hypothesis already used in previous work, provided that
every individual's sequence of cultural traits is a combination of the
sequences associated to the prototypes. This can be considered indirect
empirical evidence in favor of social science theories having inspired the
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01644</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01644</id><created>2015-06-04</created><authors><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>The Meta Distribution of the SIR in Poisson Bipolar and Cellular
  Networks</title><categories>cs.IT cs.NI math.IT math.PR</categories><comments>15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The calculation of the SIR distribution at the typical receiver (or,
equivalently, the success probability of transmissions over the typical link)
in Poisson bipolar and cellular networks with Rayleigh fading is relatively
straightforward, but it only provides limited information on the success
probabilities of the individual links. This paper introduces the notion of the
meta distribution of the SIR, which is the distribution of the conditional
success probability $P$ given the point process, and provides bounds, an exact
analytical expression, and a simple approximation for it. The meta distribution
provides fine-grained information on the SIR and answers questions such as
&quot;What fraction of users in a Poisson cellular network achieve 90% link
reliability if the required SIR is 5 dB?&quot;. Interestingly, in the bipolar model,
if the transmit probability $p$ is reduced while increasing the network density
$\lambda$ such that the density of concurrent transmitters $\lambda p$ stays
constant as $p\to 0$, $P$ degenerates to a constant, i.e., all links have
exactly the same success probability in the limit, which is the one of the
typical link. In contrast, in the cellular case, if the interfering base
stations are active independently with probability $p$, the variance of $P$
approaches a non-zero constant when $p$ is reduced to $0$ while keeping the
mean success probability constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01652</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01652</id><created>2015-06-04</created><authors><author><keyname>Giannopoulou</keyname><forenames>Archontia C.</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author></authors><title>Polynomial Fixed-Parameter Algorithms: A Case Study for Longest Path on
  Interval Graphs</title><categories>cs.CC</categories><comments>34 pages, 1 figure, 1 algorithm, 4 reduction rules</comments><acm-class>G.2.2; F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of fixed-parameter algorithms for problems already known
to be solvable in polynomial time. The main motivation is to get more efficient
algorithms for problems with unattractive polynomial running times. Here, we
focus on a fundamental graph problem: Longest Path; it is NP-hard in general
but known to be solvable in $O(n^4)$ time on $n$-vertex interval graphs. We
show how to solve Longest Path on interval graphs, parameterized by vertex
deletion number $k$ to proper interval graphs, in $O(k^{9} n)$ time. Notably,
Longest Path is trivially solvable in linear time on proper interval graphs,
and the parameter value $k$ can be approximated up to a factor of $4$ in linear
time. From a more general perspective, we believe that the idea of using
parameterized complexity analysis for polynomial-time solvable problems offers
a very fertile ground for future studies for all sorts of algorithmic problems.
It may enable a refined understanding of efficiency aspects for polynomial-time
solvable problems similarly to what classical parameterized complexity analysis
does for NP-hard problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01675</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01675</id><created>2015-06-04</created><authors><author><keyname>Rao</keyname><forenames>Ashwini</forenames></author><author><keyname>Schaub</keyname><forenames>Florian</forenames></author><author><keyname>Sadeh</keyname><forenames>Norman</forenames></author></authors><title>What do they know about me? Contents and Concerns of Online Behavioral
  Profiles</title><categories>cs.CY</categories><comments>in Ashwini Rao, Florian Schaub, and Norman Sadeh What do they know
  about me? Contents and Concerns of Online Behavioral Profiles (2014) ASE
  BigData/SocialInformatics/PASSAT/BioMedCom Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data aggregators collect large amount of information about individual users
and create detailed online behavioral profiles of individuals. Behavioral
profiles benefit users by improving products and services. However, they have
also raised concerns regarding user privacy, transparency of collection
practices and accuracy of data in the profiles. To improve transparency, some
companies are allowing users to access their behavioral profiles. In this work,
we investigated behavioral profiles of users by utilizing these access
mechanisms. Using in-person interviews (n=8), we analyzed the data shown in the
profiles, elicited user concerns, and estimated accuracy of profiles. We
confirmed our interview findings via an online survey (n=100). To assess the
claim of improving transparency, we compared data shown in profiles with the
data that companies have about users. More than 70% of the participants
expressed concerns about collection of sensitive data such as credit and health
information, level of detail and how their data may be used. We found a large
gap between the data shown in profiles and the data possessed by companies. A
large number of profiles were inaccurate with as much as 80% inaccuracy. We
discuss implications for public policy management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01684</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01684</id><created>2015-06-04</created><authors><author><keyname>Bauer</keyname><forenames>Martin</forenames></author><author><keyname>H&#xf6;tzer</keyname><forenames>Johannes</forenames></author><author><keyname>Steinmetz</keyname><forenames>Philipp</forenames></author><author><keyname>Jainta</keyname><forenames>Marcus</forenames></author><author><keyname>Berghoff</keyname><forenames>Marco</forenames></author><author><keyname>Schornbaum</keyname><forenames>Florian</forenames></author><author><keyname>Godenschwager</keyname><forenames>Christian</forenames></author><author><keyname>K&#xf6;stler</keyname><forenames>Harald</forenames></author><author><keyname>Nestler</keyname><forenames>Britta</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author></authors><title>Massively Parallel Phase-Field Simulations for Ternary Eutectic
  Directional Solidification</title><categories>cs.DC physics.comp-ph</categories><comments>submitted to Supercomputing 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microstructures forming during ternary eutectic directional solidification
processes have significant influence on the macroscopic mechanical properties
of metal alloys. For a realistic simulation, we use the well established
thermodynamically consistent phase-field method and improve it with a new grand
potential formulation to couple the concentration evolution. This extension is
very compute intensive due to a temperature dependent diffusive concentration.
We significantly extend previous simulations that have used simpler phase-field
models or were performed on smaller domain sizes. The new method has been
implemented within the massively parallel HPC framework waLBerla that is
designed to exploit current supercomputers efficiently. We apply various
optimization techniques, including buffering techniques, explicit SIMD kernel
vectorization, and communication hiding. Simulations utilizing up to 262,144
cores have been run on three different supercomputing architectures and weak
scalability results are shown. Additionally, a hierarchical, mesh-based data
reduction strategy is developed to keep the I/O problem manageable at scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01688</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01688</id><created>2015-06-04</created><authors><author><keyname>Berns</keyname><forenames>Andrew</forenames></author></authors><title>Avatar: A Time- and Space-Efficient Self-Stabilizing Overlay Network</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Overlay networks present an interesting challenge for fault-tolerant
computing. Many overlay networks operate in dynamic environments (e.g. the
Internet), where faults are frequent and widespread, and the number of
processes in a system may be quite large. Recently, self-stabilizing overlay
networks have been presented as a method for managing this complexity.
\emph{Self-stabilizing overlay networks} promise that, starting from any
weakly-connected configuration, a correct overlay network will eventually be
built. To date, this guarantee has come at a cost: nodes may either have high
degree during the algorithm's execution, or the algorithm may take a long time
to reach a legal configuration. In this paper, we present the first
self-stabilizing overlay network algorithm that does not incur this penalty.
Specifically, we (i) present a new locally-checkable overlay network based upon
a binary search tree, and (ii) provide a randomized algorithm for
self-stabilization that terminates in an expected polylogarithmic number of
rounds \emph{and} increases a node's degree by only a polylogarithmic factor in
expectation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01695</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01695</id><created>2015-06-04</created><authors><author><keyname>Das</keyname><forenames>Bireswar</forenames></author><author><keyname>Enduri</keyname><forenames>Murali Krishna</forenames></author><author><keyname>Reddy</keyname><forenames>I. Vinod</forenames></author></authors><title>Polynomial-time Algorithm for Isomorphism of Graphs with Clique-width at
  most 3</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The clique-width is a measure of complexity of decomposing graphs into
certain tree-like structures. The class of graphs with bounded clique-width
contains bounded tree-width graphs. While there are many results on the graph
isomorphism problem for bounded tree-width graphs, very little is known about
isomorphism of bounded clique-width graphs. We give the first polynomial-time
graph isomorphism algorithm for graphs with clique-width at most 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01698</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01698</id><created>2015-06-04</created><authors><author><keyname>Rohrbach</keyname><forenames>Anna</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>The Long-Short Story of Movie Description</title><categories>cs.CV cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating descriptions for videos has many applications including assisting
blind people and human-robot interaction. The recent advances in image
captioning as well as the release of large-scale movie description datasets
such as MPII Movie Description allow to study this task in more depth. Many of
the proposed methods for image captioning rely on pre-trained object classifier
CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating
descriptions. While image description focuses on objects, we argue that it is
important to distinguish verbs, objects, and places in the challenging setting
of movie description. In this work we show how to learn robust visual
classifiers from the weak annotations of the sentence descriptions. Based on
these visual classifiers we learn how to generate a description using an LSTM.
We explore different design choices to build and train the LSTM and achieve the
best performance to date on the challenging MPII-MD dataset. We compare and
analyze our approach and prior work along various dimensions to better
understand the key challenges of the movie description task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01701</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01701</id><created>2015-01-25</created><authors><author><keyname>Kalinovsky</keyname><forenames>Yakiv O.</forenames></author><author><keyname>Boyarinova</keyname><forenames>Yuliya E.</forenames></author><author><keyname>Khitsko</keyname><forenames>Iana V.</forenames></author></authors><title>Reversible Digital Filters Total Parametric Sensitivity Optimization
  using Non-canonical Hypercomplex Number Systems</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital filter construction method, which is optimal by parametric
sensitivity, based on using of non-canonical hypercomplex number systems is
proposed and investigated. It is shown that the use of non-canonical
hypercomplex number system with greater number of non-zero structure constants
in multiplication table can significantly improve the sensitivity of the
digital filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01709</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01709</id><created>2015-06-04</created><authors><author><keyname>Farrugia</keyname><forenames>Vincent E.</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>H&#xe9;ctor P.</forenames></author><author><keyname>Yannakakis</keyname><forenames>Georgios N.</forenames></author></authors><title>The Preference Learning Toolbox</title><categories>stat.ML cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preference learning (PL) is a core area of machine learning that handles
datasets with ordinal relations. As the number of generated data of ordinal
nature is increasing, the importance and role of the PL field becomes central
within machine learning research and practice. This paper introduces an open
source, scalable, efficient and accessible preference learning toolbox that
supports the key phases of the data training process incorporating various
popular data preprocessing, feature selection and preference learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01710</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01710</id><created>2015-06-04</created><authors><author><keyname>Bora</keyname><forenames>Dibya Jyoti</forenames></author><author><keyname>Gupta</keyname><forenames>Anil Kumar</forenames></author></authors><title>A Novel Approach Towards Clustering Based Image Segmentation</title><categories>cs.CV</categories><comments>5 pages, 7 figures, 1 table in International Journal of Emerging
  Science and Engineering, Volume-2 Issue-11, September 2014. arXiv admin note:
  text overlap with arXiv:1506.01472</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer vision, image segmentation is always selected as a major research
topic by researchers. Due to its vital rule in image processing, there always
arises the need of a better image segmentation method. Clustering is an
unsupervised study with its application in almost every field of science and
engineering. Many researchers used clustering in image segmentation process.
But still there requires improvement of such approaches. In this paper, a novel
approach for clustering based image segmentation is proposed. Here, we give
importance on color space and choose lab for this task. The famous hard
clustering algorithm K-means is used, but as its performance is dependent on
choosing a proper distance measure, so, we go for cosine distance measure. Then
the segmented image is filtered with sobel filter. The filtered image is
analyzed with marker watershed algorithm to have the final segmented result of
our original image. The MSE and PSNR values are evaluated to observe the
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01730</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01730</id><created>2015-06-04</created><authors><author><keyname>Larrosa</keyname><forenames>Juan M. C.</forenames></author></authors><title>Coauthorship and Thematic Networks in AAEP Annual Meetings</title><categories>cs.SI cs.DL</categories><comments>30 pages, 12 Figures, 16 Tables</comments><msc-class>91D30</msc-class><acm-class>J.4; H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the coauthorship production of the AAEP Annual Meeting since 1964.
We use social network analysis for creating coauthorship networks and given
that any paper must be tagged with two JEL codes, we use this information for
also structuring a thematic network. Then we calculate network metrics and find
main actors and clusters for coauthors and topics. We distinguish a gender gap
in the sample. Thematic networks show a cluster of codes and the analysis of
the cluster shows the preeminence of the tags related to trade, econometric,
distribution/poverty and health and education topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01732</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01732</id><created>2015-06-04</created><authors><author><keyname>Pillai</keyname><forenames>Sudeep</forenames></author><author><keyname>Leonard</keyname><forenames>John</forenames></author></authors><title>Monocular SLAM Supported Object Recognition</title><categories>cs.RO cs.CV</categories><comments>Accepted to appear at Robotics: Science and Systems 2015, Rome, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we develop a monocular SLAM-aware object recognition system
that is able to achieve considerably stronger recognition performance, as
compared to classical object recognition systems that function on a
frame-by-frame basis. By incorporating several key ideas including multi-view
object proposals and efficient feature encoding methods, our proposed system is
able to detect and robustly recognize objects in its environment using a single
RGB camera in near-constant time. Through experiments, we illustrate the
utility of using such a system to effectively detect and recognize objects,
incorporating multiple object viewpoint detections into a unified prediction
hypothesis. The performance of the proposed recognition system is evaluated on
the UW RGB-D Dataset, showing strong recognition performance and scalable
run-time performance compared to current state-of-the-art recognition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01739</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01739</id><created>2015-06-04</created><authors><author><keyname>Battistoni</keyname><forenames>Roberto</forenames></author><author><keyname>Di Pietro</keyname><forenames>Roberto</forenames></author><author><keyname>Lombardi</keyname><forenames>Flavio</forenames></author></authors><title>CloRoFor: Cloud Robust Forensics</title><categories>cs.DC cs.CR cs.NI</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The malicious alteration of machine time is a big challenge in computer
forensics. Detecting such changes and reconstructing the actual timeline of
events is of paramount importance. However, this can be difficult since the
attacker has many opportunities and means to hide such changes. In particular,
cloud computing, host and guest machine time can be manipulated in various ways
by an attacker. Guest virtual machines are especially vulnerable to attacks
coming from their (more privileged) host. As such, it is important to guarantee
the timeline integrity of both hosts and guests in a cloud, or at least to
ensure that the alteration of such timeline does not go undetected. In this
paper we survey the issues related to host and guest machine time integrity in
the cloud. Further, we describe a novel architecture for host and guest time
alteration detection and correction/resilience with respect to compromised
hosts and guests. The proposed framework has been implemented on an especially
built simulator. Collected results are evaluated and discussed. Performance
figures show the feasibility of our proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01743</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01743</id><created>2015-06-04</created><updated>2016-01-29</updated><authors><author><keyname>Moniz</keyname><forenames>Nuno</forenames></author><author><keyname>Torgo</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Eirinaki</keyname><forenames>Magdalini</forenames></author></authors><title>Socially Driven News Recommendation</title><categories>cs.IR</categories><comments>17 pages, 2 figures, submitted to the ACM Transactions on Intelligent
  Systems and Technology (ACM TIST), Special Issue on Social Media Processing</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The participatory Web has enabled the ubiquitous and pervasive access of
information, accompanied by an increase of speed and reach in information
sharing. Data dissemination services such as news aggregators are expected to
provide up-to-date, real-time information to the end users. News aggregators
are in essence recommendation systems that filter and rank news stories in
order to select the few that will appear on the users front screen at any time.
One of the main challenges in such systems is to address the recency and
latency problems, that is, to identify as soon as possible how important a news
story is. In this work we propose an integrated framework that aims at
predicting the importance of news items upon their publication with a focus on
recent and highly popular news, employing resampling strategies, and at
translating the result into concrete news rankings. We perform an extensive
experimental evaluation using real-life datasets of the proposed framework as
both a stand-alone system and when applied to news recommendations from Google
News. Additionally, we propose and evaluate a combinatorial solution to the
augmentation of official media recommendations with social information. Results
show that the proposed approach complements and enhances the news rankings
generated by state-of-the-art systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01744</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01744</id><created>2015-06-04</created><authors><author><keyname>Zhang</keyname><forenames>Chicheng</forenames></author><author><keyname>Song</keyname><forenames>Jimin</forenames></author><author><keyname>Chen</keyname><forenames>Kevin C</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author></authors><title>Spectral Learning of Large Structured HMMs for Comparative Epigenomics</title><categories>stat.ML cs.LG math.ST q-bio.GN stat.TH</categories><comments>27 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a latent variable model and an efficient spectral algorithm
motivated by the recent emergence of very large data sets of chromatin marks
from multiple human cell types. A natural model for chromatin data in one cell
type is a Hidden Markov Model (HMM); we model the relationship between multiple
cell types by connecting their hidden states by a fixed tree of known
structure. The main challenge with learning parameters of such models is that
iterative methods such as EM are very slow, while naive spectral methods result
in time and space complexity exponential in the number of cell types. We
exploit properties of the tree structure of the hidden states to provide
spectral algorithms that are more computationally efficient for current
biological datasets. We provide sample complexity bounds for our algorithm and
evaluate it experimentally on biological data from nine human cell types.
Finally, we show that beyond our specific model, some of our algorithmic ideas
can be applied to other graphical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01747</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01747</id><created>2015-06-04</created><authors><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author><author><keyname>Vorsanger</keyname><forenames>Gregory</forenames></author></authors><title>Weighted Sampling Without Replacement from Data Streams</title><categories>cs.DS</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted sampling without replacement has proved to be a very important tool
in designing new algorithms. Efraimidis and Spirakis (IPL 2006) presented an
algorithm for weighted sampling without replacement from data streams. Their
algorithm works under the assumption of precise computations over the interval
[0,1]. Cohen and Kaplan (VLDB 2008) used similar methods for their bottom-k
sketches.
  Efraimidis and Spirakis ask as an open question whether using finite
precision arithmetic impacts the accuracy of their algorithm. In this paper we
show a method to avoid this problem by providing a precise reduction from
k-sampling without replacement to k-sampling with replacement. We call the
resulting method Cascade Sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01749</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01749</id><created>2015-06-04</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Metric Dimension Parameterized by Max Leaf Number</title><categories>cs.DS</categories><comments>11 pages, 2 figures; to appear in J. Graph Algorithms &amp; Applications</comments><acm-class>F.2.2</acm-class><journal-ref>J. Graph Algorithms &amp; Applications 19 (1): 313-323, 2015</journal-ref><doi>10.7155/jgaa.00360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The metric dimension of a graph is the size of the smallest set of vertices
whose distances distinguish all pairs of vertices in the graph. We show that
this graph invariant may be calculated by an algorithm whose running time is
linear in the input graph size, added to a function of the largest possible
number of leaves in a spanning tree of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01753</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01753</id><created>2015-06-02</created><authors><author><keyname>Ali</keyname><forenames>Abdelmohsen</forenames></author><author><keyname>Hamouda</keyname><forenames>Walaa</forenames></author></authors><title>Low Power Wideband Sensing for One-Bit Quantized Cognitive Radio Systems</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposes an ultra low power wideband spectrum sensing architecture by
utilizing a one-bit quantization at the cognitive radio (CR) receiver. The
impact of this aggressive quantization is quantified and it is shown that the
proposed method is robust to low signal-to-noise ratios (SNR). We derive
closed-form expressions for both false alarm and detection probabilities. The
sensing performance and the analytical results are assessed through comparisons
with respective results from computer simulations. The results indicate that
the proposed method provides significant saving in power, complexity, and
sensing period on the account of an acceptable range of performance
degradation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01760</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01760</id><created>2015-06-04</created><authors><author><keyname>Ma</keyname><forenames>Yuchi</forenames></author><author><keyname>Yang</keyname><forenames>Ning</forenames></author><author><keyname>Li</keyname><forenames>Chuan</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Predicting Neighbor Distribution in Heterogeneous Information Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, considerable attention has been devoted to the prediction problems
arising from heterogeneous information networks. In this paper, we present a
new prediction task, Neighbor Distribution Prediction (NDP), which aims at
predicting the distribution of the labels on neighbors of a given node and is
valuable for many different applications in heterogeneous information networks.
The challenges of NDP mainly come from three aspects: the infinity of the state
space of a neighbor distribution, the sparsity of available data, and how to
fairly evaluate the predictions. To address these challenges, we first propose
an Evolution Factor Model (EFM) for NDP, which utilizes two new structures
proposed in this paper, i.e. Neighbor Distribution Vector (NDV) to represent
the state of a given node's neighbors, and Neighbor Label Evolution Matrix
(NLEM) to capture the dynamics of a neighbor distribution, respectively. We
further propose a learning algorithm for Evolution Factor Model. To overcome
the problem of data sparsity, the learning algorithm first clusters all the
nodes and learns an NLEM for each cluster instead of for each node. For fairly
evaluating the predicting results, we propose a new metric: Virtual Accuracy
(VA), which takes into consideration both the absolute accuracy and the
predictability of a node. Extensive experiments conducted on three real
datasets from different domains validate the effectiveness of our proposed
model EFM and metric VA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01762</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01762</id><created>2015-06-04</created><updated>2015-07-14</updated><authors><author><keyname>Ishii</keyname><forenames>Daisuke</forenames></author><author><keyname>Yonezaki</keyname><forenames>Naoki</forenames></author><author><keyname>Goldsztejn</keyname><forenames>Alexandre</forenames></author></authors><title>Monitoring Bounded LTL Properties Using Interval Analysis</title><categories>cs.LO cs.NA cs.SY</categories><comments>Appeared in NSV'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verification of temporal logic properties plays a crucial role in proving the
desired behaviors of hybrid systems. In this paper, we propose an interval
method for verifying the properties described by a bounded linear temporal
logic. We relax the problem to allow outputting an inconclusive result when
verification process cannot succeed with a prescribed precision, and present an
efficient and rigorous monitoring algorithm that demonstrates that the problem
is decidable. This algorithm performs a forward simulation of a hybrid
automaton, detects a set of time intervals in which the atomic propositions
hold, and validates the property by propagating the time intervals. A
continuous state at a certain time computed in each step is enclosed by an
interval vector that is proven to contain a unique solution. In the
experiments, we show that the proposed method provides a useful tool for formal
analysis of nonlinear and complex hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01769</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01769</id><created>2015-06-04</created><authors><author><keyname>Inkulu</keyname><forenames>Rajasekhar</forenames></author><author><keyname>Kapoor</keyname><forenames>Sanjiv</forenames></author></authors><title>Coresets of obstacles in approximating Euclidean shortest path amid
  convex obstacles</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set \cal{P} of non-intersecting convex obstacles, we find a sketch
\Omega of \cal{P} that helps in finding an obstacle avoiding approximate
Euclidean shortest path between two given points amid \mathcal{P} efficiently.
For both \mathbb{R}^2 and \mathbb{R}^3, we devise algorithms to compute a
(1+\epsilon)-approximate shortest path when two points s and t are given a
priori with the polygonal domain. Further, in \mathbb{R}^2, we devise an
algorithm to preprocess obstacles to output a (2+\epsilon)-approximate distance
between any given pair of query points. In \mathbb{R}^2, the data structures
constructed at the end of the preprocessing are improving the space complexity
of the known algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01780</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01780</id><created>2015-06-05</created><authors><author><keyname>Agarwal</keyname><forenames>Saurav</forenames></author><author><keyname>Tamjidi</keyname><forenames>Amirhossein</forenames></author><author><keyname>Chakravorty</keyname><forenames>Suman</forenames></author></authors><title>Motion Planning in Non-Gaussian Belief Spaces (M3P): The Case of a
  Kidnapped Robot</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning under uncertainty is a key requirement for physical systems due to
the noisy nature of actuators and sensors. Using a belief space approach,
planning solutions tend to generate actions that result in information seeking
behavior which reduce state uncertainty. While recent work has dealt with
planning for Gaussian beliefs, for many cases, a multi-modal belief is a more
accurate representation of the underlying belief. This is particularly true in
environments with information symmetry that cause uncertain data associations
which naturally lead to a multi-modal hypothesis on the state. Thus, a planner
cannot simply base actions on the most-likely state. We propose an algorithm
that uses a Receding Horizon Planning approach to plan actions that
sequentially disambiguate the multi-modal belief to a uni-modal Gaussian and
achieve tight localization on the true state, called a Multi-Modal Motion
Planner (M3P). By combining a Gaussian sampling-based belief space planner with
M3P, and introducing a switching behavior in the planner and belief
representation, we present a holistic end-to-end solution for the belief space
planning problem. Simulation results for a 2D ground robot navigation problem
are presented that demonstrate our method's performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01792</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01792</id><created>2015-06-05</created><updated>2015-08-18</updated><authors><author><keyname>Sommer</keyname><forenames>Philipp</forenames></author><author><keyname>Kusy</keyname><forenames>Branislav</forenames></author><author><keyname>Valencia</keyname><forenames>Philip</forenames></author><author><keyname>Dungavell</keyname><forenames>Ross</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author></authors><title>Delay-Tolerant Networking for Long-Term Animal Tracking</title><categories>cs.NI</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enabling Internet connectivity for mobile objects that do not have a
permanent home or regular movements is a challenge due to their varying energy
budget, intermittent wireless connectivity, and inaccessibility. We present a
hardware and software framework that offers robust data collection, adaptive
execution of sensing tasks, and flexible remote reconfiguration of devices
deployed on nomadic mobile objects such as animals. The framework addresses the
overall complexity through a multi-tier architecture with low tier devices
operating on a tight energy harvesting budget and high tier cloud services
offering seamless delay-tolerant presentation of data to end users. Based on
our multi-year experience of applying this framework to animal tracking and
monitoring applications, we present the main challenges that we have
encountered, the design of software building blocks that address these
challenges, and examples of the data we collected on flying foxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01799</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01799</id><created>2015-06-05</created><authors><author><keyname>Abboud</keyname><forenames>Amir</forenames></author><author><keyname>Williams</keyname><forenames>Virginia Vassilevska</forenames></author><author><keyname>Wang</keyname><forenames>Joshua</forenames></author></authors><title>Approximation and Fixed Parameter Subquadratic Algorithms for Radius and
  Diameter</title><categories>cs.DS</categories><comments>Submitted to FOCS 15 on April 2nd</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The radius and diameter are fundamental graph parameters. They are defined as
the minimum and maximum of the eccentricities in a graph, respectively, where
the eccentricity of a vertex is the largest distance from the vertex to another
node. In directed graphs, there are several versions of these problems. For
instance, one may choose to define the eccentricity of a node in terms of the
largest distance into the node, out of the node, the sum of the two directions
(i.e. roundtrip) and so on. All versions of diameter and radius can be solved
via solving all-pairs shortest paths (APSP), followed by a fast postprocessing
step. Solving APSP, however, on $n$-node graphs requires $\Omega(n^2)$ time
even in sparse graphs, as one needs to output $n^2$ distances.
  Motivated by known and new negative results on the impossibility of computing
these measures exactly in general graphs in truly subquadratic time, under
plausible assumptions, we search for \emph{approximation} and \emph{fixed
parameter subquadratic} algorithms, and for reasons why they do not exist.
  Our results include: - Truly subquadratic approximation algorithms for most
of the versions of Diameter and Radius with \emph{optimal} approximation
guarantees (given truly subquadratic time), under plausible assumptions. In
particular, there is a $2$-approximation algorithm for directed Radius with
one-way distances that runs in $\tilde{O}(m\sqrt{n})$ time, while a
$(2-\delta)$-approximation algorithm in $O(n^{2-\epsilon})$ time is unlikely. -
On graphs with treewidth $k$, we can solve the problems in
$2^{O(k\log{k})}n^{1+o(1)}$ time. We show that these algorithms are near
optimal since even a $(3/2-\delta)$-approximation algorithm that runs in time
$2^{o(k)}n^{2-\epsilon}$ would refute the plausible assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01808</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01808</id><created>2015-06-05</created><authors><author><keyname>Orhon</keyname><forenames>H. Eftun</forenames></author><author><keyname>Odabas</keyname><forenames>Caner</forenames></author><author><keyname>Uyanik</keyname><forenames>Ismail</forenames></author><author><keyname>Morgul</keyname><forenames>Omer</forenames></author><author><keyname>Saranli</keyname><forenames>Uluc</forenames></author></authors><title>Extending The Lossy Spring-Loaded Inverted Pendulum Model with a
  Slider-Crank Mechanism</title><categories>cs.RO</categories><comments>To appear in The 17th International Conference on Advanced Robotics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spring Loaded Inverted Pendulum (SLIP) model has a long history in describing
running behavior in animals and humans as well as has been used as a design
basis for robots capable of dynamic locomotion. Anchoring the SLIP for lossy
physical systems resulted in newer models which are extended versions of
original SLIP with viscous damping in the leg. However, such lossy models
require an additional mechanism for pumping energy to the system to control the
locomotion and to reach a limit-cycle. Some studies solved this problem by
adding an actively controllable torque actuation at the hip joint and this
actuation has been successively used in many robotic platforms, such as the
popular RHex robot. However, hip torque actuation produces forces on the COM
dominantly at forward direction with respect to ground, making height control
challenging especially at slow speeds. The situation becomes more severe when
the horizontal speed of the robot reaches zero, i.e. steady hoping without
moving in horizontal direction, and the system reaches to singularity in which
vertical degrees of freedom is completely lost. To this end, we propose an
extension of the lossy SLIP model with a slider-crank mechanism, SLIP- SCM,
that can generate a stable limit-cycle when the body is constrained to vertical
direction. We propose an approximate analytical solution to the nonlinear
system dynamics of SLIP- SCM model to characterize its behavior during the
locomotion. Finally, we perform a fixed-point stability analysis on SLIP-SCM
model using our approximate analytical solution and show that proposed model
exhibits stable behavior in our range of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01821</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01821</id><created>2015-06-05</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>On the quality of emergence in complex collective systems</title><categories>cs.OH</categories><comments>Paper submitted for publication for a book to be edited by Prof.
  Mohamed Nemiche and to be published by Springer (title and series still
  unknown)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of elements towards a classification of the quality of emergence in
emergent collective systems are provided. By using those elements, several
classes of emergent systems are exemplified, ranging from simple aggregations
of simple parts up to complex organizations of complex collective systems. In
so doing, the factors likely to play a a significant role in the persistence of
emergence and its opposite are highlighted. From this, new elements for
discussion are identified also considering elements from the System of Leibniz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01825</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01825</id><created>2015-06-05</created><authors><author><keyname>Harsiti</keyname></author><author><keyname>Munandar</keyname><forenames>Tb. Ai</forenames></author><author><keyname>Sigit</keyname><forenames>Haris Triono</forenames></author></authors><title>Implementation Of Fuzzy-C4.5 Classification As a Decision Support For
  Students Choice Of Major Specialization</title><categories>cs.CY</categories><comments>5 pages</comments><journal-ref>International Journal of Engineering Research &amp; Technology
  (IJERT), Vol. 2 Issue 11, November-2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determination of major specialization is important to lead the student to
focus on areas of study that are of interest as well as in accordance with its
academic credentials. Currently, the determination of major specialization is
done by asking directly to students, regardless of academic outcomes that have
been achieved in the previo us semester. This study discusses the development
of a hybrid model from fuzzy Mamdani and C4.5 algorithm to analyze the
determination of major specialization in informatics engineering courses of
Universities Raya Serang, where C4.5 algorithm is used as a shaper rule (rule)
which is used in the inference stage. Establishment of rules (decision tree)
performed using Weka applications, while for the determination of the decision
support analysis specialization majors using Mamdani fuzzy concept, the
application is done using the help of MATLAB. The results showed that 17 of the
126 students who either choose according to variable concentrations used in
this study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01829</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01829</id><created>2015-06-05</created><authors><author><keyname>Lajugie</keyname><forenames>R&#xe9;mi</forenames><affiliation>SIERRA, DI-ENS</affiliation></author><author><keyname>Bojanowski</keyname><forenames>Piotr</forenames><affiliation>WILLOW, DI-ENS</affiliation></author><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>SIERRA, DI-ENS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>SIERRA, DI-ENS</affiliation></author></authors><title>Semidefinite and Spectral Relaxations for Multi-Label Classification</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of multi-label classification. We
consider linear classifiers and propose to learn a prior over the space of
labels to directly leverage the performance of such methods. This prior takes
the form of a quadratic function of the labels and permits to encode both
attractive and repulsive relations between labels. We cast this problem as a
structured prediction one aiming at optimizing either the accuracies of the
predictors or the F 1-score. This leads to an optimization problem closely
related to the max-cut problem, which naturally leads to semidefinite and
spectral relaxations. We show on standard datasets how such a general prior can
improve the performances of multi-label techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01830</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01830</id><created>2015-06-05</created><updated>2015-06-09</updated><authors><author><keyname>Kiti&#x107;</keyname><forenames>Sr&#x111;an</forenames><affiliation>PANAMA</affiliation></author><author><keyname>Bertin</keyname><forenames>Nancy</forenames><affiliation>PANAMA</affiliation></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>PANAMA</affiliation></author></authors><title>Sparsity and cosparsity for audio declipping: a flexible non-convex
  approach</title><categories>cs.SD</categories><proxy>ccsd</proxy><journal-ref>LVA/ICA 2015 - The 12th International Conference on Latent
  Variable Analysis and Signal Separation, Aug 2015, Liberec, Czech Republic</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the empirical performance of the sparse synthesis
versus sparse analysis regularization for the ill-posed inverse problem of
audio declipping. We develop a versatile non-convex heuristics which can be
readily used with both data models. Based on this algorithm, we report that, in
most cases, the two models perform almost similarly in terms of signal
enhancement. However, the analysis version is shown to be amenable for real
time audio processing, when certain analysis operators are considered. Both
versions outperform state-of-the-art methods in the field, especially for the
severely saturated signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01853</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01853</id><created>2015-06-05</created><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Power Efficient and Secure Full-Duplex Wireless Communication Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, invited paper, IEEE Conference on Communications and Network
  Security (CNS) 2015 in Florence, Italy, on September 30, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study resource allocation for a full-duplex (FD) radio base
station serving multiple half-duplex (HD) downlink and uplink users
simultaneously. The considered resource allocation algorithm design is
formulated as a non-convex optimization problem taking into account minimum
required receive signal-to-interference-plus-noise ratios (SINRs) for downlink
and uplink communication and maximum tolerable SINRs at potential
eavesdroppers. The proposed optimization framework enables secure downlink and
uplink communication via artificial noise generation in the downlink for
interfering the potential eavesdroppers. We minimize the weighted sum of the
total downlink and uplink transmit power by jointly optimizing the downlink
beamformer, the artificial noise covariance matrix, and the uplink transmit
power. We adopt a semidefinite programming (SDP) relaxation approach to obtain
a tractable solution for the considered problem. The tightness of the SDP
relaxation is revealed by examining a sufficient condition for the global
optimality of the solution. Simulation results demonstrate the excellent
performance achieved by the proposed scheme and the significant transmit power
savings enabled optimization of the artificial noise covariance matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01864</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01864</id><created>2015-06-05</created><updated>2015-08-25</updated><authors><author><keyname>Yakovlev</keyname><forenames>Konstantin</forenames></author><author><keyname>Baskin</keyname><forenames>Egor</forenames></author><author><keyname>Hramoin</keyname><forenames>Ivan</forenames></author></authors><title>Grid-based angle-constrained path planning</title><categories>cs.AI</categories><comments>13 pages (12 pages: main text, 1 page: references), 7 figures, 20
  references, submitted 2015-June-22 to &quot;The 38 German Conference on Artificial
  Intelligence&quot; (KI-2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Square grids are commonly used in robotics and game development as spatial
models and well known in AI community heuristic search algorithms (such as A*,
JPS, Theta* etc.) are widely used for path planning on grids. A lot of research
is concentrated on finding the shortest (in geometrical sense) paths while in
many applications finding smooth paths (rather than the shortest ones but
containing sharp turns) is preferable. In this paper we study the problem of
generating smooth paths and concentrate on angle constrained path planning. We
put angle-constrained path planning problem formally and present a new
algorithm tailored to solve it - LIAN. We examine LIAN both theoretically and
empirically. We show that it is sound and complete (under some restrictions).
We also show that LIAN outperforms the analogues when solving numerous path
planning tasks within urban outdoor navigation scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01866</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01866</id><created>2015-06-05</created><authors><author><keyname>Stotz</keyname><forenames>David</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Characterizing degrees of freedom through additive combinatorics</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. on Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a formal connection between the problem of characterizing
degrees of freedom (DoF) in constant single-antenna interference channels
(ICs), with general channel matrix, and the field of additive combinatorics.
The theory we develop is based on a recent breakthrough result by Hochman in
fractal geometry. Our first main contribution is an explicit condition on the
channel matrix to admit full, i.e., $K/2$ DoF; this condition is satisfied for
almost all channel matrices. We also provide a construction of corresponding
DoF-optimal input distributions. The second main result is a new DoF-formula
exclusively in terms of Shannon entropies. This formula is more amenable to
both analytical statements and numerical evaluations than the DoF-formula by Wu
et al., which is in terms of R\'enyi information dimension. We then use the new
DoF-formula to shed light on the hardness of finding the exact number of DoF in
ICs with rational channel coefficients, and to improve the best known bounds on
the DoF of a well-studied channel matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01871</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01871</id><created>2015-06-05</created><authors><author><keyname>Chen</keyname><forenames>Yang</forenames></author><author><keyname>Cheng</keyname><forenames>Cheng</forenames></author><author><keyname>Sun</keyname><forenames>Qiyu</forenames></author></authors><title>Reconstruction of sparse wavelet signals from partial Fourier
  measurements</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2015.2478007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that high-dimensional sparse wavelet signals of finite
levels can be constructed from their partial Fourier measurements on a
deterministic sampling set with cardinality about a multiple of signal
sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01872</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01872</id><created>2015-06-05</created><authors><author><keyname>Fan</keyname><forenames>Jie</forenames></author></authors><title>Logics of Essence and Accident</title><categories>cs.LO</categories><comments>under submission. arXiv admin note: substantial text overlap with
  arXiv:1505.03950</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the literature, essence is formalized in two different ways, either de
dicto, or de re. Following \cite{Marcos:2005}, we adopt its de dicto
formalization: a formula is essential, if once it is true, it is necessarily
true; otherwise, it is accidental. In this article, we study the model theory
and axiomatization of the logic of essence and accident, i.e. the logic with
essence operator (or accident operator) as the only primitive modality. We show
that the logic of essence and accident is less expressive than modal logic on
non-reflexive models, but the two logics are equally expressive on reflexive
models. We prove that some frame properties are undefinable in the logic of
essence and accident, while some are. We propose the suitable bisimulation for
this logic, based on which we characterize the expressive power of this logic
within modal logic and within first-order logic. We axiomatize this logic over
various frame classes, among which the symmetric case is missing, and our
method is more suitable than those in the literature. We also find a method to
compute certain axioms used to axiomatize this logic over special frames in the
literature. As a side effect, we answer some open questions raised in
\cite{Marcos:2005}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01883</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01883</id><created>2015-06-05</created><authors><author><keyname>Xuan</keyname><forenames>Jifeng</forenames></author><author><keyname>Cornu</keyname><forenames>Benoit</forenames></author><author><keyname>Martinez</keyname><forenames>Matias</forenames></author><author><keyname>Baudry</keyname><forenames>Benoit</forenames></author><author><keyname>Seinturier</keyname><forenames>Lionel</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>Dynamic Analysis can be Improved with Automatic Test Suite Refactoring</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Developers design test suites to automatically verify that software
meets its expected behaviors. Many dynamic analysis techniques are performed on
the exploitation of execution traces from test cases. However, in practice,
there is only one trace that results from the execution of one manually-written
test case.
  Objective: In this paper, we propose a new technique of test suite
refactoring, called B-Refactoring. The idea behind B-Refactoring is to split a
test case into small test fragments, which cover a simpler part of the control
flow to provide better support for dynamic analysis.
  Method: For a given dynamic analysis technique, our test suite refactoring
approach monitors the execution of test cases and identifies small test cases
without loss of the test ability. We apply B-Refactoring to assist two existing
analysis tasks: automatic repair of if-statements bugs and automatic analysis
of exception contracts.
  Results: Experimental results show that test suite refactoring can
effectively simplify the execution traces of the test suite. Three real-world
bugs that could previously not be fixed with the original test suite are fixed
after applying B-Refactoring; meanwhile, exception contracts are better
verified via applying B-Refactoring to original test suites.
  Conclusions: We conclude that applying B-Refactoring can effectively improve
the purity of test cases. Existing dynamic analysis tasks can be enhanced by
test suite refactoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01900</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01900</id><created>2015-06-05</created><updated>2015-10-28</updated><authors><author><keyname>Arjevani</keyname><forenames>Yossi</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Communication Complexity of Distributed Convex Learning and Optimization</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental limits to communication-efficient distributed
methods for convex learning and optimization, under different assumptions on
the information available to individual machines, and the types of functions
considered. We identify cases where existing algorithms are already worst-case
optimal, as well as cases where room for further improvement is still possible.
Among other things, our results indicate that without similarity between the
local objective functions (due to statistical data similarity or otherwise)
many communication rounds may be required, even if the machines have unbounded
computational power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01906</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01906</id><created>2015-06-05</created><authors><author><keyname>Ibrahim</keyname><forenames>Hossam S.</forenames></author><author><keyname>Abdou</keyname><forenames>Sherif M.</forenames></author><author><keyname>Gheith</keyname><forenames>Mervat</forenames></author></authors><title>Idioms-Proverbs Lexicon for Modern Standard Arabic and Colloquial
  Sentiment Analysis</title><categories>cs.CL</categories><comments>arXiv admin note: text overlap with arXiv:1505.03105</comments><journal-ref>International Journal of Computer Applications 118(11):26-31, May
  2015</journal-ref><doi>10.5120/20790-3435</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although, the fair amount of works in sentiment analysis (SA) and opinion
mining (OM) systems in the last decade and with respect to the performance of
these systems, but it still not desired performance, especially for
morphologically-Rich Language (MRL) such as Arabic, due to the complexities and
challenges exist in the nature of the languages itself. One of these challenges
is the detection of idioms or proverbs phrases within the writer text or
comment. An idiom or proverb is a form of speech or an expression that is
peculiar to itself. Grammatically, it cannot be understood from the individual
meanings of its elements and can yield different sentiment when treats as
separate words. Consequently, In order to facilitate the task of detection and
classification of lexical phrases for automated SA systems, this paper presents
AIPSeLEX a novel idioms/ proverbs sentiment lexicon for modern standard Arabic
(MSA) and colloquial. AIPSeLEX is manually collected and annotated at sentence
level with semantic orientation (positive or negative). The efforts of manually
building and annotating the lexicon are reported. Moreover, we build a
classifier that extracts idioms and proverbs, phrases from text using n-gram
and similarity measure methods. Finally, several experiments were carried out
on various data, including Arabic tweets and Arabic microblogs (hotel
reservation, product reviews, and TV program comments) from publicly available
Arabic online reviews websites (social media, blogs, forums, e-commerce web
sites) to evaluate the coverage and accuracy of AIPSeLEX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01910</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01910</id><created>2015-06-05</created><authors><author><keyname>Sankhe</keyname><forenames>Kunal</forenames></author><author><keyname>Pradhan</keyname><forenames>Chandan</forenames></author><author><keyname>Kumar</keyname><forenames>Sumit</forenames></author><author><keyname>Ramamurthy</keyname><forenames>Garimella</forenames></author></authors><title>Machine Learning Based Cooperative Relay Selection in Virtual MIMO</title><categories>cs.NI</categories><comments>6 Pages, 8 figures, 3 tables, Accepted in Wireless Telecommunications
  Symposium 2015 and available in WTS 2015 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cellular systems, virtual multiple-input multiple-output (V-MIMO)
technology promises to achieve performance gains comparable to conventional
MIMO. In this paper, we propose cooperative relay selection algorithm based on
machine learning techniques. Willingness of user to cooperate in V-MIMO depends
on his current battery power, time and day along with incentives offered by
service provider. Every user has different criterion to participate in V-MIMO,
but allows a specific behavior pattern. Therefore, it is required to predict
willing users in the neighborhood of source user (SU), before selecting users
as cooperative nodes. Only inactive users belonging to Virtual Antenna Array
(VAA) cell of SU are assumed to cooperate. This reduces control overheads in
cooperative node discovery. In this paper, we employ prediction algorithm using
two machine learning techniques i.e. ANN and SVM to find out inactive willing
users within VAA cell. The parameters such as MSE, accuracy, precision and
recall are calculated to evaluate performance of ANN and SVM model. Prediction
using ANN has MSE of 3% with average accuracy of 97% (variance 0.37), whereas
SVM has MSE of 2.58% with average accuracy of 97.56% (variance 0.17). We also
observe that proposed prediction method reduces the node discovery time by
approximately 29%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01911</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01911</id><created>2015-06-05</created><updated>2016-02-10</updated><authors><author><keyname>Pigou</keyname><forenames>Lionel</forenames></author><author><keyname>Oord</keyname><forenames>A&#xe4;ron van den</forenames></author><author><keyname>Dieleman</keyname><forenames>Sander</forenames></author><author><keyname>Van Herreweghe</keyname><forenames>Mieke</forenames></author><author><keyname>Dambre</keyname><forenames>Joni</forenames></author></authors><title>Beyond Temporal Pooling: Recurrence and Temporal Convolutions for
  Gesture Recognition in Video</title><categories>cs.CV cs.AI cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have demonstrated the power of recurrent neural networks for
machine translation, image captioning and speech recognition. For the task of
capturing temporal structure in video, however, there still remain numerous
open research questions. Current research suggests using a simple temporal
feature pooling strategy to take into account the temporal aspect of video. We
demonstrate that this method is not sufficient for gesture recognition, where
temporal information is more discriminative compared to general video
classification tasks. We explore deep architectures for gesture recognition in
video and propose a new end-to-end trainable neural network architecture
incorporating temporal convolutions and bidirectional recurrence. Our main
contributions are twofold; first, we show that recurrence is crucial for this
task; second, we show that adding temporal convolutions leads to significant
improvements. We evaluate the different approaches on the Montalbano gesture
recognition dataset, where we achieve state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01914</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01914</id><created>2015-06-05</created><authors><author><keyname>Laxstr&#xf6;m</keyname><forenames>Niklas</forenames></author><author><keyname>Giner</keyname><forenames>Pau</forenames></author><author><keyname>Thottingal</keyname><forenames>Santhosh</forenames></author></authors><title>Content Translation: Computer-assisted translation tool for Wikipedia
  articles</title><categories>cs.CL</categories><comments>EAMT 2015 user study</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The quality and quantity of articles in each Wikipedia language varies
greatly. Translating from another Wikipedia is a natural way to add more
content, but the translation process is not properly supported in the software
used by Wikipedia. Past computer-assisted translation tools built for Wikipedia
are not commonly used. We created a tool that adapts to the specific needs of
an open community and to the kind of content in Wikipedia. Qualitative and
quantitative data indicates that the new tool helps users translate articles
easier and faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01929</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01929</id><created>2015-06-05</created><updated>2015-09-27</updated><authors><author><keyname>Weinzaepfel</keyname><forenames>Philippe</forenames></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>Learning to track for spatio-temporal action localization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an effective approach for spatio-temporal action localization in
realistic videos. The approach first detects proposals at the frame-level and
scores them with a combination of static and motion CNN features. It then
tracks high-scoring proposals throughout the video using a
tracking-by-detection approach. Our tracker relies simultaneously on
instance-level and class-level detectors. The tracks are scored using a
spatio-temporal motion histogram, a descriptor at the track level, in
combination with the CNN features. Finally, we perform temporal localization of
the action using a sliding-window approach at the track level. We present
experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB
and UCF-101 action localization datasets, where our approach outperforms the
state of the art with a margin of 15%, 7% and 12% respectively in mAP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01930</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01930</id><created>2015-06-05</created><authors><author><keyname>Kaminski</keyname><forenames>Benjamin Lucien</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author></authors><title>On the Hardness of Almost-Sure Termination</title><categories>cs.LO cs.CC</categories><comments>MFCS 2015. arXiv admin note: text overlap with arXiv:1410.7225</comments><acm-class>F.1.2; F.1.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the computational hardness of computing expected
outcomes and deciding (universal) (positive) almost-sure termination of
probabilistic programs. It is shown that computing lower and upper bounds of
expected outcomes is $\Sigma_1^0$- and $\Sigma_2^0$-complete, respectively.
Deciding (universal) almost-sure termination as well as deciding whether the
expected outcome of a program equals a given rational value is shown to be
$\Pi^0_2$-complete. Finally, it is shown that deciding (universal) positive
almost-sure termination is $\Sigma_2^0$-complete ($\Pi_3^0$-complete).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01939</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01939</id><created>2014-12-23</created><authors><author><keyname>Mansour</keyname><forenames>Abdelmajid Hassan</forenames></author><author><keyname>Salh</keyname><forenames>Gafar Zen Alabdeen</forenames></author><author><keyname>Alhalemi</keyname><forenames>Ali Shaif</forenames></author></authors><title>Facial Expressions recognition Based on Principal Component Analysis
  (PCA)</title><categories>cs.CV</categories><comments>6 pages, 13 figures, 9 tables, Volume 18 Number 5; Dec 2014; pp.
  188-193</comments><doi>10.14445/22312803/IJCTT-V18P143</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The facial expression recognition is an ocular task that can be performed
without human discomfort, is really a speedily growing on the computer research
field. There are many applications and programs uses facial expression to
evaluate human character, judgment, feelings, and viewpoint. The process of
recognizing facial expression is a hard task due to the several circumstances
such as facial occlusions, face shape, illumination, face colors, and etc. This
paper present a PCA methodology to distinguish expressions of faces under
different circumstances and identifying it. Relies on Eigen faces technique
using standard Data base images. So as to overcome the problem of difficulty to
computers to identify the features and expressions of persons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01943</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01943</id><created>2015-06-05</created><authors><author><keyname>Zakaria</keyname><forenames>Sakyoud</forenames></author><author><keyname>Rey</keyname><forenames>Ga&#xeb;tan</forenames></author><author><keyname>Mohamed</keyname><forenames>Eladnani</forenames></author><author><keyname>Lavirotte</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Abdelaziz</keyname><forenames>El Fazziki</forenames></author><author><keyname>Tigli</keyname><forenames>Jean-Yves</forenames></author></authors><title>Smart Geographic object: Toward a new understanding of GIS Technology in
  Ubiquitous Computing</title><categories>cs.HC</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Volume 12,
  Issue 2, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the fundamental aspects of ubiquitous computing is the instrumentation
of the real world by smart devices. This instrumentation constitutes an
opportunity to rethink the interactions between human beings and their
environment on the one hand, and between the components of this environment on
the other. In this paper we discuss what this understanding of ubiquitous
computing can bring to geographic science and particularly to GIS technology.
Our main idea is the instrumentation of the geographic environment through the
instrumentation of geographic objects composing it. And then investigate how
this instrumentation can meet the current limitations of GIS technology, and
offers a new stage of rapprochement between the earth and its abstraction. As
result, the current research work proposes a new concept we named Smart
Geographic Object SGO. The latter is a convergence point between the smart
objects and geographic objects, two concepts appertaining respectively to t
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01946</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01946</id><created>2015-06-05</created><authors><author><keyname>Joy</keyname><forenames>Joshua</forenames></author><author><keyname>Yu</keyname><forenames>Yu-Ting</forenames></author><author><keyname>Perez</keyname><forenames>Victor</forenames></author><author><keyname>Lu</keyname><forenames>Dennis</forenames></author><author><keyname>Gerla</keyname><forenames>Mario</forenames></author></authors><title>A New Approach to Coding in Content Based MANETs</title><categories>cs.NI</categories><doi>10.12720/jcm.9.8.588-596</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In content-based mobile ad hoc networks (CB-MANETs), random linear network
coding (NC) can be used to reliably disseminate large files under intermittent
connectivity. Conventional NC involves random unrestricted coding at
intermediate nodes. This however is vulnerable to pollution attacks. To avoid
attacks, a brute force approach is to restrict the mixing at the source.
However, source restricted NC generally reduces the robustness of the code in
the face of errors, losses and mobility induced intermittence. CB-MANETs
introduce a new option. Caching is common in CB MANETs and a fully reassembled
cached file can be viewed as a new source. Thus, NC packets can be mixed at all
sources (including the originator and the intermediate caches) yet still
providing protection from pollution. The hypothesis we wish to test in this
paper is whether in CB-MANETs with sufficient caches of a file, the performance
(in terms of robustness) of the restricted coding equals that of unrestricted
coding.
  In this paper, we examine and compare unrestricted coding to full cache
coding, source only coding, and no coding. As expected, we find that full cache
coding remains competitive with unrestricted coding while maintaining full
protection against pollution attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01949</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01949</id><created>2015-06-05</created><authors><author><keyname>Danner</keyname><forenames>Norman</forenames></author><author><keyname>Licata</keyname><forenames>Daniel R.</forenames></author><author><keyname>Ramyaa</keyname><forenames>Ramyaa</forenames></author></authors><title>Denotational cost semantics for functional languages with inductive
  types</title><categories>cs.PL</categories><comments>To appear in ICFP 2015</comments><acm-class>F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central method for analyzing the asymptotic complexity of a functional
program is to extract and then solve a recurrence that expresses evaluation
cost in terms of input size. The relevant notion of input size is often
specific to a datatype, with measures including the length of a list, the
maximum element in a list, and the height of a tree. In this work, we give a
formal account of the extraction of cost and size recurrences from higher-order
functional programs over inductive datatypes. Our approach allows a wide range
of programmer-specified notions of size, and ensures that the extracted
recurrences correctly predict evaluation cost. To extract a recurrence from a
program, we first make costs explicit by applying a monadic translation from
the source language to a complexity language, and then abstract datatype values
as sizes. Size abstraction can be done semantically, working in models of the
complexity language, or syntactically, by adding rules to a preorder judgement.
We give several different models of the complexity language, which support
different notions of size. Additionally, we prove by a logical relations
argument that recurrences extracted by this process are upper bounds for
evaluation cost; the proof is entirely syntactic and therefore applies to all
of the models we consider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01952</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01952</id><created>2015-05-23</created><authors><author><keyname>Kokabifar</keyname><forenames>E.</forenames></author><author><keyname>Loghmani</keyname><forenames>G. B.</forenames></author><author><keyname>Latif</keyname><forenames>A.</forenames></author></authors><title>Digital image watermarking using normal matrices</title><categories>math.NA cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents techniques for digital image watermarking based on
eigenvalue decomposition of normal matrices. The introduced methods are
convenient and self-explanatory, achieve satisfactory results, as well as
require less and easy computations compared to some current methods. Through
the proposed methods, host images and watermarks are transformed to the space
of normal matrices, and the properties of spectral decompositions are dealt
with to obtain watermarked images. Watermark extraction is carried out via a
procedure similar to embedding. Experimental results are provided to illustrate
the reliability and robustness of the methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01955</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01955</id><created>2015-06-05</created><authors><author><keyname>Dougherty</keyname><forenames>Steven T.</forenames></author><author><keyname>Kim</keyname><forenames>Jon-Lark</forenames></author><author><keyname>Ozkaya</keyname><forenames>Buket</forenames></author><author><keyname>Sok</keyname><forenames>Lin</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>The combinatorics of LCD codes: Linear Programming bound and orthogonal
  matrices</title><categories>cs.IT math.IT</categories><comments>submitted to Linear Algebra and Applications on June, 1, 2015</comments><msc-class>94B05, 20H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Complementary Dual codes (LCD) are binary linear codes that meet their
dual trivially. We construct LCD codes using orthogonal matrices, self-dual
codes, combinatorial designs and Gray map from codes over the family of rings
$R_k$. We give a linear programming bound on the largest size of an LCD code of
given length and minimum distance. We make a table of lower bounds for this
combinatorial function for modest values of the parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01959</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01959</id><created>2015-06-05</created><updated>2015-12-22</updated><authors><author><keyname>Lee</keyname><forenames>Namgil</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Regularized Computation of Approximate Pseudoinverse of Large Matrices
  Using Low-Rank Tensor Train Decompositions</title><categories>math.NA cs.NA</categories><comments>28 pages</comments><msc-class>15A09, 65F08, 65F20, 65F22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for low-rank approximation of Moore-Penrose
pseudoinverses (MPPs) of large-scale matrices using tensor networks. The
computed pseudoinverses can be useful for solving or preconditioning of
large-scale overdetermined or underdetermined systems of linear equations. The
computation is performed efficiently and stably based on the modified
alternating least squares (MALS) scheme using low-rank tensor train (TT)
decompositions and tensor network contractions. The formulated large-scale
optimization problem is reduced to sequential smaller-scale problems for which
any standard and stable algorithms can be applied. Regularization technique is
incorporated in order to alleviate ill-posedness and obtain robust low-rank
approximations. Numerical simulation results illustrate that the regularized
pseudoinverses of a wide class of non-square or nonsymmetric matrices admit
good approximate low-rank TT representations. Moreover, we demonstrated that
the computational cost of the proposed method is only logarithmic in the matrix
size given that the TT-ranks of a data matrix and its approximate pseudoinverse
are bounded. It is illustrated that a strongly nonsymmetric
convection-diffusion problem can be efficiently solved by using the
preconditioners computed by the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01965</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01965</id><created>2015-06-04</created><authors><author><keyname>Lebre</keyname><forenames>Marie-Ange</forenames><affiliation>CITI, VALEO</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CITI</affiliation></author><author><keyname>M&#xe9;nard</keyname><forenames>Eric</forenames><affiliation>VALEO</affiliation></author><author><keyname>Garnault</keyname><forenames>Alexandre</forenames><affiliation>VALEO</affiliation></author><author><keyname>Brada&#xef;</keyname><forenames>Benazouz</forenames><affiliation>VALEO</affiliation></author><author><keyname>Picron</keyname><forenames>Vanessa</forenames><affiliation>VALEO</affiliation></author></authors><title>Real scenario and simulations on GLOSA traffic light system for reduced
  CO2 emissions, waiting time and travel time</title><categories>cs.MA</categories><comments>in 22nd ITS World Congress, Oct 2015, Bordeaux, France. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative ITS is enabling vehicles to communicate with the infrastructure
to provide improvements in traffic control. A promising approach consists in
anticipating the road profile and the upcoming dynamic events like traffic
lights. This topic has been addressed in the French public project Co-Drive
through functions developed by Valeo named Green Light Optimal Speed Advisor
(GLOSA). The system advises the optimal speed to pass the next traffic light
without stopping. This paper presents results of its performance in different
scenarios through simulations and real driving measurements. A scaling is done
in an urban area, with different penetration rates in vehicle and
infrastructure equipment for vehicular communication. Our simulation results
indicate that GLOSA can reduce CO2 emissions, waiting time and travel time,
both in experimental conditions and in real traffic conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01966</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01966</id><created>2015-06-05</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Ricciutelli</keyname><forenames>Giacomo</forenames></author><author><keyname>Maturo</keyname><forenames>Nicola</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>Performance assessment and design of finite length LDPC codes for the
  Gaussian wiretap channel</title><categories>cs.IT cs.CR math.IT</categories><comments>6 pages, 3 figures, IEEE ICC 2015 - Workshop on Wireless Physical
  Layer Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the reliability and secrecy performance achievable by
practical LDPC codes over the Gaussian wiretap channel. While several works
have already addressed this problem in asymptotic conditions, i.e., under the
hypothesis of codewords of infinite length, only a few approaches exist for the
finite length regime. We propose an approach to measure the performance of
practical codes and compare it with that achievable in asymptotic conditions.
Moreover, based on the secrecy metrics we adopt to achieve this target, we
propose a code optimization algorithm which allows to design irregular LDPC
codes able to approach the ultimate performance limits even at moderately small
codeword lengths (in the order of 10000 bits).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01971</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01971</id><created>2015-06-05</created><authors><author><keyname>G&#xfc;neri</keyname><forenames>Cem</forenames></author><author><keyname>&#xd6;zkaya</keyname><forenames>Buket</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>Quasi-Cyclic Complementary Dual Code</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LCD codes are linear codes that intersect with their dual trivially. Quasi
cyclic codes that are LCD are characterized and studied by using their
concatenated structure. Some asymptotic results are derived. Hermitian LCD
codes are introduced to that end and their cyclic subclass is characterized.
Constructions of QCCD codes from codes over larger alphabets are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01972</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01972</id><created>2015-06-05</created><updated>2016-02-05</updated><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Yuan</keyname><forenames>Yang</forenames></author></authors><title>Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives</title><categories>cs.LG cs.DS math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many classical algorithms are found until several years later to outlive the
confines in which they were conceived, and continue to be relevant in
unforeseen settings. In this paper, we show that SVRG is one such method:
originally designed for strongly convex objectives, is also very robust under
non-strongly convex or sum-of-non-convex settings.
  If $f(x)$ is a sum of smooth, convex functions but $f$ is not strongly convex
(such as Lasso or logistic regression), we propose a variant SVRG++ that makes
a novel choice of growing epoch length on top of SVRG. SVRG++ is a direct,
faster variant of SVRG in this setting.
  If $f(x)$ is a sum of non-convex functions but $f$ is strongly convex, we
show that the convergence of SVRG linearly depends on the non-convexity
parameter of the summands. This improves the best known result in this setting,
and gives better running time for stochastic PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01973</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01973</id><created>2015-06-05</created><updated>2015-06-10</updated><authors><author><keyname>Kim</keyname><forenames>Jinha</forenames><affiliation>POSTECH, South Korea</affiliation><affiliation>Oracle Labs, USA</affiliation></author><author><keyname>Shin</keyname><forenames>Hyungyu</forenames><affiliation>POSTECH, South Korea</affiliation></author><author><keyname>Han</keyname><forenames>Wook-Shin</forenames><affiliation>POSTECH, South Korea</affiliation></author><author><keyname>Hong</keyname><forenames>Sungpack</forenames><affiliation>Oracle Labs, USA</affiliation></author><author><keyname>Chafi</keyname><forenames>Hassan</forenames><affiliation>Oracle Labs, USA</affiliation></author></authors><title>Taming Subgraph Isomorphism for RDF Query Processing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RDF data are used to model knowledge in various areas such as life sciences,
Semantic Web, bioinformatics, and social graphs. The size of real RDF data
reaches billions of triples. This calls for a framework for efficiently
processing RDF data. The core function of processing RDF data is subgraph
pattern matching. There have been two completely different directions for
supporting efficient subgraph pattern matching. One direction is to develop
specialized RDF query processing engines exploiting the properties of RDF data
for the last decade, while the other direction is to develop efficient subgraph
isomorphism algorithms for general, labeled graphs for over 30 years. Although
both directions have a similar goal (i.e., finding subgraphs in data graphs for
a given query graph), they have been independently researched without clear
reason. We argue that a subgraph isomorphism algorithm can be easily modified
to handle the graph homomorphism, which is the RDF pattern matching semantics,
by just removing the injectivity constraint. In this paper, based on the
state-of-the-art subgraph isomorphism algorithm, we propose an in-memory
solution, TurboHOM++, which is tamed for the RDF processing, and we compare it
with the representative RDF processing engines for several RDF benchmarks in a
server machine where billions of triples can be loaded in memory. In order to
speed up TurboHOM++, we also provide a simple yet effective transformation and
a series of optimization techniques. Extensive experiments using several RDF
benchmarks show that TurboHOM++ consistently and significantly outperforms the
representative RDF engines. Specifically, TurboHOM++ outperforms its
competitors by up to five orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01976</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01976</id><created>2015-06-05</created><authors><author><keyname>Peters</keyname><forenames>Jeffrey R.</forenames></author><author><keyname>Surana</keyname><forenames>Amit</forenames></author><author><keyname>Bertuccelli</keyname><forenames>Luca</forenames></author></authors><title>Eye-Tracking Metrics for Task-Based Supervisory Control</title><categories>cs.HC stat.AP</categories><comments>6 pages, 4 figures, 1 table</comments><acm-class>H.1.2; H.5.2; I.2.9; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task-based, rather than vehicle-based, control architectures have been shown
to provide superior performance in certain human supervisory control missions.
These results motivate the need for the development of robust, reliable
usability metrics to aid in creating interfaces for use in this domain. To this
end, we conduct a pilot usability study of a particular task-based supervisory
control interface called the Research Environment for Supervisory Control of
Heterogenous Unmanned Vehicles (RESCHU). In particular, we explore the use of
eye-tracking metrics as an objective means of evaluating the RESCHU interface
and providing guidance in improving usability. Our main goals for this study
are to 1) better understand how eye-tracking can augment standard usability
metrics, 2) formulate initial models of operator behavior, and 3) identify
interesting areas of future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01977</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01977</id><created>2015-06-05</created><authors><author><keyname>Glenski</keyname><forenames>Maria</forenames></author><author><keyname>Johnston</keyname><forenames>Thomas J.</forenames></author><author><keyname>Weninger</keyname><forenames>Tim</forenames></author></authors><title>Random Voting Effects in Social-Digital Spaces: A case study of Reddit
  Post Submissions</title><categories>cs.SI cs.CY cs.MM</categories><comments>Paper preprint accepted to 2015 ACM Hypertext Conference</comments><acm-class>H.1.2; H.5.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  At a time when information seekers first turn to digital sources for news and
opinion, it is critical that we understand the role that social media plays in
human behavior. This is especially true when information consumers also act as
information producers and editors by their online activity. In order to better
understand the effects that editorial ratings have on online human behavior, we
report the results of a large-scale in-vivo experiment in social media. We find
that small, random rating manipulations on social media submissions created
significant changes in downstream ratings resulting in significantly different
final outcomes. Positive treatment resulted in a positive effect that increased
the final rating by 11.02% on average. Compared to the control group, positive
treatment also increased the probability of reaching a high rating (&gt;=2000) by
24.6%. Contrary to the results of related work we also find that negative
treatment resulted in a negative effect that decreased the final rating by
5.15% on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.01978</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.01978</id><created>2015-06-05</created><authors><author><keyname>Gallotti</keyname><forenames>Riccardo</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author></authors><title>Information measures and cognitive limits in multilayer navigation</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>16 pages+9 pages of supplementary material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cities and their transportation systems become increasingly complex and
multimodal as they grow, and it is natural to wonder if it is possible to
quantitatively characterize our difficulty to navigate in them and whether such
navigation exceeds our cognitive limits. A transition between different
searching strategies for navigating in metropolitan maps has been observed for
large, complex metropolitan networks. This evidence suggests the existence of
another limit associated to the cognitive overload and caused by large amounts
of information to process. In this light, we analyzed the world's 15 largest
metropolitan networks and estimated the information limit for determining a
trip in a transportation system to be on the order of 8 bits. Similar to the
&quot;Dunbar number,&quot; which represents a limit to the size of an individual's
friendship circle, our cognitive limit suggests that maps should not consist of
more than about $250$ connections points to be easily readable. We also show
that including connections with other transportation modes dramatically
increases the information needed to navigate in multilayer transportation
networks: in large cities such as New York, Paris, and Tokyo, more than $80\%$
of trips are above the 8-bit limit. Multimodal transportation systems in large
cities have thus already exceeded human cognitive limits and consequently the
traditional view of navigation in cities has to be revised substantially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02004</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02004</id><created>2015-06-05</created><authors><author><keyname>Faruqui</keyname><forenames>Manaal</forenames></author><author><keyname>Tsvetkov</keyname><forenames>Yulia</forenames></author><author><keyname>Yogatama</keyname><forenames>Dani</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Smith</keyname><forenames>Noah</forenames></author></authors><title>Sparse Overcomplete Word Vector Representations</title><categories>cs.CL</categories><comments>Proceedings of ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current distributed representations of words show little resemblance to
theories of lexical semantics. The former are dense and uninterpretable, the
latter largely based on familiar, discrete classes (e.g., supersenses) and
relations (e.g., synonymy and hypernymy). We propose methods that transform
word vectors into sparse (and optionally binary) vectors. The resulting
representations are more similar to the interpretable features typically used
in NLP, though they are discovered automatically from raw corpora. Because the
vectors are highly sparse, they are computationally easy to work with. Most
importantly, we find that they outperform the original vectors on benchmark
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02005</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02005</id><created>2015-06-05</created><updated>2016-02-28</updated><authors><author><keyname>Roy</keyname><forenames>Shibdas</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Robust $H_\infty$ Estimation of Uncertain Linear Quantum Systems</title><categories>cs.SY math.OC quant-ph</categories><comments>Minor changes, Accepted version, International Journal of Robust and
  Nonlinear Control</comments><doi>10.1002/rnc.3530</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider classical estimators for a class of physically realizable linear
quantum systems. Optimal estimation using a complex Kalman filter for this
problem has been previously explored. Here, we study robust $H_\infty$
estimation for uncertain linear quantum systems. The estimation problem is
solved by converting it to a suitably scaled $H_\infty$ control problem. The
solution is obtained in the form of two algebraic Riccati equations. Relevant
examples involving dynamic squeezers are presented to illustrate the efficacy
of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02013</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02013</id><created>2015-06-05</created><authors><author><keyname>Li</keyname><forenames>James</forenames></author><author><keyname>Bax</keyname><forenames>Eric</forenames></author><author><keyname>Roy</keyname><forenames>Nilanjan</forenames></author><author><keyname>Leistra</keyname><forenames>Andrea</forenames></author></authors><title>VCG Payments for Portfolio Allocations in Online Advertising</title><categories>cs.GT q-fin.RM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some online advertising offers pay only when an ad elicits a response.
Randomness and uncertainty about response rates make showing those ads a risky
investment for online publishers. Like financial investors, publishers can use
portfolio allocation over multiple advertising offers to pursue revenue while
controlling risk. Allocations over multiple offers do not have a distinct
winner and runner-up, so the usual second-price mechanism does not apply. This
paper develops a pricing mechanism for portfolio allocations. The mechanism is
efficient, truthful, and rewards offers that reduce risk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02020</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02020</id><created>2015-06-05</created><authors><author><keyname>Gopalakrishnan</keyname><forenames>Ragavendran</forenames></author><author><keyname>Bax</keyname><forenames>Eric</forenames></author><author><keyname>Chitrapura</keyname><forenames>Krishna Prasad</forenames></author><author><keyname>Garg</keyname><forenames>Sachin</forenames></author></authors><title>Portfolio Allocation for Sellers in Online Advertising</title><categories>cs.GT q-fin.PM q-fin.RM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In markets for online advertising, some advertisers pay only when users
respond to ads. So publishers estimate ad response rates and multiply by
advertiser bids to estimate expected revenue for showing ads. Since these
estimates may be inaccurate, the publisher risks not selecting the ad for each
ad call that would maximize revenue. The variance of revenue can be decomposed
into two components -- variance due to `uncertainty' because the true response
rate is unknown, and variance due to `randomness' because realized response
statistics fluctuate around the true response rate. Over a sequence of many ad
calls, the variance due to randomness nearly vanishes due to the law of large
numbers. However, the variance due to uncertainty doesn't diminish.
  We introduce a technique for ad selection that augments existing estimation
and explore-exploit methods. The technique uses methods from portfolio
optimization to produce a distribution over ads rather than selecting the
single ad that maximizes estimated expected revenue. Over a sequence of similar
ad calls, ads are selected according to the distribution. This approach
decreases the effects of uncertainty and increases revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02024</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02024</id><created>2015-06-05</created><authors><author><keyname>Shaviv</keyname><forenames>Dor</forenames></author><author><keyname>Nguyen</keyname><forenames>Phan-Minh</forenames></author><author><keyname>Ozgur</keyname><forenames>Ayfer</forenames></author></authors><title>Capacity of the Energy Harvesting Channel with a Finite Battery</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an energy harvesting channel, in which the transmitter is powered
by an exogenous stochastic energy harvesting process $E_t$, such that $0\leq
E_t\leq\bar{E}$, which can be stored in a battery of finite size $\bar{B}$. We
provide a simple and insightful formula for the approximate capacity of this
channel with bounded guarantee on the approximation gap independent of system
parameters. This approximate characterization of the capacity identifies two
qualitatively different operating regimes for this channel: in the large
battery regime, when $\bar{B}\geq \bar{E}$, the capacity is approximately equal
to that of an AWGN channel with an average power constraint equal to the
average energy harvesting rate, i.e. it depends only on the mean of $E_t$ and
is (almost) independent of the distribution of $E_t$ and the exact value of
$\bar{B}$. In particular, this suggests that a battery size
$\bar{B}\approx\bar{E}$ is approximately sufficient to extract the infinite
battery capacity of the system. In the small battery regime, when
$\bar{B}&lt;\bar{E}$, we clarify the dependence of the capacity on the
distribution of $E_t$ and the value of $\bar{B}$.
  There are three steps to proving this result: 1) we characterize the capacity
of this channel as an $n$-letter mutual information rate under various
assumptions on the availability of energy arrival information; 2) we
characterize the approximately optimal online power control policy that
maximizes the long-term average throughput of the system; 3) we show that the
information-theoretic capacity of this channel is equal, within a constant gap,
to its long-term average throughput. This last result provides a connection
between the information- and communication-theoretic formulations of the
energy-harvesting communication problem that have been so far studied in
isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02025</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02025</id><created>2015-06-05</created><updated>2016-02-04</updated><authors><author><keyname>Jaderberg</keyname><forenames>Max</forenames></author><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author></authors><title>Spatial Transformer Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks define an exceptionally powerful class of
models, but are still limited by the lack of ability to be spatially invariant
to the input data in a computationally and parameter efficient manner. In this
work we introduce a new learnable module, the Spatial Transformer, which
explicitly allows the spatial manipulation of data within the network. This
differentiable module can be inserted into existing convolutional
architectures, giving neural networks the ability to actively spatially
transform feature maps, conditional on the feature map itself, without any
extra training supervision or modification to the optimisation process. We show
that the use of spatial transformers results in models which learn invariance
to translation, scale, rotation and more generic warping, resulting in
state-of-the-art performance on several benchmarks, and for a number of classes
of transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02026</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02026</id><created>2015-06-05</created><authors><author><keyname>Shaviv</keyname><forenames>Dor</forenames></author><author><keyname>Ozgur</keyname><forenames>Ayfer</forenames></author><author><keyname>Permuter</keyname><forenames>Haim</forenames></author></authors><title>Can Feedback Increase the Capacity of the Energy Harvesting Channel?</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate if feedback can increase the capacity of an energy harvesting
communication channel where a transmitter powered by an exogenous energy
arrival process and equipped with a finite battery communicates to a receiver
over a memoryless channel. For a simple special case where the energy arrival
process is deterministic and the channel is a BEC, we explicitly compute the
feed-forward and feedback capacities and show that feedback can strictly
increase the capacity of this channel. Building on this example, we also show
that feedback can increase the capacity when the energy arrivals are i.i.d.
known noncausally at the transmitter and the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02031</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02031</id><created>2015-06-05</created><authors><author><keyname>Giaconi</keyname><forenames>Giulio</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Smart Meter Privacy with an Energy Harvesting Device and Instantaneous
  Power Constraints</title><categories>cs.IT math.IT</categories><comments>To be published in IEEE ICC2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A smart meter (SM) periodically measures end-user electricity consumption and
reports it to a utility provider (UP). Despite the advantages of SMs, their use
leads to serious concerns about consumer privacy. In this paper, SM privacy is
studied by considering the presence of an energy harvesting device (EHD) as a
means of masking the user's input load. The user can satisfy part or all of
his/her energy needs from the EHD, and hence, less information can be leaked to
the UP via the SM. The EHD is typically equipped with a rechargeable energy
storage device, i.e., a battery, whose instantaneous energy content limits the
user's capability in covering his/her energy usage. Privacy is measured by the
information leaked about the user's real energy consumption when the UP
observes the energy requested from the grid, which the SM reads and reports to
the UP. The minimum information leakage rate is characterized as a computable
information theoretic single-letter expression when the EHD battery capacity is
either infinite or zero. Numerical results are presented for a discrete binary
input load to illustrate the potential privacy gains from the existence of a
storage device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02047</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02047</id><created>2015-06-05</created><authors><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author></authors><title>Bias vs structure of polynomials in large fields, and applications in
  effective algebraic geometry and coding theory</title><categories>cs.DM math.CO math.NT</categories><msc-class>11C08</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $f$ be a polynomial of degree $d$ in $n$ variables over a finite field
$\mathbb{F}$. The polynomial is said to be unbiased if the distribution of
$f(x)$ for a uniform input $x \in \mathbb{F}^n$ is close to the uniform
distribution over $\mathbb{F}$, and is called biased otherwise. The polynomial
is said to have low rank if it can be expressed as a composition of a few lower
degree polynomials. Green and Tao [Contrib. Discrete Math 2009] and Kaufman and
Lovett [FOCS 2008] showed that bias implies low rank for fixed degree
polynomials over fixed prime fields. This lies at the heart of many tools in
higher order Fourier analysis. In this work, we extend this result to all prime
fields (of size possibly growing with $n$). We also provide a generalization to
nonprime fields in the large characteristic case. However, we state all our
applications in the prime field setting for the sake of simplicity of
presentation.
  As an immediate application, we obtain improved bounds for a suite of
problems in effective algebraic geometry, including Hilbert nullstellensatz,
radical membership and counting rational points in low degree varieties.
  Using the above generalization to large fields as a starting point, we are
also able to settle the list decoding radius of fixed degree Reed-Muller codes
over growing fields. The case of fixed size fields was solved by Bhowmick and
Lovett [STOC 2015], which resolved a conjecture of Gopalan-Klivans-Zuckerman
[STOC 2008]. Here, we show that the list decoding radius is equal the minimum
distance of the code for all fixed degrees, even when the field size is
possibly growing with $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02059</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02059</id><created>2015-06-05</created><updated>2016-01-26</updated><authors><author><keyname>Yu</keyname><forenames>Haonan</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>Sentence Directed Video Object Codetection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of video object codetection by leveraging the weak
semantic constraint implied by sentences that describe the video content.
Unlike most existing work that focuses on codetecting large objects which are
usually salient both in size and appearance, we can codetect objects that are
small or medium sized. Our method assumes no human pose or depth information
such as is required by the most recent state-of-the-art method. We employ weak
semantic constraint on the codetection process by pairing the video with
sentences. Although the semantic information is usually simple and weak, it can
greatly boost the performance of our codetection framework by reducing the
search space of the hypothesized object detections. Our experiment demonstrates
an average IoU score of 0.423 on a new challenging dataset which contains 15
object classes and 150 videos with 12,509 frames in total, and an average IoU
score of 0.373 on a subset of an existing dataset, originally intended for
activity recognition, which contains 5 object classes and 75 videos with 8,854
frames in total.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02060</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02060</id><created>2015-02-26</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Similarity, Cardinality and Entropy for Bipolar Fuzzy Set in the
  Framework of Penta-valued Representation</title><categories>cs.AI</categories><comments>6 pages. Submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper one presents new similarity, cardinality and entropy measures
for bipolar fuzzy set and for its particular forms like intuitionistic,
paraconsistent and fuzzy set. All these are constructed in the framework of
multi-valued representations and are based on a penta-valued logic that uses
the following logical values: true, false, unknown, contradictory and
ambiguous. Also a new distance for bounded real interval was defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02061</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02061</id><created>2015-02-26</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>Entropy and Syntropy in the Context of Five-Valued Logics</title><categories>cs.AI</categories><comments>9 pages. Submitted to journal</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a five-valued representation of bifuzzy sets. This
representation is related to a five-valued logic that uses the following
values: true, false, inconsistent, incomplete and ambiguous. In the framework
of five-valued representation, formulae for similarity, entropy and syntropy of
bifuzzy sets are constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02066</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02066</id><created>2015-06-05</created><updated>2016-02-24</updated><authors><author><keyname>Sarkar</keyname><forenames>Camellia</forenames></author><author><keyname>Yadav</keyname><forenames>Alok</forenames></author><author><keyname>Jalan</keyname><forenames>Sarika</forenames></author></authors><title>Multilayer network decoding versatility and trust</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>16 pages, 5 figures</comments><journal-ref>EPL 113, 18007 (2016)</journal-ref><doi>10.1209/0295-5075/113/18007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, the multilayer networks have increasingly been realized
as a more realistic framework to understand emergent physical phenomena in
complex real world systems. We analyze a massive time-varying social data drawn
from the largest film industry of the world under multilayer network framework.
The framework enables us to evaluate the versatility of actors, which turns out
to be an intrinsic property of lead actors. Versatility in dimers suggests that
working with different types of nodes are more beneficial than with similar
ones. However, the triangles yield a different relation between type of
co-actor and the success of lead nodes indicating the importance of higher
order motifs in understanding the properties of the underlying system.
Furthermore, despite the degree-degree correlations of entire networks being
neutral, multilayering picks up different values of correlation indicating
positive connotations like trust, in the recent years. Analysis of weak ties of
the industry uncovers nodes from lower degree regime being important in linking
Bollywood clusters. The framework and the tools used herein may be used for
unraveling the complexity of other real world systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02071</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02071</id><created>2015-06-05</created><authors><author><keyname>Hiney</keyname><forenames>Jason</forenames></author><author><keyname>Dakve</keyname><forenames>Tejas</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Gaj</keyname><forenames>Kris</forenames></author></authors><title>Using Facebook for Image Steganography</title><categories>cs.MM cs.CR</categories><comments>6 pages, 4 figures, 2 tables. Accepted to Fourth International
  Workshop on Cyber Crime (IWCC 2015), co-located with 10th International
  Conference on Availability, Reliability and Security (ARES 2015), Toulouse,
  France, 24-28 August 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because Facebook is available on hundreds of millions of desktop and mobile
computing platforms around the world and because it is available on many
different kinds of platforms (from desktops and laptops running Windows, Unix,
or OS X to hand held devices running iOS, Android, or Windows Phone), it would
seem to be the perfect place to conduct steganography. On Facebook, information
hidden in image files will be further obscured within the millions of pictures
and other images posted and transmitted daily. Facebook is known to alter and
compress uploaded images so they use minimum space and bandwidth when displayed
on Facebook pages. The compression process generally disrupts attempts to use
Facebook for image steganography. This paper explores a method to minimize the
disruption so JPEG images can be used as steganography carriers on Facebook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02075</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02075</id><created>2015-06-05</created><authors><author><keyname>Bordes</keyname><forenames>Antoine</forenames></author><author><keyname>Usunier</keyname><forenames>Nicolas</forenames></author><author><keyname>Chopra</keyname><forenames>Sumit</forenames></author><author><keyname>Weston</keyname><forenames>Jason</forenames></author></authors><title>Large-scale Simple Question Answering with Memory Networks</title><categories>cs.LG cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training large-scale question answering systems is complicated because
training sources usually cover a small portion of the range of possible
questions. This paper studies the impact of multitask and transfer learning for
simple question answering; a setting for which the reasoning required to answer
is quite easy, as long as one can retrieve the correct evidence given a
question, which can be difficult in large-scale conditions. To this end, we
introduce a new dataset of 100k questions that we use in conjunction with
existing benchmarks. We conduct our study within the framework of Memory
Networks (Weston et al., 2015) because this perspective allows us to eventually
scale up to more complex reasoning, and show that Memory Networks can be
successfully trained to achieve excellent performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02078</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02078</id><created>2015-06-05</created><updated>2015-11-16</updated><authors><author><keyname>Karpathy</keyname><forenames>Andrej</forenames></author><author><keyname>Johnson</keyname><forenames>Justin</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Visualizing and Understanding Recurrent Networks</title><categories>cs.LG cs.CL cs.NE</categories><comments>changing style, adding references, minor changes to text</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNNs), and specifically a variant with Long
Short-Term Memory (LSTM), are enjoying renewed interest as a result of
successful applications in a wide range of machine learning problems that
involve sequential data. However, while LSTMs provide exceptional results in
practice, the source of their performance and their limitations remain rather
poorly understood. Using character-level language models as an interpretable
testbed, we aim to bridge this gap by providing an analysis of their
representations, predictions and error types. In particular, our experiments
reveal the existence of interpretable cells that keep track of long-range
dependencies such as line lengths, quotes and brackets. Moreover, our
comparative analysis with finite horizon n-gram models traces the source of the
LSTM improvements to long-range structural dependencies. Finally, we provide
analysis of the remaining errors and suggests areas for further study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02079</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02079</id><created>2015-06-05</created><authors><author><keyname>Kazhdan</keyname><forenames>Michael</forenames></author><author><keyname>Lillaney</keyname><forenames>Kunal</forenames></author><author><keyname>Roncal</keyname><forenames>William</forenames></author><author><keyname>Bock</keyname><forenames>Davi</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author></authors><title>Gradient-Domain Fusion for Color Correction in Large EM Image Stacks</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new gradient-domain technique for processing registered EM image
stacks to remove inter-image discontinuities while preserving intra-image
detail. To this end, we process the image stack by first performing anisotropic
smoothing along the slice axis and then solving a Poisson equation within each
slice to re-introduce the detail. The final image stack is continuous across
the slice axis and maintains sharp details within each slice. Adapting existing
out-of-core techniques for solving the linear system, we describe a parallel
algorithm with time complexity that is linear in the size of the data and space
complexity that is sub-linear, allowing us to process datasets as large as five
teravoxels with a 600 MB memory footprint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02080</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02080</id><created>2015-06-05</created><authors><author><keyname>Martinez-Cantin</keyname><forenames>Ruben</forenames></author></authors><title>Local Nonstationarity for Efficient Bayesian Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimization has shown to be a fundamental global optimization
algorithm in many applications: ranging from automatic machine learning,
robotics, reinforcement learning, experimental design, simulations, etc. The
most popular and effective Bayesian optimization relies on a surrogate model in
the form of a Gaussian process due to its flexibility to represent a prior over
function. However, many algorithms and setups relies on the stationarity
assumption of the Gaussian process. In this paper, we present a novel
nonstationary strategy for Bayesian optimization that is able to outperform the
state of the art in Bayesian optimization both in stationary and nonstationary
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02082</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02082</id><created>2015-06-05</created><updated>2015-06-14</updated><authors><author><keyname>Alipour</keyname><forenames>Philip B.</forenames></author><author><keyname>Magnusson</keyname><forenames>Matteus</forenames></author><author><keyname>Olsson</keyname><forenames>Martin W.</forenames></author><author><keyname>Ghasemi</keyname><forenames>Nooshin H.</forenames></author><author><keyname>Henesey</keyname><forenames>Lawrence</forenames></author></authors><title>A Real-time Cargo Damage Management System via a Sorting Array
  Triangulation Technique</title><categories>cs.AI</categories><comments>This article is a report on a developed IDSS system/prototype aimed
  to be published for a journal conference proceedings and/or a full paper
  under Computer Science and Software Engineering categories. 28 pages; 10
  Figures including graphs; 5 tables; presentation file is available at
  http://web.uvic.ca/~phibal12/Presentations/IDSS_proj.pptx Ask authors for
  full code and/or other files</comments><acm-class>H.4.2; H.5; I.2; D.2; I.4; I.5; B.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report covers an intelligent decision support system (IDSS), which
handles an efficient and effective way to rapidly inspect containerized cargos
for defection. Defection is either cargo exposure to radiation, physical
damages such as holes, punctured surfaces, iron surface oxidation, etc. The
system uses a sorting array triangulation technique (SAT) and surface damage
detection (SDD) to conduct the inspection. This new technique saves time and
money on finding damaged goods during transportation such that, instead of
running $n$ inspections on $n$ containers, only 3 inspections per triangulation
or a ratio of $3:n$ is required, assuming $n &gt; 3$ containers. The damaged stack
in the array is virtually detected contiguous to an actually-damaged cargo by
calculating nearby distances of such cargos, delivering reliable estimates for
the whole local stack population. The estimated values on damaged, somewhat
damaged and undamaged cargo stacks, are listed and profiled after being sorted
by the program, thereby submitted to the manager for a final decision. The
report describes the problem domain and the implementation of the simulator
prototype, showing how the system operates via software, hardware with/without
human agents, conducting real-time inspections and management per se.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02083</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02083</id><created>2015-06-05</created><authors><author><keyname>Xu</keyname><forenames>Min</forenames></author></authors><title>Automatic tracking of protein vesicles</title><categories>q-bio.QM cs.CV</categories><comments>Author's master thesis (University of Southern California, May 2009).
  Adviser: Sergey Lototsky. ISBN: 9781109140439</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advance of fluorescence imaging technologies, recently cell
biologists are able to record the movement of protein vesicles within a living
cell. Automatic tracking of the movements of these vesicles become key for
qualitative analysis of dynamics of theses vesicles. In this thesis, we
formulate such tracking problem as video object tracking problem, and design a
dynamic programming method for tracking single object. Our experiments on
simulation data show that the method can identify a track with high accuracy
which is robust to the choose of tracking parameters and presence of high level
noise. We then extend this method to the tracking multiple objects using the
track elimination strategy. In multiple object tracking, the above approach
often fails to correctly identify a track when two tracks cross. We solve this
problem by incorporating the Kalman filter into the dynamic programming
framework. Our experiments on simulated data show that the tracking accuracy is
significantly improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02085</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02085</id><created>2015-06-05</created><authors><author><keyname>Xu</keyname><forenames>Min</forenames></author><author><keyname>Setiono</keyname><forenames>Rudy</forenames></author></authors><title>Gene selection for cancer classification using a hybrid of univariate
  and multivariate feature selection methods</title><categories>q-bio.QM cs.CE cs.LG stat.ML</categories><journal-ref>Applied Genomics and Proteomics. 2003:2(2)79-91</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various approaches to gene selection for cancer classification based on
microarray data can be found in the literature and they may be grouped into two
categories: univariate methods and multivariate methods. Univariate methods
look at each gene in the data in isolation from others. They measure the
contribution of a particular gene to the classification without considering the
presence of the other genes. In contrast, multivariate methods measure the
relative contribution of a gene to the classification by taking the other genes
in the data into consideration. Multivariate methods select fewer genes in
general. However, the selection process of multivariate methods may be
sensitive to the presence of irrelevant genes, noises in the expression and
outliers in the training data. At the same time, the computational cost of
multivariate methods is high. To overcome the disadvantages of the two types of
approaches, we propose a hybrid method to obtain gene sets that are small and
highly discriminative.
  We devise our hybrid method from the univariate Maximum Likelihood method
(LIK) and the multivariate Recursive Feature Elimination method (RFE). We
analyze the properties of these methods and systematically test the
effectiveness of our proposed method on two cancer microarray datasets. Our
experiments on a leukemia dataset and a small, round blue cell tumors dataset
demonstrate the effectiveness of our hybrid method. It is able to discover sets
consisting of fewer genes than those reported in the literature and at the same
time achieve the same or better prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02087</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02087</id><created>2015-06-05</created><authors><author><keyname>Xu</keyname><forenames>Min</forenames></author></authors><title>Global Gene Expression Analysis Using Machine Learning Methods</title><categories>q-bio.QM cs.CE cs.LG stat.ML</categories><comments>Author's master thesis (National University of Singapore, May 2003).
  Adviser: Rudy Setiono</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microarray is a technology to quantitatively monitor the expression of large
number of genes in parallel. It has become one of the main tools for global
gene expression analysis in molecular biology research in recent years. The
large amount of expression data generated by this technology makes the study of
certain complex biological problems possible and machine learning methods are
playing a crucial role in the analysis process. At present, many machine
learning methods have been or have the potential to be applied to major areas
of gene expression analysis. These areas include clustering, classification,
dynamic modeling and reverse engineering.
  In this thesis, we focus our work on using machine learning methods to solve
the classification problems arising from microarray data. We first identify the
major types of the classification problems; then apply several machine learning
methods to solve the problems and perform systematic tests on real and
artificial datasets. We propose improvement to existing methods. Specifically,
we develop a multivariate and a hybrid feature selection method to obtain high
classification performance for high dimension classification problems. Using
the hybrid feature selection method, we are able to identify small sets of
features that give predictive accuracy that is as good as that from other
methods which require many more features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02089</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02089</id><created>2015-06-05</created><authors><author><keyname>Spasojevic</keyname><forenames>Nemanja</forenames></author><author><keyname>Li</keyname><forenames>Zhisheng</forenames></author><author><keyname>Rao</keyname><forenames>Adithya</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Prantik</forenames></author></authors><title>When-To-Post on Social Networks</title><categories>cs.SI</categories><comments>10 pages, to appear in KDD2015</comments><acm-class>H.1.2; J.4</acm-class><doi>10.1145/2783258.2788584</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many users on social networks, one of the goals when broadcasting content
is to reach a large audience. The probability of receiving reactions to a
message differs for each user and depends on various factors, such as location,
daily and weekly behavior patterns and the visibility of the message. While
previous work has focused on overall network dynamics and message flow
cascades, the problem of recommending personalized posting times has remained
an underexplored topic of research. In this study, we formulate a when-to-post
problem, where the objective is to find the best times for a user to post on
social networks in order to maximize the probability of audience responses. To
understand the complexity of the problem, we examine user behavior in terms of
post-to-reaction times, and compare cross-network and cross-city weekly
reaction behavior for users in different cities, on both Twitter and Facebook.
We perform this analysis on over a billion posted messages and observed
reactions, and propose multiple approaches for generating personalized posting
schedules. We empirically assess these schedules on a sampled user set of 0.5
million active users and more than 25 million messages observed over a 56 day
period. We show that users see a reaction gain of up to 17% on Facebook and 4%
on Twitter when the recommended posting times are used. We open the dataset
used in this study, which includes timestamps for over 144 million posts and
over 1.1 billion reactions. The personalized schedules derived here are used in
a fully deployed production system to recommend posting times for millions of
users every day.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02096</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02096</id><created>2015-06-05</created><updated>2015-11-03</updated><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author></authors><title>Optimum-width upward drawings of trees</title><categories>cs.CG</categories><comments>Revised version includes some of the results of ArXiV 1502.02753</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An upward drawing of a tree is a drawing such that no parents are below their
children. It is order-preserving if the edges to children appear in prescribed
order around each node. Chan showed that any tree has an upward
order-preserving drawing with width O(log n). In this paper, we present
linear-time algorithms that finds upward with instance-optimal width, i.e., the
width is the minimum-possible for the input tree.
  We study two different models. In the first model, the drawings need not be
order-preserving; a very simple algorithm then finds straight-line drawings of
optimal width. In the second model, the drawings must be order-preserving; and
we give an algorithm that finds optimum-width poly-line drawings, i.e., edges
are allowed to have bends. We also briefly study order-preserving upward
straight-line drawings, and show that some trees require larger width if
drawings must be straight-line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02100</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02100</id><created>2015-06-05</created><authors><author><keyname>Muhammad</keyname><forenames>Khan</forenames></author><author><keyname>Sajjad</keyname><forenames>Muhammad</forenames></author><author><keyname>Mehmood</keyname><forenames>Irfan</forenames></author><author><keyname>Rho</keyname><forenames>Seungmin</forenames></author><author><keyname>Baik</keyname><forenames>Sung Wook</forenames></author></authors><title>A novel magic LSB substitution method (M-LSB-SM) using multi-level
  encryption and achromatic component of an image</title><categories>cs.MM</categories><comments>This paper has been published in Multimedia Tools and Applications
  Journal with impact factor=1.058. The readers can study the formatted paper
  using the following link:
  http://link.springer.com/article/10.1007/s11042-015-2671-9. Please use
  sci-hub.org for downloading this paper if you are unable to access it freely
  or email us at khan.muhammad.icp@gmail.com</comments><journal-ref>Multimedia Tools and Applications, pp. 1-27, 2015</journal-ref><doi>10.1007/s11042-015-2671-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image Steganography is a thriving research area of information security where
secret data is embedded in images to hide its existence while getting the
minimum possible statistical detectability. This paper proposes a novel magic
least significant bit substitution method (M-LSB-SM) for RGB images. The
proposed method is based on the achromatic component (I-plane) of the
hue-saturation-intensity (HSI) color model and multi-level encryption (MLE) in
the spatial domain. The input image is transposed and converted into an HSI
color space. The I-plane is divided into four sub-images of equal size,
rotating each sub-image with a different angle using a secret key. The secret
information is divided into four blocks, which are then encrypted using an MLE
algorithm (MLEA). Each sub-block of the message is embedded into one of the
rotated sub-images based on a specific pattern using magic LSB substitution.
Experimental results validate that the proposed method not only enhances the
visual quality of stego images but also provides good imperceptibility and
multiple security levels as compared to several existing prominent methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02102</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02102</id><created>2015-06-05</created><authors><author><keyname>Wang</keyname><forenames>Xinhua</forenames></author></authors><title>Frequency-domain analysis of nonlinear and linear integrators</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, frequency-domain analysis based on frequency sweep method is
presented for a nonlinear double integrator and a new linear integrator. All
the two types of integrators can estimate the onefold and double integrals of a
signal synchronously. With respect to the linear double integrator, the
nonlinear integrator has better estimation performance and stronger robustness.
Importantly, the integrator parameters can be regulated from the
frequency-domain analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02106</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02106</id><created>2015-06-05</created><updated>2015-11-10</updated><authors><author><keyname>Bearman</keyname><forenames>Amy</forenames></author><author><keyname>Russakovsky</keyname><forenames>Olga</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>What's the point: Semantic segmentation with point supervision</title><categories>cs.CV</categories><comments>minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantic image segmentation task presents a trade-off between test
accuracy and the cost of obtaining training annotations. Detailed per-pixel
annotations enable training accurate models but are very expensive to obtain;
image-level class labels are an order of magnitude cheaper but result in less
accurate models. We take a natural step from image-level annotation towards
stronger supervision: we ask annotators to point to an object if one exists. We
demonstrate that this adds negligible additional annotation cost. We
incorporate this point supervision along with a novel objectness potential in
the training loss function of a state-of-the-art CNN model. The combined effect
of these two extensions is a 12.9% increase in mean intersection over union on
the PASCAL VOC 2012 segmentation task compared to a CNN model trained with only
image-level labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02107</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02107</id><created>2015-06-05</created><updated>2015-10-09</updated><authors><author><keyname>Ding</keyname><forenames>Jie</forenames></author><author><keyname>Noshad</keyname><forenames>Mohammad</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>Data-Driven Learning of the Number of States in Multi-State
  Autoregressive Models</title><categories>stat.ML cs.LG</categories><comments>This paper will appear in the Proceedings of 53rd Annual Allerton
  Conference on Communication, Control, and Computing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the class of multi-state autoregressive processes
that can be used to model non-stationary time-series of interest. In order to
capture different autoregressive (AR) states underlying an observed time
series, it is crucial to select the appropriate number of states. We propose a
new model selection technique based on the Gap statistics, which uses a null
reference distribution on the stable AR filters to check whether adding a new
AR state significantly improves the performance of the model. To that end, we
define a new distance measure between AR filters based on mean squared
prediction error (MSPE), and propose an efficient method to generate random
stable filters that are uniformly distributed in the coefficient space.
Numerical results are provided to evaluate the performance of the proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02108</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02108</id><created>2015-06-05</created><updated>2015-09-08</updated><authors><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Reid</keyname><forenames>Ian</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Deeply Learning the Messages in Message Passing Inference</title><categories>cs.CV cs.LG stat.ML</categories><comments>11 pages. Appearing in Proc. The Twenty-ninth Annual Conference on
  Neural Information Processing Systems (NIPS), 2015, Montreal, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep structured output learning shows great promise in tasks like semantic
image segmentation. We proffer a new, efficient deep structured model learning
scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be
used to estimate the messages in message passing inference for structured
prediction with Conditional Random Fields (CRFs). With such CNN message
estimators, we obviate the need to learn or evaluate potential functions for
message calculation. This confers significant efficiency for learning, since
otherwise when performing structured learning for a CRF with CNN potentials it
is necessary to undertake expensive inference for every stochastic gradient
iteration. The network output dimension for message estimation is the same as
the number of classes, in contrast to the network output for general CNN
potential functions in CRFs, which is exponential in the order of the
potentials. Hence CNN message learning has fewer network parameters and is more
scalable for cases that a large number of classes are involved. We apply our
method to semantic image segmentation on the PASCAL VOC 2012 dataset. We
achieve an intersection-over-union score of 73.4 on its test set, which is the
best reported result for methods using the VOC training images alone. This
impressive performance demonstrates the effectiveness and usefulness of our CNN
message learning method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02113</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02113</id><created>2015-06-05</created><authors><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Selective Greedy Equivalence Search: Finding Optimal Bayesian Networks
  Using a Polynomial Number of Score Evaluations</title><categories>cs.LG cs.AI</categories><comments>Full version of UAI paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Selective Greedy Equivalence Search (SGES), a restricted version
of Greedy Equivalence Search (GES). SGES retains the asymptotic correctness of
GES but, unlike GES, has polynomial performance guarantees. In particular, we
show that when data are sampled independently from a distribution that is
perfect with respect to a DAG ${\cal G}$ defined over the observable variables
then, in the limit of large data, SGES will identify ${\cal G}$'s equivalence
class after a number of score evaluations that is (1) polynomial in the number
of nodes and (2) exponential in various complexity measures including
maximum-number-of-parents, maximum-clique-size, and a new measure called {\em
v-width} that is at least as small as---and potentially much smaller than---the
other two. More generally, we show that for any hereditary and
equivalence-invariant property $\Pi$ known to hold in ${\cal G}$, we retain the
large-sample optimality guarantees of GES even if we ignore any GES deletion
operator during the backward phase that results in a state for which $\Pi$ does
not hold in the common-descendants subgraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02117</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02117</id><created>2015-06-06</created><authors><author><keyname>Long</keyname><forenames>Mingsheng</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author></authors><title>Learning Multiple Tasks with Deep Relationship Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks trained on large-scale dataset can learn transferable
features that promote learning multiple tasks for inductive transfer and
labeling mitigation. As deep features eventually transition from general to
specific along the network, a fundamental problem is how to exploit the
relationship structure across different tasks while accounting for the feature
transferability in the task-specific layers. In this work, we propose a novel
Deep Relationship Network (DRN) architecture for multi-task learning by
discovering correlated tasks based on multiple task-specific layers of a deep
convolutional neural network. DRN models the task relationship by imposing
matrix normal priors over the network parameters of all task-specific layers,
including higher feature layers and classifier layer that are not transferable
safely. By jointly learning the transferable features and task relationships,
DRN is able to alleviate the dilemma of negative-transfer in the feature layers
and under-transfer in the classifier layer. Empirical evidence shows that DRN
yields state-of-the-art classification results on standard multi-domain object
recognition datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02128</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02128</id><created>2015-06-06</created><authors><author><keyname>Chang</keyname><forenames>Wen-Jan</forenames></author><author><keyname>Rua-HuanTsaih</keyname></author><author><keyname>Yen</keyname><forenames>David C.</forenames></author><author><keyname>Han</keyname><forenames>Tzu-Shian</forenames></author></authors><title>The ICT predicament of new ICT-enabled service</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advancement of information and communication technologies (ICT) has
triggered many ICT-enabled services. Regarding this service, the complementary
ICT system involves with customers' devices, industry-wide ICT development and
nation-wide ICT infrastructure, which are difficult for any individual
organization to control. The ICT predicament is the phenomenon that the
complementary ICT system is inferior in delivering the promised service quality
of new ICT-enabled service. With the ICT predicament, companies face the
decision-making dilemma in launching the new service or postponing the launch.
This study proposes a process to resolve the decision-making dilemma regarding
the ICT predicament.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02132</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02132</id><created>2015-06-06</created><authors><author><keyname>Pradhan</keyname><forenames>Chandan</forenames></author><author><keyname>Sankhe</keyname><forenames>Kunal</forenames></author><author><keyname>Kumar</keyname><forenames>Sumit</forenames></author><author><keyname>Murthy</keyname><forenames>Garimella Rama</forenames></author></authors><title>Full-Duplex eNodeB and UE Design for 5G Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>7 pages and 6 figures, Accepted and will also be available in
  proceedings of Wireless Telecommunications Symposium (WTS) 2015</comments><report-no>1506.01910</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent progress in the area of self-interference cancellation (SIC)
design has enabled the development of full-duplex (FD) single and multiple
antenna systems. In this paper, we propose a design for FD eNodeB (eNB) and
user equipment (UE) for 5G networks. The use of FD operation enables
simultaneous in-band uplink and downlink operation and thereby cutting down the
spectrum requirement by half. FD operation requires the same subcarrier
allocation to UE in both uplink and downlink. Long Term Evolution LTE) uses
orthogonal frequency division multiple access (OFDMA) for downlink. To enable
FD operation, we propose using single carrier frequency division multiple
access SC-FDMA) for downlink along with the conventional method of using it for
uplink. Taking advantage of channel reciprocity, singular value decomposition
(SVD) based eamforming in the downlink allows multiple users (MU) to operate on
same set of subcarriers. In uplink, frequency domain minimum mean square error
(MMSE) equalizer along with successive interference cancellation with optimal
ordering (SSIC-OO) algorithm is used to decouple signals of users operating in
the same set of subcarriers. The work includes simulations showing efficient FD
operation both at UE and eNB for downlink and uplink respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02142</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02142</id><created>2015-06-06</created><updated>2015-10-31</updated><authors><author><keyname>Gal</keyname><forenames>Yarin</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Dropout as a Bayesian Approximation: Representing Model Uncertainty in
  Deep Learning</title><categories>stat.ML cs.LG</categories><comments>11 pages, 6 figures; Minor corrections in experiments section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning tools have gained tremendous attention in applied machine
learning. However such tools for regression and classification do not capture
model uncertainty. In comparison, Bayesian models offer a mathematically
grounded framework to reason about model uncertainty, but usually come with a
prohibitive computational cost. In this paper we develop a new theoretical
framework casting dropout training in deep neural networks (NNs) as approximate
Bayesian inference in deep Gaussian processes. A direct result of this theory
gives us tools to model uncertainty with dropout NNs -- extracting information
from existing models that has been thrown away so far. This mitigates the
problem of representing uncertainty in deep learning without sacrificing either
computational complexity or test accuracy. We perform an extensive study of the
properties of dropout's uncertainty. Various network architectures and
non-linearities are assessed on tasks of regression and classification, using
MNIST as an example. We show a considerable improvement in predictive
log-likelihood and RMSE compared to existing state-of-the-art methods, and
finish by using dropout's uncertainty in deep reinforcement learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02152</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02152</id><created>2015-06-06</created><authors><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author></authors><title>Nested Lattice Codes for Secure Bidirectional Relaying with Asymmetric
  Channel Gains</title><categories>cs.IT math.IT</categories><comments>7 pages. This is an updated version of an invited paper at the 2015
  IEEE Information Theory Workshop held at Jerusalem, Israel</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic problem of secure bidirectional relaying involves two users who
want to exchange messages via an intermediate &quot;honest-but-curious&quot; relay node.
There is no direct link between the users, all communication must take place
via the relay node. The links between the user nodes and the relay are wireless
links with Gaussian noise. It is required that the users' messages be kept
secure from the relay. In prior work, we proposed coding schemes based on
nested lattices for this problem, assuming that the channel gains from the two
user nodes to the relay are identical. We also analyzed the power-rate tradeoff
for secure and reliable message exchange using our coding schemes. In this
paper, we extend our prior work to the case when the channel gains are not
necessarily identical, and are known to the relay node but perhaps not to the
users. We show that using our scheme, perfect secrecy can be obtained only for
certain values of the channel gains, and analyze the power-rate tradeoff in
these cases. We also make similar observations for our strongly-secure scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02153</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02153</id><created>2015-06-06</created><authors><author><keyname>Facchini</keyname><forenames>Alessandro</forenames></author><author><keyname>Murlak</keyname><forenames>Filip</forenames></author><author><keyname>Skrzypczak</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Index problems for game automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given regular language of infinite trees, one can ask about the minimal
number of priorities needed to recognise this language with a
non-deterministic, alternating, or weak alternating parity automaton. These
questions are known as, respectively, the non-deterministic, alternating, and
weak Rabin-Mostowski index problems. Whether they can be answered effectively
is a long-standing open problem, solved so far only for languages recognisable
by deterministic automata (the alternating variant trivialises).
  We investigate a wider class of regular languages, recognisable by so-called
game automata, which can be seen as the closure of deterministic ones under
complementation and composition. Game automata are known to recognise languages
arbitrarily high in the alternating Rabin-Mostowski index hierarchy, i.e., the
alternating index problem does not trivialise any more.
  Our main contribution is that all three index problems are decidable for
languages recognisable by game automata. Additionally, we show that it is
decidable whether a given regular language can be recognised by a game
automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02154</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02154</id><created>2015-06-06</created><authors><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author><author><keyname>Fan</keyname><forenames>Hongqi</forenames></author><author><keyname>Fu</keyname><forenames>Qiang</forenames></author><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author></authors><title>Bayesian De-quantization and Data Compression for Low-Energy
  Physiological Signal Telemonitoring</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the issue of applying quantized compressed sensing (CS) on
low-energy telemonitoring. So far, few works studied this problem in
applications where signals were only approximately sparse. We propose a
two-stage data compressor based on quantized CS, where signals are compressed
by compressed sensing and then the compressed measurements are quantized with
only 2 bits per measurement. This compressor can greatly reduce the
transmission bit-budget. To recover signals from underdetermined, quantized
measurements, we develop a Bayesian De-quantization algorithm. It can exploit
both the model of quantization errors and the correlated structure of
physiological signals to improve the quality of recovery. The proposed data
compressor and the recovery algorithm are validated on a dataset recorded on 12
subjects during fast running. Experiment results showed that an averaged 2.596
beat per minute (BPM) estimation error was achieved by jointly using compressed
sensing with 50% compression ratio and a 2-bit quantizer. The results imply
that we can effectively transmit n bits instead of n samples, which is a
substantial improvement for low-energy wireless telemonitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02155</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02155</id><created>2015-06-06</created><updated>2015-11-04</updated><authors><author><keyname>Sriperumbudur</keyname><forenames>Bharath K.</forenames></author><author><keyname>Szabo</keyname><forenames>Zoltan</forenames></author></authors><title>Optimal Rates for Random Fourier Features</title><categories>math.ST cs.LG math.FA stat.ML stat.TH</categories><comments>To appear at NIPS-2015</comments><msc-class>60E10, 62Gxx, 62Exx, 62H12, 42Bxx, 46E22</msc-class><acm-class>G.3; I.2.6; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel methods represent one of the most powerful tools in machine learning
to tackle problems expressed in terms of function values and derivatives due to
their capability to represent and model complex relations. While these methods
show good versatility, they are computationally intensive and have poor
scalability to large data as they require operations on Gram matrices. In order
to mitigate this serious computational limitation, recently randomized
constructions have been proposed in the literature, which allow the application
of fast linear algorithms. Random Fourier features (RFF) are among the most
popular and widely applied constructions: they provide an easily computable,
low-dimensional feature representation for shift-invariant kernels. Despite the
popularity of RFFs, very little is understood theoretically about their
approximation quality. In this paper, we provide a detailed finite-sample
theoretical analysis about the approximation quality of RFFs by (i)
establishing optimal (in terms of the RFF dimension, and growing set size)
performance guarantees in uniform norm, and (ii) presenting guarantees in $L^r$
($1\le r&lt;\infty$) norms. We also propose an RFF approximation to derivatives of
a kernel with a theoretical study on its approximation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02158</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02158</id><created>2015-06-06</created><updated>2016-01-18</updated><authors><author><keyname>Gal</keyname><forenames>Yarin</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Bayesian Convolutional Neural Networks with Bernoulli Approximate
  Variational Inference</title><categories>stat.ML cs.LG</categories><comments>12 pages, 3 figures, ICLR format, updated with reviewer comments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) work well on large datasets. But
labelled data is hard to collect, and in some applications larger amounts of
data are not available. The problem then is how to use CNNs with small data --
as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better
robustness to over-fitting on small data than traditional approaches. This is
by placing a probability distribution over the CNN's kernels. We approximate
our model's intractable posterior with Bernoulli variational distributions,
requiring no additional model parameters.
  On the theoretical side, we cast dropout network training as approximate
inference in Bayesian neural networks. This allows us to implement our model
using existing tools in deep learning with no increase in time complexity,
while highlighting a negative result in the field. We show a considerable
improvement in classification accuracy compared to standard techniques and
improve on published state-of-the-art results for CIFAR-10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02159</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02159</id><created>2015-06-06</created><authors><author><keyname>Kasai</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Mishra</keyname><forenames>Bamdev</forenames></author></authors><title>Riemannian preconditioning for tensor completion</title><categories>cs.NA cs.LG math.OC</categories><comments>Supplementary material included in the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel Riemannian preconditioning approach for the tensor
completion problem with rank constraint. A Riemannian metric or inner product
is proposed that exploits the least-squares structure of the cost function and
takes into account the structured symmetry in Tucker decomposition. The
specific metric allows to use the versatile framework of Riemannian
optimization on quotient manifolds to develop a preconditioned nonlinear
conjugate gradient algorithm for the problem. To this end, concrete matrix
representations of various optimization-related ingredients are listed.
Numerical comparisons suggest that our proposed algorithm robustly outperforms
state-of-the-art algorithms across different problem instances encompassing
various synthetic and real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02160</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02160</id><created>2015-06-06</created><updated>2015-08-14</updated><authors><author><keyname>Ding</keyname><forenames>Jie</forenames></author><author><keyname>Noshad</keyname><forenames>Mohammad</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>Complementary Lattice Arrays for Coded Aperture Imaging</title><categories>cs.IT astro-ph.IM math.CO math.IT physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider complementary lattice arrays in order to enable a
broader range of designs for coded aperture imaging systems. We provide a
general framework and methods that generate richer and more flexible designs
than existing ones. Besides this, we review and interpret the state-of-the-art
uniformly redundant arrays (URA) designs, broaden the related concepts, and
further propose some new design methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02162</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02162</id><created>2015-06-06</created><authors><author><keyname>Jabbari</keyname><forenames>Shahin</forenames></author><author><keyname>Rogers</keyname><forenames>Ryan</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Learning from Rational Behavior: Predicting Solutions to Unknown Linear
  Programs</title><categories>cs.DS cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define and study the problem of predicting the solution to a linear
program, given only partial information about its objective and constraints.
This generalizes the problem of learning to predict the purchasing behavior of
a rational agent who has an unknown objective function, which has been studied
under the name &quot;Learning from Revealed Preferences&quot;. We give mistake bound
learning algorithms in two settings: in the first, the objective of the linear
program is known to the learner, but there is an arbitrary, fixed set of
constraints which are unknown. Each example given to the learner is defined by
an additional, known constraint, and the goal of the learner is to predict the
optimal solution of the linear program given the union of the known and unknown
constraints. This models, among other things, the problem of predicting the
behavior of a rational agent whose goals are known, but whose resources are
unknown. In the second setting, the objective of the linear program is unknown,
and changing in a controlled way. The constraints of the linear program may
also change every day, but are known. An example is given by a set of
constraints and partial information about the objective, and the task of the
learner is again to predict the optimal solution of the partially known linear
program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02163</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02163</id><created>2015-06-06</created><authors><author><keyname>Deng</keyname><forenames>Zhun</forenames></author><author><keyname>Ding</keyname><forenames>Jie</forenames></author><author><keyname>Noshad</keyname><forenames>Mohammad</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>Capacity of Hexagonal Checkerboard Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new method to bound the capacity of checkerboard
codes on the hexagonal lattice. This produces rigorous bounds that are tighter
than those commonly known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02167</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02167</id><created>2015-06-06</created><updated>2015-12-07</updated><authors><author><keyname>Chakrabarti</keyname><forenames>Ayan</forenames></author></authors><title>Color Constancy by Learning to Predict Chromaticity from Luminance</title><categories>cs.CV</categories><comments>Appears in Advances in Neural Information Processing Systems 28 (NIPS
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Color constancy is the recovery of true surface color from observed color,
and requires estimating the chromaticity of scene illumination to correct for
the bias it induces. In this paper, we show that the per-pixel color statistics
of natural scenes---without any spatial or semantic context---can by themselves
be a powerful cue for color constancy. Specifically, we describe an illuminant
estimation method that is built around a &quot;classifier&quot; for identifying the true
chromaticity of a pixel given its luminance (absolute brightness across color
channels). During inference, each pixel's observed color restricts its true
chromaticity to those values that can be explained by one of a candidate set of
illuminants, and applying the classifier over these values yields a
distribution over the corresponding illuminants. A global estimate for the
scene illuminant is computed through a simple aggregation of these
distributions across all pixels. We begin by simply defining the
luminance-to-chromaticity classifier by computing empirical histograms over
discretized chromaticity and luminance values from a training set of natural
images. These histograms reflect a preference for hues corresponding to smooth
reflectance functions, and for achromatic colors in brighter pixels. Despite
its simplicity, the resulting estimation algorithm outperforms current
state-of-the-art color constancy methods. Next, we propose a method to learn
the luminance-to-chromaticity classifier &quot;end-to-end&quot;. Using stochastic
gradient descent, we set chromaticity-luminance likelihoods to minimize errors
in the final scene illuminant estimates on a training set. This leads to
further improvements in accuracy, most significantly in the tail of the error
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02170</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02170</id><created>2015-06-06</created><authors><author><keyname>Rughani</keyname><forenames>Megha</forenames></author><author><keyname>Shivakrishna</keyname><forenames>D.</forenames></author></authors><title>Hybridized Feature Extraction and Acoustic Modelling Approach for
  Dysarthric Speech Recognition</title><categories>cs.SD cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dysarthria is malfunctioning of motor speech caused by faintness in the human
nervous system. It is characterized by the slurred speech along with physical
impairment which restricts their communication and creates the lack of
confidence and affects the lifestyle. This paper attempt to increase the
efficiency of Automatic Speech Recognition (ASR) system for unimpaired speech
signal. It describes state of art of research into improving ASR for speakers
with dysarthria by means of incorporated knowledge of their speech production.
Hybridized approach for feature extraction and acoustic modelling technique
along with evolutionary algorithm is proposed for increasing the efficiency of
the overall system. Here number of feature vectors are varied and tested the
system performance. It is observed that system performance is boosted by
genetic algorithm. System with 16 acoustic features optimized with genetic
algorithm has obtained highest recognition rate of 98.28% with training time of
5:30:17.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02178</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02178</id><created>2015-06-06</created><updated>2016-03-07</updated><authors><author><keyname>Tzionas</keyname><forenames>Dimitrios</forenames></author><author><keyname>Ballan</keyname><forenames>Luca</forenames></author><author><keyname>Srikantha</keyname><forenames>Abhilash</forenames></author><author><keyname>Aponte</keyname><forenames>Pablo</forenames></author><author><keyname>Pollefeys</keyname><forenames>Marc</forenames></author><author><keyname>Gall</keyname><forenames>Juergen</forenames></author></authors><title>Capturing Hands in Action using Discriminative Salient Points and
  Physics Simulation</title><categories>cs.CV</categories><comments>Accepted for publication by the International Journal of Computer
  Vision (IJCV) on 16.02.2016 (submitted on 17.10.14). A combination into a
  single framework of an ECCV'12 multicamera-RGB and a monocular-RGBD GCPR'14
  hand tracking paper with several extensions, additional experiments and
  details</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hand motion capture is a popular research field, recently gaining more
attention due to the ubiquity of RGB-D sensors. However, even most recent
approaches focus on the case of a single isolated hand. In this work, we focus
on hands that interact with other hands or objects and present a framework that
successfully captures motion in such interaction scenarios for both rigid and
articulated objects. Our framework combines a generative model with
discriminatively trained salient points to achieve a low tracking error and
with collision detection and physics simulation to achieve physically plausible
estimates even in case of occlusions and missing visual data. Since all
components are unified in a single objective function which is almost
everywhere differentiable, it can be optimized with standard optimization
techniques. Our approach works for monocular RGB-D sequences as well as setups
with multiple synchronized RGB cameras. For a qualitative and quantitative
evaluation, we captured 29 sequences with a large variety of interactions and
up to 150 degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02181</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02181</id><created>2015-06-06</created><authors><author><keyname>Thrampoulidis</keyname><forenames>Chrtistos</forenames></author><author><keyname>Abbasi</keyname><forenames>Ehsan</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>The LASSO with Non-linear Measurements is Equivalent to One With Linear
  Measurements</title><categories>math.ST cs.IT math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider estimating an unknown, but structured, signal $x_0\in R^n$ from $m$
measurement $y_i=g_i(a_i^Tx_0)$, where the $a_i$'s are the rows of a known
measurement matrix $A$, and, $g$ is a (potentially unknown) nonlinear and
random link-function. Such measurement functions could arise in applications
where the measurement device has nonlinearities and uncertainties. It could
also arise by design, e.g., $g_i(x)=\text{sign}(x+z_i)$, corresponds to noisy
1-bit quantized measurements. Motivated by the classical work of Brillinger,
and more recent work of Plan and Vershynin, we estimate $x_0$ via solving the
Generalized-LASSO for some regularization parameter $\lambda&gt;0$ and some
(typically non-smooth) convex structure-inducing regularizer function. While
this approach seems to naively ignore the nonlinear function $g$, both
Brillinger (in the non-constrained case) and Plan and Vershynin have shown
that, when the entries of $A$ are iid standard normal, this is a good estimator
of $x_0$ up to a constant of proportionality $\mu$, which only depends on $g$.
In this work, we considerably strengthen these results by obtaining explicit
expressions for the squared error, for the \emph{regularized} LASSO, that are
asymptotically \emph{precise} when $m$ and $n$ grow large. A main result is
that the estimation performance of the Generalized LASSO with non-linear
measurements is \emph{asymptotically the same} as one whose measurements are
linear $y_i=\mu a_i^Tx_0 + \sigma z_i$, with $\mu = E\gamma g(\gamma)$ and
$\sigma^2 = E(g(\gamma)-\mu\gamma)^2$, and, $\gamma$ standard normal. To the
best of our knowledge, the derived expressions on the estimation performance
are the first-known precise results in this context. One interesting
consequence of our result is that the optimal quantizer of the measurements
that minimizes the estimation error of the LASSO is the celebrated Lloyd-Max
quantizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02184</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02184</id><created>2015-06-06</created><authors><author><keyname>Ye</keyname><forenames>Jun</forenames></author><author><keyname>Hu</keyname><forenames>Hao</forenames></author><author><keyname>Li</keyname><forenames>Kai</forenames></author><author><keyname>Qi</keyname><forenames>Guo-Jun</forenames></author><author><keyname>Hua</keyname><forenames>Kien A.</forenames></author></authors><title>First-Take-All: Temporal Order-Preserving Hashing for 3D Action Videos</title><categories>cs.CV</categories><comments>9 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the prevalence of the commodity depth cameras, the new paradigm of user
interfaces based on 3D motion capturing and recognition have dramatically
changed the way of interactions between human and computers. Human action
recognition, as one of the key components in these devices, plays an important
role to guarantee the quality of user experience. Although the model-driven
methods have achieved huge success, they cannot provide a scalable solution for
efficiently storing, retrieving and recognizing actions in the large-scale
applications. These models are also vulnerable to the temporal translation and
warping, as well as the variations in motion scales and execution rates. To
address these challenges, we propose to treat the 3D human action recognition
as a video-level hashing problem and propose a novel First-Take-All (FTA)
Hashing algorithm capable of hashing the entire video into hash codes of fixed
length. We demonstrate that this FTA algorithm produces a compact
representation of the video invariant to the above mentioned variations,
through which action recognition can be solved by an efficient nearest neighbor
search by the Hamming distance between the FTA hash codes. Experiments on the
public 3D human action datasets shows that the FTA algorithm can reach a
recognition accuracy higher than 80%, with about 15 bits per frame considering
there are 65 frames per video over the datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02188</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02188</id><created>2015-06-06</created><authors><author><keyname>Chow</keyname><forenames>Yinlam</forenames></author><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach</title><categories>cs.AI math.OC</categories><comments>Submitted to NIPS 15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of decision making within a Markov
decision process (MDP) framework where risk and modeling errors are taken into
account. Our approach is to minimize a risk-sensitive conditional-value-at-risk
(CVaR) objective, as opposed to a standard risk-neutral expectation. We refer
to such problem as CVaR MDP. Our first contribution is to show that a CVaR
objective, besides capturing risk sensitivity, has an alternative
interpretation as expected cost under worst-case modeling errors, for a given
error budget. This result, which is of independent interest, motivates CVaR
MDPs as a unifying framework for risk-sensitive and robust decision making. Our
second contribution is to present an approximate value-iteration algorithm for
CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first
solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we
present results from numerical experiments that corroborate our theoretical
findings and show the practicality of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02190</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02190</id><created>2015-06-06</created><updated>2015-11-09</updated><authors><author><keyname>Tang</keyname><forenames>Lei</forenames></author></authors><title>Thresholding for Top-k Recommendation with Temporal Dynamics</title><categories>cs.IR cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on top-k recommendation in domains where underlying data
distribution shifts overtime. We propose to learn a time-dependent bias for
each item over whatever existing recommendation engine. Such a bias learning
process alleviates data sparsity in constructing the engine, and at the same
time captures recent trend shift observed in data. We present an alternating
optimization framework to resolve the bias learning problem, and develop
methods to handle a variety of commonly used recommendation evaluation
criteria, as well as large number of items and users in practice. The proposed
algorithm is examined, both offline and online, using real world data sets
collected from the largest retailer worldwide. Empirical results demonstrate
that the bias learning can almost always boost recommendation performance. We
encourage other practitioners to adopt it as a standard component in
recommender systems where temporal dynamics is a norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02200</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02200</id><created>2015-06-06</created><authors><author><keyname>Ponciano</keyname><forenames>Lesandro</forenames></author><author><keyname>Brasileiro</keyname><forenames>Francisco</forenames></author><author><keyname>Andrade</keyname><forenames>Nazareno</forenames></author><author><keyname>Sampaio</keyname><forenames>L&#xed;via</forenames></author></authors><title>Considering Human Aspects on Strategies for Designing and Managing
  Distributed Human Computation</title><categories>cs.HC</categories><comments>3 figures, 1 table</comments><journal-ref>Journal of Internet Services and Applications 2014, 5:10</journal-ref><doi>10.1186/s13174-014-0010-4</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A human computation system can be viewed as a distributed system in which the
processors are humans, called workers. Such systems harness the cognitive power
of a group of workers connected to the Internet to execute relatively simple
tasks, whose solutions, once grouped, solve a problem that systems equipped
with only machines could not solve satisfactorily. Examples of such systems are
Amazon Mechanical Turk and the Zooniverse platform. A human computation
application comprises a group of tasks, each of them can be performed by one
worker. Tasks might have dependencies among each other. In this study, we
propose a theoretical framework to analyze such type of application from a
distributed systems point of view. Our framework is established on three
dimensions that represent different perspectives in which human computation
applications can be approached: quality-of-service requirements, design and
management strategies, and human aspects. By using this framework, we review
human computation in the perspective of programmers seeking to improve the
design of human computation applications and managers seeking to increase the
effectiveness of human computation infrastructures in running such
applications. In doing so, besides integrating and organizing what has been
done in this direction, we also put into perspective the fact that the human
aspects of the workers in such systems introduce new challenges in terms of,
for example, task assignment, dependency management, and fault prevention and
tolerance. We discuss how they are related to distributed systems and other
areas of knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02203</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02203</id><created>2015-06-06</created><authors><author><keyname>Ronchi</keyname><forenames>Matteo Ruggero</forenames></author><author><keyname>Perona</keyname><forenames>Pietro</forenames></author></authors><title>Describing Common Human Visual Actions in Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Which common human actions and interactions are recognizable in monocular
still images? Which involve objects and/or other people? How many is a person
performing at a time? We address these questions by exploring the actions and
interactions that are detectable in the images of the MS COCO dataset. We make
two main contributions. First, a list of 140 common `visual actions', obtained
by analyzing the largest on-line verb lexicon currently available for English
(VerbNet) and human sentences used to describe images in MS COCO. Second, a
complete set of annotations for those `visual actions', composed of
subject-object and associated verb, which we call COCO-a (a for `actions').
COCO-a is larger than existing action datasets in terms of number of actions
and instances of these actions, and is unique because it is data-driven, rather
than experimenter-biased. Other unique features are that it is exhaustive, and
that all subjects and objects are localized. A statistical analysis of the
accuracy of our annotations and of each action, interaction and subject-object
combination is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02204</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02204</id><created>2015-06-06</created><updated>2015-06-10</updated><authors><author><keyname>Liu</keyname><forenames>Chunlei</forenames></author></authors><title>Kasami type codes of higher relative dimension</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1506.01498</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some new Kasami type codes of higher relative dimension is introduced. Their
weight distribution is determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02211</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02211</id><created>2015-06-06</created><authors><author><keyname>Dong</keyname><forenames>Chao</forenames></author><author><keyname>Zhu</keyname><forenames>Ximei</forenames></author><author><keyname>Deng</keyname><forenames>Yubin</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author></authors><title>Boosting Optical Character Recognition: A Super-Resolution Approach</title><categories>cs.CV</categories><comments>5 pages, 8 figures</comments><acm-class>I.4.3; I.4.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text image super-resolution is a challenging yet open research problem in the
computer vision community. In particular, low-resolution images hamper the
performance of typical optical character recognition (OCR) systems. In this
article, we summarize our entry to the ICDAR2015 Competition on Text Image
Super-Resolution. Experiments are based on the provided ICDAR2015 TextSR
dataset and the released Tesseract-OCR 3.02 system. We report that our winning
entry of text image super-resolution framework has largely improved the OCR
performance with low-resolution images used as input, reaching an OCR accuracy
score of 77.19%, which is comparable with that of using the original
high-resolution images 78.80%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02212</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02212</id><created>2015-06-06</created><authors><author><keyname>Yi</keyname><forenames>Jiawang</forenames></author><author><keyname>Tan</keyname><forenames>Guanzheng</forenames></author></authors><title>Nonlinear compressed sensing based on composite mappings and its
  pointwise linearization</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical compressed sensing (CS) allows us to recover structured signals
from far few linear measurements than traditionally prescribed, thereby
efficiently decreasing sampling rates. However, if there exist nonlinearities
in the measurements, is it still possible to recover sparse or structured
signals from the nonlinear measurements? The research of nonlinear CS is
devoted to answering this question. In this paper, unlike the existing research
angles of nonlinear CS, we study it from the perspective of mapping
decomposition, and propose a new concept, namely, nonlinear CS based on
composite mappings. Through the analysis of two forms of a nonlinear composite
mapping Phi, i.e., Phi(x) = F(Ax) and Phi(x) = AF(x), we give the requirements
respectively for the sensing matrix A and the nonlinear mapping F when
reconstructing all sparse signals exactly from the nonlinear measurements
Phi(x). Besides, we also provide a special pointwise linearization method,
which can turn the nonlinear composite mapping Phi, at each point in its
domain, into an equivalent linear composite mapping. This linearization method
can guarantee the exact recovery of all given sparse signals even if Phi is not
an injection for all sparse signals. It may help us build an algorithm
framework for the composite nonlinear CS in which we can take full advantage of
the existing recovery algorithms belonging to linear CS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02216</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02216</id><created>2015-06-07</created><updated>2015-11-02</updated><authors><author><keyname>Chung</keyname><forenames>Junyoung</forenames></author><author><keyname>Kastner</keyname><forenames>Kyle</forenames></author><author><keyname>Dinh</keyname><forenames>Laurent</forenames></author><author><keyname>Goel</keyname><forenames>Kratarth</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>A Recurrent Latent Variable Model for Sequential Data</title><categories>cs.LG</categories><comments>Fixed typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore the inclusion of latent random variables into the
dynamic hidden state of a recurrent neural network (RNN) by combining elements
of the variational autoencoder. We argue that through the use of high-level
latent random variables, the variational RNN (VRNN)1 can model the kind of
variability observed in highly structured sequential data such as natural
speech. We empirically evaluate the proposed model against related sequential
models on four speech datasets and one handwriting dataset. Our results show
the important roles that latent random variables can play in the RNN dynamic
hidden state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02220</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02220</id><created>2015-06-07</created><authors><author><keyname>Abbasi</keyname><forenames>Hameer</forenames></author><author><keyname>Abbassi</keyname><forenames>Shahid H.</forenames></author><author><keyname>Qureshi</keyname><forenames>Ijaz M.</forenames></author></authors><title>A framework for the simulation of CR-VANET channel sensing, coordination
  and allocation</title><categories>cs.NI</categories><comments>20 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VANETs are considered as networks of critical future importance, with the
main concern being the safety of travelers and infrastructure. A lot of methods
for coordination and channel allocation in the context of VANETs are being
introduced. As such, the need of a framework to reliably compare the relative
performances of different channel sensing, allocation and coordination schemes
which takes into account the movement of vehicles is felt. In this paper, we
introduce a framework that can be used to define and compare such schemes in a
variety of scenarios. Simulation results clearly show the robustness of our
technique by eliminating the misdetections and reducing to a great extent the
false alarms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02222</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02222</id><created>2015-06-07</created><updated>2015-11-23</updated><authors><author><keyname>Wang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Dunson</keyname><forenames>David</forenames></author><author><keyname>Leng</keyname><forenames>Chenlei</forenames></author></authors><title>No penalty no tears: Least squares in high-dimensional linear models</title><categories>stat.ME cs.LG math.ST stat.ML stat.TH</categories><comments>corrected citation format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ordinary least squares (OLS) is the default method for fitting linear models,
but is not applicable for problems with dimensionality larger than the sample
size. For these problems, we advocate the use of a generalized version of OLS
motivated by ridge regression, and propose two novel three-step algorithms
involving least squares fitting and hard thresholding. The algorithms are
methodologically simple to understand intuitively, computationally easy to
implement efficiently, and theoretically appealing for choosing models
consistently. Numerical exercises comparing our methods with penalization-based
approaches in simulations and data analyses illustrate the great potential of
the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02226</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02226</id><created>2015-06-07</created><authors><author><keyname>Wang</keyname><forenames>Bingchen</forenames></author><author><keyname>Zhang</keyname><forenames>Chenglong</forenames></author><author><keyname>Song</keyname><forenames>Lei</forenames></author><author><keyname>Zhao</keyname><forenames>Lianhe</forenames></author><author><keyname>Dou</keyname><forenames>Yu</forenames></author><author><keyname>Yu</keyname><forenames>Zihao</forenames></author></authors><title>Design and optimization of DBSCAN Algorithm based on CUDA</title><categories>cs.DC</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DBSCAN is a very classic algorithm for data clus- tering, which is widely
used in many fields. However, with the data scale growing much more bigger than
before, the traditional serial algorithm can not meet the performance
requirement. Recently, parallel computing based on CUDA has developed very fast
and has great advantage on big data. This paper summarizes the algorithms
proposed before and improves the performance of the old DBSCAN algorithm by
using CUDA and parallel computing. The algorithm uses shared memory as much as
possible compared with other algorithms and it has very good scalability. A
data set is tested on the new version of DBSCAN. Finally, we analyze the
results and give a conclusion that our algorithm is approximately 97 times
faster than the serial version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02227</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02227</id><created>2015-06-07</created><authors><author><keyname>Csiba</keyname><forenames>Dominik</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Primal Method for ERM with Flexible Mini-batching Schemes and Non-convex
  Losses</title><categories>math.OC cs.DS cs.LG stat.ML</categories><comments>13 pages, 3 figures, 2 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we develop a new algorithm for regularized empirical risk
minimization. Our method extends recent techniques of Shalev-Shwartz [02/2015],
which enable a dual-free analysis of SDCA, to arbitrary mini-batching schemes.
Moreover, our method is able to better utilize the information in the data
defining the ERM problem. For convex loss functions, our complexity results
match those of QUARTZ, which is a primal-dual method also allowing for
arbitrary mini-batching schemes. The advantage of a dual-free analysis comes
from the fact that it guarantees convergence even for non-convex loss
functions, as long as the average loss is convex. We illustrate through
experiments the utility of being able to design arbitrary mini-batching
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02228</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02228</id><created>2015-06-07</created><updated>2015-06-22</updated><authors><author><keyname>Ding</keyname><forenames>Dawei</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Strong converse exponents for the feedback-assisted classical capacity
  of entanglement-breaking channels</title><categories>quant-ph cs.IT math.IT</categories><comments>20 pages, 2 figures, v2: minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum entanglement can be used in a communication scheme to establish a
correlation between successive channel inputs that is impossible by classical
means. It is known that the classical capacity of quantum channels can be
enhanced by such entangled encoding schemes, but this is not always the case.
In this paper, we prove that a strong converse theorem holds for the classical
capacity of an entanglement-breaking channel even when it is assisted by a
classical feedback link from the receiver to the transmitter. In doing so, we
identify a bound on the strong converse exponent, which determines the
exponentially decaying rate at which the success probability tends to zero when
the communication rate exceeds the capacity. Proving a strong converse, along
with an achievability theorem, shows that the classical capacity is a sharp
boundary between reliable and unreliable communication regimes. One of the main
tools in our proof is the sandwiched Renyi relative entropy. The same method of
proof is used to derive an exponential bound on the success probability when
communicating over an arbitrary quantum channel assisted by classical feedback,
provided that the transmitter does not use entangled encoding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02243</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02243</id><created>2015-06-07</created><updated>2015-12-25</updated><authors><author><keyname>Papoutsakis</keyname><forenames>Ioannis</forenames></author></authors><title>On approximating tree spanners that are breadth first search trees</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tree $t$-spanner $T$ of a graph $G$ is a spanning tree of $G$ such that the
distance in $T$ between every pair of verices is at most $t$ times the distance
in $G$ between them. There are efficient algorithms that find a tree $t\cdot
O(\log n)$-spanner of a graph $G$, when $G$ admits a tree $t$-spanner. In this
paper, the search space is narrowed to $v$-concentrated spanning trees, a
simple family that includes all the breadth first search trees starting from
vertex $v$. In this case, it is not easy to find approximate tree spanners
within factor almost $o(\log n)$. Specifically, let $m$ and $t$ be integers,
such that $m&gt;0$ and $t\geq 7$. If there is an efficient algorithm that receives
as input a graph $G$ and a vertex $v$ and returns a $v$-concentrated tree
$t\cdot o((\log n)^{m/(m+1)})$-spanner of $G$, when $G$ admits a
$v$-concentrated tree $t$-spanner, then there is an algorithm that decides
3-SAT in quasi-polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02245</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02245</id><created>2015-06-07</created><authors><author><keyname>Chen</keyname><forenames>Min</forenames></author><author><keyname>Golan</keyname><forenames>Amos</forenames></author></authors><title>What May Visualization Processes Optimize?</title><categories>cs.HC cs.IT math.IT</categories><comments>10 pages</comments><doi>10.1109/TVCG.2015.2513410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an abstract model of visualization and inference
processes and describe an information-theoretic measure for optimizing such
processes. In order to obtain such an abstraction, we first examined six
classes of workflows in data analysis and visualization, and identified four
levels of typical visualization components, namely disseminative,
observational, analytical and model-developmental visualization. We noticed a
common phenomenon at different levels of visualization, that is, the
transformation of data spaces (referred to as alphabets) usually corresponds to
the reduction of maximal entropy along a workflow. Based on this observation,
we establish an information-theoretic measure of cost-benefit ratio that may be
used as a cost function for optimizing a data visualization process. To
demonstrate the validity of this measure, we examined a number of successful
visualization processes in the literature, and showed that the
information-theoretic measure can mathematically explain the advantages of such
processes over possible alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02247</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02247</id><created>2015-06-07</created><authors><author><keyname>Galiano</keyname><forenames>Gonzalo</forenames></author><author><keyname>Schiavi</keyname><forenames>Emanuel</forenames></author><author><keyname>Velasco</keyname><forenames>Juli&#xe1;n</forenames></author></authors><title>Well-possedness of the continuous Neighborhood filter and its
  reformulation through the decreasing rearrangement</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the existence and uniqueness of solutions of a time-continuous
version of the Neighborhood filter. In the line of previous works, we
reformulate the usual pixel-based version of the filter introducing functional
rearrangements, which are shown to be useful both for the filter analysis and
for its fast implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02256</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02256</id><created>2015-06-07</created><authors><author><keyname>Tang</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Pan</keyname><forenames>Yiqiao</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyong</forenames></author></authors><title>Knowledge Transfer Pre-training</title><categories>cs.LG cs.NE stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1505.04630</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pre-training is crucial for learning deep neural networks. Most of existing
pre-training methods train simple models (e.g., restricted Boltzmann machines)
and then stack them layer by layer to form the deep structure. This layer-wise
pre-training has found strong theoretical foundation and broad empirical
support. However, it is not easy to employ such method to pre-train models
without a clear multi-layer structure,e.g., recurrent neural networks (RNNs).
This paper presents a new pre-training approach based on knowledge transfer
learning. In contrast to the layer-wise approach which trains model components
incrementally, the new approach trains the entire model as a whole but with an
easier objective function. This is achieved by utilizing soft targets produced
by a prior trained model (teacher model). Compared to the conventional
layer-wise methods, this new method does not care about the model structure, so
can be used to pre-train very complex models. Experiments on a speech
recognition task demonstrated that with this approach, complex RNNs can be well
trained with a weaker deep neural network (DNN) model. Furthermore, the new
method can be combined with conventional layer-wise pre-training to deliver
additional gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02263</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02263</id><created>2015-06-07</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author></authors><title>On Network Proximity in Web Applications</title><categories>cs.CY cs.HC cs.NI cs.SE</categories><comments>submitted to DCCN 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss one approach for development and deployment of web
sites (web pages) devoted to the description of objects (events) with a
precisely delineated geographic scope. This article describes the usage of
context-aware programming models for web development. In our paper, we propose
mechanisms to create mobile web applications which content links to some
predefined geographic area. The accuracy of such a binding allows us to
distinguish individual areas within the same indoor space. Target areas for
such development are applications for Smart Cities and retail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02264</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02264</id><created>2015-06-07</created><updated>2015-11-27</updated><authors><author><keyname>Hoshen</keyname><forenames>Yedid</forenames></author><author><keyname>Peleg</keyname><forenames>Shmuel</forenames></author></authors><title>Visual Learning of Arithmetic Operations</title><categories>cs.LG cs.AI cs.CV</categories><comments>To appear in AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple Neural Network model is presented for end-to-end visual learning of
arithmetic operations from pictures of numbers. The input consists of two
pictures, each showing a 7-digit number. The output, also a picture, displays
the number showing the result of an arithmetic operation (e.g., addition or
subtraction) on the two input numbers. The concepts of a number, or of an
operator, are not explicitly introduced. This indicates that addition is a
simple cognitive task, which can be learned visually using a very small number
of neurons.
  Other operations, e.g., multiplication, were not learnable using this
architecture. Some tasks were not learnable end-to-end (e.g., addition with
Roman numerals), but were easily learnable once broken into two separate
sub-tasks: a perceptual \textit{Character Recognition} and cognitive
\textit{Arithmetic} sub-tasks. This indicates that while some tasks may be
easily learnable end-to-end, other may need to be broken into sub-tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02265</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02265</id><created>2015-06-07</created><authors><author><keyname>Wang</keyname><forenames>Yilun</forenames></author><author><keyname>Zhang</keyname><forenames>Sheng</forenames></author><author><keyname>Zheng</keyname><forenames>Junjie</forenames></author><author><keyname>Chen</keyname><forenames>Heng</forenames></author><author><keyname>Chen</keyname><forenames>Huafu</forenames></author></authors><title>Randomized Structural Sparsity based Support Identification with
  Applications to Locating Activated or Discriminative Brain Areas: A
  Multi-center Reproducibility Study</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1410.4650</comments><msc-class>68T01</msc-class><acm-class>I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on how to locate the relevant or discriminative brain
regions related with external stimulus or certain mental decease, which is also
called support identification, based on the neuroimaging data. The main
difficulty lies in the extremely high dimensional voxel space and relatively
few training samples, easily resulting in an unstable brain region discovery
(or called feature selection in context of pattern recognition). When the
training samples are from different centers and have betweencenter variations,
it will be even harder to obtain a reliable and consistent result.
Corresponding, we revisit our recently proposed algorithm based on stability
selection and structural sparsity. It is applied to the multi-center MRI data
analysis for the first time. A consistent and stable result is achieved across
different centers despite the between-center data variation while many other
state-of-the-art methods such as two sample t-test fail. Moreover, we have
empirically showed that the performance of this algorithm is robust and
insensitive to several of its key parameters. In addition, the support
identification results on both functional MRI and structural MRI are
interpretable and can be the potential biomarkers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02268</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02268</id><created>2015-06-07</created><authors><author><keyname>Grispos</keyname><forenames>George</forenames></author><author><keyname>Glisson</keyname><forenames>William Bradley</forenames></author><author><keyname>Storer</keyname><forenames>Tim</forenames></author></authors><title>Recovering Residual Forensic Data from Smartphone Interactions with
  Cloud Storage Providers</title><categories>cs.CR</categories><journal-ref>2015. In The Cloud Security Ecosystem, edited by Ryan Ko and
  Kim-Kwang Raymond Choo, Syngress, Boston, Pages 347-382</journal-ref><doi>10.1016/B978-0-12-801595-7.00016-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing demand for cloud storage services such as Dropbox, Box,
Syncplicity and SugarSync. These public cloud storage services can store
gigabytes of corporate and personal data in remote data centres around the
world, which can then be synchronized to multiple devices. This creates an
environment which is potentially conducive to security incidents, data breaches
and other malicious activities. The forensic investigation of public cloud
environments presents a number of new challenges for the digital forensics
community. However, it is anticipated that end-devices such as smartphones,
will retain data from these cloud storage services. This research investigates
how forensic tools that are currently available to practitioners can be used to
provide a practical solution for the problems related to investigating cloud
storage environments. The research contribution is threefold. First, the
findings from this research support the idea that end-devices which have been
used to access cloud storage services can be used to provide a partial view of
the evidence stored in the cloud service. Second, the research provides a
comparison of the number of files which can be recovered from different
versions of cloud storage applications. In doing so, it also supports the idea
that amalgamating the files recovered from more than one device can result in
the recovery of a more complete dataset. Third, the chapter contributes to the
documentation and evidentiary discussion of the artefacts created from specific
cloud storage applications and different versions of these applications on iOS
and Android smartphones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02270</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02270</id><created>2015-06-07</created><authors><author><keyname>Kahl</keyname><forenames>Thomas</forenames></author></authors><title>Topological abstraction of higher-dimensional automata</title><categories>cs.FL math.AT</categories><comments>25 pages, 8 figures</comments><msc-class>68Q85, 68Q45, 55N35, 55U99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-dimensional automata constitute a very expressive model for concurrent
systems. In this paper, we discuss &quot;topological abstraction&quot; of
higher-dimensional automata, i.e., the replacement of HDAs by smaller ones that
can be considered equivalent from both a computer scientific and a topological
point of view. By definition, topological abstraction preserves the homotopy
type, the trace category, and the homology graph of an HDA. We establish
conditions under which cube collapses yield topological abstractions of HDAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02275</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02275</id><created>2015-06-07</created><updated>2015-08-22</updated><authors><author><keyname>Pavalanathan</keyname><forenames>Umashanthi</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>Confounds and Consequences in Geotagged Twitter Data</title><categories>cs.CL</categories><comments>final version for EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is often used in quantitative studies that identify
geographically-preferred topics, writing styles, and entities. These studies
rely on either GPS coordinates attached to individual messages, or on the
user-supplied location field in each profile. In this paper, we compare these
data acquisition techniques and quantify the biases that they introduce; we
also measure their effects on linguistic analysis and text-based geolocation.
GPS-tagging and self-reported locations yield measurably different corpora, and
these linguistic differences are partially attributable to differences in
dataset composition by age and gender. Using a latent variable model to induce
age and gender, we show how these demographic variables interact with geography
to affect language use. We also show that the accuracy of text-based
geolocation varies with population demographics, giving the best results for
men above the age of 40.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02281</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02281</id><created>2015-06-07</created><authors><author><keyname>Chang</keyname><forenames>Zheng</forenames></author><author><keyname>Ristaniemi</keyname><forenames>Tapani</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Queueing Game For Spectrum Access in Cognitive Radio Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of spectrum access decision-making
for the Secondary Users (SUs) in the cognitive radio networks. When the Primary
Users (PUs) are absent on certain frequency bandwidth, SUs can formulate a
queue and wait for the Base Station (BS) to serve. The queue of the SUs will be
dismissed if the PU is emerging in the system. Leveraging the queueing game
approaches, the decision-making process of the SUs that whether to queue or not
is studied. Both individual equilibrium and social optimization strategies are
derived analytically. Moreover, the optimal pricing strategy of the service
provider is investigated as well. Our proposed algorithms and corresponding
analysis are validated through simulation studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02288</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02288</id><created>2015-06-07</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>Robust and Tuneable Family of Gossiping Algorithms</title><categories>cs.DC</categories><comments>Paper presented at the 20th Euromicro International Conference on
  Parallel, Distributed and Network-Based Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a family of gossiping algorithms whose members share the same
structure though they vary their performance in function of a combinatorial
parameter. We show that such parameter may be considered as a &quot;knob&quot;
controlling the amount of communication parallelism characterizing the
algorithms. After this we introduce procedures to operate the knob and choose
parameters matching the amount of communication channels currently provided by
the available communication system(s). In so doing we provide a robust
mechanism to tune the production of requests for communication after the
current operational conditions of the consumers of such requests. This can be
used to achieve high performance and programmatic avoidance of undesirable
events such as message collisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02289</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02289</id><created>2015-06-07</created><authors><author><keyname>Goga</keyname><forenames>Oana</forenames></author><author><keyname>Loiseau</keyname><forenames>Patrick</forenames></author><author><keyname>Sommer</keyname><forenames>Robin</forenames></author><author><keyname>Teixeira</keyname><forenames>Renata</forenames></author><author><keyname>Gummadi</keyname><forenames>Krishna P.</forenames></author></authors><title>On the Reliability of Profile Matching Across Large Online Social
  Networks</title><categories>cs.SI</categories><comments>12 pages. To appear in KDD 2015. Extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matching the profiles of a user across multiple online social networks brings
opportunities for new services and applications as well as new insights on user
online behavior, yet it raises serious privacy concerns. Prior literature has
proposed methods to match profiles and showed that it is possible to do it
accurately, but using evaluations that focused on sampled datasets only. In
this paper, we study the extent to which we can reliably match profiles in
practice, across real-world social networks, by exploiting public attributes,
i.e., information users publicly provide about themselves. Today's social
networks have hundreds of millions of users, which brings completely new
challenges as a reliable matching scheme must identify the correct matching
profile out of the millions of possible profiles. We first define a set of
properties for profile attributes--Availability, Consistency,
non-Impersonability, and Discriminability (ACID)--that are both necessary and
sufficient to determine the reliability of a matching scheme. Using these
properties, we propose a method to evaluate the accuracy of matching schemes in
real practical cases. Our results show that the accuracy in practice is
significantly lower than the one reported in prior literature. When considering
entire social networks, there is a non-negligible number of profiles that
belong to different users but have similar attributes, which leads to many
false matches. Our paper sheds light on the limits of matching profiles in the
real world and illustrates the correct methodology to evaluate matching schemes
in realistic scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02294</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02294</id><created>2015-06-07</created><authors><author><keyname>Gong</keyname><forenames>Neil Zhenqiang</forenames></author><author><keyname>Payer</keyname><forenames>Mathias</forenames></author><author><keyname>Moazzezi</keyname><forenames>Reza</forenames></author><author><keyname>Frank</keyname><forenames>Mario</forenames></author></authors><title>Towards Forgery-Resistant Touch-based Biometric Authentication on Mobile
  Devices</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices store a diverse set of private user data and have gradually
become a hub to control users' other personal Internet-of-Things devices.
Access control on mobile devices is therefore highly important. The widely
accepted solution is to protect access by asking for the correct password.
However, password authentication is tedious, e.g., a user needs to input a
password every time she wants to use the device. Moreover, existing biometrics
such as face, fingerprint, and touch behaviors are vulnerable to forgery
attacks.
  We propose a new touch-based biometric authentication system that is passive
and secure against forgery attacks. In our touch-based authentication, a user's
touch behavior is a function of some random &quot;secret&quot;. The user can
subconsciously know the secret while touching the device's screen. However, an
attacker cannot know the secret at the time of attacks, which makes it
challenging to perform forgery attacks even if the attacker has already
obtained the user's touch behaviors. We evaluate our touch-based authentication
system via collecting data from 25 subjects. Results are promising: the random
secrets do not influence user experience and our system achieves 0.18 smaller
Equal Error Rates (EERs) than previous touch-based authentication for targeted
forgery attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02306</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02306</id><created>2015-06-07</created><authors><author><keyname>Lahiri</keyname><forenames>Shibamouli</forenames></author></authors><title>SQUINKY! A Corpus of Sentence-level Formality, Informativeness, and
  Implicature</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a corpus of 7,032 sentences rated by human annotators for
formality, informativeness, and implicature on a 1-7 scale. The corpus was
annotated using Amazon Mechanical Turk. Reliability in the obtained judgments
was examined by comparing mean ratings across two MTurk experiments, and
correlation with pilot annotations (on sentence formality) conducted in a more
controlled setting. Despite the subjectivity and inherent difficulty of the
annotation task, correlations between mean ratings were quite encouraging,
especially on formality and informativeness. We further explored correlation
between the three linguistic variables, genre-wise variation of ratings and
correlations within genres, compatibility with automatic stylistic scoring, and
sentential make-up of a document in terms of style. To date, our corpus is the
largest sentence-level annotated corpus released for formality,
informativeness, and implicature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02311</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02311</id><created>2015-06-07</created><authors><author><keyname>Fraczek</keyname><forenames>Wojciech</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>StegBlocks: ensuring perfect undetectability of network steganography</title><categories>cs.MM cs.CR</categories><comments>6 pages, 1 figure, Accepted to Fourth International Workshop on Cyber
  Crime (IWCC 2015), co-located with 10th International Conference on
  Availability, Reliability and Security (ARES 2015), Toulouse, France, 24-28
  August 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents StegBlocks, which defines a new concept for performing
undetectable hidden communication. StegBlocks is a general approach for
constructing methods of network steganography. In StegBlocks, one has to
determine objects with defined properties which will be used to transfer hidden
messages. The objects are dependent on a specific network protocol (or
application) used as a carrier for a given network steganography method.
Moreover, the paper presents the approach to perfect undetectability of network
steganography, which was developed based on the rules of undetectability for
general steganography. The approach to undetectability of network steganography
was used to show the possibility of developing perfectly undetectable network
steganography methods using the StegBlocks concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02312</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02312</id><created>2015-06-07</created><authors><author><keyname>Pereira</keyname><forenames>Renato de Pontes</forenames></author><author><keyname>Engel</keyname><forenames>Paulo Martins</forenames></author></authors><title>A Framework for Constrained and Adaptive Behavior-Based Agents</title><categories>cs.AI cs.LG cs.RO cs.SY</categories><comments>2015; 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behavior Trees are commonly used to model agents for robotics and games,
where constrained behaviors must be designed by human experts in order to
guarantee that these agents will execute a specific chain of actions given a
specific set of perceptions. In such application areas, learning is a desirable
feature to provide agents with the ability to adapt and improve interactions
with humans and environment, but often discarded due to its unreliability. In
this paper, we propose a framework that uses Reinforcement Learning nodes as
part of Behavior Trees to address the problem of adding learning capabilities
in constrained agents. We show how this framework relates to Options in
Hierarchical Reinforcement Learning, ensuring convergence of nested learning
nodes, and we empirically show that the learning nodes do not affect the
execution of other nodes in the tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02327</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02327</id><created>2015-06-07</created><authors><author><keyname>Chung</keyname><forenames>Cheng-Tao</forenames></author><author><keyname>Tsai</keyname><forenames>Cheng-Yu</forenames></author><author><keyname>Lu</keyname><forenames>Hsiang-Hung</forenames></author><author><keyname>Liou</keyname><forenames>Yuan-ming</forenames></author><author><keyname>Wu</keyname><forenames>Yen-Chen</forenames></author><author><keyname>Lu</keyname><forenames>Yen-Ju</forenames></author><author><keyname>Lee</keyname><forenames>Hung-yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for
  Unsupervised Discovery of Linguistic Units and Generation of High Quality
  Features</title><categories>cs.CL cs.LG cs.NE</categories><comments>submitted to Interspeech 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes the work done by the authors for the Zero Resource
Speech Challenge organized in the technical program of Interspeech 2015. The
goal of the challenge is to discover linguistic units directly from unlabeled
speech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work
automatically discovers multiple sets of acoustic tokens from the given corpus.
Each acoustic token set is specified by a set of hyperparameters that describe
the model configuration. These sets of acoustic tokens carry different
characteristics of the given corpus and the language behind thus can be
mutually reinforced. The multiple sets of token labels are then used as the
targets of a Multi-target DNN (MDNN) trained on low-level acoustic features.
Bottleneck features extracted from the MDNN are used as feedback for the MAT
and the MDNN itself. We call this iterative system the Multi-layered Acoustic
Tokenizing Deep Neural Network (MAT-DNN) which generates high quality features
for track 1 of the challenge and acoustic tokens for track 2 of the challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02328</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02328</id><created>2015-06-07</created><updated>2015-08-17</updated><authors><author><keyname>Ye</keyname><forenames>Guangnan</forenames></author><author><keyname>Li</keyname><forenames>Yitong</forenames></author><author><keyname>Xu</keyname><forenames>Hongliang</forenames></author><author><keyname>Liu</keyname><forenames>Dong</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>EventNet: A Large Scale Structured Concept Library for Complex Event
  Detection in Video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-specific concepts are the semantic concepts designed for the events of
interest, which can be used as a mid-level representation of complex events in
videos. Existing methods only focus on defining event-specific concepts for a
small number of predefined events, but cannot handle novel unseen events. This
motivates us to build a large scale event-specific concept library that covers
as many real-world events and their concepts as possible. Specifically, we
choose WikiHow, an online forum containing a large number of how-to articles on
human daily life events. We perform a coarse-to-fine event discovery process
and discover 500 events from WikiHow articles. Then we use each event name as
query to search YouTube and discover event-specific concepts from the tags of
returned videos. After an automatic filter process, we end up with 95,321
videos and 4,490 concepts. We train a Convolutional Neural Network (CNN) model
on the 95,321 videos over the 500 events, and use the model to extract deep
learning feature from video content. With the learned deep learning feature, we
train 4,490 binary SVM classifiers as the event-specific concept library. The
concepts and events are further organized in a hierarchical structure defined
by WikiHow, and the resultant concept library is called EventNet. Finally, the
EventNet concept library is used to generate concept based representation of
event videos. To the best of our knowledge, EventNet represents the first video
event ontology that organizes events and their concepts into a semantic
structure. It offers great potential for event retrieval and browsing.
Extensive experiments over the zero-shot event retrieval task when no training
samples are available show that the EventNet concept library consistently and
significantly outperforms the state-of-the-art (such as the 20K ImageNet
concepts trained with CNN) by a large margin up to 207%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02338</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02338</id><created>2015-06-07</created><updated>2015-06-10</updated><authors><author><keyname>Trask</keyname><forenames>Andrew</forenames></author><author><keyname>Gilmore</keyname><forenames>David</forenames></author><author><keyname>Russell</keyname><forenames>Matthew</forenames></author></authors><title>Modeling Order in Neural Word Embeddings at Scale</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural Language Processing (NLP) systems commonly leverage bag-of-words
co-occurrence techniques to capture semantic and syntactic word relationships.
The resulting word-level distributed representations often ignore morphological
information, though character-level embeddings have proven valuable to NLP
tasks. We propose a new neural language model incorporating both word order and
character order in its embedding. The model produces several vector spaces with
meaningful substructure, as evidenced by its performance of 85.8% on a recent
word-analogy task, exceeding best published syntactic word-analogy scores by a
58% error margin. Furthermore, the model includes several parallel training
methods, most notably allowing a skip-gram network with 160 billion parameters
to be trained overnight on 3 multi-core CPUs, 14x larger than the previous
largest neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02344</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02344</id><created>2015-06-07</created><updated>2015-06-18</updated><authors><author><keyname>Asteris</keyname><forenames>Megasthenis</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>and</keyname><forenames>Han-Gyol Yi</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>Bharath</forenames></author></authors><title>Stay on path: PCA along graph paths</title><categories>stat.ML cs.IT cs.LG math.IT math.OC</categories><comments>12 pages, 5 figures, In Proceedings of International Conference on
  Machine Learning (ICML) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a variant of (sparse) PCA in which the set of feasible support
sets is determined by a graph. In particular, we consider the following
setting: given a directed acyclic graph $G$ on $p$ vertices corresponding to
variables, the non-zero entries of the extracted principal component must
coincide with vertices lying along a path in $G$.
  From a statistical perspective, information on the underlying network may
potentially reduce the number of observations required to recover the
population principal component. We consider the canonical estimator which
optimally exploits the prior knowledge by solving a non-convex quadratic
maximization on the empirical covariance. We introduce a simple network and
analyze the estimator under the spiked covariance model. We show that side
information potentially improves the statistical complexity.
  We propose two algorithms to approximate the solution of the constrained
quadratic maximization, and recover a component with the desired properties. We
empirically evaluate our schemes on synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02345</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02345</id><created>2015-06-07</created><authors><author><keyname>Saveljev</keyname><forenames>Vladimir</forenames></author></authors><title>Wavelets and continuous wavelet transform for autostereoscopic multiview
  images</title><categories>cs.CV</categories><comments>4 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the reference functions for the synthesis and analysis of the
autostereoscopic multiview and integral images in three-dimensional displays we
introduced. In the current paper, we propose the wavelets to analyze such
images. The wavelets are built on the reference functions as on the scaling
functions of the wavelet analysis. The continuous wavelet transform was
successfully applied to the testing wireframe binary objects. The restored
locations correspond to the structure of the testing wireframe binary objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02348</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02348</id><created>2015-06-08</created><authors><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author><author><keyname>Kakade</keyname><forenames>Sham</forenames></author><author><keyname>Netrapalli</keyname><forenames>Praneeth</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>Convergence Rates of Active Learning for Maximum Likelihood Estimation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An active learner is given a class of models, a large set of unlabeled
examples, and the ability to interactively query labels of a subset of these
examples; the goal of the learner is to learn a model in the class that fits
the data well.
  Previous theoretical work has rigorously characterized label complexity of
active learning, but most of this work has focused on the PAC or the agnostic
PAC model. In this paper, we shift our attention to a more general setting --
maximum likelihood estimation. Provided certain conditions hold on the model
class, we provide a two-stage active learning algorithm for this problem. The
conditions we require are fairly general, and cover the widely popular class of
Generalized Linear Models, which in turn, include models for binary and
multi-class classification, regression, and conditional random fields.
  We provide an upper bound on the label requirement of our algorithm, and a
lower bound that matches it up to lower order terms. Our analysis shows that
unlike binary classification in the realizable case, just a single extra round
of interaction is sufficient to achieve near-optimal performance in maximum
likelihood estimation. On the empirical side, the recent work in
~\cite{Zhang12} and~\cite{Zhang14} (on active linear and logistic regression)
shows the promise of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02349</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02349</id><created>2015-06-08</created><authors><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Local Codes with Addition Based Repair</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the complexities of repair algorithms for locally repairable
codes and propose a class of codes that repair single node failures using
addition operations only, or codes with addition based repair. We construct two
families of codes with addition based repair. The first family attains distance
one less than the Singleton-like upper bound, while the second family attains
the Singleton-like upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02351</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02351</id><created>2015-06-08</created><updated>2016-02-14</updated><authors><author><keyname>Zhao</keyname><forenames>Junbo</forenames></author><author><keyname>Mathieu</keyname><forenames>Michael</forenames></author><author><keyname>Goroshin</keyname><forenames>Ross</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Stacked What-Where Auto-encoders</title><categories>stat.ML cs.LG cs.NE</categories><comments>Workshop track - ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel architecture, the &quot;stacked what-where auto-encoders&quot;
(SWWAE), which integrates discriminative and generative pathways and provides a
unified approach to supervised, semi-supervised and unsupervised learning
without relying on sampling during training. An instantiation of SWWAE uses a
convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and
employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the
reconstruction. The objective function includes reconstruction terms that
induce the hidden states in the Deconvnet to be similar to those of the
Convnet. Each pooling layer produces two sets of variables: the &quot;what&quot; which
are fed to the next layer, and its complementary variable &quot;where&quot; that are fed
to the corresponding layer in the generative decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02354</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02354</id><created>2015-06-08</created><authors><author><keyname>Shrestha</keyname><forenames>Anish Prasad</forenames></author><author><keyname>Kwak</keyname><forenames>Kyung Sup</forenames></author></authors><title>Secure Ad-hoc Routing Scheme</title><categories>cs.IT cs.CR cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates on the problem of combining routing scheme and
physical layer security in multihop wireless networks with cooperative
diversity. We propose an ad-hoc natured hop-by-hop best secure relay selection
in a multihop network with several relays and an eavesdropper at each hop which
provides a safe routing scheme to transmit confidential message from
transmitter to legitimate receiver. The selection is based on the instantaneous
channel conditions of relay and eavesdropper at each hop. A theoretical
analysis is performed to derive new closed form expressions for probability of
non-zero secrecy capacity along with the exact end to end secrecy outage
probability at a normalized secrecy rate. Furthermore, we provide the
asymptotic expression to gain insights on the diversity gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02361</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02361</id><created>2015-06-08</created><authors><author><keyname>Chevallier</keyname><forenames>Julien</forenames><affiliation>JAD</affiliation></author><author><keyname>Caceres</keyname><forenames>Maria J.</forenames><affiliation>LJLL, MAMBA</affiliation></author><author><keyname>Doumic</keyname><forenames>Marie</forenames><affiliation>LJLL, MAMBA</affiliation></author><author><keyname>Reynaud-Bouret</keyname><forenames>Patricia</forenames><affiliation>JAD</affiliation></author></authors><title>Microscopic approach of a time elapsed neural model</title><categories>cs.NE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spike trains are the main components of the information processing in the
brain. To model spike trains several point processes have been investigated in
the literature. And more macroscopic approaches have also been studied, using
partial differential equation models. The main aim of the present article is to
build a bridge between several point processes models (Poisson, Wold, Hawkes)
that have been proved to statistically fit real spike trains data and
age-structured partial differential equations as introduced by Pakdaman,
Perthame and Salort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02367</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02367</id><created>2015-06-08</created><updated>2015-07-02</updated><authors><author><keyname>Bendkowski</keyname><forenames>Maciej</forenames><affiliation>TCS</affiliation></author><author><keyname>Grygiel</keyname><forenames>Katarzyna</forenames><affiliation>TCS</affiliation></author><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>TCS, LIP</affiliation></author><author><keyname>Zaionc</keyname><forenames>Marek</forenames><affiliation>TCS</affiliation></author></authors><title>A natural counting of lambda terms</title><categories>cs.LO cs.DM cs.PL math.CO math.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the sequences of numbers corresponding to lambda terms of given
sizes, where the size is this of lambda terms with de Bruijn indices in a very
natural model where all the operators have size 1. For plain lambda terms, the
sequence corresponds to two families of binary trees for which we exhibit
bijections. We study also the distribution of normal forms, head normal forms
and strongly normalizing terms. In particular we show that strongly normalizing
terms are of density 0 among plain terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02369</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02369</id><created>2015-06-08</created><authors><author><keyname>Muscholl</keyname><forenames>Anca</forenames><affiliation>LaBRI</affiliation></author></authors><title>Automated Synthesis of Distributed Controllers</title><categories>cs.FL cs.LO cs.SY</categories><comments>ICALP 2015, Jul 2015, Kyoto, Japan</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthesis is a particularly challenging problem for concurrent programs. At
the same time it is a very promising approach, since concurrent programs are
difficult to get right, or to analyze with traditional verification techniques.
This paper gives an introduction to distributed synthesis in the setting of
Mazurkiewicz traces, and its applications to decentralized runtime monitoring.
1 Context Modern computing systems are increasingly distributed and
heterogeneous. Software needs to be able to exploit these advances, providing
means for applications to be more performant. Traditional concurrent
programming paradigms, as in Java, are based on threads, shared-memory, and
locking mechanisms that guard access to common data. More recent paradigms like
the reactive programming model of Erlang [4] and Scala [35,36] replace shared
memory by asynchronous message passing, where sending a message is
non-blocking. In all these concurrent frameworks, writing reliable software is
a serious challenge. Programmers tend to think about code mostly in a
sequential way, and it is hard to grasp all possible schedulings of events in a
concurrent execution. For similar reasons, verification and analysis of
concurrent programs is a difficult task. Testing, which is still the main
method for error detection in software, has low coverage for concurrent
programs. The reason is that bugs in such programs are difficult to reproduce:
they may happen under very specific thread schedules and the likelihood of
taking such corner-case schedules is very low. Automated verification, such as
model-checking and other traditional exploration techniques, can handle very
limited instances of concurrent programs, mostly because of the very large
number of possible states and of possible interleavings of executions. Formal
analysis of programs requires as a prerequisite a clean mathematical model for
programs. Verification of sequential programs starts usually with an
abstraction step -- reducing the value domains of variables to finite domains,
viewing conditional branching as non-determinism, etc. Another major
simplification consists in disallowing recursion. This leads to a very robust
computational model, namely finite-state automata and regular languages.
Regular languages of words (and trees) are particularly well understood
notions. The deep connections between logic and automata revealed by the
foundational work of B\&quot;uchi, Rabin, and others, are the main ingredients in
automata-based verification .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02386</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02386</id><created>2015-06-08</created><updated>2016-02-24</updated><authors><author><keyname>Buchnik</keyname><forenames>Eliav</forenames></author><author><keyname>Cohen</keyname><forenames>Edith</forenames></author></authors><title>Reverse Ranking by Graph Structure: Model and Scalable Algorithms</title><categories>cs.SI</categories><comments>13 pages, 4 figures, Sigmetrics 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distances in a network capture relations between nodes and are the basis of
centrality, similarity, and influence measures. Often, however, the relevance
of a node $u$ to a node $v$ is more precisely measured not by the magnitude of
the distance, but by the number of nodes that are closer to $v$ than $u$. That
is, by the {\em rank} of $u$ in an ordering of nodes by increasing distance
from $v$.
  We identify and address fundamental challenges in rank-based graph mining. We
first consider single-source computation of reverse-ranks and design a
&quot;Dijkstra-like&quot; algorithm which computes nodes in order of increasing
approximate reverse rank while only traversing edges adjacent to returned
nodes. We then define {\em reverse-rank influence}, which naturally extends
reverse nearest neighbors influence [Korn and Muthukrishnan 2000] and builds on
a well studied distance-based influence. We present near-linear algorithms for
greedy approximate reverse-rank influence maximization. The design relies on
our single-source algorithm. Our algorithms utilize near-linear preprocessing
of the network to compute all-distance sketches. As a contribution of
independent interest, we present a novel algorithm for computing these
sketches, which have many other applications, on multi-core architectures.
  We complement our algorithms by establishing the hardness of computing {\em
exact} reverse-ranks for a single source and {\em exact} reverse-rank
influence. This implies that when using near-linear algorithms, the small
relative errors we obtain are the best we can currently hope for.
  Finally, we conduct an experimental evaluation on graphs with tens of
millions of edges, demonstrating both scalability and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02396</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02396</id><created>2015-06-08</created><updated>2016-01-25</updated><authors><author><keyname>Peng</keyname><forenames>Zhimin</forenames></author><author><keyname>Xu</keyname><forenames>Yangyang</forenames></author><author><keyname>Yan</keyname><forenames>Ming</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author></authors><title>ARock: an Algorithmic Framework for Asynchronous Parallel Coordinate
  Updates</title><categories>math.OC cs.DC stat.ML</categories><comments>updated the linear convergence proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a fixed point to a nonexpansive operator, i.e., $x^*=Tx^*$, abstracts
many problems in numerical linear algebra, optimization, and other areas of
scientific computing. To solve fixed-point problems, we propose ARock, an
algorithmic framework in which multiple agents (machines, processors, or cores)
update $x$ in an asynchronous parallel fashion. Asynchrony is crucial to
parallel computing since it reduces synchronization wait, relaxes communication
bottleneck, and thus speeds up computing significantly. At each step of ARock,
an agent updates a randomly selected coordinate $x_i$ based on possibly
out-of-date information on $x$. The agents share $x$ through either global
memory or communication. If writing $x_i$ is atomic, the agents can read and
write $x$ without memory locks.
  Theoretically, we show that if the nonexpansive operator $T$ has a fixed
point, then with probability one, ARock generates a sequence that converges to
a fixed points of $T$. Our conditions on $T$ and step sizes are weaker than
comparable work. Linear convergence is also obtained.
  We propose special cases of ARock for linear systems, convex optimization,
machine learning, as well as distributed and decentralized consensus problems.
Numerical experiments of solving sparse logistic regression problems are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02400</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02400</id><created>2015-06-08</created><updated>2016-01-15</updated><authors><author><keyname>Brunton</keyname><forenames>Alan</forenames></author><author><keyname>Arikan</keyname><forenames>Can Ates</forenames></author><author><keyname>Urban</keyname><forenames>Philipp</forenames></author></authors><title>Pushing the Limits of 3D Color Printing: Error Diffusion with
  Translucent Materials</title><categories>cs.GR</categories><comments>15 pages, 14 figures; includes supplemental figures</comments><journal-ref>ACM Transactions on Graphics, 35(1), Article 4, December 2015</journal-ref><doi>10.1145/2832905</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate color reproduction is important in many applications of 3D printing,
from design prototypes to 3D color copies or portraits. Although full color is
available via other technologies, multi-jet printers have greater potential for
graphical 3D printing, in terms of reproducing complex appearance properties.
However, to date these printers cannot produce full color, and doing so poses
substantial technical challenges, from the shear amount of data to the
translucency of the available color materials. In this paper, we propose an
error diffusion halftoning approach to achieve full color with multi-jet
printers, which operates on multiple isosurfaces or layers within the object.
We propose a novel traversal algorithm for voxel surfaces, which allows the
transfer of existing error diffusion algorithms from 2D printing. The resulting
prints faithfully reproduce colors, color gradients and fine-scale details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02420</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02420</id><created>2015-06-08</created><updated>2016-02-24</updated><authors><author><keyname>Kashyap</keyname><forenames>Salil</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>On the Feasibility of Wireless Energy Transfer Using Massive Antenna
  Arrays</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. Wireless Communication, 2016 (14 pages, 10
  figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We illustrate potential benefits of using massive antenna arrays for wireless
energy transfer (WET). Specifically, we analyze probability of outage in WET
over fading channels when a base station (BS) with multiple antennas beamforms
energy to a wireless sensor node (WSN). Our analytical results show that by
using massive antenna arrays, the range of WET can be increased for a given
target outage probability. We prove that by using multiple-antenna arrays at
the BS, a lower downlink energy is required to get the same outage performance,
resulting into savings of radiated energy. We show that for energy levels used
in WET, the outage performance with least-squares or minimum mean-square error
channel estimates is same as that obtained based on perfect channel estimates.
We observe that a strong line-of-sight component between the BS and WSN lowers
outage probability. Furthermore, by deploying more antennas at the BS, a larger
energy can be transferred reliably to the WSN at a given target outage
performance for the sensor to be able to perform its main tasks. In our
numerical examples, the RF power received at the input of the sensor is assumed
to be on the order of a mW, such that the rectenna operates at an efficiency in
the order of 50 %.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02423</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02423</id><created>2015-06-08</created><authors><author><keyname>Rahmany</keyname><forenames>Sajjad</forenames></author><author><keyname>Basiri</keyname><forenames>Abdolali</forenames></author><author><keyname>-Alizadeh</keyname><forenames>Benyamin M.</forenames></author></authors><title>An Elimination Method to Solve Interval Polynomial Systems</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are several efficient methods to solve linear interval polynomial
systems in the context of interval computations, however, the general case of
interval polynomial systems is not yet covered as well. In this paper we
introduce a new elimination method to solve and analyse interval polynomial
systems, in general case. This method is based on computational algebraic
geometry concepts such as polynomial ideals and Groebner basis computation.
Specially, we use the comprehensive Groebner system concept to keep the
dependencies between interval coefficients. At the end of paper, we will state
some applications of our method to evaluate its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02424</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02424</id><created>2015-06-08</created><authors><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Zhao</keyname><forenames>Zhongkai</forenames></author></authors><title>Algorithms for finding transposons in gene sequences</title><categories>q-bio.GN cs.CE cs.DS</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the process of evolution, some genes will change their relative
positions in gene sequence. These &quot;jumping genes&quot; are called transposons.
Through some intuitive rules, we give a criterion to determine transposons
among gene sequences of different individuals of the same species. Then we turn
this problem into graph theory and give algorithms for different situations
with acceptable time complexities. One of these algorithms has been reported
briefly as the &quot;iteration algorithm&quot; in Kang et al.'s paper (in this paper,
transposon is called &quot;core-gene-defined genome organizational framework&quot;,
cGOF). This paper provides the omitted details and discussions on general
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02428</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02428</id><created>2015-06-08</created><authors><author><keyname>Bhatia</keyname><forenames>Kush</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author></authors><title>Robust Regression via Hard Thresholding</title><categories>cs.LG stat.ML</categories><comments>24 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of Robust Least Squares Regression (RLSR) where several
response variables can be adversarially corrupted. More specifically, for a
data matrix X \in R^{p x n} and an underlying model w*, the response vector is
generated as y = X'w* + b where b \in R^n is the corruption vector supported
over at most C.n coordinates. Existing exact recovery results for RLSR focus
solely on L1-penalty based convex formulations and impose relatively strict
model assumptions such as requiring the corruptions b to be selected
independently of X.
  In this work, we study a simple hard-thresholding algorithm called TORRENT
which, under mild conditions on X, can recover w* exactly even if b corrupts
the response variables in an adversarial manner, i.e. both the support and
entries of b are selected adversarially after observing X and w*. Our results
hold under deterministic assumptions which are satisfied if X is sampled from
any sub-Gaussian distribution. Finally unlike existing results that apply only
to a fixed w*, generated independently of X, our results are universal and hold
for any w* \in R^p.
  Next, we propose gradient descent-based extensions of TORRENT that can scale
efficiently to large scale problems, such as high dimensional sparse recovery
and prove similar recovery guarantees for these extensions. Empirically we find
TORRENT, and more so its extensions, offering significantly faster recovery
than the state-of-the-art L1 solvers. For instance, even on moderate-sized
datasets (with p = 50K) with around 40% corrupted responses, a variant of our
proposed method called TORRENT-HYB is more than 20x faster than the best L1
solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02431</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02431</id><created>2015-06-08</created><updated>2015-08-11</updated><authors><author><keyname>Ranco</keyname><forenames>Gabriele</forenames></author><author><keyname>Aleksovski</keyname><forenames>Darko</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Gr&#x10d;ar</keyname><forenames>Miha</forenames></author><author><keyname>Mozeti&#x10d;</keyname><forenames>Igor</forenames></author></authors><title>The Effects of Twitter Sentiment on Stock Price Returns</title><categories>cs.CY cs.SI</categories><journal-ref>PLoS ONE 10(9): e0138441 (2015)</journal-ref><doi>10.1371/journal.pone.0138441</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media are increasingly reflecting and influencing behavior of other
complex systems. In this paper we investigate the relations between a well-know
micro-blogging platform Twitter and financial markets. In particular, we
consider, in a period of 15 months, the Twitter volume and sentiment about the
30 stock companies that form the Dow Jones Industrial Average (DJIA) index. We
find a relatively low Pearson correlation and Granger causality between the
corresponding time series over the entire time period. However, we find a
significant dependence between the Twitter sentiment and abnormal returns
during the peaks of Twitter volume. This is valid not only for the expected
Twitter volume peaks (e.g., quarterly announcements), but also for peaks
corresponding to less obvious events. We formalize the procedure by adapting
the well-known &quot;event study&quot; from economics and finance to the analysis of
Twitter data. The procedure allows to automatically identify events as Twitter
volume peaks, to compute the prevailing sentiment (positive or negative)
expressed in tweets at these peaks, and finally to apply the &quot;event study&quot;
methodology to relate them to stock returns. We show that sentiment polarity of
Twitter peaks implies the direction of cumulative abnormal returns. The amount
of cumulative abnormal returns is relatively low (about 1-2%), but the
dependence is statistically significant for several days after the events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02432</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02432</id><created>2015-06-08</created><authors><author><keyname>Henderson</keyname><forenames>Craig</forenames></author><author><keyname>Izquierdo</keyname><forenames>Ebroul</forenames></author></authors><title>Reflection Invariance: an important consideration of image orientation</title><categories>cs.CV</categories><comments>7 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper, we consider the state of computer vision research
with respect to invariance to the horizontal orientation of an image -- what we
term reflection invariance. We describe why we consider reflection invariance
to be an important property and provide evidence where the absence of this
invariance produces surprising inconsistencies in state-of-the-art systems. We
demonstrate inconsistencies in methods of object detection and scene
classification when they are presented with images and the horizontal mirror of
those images. Finally, we examine where some of the invariance is exhibited in
feature detection and descriptors, and make a case for future consideration of
reflection invariance as a measure of quality in computer vision algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02434</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02434</id><created>2015-06-08</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Hansen</keyname><forenames>Kristoffer Arnsfelt</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author></authors><title>Strategy Complexity of Concurrent Stochastic Games with Safety and
  Reachability Objectives</title><categories>cs.GT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider finite-state concurrent stochastic games, played by k&gt;=2 players
for an infinite number of rounds, where in every round, each player
simultaneously and independently of the other players chooses an action,
whereafter the successor state is determined by a probability distribution
given by the current state and the chosen actions. We consider reachability
objectives that given a target set of states require that some state in the
target is visited, and the dual safety objectives that given a target set
require that only states in the target set are visited. We are interested in
the complexity of stationary strategies measured by their patience, which is
defined as the inverse of the smallest nonzero probability employed. Our main
results are as follows: We show that in two-player zero-sum concurrent
stochastic games (with reachability objective for one player and the
complementary safety objective for the other player): (i) the optimal bound on
the patience of optimal and epsilon-optimal strategies, for both players is
doubly exponential; and (ii) even in games with a single nonabsorbing state
exponential (in the number of actions) patience is necessary. In general we
study the class of non-zero-sum games admitting stationary epsilon-Nash
equilibria. We show that if there is at least one player with reachability
objective, then doubly-exponential patience may be needed for epsilon-Nash
equilibrium strategies, whereas in contrast if all players have safety
objectives, the optimal bound on patience for epsilon-Nash equilibrium
strategies is only exponential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02438</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02438</id><created>2015-06-08</created><updated>2016-01-21</updated><authors><author><keyname>Schulman</keyname><forenames>John</forenames></author><author><keyname>Moritz</keyname><forenames>Philipp</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Jordan</keyname><forenames>Michael</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>High-Dimensional Continuous Control Using Generalized Advantage
  Estimation</title><categories>cs.LG cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy gradient methods are an appealing approach in reinforcement learning
because they directly optimize the cumulative reward and can straightforwardly
be used with nonlinear function approximators such as neural networks. The two
main challenges are the large number of samples typically required, and the
difficulty of obtaining stable and steady improvement despite the
nonstationarity of the incoming data. We address the first challenge by using
value functions to substantially reduce the variance of policy gradient
estimates at the cost of some bias, with an exponentially-weighted estimator of
the advantage function that is analogous to TD(lambda). We address the second
challenge by using trust region optimization procedure for both the policy and
the value function, which are represented by neural networks.
  Our approach yields strong empirical results on highly challenging 3D
locomotion tasks, learning running gaits for bipedal and quadrupedal simulated
robots, and learning a policy for getting the biped to stand up from starting
out lying on the ground. In contrast to a body of prior work that uses
hand-crafted policy representations, our neural network policies map directly
from raw kinematics to joint torques. Our algorithm is fully model-free, and
the amount of simulated experience required for the learning tasks on 3D bipeds
corresponds to 1-2 weeks of real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02442</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02442</id><created>2015-06-08</created><authors><author><keyname>Rusu</keyname><forenames>Irena</forenames></author></authors><title>NP-hardness of sortedness constraints</title><categories>cs.CC cs.AI cs.DM</categories><comments>15 pages, 4 figures</comments><msc-class>90C10, 68Q25</msc-class><acm-class>F.2.0; F.4.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Constraint Programming, global constraints allow to model and solve many
combinatorial problems. Among these constraints, several sortedness constraints
have been defined, for which propagation algorithms are available, but for
which the tractability is not settled. We show that the sort(U,V) constraint
(Older et. al, 1995) is intractable for integer variables whose domains are not
limited to intervals. As a consequence, the similar result holds for the
sort(U,V, P) constraint (Zhou, 1996). Moreover, the intractability holds even
under the stability condition present in the recently introduced
keysorting(U,V,Keys,P) constraint (Carlsson et al., 2014), and requiring that
the order of the variables with the same value in the list U be preserved in
the list V. Therefore, keysorting(U,V,Keys,P) is intractable as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02448</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02448</id><created>2015-06-08</created><updated>2015-08-21</updated><authors><author><keyname>Shajaiah</keyname><forenames>Haya</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>An Efficient Multi-Carrier Resource Allocation with User Discrimination
  Framework for 5G Wireless Systems</title><categories>cs.NI</categories><comments>Under Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an efficient resource allocation with user
discrimination framework for 5G Wireless Systems to allocate multiple carriers
resources among users with elastic and inelastic traffic. Each application
running on the user equipment (UE) is assigned an application utility function.
In the proposed model, different classes of user groups are considered and
users are partitioned into different groups based on the carriers coverage
area. Each user has a minimum required application rate based on its class and
the type of its application. Our objective is to allocate multiple carriers
resources optimally among users, that belong to different classes, located
within the carriers' coverage area. We use a utility proportional fairness
approach in the utility percentage of the application running on the UE. Each
user is guaranteed a minimum quality of service (QoS) with a priority criterion
that is based on user's class and the type of application running on the UE. In
addition, we prove the existence of optimal solutions for the proposed resource
allocation optimization problem and present a multi-carrier resource allocation
with user discrimination algorithm. Finally, we present simulation results for
the performance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02449</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02449</id><created>2015-06-08</created><updated>2015-06-09</updated><authors><author><keyname>Blagus</keyname><forenames>Neli</forenames></author><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Empirical comparison of network sampling techniques</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the past few years, the storage and analysis of large-scale and fast
evolving networks present a great challenge. Therefore, a number of different
techniques have been proposed for sampling large networks. In general, network
exploration techniques approximate the original networks more accurately than
random node and link selection. Yet, link selection with additional subgraph
induction step outperforms most other techniques. In this paper, we apply
subgraph induction also to random walk and forest-fire sampling. We analyze
different real-world networks and the changes of their properties introduced by
sampling. We compare several sampling techniques based on the match between the
original networks and their sampled variants. The results reveal that the
techniques with subgraph induction underestimate the degree and clustering
distribution, while overestimate average degree and density of the original
networks. Techniques without subgraph induction step exhibit exactly the
opposite behavior. Hence, the performance of the sampling techniques from
random selection category compared to network exploration sampling does not
differ significantly, while clear differences exist between the techniques with
subgraph induction step and the ones without it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02455</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02455</id><created>2015-06-08</created><authors><author><keyname>Abbes</keyname><forenames>Samy</forenames></author><author><keyname>Mairesse</keyname><forenames>Jean</forenames></author></authors><title>Uniform generation in trace monoids</title><categories>cs.FL</categories><comments>Full version of the paper in MFCS 2015 with the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of random uniform generation of traces (the elements
of a free partially commutative monoid) in light of the uniform measure on the
boundary at infinity of the associated monoid. We obtain a product
decomposition of the uniform measure at infinity if the trace monoid has
several irreducible components-a case where other notions such as Parry
measures, are not defined. Random generation algorithms are then examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02465</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02465</id><created>2015-06-08</created><updated>2016-02-24</updated><authors><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Kerschke</keyname><forenames>Pascal</forenames></author><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author><author><keyname>Lindauer</keyname><forenames>Marius</forenames></author><author><keyname>Malitsky</keyname><forenames>Yuri</forenames></author><author><keyname>Frechette</keyname><forenames>Alexandre</forenames></author><author><keyname>Hoos</keyname><forenames>Holger</forenames></author><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author><author><keyname>Tierney</keyname><forenames>Kevin</forenames></author><author><keyname>Vanschoren</keyname><forenames>Joaquin</forenames></author></authors><title>ASlib: A Benchmark Library for Algorithm Selection</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of algorithm selection involves choosing an algorithm from a set of
algorithms on a per-instance basis in order to exploit the varying performance
of algorithms over a set of instances. The algorithm selection problem is
attracting increasing attention from researchers and practitioners in AI. Years
of fruitful applications in a number of domains have resulted in a large amount
of data, but the community lacks a standard format or repository for this data.
This situation makes it difficult to share and compare different approaches
effectively, as is done in other, more established fields. It also
unnecessarily hinders new researchers who want to work in this area. To address
this problem, we introduce a standardized format for representing algorithm
selection scenarios and a repository that contains a growing number of data
sets from the literature. Our format has been designed to be able to express a
wide variety of different scenarios. Demonstrating the breadth and power of our
platform, we describe a set of example experiments that build and evaluate
algorithm selection models through a common interface. The results display the
potential of algorithm selection to achieve significant performance
improvements across a broad range of problems and algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02483</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02483</id><created>2015-06-08</created><updated>2015-11-29</updated><authors><author><keyname>Chakraborty</keyname><forenames>Souymodip</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author></authors><title>On the Hardness of PCTL Satisfiability</title><categories>cs.LO</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  Theorem 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the satisfiability problem for probabilistic CTL (PCTL,
for short) is undecidable. By a reduction from $1\frac{1}{2}$-player games with
PCTL winning objectives, we establish that the PCTL satisfiability problem is
${\Sigma}_1^1$-hard. We present an exponential-time algorithm for the
satisfiability of a bounded, negation-closed fragment of PCTL, and show that
the satisfiability problem for this fragment is EXPTIME-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02484</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02484</id><created>2015-06-05</created><updated>2015-09-06</updated><authors><author><keyname>Bianchi</keyname><forenames>G.</forenames></author><author><keyname>Kuznetsov</keyname><forenames>N. V.</forenames></author><author><keyname>Leonov</keyname><forenames>G. A.</forenames></author><author><keyname>Yuldashev</keyname><forenames>M. V.</forenames></author><author><keyname>Yuldashev</keyname><forenames>R. V.</forenames></author></authors><title>Limitations of PLL simulation: hidden oscillations in MatLab and SPICE</title><categories>cs.OH math.DS nlin.CD</categories><journal-ref>IEEE 2015 7th International Congress on Ultra Modern
  Telecommunications and Control Systems and Workshops (ICUMT), 2015, pp. 79-84</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear analysis of the phase-locked loop (PLL) based circuits is a
challenging task, thus in modern engineering literature simplified mathematical
models and simulation are widely used for their study. In this work the
limitations of numerical approach is discussed and it is shown that, e.g.
hidden oscillations may not be found by simulation. Corresponding examples in
SPICE and MatLab, which may lead to wrong conclusions concerning the
operability of PLL-based circuits, are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02509</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02509</id><created>2015-06-08</created><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>SVM and ELM: Who Wins? Object Recognition with Deep Convolutional
  Features from ImageNet</title><categories>cs.LG cs.CV</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning with a convolutional neural network (CNN) has been proved to be
very effective in feature extraction and representation of images. For image
classification problems, this work aim at finding which classifier is more
competitive based on high-level deep features of images. In this report, we
have discussed the nearest neighbor, support vector machines and extreme
learning machines for image classification under deep convolutional activation
feature representation. Specifically, we adopt the benchmark object recognition
dataset from multiple sources with domain bias for evaluating different
classifiers. The deep features of the object dataset are obtained by a
well-trained CNN with five convolutional layers and three fully-connected
layers on the challenging ImageNet. Experiments demonstrate that the ELMs
outperform SVMs in cross-domain recognition tasks. In particular,
state-of-the-art results are obtained by kernel ELM which outperforms SVMs with
about 4% of the average accuracy. The features and codes are available in
http://www.escience.cn/people/lei/index.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02510</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02510</id><created>2015-06-08</created><authors><author><keyname>Dikmen</keyname><forenames>Onur</forenames></author></authors><title>Learning Mixtures of Ising Models using Pseudolikelihood</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum pseudolikelihood method has been among the most important methods for
learning parameters of statistical physics models, such as Ising models. In
this paper, we study how pseudolikelihood can be derived for learning
parameters of a mixture of Ising models. The performance of the proposed
approach is demonstrated for Ising and Potts models on both synthetic and real
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02515</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02515</id><created>2015-06-08</created><updated>2015-12-07</updated><authors><author><keyname>Lebedev</keyname><forenames>Vadim</forenames></author><author><keyname>Lempitsky</keyname><forenames>Victor</forenames></author></authors><title>Fast ConvNets Using Group-wise Brain Damage</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the idea of brain damage, i.e. the pruning of the coefficients of
a neural network, and suggest how brain damage can be modified and used to
speedup convolutional layers. The approach uses the fact that many efficient
implementations reduce generalized convolutions to matrix multiplications. The
suggested brain damage process prunes the convolutional kernel tensor in a
group-wise fashion by adding group-sparsity regularization to the standard
training process. After such group-wise pruning, convolutions can be reduced to
multiplications of thinned dense matrices, which leads to speedup. In the
comparison on AlexNet, the method achieves very competitive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02516</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02516</id><created>2015-06-08</created><updated>2015-11-03</updated><authors><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author><author><keyname>Hermann</keyname><forenames>Karl Moritz</forenames></author><author><keyname>Suleyman</keyname><forenames>Mustafa</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Learning to Transduce with Unbounded Memory</title><categories>cs.NE cs.CL cs.LG</categories><comments>14 pages, 4 figures, NIPS 2015</comments><msc-class>68T05</msc-class><acm-class>I.5.1; I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, strong results have been demonstrated by Deep Recurrent Neural
Networks on natural language transduction problems. In this paper we explore
the representational power of these models using synthetic grammars designed to
exhibit phenomena similar to those found in real transduction problems such as
machine translation. These experiments lead us to propose new memory-based
recurrent networks that implement continuously differentiable analogues of
traditional data structures such as Stacks, Queues, and DeQues. We show that
these architectures exhibit superior generalisation performance to Deep RNNs
and are often able to learn the underlying generating algorithms in our
transduction experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02517</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02517</id><created>2015-06-08</created><updated>2015-11-10</updated><authors><author><keyname>Campello</keyname><forenames>Antonio</forenames></author><author><keyname>Jorge</keyname><forenames>Grasiele C.</forenames></author><author><keyname>Strapasson</keyname><forenames>and Jo&#xe3;o</forenames></author><author><keyname>Costa</keyname><forenames>Sueli I. R.</forenames></author></authors><title>Perfect codes in the lp metric</title><categories>math.CO cs.IT math.IT</categories><comments>21 pages, 9 figures, minor corrections, accepted for publication
  European Journal of Combinatorics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate perfect codes in $\mathbb{Z}^n$ under the $\ell_p$ metric.
Upper bounds for the packing radius $r$ of a linear perfect code, in terms of
the metric parameter $p$ and the dimension $n$ are derived. For $p = 2$ and $n
= 2, 3$, we determine all radii for which there are linear perfect codes. The
non-existence results for codes in $\mathbb{Z}^n$ presented here imply
non-existence results for codes over finite alphabets $\mathbb{Z}_q$, when the
alphabet size is large enough, and has implications on some recent
constructions of spherical codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02530</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02530</id><created>2015-06-08</created><authors><author><keyname>Ma</keyname><forenames>Chenxin</forenames></author><author><keyname>Tappenden</keyname><forenames>Rachael</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Linear Convergence of the Randomized Feasible Descent Method Under the
  Weak Strong Convexity Assumption</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we generalize the framework of the feasible descent method
(FDM) to a randomized (R-FDM) and a coordinate-wise random feasible descent
method (RC-FDM) framework. We show that the famous SDCA algorithm for
optimizing the SVM dual problem, or the stochastic coordinate descent method
for the LASSO problem, fits into the framework of RC-FDM. We prove linear
convergence for both R-FDM and RC-FDM under the weak strong convexity
assumption. Moreover, we show that the duality gap converges linearly for
RC-FDM, which implies that the duality gap also converges linearly for SDCA
applied to the SVM dual problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02531</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02531</id><created>2015-06-08</created><authors><author><keyname>Duhart</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Sauvage</keyname><forenames>Pierre</forenames></author><author><keyname>Bertelle</keyname><forenames>Cyrille</forenames></author></authors><title>EMMA: A Resource Oriented Framework for Service Choreography over
  Wireless Sensor and Actor Networks</title><categories>cs.NI cs.DC</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current Internet of Things (IoT) development requires service distribution
over Wireless Sensor and Actor Networks (WSAN) to deal with the drastic
increasing of network management complexity. Because of the specific
constraints of WSAN, centralized approaches are strongly limited. Multi-hop
communication used by WSAN introduces transmission latency, packet errors,
router congestion and security issues. As it uses local services, a
decentralized service model avoid long path communications between nodes and
applications. But the main issue is then to have such local services installed
on the desired nodes. Environment Monitoring and Management Agent (EMMA) system
proposes a set of software to deploy and to execute such services over Wireless
Sensor and Actor Networks (WSAN) through a middleware based on Resource
Oriented Architecture (ROA). Its Internet integration and the local management
of data heterogeneity are facilitated through the use of current standard
protocols such as IPv6 LoW Power Wireless Area Networks (6LoWPAN) and
Constrained Application Protocol (CoAP). This contribution presents EMMA
middleware, methodology and tools used to determine efficient service mapping
and its deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02535</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02535</id><created>2015-06-08</created><updated>2015-11-20</updated><authors><author><keyname>Fortier-Dubois</keyname><forenames>Louis</forenames></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author><author><keyname>Robitaille</keyname><forenames>Louis-Emile</forenames></author><author><keyname>Roy</keyname><forenames>Jean-Francis</forenames></author></authors><title>Efficient Learning of Ensembles with QuadBoost</title><categories>cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first present a general risk bound for ensembles that depends on the Lp
norm of the weighted combination of voters which can be selected from a
continuous set. We then propose a boosting method, called QuadBoost, which is
strongly supported by the general risk bound and has very simple rules for
assigning the voters' weights. Moreover, QuadBoost exhibits a rate of decrease
of its empirical error which is slightly faster than the one achieved by
AdaBoost. The experimental results confirm the expectation of the theory that
QuadBoost is a very efficient method for learning ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02541</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02541</id><created>2015-06-08</created><authors><author><keyname>Sanjari</keyname><forenames>S.</forenames></author><author><keyname>Ozgoli</keyname><forenames>S.</forenames></author></authors><title>Finite time analysis based on Sum of Squares Technique: Applied to the
  super-twisting second order sliding mode control</title><categories>cs.SY</categories><report-no>1506.2541</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite time analysis of the continuous system is investigated through both
stability and stabilization based on Sum of squares programming. A systematic
approach is proposed to construct Lyapunov function and Control Lyapunov
function for this objective. The Region of reaching, the set which has the
property that all trajectories starting from initial point inside it reach to
the origin in finite time, is introduced, and The largest subset of region of
reaching is estimated using Lyapunov based technique. The main results are
presented to give sufficient conditions which can be translated by
semi-definite program. These conditions are provided a feasibility problem
involving sum of squares constraints. The results of the paper are then
verified by several simulation and numerical examples. Furthermore, one
important practical application namely as a super twisting second order sliding
mode control also is presented using the proposed result to illustrate its
effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02543</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02543</id><created>2015-06-08</created><authors><author><keyname>Islam</keyname><forenames>Noman</forenames></author><author><keyname>Shaikh</keyname><forenames>Zubair A.</forenames></author></authors><title>A novel approach to Service Discovery in Mobile Adhoc Network</title><categories>cs.NI</categories><comments>International Networking and Communications Conference (INCC-2008)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Adhoc Network (MANET) is a network of a number of mobile routers and
associated hosts, organized in a random fashion via wireless links. During
recent years MANET has gained enormous amount of attention and has been widely
used for not only military purposes but for search-and-rescue operations,
intelligent transportation system, data collection, virtual classrooms and
ubiquitous computing. Service Discovery is one of the most important issues in
MANET. It is defined as the process of facilitating service providers to
advertise their services in a dynamic way and to allow consumers to discover
and access those services in an efficient and scalable manner. In this paper,
we are proposing a flexible and efficient approach to service discovery for
MANET by extending the work of Torres et al. (2004)...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02544</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02544</id><created>2015-06-08</created><updated>2015-12-04</updated><authors><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author><author><keyname>Voinea</keyname><forenames>Stephen</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Learning with Group Invariant Features: A Kernel Perspective</title><categories>cs.LG cs.CV stat.ML</categories><comments>NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze in this paper a random feature map based on a theory of invariance
I-theory introduced recently. More specifically, a group invariant signal
signature is obtained through cumulative distributions of group transformed
random projections. Our analysis bridges invariant feature learning with kernel
methods, as we show that this feature map defines an expected Haar integration
kernel that is invariant to the specified group action. We show how this
non-linear random feature map approximates this group invariant kernel
uniformly on a set of $N$ points. Moreover, we show that it defines a function
space that is dense in the equivalent Invariant Reproducing Kernel Hilbert
Space. Finally, we quantify error rates of the convergence of the empirical
risk minimization, as well as the reduction in the sample complexity of a
learning algorithm using such an invariant representation for signal
classification, in a classical supervised learning setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02548</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02548</id><created>2015-06-08</created><authors><author><keyname>Cohen</keyname><forenames>Stephen D.</forenames></author><author><keyname>Hasan</keyname><forenames>Sartaj Ul</forenames></author><author><keyname>Panario</keyname><forenames>Daniel</forenames></author><author><keyname>Wang</keyname><forenames>Qiang</forenames></author></authors><title>An asymptotic formula for the number of irreducible transformation shift
  registers</title><categories>math.CO cs.IT math.IT</categories><comments>14 pages</comments><msc-class>5B33, 12E20, 11T71, 12E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of enumerating the number of irreducible
transformation shift registers. We give an asymptotic formula for the number of
irreducible transformation shift registers in some special cases. Moreover, we
derive a short proof for the exact number of irreducible transformation shift
registers of order two using a recent generalization of a theorem of Carlitz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02550</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02550</id><created>2015-06-08</created><updated>2015-06-29</updated><authors><author><keyname>Komiyama</keyname><forenames>Junpei</forenames></author><author><keyname>Honda</keyname><forenames>Junya</forenames></author><author><keyname>Kashima</keyname><forenames>Hisashi</forenames></author><author><keyname>Nakagawa</keyname><forenames>Hiroshi</forenames></author></authors><title>Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem</title><categories>stat.ML cs.LG</categories><comments>26 pages, 10 figures, to appear in COLT2015 (ver.3: revised related
  work (RUCB))</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the $K$-armed dueling bandit problem, a variation of the standard
stochastic bandit problem where the feedback is limited to relative comparisons
of a pair of arms. We introduce a tight asymptotic regret lower bound that is
based on the information divergence. An algorithm that is inspired by the
Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010)
is proposed, and its regret is analyzed. The proposed algorithm is found to be
the first one with a regret upper bound that matches the lower bound.
Experimental comparisons of dueling bandit algorithms show that the proposed
algorithm significantly outperforms existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02553</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02553</id><created>2015-06-08</created><authors><author><keyname>Islam</keyname><forenames>Noman</forenames></author><author><keyname>Shaikh</keyname><forenames>Zubair A.</forenames></author></authors><title>Exploiting Correlation among Data Items for Cache Replacement in Ad-hoc
  Networks</title><categories>cs.NI</categories><comments>2nd IEEE International Conference on Information Management and
  Engineering (IEEE ICIME 2010), Chengdu, China, April 14-16, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have proposed a novel technique for cache replacement in
Ad-hoc Network based on the mining of Association Rules
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02554</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02554</id><created>2015-06-08</created><updated>2016-01-08</updated><authors><author><keyname>Heinze</keyname><forenames>Christina</forenames></author><author><keyname>McWilliams</keyname><forenames>Brian</forenames></author><author><keyname>Meinshausen</keyname><forenames>Nicolai</forenames></author></authors><title>DUAL-LOCO: Distributing Statistical Estimation Using Random Projections</title><categories>stat.ML cs.DC cs.LG</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present DUAL-LOCO, a communication-efficient algorithm for distributed
statistical estimation. DUAL-LOCO assumes that the data is distributed
according to the features rather than the samples. It requires only a single
round of communication where low-dimensional random projections are used to
approximate the dependences between features available to different workers. We
show that DUAL-LOCO has bounded approximation error which only depends weakly
on the number of workers. We compare DUAL-LOCO against a state-of-the-art
distributed optimization method on a variety of real world datasets and show
that it obtains better speedups while retaining good accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02556</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02556</id><created>2015-06-08</created><authors><author><keyname>Islam</keyname><forenames>Noman</forenames></author><author><keyname>Shaikh</keyname><forenames>Zubair A.</forenames></author></authors><title>Service Discovery in Mobile Ad hoc Networks Using Association Rules
  Mining</title><categories>cs.NI</categories><comments>13th IEEE International Multitopic Conference 2009 (INMIC-2009),
  Islamabad, Pakistan, December 14-15, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have proposed a novel approach to Service Discovery in Mobile Ad hoc
Networks. We have simulated the proposed approach in JIST/SWANS simulator and
the results have shown significant performance improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02557</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02557</id><created>2015-06-08</created><updated>2015-12-20</updated><authors><author><keyname>Kingma</keyname><forenames>Diederik P.</forenames></author><author><keyname>Salimans</keyname><forenames>Tim</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Variational Dropout and the Local Reparameterization Trick</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a local reparameterizaton technique for greatly reducing the
variance of stochastic gradients for variational Bayesian inference (SGVB) of a
posterior over model parameters, while retaining parallelizability. This local
reparameterization translates uncertainty about global parameters into local
noise that is independent across datapoints in the minibatch. Such
parameterizations can be trivially parallelized and have variance that is
inversely proportional to the minibatch size, generally leading to much faster
convergence. Additionally, we explore a connection with dropout: Gaussian
dropout objectives correspond to SGVB with local reparameterization, a
scale-invariant prior and proportionally fixed posterior variance. Our method
allows inference of more flexibly parameterized posteriors; specifically, we
propose variational dropout, a generalization of Gaussian dropout where the
dropout rates are learned, often leading to better models. The method is
demonstrated through several experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02558</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02558</id><created>2015-06-08</created><authors><author><keyname>Islam</keyname><forenames>Noman</forenames></author><author><keyname>Shaikh</keyname><forenames>Zubair A.</forenames></author><author><keyname>Talpur</keyname><forenames>Shahnawaz</forenames></author></authors><title>Towards a Grid-based approach to Traffic Routing in VANET</title><categories>cs.NI</categories><comments>Paper presented at E-Indus (E-Indus 2008), Institute of Industrial
  and Electronics Engineering, Karachi, Pakistan, April 29th, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have addressed a very important issue of Traffic
Management.Our proposed approach provides assistance in traffic routing by
integrating VANET and Grid Computing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02561</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02561</id><created>2015-06-08</created><authors><author><keyname>Jabbour</keyname><forenames>Said</forenames></author><author><keyname>Sais</keyname><forenames>Lakhdar</forenames></author><author><keyname>Salhi</keyname><forenames>Yakoub</forenames></author></authors><title>On SAT Models Enumeration in Itemset Mining</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequent itemset mining is an essential part of data analysis and data
mining. Recent works propose interesting SAT-based encodings for the problem of
discovering frequent itemsets. Our aim in this work is to define strategies for
adapting SAT solvers to such encodings in order to improve models enumeration.
In this context, we deeply study the effects of restart, branching heuristics
and clauses learning. We then conduct an experimental evaluation on SAT-Based
itemset mining instances to show how SAT solvers can be adapted to obtain an
efficient SAT model enumerator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02565</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02565</id><created>2015-06-08</created><updated>2015-12-23</updated><authors><author><keyname>Kim</keyname><forenames>Yong-Deok</forenames></author><author><keyname>Jang</keyname><forenames>Taewoong</forenames></author><author><keyname>Han</keyname><forenames>Bohyung</forenames></author><author><keyname>Choi</keyname><forenames>Seungjin</forenames></author></authors><title>Learning to Select Pre-Trained Deep Representations with Bayesian
  Evidence Framework</title><categories>cs.CV cs.LG stat.ML</categories><comments>9 pages, 5 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian evidence framework to facilitate transfer learning from
pre-trained deep convolutional neural networks (CNNs). Our framework is
formulated on top of a least squares SVM (LS-SVM) classifier, which is simple
and fast in both training and testing, and achieves competitive performance in
practice. The regularization parameters in LS-SVM is estimated automatically
without grid search and cross-validation by maximizing evidence, which is a
useful measure to select the best performing CNN out of multiple candidates for
transfer learning; the evidence is optimized efficiently by employing Aitken's
delta-squared process, which accelerates convergence of fixed point update. The
proposed Bayesian evidence framework also provides a good solution to identify
the best ensemble of heterogeneous CNNs through a greedy algorithm. Our
Bayesian evidence framework for transfer learning is tested on 12 visual
recognition datasets and illustrates the state-of-the-art performance
consistently in terms of prediction accuracy and modeling efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02568</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02568</id><created>2015-06-08</created><updated>2015-06-12</updated><authors><author><keyname>Xu</keyname><forenames>Liqing</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author></authors><title>Deterministic Construction of RIP Matrices in Compressed Sensing from
  Constant Weight Codes</title><categories>cs.IT math.IT</categories><comments>11 pages, submitted</comments><msc-class>94A15, 94BXX</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expicit restricted isometry property (RIP) measurement matrices are
needed in practical application of compressed sensing in signal processing. RIP
matrices from Reed-Solomon codes, BCH codes, orthogonal codes, expander graphs
have been proposed and analysised. On the other hand binary constant weight
codes have been studied for many years and many optimal or near-optimal small
weight and ditance constant weight codes have been determined. In this paper we
propose a new deterministic construction of RIP measurement matrices in
compressed sensing from binary and ternary contant weight codes. The sparse
orders and the number of budged rows in the new constant-weight-code-based RIP
matrices can be arbitrary. These contant-weight-code based RIP matrices have
better parameters compared with the DeVore RIP matrices when the sizes are
small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02572</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02572</id><created>2015-06-08</created><updated>2015-06-29</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Shaikhet</keyname><forenames>Alina</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Probing Convex Polygons with a Wedge</title><categories>cs.CG</categories><comments>30 pages, 26 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimizing the number of probes is one of the main challenges in
reconstructing geometric objects with probing devices. In this paper, we
investigate the problem of using an $\omega$-wedge probing tool to determine
the exact shape and orientation of a convex polygon. An $\omega$-wedge consists
of two rays emanating from a point called the apex of the wedge and the two
rays forming an angle $\omega$. A valid $\omega$-probe of a convex polygon $O$
contains $O$ within the $\omega$-wedge and its outcome consists of the
coordinates of the apex, the orientation of both rays and the coordinates of
the closest (to the apex) points of contact between $O$ and each of the rays.
  We present algorithms minimizing the number of probes and prove their
optimality. In particular, we show how to reconstruct a convex $n$-gon (with
all internal angles of size larger than $\omega$) using $2n-2$ $\omega$-probes;
if $\omega = \pi/2$, the reconstruction uses $2n-3$ $\omega$-probes. We show
that both results are optimal. Let $N_B$ be the number of vertices of $O$ whose
internal angle is at most $\omega$, (we show that $0 \leq N_B \leq 3$). We
determine the shape and orientation of a general convex $n$-gon with $N_B=1$
(respectively $N_B=2$, $N_B=3$) using $2n-1$ (respectively $2n+3$, $2n+5$)
$\omega$-probes. We prove optimality for the first case. Assuming the algorithm
knows the value of $N_B$ in advance, the reconstruction of $O$ with $N_B=2$ or
$N_B=3$ can be achieved with $2n+2$ probes,- which is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02574</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02574</id><created>2015-06-08</created><updated>2015-11-25</updated><authors><author><keyname>Simpson</keyname><forenames>Olivia</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>McGregor</keyname><forenames>Andrew</forenames></author></authors><title>Catching the head, tail, and everything in between: a streaming
  algorithm for the degree distribution</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degree distribution is one of the most fundamental graph properties of
interest for real-world graphs. It has been widely observed in numerous domains
that graphs typically have a tailed or scale-free degree distribution. While
the average degree is usually quite small, the variance is quite high and there
are vertices with degrees at all scales. We focus on the problem of
approximating the degree distribution of a large streaming graph, with small
storage. We design an algorithm headtail, whose main novelty is a new estimator
of infrequent degrees using truncated geometric random variables. We give a
mathematical analysis of headtail and show that it has excellent behavior in
practice. We can process streams will millions of edges with storage less than
1% and get extremely accurate approximations for all scales in the degree
distribution.
  We also introduce a new notion of Relative Hausdorff distance between tailed
histograms. Existing notions of distances between distributions are not
suitable, since they ignore infrequent degrees in the tail. The Relative
Hausdorff distance measures deviations at all scales, and is a more suitable
distance for comparing degree distributions. By tracking this new measure, we
are able to give strong empirical evidence of the convergence of headtail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02575</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02575</id><created>2015-06-08</created><authors><author><keyname>Simperler</keyname><forenames>Alexandra</forenames></author><author><keyname>Wilson</keyname><forenames>Greg</forenames></author></authors><title>Software Carpentry get more done in less time</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this study was to investigate if participants of Software
Carpentry (SC) get more done in less time. We asked 32 questions to assess 24
former participants to analyse if SC gave them the computing skills to
accomplish this. Our research shows that time was already saved during the
workshop as it could shorten the learning process of new skills. A majority of
participants were able to use these new skills straight away and thus could
speed up their day to day work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02582</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02582</id><created>2015-06-08</created><updated>2015-08-12</updated><authors><author><keyname>Yu</keyname><forenames>Huizhen</forenames></author></authors><title>On Convergence of Emphatic Temporal-Difference Learning</title><categories>cs.LG</categories><comments>45 pages; an oversight in a proof in Appendix C of the first version
  has been corrected. A shorter article based on the first version appeared at
  the 28th Annual Conference on Learning Theory (COLT), 2015</comments><msc-class>90C40, 62L20, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider emphatic temporal-difference learning algorithms for policy
evaluation in discounted Markov decision processes with finite spaces. Such
algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an
improved solution to the problem of divergence of off-policy
temporal-difference learning with linear function approximation. We present in
this paper the first convergence proofs for two emphatic algorithms,
ETD($\lambda$) and ELSTD($\lambda$). We prove, under general off-policy
conditions, the convergence in $L^1$ for ELSTD($\lambda$) iterates, and the
almost sure convergence of the approximate value functions calculated by both
algorithms using a single infinitely long trajectory. Our analysis involves new
techniques with applications beyond emphatic algorithms leading, for example,
to the first proof that standard TD($\lambda$) also converges under off-policy
training for $\lambda$ sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02585</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02585</id><created>2015-06-08</created><authors><author><keyname>Peng</keyname><forenames>Zhimin</forenames></author><author><keyname>Gurram</keyname><forenames>Prudhvi</forenames></author><author><keyname>Kwon</keyname><forenames>Heesung</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author></authors><title>Optimal Sparse Kernel Learning for Hyperspectral Anomaly Detection</title><categories>cs.LG</categories><comments>4 pages, 1 figure, 5th workshop on Hyperspectral image and signal
  processing: evolution in remote sensing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel framework of sparse kernel learning for Support Vector
Data Description (SVDD) based anomaly detection is presented. In this work,
optimal sparse feature selection for anomaly detection is first modeled as a
Mixed Integer Programming (MIP) problem. Due to the prohibitively high
computational complexity of the MIP, it is relaxed into a Quadratically
Constrained Linear Programming (QCLP) problem. The QCLP problem can then be
practically solved by using an iterative optimization method, in which multiple
subsets of features are iteratively found as opposed to a single subset. The
QCLP-based iterative optimization problem is solved in a finite space called
the \emph{Empirical Kernel Feature Space} (EKFS) instead of in the input space
or \emph{Reproducing Kernel Hilbert Space} (RKHS). This is possible because of
the fact that the geometrical properties of the EKFS and the corresponding RKHS
remain the same. Now, an explicit nonlinear exploitation of the data in a
finite EKFS is achievable, which results in optimal feature ranking.
Experimental results based on a hyperspectral image show that the proposed
method can provide improved performance over the current state-of-the-art
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02588</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02588</id><created>2015-06-08</created><updated>2015-11-30</updated><authors><author><keyname>Douze</keyname><forenames>Matthijs</forenames></author><author><keyname>Revaud</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Verbeek</keyname><forenames>Jakob</forenames></author><author><keyname>J&#xe9;gou</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>Circulant temporal encoding for video retrieval and temporal alignment</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of specific video event retrieval. Given a query video
of a specific event, e.g., a concert of Madonna, the goal is to retrieve other
videos of the same event that temporally overlap with the query. Our approach
encodes the frame descriptors of a video to jointly represent their appearance
and temporal order. It exploits the properties of circulant matrices to
efficiently compare the videos in the frequency domain. This offers a
significant gain in complexity and accurately localizes the matching parts of
videos. The descriptors can be compressed in the frequency domain with a
product quantizer adapted to complex numbers. In this case, video retrieval is
performed without decompressing the descriptors. We also consider the temporal
alignment of a set of videos. We exploit the matching confidence and an
estimate of the temporal offset computed for all pairs of videos by our
retrieval approach. Our robust algorithm aligns the videos on a global timeline
by maximizing the set of temporally consistent matches. The global temporal
alignment enables synchronous playback of the videos of a given scene.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02594</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02594</id><created>2015-06-08</created><authors><author><keyname>Zhao</keyname><forenames>Qingyuan</forenames></author><author><keyname>Erdogdu</keyname><forenames>Murat A.</forenames></author><author><keyname>He</keyname><forenames>Hera Y.</forenames></author><author><keyname>Rajaraman</keyname><forenames>Anand</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>SEISMIC: A Self-Exciting Point Process Model for Predicting Tweet
  Popularity</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>10 pages, published in KDD 2015</comments><msc-class>60G55, 62P25</msc-class><acm-class>H.2.8</acm-class><journal-ref>KDD '15, Proceedings of the 21th ACM SIGKDD International
  Conference on Knowledge Discovery and Data Mining (2015), Pages 1513-1522</journal-ref><doi>10.1145/2783258.2783401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networking websites allow users to create and share content. Big
information cascades of post resharing can form as users of these sites reshare
others' posts with their friends and followers. One of the central challenges
in understanding such cascading behaviors is in forecasting information
outbreaks, where a single post becomes widely popular by being reshared by many
users. In this paper, we focus on predicting the final number of reshares of a
given post. We build on the theory of self-exciting point processes to develop
a statistical model that allows us to make accurate predictions. Our model
requires no training or expensive feature engineering. It results in a simple
and efficiently computable formula that allows us to answer questions, in
real-time, such as: Given a post's resharing history so far, what is our
current estimate of its final number of reshares? Is the post resharing cascade
past the initial stage of explosive growth? And, which posts will be the most
reshared in the future? We validate our model using one month of complete
Twitter data and demonstrate a strong improvement in predictive accuracy over
existing approaches. Our model gives only 15% relative error in predicting
final size of an average information cascade after observing it for just one
hour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02597</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02597</id><created>2015-06-08</created><authors><author><keyname>Dytso</keyname><forenames>Alex</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author></authors><title>Interference as Noise: Friend or Foe?</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that for the two-user Gaussian Interference Channel (G-IC)
Treating Interference as Noise without Time Sharing (TINnoTS) achieves the
closure of the capacity region to within either a constant gap, or to within a
gap of the order O(logln(min(S,I))) where S is the largest Signal to Noise
Ratio (SNR) on the direct links and I is the largest Interference to Noise
Ratio (INR) on the cross links. As a consequence, TINnoTS is optimal from a
generalized Degrees of Freedom (gDoF) perspective for all channel gains except
for a subset of zero measure. TINnoTS with Gaussian inputs is known to be
optimal to within 1/2 bit for a subset of the weak interference regime.
Surprisingly, this paper shows that TINnoTS is gDoG optimal in all parameter
regimes, even in the strong and very strong interference regimes where joint
decoding of Gaussian inputs is optimal. For approximate optimality of TINnoTS
in all parameter regimes it is critical to use non-Gaussian inputs. This work
thus proposes to use mixed inputs as channel inputs where a mixed input is the
sum of a discrete and a Gaussian random variable. Interestingly, compared to
the Han-Kobayashi inner bound, the discrete part of a mixed input is shown to
effectively act as a common message in the sense that, although treated as
noise, its effect on the achievable rate region is as if it were jointly
decoded together with the desired messages at a non-intended receiver. The
practical implication is that a discrete interfering input is a 'friend', while
a Gaussian interfering input is in general a 'foe'. Since TINnoTS requires
neither joint decoding nor time sharing, the results of this paper are
applicable to a variety of oblivions or asynchronous channels, such as the
block asynchronous G-IC (which is not an information stable) and the G-IC with
partial codebook knowledge at one or more receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02602</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02602</id><created>2015-06-08</created><updated>2015-12-01</updated><authors><author><keyname>Banerjee</keyname><forenames>Soumya Jyoti</forenames></author><author><keyname>Azharuddin</keyname><forenames>Mohammad</forenames></author><author><keyname>Sen</keyname><forenames>Debanjan</forenames></author><author><keyname>Savale</keyname><forenames>Smruti</forenames></author><author><keyname>Datta</keyname><forenames>Himadri</forenames></author><author><keyname>Dasgupta</keyname><forenames>Anjan Kr</forenames></author><author><keyname>Roy</keyname><forenames>Soumen</forenames></author></authors><title>Using complex networks towards information retrieval and diagnostics in
  multidimensional imaging</title><categories>cs.IR cond-mat.stat-mech physics.soc-ph q-bio.QM</categories><comments>Replaced by published version. Detailed Supplementary Information on
  journal website</comments><journal-ref>Scientific Reports, 5: 17271 (2015)</journal-ref><doi>10.1038/srep17271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fresh and broad yet simple approach towards information
retrieval in general and diagnostics in particular by applying the theory of
complex networks on multidimensional, dynamic images. We demonstrate a
successful use of our method with the time series generated from high content
thermal imaging videos of patients suffering from the aqueous deficient dry eye
(ADDE) disease. Remarkably, network analyses of thermal imaging time series of
contact lens users and patients upon whom Laser-Assisted in situ Keratomileusis
(Lasik) surgery has been conducted, exhibit pronounced similarity with results
obtained from ADDE patients. We also propose a general framework for the
transformation of multidimensional images to networks for futuristic biometry.
Our approach is general and scalable to other fluctuation-based devices where
network parameters derived from fluctuations, act as effective discriminators
and diagnostic markers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02616</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02616</id><created>2015-06-08</created><authors><author><keyname>Cord-Landwehr</keyname><forenames>Andreas</forenames></author><author><keyname>Lenzner</keyname><forenames>Pascal</forenames></author></authors><title>Network Creation Games: Think Global - Act Local</title><categories>cs.GT</categories><comments>An extended abstract of this paper has been accepted for publication
  in the proceedings of the 40th International Conference on Mathematical
  Foundations on Computer Science</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a non-cooperative game-theoretic model for the formation of
communication networks by selfish agents. Each agent aims for a central
position at minimum cost for creating edges. In particular, the general model
(Fabrikant et al., PODC'03) became popular for studying the structure of the
Internet or social networks. Despite its significance, locality in this game
was first studied only recently (Bil\`o et al., SPAA'14), where a worst case
locality model was presented, which came with a high efficiency loss in terms
of quality of equilibria. Our main contribution is a new and more optimistic
view on locality: agents are limited in their knowledge and actions to their
local view ranges, but can probe different strategies and finally choose the
best. We study the influence of our locality notion on the hardness of
computing best responses, convergence to equilibria, and quality of equilibria.
Moreover, we compare the strength of local versus non-local strategy-changes.
Our results address the gap between the original model and the worst case
locality variant. On the bright side, our efficiency results are in line with
observations from the original model, yet we have a non-constant lower bound on
the price of anarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02617</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02617</id><created>2015-06-08</created><authors><author><keyname>Neyshabur</keyname><forenames>Behnam</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Path-SGD: Path-Normalized Optimization in Deep Neural Networks</title><categories>cs.LG cs.CV cs.NE stat.ML</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the choice of SGD for training deep neural networks by
reconsidering the appropriate geometry in which to optimize the weights. We
argue for a geometry invariant to rescaling of weights that does not affect the
output of the network, and suggest Path-SGD, which is an approximate steepest
descent method with respect to a path-wise regularizer related to max-norm
regularization. Path-SGD is easy and efficient to implement and leads to
empirical gains over SGD and AdaGrad.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02618</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02618</id><created>2015-06-08</created><authors><author><keyname>Bliss</keyname><forenames>Nathan</forenames></author><author><keyname>Sommars</keyname><forenames>Jeff</forenames></author><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author><author><keyname>Yu</keyname><forenames>Xiangcheng</forenames></author></authors><title>Solving Polynomial Systems in the Cloud with Polynomial Homotopy
  Continuation</title><categories>cs.MS cs.NA cs.SC math.AG math.NA</categories><comments>Accepted for publication in the Proceedings of CASC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial systems occur in many fields of science and engineering.
Polynomial homotopy continuation methods apply symbolic-numeric algorithms to
solve polynomial systems. We describe the design and implementation of our web
interface and reflect on the application of polynomial homotopy continuation
methods to solve polynomial systems in the cloud. Via the graph isomorphism
problem we organize and classify the polynomial systems we solved. The
classification with the canonical form of a graph identifies newly submitted
systems with systems that have already been solved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02620</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02620</id><created>2015-06-08</created><updated>2016-02-14</updated><authors><author><keyname>Lee</keyname><forenames>Ching-pei</forenames></author><author><keyname>Chang</keyname><forenames>Kai-Wei</forenames></author><author><keyname>Upadhyay</keyname><forenames>Shyam</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author></authors><title>Distributed Training of Structured SVM</title><categories>stat.ML cs.DC cs.LG</categories><comments>NIPS Workshop on Optimization for Machine Learning, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training structured prediction models is time-consuming. However, most
existing approaches only use a single machine, thus, the advantage of computing
power and the capacity for larger data sets of multiple machines have not been
exploited. In this work, we propose an efficient algorithm for distributedly
training structured support vector machines based on a distributed
block-coordinate descent method. Both theoretical and experimental results
indicate that our method is efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02626</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02626</id><created>2015-06-08</created><updated>2015-10-30</updated><authors><author><keyname>Han</keyname><forenames>Song</forenames></author><author><keyname>Pool</keyname><forenames>Jeff</forenames></author><author><keyname>Tran</keyname><forenames>John</forenames></author><author><keyname>Dally</keyname><forenames>William J.</forenames></author></authors><title>Learning both Weights and Connections for Efficient Neural Networks</title><categories>cs.NE cs.CV cs.LG</categories><comments>Published as a conference paper at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks are both computationally intensive and memory intensive,
making them difficult to deploy on embedded systems. Also, conventional
networks fix the architecture before training starts; as a result, training
cannot improve the architecture. To address these limitations, we describe a
method to reduce the storage and computation required by neural networks by an
order of magnitude without affecting their accuracy by learning only the
important connections. Our method prunes redundant connections using a
three-step method. First, we train the network to learn which connections are
important. Next, we prune the unimportant connections. Finally, we retrain the
network to fine tune the weights of the remaining connections. On the ImageNet
dataset, our method reduced the number of parameters of AlexNet by a factor of
9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar
experiments with VGG-16 found that the number of parameters can be reduced by
13x, from 138 million to 10.3 million, again with no loss of accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02629</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02629</id><created>2015-06-08</created><updated>2015-09-25</updated><authors><author><keyname>Dwork</keyname><forenames>Cynthia</forenames></author><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Pitassi</keyname><forenames>Toniann</forenames></author><author><keyname>Reingold</keyname><forenames>Omer</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Generalization in Adaptive Data Analysis and Holdout Reuse</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Overfitting is the bane of data analysts, even when data are plentiful.
Formal approaches to understanding this problem focus on statistical inference
and generalization of individual analysis procedures. Yet the practice of data
analysis is an inherently interactive and adaptive process: new analyses and
hypotheses are proposed after seeing the results of previous ones, parameters
are tuned on the basis of obtained results, and datasets are shared and reused.
An investigation of this gap has recently been initiated by the authors in
(Dwork et al., 2014), where we focused on the problem of estimating
expectations of adaptively chosen functions.
  In this paper, we give a simple and practical method for reusing a holdout
(or testing) set to validate the accuracy of hypotheses produced by a learning
algorithm operating on a training set. Reusing a holdout set adaptively
multiple times can easily lead to overfitting to the holdout set itself. We
give an algorithm that enables the validation of a large number of adaptively
chosen hypotheses, while provably avoiding overfitting. We illustrate the
advantages of our algorithm over the standard use of the holdout set via a
simple synthetic experiment.
  We also formalize and address the general problem of data reuse in adaptive
data analysis. We show how the differential-privacy based approach given in
(Dwork et al., 2014) is applicable much more broadly to adaptive data analysis.
We then show that a simple approach based on description length can also be
used to give guarantees of statistical validity in adaptive settings. Finally,
we demonstrate that these incomparable approaches can be unified via the notion
of approximate max-information that we introduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02632</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02632</id><created>2015-06-08</created><updated>2016-02-26</updated><authors><author><keyname>A.</keyname><forenames>Prashanth L.</forenames></author><author><keyname>Jie</keyname><forenames>Cheng</forenames></author><author><keyname>Fu</keyname><forenames>Michael</forenames></author><author><keyname>Marcus</keyname><forenames>Steve</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author></authors><title>Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and
  Control</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cumulative prospect theory (CPT) is known to model human decisions well, with
substantial empirical evidence supporting this claim. CPT works by distorting
probabilities and is more general than the classic expected utility and
coherent risk measures. We bring this idea to a risk-sensitive reinforcement
learning (RL) setting and design algorithms for both estimation and control.
The RL setting presents two particular challenges when CPT is applied:
estimating the CPT objective requires estimations of the entire distribution of
the value function and finding a randomized optimal policy. The estimation
scheme that we propose uses the empirical distribution to estimate the
CPT-value of a random variable. We then use this scheme in the inner loop of a
CPT-value optimization procedure that is based on the well-known simulation
optimization idea of simultaneous perturbation stochastic approximation (SPSA).
We provide theoretical convergence guarantees for all the proposed algorithms
and also illustrate the usefulness of CPT-based criteria in a traffic signal
control application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02633</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02633</id><created>2015-06-08</created><authors><author><keyname>Rieser</keyname><forenames>Antonio</forenames></author></authors><title>A Topological Approach to Spectral Clustering</title><categories>cs.LG stat.ML</categories><comments>9 Pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a clustering algorithm which, for input, takes data assumed to be
sampled from a uniform distribution supported on a metric space $X$, and
outputs a clustering of the data based on a topological estimate of the
connected components of $X$. The algorithm works by choosing a weighted graph
on the samples from a natural one-parameter family of graphs using an error
based on the heat operator on the graphs. The estimated connected components of
$X$ are identified as the support of the eigenfunctions of the heat operator
with eigenvalue $1$, which allows the algorithm to work without requiring the
number of expected clusters as input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02639</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02639</id><created>2015-06-08</created><updated>2015-08-19</updated><authors><author><keyname>Beame</keyname><forenames>Paul</forenames></author><author><keyname>Liew</keyname><forenames>Vincent</forenames></author></authors><title>New Limits for Knowledge Compilation and Applications to Exact Model
  Counting</title><categories>cs.AI</categories><comments>Full version of paper appearing UAI 2015 updated to include new
  references to related work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show new limits on the efficiency of using current techniques to make
exact probabilistic inference for large classes of natural problems. In
particular we show new lower bounds on knowledge compilation to SDD and DNNF
forms. We give strong lower bounds on the complexity of SDD representations by
relating SDD size to best-partition communication complexity. We use this
relationship to prove exponential lower bounds on the SDD size for representing
a large class of problems that occur naturally as queries over probabilistic
databases. A consequence is that for representing unions of conjunctive
queries, SDDs are not qualitatively more concise than OBDDs. We also derive
simple examples for which SDDs must be exponentially less concise than FBDDs.
Finally, we derive exponential lower bounds on the sizes of DNNF
representations using a new quasipolynomial simulation of DNNFs by
nondeterministic FBDDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02640</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02640</id><created>2015-06-08</created><updated>2015-11-12</updated><authors><author><keyname>Redmon</keyname><forenames>Joseph</forenames></author><author><keyname>Divvala</keyname><forenames>Santosh</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author></authors><title>You Only Look Once: Unified, Real-Time Object Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present YOLO, a new approach to object detection. Prior work on object
detection repurposes classifiers to perform detection. Instead, we frame object
detection as a regression problem to spatially separated bounding boxes and
associated class probabilities. A single neural network predicts bounding boxes
and class probabilities directly from full images in one evaluation. Since the
whole detection pipeline is a single network, it can be optimized end-to-end
directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes
images in real-time at 45 frames per second. A smaller version of the network,
Fast YOLO, processes an astounding 155 frames per second while still achieving
double the mAP of other real-time detectors. Compared to state-of-the-art
detection systems, YOLO makes more localization errors but is far less likely
to predict false detections where nothing exists. Finally, YOLO learns very
general representations of objects. It outperforms all other detection methods,
including DPM and R-CNN, by a wide margin when generalizing from natural images
to artwork on both the Picasso Dataset and the People-Art Dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02649</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02649</id><created>2015-06-08</created><authors><author><keyname>Gonen</keyname><forenames>Alon</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>Faster SGD Using Sketched Conditioning</title><categories>cs.NA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method for speeding up stochastic optimization algorithms
via sketching methods, which recently became a powerful tool for accelerating
algorithms for numerical linear algebra. We revisit the method of conditioning
for accelerating first-order methods and suggest the use of sketching methods
for constructing a cheap conditioner that attains a significant speedup with
respect to the Stochastic Gradient Descent (SGD) algorithm. While our
theoretical guarantees assume convexity, we discuss the applicability of our
method to deep neural networks, and experimentally demonstrate its merits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02677</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02677</id><created>2015-06-08</created><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author></authors><title>Convex Optimization Approach for Stable Decomposition of Stream of
  Pulses</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of estimating the delays and amplitudes of
a weighted superposition of pulses, called stream of pulses. This problem is
motivated by a variety of applications, such as ultrasound and radar. This
paper shows that the recovery error of a tractable convex optimization problem
is proportional to the noise level. Additionally, the estimated delays are
clustered around the true delays. This holds provided that the pulse meets a
few mild localization properties and that a separation condition holds. If the
amplitudes are known to be positive, the separation is unnecessary. In this
case, the recovery error is proportional to the noise level and depends on the
maximal number of delays within a resolution cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02678</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02678</id><created>2015-06-08</created><authors><author><keyname>Evako</keyname><forenames>Alexander V.</forenames></author></authors><title>Topology-preserving digitization of n-dimensional objects by
  constructing cubical models</title><categories>cs.DM</categories><comments>9 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1503.03491</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new cubical space model for the representation of
continuous objects and surfaces in the n-dimensional Euclidean space by
discrete sets of points. The cubical space model concerns the process of
converting a continuous object in its digital counterpart, which is a graph,
enabling us to apply notions and operations used in digital imaging to cubical
spaces. We formulate a definition of a simple n-cube and prove that deleting or
attaching a simple n-cube does not change the homotopy type of a cubical space.
Relying on these results, we design a procedure, which preserves basic
topological properties of an n-dimensional object, for constructing compressed
cubical and digital models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02686</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02686</id><created>2015-06-08</created><authors><author><keyname>Montanez</keyname><forenames>George D.</forenames></author><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames></author></authors><title>The LICORS Cabinet: Nonparametric Algorithms for Spatio-temporal
  Prediction</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the task of unsupervised spatio-temporal forecasting (e.g., learning to
predict video data without labels), we propose two new nonparametric predictive
state algorithms, Moonshine and One Hundred Proof. The algorithms are
conceptually simple and make few assumptions on the underlying spatio-temporal
process yet have strong predictive performance and provide predictive
distributions over spatio-temporal data. The latter property allows for
likelihood estimation under the models, for classification and other
probabilistic inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02690</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02690</id><created>2015-06-08</created><updated>2015-08-07</updated><authors><author><keyname>Wang</keyname><forenames>Zhiguang</forenames></author><author><keyname>Oates</keyname><forenames>Tim</forenames></author><author><keyname>Lo</keyname><forenames>James</forenames></author></authors><title>Adaptive Normalized Risk-Averting Training For Deep Neural Networks</title><categories>cs.LG cs.NE stat.ML</categories><comments>17 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes a set of new error criteria and learning approaches,
Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex
optimization problem in training deep neural networks (DNNs). Theoretically, we
demonstrate its effectiveness on global and local convexity lower-bounded by
the standard $L_p$-norm error. By analyzing the gradient on the convexity index
$\lambda$, we explain the reason why to learn $\lambda$ adaptively using
gradient descent works. In practice, we show how this method improves training
of deep neural networks to solve visual recognition tasks on the MNIST and
CIFAR-10 datasets. Without using pretraining or other tricks, we obtain results
comparable or superior to those reported in recent literature on the same tasks
using standard ConvNets + MSE/cross entropy. Performance on deep/shallow
multilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can
be combined with other quasi-Newton training methods, innovative network
variants, regularization techniques and other specific tricks in DNNs. Other
than unsupervised pretraining, it provides a new perspective to address the
non-convex optimization problem in DNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02693</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02693</id><created>2015-06-08</created><authors><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Zhu</keyname><forenames>Junan</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author></authors><title>Approximate Message Passing with Universal Denoising</title><categories>cs.IT math.IT</categories><comments>11 pages, 7 figures. Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study compressed sensing (CS) signal reconstruction problems where an
input signal is measured via matrix multiplication under additive white
Gaussian noise. Our signals are assumed to be stationary and ergodic, but the
input statistics are unknown; the goal is to provide reconstruction algorithms
that are universal to the input statistics. We present a novel algorithmic
framework that combines: (i) the approximate message passing (AMP) CS
reconstruction framework, which solves the matrix channel recovery problem by
iterative scalar channel denoising; (ii) a universal denoising scheme based on
context quantization, which partitions the stationary ergodic signal denoising
into independent and identically distributed (i.i.d.) subsequence denoising;
and (iii) a density estimation approach that approximates the probability
distribution of an i.i.d. sequence by fitting a Gaussian mixture (GM) model. In
addition to the algorithmic framework, we provide three contributions: (i)
numerical results showing that state evolution holds for non-separable Bayesian
sliding-window denoisers; (ii) an i.i.d. denoiser based on a modified GM
learning algorithm; and (iii) a universal denoiser that does not require the
input signal to be bounded. We provide two implementations of our universal CS
recovery algorithm with one being faster and the other being more accurate. The
two implementations compare favorably with existing reconstruction algorithms
in terms of both reconstruction quality and runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02703</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02703</id><created>2015-06-08</created><authors><author><keyname>Thakur</keyname><forenames>Mohit</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Quasi-Concavity for Gaussian Multicast Relay Channels</title><categories>cs.IT math.IT</categories><comments>Long version of a paper presented at the 2015 IEEE International
  Symposium on Information Theory (ISIT) in Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Upper and lower bounds on the capacity of Gaussian multicast relay channels
are shown to be quasi-concave in the receiver signal-to-noise ratios and the
transmit correlation coefficient. The bounds considered are the cut-set bound,
decode-forward (DF) rates, and quantize-forward rates. The DF rates are shown
to be quasi-concave in the relay position and this property is used to optimize
the relay position for several networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02711</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02711</id><created>2015-06-08</created><authors><author><keyname>Paterson</keyname><forenames>Maura B.</forenames></author><author><keyname>Stinson</keyname><forenames>Douglas R.</forenames></author></authors><title>Combinatorial Characterizations of Algebraic Manipulation Detection
  Codes Involving Generalized Difference Families</title><categories>math.CO cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a mathematical analysis of optimal algebraic manipulation
detection (AMD) codes. We prove several lower bounds on the success probability
of an adversary and we then give some combinatorial characterizations of AMD
codes that meet the bounds with equality. These characterizations involve
various types of generalized difference families. Constructing these difference
families is an interesting problem in its own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02717</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02717</id><created>2015-06-08</created><updated>2015-06-29</updated><authors><author><keyname>Kirchner</keyname><forenames>Paul</forenames></author><author><keyname>Fouque</keyname><forenames>Pierre-Alain</forenames></author></authors><title>An Improved BKW Algorithm for LWE with Applications to Cryptography and
  Lattices</title><categories>cs.CR cs.DS cs.LG</categories><comments>CRYPTO 2015</comments><acm-class>I.1.2; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the Learning With Errors problem and its binary
variant, where secrets and errors are binary or taken in a small interval. We
introduce a new variant of the Blum, Kalai and Wasserman algorithm, relying on
a quantization step that generalizes and fine-tunes modulus switching. In
general this new technique yields a significant gain in the constant in front
of the exponent in the overall complexity. We illustrate this by solving p
within half a day a LWE instance with dimension n = 128, modulus $q = n^2$,
Gaussian noise $\alpha = 1/(\sqrt{n/\pi} \log^2 n)$ and binary secret, using
$2^{28}$ samples, while the previous best result based on BKW claims a time
complexity of $2^{74}$ with $2^{60}$ samples for the same parameters. We then
introduce variants of BDD, GapSVP and UniqueSVP, where the target point is
required to lie in the fundamental parallelepiped, and show how the previous
algorithm is able to solve these variants in subexponential time. Moreover, we
also show how the previous algorithm can be used to solve the BinaryLWE problem
with n samples in subexponential time $2^{(\ln 2/2+o(1))n/\log \log n}$. This
analysis does not require any heuristic assumption, contrary to other algebraic
approaches; instead, it uses a variant of an idea by Lyubashevsky to generate
many samples from a small number of samples. This makes it possible to
asymptotically and heuristically break the NTRU cryptosystem in subexponential
time (without contradicting its security assumption). We are also able to solve
subset sum problems in subexponential time for density $o(1)$, which is of
independent interest: for such density, the previous best algorithm requires
exponential time. As a direct application, we can solve in subexponential time
the parameters of a cryptosystem based on this problem proposed at TCC 2010.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02719</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02719</id><created>2015-06-08</created><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Medina</keyname><forenames>Andres Munoz</forenames></author></authors><title>Non-parametric Revenue Optimization for Generalized Second Price
  Auctions</title><categories>cs.LG cs.GT</categories><comments>To be published in Proceedings of UAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an extensive analysis of the key problem of learning optimal
reserve prices for generalized second price auctions. We describe two
algorithms for this task: one based on density estimation, and a novel
algorithm benefiting from solid theoretical guarantees and with a very
favorable running-time complexity of $O(n S \log (n S))$, where $n$ is the
sample size and $S$ the number of slots. Our theoretical guarantees are more
favorable than those previously presented in the literature. Additionally, we
show that even if bidders do not play at an equilibrium, our second algorithm
is still well defined and minimizes a quantity of interest. To our knowledge,
this is the first attempt to apply learning algorithms to the problem of
reserve price optimization in GSP auctions. Finally, we present the first
convergence analysis of empirical equilibrium bidding functions to the unique
symmetric Bayesian-Nash equilibrium of a GSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02721</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02721</id><created>2015-06-08</created><authors><author><keyname>Ramos</keyname><forenames>Arthur F.</forenames></author><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Anjolina G.</forenames></author></authors><title>On the Groupoid Model of Computational Paths</title><categories>cs.LO</categories><comments>16 pages, LSFA 2015 - Preliminary Acceptance</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this work is to study mathematical properties of
computational paths. Originally proposed by de Queiroz &amp; Gabbay (1994) as
`sequences or rewrites', computational paths are taken to be terms of the
identity type of Martin L\&quot;of's Intensional Type Theory, since these paths can
be seen as the grounds on which the propositional equality between two
computational objects stand. From this perspective, this work aims to show that
one of the properties of the identity type is present on computational paths.
We are refering to the fact that that the identity type induces a groupoid
structure, as proposed by Hofmann &amp; Streicher (1994). Using categorical
semantics, we show that computational paths induce a groupoid structure. We
also show that computational paths are capable of inducing higher categorical
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02732</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02732</id><created>2015-06-08</created><authors><author><keyname>Song</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Zhiguang</forenames></author><author><keyname>Ye</keyname><forenames>Yangdong</forenames></author><author><keyname>Fan</keyname><forenames>Ming</forenames></author></authors><title>Empirical Studies on Symbolic Aggregation Approximation Under
  Statistical Perspectives for Knowledge Discovery in Time Series</title><categories>cs.LG cs.IT math.IT</categories><comments>7 pages, 6 figures. Accepted by FSKD 2015</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Symbolic Aggregation approXimation (SAX) has been the de facto standard
representation methods for knowledge discovery in time series on a number of
tasks and applications. So far, very little work has been done in empirically
investigating the intrinsic properties and statistical mechanics in SAX words.
In this paper, we applied several statistical measurements and proposed a new
statistical measurement, i.e. information embedding cost (IEC) to analyze the
statistical behaviors of the symbolic dynamics. Our experiments on the
benchmark datasets and the clinical signals demonstrate that SAX can always
reduce the complexity while preserving the core information embedded in the
original time series with significant embedding efficiency. Our proposed IEC
score provide a priori to determine if SAX is adequate for specific dataset,
which can be generalized to evaluate other symbolic representations. Our work
provides an analytical framework with several statistical tools to analyze,
evaluate and further improve the symbolic dynamics for knowledge discovery in
time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02739</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02739</id><created>2015-06-08</created><updated>2015-06-16</updated><authors><author><keyname>Rashkin</keyname><forenames>Hannah</forenames></author><author><keyname>Singh</keyname><forenames>Sameer</forenames></author><author><keyname>Choi</keyname><forenames>Yejin</forenames></author></authors><title>Connotation Frames: Typed Relations of Implied Sentiment in
  Predicate-Argument Structure</title><categories>cs.CL</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through a choice of a predicate (e.g., &quot;violate&quot;), a writer can convey subtle
sentiments and value judgements toward the arguments of a verb (e.g.,
projecting the agent as an &quot;antagonist&quot; and the theme as a &quot;victim&quot;). We
introduce connotation frames to encode the rich dimensions of implied
sentiment, value judgements, and effect evaluation as typed relations that
these choices influence, and propose a factor graph formulation that captures
the inter-play among different types of connotative relations at the
lexicon-level. Experimental results confirm that our model is effective in
predicting connotative sentiments compared to strong baselines and existing
sentiment lexicons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02740</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02740</id><created>2015-06-08</created><authors><author><keyname>Zhang</keyname><forenames>Yiwei</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>Snake-in-the-Box Codes for Rank Modulation under Kendall's $\tau$-Metric</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1311.4703 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a Gray code in the scheme of rank modulation for flash memories, the
codewords are permutations and two consecutive codewords are obtained using a
push-to-the-top operation. We consider snake-in-the-box codes under Kendall's
$\tau$-metric, which is a Gray code capable of detecting one Kendall's
$\tau$-error. We answer two open problems posed by Horovitz and Etzion.
Firstly, we prove the validity of a construction given by them, resulting in a
snake of size $M_{2n+1}=\frac{(2n+1)!}{2}-2n+1$. Secondly, we come up with a
different construction aiming at a longer snake of size
$M_{2n+1}=\frac{(2n+1)!}{2}-2n+3$. The construction is applied successfully to
$S_7$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02742</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02742</id><created>2015-06-08</created><updated>2016-02-18</updated><authors><author><keyname>Kaji</keyname><forenames>Shizuo</forenames></author><author><keyname>Maeno</keyname><forenames>Toshiaki</forenames></author><author><keyname>Nuida</keyname><forenames>Koji</forenames></author><author><keyname>Numata</keyname><forenames>Yasuhide</forenames></author></authors><title>Polynomial Expressions of Carries in p-ary Arithmetics</title><categories>math.CO cs.CR cs.IT math.IT math.NT</categories><comments>(v2) Improved results and new observations (v3) The authors are
  notified that our main theorem (Theorem 2) appears (by a different approach)
  in [C. Sturtivant, G. S. Frandsen: Theoretical Computer Science 112 (1993)
  291-309]. The authors would like to keep this preprint online for reference
  purposes</comments><msc-class>11T06 (primary), 05E05, 68R05, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that any $n$-variable function on a finite prime field of
characteristic $p$ can be expressed as a polynomial over the same field with at
most $p^n$ monomials. However, it is not obvious to determine the polynomial
for a given concrete function. In this paper, we study the concrete polynomial
expressions of the carries in addition and multiplication of $p$-ary integers.
For the case of addition, our result gives a new family of symmetric
polynomials, which generalizes the known result for the binary case $p = 2$
where the carries are given by elementary symmetric polynomials. On the other
hand, for the case of multiplication of $n$ single-digit integers, we give a
simple formula of the polynomial expression for the carry to the next digit
using the Bernoulli numbers, and show that it has only $(n+1)(p-1)/2 + 1$
monomials, which is significantly fewer than the worst-case number $p^n$ of
monomials for general functions. We also discuss applications of our results to
cryptographic computation on encrypted data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02750</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02750</id><created>2015-06-08</created><authors><author><keyname>Astudillo</keyname><forenames>C&#xe9;sar A.</forenames></author><author><keyname>Oommen</keyname><forenames>B. John</forenames></author></authors><title>Self Organizing Maps Whose Topologies Can Be Learned With Adaptive
  Binary Search Trees Using Conditional Rotations</title><categories>cs.NE cs.AI</categories><journal-ref>C\'esar A. Astudillo and B. John Oommen. Self Organizing Maps
  Whose Topologies Can Be Learned With Adaptive Binary Search Trees Using
  Conditional Rotations. Pattern Recognition, 47(1), 2014</journal-ref><doi>10.1016/j.patcog.2013.04.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous variants of Self-Organizing Maps (SOMs) have been proposed in the
literature, including those which also possess an underlying structure, and in
some cases, this structure itself can be defined by the user Although the
concepts of growing the SOM and updating it have been studied, the whole issue
of using a self-organizing Adaptive Data Structure (ADS) to further enhance the
properties of the underlying SOM, has been unexplored. In an earlier work, we
impose an arbitrary, user-defined, tree-like topology onto the codebooks, which
consequently enforced a neighborhood phenomenon and the so-called tree-based
Bubble of Activity. In this paper, we consider how the underlying tree itself
can be rendered dynamic and adaptively transformed. To do this, we present
methods by which a SOM with an underlying Binary Search Tree (BST) structure
can be adaptively re-structured using Conditional Rotations (CONROT). These
rotations on the nodes of the tree are local, can be done in constant time, and
performed so as to decrease the Weighted Path Length (WPL) of the entire tree.
In doing this, we introduce the pioneering concept referred to as Neural
Promotion, where neurons gain prominence in the Neural Network (NN) as their
significance increases. We are not aware of any research which deals with the
issue of Neural Promotion. The advantages of such a scheme is that the user
need not be aware of any of the topological peculiarities of the stochastic
data distribution. Rather, the algorithm, referred to as the TTOSOM with
Conditional Rotations (TTOCONROT), converges in such a manner that the neurons
are ultimately placed in the input space so as to represent its stochastic
distribution, and additionally, the neighborhood properties of the neurons suit
the best BST that represents the data. These properties have been confirmed by
our experimental results on a variety of data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02751</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02751</id><created>2015-06-08</created><updated>2015-09-22</updated><authors><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author></authors><title>Guaranteed Blind Sparse Spikes Deconvolution via Lifting and Convex
  Optimization</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural recordings, returns from radars and sonars, images in astronomy and
single-molecule microscopy can be modeled as a linear superposition of a small
number of scaled and delayed copies of a band-limited or diffraction-limited
point spread function, which is either determined by the nature or designed by
the users; in other words, we observe the convolution between a point spread
function and a sparse spike signal with unknown amplitudes and delays. While it
is of great interest to accurately resolve the spike signal from as few samples
as possible, however, when the point spread function is not known a priori,
this problem is terribly ill-posed. This paper proposes a convex optimization
framework to simultaneously estimate the point spread function as well as the
spike signal, by mildly constraining the point spread function to lie in a
known low-dimensional subspace. By applying the lifting trick, we obtain an
underdetermined linear system of an ensemble of signals with joint spectral
sparsity, to which atomic norm minimization is applied. Under mild randomness
assumptions of the low-dimensional subspace as well as a separation condition
of the spike signal, we prove the proposed algorithm, dubbed as AtomicLift, is
guaranteed to recover the spike signal up to a scaling factor as soon as the
number of samples is large enough. The extension of AtomicLift to handle noisy
measurements is also discussed. Numerical examples are provided to validate the
effectiveness of the proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02753</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02753</id><created>2015-06-08</created><updated>2015-12-03</updated><authors><author><keyname>Dosovitskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>Inverting Visual Representations with Convolutional Networks</title><categories>cs.NE cs.CV cs.LG</categories><comments>Version 3 - Added first results with a new GAN-based inversion
  approach (figures 12 and 13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature representations, both hand-designed and learned ones, are often hard
to analyze and interpret, even when they are extracted from visual data. We
propose a new approach to study image representations by inverting them with an
up-convolutional neural network. We apply the method to shallow representations
(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our
approach provides significantly better reconstructions than existing methods,
revealing that there is surprisingly rich information contained in these
features when combined with a strong prior. Inverting a deep network trained on
ImageNet provides several insights into the properties of the feature
representation learned by the network. Most strikingly, the colors and the
rough contours of an image can be reconstructed from activations in higher
network layers and even from the predicted class probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02761</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02761</id><created>2015-06-08</created><updated>2016-02-23</updated><authors><author><keyname>Ji</keyname><forenames>Shihao</forenames></author><author><keyname>Yun</keyname><forenames>Hyokun</forenames></author><author><keyname>Yanardag</keyname><forenames>Pinar</forenames></author><author><keyname>Matsushima</keyname><forenames>Shin</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author></authors><title>WordRank: Learning Word Embeddings via Robust Ranking</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedding words in a vector space has gained a lot of attention in recent
years. While state-of-the-art methods provide efficient computation of word
similarities via a low-dimensional matrix embedding, their motivation is often
left unclear. In this paper, we argue that word embedding can be naturally
viewed as a ranking problem due to the ranking nature of the evaluation
metrics. Then, based on this insight, we propose a novel framework WordRank
that efficiently estimates word representations via robust ranking, in which
the attention mechanism and robustness to noise are readily achieved via the
DCG-like ranking losses. The performance of WordRank is measured in word
similarity and word analogy benchmarks, and the results are compared to the
state-of-the-art word embedding techniques. Our algorithm is very competitive
to the state-of-the-arts on large corpora, while outperforms them by a
significant margin when the training set is limited (i.e., sparse and noisy).
With 17 million tokens, WordRank performs almost as well as existing methods
using 7.2 billion tokens on a popular word similarity benchmark. Our
multi-machine distributed implementation of WordRank is open sourced for
general usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02762</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02762</id><created>2015-06-08</created><authors><author><keyname>Wang</keyname><forenames>Xinhua</forenames></author></authors><title>Design and frequency-domain analysis of linear high-gain
  integral-derivative observer</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1306.4807</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a slimple linear high-gain integral-derivative observer is
presented based on singular perturbation technique. The proposed
integral-derivative observer can estimate synchronously the multiple integrals
and derivatives of a signal. The merits of the presented integral-derivative
observer include its synchronous estimation of integrals and derivatives,
simple implementation, sufficient stochastic noises rejection and almost no
drift phenomenon. The theoretical results are confirmed by the frequency-domain
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02776</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02776</id><created>2015-06-09</created><authors><author><keyname>YD</keyname><forenames>Sumith</forenames></author></authors><title>Fast Geometric Fit Algorithm for Sphere Using Exact Solution</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sphere fitting is a common problem in almost all science and engineering
disciplines. Most of methods available are iterative in behavior. This involves
fitting of the parameters in a least square sense or in a geometric sense. Here
we extend the methods of Thomas Chan and Landau who fitted the 2D data using
circle. This work closely resemble their work in redefining the error estimate
and solving the sphere fitting problem exactly. The solutions for center and
radius of the sphere can be found exactly and the equations can be hard coded
for high performance. We have also shown some comparison with other popular
methods and how this method behaves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02783</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02783</id><created>2015-06-09</created><authors><author><keyname>Alguliyev</keyname><forenames>Rasim</forenames></author><author><keyname>Aliguliyev</keyname><forenames>Ramiz</forenames></author><author><keyname>Ismayilova</keyname><forenames>Nigar</forenames></author></authors><title>Weighted Impact Factor (WIF) for assessing the quality of scientific
  journals</title><categories>cs.DL</categories><comments>14 pages, 28 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays impact factor is the significant indicator for journal evaluation.
In impact factor calculation is used number of all citations to journal,
regardless of the prestige of cited journals, however, scientific units (paper,
researcher, journal or scientific organization) cited by journals with high
impact factor or researchers with high Hirsch index are more important than
objects cited by journals without impact factor or unknown researcher. In this
paper was offered weighted impact factor for getting more accurate rankings for
journals, which consider not only quantity of citations, but also quality of
citing journals. Correlation coefficients among different indicators for
journal evaluation: impact factors by Thomson Scientific, weighted impact
factors offered by different researchers, average and medians of all citing
journals impact factors and 5-year impact factors were analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02784</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02784</id><created>2015-06-09</created><updated>2015-10-19</updated><authors><author><keyname>Liu</keyname><forenames>Song</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author></authors><title>Estimating Posterior Ratio for Classification: Transfer Learning from
  Probabilistic Perspective</title><categories>stat.ML cs.LG</categories><comments>Revision Comments: The proofs were corrected from a few mistakes. The
  title and the introduction was changed. We have also re-run a few experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transfer learning assumes classifiers of similar tasks share certain
parameter structures. Unfortunately, modern classifiers uses sophisticated
feature representations with huge parameter spaces which lead to costly
transfer. Under the impression that changes from one classifier to another
should be ``simple'', an efficient transfer learning criteria that only learns
the ``differences'' is proposed in this paper. We train a \emph{posterior
ratio} which turns out to minimizes the upper-bound of the target learning
risk. The model of posterior ratio does not have to share the same parameter
space with the source classifier at all so it can be easily modelled and
efficiently trained. The resulting classifier therefore is obtained by simply
multiplying the existing probabilistic-classifier with the learned posterior
ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02785</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02785</id><created>2015-06-09</created><authors><author><keyname>Sutherland</keyname><forenames>Dougal J.</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author></authors><title>On the Error of Random Fourier Features</title><categories>cs.LG stat.ML</categories><comments>Published at UAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel methods give powerful, flexible, and theoretically grounded approaches
to solving many problems in machine learning. The standard approach, however,
requires pairwise evaluations of a kernel function, which can lead to
scalability issues for very large datasets. Rahimi and Recht (2007) suggested a
popular approach to handling this problem, known as random Fourier features.
The quality of this approximation, however, is not well understood. We improve
the uniform error bound of that paper, as well as giving novel understandings
of the embedding's variance, approximation error, and use in some machine
learning methods. We also point out that surprisingly, of the two main variants
of those features, the more widely used is strictly higher-variance for the
Gaussian kernel and has worse bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02790</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02790</id><created>2015-06-09</created><updated>2016-01-20</updated><authors><author><keyname>Salehi</keyname><forenames>Saeed</forenames></author><author><keyname>Seraji</keyname><forenames>Payam</forenames></author></authors><title>Godel-Rosser's Incompleteness Theorems for Non-Recursively Enumerable
  Theories</title><categories>math.LO cs.LO</categories><comments>12 pages. Keywords: Godel's Incompleteness, Recursive Enumerability,
  Rosser's Trick, Craig's Trick, Definability</comments><msc-class>03F40, 03F30, 03D35, 03D25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Godel's First Incompleteness Theorem is generalized to definable theories,
which are not necessarily recursively enumerable, by using a couple of
syntactic-semantic notions; one is the consistency of a theory with the set of
all true $\Pi_n$-sentences or equivalently the $\Sigma_n$-soundness of the
theory, and the other is $n$-consistency the restriction of
$\omega$-consistency to the $\Sigma_n$-formulas. It is also shown that Rosser's
Incompleteness Theorem does not generally hold for definable non-recursively
enumerable theories; whence Godel-Rosser's Incompleteness Theorem is optimal in
a sense. Though the proof of the incompleteness theorem using the
$\Sigma_n$-soundness assumption is constructive, it is shown that there is no
constructive proof for the incompleteness theorem using the $n$-consistency
assumption, for $n\!&gt;\!2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02792</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02792</id><created>2015-06-09</created><authors><author><keyname>Shaviv</keyname><forenames>Dor</forenames></author><author><keyname>Ozgur</keyname><forenames>Ayfer</forenames></author></authors><title>Capacity of the AWGN Channel with Random Battery Recharges</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider communication over the AWGN channel with a transmitter whose
battery is recharged with RF energy transfer at random times known to the
receiver. We assume that the recharging process is i.i.d. Bernoulli. We
characterize the capacity of this channel as the limit of an $n$-letter maximum
mutual information rate under both causal and noncausal transmitter knowledge
of the battery recharges. With noncausal knowledge, it is possible to
explicitly identify the maximizing input distribution, which we use to
demonstrate that the capacity with noncausal knowledge of the battery recharges
is strictly larger than that with causal knowledge. We then proceed to derive
explicit upper and lower bounds on the capacity, which are within 1.05
bits/s/Hz of each other for all parameter values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02794</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02794</id><created>2015-06-09</created><authors><author><keyname>Kardan</keyname><forenames>Ahmad A.</forenames></author><author><keyname>Speily</keyname><forenames>Omid R. B.</forenames></author><author><keyname>Bahrani</keyname><forenames>Yosra</forenames></author></authors><title>Modelling the Effectiveness of Curriculum in Educational Systems Using
  Bayesian Networks</title><categories>cs.CY</categories><comments>6 pages, 9 figures, International Journal of advanced studies in
  Computer Science and Engineering IJASCSE Volume 4, Issue 5, 2015</comments><journal-ref>IJASCSE Volume 4, Issue 5, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, online education has been considered as one of the most
widely used IT services. Researchers in this field face many challenges in the
realm of Electronic learning services. Nowadays, many researchers in the field
of learning and eLearning study curriculum planning, considering its complexity
and the various numbers of effective parameters. The success of a curriculum is
a multifaceted issue which needs analytical modelling for precise simulations
of the different learning scenarios. In this paper, parameters involved in the
learning process will be identified and a curriculum will be propounded.
Furthermore, a Curriculum model will be proposed using the behavior of the
user, based on the logs of the server. This model will estimate the success
rate of the users while taking courses. Authentic Bayesian networks have been
used for modelling. In order to evaluate the proposed model, the data of three
consecutive semesters of 117 MS IT Students of E-Learning Center of Amirkabir
University of Technology has been used. The assessment clarifies the effects of
various parameters on the success of curriculum planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02796</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02796</id><created>2015-06-09</created><authors><author><keyname>Beroule</keyname><forenames>Beno&#xee;t</forenames></author><author><keyname>Foug&#xe8;res</keyname><forenames>Alain-J&#xe9;r&#xf4;me</forenames></author><author><keyname>Ostrosi</keyname><forenames>Egon</forenames></author></authors><title>Agent-Based Product Configuration: towards Generalized Consensus Seeking</title><categories>cs.MA cs.SE</categories><comments>8 pages, 8 figures, 5 tables</comments><journal-ref>IJCSI Volume 11, Issue 6, November 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper will present an evolution of a fuzzy agent based platform which
performed products configuration. As a first step, we used the notion of
consensus to establish robust results at the end of the configuration process.
We implemented the concept of generalized consensus which implied the
consideration of consensuses from the beginning, in this way robust data are
treated during the entire process and the final result enables the designer to
distinguish the robust components and flexible ones in a set of configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02797</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02797</id><created>2015-06-09</created><updated>2016-01-12</updated><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Langiu</keyname><forenames>Alessio</forenames></author><author><keyname>Lecroq</keyname><forenames>Thierry</forenames></author><author><keyname>Lefebvre</keyname><forenames>Arnaud</forenames></author><author><keyname>Mignosi</keyname><forenames>Filippo</forenames></author><author><keyname>Peltom&#xe4;ki</keyname><forenames>Jarkko</forenames></author><author><keyname>Prieur-Gaston</keyname><forenames>&#xc9;lise</forenames></author></authors><title>Abelian Powers and Repetitions in Sturmian Words</title><categories>math.CO cs.DM cs.FL math.NT</categories><comments>Extended version, substantial improvements w.r.t. v2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Richomme, Saari and Zamboni (J.~Lond.~Math.~Soc.~83:~79--95,~2011) proved
that at every position of a Sturmian word starts an abelian power of exponent
$k$ for every $k &gt; 0$. We improve on this result by studying the maximum
exponents of abelian powers and abelian repetitions (an abelian repetition is
an analogue of a fractional power) in Sturmian words. We give a formula for
computing the maximum exponent of an abelian power of abelian period $m$
starting at a given position in any Sturmian word of rotation angle $\alpha$.
vAs an analogue of the critical exponent, we introduce the abelian critical
exponent $A(s_\alpha)$ of a Sturmian word $s_\alpha$ of angle $\alpha$ as the
quantity $A(s_\alpha) = limsup\ k_{m}/m=limsup\ k'_{m}/m$, where $k_{m}$ (resp.
$k'_{m}$) denotes the maximum exponent of an abelian power (resp.~of an abelian
repetition) of abelian period $m$ (the superior limits coincide for Sturmian
words). We show that $A(s_\alpha)$ equals the Lagrange constant of the number
$\alpha$. This yields a formula for computing $A(s_\alpha)$ in terms of the
partial quotients of the continued fraction expansion of $\alpha$. Using this
formula, we prove that $A(s_\alpha) \geq \sqrt{5}$ and that the equality holds
for the Fibonacci word. We further prove that $A(s_\alpha)$ is finite if and
only if $\alpha$ has bounded partial quotients, that is, if and only if
$s_{\alpha}$ is $\beta$-power-free for some real number $\beta$. Concerning the
infinite Fibonacci word, we prove that: i) The longest prefix that is an
abelian repetition of period $F_j$, $j&gt;1$, has length $F_j( F_{j+1}+F_{j-1}
+1)-2$ if $j$ is even or $F_j( F_{j+1}+F_{j-1} )-2$ if $j$ is odd, where
$F_{j}$ is the $j$th Fibonacci number; ii) The minimum abelian period of any
factor is a Fibonacci number. Further, we derive a formula for the minimum
abelian periods of the finite Fibonacci words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02799</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02799</id><created>2015-06-09</created><updated>2015-09-21</updated><authors><author><keyname>D'Aronco</keyname><forenames>Stefano</forenames></author><author><keyname>Toni</keyname><forenames>Laura</forenames></author><author><keyname>Mena</keyname><forenames>Sergio</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoqing</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Improved Utility-based Congestion Control for Delay-Constrained
  Communication (Extended Version)</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel congestion control algorithm for
delay-constrained communication over best effort packet switched networks. Due
to the presence of buffers in the internal network nodes, each congestion
episode leads to buffer queueing and thus to an increasing delivery delay. It
is therefore essential to properly control congestions in delay-sensitive
applications. Delay-based congestion algorithms could offer a viable solution
since they tend to minimize the queueing delay. Unfortunately they do not
cohabit well with other types of congestion algorithms, such as loss-based
algorithms, that are not regulated by delay constraints. Our target is to
propose a congestion control algorithm able to both maintain a bounded queueing
delay when the network conditions allows for it and to avoid starvation when
competing against flows controlled by other types of policies. Our
Delay-Constrained Congestion Control algorithm exactly achieves this double
objective by using a non-linear mapping between the experienced delay and the
penalty value used in rate update equation in the controller, and by combining
delay and loss feedback information in a single term based on packet
interarrival measurements. We provide a stability analysis of our new algorithm
and show its performance in simulation results that are carried out in the NS3
framework. They show that our algorithm compares favorably to other congestion
control algorithms that share similar objectives. In particular, the simulation
results show good fairness properties of our controller in different scenarios,
with relatively low self inflicted delay and good ability to work also in lossy
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02804</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02804</id><created>2015-06-09</created><authors><author><keyname>Albaladejo</keyname><forenames>Miguel B&#xe1;guena</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author><author><keyname>Manzoni</keyname><forenames>Pietro</forenames></author></authors><title>Measurement-Based Modelling of LTE Performance in Dublin City</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LTE/4G is the next generation of cellular network which specifically aims to
improve the network performance for data traffic and is currently being rolled
out by many network operators. We present results from an extensive LTE
measurement campaign in Dublin, Ireland using a custom performance measurement
tool. Performance data was measured at a variety of locations within the city
(including cell edge locations, indoors, outdoors etc) as well as for mobile
users on public transport within the city. Using this data we derive a model of
the characteristics of link layer RTT and bandwidth vs link signal strength.
This model is suited to use for performance evaluation of applications and
services, and since it is based on real measurements it allows realistic
evaluation of performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02808</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02808</id><created>2015-06-09</created><authors><author><keyname>P</keyname><forenames>Kirana Kumara</forenames></author></authors><title>Simulations using meshfree methods</title><categories>cs.CE physics.comp-ph</categories><comments>preprint (draft) + 3 figures, 1 table, 2 appendices, 2 images, 1
  MATLAB code</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, attempt is made to solve a few problems using the Polynomial
Point Collocation Method (PPCM), the Radial Point Collocation Method (RPCM),
Smoothed Particle Hydrodynamics (SPH), and the Finite Point Method (FPM). A few
observations on the accuracy of these methods are recorded. All the simulations
in this paper are three dimensional linear elastostatic simulations, without
accounting for body forces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02812</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02812</id><created>2015-06-09</created><authors><author><keyname>Guan</keyname><forenames>Xuechong</forenames></author></authors><title>A study on central soft sets: Definitions and basic operations</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new kind of soft sets related with some common decision
making problems in real life called central soft sets is introduced. Properties
of some basic operations on central soft sets are shown. It is investigated
that some classic operations between soft sets can be obtained by central soft
sets with selecting different central sets. We initiate the concepts of an
evaluation system for a parameters set and its optional solutions. An algorithm
is presented to solve such decision making problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02816</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02816</id><created>2015-06-09</created><updated>2015-06-17</updated><authors><author><keyname>Gkotsis</keyname><forenames>George</forenames></author><author><keyname>Liakata</keyname><forenames>Maria</forenames></author><author><keyname>Pedrinaci</keyname><forenames>Carlos</forenames></author><author><keyname>Domingue</keyname><forenames>John</forenames></author></authors><title>Leveraging Textual Features for Best Answer Prediction in
  Community-based Question Answering</title><categories>cs.CL cs.IR</categories><comments>1 figure, 2 tables</comments><acm-class>H.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of determining the best answer in
Community-based Question Answering (CQA) websites by focussing on the content.
In particular, we present a system, ACQUA [http://acqua.kmi.open.ac.uk], that
can be installed onto the majority of browsers as a plugin. The service offers
a seamless and accurate prediction of the answer to be accepted. Previous
research on this topic relies on the exploitation of community feedback on the
answers, which involves rating of either users (e.g., reputation) or answers
(e.g. scores manually assigned to answers). We propose a new technique that
leverages the content/textual features of answers in a novel way. Our approach
delivers better results than related linguistics-based solutions and manages to
match rating-based approaches. More specifically, the gain in performance is
achieved by rendering the values of these features into a discretised form. We
also show how our technique manages to deliver equally good results in
real-time settings, as opposed to having to rely on information not always
readily available, such as user ratings and answer scores. We ran an evaluation
on 21 StackExchange websites covering around 4 million questions and more than
8 million answers. We obtain 84% average precision and 70% recall, which shows
that our technique is robust, effective, and widely applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02820</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02820</id><created>2015-06-09</created><authors><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author><author><keyname>Ulmschneider</keyname><forenames>Markus</forenames></author></authors><title>Decoding of Repeated-Root Cyclic Codes up to New Bounds on Their Minimum
  Distance</title><categories>cs.IT cs.CR cs.DM math.CO math.IT math.NT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The well-known approach of Bose, Ray-Chaudhuri and Hocquenghem and its
generalization by Hartmann and Tzeng are lower bounds on the minimum distance
of simple-root cyclic codes. We generalize these two bounds to the case of
repeated-root cyclic codes and present a syndrome-based burst error decoding
algorithm with guaranteed decoding radius based on an associated folded cyclic
code. Furthermore, we present a third technique for bounding the minimum
Hamming distance based on the embedding of a given repeated-root cyclic code
into a repeated-root cyclic product code. A second quadratic-time probabilistic
burst error decoding procedure based on the third bound is outlined. Index
Terms Bound on the minimum distance, burst error, efficient decoding, folded
code, repeated-root cyclic code, repeated-root cyclic product code
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02822</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02822</id><created>2015-06-09</created><updated>2015-07-26</updated><authors><author><keyname>Court&#xe8;s</keyname><forenames>Ludovic</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Wurmus</keyname><forenames>Ricardo</forenames></author></authors><title>Reproducible and User-Controlled Software Environments in HPC with Guix</title><categories>cs.DC cs.OS cs.SE</categories><comments>2nd International Workshop on Reproducibility in Parallel Computing
  (RepPar), Aug 2015, Vienne, Austria. http://reppar.org/</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support teams of high-performance computing (HPC) systems often find
themselves between a rock and a hard place: on one hand, they understandably
administrate these large systems in a conservative way, but on the other hand,
they try to satisfy their users by deploying up-to-date tool chains as well as
libraries and scientific software. HPC system users often have no guarantee
that they will be able to reproduce results at a later point in time, even on
the same system-software may have been upgraded, removed, or recompiled under
their feet, and they have little hope of being able to reproduce the same
software environment elsewhere. We present GNU Guix and the functional package
management paradigm and show how it can improve reproducibility and sharing
among researchers with representative use cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02833</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02833</id><created>2015-06-09</created><authors><author><keyname>Sa&#xe0;-Garriga</keyname><forenames>Albert</forenames></author><author><keyname>Castells-Rufas</keyname><forenames>David</forenames></author><author><keyname>Carrabina</keyname><forenames>Jordi</forenames></author></authors><title>OMP2HMPP: Compiler Framework for Energy Performance Trade-off Analysis
  of Automatically Generated Codes</title><categories>cs.DC cs.PF cs.PL</categories><acm-class>D.3.2; D.3.4</acm-class><journal-ref>IJCSI International Journal of Computer Science Issues, Volume 12,
  Issue 2, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present OMP2HMPP, a tool that, in a first step, automatically translates
OpenMP code into various possible transformations of HMPP. In a second step
OMP2HMPP executes all variants to obtain the performance and power consumption
of each transformation. The resulting trade-off can be used to choose the more
convenient version. After running the tool on a set of codes from the Polybench
benchmark we show that the best automatic transformation is equivalent to a
manual one done by an expert. Compared with original OpenMP code running in 2
quad-core processors we obtain an average speed-up of 31x and 5.86x factor in
operations per watt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02850</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02850</id><created>2015-06-09</created><authors><author><keyname>Basilico</keyname><forenames>Nicola</forenames></author><author><keyname>De Nittis</keyname><forenames>Giuseppe</forenames></author><author><keyname>Gatti</keyname><forenames>Nicola</forenames></author></authors><title>Adversarial patrolling with spatially uncertain alarm signals</title><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When securing complex infrastructures or large environments, constant
surveillance of every area is not affordable. To cope with this issue, a common
countermeasure is the usage of cheap but wide-ranged sensors, able to detect
suspicious events that occur in large areas, supporting patrollers to improve
the effectiveness of their strategies. However, such sensors are commonly
affected by uncertainty. In the present paper, we focus on spatially uncertain
alarm signals. That is, the alarm system is able to detect an attack but it is
uncertain on the exact position where the attack is taking place. This is
common when the area to be secured is wide such as in border patrolling and
fair site surveillance. We propose, to the best of our knowledge, the first
Patrolling Security Game model where a Defender is supported by a spatially
uncertain alarm system which non-deterministically generates signals once a
target is under attack. We show that finding the optimal strategy in arbitrary
graphs is APX-hard even in zero-sum games and we provide two (exponential time)
exact algorithms and two (polynomial time) approximation algorithms.
Furthermore, we analyse what happens in environments with special topologies,
showing that in linear and cycle graphs the optimal patrolling strategy can be
found in polynomial time, de facto allowing our algorithms to be used in
real-life scenarios, while in trees the problem is NP-hard. Finally, we show
that without false positives and missed detections, the best patrolling
strategy reduces to stay in a place, wait for a signal, and respond to it at
best. This strategy is optimal even with non-negligible missed detection rates,
which, unfortunately, affect every commercial alarm system. We evaluate our
methods in simulation, assessing both quantitative and qualitative aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02857</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02857</id><created>2015-06-09</created><updated>2016-03-03</updated><authors><author><keyname>Adj&#xe9;</keyname><forenames>Assal&#xe9;</forenames></author></authors><title>Overapproximating the Reachable Values Set of Piecewise Affine Systems
  Coupling Policy Iterations with Piecewise Quadratic Lyapunov Functions</title><categories>math.OC cs.NA</categories><comments>21 pages, 2 figures</comments><msc-class>93B40, 93B50, 93D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have recently constructed a piecewise quadratic Lyapunov function to prove
the boundedness of the reachable values set of piecewise affine discrete-time
systems. The method developed also provided an overapproximation of the
reachable values set. In this paper, we refine the latter overapproximation
extending previous works combining policy iterations with quadratic Lyapunov
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02863</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02863</id><created>2015-06-09</created><authors><author><keyname>Champarnaud</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author><author><keyname>Ouali-Sebti</keyname><forenames>Nadia</forenames></author><author><keyname>Ziadi</keyname><forenames>Djelloul</forenames></author></authors><title>Bottom Up Quotients and Residuals for Tree Languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the notion of tree language quotients to bottom-up
quotients. Instead of computing the residual of a tree language from top to
bottom and producing a list of tree languages, we show how to compute a set of
k-ary trees, where k is an arbitrary integer. We define the quotient formula
for different combinations of tree languages: union, symbol products,
compositions, iterated symbol products and iterated composition. These
computations lead to the definition of the bottom-up quotient tree automaton,
that turns out to be the minimal deterministic tree automaton associated with a
regular tree language in the case of the 0-ary trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02865</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02865</id><created>2015-06-09</created><authors><author><keyname>Jurrius</keyname><forenames>Relinde</forenames></author><author><keyname>Pellikaan</keyname><forenames>Ruud</forenames></author></authors><title>On defining generalized rank weights</title><categories>cs.IT math.CO math.IT</categories><comments>15 pages; extended abstract accepted for presentation at ACA2015
  (http://www.usthb.dz/spip.php?article1039)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the generalized rank weights, with a definition
implied by the study of the generalized rank weight enumerator. We study rank
metric codes over $L$, where $L$ is a finite Galois extension of a field $K$.
This is a generalization of the case where $K = \mathbb{F}_q$ and $L =
\mathbb{F}_{q^m}$ of Gabidulin codes to arbitrary characteristic. We show
equivalence to previous definitions, in particular the ones by
Kurihara-Matsumoto-Uyematsu, Oggier-Sboui and Ducoat. As an application of the
notion of generalized rank weights, we discuss codes that are degenerate with
respect to the rank metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02876</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02876</id><created>2015-06-09</created><authors><author><keyname>Hakiri</keyname><forenames>Akram</forenames><affiliation>LAAS</affiliation></author><author><keyname>Berthou</keyname><forenames>Pascal</forenames><affiliation>UPS, LAAS</affiliation></author></authors><title>Leveraging SDN for The 5G Networks: Trends, Prospects and Challenges</title><categories>cs.NI</categories><comments>appears in Software Defined Mobile Networks : Beyond LTE Network
  Architecture, Wiley Series in Communications Networking \&amp; Distributed
  Systems 2015, Mobile \&amp; Wireless Communications, 978-1-118-90028-4</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today 4G mobile systems are evolving to provide IP connectivity for diverse
applications and services up to 1Gbps. They are designed to optimize the
network performance, improve cost efficiency and facilitate the uptake of mass
market IP-based services. Nevertheless, the growing demand and the diverse
patterns of mobile traffic place an increasing strain on cellular networks. To
cater to the large volumes of traffic delivered by the new services and
applications, the future 5G network will provide the fundamental infrastructure
for billions of new devices with less predictable traffic patterns will join
the network. The 5G technology is presently in its early research stages, so
researches are currently underway exploring different architectural paths to
address their key drivers. SDN techniques have been seen as promising enablers
for this vision of carrier networks, which will likely play a crucial role in
the design of 5G wireless networks. A critical understanding of this emerging
paradigm is necessary to address the multiple challenges of the future
SDN-enabled 5G technology. To address this requirement, a survey the emerging
trends and prospects, followed by in-depth discussion of major challenges in
this area are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02890</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02890</id><created>2015-06-09</created><authors><author><keyname>Firouzbakht</keyname><forenames>Koorosh</forenames></author><author><keyname>Noubir</keyname><forenames>Guevara</forenames></author><author><keyname>Salehi</keyname><forenames>Masoud</forenames></author></authors><title>Constrained Bimatrix Games in Wireless Communications</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><journal-ref>IEEE Transactions on Communications, Volume 64 , Issue 1 (2015)</journal-ref><doi>10.1109/TCOMM.2015.2504085</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a constrained bimatrix game framework that can be used to model
many practical problems in many disciplines, including jamming in packetized
wireless networks. In contrast to the widely used zero-sum framework, in
bimatrix games it is no longer required that the sum of the players' utilities
to be zero or constant, thus, can be used to model a much larger class of
jamming problems. Additionally, in contrast to the standard bimatrix games, in
constrained bimatrix games the players' strategies must satisfy some linear
constraint/inequality, consequently, not all strategies are feasible and the
existence of the Nash equilibrium (NE) is not guaranteed anymore. We provide
the necessary and sufficient conditions under which the existence of the Nash
equilibrium is guaranteed, and show that the equilibrium pairs and the Nash
equilibrium solution of the constrained game corresponds to the global maximum
of a quadratic program. Finally, we use our game theoretic framework to find
the optimal transmission and jamming strategies for a typical wireless link
under power limited jamming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02891</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02891</id><created>2015-06-09</created><authors><author><keyname>Elkind</keyname><forenames>Edith</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>Properties of Multiwinner Voting Rules</title><categories>cs.GT</categories><comments>31 pages</comments><msc-class>91B14</msc-class><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to propose and study properties of multiwinner
voting rules which can be consider as generalisations of single-winner scoring
voting rules. We consider SNTV, Bloc, k-Borda, STV, and several variants of
Chamberlin--Courant's and Monroe's rules and their approximations. We identify
two broad natural classes of multiwinner score-based rules, and show that many
of the existing rules can be captured by one or both of these approaches. We
then formulate a number of desirable properties of multiwinner rules, and
evaluate the rules we consider with respect to these properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02897</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02897</id><created>2015-06-09</created><updated>2015-11-08</updated><authors><author><keyname>Pfister</keyname><forenames>Tomas</forenames></author><author><keyname>Charles</keyname><forenames>James</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Flowing ConvNets for Human Pose Estimation in Videos</title><categories>cs.CV</categories><comments>ICCV'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this work is human pose estimation in videos, where multiple
frames are available. We investigate a ConvNet architecture that is able to
benefit from temporal context by combining information across the multiple
frames using optical flow.
  To this end we propose a network architecture with the following novelties:
(i) a deeper network than previously investigated for regressing heatmaps; (ii)
spatial fusion layers that learn an implicit spatial model; (iii) optical flow
is used to align heatmap predictions from neighbouring frames; and (iv) a final
parametric pooling layer which learns to combine the aligned heatmaps into a
pooled confidence map.
  We show that this architecture outperforms a number of others, including one
that uses optical flow solely at the input layers, one that regresses joint
coordinates directly, and one that predicts heatmaps without spatial fusion.
  The new architecture outperforms the state of the art by a large margin on
three video pose estimation datasets, including the very challenging Poses in
the Wild dataset, and outperforms other deep methods that don't use a graphical
model on the single-image FLIC benchmark (and also Chen &amp; Yuille and Tompson et
al. in the high precision region).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02903</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02903</id><created>2015-06-09</created><updated>2015-11-02</updated><authors><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author></authors><title>Mixing Time Estimation in Reversible Markov Chains from a Single Sample
  Path</title><categories>cs.LG stat.ML</categories><comments>28 pages; minor clarification in Appendix A concerning lower bounds</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides the first procedure for computing a fully
data-dependent interval that traps the mixing time $t_{\text{mix}}$ of a finite
reversible ergodic Markov chain at a prescribed confidence level. The interval
is computed from a single finite-length sample path from the Markov chain, and
does not require the knowledge of any parameters of the chain. This stands in
contrast to previous approaches, which either only provide point estimates, or
require a reset mechanism, or additional prior knowledge. The interval is
constructed around the relaxation time $t_{\text{relax}}$, which is strongly
related to the mixing time, and the width of the interval converges to zero
roughly at a $\sqrt{n}$ rate, where $n$ is the length of the sample path. Upper
and lower bounds are given on the number of samples required to achieve
constant-factor multiplicative accuracy. The lower bounds indicate that, unless
further restrictions are placed on the chain, no procedure can achieve this
accuracy level before seeing each state at least $\Omega(t_{\text{relax}})$
times on the average. Finally, future directions of research are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02914</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02914</id><created>2015-06-09</created><updated>2015-06-15</updated><authors><author><keyname>Gabri&#xe9;</keyname><forenames>Marylou</forenames></author><author><keyname>Tramel</keyname><forenames>Eric W.</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author></authors><title>Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer
  Free Energy</title><categories>cond-mat.dis-nn cs.LG cs.NE stat.ML</categories><comments>8 pages, 7 figures, demo online at
  http://www.lps.ens.fr/~krzakala/WASP.html</comments><journal-ref>Advances in Neural Information Processing Systems (NIPS 2015) 28,
  pages 640--648</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann machines are undirected neural networks which have been
shown to be effective in many applications, including serving as
initializations for training deep multi-layer neural networks. One of the main
reasons for their success is the existence of efficient and practical
stochastic algorithms, such as contrastive divergence, for unsupervised
training. We propose an alternative deterministic iterative procedure based on
an improved mean field method from statistical physics known as the
Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides
performance equal to, and sometimes superior to, persistent contrastive
divergence, while also providing a clear and easy to evaluate objective
function. We believe that this strategy can be easily generalized to other
models as well as to more accurate higher-order approximations, paving the way
for systematic improvements in training Boltzmann machines with hidden units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02922</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02922</id><created>2015-06-09</created><authors><author><keyname>Gkatzia</keyname><forenames>Dimitra</forenames></author><author><keyname>Hastie</keyname><forenames>Helen</forenames></author></authors><title>An Ensemble method for Content Selection for Data-to-text Systems</title><categories>cs.CL cs.AI</categories><comments>3 pages, 2 figures, 1st International Workshop on Data-to-text
  Generation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach for automatic report generation from time-series
data, in the context of student feedback generation. Our proposed methodology
treats content selection as a multi-label classification (MLC) problem, which
takes as input time-series data (students' learning data) and outputs a summary
of these data (feedback). Unlike previous work, this method considers all data
simultaneously using ensembles of classifiers, and therefore, it achieves
higher accuracy and F- score compared to meaningful baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02923</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02923</id><created>2015-06-09</created><authors><author><keyname>Ibraheem</keyname><forenames>Abdulrahman Oladipupo</forenames></author></authors><title>Compact Shape Trees: A Contribution to the Forest of Shape
  Correspondences and Matching Methods</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel technique, termed compact shape trees, for computing
correspondences of single-boundary 2-D shapes in O(n2) time. Together with zero
or more features defined at each of n sample points on the shape's boundary,
the compact shape tree of a shape comprises the O(n) collection of vectors
emanating from any of the sample points on the shape's boundary to the rest of
the sample points on the boundary. As it turns out, compact shape trees have a
number of elegant properties both in the spatial and frequency domains. In
particular, via a simple vector-algebraic argument, we show that the O(n)
collection of vectors in a compact shape tree possesses at least the same
discriminatory power as the O(n2) collection of lines emanating from each
sample point to every other sample point on a shape's boundary. In addition, we
describe neat approaches for achieving scale and rotation invariance with
compact shape trees in the spatial domain; by viewing compact shape trees as
aperiodic discrete signals, we also prove scale and rotation invariance
properties for them in the Fourier domain. Towards these, along the way, using
concepts from differential geometry and the Calculus, we propose a novel theory
for sampling 2-D shape boundaries in a scale and rotation invariant manner.
Finally, we propose a number of shape recognition experiments to test the
efficacy of our concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02930</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02930</id><created>2015-06-09</created><authors><author><keyname>Duris</keyname><forenames>Frantisek</forenames></author></authors><title>Theory of the effectivity of human problem solving</title><categories>cs.AI</categories><msc-class>68T20</msc-class><acm-class>I.2.0; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to solve problems effectively is one of the hallmarks of human
cognition. Yet, in our opinion it gets far less research focus than it rightly
deserves. In this paper we outline a framework in which this effectivity can be
studied; we identify the possible roots and scope of this effectivity and the
cognitive processes directly involved. More particularly, we have observed that
people can use cognitive mechanisms to drive problem solving by the same manner
on which an optimal problem solving strategy suggested by Solomonoff (1986) is
based. Furthermore, we provide evidence for cognitive substrate hypothesis
(Cassimatis, 2006) which states that human level AI in all domains can be
achieved by a relatively small set of cognitive mechanisms. The results
presented in this paper can serve both cognitive psychology in better
understanding of human problem solving processes, and artificial intelligence
in designing more human like intelligent agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02931</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02931</id><created>2015-06-09</created><updated>2015-11-04</updated><authors><author><keyname>Cunningham</keyname><forenames>Oscar</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Heunen</keyname><forenames>Chris</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Axiomatizing complete positivity</title><categories>math.CT cs.LO</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 148-157</journal-ref><doi>10.4204/EPTCS.195.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two ways to turn a categorical model for pure quantum theory into
one for mixed quantum theory, both resulting in a category of completely
positive maps. One has quantum systems as objects, whereas the other also
allows classical systems on an equal footing. The former has been axiomatized
using environment structures. We extend this axiomatization to the latter by
introducing decoherence structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02936</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02936</id><created>2015-06-09</created><updated>2015-06-28</updated><authors><author><keyname>Yao</keyname><forenames>Penghui</forenames></author></authors><title>Parity Decision Tree Complexity and 4-Party Communication Complexity of
  XOR-functions Are Polynomially Equivalent</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we study the relation between the parity decision tree
complexity of a boolean function $f$, denoted by $\mathrm{D}_{\oplus}(f)$, and
the $k$-party number-in-hand multiparty communication complexity of the XOR
functions $F(x_1,\ldots, x_k)= f(x_1\oplus\cdots\oplus x_k)$, denoted by
$\mathrm{CC}^{(k)}(F)$. It is known that $\mathrm{CC}^{(k)}(F)\leq
k\cdot\mathrm{D}_{\oplus}(f)$ because the players can simulate the parity
decision tree that computes $f$. In this note, we show that
\[\mathrm{D}_{\oplus}(f)\leq O\big(\mathrm{CC}^{(4)}(F)^5\big).\] Our main tool
is a recent result from additive combinatorics due to Sanders. As
$\mathrm{CC}^{(k)}(F)$ is non-decreasing as $k$ grows, the parity decision tree
complexity of $f$ and the communication complexity of the corresponding
$k$-argument XOR functions are polynomially equivalent whenever $k\geq 4$.
  Remark: After the first version of this paper was finished, we discovered
that Hatami and Lovett had already discovered the same result a few years ago,
without writing it up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02937</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02937</id><created>2015-06-09</created><updated>2015-09-01</updated><authors><author><keyname>Irukulapati</keyname><forenames>Naga V.</forenames></author><author><keyname>Marsella</keyname><forenames>Domenico</forenames></author><author><keyname>Johannisson</keyname><forenames>Pontus</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Secondini</keyname><forenames>Marco</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author></authors><title>Stochastic Digital Backpropagation with Residual Memory Compensation</title><categories>cs.IT math.IT physics.optics</categories><comments>7 pages, accepted to publication in 'Journal of Lightwave Technology
  (JLT)'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic digital backpropagation (SDBP) is an extension of digital
backpropagation (DBP) and is based on the maximum a posteriori principle. SDBP
takes into account noise from the optical amplifiers in addition to handling
deterministic linear and nonlinear impairments. The decisions in SDBP are taken
on a symbol-by-symbol (SBS) basis, ignoring any residual memory, which may be
present due to non-optimal processing in SDBP. In this paper, we extend SDBP to
account for memory between symbols. In particular, two different methods are
proposed: a Viterbi algorithm (VA) and a decision directed approach. Symbol
error rate (SER) for memory-based SDBP is significantly lower than the
previously proposed SBS-SDBP. For inline dispersion-managed links, the VA-SDBP
has up to 10 and 14 times lower SER than DBP for QPSK and 16-QAM, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02954</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02954</id><created>2015-06-09</created><authors><author><keyname>Mood</keyname><forenames>Benjamin</forenames></author><author><keyname>Gupta</keyname><forenames>Debayan</forenames></author><author><keyname>Butler</keyname><forenames>Kevin</forenames></author><author><keyname>Feigenbaum</keyname><forenames>Joan</forenames></author></authors><title>Reuse It Or Lose It: More Efficient Secure Computation Through Reuse of
  Encrypted Values</title><categories>cs.CR</categories><comments>20 pages, shorter conference version published in Proceedings of the
  2014 ACM SIGSAC Conference on Computer and Communications Security, Pages
  582-596, ACM New York, NY, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-party secure function evaluation (SFE) has become significantly more
feasible, even on resource-constrained devices, because of advances in
server-aided computation systems. However, there are still bottlenecks,
particularly in the input validation stage of a computation. Moreover, SFE
research has not yet devoted sufficient attention to the important problem of
retaining state after a computation has been performed so that expensive
processing does not have to be repeated if a similar computation is done again.
This paper presents PartialGC, an SFE system that allows the reuse of encrypted
values generated during a garbled-circuit computation. We show that using
PartialGC can reduce computation time by as much as 96% and bandwidth by as
much as 98% in comparison with previous outsourcing schemes for secure
computation. We demonstrate the feasibility of our approach with two sets of
experiments, one in which the garbled circuit is evaluated on a mobile device
and one in which it is evaluated on a server. We also use PartialGC to build a
privacy-preserving &quot;friend finder&quot; application for Android. The reuse of
previous inputs to allow stateful evaluation represents a new way of looking at
SFE and further reduces computational barriers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02955</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02955</id><created>2015-06-09</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Shen</keyname><forenames>Hui</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author></authors><title>A Decision-Aided Parallel SC-List Decoder for Polar Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a decision-aided scheme for parallel SC-List
decoding of polar codes. At the parallel SC-List decoder, each survival path is
extended based on multiple information bits, therefore the number of split
paths becomes very large and the sorting to find the top L paths becomes very
complex. We propose a decision-aided scheme to reduce the number of split paths
and thus reduce the sorting complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02963</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02963</id><created>2015-06-09</created><updated>2016-01-29</updated><authors><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Garas</keyname><forenames>Antonios</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Value of peripheral nodes in controlling multilayer networks</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 7 figures</comments><journal-ref>Phys. Rev. E 93, 012309 (2016)</journal-ref><doi>10.1103/PhysRevE.93.012309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the controllability of a two-layer network, where driver nodes can
be chosen randomly only from one layer. Each layer contains a scale-free
network with directed links and the node dynamics depends on the incoming links
from other nodes. We combine the in-degree and out-degree values to assign an
importance value $w$ to each node, and distinguish between peripheral nodes
with low $w$ and central nodes with high $w$. Based on numerical simulations,
we find that the controllable part of the network is larger when choosing low
$w$ nodes to connect the two layers. The control is as efficient when
peripheral nodes are driver nodes as it is for the case of more central nodes.
However, if we assume a cost to utilize nodes that is proportional to their
overall degree, utilizing peripheral nodes to connect the two layers or to act
as driver nodes is not only the most cost-efficient solution, it is also the
one that performs best in controlling the two-layer network among the different
interconnecting strategies we have tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02972</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02972</id><created>2015-06-09</created><authors><author><keyname>Kumar</keyname><forenames>Jitender</forenames></author><author><keyname>Krishna</keyname><forenames>K. V.</forenames></author></authors><title>Syntactic semigroup problem for the semigroup reducts of Affine
  Near-semirings over Brandt Semigroups</title><categories>cs.FL</categories><msc-class>20M35, 68Q70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The syntactic semigroup problem is to decide whether a given finite semigroup
is syntactic or not. This work investigates the syntactic semigroup problem for
both the semigroup reducts of $A^+(B_n)$, the affine near-semiring over a
Brandt semigroup $B_n$. It is ascertained that both the semigroup reducts of
$A^+(B_n)$ are syntactic semigroups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02973</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02973</id><created>2015-06-09</created><authors><author><keyname>Karczmarz</keyname><forenames>Adam</forenames></author><author><keyname>&#x141;&#x105;cki</keyname><forenames>Jakub</forenames></author></authors><title>Fast and simple connectivity in graph timelines</title><categories>cs.DS</categories><comments>21 pages, extended abstract to appear in WADS'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of answering connectivity queries about a
\emph{graph timeline}. A graph timeline is a sequence of undirected graphs
$G_1,\ldots,G_t$ on a common set of vertices of size $n$ such that each graph
is obtained from the previous one by an addition or a deletion of a single
edge. We present data structures, which preprocess the timeline and can answer
the following queries:
  - forall$(u,v,a,b)$ -- does the path $u\to v$ exist in each of
$G_a,\ldots,G_b$?
  - exists$(u,v,a,b)$ -- does the path $u\to v$ exist in any of
$G_a,\ldots,G_b$?
  - forall2$(u,v,a,b)$ -- do there exist two edge-disjoint paths connecting $u$
and $v$ in each of $G_a,\ldots,G_b$
  We show data structures that can answer forall and forall2 queries in $O(\log
n)$ time after preprocessing in $O(m+t\log n)$ time. Here by $m$ we denote the
number of edges that remain unchanged in each graph of the timeline. For the
case of exists queries, we show how to extend an existing data structure to
obtain a preprocessing/query trade-off of $\langle O(m+\min(nt, t^{2-\alpha})),
O(t^\alpha)\rangle$ and show a matching conditional lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02975</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02975</id><created>2015-06-09</created><authors><author><keyname>Zhao</keyname><forenames>Vincent</forenames></author><author><keyname>Zucker</keyname><forenames>Steven</forenames></author></authors><title>Active Sets Improves Learning for Mixture Models</title><categories>stat.ML cs.LG q-bio.QM</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an algorithm to learn Bernoulli Mixture Models based on the
principle that some variables are more informative than others. Working from an
information-theoretic perspective, we propose both backward and forward schemes
for selecting the informative 'active' variables and using them to guide EM.
The result is a stagewise EM algorithm, analogous to stagewise approaches to
linear regression, that should be applicable to neuroscience (and other)
datasets with confounding (or irrelevant) variables. Results on synthetic and
MNIST datasets illustrate the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02976</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02976</id><created>2015-06-09</created><authors><author><keyname>Rodrigues</keyname><forenames>Jose F.</forenames><suffix>Jr.</suffix></author><author><keyname>Traina</keyname><forenames>Agma J. M.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Maria Cristina F.</forenames></author><author><keyname>Traina</keyname><forenames>Caetano</forenames><suffix>Jr</suffix></author></authors><title>Reviewing Data Visualization: an Analytical Taxonomical Study</title><categories>cs.GR</categories><comments>Published in the Proceedings of the Information Visualization
  Conference as Jose Rodrigues, Agma Traina, Maria Oliveira, Caetano Traina,
  Reviewing Data Visualization: an Analytical Taxonomical Study In: 10th
  International Conference on Information Visualization, 2006, 713-720 IEEE
  Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an analytical taxonomy that can suitably describe, rather
than simply classify, techniques for data presentation. Unlike previous works,
we do not consider particular aspects of visualization techniques, but their
mechanisms and foundational vision perception. Instead of just adjusting
visualization research to a classification system, our aim is to better
understand its process. For doing so, we depart from elementary concepts to
reach a model that can describe how visualization techniques work and how they
convey meaning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.02990</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.02990</id><created>2015-06-09</created><authors><author><keyname>Lou</keyname><forenames>Chung-Yu</forenames></author><author><keyname>Daneshrad</keyname><forenames>Babak</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Convolutional-Code-Specific CRC Code Design</title><categories>cs.IT math.IT</categories><comments>12 pages, 8 figures, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic redundancy check (CRC) codes check if a codeword is correctly
received. This paper presents an algorithm to design CRC codes that are
optimized for the code-specific error behavior of a specified feedforward
convolutional code. The algorithm utilizes two distinct approaches to computing
undetected error probability of a CRC code used with a specific convolutional
code. The first approach enumerates the error patterns of the convolutional
code and tests if each of them is detectable. The second approach reduces
complexity significantly by exploiting the equivalence of the undetected error
probability to the frame error rate of an equivalent catastrophic convolutional
code. The error events of the equivalent convolutional code are exactly the
undetectable errors for the original concatenation of CRC and convolutional
codes. This simplifies the computation because error patterns do not need to be
individually checked for detectability. As an example, we optimize CRC codes
for a commonly used 64-state convolutional code for information length k=1024
demonstrating significant reduction in undetected error probability compared to
the existing CRC codes with the same degrees. For a fixed target undetected
error probability, the optimized CRC codes typically require 2 fewer bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03004</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03004</id><created>2015-06-09</created><authors><author><keyname>Guo</keyname><forenames>Yingjie</forenames></author><author><keyname>Wu</keyname><forenames>Linzhi</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author><author><keyname>Wu</keyname><forenames>Bin</forenames></author><author><keyname>Wang</keyname><forenames>Xiaotian</forenames></author></authors><title>The Improved Job Scheduling Algorithm of Hadoop Platform</title><categories>cs.DC cs.DS</categories><comments>12 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper discussed some job scheduling algorithms for Hadoop platform, and
proposed a jobs scheduling optimization algorithm based on Bayes Classification
viewing the shortcoming of those algorithms which are used. The proposed
algorithm can be summarized as follows. In the scheduling algorithm based on
Bayes Classification, the jobs in job queue will be classified into bad job and
good job by Bayes Classification, when JobTracker gets task request, it will
select a good job from job queue, and select tasks from good job to allocate
JobTracker, then the execution result will feedback to the JobTracker.
Therefore the scheduling algorithm based on Bayes Classification influence the
job classification via learning the result of feedback with the JobTracker will
select the most appropriate job to execute on TaskTracker every time. We need
to consider the feature usage of job resource and the influence of TaskTracker
resource on task execution, the former of which we call it job feature, for
instance, the average usage rate of CPU and average usage rate of memory, the
latter node feature, such as the usage rate of CPU and the size of idle
physical memory, the two are called feature variables. Results show that it has
a significant improvement in execution efficiency and stability of job
scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03009</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03009</id><created>2015-06-09</created><authors><author><keyname>Orduna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ayllon</keyname><forenames>Juan M.</forenames></author><author><keyname>Martin-Martin</keyname><forenames>Alberto</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Methods for estimating the size of Google Scholar</title><categories>cs.DL</categories><comments>22 pages, 4 figures and 6 tables. arXiv admin note: text overlap with
  arXiv:1407.6239</comments><doi>10.1007/s11192-015-1614-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of academic search engines (mainly Google Scholar and Microsoft
Academic Search) that aspire to index the entirety of current academic
knowledge has revived and increased interest in the size of the academic web.
The main objective of this paper is to propose various methods to estimate the
current size (number of indexed documents) of Google Scholar (May 2014) and to
determine its validity, precision and reliability. To do this, we present,
apply and discuss three empirical methods: an external estimate based on
empirical studies of Google Scholar coverage, and two internal estimate methods
based on direct, empty and absurd queries, respectively. The results, despite
providing disparate values, place the estimated size of Google Scholar at
around 160 to 165 million documents. However, all the methods show considerable
limitations and uncertainties due to inconsistencies in the Google Scholar
search functionalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03011</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03011</id><created>2015-06-09</created><updated>2015-09-10</updated><authors><author><keyname>Goroshin</keyname><forenames>Ross</forenames></author><author><keyname>Mathieu</keyname><forenames>Michael</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Learning to Linearize Under Uncertainty</title><categories>cs.CV</categories><comments>To appear at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training deep feature hierarchies to solve supervised learning tasks has
achieved state of the art performance on many problems in computer vision.
However, a principled way in which to train such hierarchies in the
unsupervised setting has remained elusive. In this work we suggest a new
architecture and loss for training deep feature hierarchies that linearize the
transformations observed in unlabeled natural video sequences. This is done by
training a generative model to predict video frames. We also address the
problem of inherent uncertainty in prediction by introducing latent variables
that are non-deterministic functions of the input into the network
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03012</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03012</id><created>2015-06-09</created><authors><author><keyname>Orduna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Aytac</keyname><forenames>Selenay</forenames></author></authors><title>Revealing the online network between University and Industry: The case
  of Turkey</title><categories>cs.DL</categories><comments>20 pages, 2 figures and 7 tables. Tables 4 and 5 include additional
  figures</comments><doi>10.1007/s11192-015-1596-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper attempts to explore the relationship between the Turkish
academic and industry systems by mapping the relationships under web
indicators. We used the top 100 Turkish universities and the top 10 Turkish
companies in 10 industrial sectors in order to observe the performance of web
impact indicators. Total page count metric is obtained through Google Turkey
and the pure link metrics have been gathered from Open Site Explorer. The
indicators obtained both for web presence and web visibility indicated that
there are significant differences between the group of academic institutions
and those related to companies within the web space of Turkey. However, this
current study is exploratory and should be replicated with a larger sample of
both Turkish universities and companies in each sector. Likewise, a
longitudinal study rather than sectional would eliminate or smooth fluctuations
of web data (especially URL mentions) as a more adequate understanding of the
relations between Turkish institutions, and their web impact, is reached.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03016</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03016</id><created>2015-06-09</created><updated>2015-06-10</updated><authors><author><keyname>Nitanda</keyname><forenames>Atsushi</forenames></author></authors><title>Accelerated Stochastic Gradient Descent for Minimizing Finite Sums</title><categories>stat.ML cs.LG</categories><comments>[v2] corrected citation to proxSVRG, corrected typos in Figure
  1(option2) and 3(R4 -&gt; R3)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an optimization method for minimizing the finite sums of smooth
convex functions. Our method incorporates an accelerated gradient descent (AGD)
and a stochastic variance reduction gradient (SVRG) in a mini-batch setting.
Unlike SVRG, our method can be directly applied to non-strongly and strongly
convex problems. We show that our method achieves a lower overall complexity
than the recently proposed methods that supports non-strongly convex problems.
Moreover, this method has a fast rate of convergence for strongly convex
problems. Our experiments show the effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03018</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03018</id><created>2015-06-09</created><authors><author><keyname>Gao</keyname><forenames>Yihan</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author></authors><title>On the Uniform Convergence of Consistent Confidence Measures</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many classification algorithms produce confidence measures in the form of
conditional probability of labels given the features of the target instance. It
is desirable to be make these confidence measures calibrated or consistent, in
the sense that they correctly capture the belief of the algorithm in the label
output. For instance, if the algorithm outputs a label with confidence measure
$p$ for $n$ times, then the output label should be correct approximately $np$
times overall. Calibrated confidence measures lead to higher interpretability
by humans and computers and enable downstream analysis or processing. In this
paper, we formally characterize the consistency of confidence measures and
prove a PAC-style uniform convergence result for the consistency of confidence
measures. We show that finite VC-dimension is sufficient for guaranteeing the
consistency of confidence measures produced by empirically consistent
classifiers. Our result also implies that we can calibrate confidence measures
produced by any existing algorithms with monotonic functions, and still get the
same generalization guarantee on consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03022</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03022</id><created>2015-06-09</created><authors><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Yan</keyname><forenames>Xiaoran</forenames></author><author><keyname>Wu</keyname><forenames>Xin-Zeng</forenames></author></authors><title>The Majority Illusion in Social Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social behaviors are often contagious, spreading through a population as
individuals imitate the decisions and choices of others. A variety of global
phenomena, from innovation adoption to the emergence of social norms and
political movements, arise as a result of people following a simple local rule,
such as copy what others are doing. However, individuals often lack global
knowledge of the behaviors of others and must estimate them from the
observations of their friends' behaviors. In some cases, the structure of the
underlying social network can dramatically skew an individual's local
observations, making a behavior appear far more common locally than it is
globally. We trace the origins of this phenomenon, which we call &quot;the majority
illusion,&quot; to the friendship paradox in social networks. As a result of this
paradox, a behavior that is globally rare may be systematically overrepresented
in the local neighborhoods of many people, i.e., among their friends. Thus, the
&quot;majority illusion&quot; may facilitate the spread of social contagions in networks
and also explain why systematic biases in social perceptions, for example, of
risky behavior, arise. Using synthetic and real-world networks, we explore how
the &quot;majority illusion&quot; depends on network structure and develop a statistical
model to calculate its magnitude in a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03027</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03027</id><created>2015-06-09</created><authors><author><keyname>Orduna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author><author><keyname>Serrano-Cobos</keyname><forenames>Jorge</forenames></author><author><keyname>Lloret-Romero</keyname><forenames>Nuria</forenames></author></authors><title>Disclosing the network structure of private companies on the web: the
  case of Spanish IBEX 35 share index</title><categories>cs.DL</categories><doi>10.1108/OIR-11-2014-0282</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common for an international company to have different brands, products
or services, information for investors, a corporate blog, affiliates, branches
in different countries, etc. If all these contents appear as independent
additional web domains (AWD), the company should be represented on the web by
all these web domains, since many of these AWDs may acquire remarkable
performance that could mask or distort the real web performance of the company,
affecting therefore on the understanding of web metrics. The main objective of
this study is to determine the amount, type, web impact and topology of the
additional web domains in commercial companies in order to get a better
understanding on their complete web impact and structure. The set of companies
belonging to the Spanish IBEX-35 stock index has been analyzed as testing
bench. We proceeded to identify and categorize all AWDs belonging to these
companies, and to apply both web impact (web presence and visibility) and
network metrics. The results show that AWDs get a high web presence but
relatively low web visibility, due to certain opacity or less dissemination of
some AWDs, favoring its isolation. This is verified by the low network density
values obtained, that occur because AWDs are strongly connected with the
corporate domain (although asymmetrically), but very weakly linked each other.
Although the processes of AWDs creation and categorization are complex (web
policy seems not to be driven by a defined or conscious plan), their influence
on the web performance of IBEX 35companies is meaningful. This research
measures the AWDs influence on companies under webometric terms for the first
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03030</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03030</id><created>2015-06-09</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author><author><keyname>Seeman</keyname><forenames>Lior</forenames></author></authors><title>Computational Extensive-Form Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define solution concepts appropriate for computationally bounded players
playing a fixed finite game. To do so, we need to define what it means for a
\emph{computational game}, which is a sequence of games that get larger in some
appropriate sense, to represent a single finite underlying extensive-form game.
Roughly speaking, we require all the games in the sequence to have essentially
the same structure as the underlying game, except that two histories that are
indistinguishable (i.e., in the same information set) in the underlying game
may correspond to histories that are only computationally indistinguishable in
the computational game. We define a computational version of both Nash
equilibrium and sequential equilibrium for computational games, and show that
every Nash (resp., sequential) equilibrium in the underlying game corresponds
to a computational Nash (resp., sequential) equilibrium in the computational
game. One advantage of our approach is that if a cryptographic protocol
represents an abstract game, then we can analyze its strategic behavior in the
abstract game, and thus separate the cryptographic analysis of the protocol
from the strategic analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03032</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03032</id><created>2015-06-08</created><authors><author><keyname>Xu</keyname><forenames>Hui</forenames></author><author><keyname>Zhou</keyname><forenames>Yangfan</forenames></author><author><keyname>Lyu</keyname><forenames>Michael R.</forenames></author></authors><title>N-Version Obfuscation: Impeding Software Tampering Replication with
  Program Diversity</title><categories>cs.CR cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tamper-resistance is a fundamental software security research area. Many
approaches have been proposed to thwart specific procedures of tampering, e.g.,
obfuscation and self-checksumming. However, to our best knowledge, none of them
can achieve theoretically tamper-resistance. Our idea is to impede the
replication of tampering via program diversification, and thus increasing the
complexity to break the whole software system. To this end, we propose to
deliver same featured, but functionally nonequivalent software copies to
different machines. We formally define the problem as N-version obfuscation,
and provide a viable means to solve the problem. Our evaluation result shows
that the time required for breaking a software system is linearly increased
with the number of software versions, which is O(n) complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03039</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03039</id><created>2015-06-09</created><updated>2016-01-10</updated><authors><author><keyname>Gorham</keyname><forenames>Jackson</forenames></author><author><keyname>Mackey</keyname><forenames>Lester</forenames></author></authors><title>Measuring Sample Quality with Stein's Method</title><categories>stat.ML cs.LG math.PR stat.ME</categories><comments>17 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To improve the efficiency of Monte Carlo estimation, practitioners are
turning to biased Markov chain Monte Carlo procedures that trade off asymptotic
exactness for computational speed. The reasoning is sound: a reduction in
variance due to more rapid sampling can outweigh the bias introduced. However,
the inexactness creates new challenges for sampler and parameter selection,
since standard measures of sample quality like effective sample size do not
account for asymptotic bias. To address these challenges, we introduce a new
computable quality measure based on Stein's method that quantifies the maximum
discrepancy between sample and target expectations over a large class of test
functions. We use our tool to compare exact, biased, and deterministic sample
sequences and illustrate applications to hyperparameter selection, convergence
rate assessment, and quantifying bias-variance tradeoffs in posterior
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03040</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03040</id><created>2015-06-09</created><authors><author><keyname>Cahill</keyname><forenames>Jameson</forenames></author><author><keyname>Chen</keyname><forenames>Xuemei</forenames></author><author><keyname>Wang</keyname><forenames>Rongrong</forenames></author></authors><title>The gap between the null space property and the restricted isomety
  property</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The null space property (NSP) and the restricted isometry property (RIP) are
two properties which have received considerable attention in the compressed
sensing literature. As the name suggests, NSP is a property that depends solely
on the null space of the measurement procedure and as such, any two matrices
which have the same null space will have NSP if either one of them does. On the
other hand, RIP is a property of the measurement procedure itself, and given an
RIP matrix it is straightforward to construct another matrix with the same null
space that is not RIP. %Furthermore, RIP is known to imply NSP and therefore
RIP is a strictly stronger assumption than NSP. We say a matrix is RIP-NSP if
it has the same null space as an RIP matrix. We show that such matrices can
provide robust recovery of compressible signals under Basis pursuit which in
many applicable settings is comparable to the guarantee that RIP provides. More
importantly, we constructively show that the RIP-NSP is stronger than NSP with
the aid of this robust recovery result, which shows that RIP is fundamentally
stronger than NSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03041</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03041</id><created>2015-06-09</created><authors><author><keyname>Borsa</keyname><forenames>Diana</forenames></author><author><keyname>Graepel</keyname><forenames>Thore</forenames></author><author><keyname>Gordon</keyname><forenames>Andrew</forenames></author></authors><title>The Wreath Process: A totally generative model of geometric shape based
  on nested symmetries</title><categories>cs.AI stat.ML</categories><comments>10 pages(double-column), 60+ figures</comments><msc-class>20-XX</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of modelling noisy but highly symmetric shapes that
can be viewed as hierarchies of whole-part relationships in which higher level
objects are composed of transformed collections of lower level objects. To this
end, we propose the stochastic wreath process, a fully generative probabilistic
model of drawings. Following Leyton's &quot;Generative Theory of Shape&quot;, we
represent shapes as sequences of transformation groups composed through a
wreath product.
  This representation emphasizes the maximization of transfer --- the idea that
the most compact and meaningful representation of a given shape is achieved by
maximizing the re-use of existing building blocks or parts.
  The proposed stochastic wreath process extends Leyton's theory by defining a
probability distribution over geometric shapes in terms of noise processes that
are aligned with the generative group structure of the shape. We propose an
inference scheme for recovering the generative history of given images in terms
of the wreath process using reversible jump Markov chain Monte Carlo methods
and Approximate Bayesian Computation. In the context of sketching we
demonstrate the feasibility and limitations of this approach on model-generated
and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03059</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03059</id><created>2015-06-09</created><authors><author><keyname>Cohen</keyname><forenames>Nadav</forenames></author><author><keyname>Sharir</keyname><forenames>Or</forenames></author><author><keyname>Shashua</keyname><forenames>Amnon</forenames></author></authors><title>Deep SimNets</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deep layered architecture that generalizes convolutional neural
networks (ConvNets). The architecture, called SimNets, is driven by two
operators: (i) a similarity function that generalizes inner-product, and (ii) a
log-mean-exp function called MEX that generalizes maximum and average. The two
operators applied in succession give rise to a standard neuron but in &quot;feature
space&quot;. The feature spaces realized by SimNets depend on the choice of the
similarity operator. The simplest setting, which corresponds to a convolution,
realizes the feature space of the Exponential kernel, while other settings
realize feature spaces of more powerful kernels (Generalized Gaussian, which
includes as special cases RBF and Laplacian), or even dynamically learned
feature spaces (Generalized Multiple Kernel Learning). As a result, the SimNet
contains a higher abstraction level compared to a traditional ConvNet. We argue
that enhanced expressiveness is important when the networks are small due to
run-time constraints (such as those imposed by mobile applications). Empirical
evaluation on CIFAR-10 validates the superior expressiveness of SimNets,
showing a significant gain in accuracy over ConvNets when computational
resources at run-time are limited. We also show that in large-scale settings,
where computational complexity is less of a concern, the additional capacity of
SimNets can be controlled with proper regularization, yielding accuracies
comparable to state of the art ConvNets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03072</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03072</id><created>2015-06-09</created><authors><author><keyname>Kumar</keyname><forenames>Vijay</forenames></author><author><keyname>Levy</keyname><forenames>Dan</forenames></author></authors><title>Clustering by transitive propagation</title><categories>cs.LG cond-mat.stat-mech stat.ML</categories><comments>13 pages + 2 appendices, figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a global optimization algorithm for clustering data given the
ratio of likelihoods that each pair of data points is in the same cluster or in
different clusters. To define a clustering solution in terms of pairwise
relationships, a necessary and sufficient condition is that belonging to the
same cluster satisfies transitivity. We define a global objective function
based on pairwise likelihood ratios and a transitivity constraint over all
triples, assigning an equal prior probability to all clustering solutions. We
maximize the objective function by implementing max-sum message passing on the
corresponding factor graph to arrive at an O(N^3) algorithm. Lastly, we
demonstrate an application inspired by mutational sequencing for decoding
random binary words transmitted through a noisy channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03099</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03099</id><created>2015-06-09</created><updated>2015-09-23</updated><authors><author><keyname>Bengio</keyname><forenames>Samy</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Jaitly</keyname><forenames>Navdeep</forenames></author><author><keyname>Shazeer</keyname><forenames>Noam</forenames></author></authors><title>Scheduled Sampling for Sequence Prediction with Recurrent Neural
  Networks</title><categories>cs.LG cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks can be trained to produce sequences of tokens given
some input, as exemplified by recent results in machine translation and image
captioning. The current approach to training them consists of maximizing the
likelihood of each token in the sequence given the current (recurrent) state
and the previous token. At inference, the unknown previous token is then
replaced by a token generated by the model itself. This discrepancy between
training and inference can yield errors that can accumulate quickly along the
generated sequence. We propose a curriculum learning strategy to gently change
the training process from a fully guided scheme using the true previous token,
towards a less guided scheme which mostly uses the generated token instead.
Experiments on several sequence prediction tasks show that this approach yields
significant improvements. Moreover, it was used successfully in our winning
entry to the MSCOCO image captioning challenge, 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03101</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03101</id><created>2015-06-09</created><authors><author><keyname>Dai</keyname><forenames>Bo</forenames></author><author><keyname>He</keyname><forenames>Niao</forenames></author><author><keyname>Dai</keyname><forenames>Hanjun</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>Scalable Bayesian Inference via Particle Mirror Descent</title><categories>cs.LG stat.CO stat.ML</categories><comments>32 pages, 26 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian methods are appealing in their flexibility in modeling complex data
and their ability to capture uncertainty in parameters. However, when Bayes'
rule does not result in closed-form, most approximate Bayesian inference
algorithms lacks either scalability or rigorous guarantees. To tackle this
challenge, we propose a scalable yet simple algorithm, Particle Mirror Descent
(PMD), to iteratively approximate the posterior density. PMD is inspired by
stochastic functional mirror descent where one descends in the density space
using a small batch of data points at each iteration, and by particle filtering
where one uses samples to approximate a function. We prove result of the first
kind that, after $T$ iterations, PMD provides a posterior density estimator
that converges in terms of $KL$-divergence to the true posterior in rate
$O(1/\sqrt{T})$. We show that PMD is competitive to several scalable Bayesian
algorithms in mixture models, Bayesian logistic regression, sparse Gaussian
processes and latent Dirichlet allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03104</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03104</id><created>2015-06-09</created><authors><author><keyname>Eager</keyname><forenames>Eric</forenames></author><author><keyname>Eberle</keyname><forenames>Megan</forenames></author><author><keyname>Peirce</keyname><forenames>James</forenames></author></authors><title>How Infectious Was #Deflategate?</title><categories>cs.SI physics.soc-ph q-bio.PE stat.AP</categories><comments>12 pages, 4 figures</comments><msc-class>92B05, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On Monday January 19, 2015 a story broke that the National Football League
(NFL) had started an investigation into whether the New England Patriots
deliberately deflated the footballs they used during their championship win
over the Indianapolis Colts. Like an infectious disease, discussion regarding
Deflategate grew rapidly on social media sites in the hours and days after the
release of the story. However, after the Super Bowl was over, the scandal
slowly began to dissipate and lost much of the attention it had originally had,
as interest in the NFL wained at the completion of its season. We construct a
simple epidemic model for the infectiousness of the Deflategate news story. We
then use data from the social media site Twitter to estimate the parameters of
this model using standard techniques from the study of inverse problems. We
find that the infectiousness (as measured by the basic reproduction number) of
Deflategate rivals that of any infectious disease that we are aware of, and is
actually more infectious than recent news stories of greater importance - both
in terms of the basic reproduction number and in terms of the average amount of
time the average tweeter continued to tweet about the news story.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03108</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03108</id><created>2015-06-09</created><updated>2015-12-13</updated><authors><author><keyname>Nagy</keyname><forenames>Marcin</forenames></author><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Teemu</forenames></author><author><keyname>Kurnikov</keyname><forenames>Arseny</forenames></author><author><keyname>Ott</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Bringing Modern Web Applications to Disconnected Networks</title><categories>cs.NI</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opportunistic networking is one way to realize pervasive applications while
placing little demand on network infrastructure, especially for operating in
less well connected environments. In contrast to the ubiquitous network access
model inherent to many cloud-based applications, for which the web browser
forms the user front end, opportunistic applications require installing
software on mobile devices. Even though app stores (when accessible) offer
scalable distribution mechanisms for applications, a designer needs to support
multiple OS platforms and only some of those are suitable for opportunistic
operation to begin with. In this paper, we present a web browser-based
interaction framework that 1) allows users to interact with opportunistic
application content without installing the respective app and 2) even supports
users whose mobile OSes do not support opportunistic networking at all via
minimal stand-alone infrastructure. We describe our system and protocol design,
validate its operation using simulations, and report on our implementation
including support for six opportunistic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03124</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03124</id><created>2015-06-09</created><authors><author><keyname>Mandal</keyname><forenames>Subhamoy</forenames></author><author><keyname>Sudarshan</keyname><forenames>Viswanath Pamulakanty</forenames></author><author><keyname>Nagaraj</keyname><forenames>Yeshaswini</forenames></author><author><keyname>Ben</keyname><forenames>Xose Luis Dean</forenames></author><author><keyname>Razansky</keyname><forenames>Daniel</forenames></author></authors><title>Multiscale edge detection and parametric shape modeling for boundary
  delineation in optoacoustic images</title><categories>physics.med-ph cs.CV</categories><comments>Engineering in Medicine and Biology Society (EMBC), 2015 37th Annual
  International Conference of the IEEE (Accepted version)</comments><journal-ref>Engineering in Medicine and Biology Society (EMBC), 2015 37th
  Annual International Conference of the IEEE , vol., no., pp.707-710, 25-29
  Aug. 2015</journal-ref><doi>10.1109/EMBC.2015.7318460</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this article, we present a novel scheme for segmenting the image boundary
(with the background) in optoacoustic small animal in vivo imaging systems. The
method utilizes a multiscale edge detection algorithm to generate a binary edge
map. A scale dependent morphological operation is employed to clean spurious
edges. Thereafter, an ellipse is fitted to the edge map through constrained
parametric transformations and iterative goodness of fit calculations. The
method delimits the tissue edges through the curve fitting model, which has
shown high levels of accuracy. Thus, this method enables segmentation of
optoacoutic images with minimal human intervention, by eliminating need of
scale selection for multiscale processing and seed point determination for
contour mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03128</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03128</id><created>2015-06-08</created><authors><author><keyname>Babjan</keyname><forenames>Jani Biju</forenames></author></authors><title>License Plate Recognition System Based on Color Coding Of License Plates</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  License Plate Recognition Systems are used to determine the license plate
number of a vehicle. The current system mainly uses Optical Character
Recognition to recognize the number plate. There are several problems to this
system. Some of them include interchanging of several letters or numbers
(letter O with digit 0), difficulty in localizing the license plate, high error
rate, use of different fonts in license plates etc. So a new system to
recognize the license plate number using color coding of license plates is
proposed in this paper. Easier localization of license plate can be done by
searching for the start or stop patters of license plates. An eight segment
display system along with traditional numbering with the first and last
segments left for start or stop patterns is proposed in this paper. Practical
applications include several areas under Internet of Things (IoT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03134</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03134</id><created>2015-06-09</created><authors><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Fortunato</keyname><forenames>Meire</forenames></author><author><keyname>Jaitly</keyname><forenames>Navdeep</forenames></author></authors><title>Pointer Networks</title><categories>stat.ML cs.CG cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new neural architecture to learn the conditional probability
of an output sequence with elements that are discrete tokens corresponding to
positions in an input sequence. Such problems cannot be trivially addressed by
existent approaches such as sequence-to-sequence and Neural Turing Machines,
because the number of target classes in each step of the output depends on the
length of the input, which is variable. Problems such as sorting variable sized
sequences, and various combinatorial optimization problems belong to this
class. Our model solves the problem of variable size output dictionaries using
a recently proposed mechanism of neural attention. It differs from the previous
attention attempts in that, instead of using attention to blend hidden units of
an encoder to a context vector at each decoder step, it uses attention as a
pointer to select a member of the input sequence as the output. We call this
architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn
approximate solutions to three challenging geometric problems -- finding planar
convex hulls, computing Delaunay triangulations, and the planar Travelling
Salesman Problem -- using training examples alone. Ptr-Nets not only improve
over sequence-to-sequence with input attention, but also allow us to generalize
to variable size output dictionaries. We show that the learnt models generalize
beyond the maximum lengths they were trained on. We hope our results on these
tasks will encourage a broader exploration of neural learning for discrete
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03137</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03137</id><created>2015-06-09</created><updated>2015-11-23</updated><authors><author><keyname>Schramm</keyname><forenames>Tselil</forenames></author><author><keyname>Weitz</keyname><forenames>Benjamin</forenames></author></authors><title>Symmetric Tensor Completion from Multilinear Entries and Learning
  Product Mixtures over the Hypercube</title><categories>cs.DS cs.LG stat.ML</categories><comments>Removed adversarial matrix completion algorithm after discovering
  that our matrix completion results can be derived from prior work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm for completing an order-$m$ symmetric low-rank tensor
from its multilinear entries in time roughly proportional to the number of
tensor entries. We apply our tensor completion algorithm to the problem of
learning mixtures of product distributions over the hypercube, obtaining new
algorithmic results. If the centers of the product distribution are linearly
independent, then we recover distributions with as many as $\Omega(n)$ centers
in polynomial time and sample complexity. In the general case, we recover
distributions with as many as $\tilde\Omega(n)$ centers in quasi-polynomial
time, answering an open problem of Feldman et al. (SIAM J. Comp.) for the
special case of distributions with incoherent bias vectors.
  Our main algorithmic tool is the iterated application of a low-rank matrix
completion algorithm for matrices with adversarially missing entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03139</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03139</id><created>2015-06-09</created><authors><author><keyname>Werling</keyname><forenames>Keenon</forenames></author><author><keyname>Angeli</keyname><forenames>Gabor</forenames></author><author><keyname>Manning</keyname><forenames>Christopher</forenames></author></authors><title>Robust Subgraph Generation Improves Abstract Meaning Representation
  Parsing</title><categories>cs.CL</categories><comments>To appear in ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Abstract Meaning Representation (AMR) is a representation for open-domain
rich semantics, with potential use in fields like event extraction and machine
translation. Node generation, typically done using a simple dictionary lookup,
is currently an important limiting factor in AMR parsing. We propose a small
set of actions that derive AMR subgraphs by transformations on spans of text,
which allows for more robust learning of this stage. Our set of construction
actions generalize better than the previous approach, and can be learned with a
simple classifier. We improve on the previous state-of-the-art result for AMR
parsing, boosting end-to-end performance by 3 F$_1$ on both the LDC2013E117 and
LDC2014T12 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03140</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03140</id><created>2015-06-09</created><updated>2015-12-07</updated><authors><author><keyname>Werling</keyname><forenames>Keenon</forenames></author><author><keyname>Chaganty</keyname><forenames>Arun</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author><author><keyname>Manning</keyname><forenames>Chris</forenames></author></authors><title>On-the-Job Learning with Bayesian Decision Theory</title><categories>cs.AI</categories><comments>As appearing in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to deploy a high-accuracy system starting with zero training
examples. We consider an &quot;on-the-job&quot; setting, where as inputs arrive, we use
real-time crowdsourcing to resolve uncertainty where needed and output our
prediction when confident. As the model improves over time, the reliance on
crowdsourcing queries decreases. We cast our setting as a stochastic game based
on Bayesian decision theory, which allows us to balance latency, cost, and
accuracy objectives in a principled way. Computing the optimal policy is
intractable, so we develop an approximation based on Monte Carlo Tree Search.
We tested our approach on three datasets---named-entity recognition, sentiment
classification, and image classification. On the NER task we obtained more than
an order of magnitude reduction in cost compared to full human annotation,
while boosting performance relative to the expert provided labels. We also
achieve a 8% F1 improvement over having a single human label the whole set, and
a 28% F1 improvement over online learning.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="78000" completeListSize="102538">1122234|79001</resumptionToken>
</ListRecords>
</OAI-PMH>
