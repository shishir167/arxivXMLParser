<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:03:23Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|92001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04382</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04382</id><created>2016-02-13</created><authors><author><keyname>Santo</keyname><forenames>Jos&#xe9; Esp&#xed;rito</forenames></author><author><keyname>Matthes</keyname><forenames>Ralph</forenames></author><author><keyname>Pinto</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>A Calculus for a Coinductive Analysis of Proof Search</title><categories>cs.LO math.LO</categories><comments>25 pages</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In reductive proof search, proofs are naturally generalized by solutions,
comprising all (possibly infinite) structures generated by locally correct,
bottom-up application of inference rules. We propose a rather natural extension
of the Curry-Howard paradigm of representation, from proofs to solutions: to
represent solutions by (possibly infinite) terms of the coinductive variant of
the typed lambda-calculus that represents proofs. We take this as a starting
point for a new, comprehensive approach to proof search; our case study is
proof search in the sequent calculus LJT for intuitionistic implication logic.
A second, finitary representation is proposed, where the lambda-calculus that
represents proofs is extended with a formal greatest fixed point. Formal sums
are used in both representations to express alternatives in the search process,
so that not only individual solutions but actually solution spaces are
expressed. Moreover, formal sums are used in the coinductive syntax to define
&quot;co-contraction&quot; (contraction bottom-up). Co-contraction is a semantical match
to a relaxed form of binding of fixed-point variables present in the finitary
system, and the latter allows the detection of cycles through the type system.
The main result is the existence of an equivalent finitary representation for
any given solution space expressed coinductively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04393</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04393</id><created>2016-02-13</created><authors><author><keyname>Maurya</keyname><forenames>Abhinav</forenames></author><author><keyname>Murray</keyname><forenames>Kenton</forenames></author><author><keyname>Liu</keyname><forenames>Yandong</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Cohen</keyname><forenames>William W.</forenames></author><author><keyname>Neill</keyname><forenames>Daniel B.</forenames></author></authors><title>Semantic Scan: Detecting Subtle, Spatially Localized Events in Text
  Streams</title><categories>cs.IR stat.ML</categories><comments>10 pages, 4 figures, KDD 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early detection and precise characterization of emerging topics in text
streams can be highly useful in applications such as timely and targeted public
health interventions and discovering evolving regional business trends. Many
methods have been proposed for detecting emerging events in text streams using
topic modeling. However, these methods have numerous shortcomings that make
them unsuitable for rapid detection of locally emerging events on massive text
streams. In this paper, we describe Semantic Scan (SS) that has been developed
specifically to overcome these shortcomings in detecting new spatially compact
events in text streams.
  Semantic Scan integrates novel contrastive topic modeling with online
document assignment and principled likelihood ratio-based spatial scanning to
identify emerging events with unexpected patterns of keywords hidden in text
streams. This enables more timely and accurate detection and characterization
of anomalous, spatially localized emerging events. Semantic Scan does not
require manual intervention or labeled training data, and is robust to noise in
real-world text data since it identifies anomalous text patterns that occur in
a cluster of new documents rather than an anomaly in a single new document.
  We compare Semantic Scan to alternative state-of-the-art methods such as
Topics over Time, Online LDA, and Labeled LDA on two real-world tasks: (i) a
disease surveillance task monitoring free-text Emergency Department chief
complaints in Allegheny County, and (ii) an emerging business trend detection
task based on Yelp reviews. On both tasks, we find that Semantic Scan provides
significantly better event detection and characterization accuracy than
competing approaches, while providing up to an order of magnitude speedup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04396</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04396</id><created>2016-02-13</created><authors><author><keyname>Li</keyname><forenames>Yanjun</forenames></author><author><keyname>Lee</keyname><forenames>Kiryung</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Optimal Sample Complexity for Stable Matrix Recovery</title><categories>cs.IT math.IT</categories><comments>5 pages. Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tremendous efforts have been made to study the theoretical and algorithmic
aspects of sparse recovery and low-rank matrix recovery. This paper fills a
theoretical gap in matrix recovery: the optimal sample complexity for stable
recovery without constants or log factors. We treat sparsity, low-rankness, and
potentially other parsimonious structures within the same framework: constraint
sets that have small covering numbers or Minkowski dimensions. We consider
three types of random measurement matrices (unstructured, rank-1, and symmetric
rank-1 matrices), following probability distributions that satisfy some mild
conditions. In all these cases, we prove a fundamental result -- the recovery
of matrices with parsimonious structures, using an optimal (or near optimal)
number of measurements, is stable with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04398</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04398</id><created>2016-02-13</created><updated>2016-02-16</updated><authors><author><keyname>Li</keyname><forenames>Yanjun</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Dimensionality Reduction for Nonlinear Regression with Two Predictor
  Vectors</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>31 pages, 1 figure. Submitted to COLT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many variables that we would like to predict depend nonlinearly on two types
of attributes. For example, prices are influenced by supply and demand. Movie
ratings are determined by demographic attributes and genre attributes. This
paper addresses the dimensionality reduction problem in such regression
problems with two predictor vectors. In particular, we assume a discriminative
model where low-dimensional linear embeddings of the two predictor vectors are
sufficient statistics for predicting a dependent variable. We show that a
simple algorithm involving singular value decomposition can accurately estimate
the embeddings provided that certain sample complexities are satisfied,
surprisingly, without specifying the nonlinear regression model. These
embeddings improve the efficiency and robustness of subsequent training, and
can serve as a pre-training algorithm for neural networks. The main results
establish sample complexities under multiple settings. Sample complexities for
different regression models only differ by constant factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04399</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04399</id><created>2016-02-13</created><authors><author><keyname>Guti&#xe9;rrez</keyname><forenames>Gilberto</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author><author><keyname>Torres</keyname><forenames>Claudio</forenames></author></authors><title>Linear Separability in Spatial Databases</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two point sets $R$ and $B$ in the plane, with cardinalities $m$ and
$n$, respectively, and each set stored in a separate R-tree, we present an
algorithm to decide whether $R$ and $B$ are linearly separable. Our algorithm
exploits the structure of the R-trees, loading into the main memory only
relevant data, and runs in $O(m\log m + n\log n)$ time in the worst case. As
experimental results, we implement the proposed algorithm and executed it on
several real and synthetic point sets, showing that the percentage of nodes of
the R-trees that are accessed and the memory usage are low in these cases. We
also present an algorithm to compute the convex hull of $n$ planar points given
in an R-tree, running in $O(n\log n)$ time in the worst case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04400</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04400</id><created>2016-02-13</created><authors><author><keyname>Singh</keyname><forenames>Ajita</forenames></author><author><keyname>Xing</keyname><forenames>Yuxuan</forenames></author><author><keyname>Seferoglu</keyname><forenames>Hulya</forenames></author></authors><title>Energy-Aware Cooperative Computation in Mobile Devices</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New data intensive applications, which are continuously emerging in daily
routines of mobile devices, significantly increase the demand for data, and
pose a challenge for current wireless networks due to scarce resources.
Although bandwidth is traditionally considered as the primary scarce resource
in wireless networks, the developments in communication theory shifts the focus
from bandwidth to other scarce resources including processing power and energy.
Especially, in device-to-device networks, where data rates are increasing
rapidly, processing power and energy are becoming the primary bottlenecks of
the network. Thus, it is crucial to develop new networking mechanisms by taking
into account the processing power and energy as bottlenecks. In this paper, we
develop an energy-aware cooperative computation framework for mobile devices.
In this setup, a group of cooperative mobile devices, within proximity of each
other, (i) use their cellular or Wi-Fi (802.11) links as their primary
networking interfaces, and (ii) exploit their device-to-device connections
(e.g., Wi-Fi Direct) to overcome processing power and energy bottlenecks. We
evaluate our energy-aware cooperative computation framework on a testbed
consisting of smartphones and tablets, and we show that it brings significant
performance benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04402</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04402</id><created>2016-02-13</created><authors><author><keyname>Du</keyname><forenames>Xin</forenames></author><author><keyname>Benner</keyname><forenames>Peter</forenames></author></authors><title>Balanced Truncation of Linear Time-Invariant Systems over
  Finite-frequency Ranges</title><categories>cs.SY math.DS math.OC</categories><comments>prepared to submit for International Journal of Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses model order reduction of LTI systems over limited
frequency intervals within the framework of balanced truncation. Two new
\emph{frequency-dependent balanced truncation} methods were developed, one is
\emph{SF-type frequency-dependent balanced truncation} to copy with the cases
that only a single dominating point of the operating frequency interval is
pre-known, the other is \emph{interval-type frequency-dependent balanced
truncation} to deal with the cases that both of the upper and lower bound of
frequency interval are known \emph{a priori}. SF-type error bound and
interval-type error bound are derived for the first time to estimate the
desired approximation error over pre-specified frequency interval. We show that
the new methods generally lead to good in-band approximation performance, at
the same time, provide accurate error bounds under certain conditions. Examples
are included for illustration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04408</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04408</id><created>2016-02-13</created><authors><author><keyname>Du</keyname><forenames>Xin</forenames></author><author><keyname>Benner</keyname><forenames>Peter</forenames></author></authors><title>Finite-Frequency Model Order Reduction of Linear Systems via
  Parameterized Frequency-dependent Balanced Truncation</title><categories>cs.SY math.DS math.OC</categories><comments>submitted to IEEE Transactions on Automatic control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Balanced truncation is one of the most common model order reduction schemes.
In this paper, we study finite-frequency model order reduction (FF-MOR)
problems of linear continuous-time systems within the framework of balanced
truncation method. Firstly, we construct a family of parameterized
frequency-dependent (PFD) mappings which generate discrete-time PFD mapped
systems and continuous-time PFD mapped systems of the given continuous-time
system. The relationships between the maximum singular value of the given
system over pre-specified frequency ranges and the maximum singular value of
the PFD mapped systems over entire frequency range are established. By
exploiting the properties of the discrete-time PFD mapped systems, a new
parameterized frequency-dependent balanced truncation (PFDBT) method providing
finite-frequency type error bound with respect to the maximum singular value of
the approximation error systems are developed. Examples are included for
illustration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04409</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04409</id><created>2016-02-13</created><authors><author><keyname>Yarkony</keyname><forenames>Julian</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author></authors><title>Convex Optimization For Non-Convex Problems via Column Generation</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply column generation to approximating complex structured objects via a
set of primitive structured objects under either the cross entropy or L2 loss.
We use L1 regularization to encourage the use of few structured primitive
objects. We attack approximation using convex optimization over an infinite
number of variables each corresponding to a primitive structured object that
are generated on demand by easy inference in the Lagrangian dual. We apply our
approach to producing low rank approximations to large 3-way tensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04410</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04410</id><created>2016-02-13</created><authors><author><keyname>Hwang</keyname><forenames>Sung-Ha</forenames></author><author><keyname>Rey-Bellet</keyname><forenames>Luc</forenames></author></authors><title>Simple Characterizations of Potential Games and Zero-sum Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide several tests to determine whether a game is a potential game or
whether it is a zero-sum equivalent game---a game which is strategically
equivalent to a zero-sum game in the same way that a potential game is
strategically equivalent to a common interest game. We present a unified
framework applicable for both potential and zero-sum equivalent games by
deriving a simple but useful characterization of these games. This allows us to
re-derive known criteria for potential games, as well as obtain several new
criteria. In particular, we prove (1) new integral tests for potential games
and for zero-sum equivalent games, (2) a new derivative test for zero-sum
equivalent games, and (3) a new representation characterization for zero-sum
equivalent games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04414</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04414</id><created>2016-02-13</created><authors><author><keyname>Adegbija</keyname><forenames>Tosiron</forenames></author><author><keyname>Gordon-Ross</keyname><forenames>Ann</forenames></author></authors><title>Temperature-aware Dynamic Optimization of Embedded Systems</title><categories>cs.AR</categories><comments>24 pages, 12 figures, Extended version of paper &quot;Thermal-aware
  Phase-based Tuning of Embedded Systems&quot; published in GLSVLSI 2014, Submitted
  to ACM TODAES</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to embedded systems` stringent design constraints, much prior work
focused on optimizing energy consumption and/or performance. Since embedded
systems typically have fewer cooling options, rising temperature, and thus
temperature optimization, is an emergent concern. Most embedded systems only
dissipate heat by passive convection, due to the absence of dedicated thermal
management hardware mechanisms. The embedded system`s temperature not only
affects the system`s reliability, but could also affect the performance, power,
and cost. Thus, embedded systems require efficient thermal management
techniques. However, thermal management can conflict with other optimization
objectives, such as execution time and energy consumption. In this paper, we
focus on managing the temperature using a synergy of cache optimization and
dynamic frequency scaling, while also optimizing the execution time and energy
consumption. This paper provides new insights on the impact of cache parameters
on efficient temperature-aware cache tuning heuristics. In addition, we present
temperature-aware phase-based tuning, TaPT, which determines Pareto optimal
clock frequency and cache configurations for fine-grained execution time,
energy, and temperature tradeoffs. TaPT enables autonomous system optimization
and also allows designers to specify temperature constraints and optimization
priorities. Experiments show that TaPT can effectively reduce execution time,
energy, and temperature, while imposing minimal hardware overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04415</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04415</id><created>2016-02-13</created><authors><author><keyname>Adegbija</keyname><forenames>Tosiron</forenames></author><author><keyname>Gordon-Ross</keyname><forenames>Ann</forenames></author><author><keyname>Munir</keyname><forenames>Arslan</forenames></author></authors><title>Phase distance mapping: a phase-based cache tuning methodology for
  embedded systems</title><categories>cs.AR</categories><comments>26 pages, Springer Design Automation for Embedded Systems, Special
  Issue on Networked Embedded Systems, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networked embedded systems typically leverage a collection of low-power
embedded systems (nodes) to collaboratively execute applications spanning
diverse application domains (e.g., video, image processing, communication,
etc.) with diverse application requirements. The individual networked nodes
must operate under stringent constraints (e.g., energy, memory, etc.) and
should be specialized to meet varying application requirements in order to
adhere to these constraints. Phase-based tuning specializes system tunable
parameters to the varying runtime requirements of different execution phases to
meet optimization goals. Since the design space for tunable systems can be very
large, one of the major challenges in phase-based tuning is determining the
best configuration for each phase without incurring significant tuning overhead
(e.g., energy and/or performance) during design space exploration. In this
paper, we propose phase distance mapping, which directly determines the best
configuration for a phase, thereby eliminating design space exploration. Phase
distance mapping applies the correlation between the characteristics and best
configuration of a known phase to determine the best configuration of a new
phase. Experimental results verify that our phase distance mapping approach,
when applied to cache tuning, determines cache configurations within 1 % of the
optimal configurations on average and yields an energy delay product savings of
27 % on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04418</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04418</id><created>2016-02-14</created><authors><author><keyname>Park</keyname><forenames>Gunwoong</forenames></author><author><keyname>Raskutti</keyname><forenames>Garvesh</forenames></author></authors><title>Identifiability assumptions for directed graphical models with feedback</title><categories>stat.ML cs.LG</categories><comments>20 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Directed graphical models provide a useful framework for modeling causal or
directional relationships for multivariate data. Prior work has largely focused
on identifiability and search algorithms for directed acyclic graphical (DAG)
models. In many applications, feedback naturally arises and directed graphical
models that permit cycles arise. However theory and methodology for directed
graphical models with feedback are considerably less developed since graphs
with cycles pose a number of additional challenges. In this paper we address
the issue of identifiability for general directed cyclic graphical (DCG) models
satisfying only the Markov assumption. In particular, in addition to the
faithfulness assumption which has already been introduced for cyclic models, we
introduce two new identifiability assumptions, one based on selecting the model
with the fewest edges and the other based on selecting the DCG model that
entails the maximum d-separation rules. We provide theoretical results
comparing these assumptions which shows that: (1) selecting models with the
largest number of d-separation rules is strictly weaker than the faithfulness
assumption; (2) unlike for DAG models, selecting models with the fewest edges
do not necessarily result in a milder assumption than the faithfulness
assumption. We also provide connections between our two new principles and
minimality assumptions which lead to a ranking of how strong and weak various
identifiability and minimality assumptions are for both DAG and DCG models. We
use our identifiability assumptions to develop search algorithms for
small-scale DCG models. Our simulations results using our search algorithms
support our theoretical results, showing that our two new principles generally
out-perform the faithfulness assumption in terms of selecting the true skeleton
for DCG models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04419</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04419</id><created>2016-02-14</created><authors><author><keyname>Boczkowski</keyname><forenames>Lucas</forenames></author><author><keyname>Korman</keyname><forenames>Amos</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author></authors><title>Self-Stabilizing Clock Synchronization with 3-bit messages</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the complexities of basic distributed computing tasks
while operating under severe fault-tolerant contexts and strong communication
constraints. We consider the self-stabilizing context, in which internal states
of processors (agents) are initially chosen by an adversary. Given a population
of $n$ agents, we assume that communication is restricted to the synchronous
PULL model, where in each round, each agent can pull information from few other
agents, chosen uniformly at random. We are primarily interested in the design
of algorithms that use messages as small as possible. Indeed, in such models,
restricting the message-size may have a profound impact on both the solvability
of the problem and its running time. We concentrate on variants of two
fundamental problems, namely, clock-synchronization, where agents aim to
synchronize their clocks modulo some $T$, and Zealot-consensus, in which agents
need to agree on a common output bit, despite the presence of at most one agent
whose output bit is fixed throughout the execution.
  Our main technical contribution is the construction of a
clock-synchronization protocol that converges in $\tilde{\mathcal
O}(\log^2T\log n)$ rounds with high probability and uses only 3 bits per
message. In addition to being self-stabilizing, this protocol is robust to the
presence of $\mathcal O(n^{1/2-\varepsilon})$ Byzantine agents, for any
positive constant~$\varepsilon$. Using this clock-synchronization protocol, we
solve the self-stabilizing Zealot-consensus problem in time $\tilde{\mathcal
O}(\log n)$ using only 4-bit messages. Our technique for obtaining the clock
synchronization is rather general and can be applied to a large family of
models where agents are passively mobile, provided that there exists a
self-stabilizing symmetric-consensus protocol defined for a single bit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04420</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04420</id><created>2016-02-14</created><updated>2016-02-15</updated><authors><author><keyname>Xu</keyname><forenames>Bolun</forenames></author><author><keyname>Dvorkin</keyname><forenames>Yury</forenames></author><author><keyname>Kirschen</keyname><forenames>Daniel S.</forenames></author><author><keyname>Silva-Monroy</keyname><forenames>C. A.</forenames></author><author><keyname>Watson</keyname><forenames>Jean-Paul</forenames></author></authors><title>A Comparison of Policies on the Participation of Storage in U.S.
  Frequency Regulation Markets</title><categories>math.OC cs.SY</categories><comments>This paper will be presented in IEEE PES General Meeting 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because energy storage systems have better ramping characteristics than
traditional generators, their participation in frequency regulation should
facilitate the balancing of load and generation. However, they cannot sustain
their output indefinitely. System operators have therefore implemented new
frequency regulation policies to take advantage of the fast ramps that energy
storage systems can deliver while alleviating the problems associated with
their limited energy capacity. This paper contrasts several U.S. policies that
directly affect the participation of energy storage systems in frequency
regulation and compares the revenues that the owners of such systems might
achieve under each policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04421</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04421</id><created>2016-02-14</created><authors><author><keyname>Liu</keyname><forenames>Mingmou</forenames></author><author><keyname>Pan</keyname><forenames>Xiaoyin</forenames></author><author><keyname>Yin</keyname><forenames>Yitong</forenames></author></authors><title>Randomized approximate nearest neighbor search with limited adaptivity</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental problem of approximate nearest neighbor search in
$d$-dimensional Hamming space $\{0,1\}^d$. We study the complexity of the
problem in the famous cell-probe model, a classic model for data structures. We
consider algorithms in the cell-probe model with limited adaptivity, where the
algorithm makes $k$ rounds of parallel accesses to the data structure for a
given $k$. For any $k\ge 1$, we give a simple randomized algorithm solving the
approximate nearest neighbor search using $k$ rounds of parallel memory
accesses, with $O(k(\log d)^{1/k})$ accesses in total. We also give a more
sophisticated randomized algorithm using $O(k+(\frac{1}{k}\log d)^{O(1/k)})$
memory accesses in $k$ rounds for large enough $k$. Both algorithms use data
structures of size polynomial in $n$, the number of points in the database.
  For the lower bound, we prove an $\Omega(\frac{1}{k}(\log d)^{1/k})$ lower
bound for the total number of memory accesses required by any randomized
algorithm solving the approximate nearest neighbor search within
$k\le\frac{\log\log d}{2\log\log\log d}$ rounds of parallel memory accesses on
any data structures of polynomial size. This lower bound shows that our first
algorithm is asymptotically optimal for any constant round $k$. And our second
algorithm approaches the asymptotically optimal tradeoff between rounds and
memory accesses, in a sense that the lower bound of memory accesses for any
$k_1$ rounds can be matched by the algorithm within $k_2=O(k_1)$ rounds. In the
extreme, for some large enough $k=\Theta\left(\frac{\log\log d}{\log\log\log
d}\right)$, our second algorithm matches the $\Theta\left(\frac{\log\log
d}{\log\log\log d}\right)$ tight bound for fully adaptive algorithms for
approximate nearest neighbor search due to Chakrabarti and Regev.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04422</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04422</id><created>2016-02-14</created><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author></authors><title>Hi Detector, What's Wrong with that Object? Identifying Irregular Object
  From Images by Modelling the Detection Score Distribution</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the challenging problem of identifying the irregular
status of objects from images in an &quot;open world&quot; setting, that is,
distinguishing the irregular status of an object category from its regular
status as well as objects from other categories in the absence of &quot;irregular
object&quot; training data. To address this problem, we propose a novel approach by
inspecting the distribution of the detection scores at multiple image regions
based on the detector trained from the &quot;regular object&quot; and &quot;other objects&quot;.
The key observation motivating our approach is that for &quot;regular object&quot; images
as well as &quot;other objects&quot; images, the region-level scores follow their own
essential patterns in terms of both the score values and the spatial
distributions while the detection scores obtained from an &quot;irregular object&quot;
image tend to break these patterns. To model this distribution, we propose to
use Gaussian Processes (GP) to construct two separate generative models for the
case of the &quot;regular object&quot; and the &quot;other objects&quot;. More specifically, we
design a new covariance function to simultaneously model the detection score at
a single region and the score dependencies at multiple regions. We finally
demonstrate the superior performance of our method on a large dataset newly
proposed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04427</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04427</id><created>2016-02-14</created><authors><author><keyname>Xu</keyname><forenames>Zheng</forenames></author><author><keyname>Burdick</keyname><forenames>Douglas</forenames></author><author><keyname>Raschid</keyname><forenames>Louiqa</forenames></author></authors><title>Exploiting Lists of Names for Named Entity Identification of Financial
  Institutions from Unstructured Documents</title><categories>cs.CL</categories><comments>17 pages, 3 figures, under review of JDIQ</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a wealth of information about financial systems that is embedded in
document collections. In this paper, we focus on a specialized text extraction
task for this domain. The objective is to extract mentions of names of
financial institutions, or FI names, from financial prospectus documents, and
to identify the corresponding real world entities, e.g., by matching against a
corpus of such entities. The tasks are Named Entity Recognition (NER) and
Entity Resolution (ER); both are well studied in the literature. Our
contribution is to develop a rule-based approach that will exploit lists of FI
names for both tasks; our solution is labeled Dict-based NER and Rank-based ER.
Since the FI names are typically represented by a root, and a suffix that
modifies the root, we use these lists of FI names to create specialized root
and suffix dictionaries. To evaluate the effectiveness of our specialized
solution for extracting FI names, we compare Dict-based NER with a general
purpose rule-based NER solution, ORG NER. Our evaluation highlights the
benefits and limitations of specialized versus general purpose approaches, and
presents additional suggestions for tuning and customization for FI name
extraction. To our knowledge, our proposed solutions, Dict-based NER and
Rank-based ER, and the root and suffix dictionaries, are the first attempt to
exploit specialized knowledge, i.e., lists of FI names, for rule-based NER and
ER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04431</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04431</id><created>2016-02-14</created><authors><author><keyname>Mishra</keyname><forenames>Vikash</forenames></author><author><keyname>Singh</keyname><forenames>Vikram</forenames></author></authors><title>Distributed Query Processing Plans generation using Teacher Learner
  Based Optimization</title><categories>cs.DB</categories><comments>12 pages</comments><journal-ref>International Journal of Information Processing,9(4),
  46-60,2015,ISSN : 0973-8215</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing popularity, the number of data sources and the amount of
data has been growing very fast in recent years. The distribution of
operational data on disperse data sources impose a challenge on processing user
queries. In such database systems, the database relations required by a query
to answer may be stored at multiple sites. This leads to an exponential
increase in the number of possible equivalent or alternatives of a user query.
Though it is not computationally reasonable to explore exhaustively all
possible query plans in a large search space, thus a strategy is requisite to
produce optimal query plans in distributed database systems. The query plan
with most cost-effective option for query processing is measured necessary and
must be generated for a given query. This paper attempts to generate such
optimal query plans using a parameter less optimization technique
Teaching-Learner Based Optimization(TLBO). The TLBO algorithm was experiential
to go one better than the other optimization algorithms for the multi objective
unconstrained and constrained benchmark problems. Experimental comparisons of
TLBO based optimal plan generation with the multiobjective genetic algorithm
based distributed query plan generation algorithm shows that for higher number
of relations, the TLBO based algorithm is able to generate comparatively better
quality Top K query plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04433</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04433</id><created>2016-02-14</created><authors><author><keyname>Long</keyname><forenames>Mingsheng</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Unsupervised Domain Adaptation with Residual Transfer Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent success of deep neural networks relies on massive amounts of
labeled data. For a target task where labeled data is unavailable, domain
adaptation can transfer a learner from a different source domain. In this
paper, we propose a new approach to domain adaptation in deep networks that can
simultaneously learn adaptive classifiers and transferable features from
labeled data in the source domain and unlabeled data in the target domain. We
relax a shared-classifier assumption made by previous methods and assume that
the source classifier and target classifier differ by a residual function. We
enable classifier adaptation by plugging several layers into the deep network
to explicitly learn the residual function with reference to the target
classifier. We embed features of multiple layers into reproducing kernel
Hilbert spaces (RKHSs) and match feature distributions for feature adaptation.
The adaptation behaviors can be achieved in most feed-forward models by
extending them with new residual layers and loss functions, which can be
trained efficiently using standard back-propagation. Empirical evidence
exhibits that the approach outperforms state of art methods on standard domain
adaptation datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04434</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04434</id><created>2016-02-14</created><authors><author><keyname>Loukas</keyname><forenames>Andreas</forenames></author><author><keyname>Foucard</keyname><forenames>Damien</forenames></author></authors><title>Frequency Analysis of Temporal Graph Signals</title><categories>cs.LG cs.SY stat.ML</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter extends the concept of graph-frequency to graph signals that
evolve with time. Our goal is to generalize and, in fact, unify the familiar
concepts from time- and graph-frequency analysis. To this end, we study a joint
temporal and graph Fourier transform (JFT) and demonstrate its attractive
properties. We build on our results to create filters which act on the joint
(temporal and graph) frequency domain, and show how these can be used to
perform interference cancellation. The proposed algorithms are distributed,
have linear complexity, and can approximate any desired joint filtering
objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04435</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04435</id><created>2016-02-14</created><authors><author><keyname>Zhukov</keyname><forenames>A.</forenames></author><author><keyname>Sidorov</keyname><forenames>D.</forenames></author><author><keyname>Foley</keyname><forenames>A.</forenames></author></authors><title>Random Forest Based Approach for Concept Drift Handling</title><categories>cs.AI cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concept drift has potential in smart grid analysis because the socio-economic
behaviour of consumers is not governed by the laws of physics. Likewise there
are also applications in wind power forecasting. In this paper we present
decision tree ensemble classification method based on the Random Forest
algorithm for concept drift. The weighted majority voting ensemble aggregation
rule is employed based on the ideas of Accuracy Weighted Ensemble (AWE) method.
Base learner weight in our case is computed for each sample evaluation using
base learners accuracy and intrinsic proximity measure of Random Forest. Our
algorithm exploits both temporal weighting of samples and ensemble pruning as a
forgetting strategy. We present results of empirical comparison of our method
with original random forest with incorporated &quot;replace-the-looser&quot; forgetting
andother state-of-the-art concept-drfit classifiers like AWE2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04436</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04436</id><created>2016-02-14</created><authors><author><keyname>Isufi</keyname><forenames>Elvin</forenames></author><author><keyname>Loukas</keyname><forenames>Andreas</forenames></author><author><keyname>Simonetto</keyname><forenames>Andrea</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Distributed Time-Varying Graph Filtering</title><categories>cs.LG cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the cornerstones of the field of signal processing on graphs are graph
filters, direct analogues of classical filters, but intended for signals
defined on graphs. This work brings forth new insights on the distributed graph
filtering problem. We design a family of autoregressive moving average (ARMA)
recursions, which (i) are able to approximate any desired graph frequency
response, and (ii) give exact solutions for tasks such as graph signal
denoising and interpolation.
  The design philosophy, which allows us to design the ARMA coefficients
independently from the underlying graph, renders the ARMA graph filters
suitable in static and, particularly, time-varying settings. The latter occur
when the graph signal and/or graph are changing over time. We show that in case
of a time-varying graph signal our approach extends naturally to a
two-dimensional filter, operating concurrently in the graph and regular time
domains. We also derive sufficient conditions for filter stability when the
graph and signal are time-varying. The analytical and numerical results
presented in this paper illustrate that ARMA graph filters are practically
appealing for static and time-varying settings, accompanied by strong
theoretical guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04440</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04440</id><created>2016-02-14</created><authors><author><keyname>Aldaihani</keyname><forenames>Reem</forenames></author><author><keyname>AboElFotoh</keyname><forenames>Hosam</forenames></author></authors><title>A new scheme for maximizing the lifetime of heterogeneous wireless
  sensor networks</title><categories>cs.NI</categories><comments>Presented in First International Conference on Computing Sciences and
  Engineering (ICCSE2015), 15-17 March 2015, Kuwait University, Kuwait</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous wireless sensor network consists of wireless sensor nodes with
different abilities, such as different computing power and different initial
energy. We present in this paper a new scheme for maximizing heterogeneous WSN
lifetime. The proposed scheme employs two types of sensor nodes that are named
(consistent with IEEE 802.15.4 standard) Full Function Device (FFD) and Reduced
Function Device (RFD). The FFDs are the expensive sensor nodes with high power
and computational capabilities compared to the RFDs which are cheap sensors
with a limited power supply. The scheme divides the network into smaller
sub-networks (regions) that are built from sectors and tracks. The objective of
this research is to balance and reduce the communication load on RFDs, reduce
the delay, and increase the connectivity and lifetime, by using a limited
number of FFDs. We investigate the performance of our scheme via numerical
simulation and compare it to other related schemes that are presented for
homogeneous WSNs with chain topology, such as Pegasis, Epegasis and Chiron. In
addition to extending the lifetime, our scheme also results in reducing the
data transmission delay compared to the related schemes. Furthermore, the
scheme increases the network security and reduces the RFDs power consumption by
preventing the direct communication with the base station (BS). The FFD is the
communication bridge between RFDs and BS. The FFD communicate with the BS using
the one-hop approach or multi-hop approach through other FFDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04450</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04450</id><created>2016-02-14</created><authors><author><keyname>Berkenkamp</keyname><forenames>Felix</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author><author><keyname>Schoellig</keyname><forenames>Angela P.</forenames></author></authors><title>Bayesian Optimization with Safety Constraints: Safe and Automatic
  Parameter Tuning in Robotics</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotics algorithms typically depend on various parameters, the choice of
which significantly affects the robot's performance. While an initial guess for
the parameters may be obtained from dynamic models of the robot, parameters are
usually tuned manually on the real system to achieve the best performance.
Optimization algorithms, such as Bayesian optimization, have been used to
automate this process. However, these methods may evaluate parameters during
the optimization process that lead to safety-critical system failures.
Recently, a safe Bayesian optimization algorithm, called SafeOpt, has been
developed and applied in robotics, which guarantees that the performance of the
system never falls below a critical value; that is, safety is defined based on
the performance function. However, coupling performance and safety is not
desirable in most cases. In this paper, we define separate functions for
performance and safety. We present a generalized SafeOpt algorithm that, given
an initial safe guess for the parameters, maximizes performance but only
evaluates parameters that satisfy all safety constraints with high probability.
It achieves this by modeling the underlying and unknown performance and
constraint functions as Gaussian processes. We provide a theoretical analysis
and demonstrate in experiments on a quadrotor vehicle that the proposed
algorithm enables fast, automatic, and safe optimization of tuning parameters.
Moreover, we show an extension to context- or environment-dependent, safe
optimization in the experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04468</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04468</id><created>2016-02-14</created><updated>2016-02-23</updated><authors><author><keyname>Koteich</keyname><forenames>Mohamad</forenames></author><author><keyname>Duc</keyname><forenames>Gilles</forenames></author><author><keyname>Maloum</keyname><forenames>Abdelmalek</forenames></author><author><keyname>Sandou</keyname><forenames>Guillaume</forenames></author></authors><title>Observability of Sensorless Electric Drives</title><categories>math.DS cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electric drives control without shaft sensors has been an active research
topic for almost three decades. It consists of estimating the rotor speed
and/or position from the currents and voltages measurement. This paper deals
with the observability conditions of electric drives in view of sensorless
control. The models of such systems are strongly nonlinear. For this reason, a
local observability approach is applied to analyze the deteriorated performance
of sensorless drives in some operating conditions. The validity of the
observability conditions is confirmed by numerical simulations and experimental
data, using an extended Kalman filter as observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04473</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04473</id><created>2016-02-14</created><authors><author><keyname>Ruster</keyname><forenames>Michael</forenames></author></authors><title>Large-Scale Reasoning with OWL</title><categories>cs.AI cs.DB</categories><comments>Part of the &quot;Knowledge Representation in the Semantic Web&quot; Seminar by
  Matthias Thimm, Koblenz 2015</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  With the growth of the Semantic Web in size and importance, more and more
knowledge is stored in machine-readable formats such as the Web Ontology
Language OWL. This paper outlines common approaches for efficient reasoning on
large-scale data consisting of billions ($10^9$) of triples. Therefore, OWL and
its sublanguages, as well as forward and backward chaining techniques are
presented. The WebPIE reasoner is discussed in detail as an example for forward
chaining using MapReduce for materialisation. Moreover, the QueryPIE reasoner
is presented as a backward chaining/hybrid approach which uses query rewriting.
Furthermore, an overview on other reasoners is given such as OWLIM and TrOWL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04474</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04474</id><created>2016-02-14</created><authors><author><keyname>Rudi</keyname><forenames>Alessandro</forenames></author><author><keyname>Camoriano</keyname><forenames>Raffaello</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author></authors><title>Generalization Properties of Learning with Random Features</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the generalization properties of regularized learning with random
features in the statistical learning theory framework. We show that optimal
learning errors can be achieved with a number of features smaller than the
number of examples. As a byproduct, we also show that learning with random
features can be seen as a form of regularization, rather than only a way to
speed up computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04478</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04478</id><created>2016-02-14</created><authors><author><keyname>Chakaravarthy</keyname><forenames>Venkatesan T.</forenames></author><author><keyname>Kapralov</keyname><forenames>Michael</forenames></author><author><keyname>Murali</keyname><forenames>Prakash</forenames></author><author><keyname>Petrini</keyname><forenames>Fabrizio</forenames></author><author><keyname>Que</keyname><forenames>Xinyu</forenames></author><author><keyname>Sabharwal</keyname><forenames>Yogish</forenames></author><author><keyname>Schieber</keyname><forenames>Baruch</forenames></author></authors><title>Subgraph Counting: Color Coding Beyond Trees</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of counting occurrences of query graphs in a large data graph,
known as subgraph counting, is fundamental to several domains such as genomics
and social network analysis. Many important special cases (e.g. triangle
counting) have received significant attention. Color coding is a very general
and powerful algorithmic technique for subgraph counting. Color coding has been
shown to be effective in several applications, but scalable implementations are
only known for the special case of {\em tree queries} (i.e. queries of
treewidth one).
  In this paper we present the first efficient distributed implementation for
color coding that goes beyond tree queries: our algorithm applies to any query
graph of treewidth $2$. Since tree queries can be solved in time linear in the
size of the data graph, our contribution is the first step into the realm of
colour coding for queries that require superlinear running time in the worst
case. This superlinear complexity leads to significant load balancing problems
on graphs with heavy tailed degree distributions. Our algorithm structures the
computation to work around high degree nodes in the data graph, and achieves
very good runtime and scalability on a diverse collection of data and query
graph pairs as a result. We also provide theoretical analysis of our
algorithmic techniques, showing asymptotic improvements in runtime on random
graphs with power law degree distributions, a popular model for real world
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04482</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04482</id><created>2016-02-14</created><authors><author><keyname>P&#xf6;ll&#xe4;nen</keyname><forenames>Antti</forenames></author><author><keyname>Westerb&#xe4;ck</keyname><forenames>Thomas</forenames></author><author><keyname>Freij-Hollanti</keyname><forenames>Ragnar</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author></authors><title>Improved Singleton-type Bounds for Locally Repairable Codes</title><categories>cs.IT math.CO math.IT</categories><comments>5 pages, submitted to the 2016 IEEE International Symposium on
  Information Theory, presents the results of arXiv:1512.05325 in an abridged
  form</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locally repairable codes (LRCs) are error correcting codes used in
distributed data storage. Besides a global level, they enable errors to be
corrected locally, reducing the need for communication between storage nodes.
There is a close connection between almost affine LRCs and matroid theory which
can be utilized to construct good LRCs and derive bounds on their performance.
  This article presents two improvements to such results in [T. Westerb\&quot;ack et
al., &quot;On the Combinatorics of Locally Repairable Codes&quot;, Arxiv: 1501.00153]:
The class of parameters $(n,k,d,r,\delta)$ for which there exists a matroid
achieving the generalized Singleton bound for LRCs, is expanded. Also, an
improved lower bound is given for $d_{\rm{max}}(n,k,r,\delta)$, the maximal
achievable minimum distance $d$ that a matroid with parameters $(n,k,r,\delta)$
can have. This bound is proved to be optimal for the main class of matroids
used to derive the existence bounds in [T. Westerb\&quot;ack et al., &quot;On the
Combinatorics of Locally Repairable Codes&quot;, Arxiv: 1501.00153] and in this
article. The results obtained directly translate to similar results on LRCs
using the connection between them and matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04484</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04484</id><created>2016-02-14</created><updated>2016-03-05</updated><authors><author><keyname>Helmbold</keyname><forenames>David P.</forenames></author><author><keyname>Long</keyname><forenames>Philip M.</forenames></author></authors><title>Fundamental differences between Dropout and Weight Decay in Deep
  Networks</title><categories>cs.LG cs.AI cs.NE math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study dropout and weight decay applied to deep networks with rectified
linear units and the quadratic loss. We show how using dropout in this context
can be viewed as adding a regularization penalty term that grows exponentially
with the depth of the network when the more traditional weight decay penalty
grows polynomially. We then show how this difference affects the inductive bias
of algorithms using one regularizer or the other: we describe a random source
of data that dropout is unwilling to fit, but that is compatible with the
inductive bias of weight decay. We also describe a source that is compatible
with the inductive bias of dropout, but not weight decay. We also show that, in
contrast with the case of generalized linear models, when used with deep
networks with rectified linear units and the quadratic loss, the regularization
penalty of dropout (a) is not just a function of the independent variables, but
also depends on the response variables, and (b) can be negative. Finally, the
dropout penalty can drive a learning algorithm to use negative weights even
when trained with monotone training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04485</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04485</id><created>2016-02-14</created><authors><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author></authors><title>Benefits of depth in neural networks</title><categories>cs.LG cs.NE stat.ML</categories><comments>For a simplified version, see http://arxiv.org/abs/1509.08101</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any positive integer $k$, there exist neural networks with $\Theta(k^3)$
layers, $\Theta(1)$ nodes per layer, and $\Theta(1)$ distinct parameters which
can not be approximated by networks with $\mathcal{O}(k)$ layers unless they
are exponentially large --- they must possess $\Omega(2^k)$ nodes. This result
is proved here for a class of nodes termed &quot;semi-algebraic gates&quot; which
includes the common choices of ReLU, maximum, indicator, and piecewise
polynomial functions, therefore establishing benefits of depth against not just
standard networks with ReLU gates, but also convolutional networks with ReLU
and maximization gates, and boosted decision trees (in this last case with a
stronger separation: $\Omega(2^{k^3})$ total tree nodes are required).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04487</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04487</id><created>2016-02-14</created><updated>2016-02-29</updated><authors><author><keyname>Katz</keyname><forenames>Daniel J.</forenames></author></authors><title>Aperiodic Crosscorrelation of Sequences Derived from Characters</title><categories>cs.IT math.CO math.IT math.NT</categories><comments>52 pages</comments><msc-class>94A55, 42A05, 11T24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that pairs of maximal linear recursive sequences (m-sequences)
typically have mean square aperiodic crosscorrelation on par with that of
random sequences, but that if one takes a pair of m-sequences where one is the
reverse of the other, and shifts them appropriately, one can get significantly
lower mean square aperiodic crosscorrelation. Sequence pairs with even lower
mean square aperiodic crosscorrelation are constructed by taking a Legendre
sequence, cyclically shifting it, and then cutting it (approximately) in half
and using the halves as the sequences of the pair. In some of these
constructions, the mean square aperiodic crosscorrelation can be lowered
further if one truncates or periodically extends (appends) the sequences. Exact
asymptotic formulae for mean squared aperiodic crosscorrelation are proved for
sequences derived from additive characters (including m-sequences and modified
versions thereof) and multiplicative characters (including Legendre sequences
and their relatives). Data is presented that shows that sequences of modest
length have performance that closely approximates the asymptotic formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04489</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04489</id><created>2016-02-14</created><authors><author><keyname>Bar-Hillel</keyname><forenames>Aharon</forenames></author><author><keyname>Krupka</keyname><forenames>Eyal</forenames></author><author><keyname>Bloom</keyname><forenames>Noam</forenames></author></authors><title>Convolutional Tables Ensemble: classification in microseconds</title><categories>cs.CV cs.LG</categories><comments>10 pages</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study classifiers operating under severe classification time constraints,
corresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble
(CTE), an inherently fast architecture for object category recognition. The
architecture is based on convolutionally-applied sparse feature extraction,
using trees or ferns, and a linear voting layer. Several structure and
optimization variants are considered, including novel decision functions, tree
learning algorithm, and distillation from CNN to CTE architecture. Accuracy
improvements of 24-45% over related art of similar speed are demonstrated on
standard object recognition benchmarks. Using Pareto speed-accuracy curves, we
show that CTE can provide better accuracy than Convolutional Neural Networks
(CNN) for a certain range of classification time constraints, or alternatively
provide similar error rates with 5-200X speedup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04490</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04490</id><created>2016-02-14</created><authors><author><keyname>Yang</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Zhefeng</forenames></author><author><keyname>Pei</keyname><forenames>Jian</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author></authors><title>Tracking Influential Nodes in Dynamic Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle a challenging problem inherent in many important
applications: tracking influential nodes in dynamic networks. Specifically, we
model a dynamic network as a stream of edge weight updates. This general model
embraces many practical scenarios as special cases, such as edge and node
insertions, deletions as well as evolving weighted graphs. Under the popularly
adopted linear threshold model, we consider two essential versions of the
problem: finding the nodes whose influences passing a user specified threshold
and finding the top-$k$ most influential nodes. Our key idea is to use the
polling-based methods and maintain a sample of random paths so that we can
approximate the influence of nodes with provable quality guarantees. We develop
an efficient algorithm that incrementally updates the sample random paths
against network changes. We also design the methods determining the proper
sample sizes for the two versions of the problem so that we can provide strong
quality guarantees and, at the same time, be efficient in both space and time.
In addition to the thorough theoretical results, our experimental results on
three real network data sets clearly demonstrate the effectiveness and
efficiency of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04493</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04493</id><created>2016-02-14</created><authors><author><keyname>Ghafghazi</keyname><forenames>Hamidreza</forenames></author><author><keyname>ElMougy</keyname><forenames>Amr</forenames></author><author><keyname>Mouftah</keyname><forenames>Hussein T.</forenames></author><author><keyname>Adams</keyname><forenames>Carlisle</forenames></author></authors><title>Secure Data Storage Structure and Privacy-Preserving Mobile Search
  Scheme for Public Safety Networks</title><categories>cs.CR</categories><comments>7 pages, 3 figures, This work has been accepted to be presented in
  Wireless Communications and Networking Conference (WCNC), 2016 IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a Public Safety (PS) situation, agents may require critical and personally
identifiable information. Therefore, not only does context and location-aware
information need to be available, but also the privacy of such information
should be preserved. Existing solutions do not address such a problem in a PS
environment. This paper proposes a framework in which anonymized Personal
Information (PI) is accessible to authorized public safety agents under a PS
circumstance. In particular, we propose a secure data storage structure along
with privacy-preserving mobile search framework, suitable for Public Safety
Networks (PSNs). As a result, availability and privacy of PI are achieved
simultaneously. However, the design of such a framework encounters substantial
challenges, including scalability, reliability of the data, computation and
communication and storage efficiency, etc. We leverage Secure Indexing (SI)
methods and modify Bloom Filters (BFs) to create a secure data storage
structure to store encrypted meta-data. As a result, our construction enables
secure and privacy-preserving multi-keyword search capability. In addition, our
system scales very well, maintains availability of data, imposes minimum delay,
and has affordable storage overhead. We provide extensive security analysis,
simulation studies, and performance comparison with the state-of-the-art
solutions to demonstrate the efficiency and effectiveness of the proposed
approach. To the best of our knowledge, this work is the first to address such
issues in the context of PSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04496</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04496</id><created>2016-02-14</created><authors><author><keyname>Goparaju</keyname><forenames>Sreechakra</forenames></author><author><keyname>Fazeli</keyname><forenames>Arman</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author></authors><title>Minimum Storage Regenerating Codes For All Parameters</title><categories>cs.IT math.IT</categories><comments>9 pages, 3 figures, a short version of this paper has been submitted
  to the 2016 IEEE International Symposium on Information Theory</comments><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerating codes for distributed storage have attracted much research
interest in the past decade. Such codes trade the bandwidth needed to repair a
failed node with the overall amount of data stored in the network. Minimum
storage regenerating (MSR) codes are an important class of optimal regenerating
codes that minimize (first) the amount of data stored per node and (then) the
repair bandwidth. Specifically, an $[n,k,d]$-$(\alpha)$ MSR code $\mathbb{C}$
over $\mathbb{F}_q$ is defined as follows. Using such a code $\mathbb{C}$, a
file $\cal{F}$ consisting of $\alpha k$ symbols over $\mathbb{F}_q$ can be
distributed among $n$ nodes, each storing $\alpha$ symbols, in such a way that:
The file $\cal{F}$ can be recovered by downloading the content of any $k$ of
the $n$ nodes; and the content of any failed node can be reconstructed by
accessing any $d$ of the remaining $n-1$ nodes and downloading $\alpha/(d-k+1)$
symbols from each of these nodes. Unfortunately, explicit constructions of
$[n,k,d]$ MSR codes are known only for certain special cases: either low rate,
namely $k/n&lt;0.5$, or high repair connectivity, namely $d = n-1$. Although
setting $d = n-1$ minimizes the repair bandwidth, it may be impractical to
connect to all the remaining nodes in order to repair a single failed node. Our
main result in this paper is an explicit construction of systematic-repair
$[n,k,d]$ MSR codes for all possible values of parameters $n,k,d$. In
particular, we construct systematic-repair MSR codes of high rate $k/n&gt;0.5$ and
low repair connectivity $k&lt; d&lt;n-1$. Such codes were not previously known to
exist. In order to construct these codes, we solve simultaneously several
repair scenarios, each of which is expressible as an interference alignment
problem. Extension of our results beyond systematic repair remains an open
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04498</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04498</id><created>2016-02-14</created><updated>2016-02-23</updated><authors><author><keyname>Bate</keyname><forenames>Andrew</forenames></author><author><keyname>Motik</keyname><forenames>Boris</forenames></author><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames></author><author><keyname>Siman&#x10d;&#xed;k</keyname><forenames>Franti&#x161;ek</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author></authors><title>Extending Consequence-Based Reasoning to SRIQ</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Consequence-based calculi are a family of reasoning algorithms for
description logics (DLs), and they combine hypertableau and resolution in a way
that often achieves excellent performance in practice. Up to now, however, they
were proposed for either Horn DLs (which do not support disjunction), or for
DLs without counting quantifiers. In this paper we present a novel
consequence-based calculus for SRIQ---a rich DL that supports both features.
This extension is non-trivial since the intermediate consequences that need to
be derived during reasoning cannot be captured using DLs themselves. The
results of our preliminary performance evaluation suggest the feasibility of
our approach in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04500</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04500</id><created>2016-02-14</created><updated>2016-02-17</updated><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Joint Pushing and Caching with a Finite Receiver Buffer: Optimal
  Policies and Throughput Analysis</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pushing and caching hold the promise of significantly increasing the
throughput of content-centric wireless networks. However, the throughput gain
of these techniques is limited by the buffer size of the receiver. To overcome
this, this paper presents a Joint Pushing and Caching (JPC) method that jointly
determines the contents to be pushed to, and to be removed from, the receiver
buffer in each timeslot. An offline and two online JPC policies are proposed
respectively based on noncausal, statistical, and causal content Request Delay
Information (RDI), which predicts a user's request time for certain content. It
is shown that the effective throughput of JPC is increased with the receiver
buffer size and the pushing channel capacity. Furthermore, the causal feedback
of user requests is found to greatly enhance the performance of online JPC
without inducing much signalling overhead in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04502</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04502</id><created>2016-02-14</created><authors><author><keyname>Fan</keyname><forenames>Bin</forenames></author><author><keyname>Kong</keyname><forenames>Qingqun</forenames></author><author><keyname>Sui</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Wang</keyname><forenames>Xinchao</forenames></author><author><keyname>Xiang</keyname><forenames>Shiming</forenames></author><author><keyname>Pan</keyname><forenames>Chunhong</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>Do We Need Binary Features for 3D Reconstruction?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary features have been incrementally popular in the past few years due to
their low memory footprints and the efficient computation of Hamming distance
between binary descriptors. They have been shown with promising results on some
real time applications, e.g., SLAM, where the matching operations are relative
few. However, in computer vision, there are many applications such as 3D
reconstruction requiring lots of matching operations between local features.
Therefore, a natural question is that is the binary feature still a promising
solution to this kind of applications? To get the answer, this paper conducts a
comparative study of binary features and their matching methods on the context
of 3D reconstruction in a recently proposed large scale mutliview stereo
dataset. Our evaluations reveal that not all binary features are capable of
this task. Most of them are inferior to the classical SIFT based method in
terms of reconstruction accuracy and completeness with a not significant better
computational performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04503</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04503</id><created>2016-02-14</created><updated>2016-02-16</updated><authors><author><keyname>Juglaret</keyname><forenames>Yannis</forenames></author><author><keyname>Hritcu</keyname><forenames>Catalin</forenames></author><author><keyname>de Amorim</keyname><forenames>Arthur Azevedo</forenames></author><author><keyname>Pierce</keyname><forenames>Benjamin C.</forenames></author></authors><title>Beyond Full Abstraction: Formalizing the Security Guarantees of
  Low-Level Compartmentalization</title><categories>cs.CR cs.PL</categories><comments>Draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compartmentalization is widely regarded as good security-engineering
practice: if we break up a large software system into mutually distrustful
components that run with minimal privileges, restricting their interactions to
conform to well-defined interfaces, we can limit the damage caused by low-level
attacks such as control-flow hijacking. But the formal guarantees provided by
such low-level compartmentalization have seen surprisingly little
investigation.
  We propose a new property, secure compartmentalization, that formally
characterizes the security guarantees provided by low-level
compartmentalization and clarifies its attacker model. We rationally
reconstruct the secure compartmentalization property starting from the
well-established notion of fully abstract compilation, by identifying and
lifting three important limitations that make standard full abstraction
unsuitable for compartmentalization. The connection to full abstraction allows
us to prove secure compartmentalization for language implementations by
adapting established proof techniques, we illustrate this for a simple unsafe
imperative language with procedures and a compiler from this language to a
compartmentalized abstract machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04504</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04504</id><created>2016-02-14</created><authors><author><keyname>Wilber</keyname><forenames>Michael J.</forenames></author><author><keyname>Shmatikov</keyname><forenames>Vitaly</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author></authors><title>Can we still avoid automatic face detection?</title><categories>cs.CV</categories><comments>To appear at WACV 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After decades of study, automatic face detection and recognition systems are
now accurate and widespread. Naturally, this means users who wish to avoid
automatic recognition are becoming less able to do so. Where do we stand in
this cat-and-mouse race? We currently live in a society where everyone carries
a camera in their pocket. Many people willfully upload most or all of the
pictures they take to social networks which invest heavily in automatic face
recognition systems. In this setting, is it still possible for
privacy-conscientious users to avoid automatic face detection and recognition?
If so, how? Must evasion techniques be obvious to be effective, or are there
still simple measures that users can use to protect themselves?
  In this work, we find ways to evade face detection on Facebook, a
representative example of a popular social network that uses automatic face
detection to enhance their service. We challenge widely-held beliefs about
evading face detection: do our old techniques such as blurring the face region
or wearing &quot;privacy glasses&quot; still work? We show that in general,
state-of-the-art detectors can often find faces even if the subject wears
occluding clothing or even if the uploader damages the photo to prevent faces
from being detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04505</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04505</id><created>2016-02-14</created><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author></authors><title>Quasi-4-Connected Components</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new decomposition of a graphs into quasi-4-connected
components, where we call a graph quasi-4-connected if it is 3-connected and it
only has separations of order 3 that remove a single vertex. Moreover, we give
a cubic time algorithm computing the decomposition of a given graph.
  Our decomposition into quasi-4-connected components refines the well-known
decompositions of graphs into biconnected and triconnected components. We
relate our decomposition to Robertson and Seymour's theory of tangles by
establishing a correspondence between the quasi-4-connected components of a
graph and its tangles of order 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04506</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04506</id><created>2016-02-14</created><authors><author><keyname>Krishna</keyname><forenames>Ranjay</forenames></author><author><keyname>Hata</keyname><forenames>Kenji</forenames></author><author><keyname>Chen</keyname><forenames>Stephanie</forenames></author><author><keyname>Kravitz</keyname><forenames>Joshua</forenames></author><author><keyname>Shamma</keyname><forenames>David A.</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael S.</forenames></author></authors><title>Embracing Error to Enable Rapid Crowdsourcing</title><categories>cs.HC cs.CV</categories><comments>10 pages, 7 figures, CHI '16, CHI: ACM Conference on Human Factors in
  Computing Systems (2016)</comments><acm-class>H.5.m</acm-class><doi>10.1145/2858036.2858115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microtask crowdsourcing has enabled dataset advances in social science and
machine learning, but existing crowdsourcing schemes are too expensive to scale
up with the expanding volume of data. To scale and widen the applicability of
crowdsourcing, we present a technique that produces extremely rapid judgments
for binary and categorical labels. Rather than punishing all errors, which
causes workers to proceed slowly and deliberately, our technique speeds up
workers' judgments to the point where errors are acceptable and even expected.
We demonstrate that it is possible to rectify these errors by randomizing task
order and modeling response latency. We evaluate our technique on a breadth of
common labeling tasks such as image verification, word similarity, sentiment
analysis and topic classification. Where prior work typically achieves a 0.25x
to 1x speedup over fixed majority vote, our approach often achieves an order of
magnitude (10x) speedup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04511</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04511</id><created>2016-02-14</created><authors><author><keyname>Xu</keyname><forenames>Hongteng</forenames></author><author><keyname>Farajtabar</keyname><forenames>Mehrdad</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author></authors><title>Learning Granger Causality for Hawkes Processes</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning Granger causality for general point processes is a very challenging
task. In this paper, we propose an effective method, learning Granger
causality, for a special but significant type of point processes --- Hawkes
process. We reveal the relationship between Hawkes process's impact function
and its Granger causality graph. Specifically, our model represents impact
functions using a series of basis functions and recovers the Granger causality
graph via group sparsity of the impact functions' coefficients. We propose an
effective learning algorithm combining a maximum likelihood estimator (MLE)
with a sparse-group-lasso (SGL) regularizer. Additionally, the flexibility of
our model allows to incorporate the clustering structure event types into
learning framework. We analyze our learning algorithm and propose an adaptive
procedure to select basis functions. Experiments on both synthetic and
real-world data show that our method can learn the Granger causality graph and
the triggering patterns of the Hawkes processes simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04513</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04513</id><created>2016-02-14</created><authors><author><keyname>Quixad&#xe1;</keyname><forenames>Ana Paula</forenames></author><author><keyname>Onodera</keyname><forenames>Andrea Naomi</forenames></author><author><keyname>Pe&#xf1;a</keyname><forenames>Norberto</forenames></author><author><keyname>Miranda</keyname><forenames>Jos&#xe9; Garcia Vivas</forenames></author><author><keyname>S&#xe1;</keyname><forenames>Katia Nunes</forenames></author></authors><title>Validity and reliability of free software for bidimensional gait
  analysis</title><categories>q-bio.QM cs.CV physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the evaluation systems of human movement that have been advancing in
recent decades, their use are not feasible for clinical practice because it has
a high cost and scarcity of trained operators to interpret their results. An
ideal videogrammetry system should be easy to use, low cost, with minimal
equipment, and fast realization. The CvMob is a free tool for dynamic
evaluation of human movements that express measurements in figures, tables, and
graphics. This paper aims to determine if CvMob is a reliable tool for the
evaluation of two dimensional human gait. This is a validity and reliability
study. The sample was composed of 56 healthy individuals who walked on a
9-meterlong walkway and were simultaneously filmed by CvMob and Vicon system
cameras. Linear trajectories and angular measurements were compared to validate
the CvMob system, and inter and intrarater findings of the same measurements
were used to determine reliability. A strong correlation (rs mean = 0.988) of
the linear trajectories between systems and inter and intrarater analysis were
found. According to the Bland-Altman method, the angles that had good agreement
between systems were maximum flexion and extension (stance and swing) of the
knee and dorsiflexion range of motion and stride length. The CvMob is a
reliable tool for analysis of linear motion and lengths in two-dimensional
evaluations of human gait. The angular measurements demonstrate high agreement
for the knee joint; however, the hip and ankle measurements were limited by
differences between systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04514</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04514</id><created>2016-02-14</created><authors><author><keyname>Boothby</keyname><forenames>Tomas</forenames></author><author><keyname>Katz</keyname><forenames>Daniel J.</forenames></author></authors><title>Low Correlation Sequences from Linear Combinations of Characters</title><categories>cs.IT math.CO math.IT math.NT</categories><comments>45 pages</comments><msc-class>94A55, 11T24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pairs of binary sequences formed using linear combinations of multiplicative
characters of finite fields are exhibited that, when compared to random
sequence pairs, simultaneously achieve significantly lower mean square
autocorrelation values (for each sequence in the pair) and significantly lower
mean square crosscorrelation values. If we define crosscorrelation merit factor
analogously to the usual merit factor for autocorrelation, then randomly
selected binary sequence pairs are known to have crosscorrelation merit factor
$1$. Our constructions provide sequence pairs with crosscorrelation merit
factor well in excess of $1$, and at the same time, the autocorrelation merit
factors of the individual sequences can also be made well in excess of $1$
(which also indicates better than average performance). The sequence pairs
studied here provide combinations of autocorrelation and crosscorrelation
performance that are not achievable using sequences formed from single
characters, such as maximal linear recursive sequences (m-sequences) and
Legendre sequences. In this study, exact asymptotic formulae are proved for the
autocorrelation and crosscorrelation merit factors of sequence pairs formed
using linear combinations of multiplicative characters. Data is presented that
shows that the asymptotic behavior is closely approximated by sequences of
modest length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04518</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04518</id><created>2016-02-14</created><authors><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author><author><keyname>Zhan</keyname><forenames>Jinchun</forenames></author></authors><title>Recursive Recovery of Sparse Signal Sequences from Compressive
  Measurements: A Review</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we review the literature on design and analysis of recursive
algorithms for reconstructing a time sequence of sparse signals from
compressive measurements. The signals are assumed to be sparse in some
transform domain or in some dictionary. Their sparsity patterns can change with
time, although, in many practical applications, the changes are gradual. An
important class of applications where this problem occurs is dynamic projection
imaging, e.g., dynamic magnetic resonance imaging (MRI) for real-time medical
applications such as interventional radiology, or dynamic computed tomography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04521</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04521</id><created>2016-02-14</created><authors><author><keyname>Shirani</keyname><forenames>F.</forenames></author><author><keyname>Heidari</keyname><forenames>M.</forenames></author><author><keyname>Pradhan</keyname><forenames>S. S.</forenames></author></authors><title>Quasi Linear Codes: Application to Point-to-Point and Multi-Terminal
  Source Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new ensemble of structured codes is introduced. These codes are called
Quasi Linear Codes (QLC). The QLC's are constructed by taking subsets of linear
codes. They have a looser structure compared to linear codes and are not closed
under addition. We argue that these codes provide gains in terms of achievable
Rate-Distortions (RD) in different multi-terminal source coding problems. We
derive the necessary covering bounds for analyzing the performance of QLC's. We
then consider the Multiple-Descriptions (MD) problem, and prove through an
example that the application of QLC's gives an improved achievable RD region
for this problem. Finally, we derive an inner bound to the achievable RD region
for the general MD problem which strictly contains all of the previous known
achievable regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04529</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04529</id><created>2016-02-14</created><authors><author><keyname>Vitale</keyname><forenames>Jonathan</forenames></author><author><keyname>Williams</keyname><forenames>Mary-Anne</forenames></author><author><keyname>Johnston</keyname><forenames>Benjamin</forenames></author></authors><title>Socially Impaired Robots: Human Social Disorders and Robots'
  Socio-Emotional Intelligence</title><categories>cs.RO cs.CY cs.HC</categories><comments>International Conference on Social Robotics 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Social robots need intelligence in order to safely coexist and interact with
humans. Robots without functional abilities in understanding others and unable
to empathise might be a societal risk and they may lead to a society of
socially impaired robots. In this work we provide a survey of three relevant
human social disorders, namely autism, psychopathy and schizophrenia, as a
means to gain a better understanding of social robots' future capability
requirements. We provide evidence supporting the idea that social robots will
require a combination of emotional intelligence and social intelligence, namely
socio-emotional intelligence. We argue that a robot with a simple
socio-emotional process requires a simulation-driven model of intelligence.
Finally, we provide some critical guidelines for designing future
socio-emotional robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04530</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04530</id><created>2016-02-14</created><updated>2016-02-17</updated><authors><author><keyname>Coquand</keyname><forenames>Thierry</forenames></author><author><keyname>Mannaa</keyname><forenames>Bassel</forenames></author></authors><title>The Independence of Markov's Principle in Type Theory</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that Markov's principle is not derivable in dependent
type theory with natural numbers and one universe. One tentative way to prove
this would be to remark that Markov's principle does not hold in a sheaf model
of type theory over Cantor space, since Markov's principle does not hold for
the generic point of this model. It is however not clear how to interpret the
universe in a sheaf model [8, 13, 16]. Instead we design an extension of type
theory, which intuitively extends type theory by the addition of a generic
point of Cantor space. We then show the consistency of this extension by a
normalization argument. Markov's principle does not hold in this extension, and
it follows that it cannot be proved in type theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04536</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04536</id><created>2016-02-14</created><authors><author><keyname>Mirrezaei</keyname><forenames>Seyed Iman</forenames></author><author><keyname>Shahparian</keyname><forenames>Javad</forenames></author></authors><title>Data Load Balancing in Heterogeneous Dynamic Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data load balancing is a challenging task in the P2P systems.
  Distributed hash table (DHT) abstraction, heterogeneous nodes, and non
uniform distribution of objects are the reasons to cause load imbalance in
structured P2P overlay networks.
  Previous works solved the load balancing problem by assuming the homogeneous
capabilities of nodes, unawareness of the link latency during transferring
load, and imposing logical structures to collect and reassign load.
  We propose a distributed load balancing algorithm with the topology awareness
feature by using the concept of virtual servers.
  In our approach, each node collects neighborhood load information from
physically close nodes and reassigns virtual servers to overlay nodes based
upon the topology of underlying network.
  Consequently, our approach converges data load balancing quickly and it also
reduces the load transfer cost between nodes.
  Moreover, our approach increases the quality of load balancing among close
nodes of overlay and it also introduces a new tradeoff between the quality of
load balancing and load transfer cost among all overlay nodes.
  Our simulations show that our approach reduces the load transfer cost and it
saves network bandwidth respectively.
  Finally, we show that the in-degree imbalance of nodes, as a consequence of
topology awareness, cannot lead to a remarkable problem in topology aware
overlays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04552</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04552</id><created>2016-02-14</created><authors><author><keyname>Dinh</keyname><forenames>David</forenames></author><author><keyname>Simhadri</keyname><forenames>Harsha Vardhan</forenames></author><author><keyname>Tang</keyname><forenames>Yuan</forenames></author></authors><title>Extending the Nested Parallel Model to the Nested Dataflow Model with
  Provably Efficient Schedulers</title><categories>cs.DC</categories><acm-class>D.1.3; G.1.0; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nested parallel (a.k.a. fork-join) model is widely used for writing
parallel programs. However, the two composition constructs, i.e. &quot;$\parallel$&quot;
(parallel) and &quot;$;$&quot; (serial), are insufficient in expressing &quot;partial
dependencies&quot; or &quot;partial parallelism&quot; in a program. We propose a new dataflow
composition construct &quot;$\leadsto$&quot; to express partial dependencies in
algorithms in a processor- and cache-oblivious way, thus extending the Nested
Parallel (NP) model to the \emph{Nested Dataflow} (ND) model. We redesign
several divide-and-conquer algorithms ranging from dense linear algebra to
dynamic-programming in the ND model and prove that they all have optimal span
while retaining optimal cache complexity. We propose the design of runtime
schedulers that map ND programs to multicore processors with multiple levels of
possibly shared caches (i.e, Parallel Memory Hierarchies) and provide
theoretical guarantees on their ability to preserve locality and load balance.
For this, we adapt space-bounded (SB) schedulers for the ND model. We show that
our algorithms have increased &quot;parallelizability&quot; in the ND model, and that SB
schedulers can use the extra parallelizability to achieve asymptotically
optimal bounds on cache misses and running time on a greater number of
processors than in the NP model. The running time for the algorithms in this
paper is $O\left(\frac{\sum_{i=0}^{h-1} Q^{*}({\mathsf t};\sigma\cdot M_i)\cdot
C_i}{p}\right)$, where $Q^{*}$ is the cache complexity of task ${\mathsf t}$,
$C_i$ is the cost of cache miss at level-$i$ cache which is of size $M_i$,
$\sigma\in(0,1)$ is a constant, and $p$ is the number of processors in an
$h$-level cache hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04560</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04560</id><created>2016-02-14</created><authors><author><keyname>Ganardi</keyname><forenames>Moses</forenames></author><author><keyname>Hucke</keyname><forenames>Danny</forenames></author><author><keyname>K&#xf6;nig</keyname><forenames>Daniel</forenames></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author></authors><title>Circuit Evaluation for Finite Semirings</title><categories>cs.CC</categories><msc-class>68W30, 16Z05, 68W10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational complexity of the circuit evaluation problem for finite
semirings is considered, where semirings are not assumed to have an additive or
multiplicative identity. The following dichotomy is shown: If a finite semiring
is such that (i) the multiplicative semigroup is solvable and (ii) it does not
contain a subsemiring with an additive identity $0$ and a multiplicative
identity $1 \neq 0$, then the circuit evaluation problem for the semiring is in
$\mathsf{DET} \subseteq \mathsf{NC}^2$. In all other cases, the circuit
evaluation problem is $\mathsf{P}$-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04562</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04562</id><created>2016-02-15</created><updated>2016-02-17</updated><authors><author><keyname>Vrbik</keyname><forenames>Paul</forenames></author></authors><title>An Illustrated Introduction to the Truncated Fourier Transform</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Truncated Fourier Transform (TFT) is a variation of the Discrete Fourier
Transform (DFT/FFT) that allows for input vectors that do NOT have length $2^n$
for $n$ a positive integer. We present the univariate version of the TFT,
originally due to Joris van der Hoeven, heavily illustrating the presentation
in order to make these methods accessible to a broader audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04567</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04567</id><created>2016-02-15</created><authors><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Zhao</keyname><forenames>Renbo</forenames></author></authors><title>Adversarial Top-$K$ Ranking</title><categories>cs.IR cs.IT cs.LG math.IT stat.ML</categories><comments>32 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the top-$K$ ranking problem where the goal is to recover the set of
top-$K$ ranked items out of a large collection of items based on partially
revealed preferences. We consider an adversarial crowdsourced setting where
there are two population sets, and pairwise comparison samples drawn from one
of the populations follow the standard Bradley-Terry-Luce model (i.e., the
chance of item $i$ beating item $j$ is proportional to the relative score of
item $i$ to item $j$), while in the other population, the corresponding chance
is inversely proportional to the relative score. When the relative size of the
two populations is known, we characterize the minimax limit on the sample size
required (up to a constant) for reliably identifying the top-$K$ items, and
demonstrate how it scales with the relative size. Moreover, by leveraging a
tensor decomposition method for disambiguating mixture distributions, we extend
our result to the more realistic scenario in which the relative population size
is unknown, thus establishing an upper bound on the fundamental limit of the
sample size for recovering the top-$K$ set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04568</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04568</id><created>2016-02-15</created><authors><author><keyname>Slaney</keyname><forenames>John</forenames></author><author><keyname>Paleo</keyname><forenames>Bruno Woltzenlogel</forenames></author></authors><title>Conflict Resolution: a First-Order Resolution Calculus with Decision
  Literals and Conflict-Driven Clause Learning</title><categories>cs.LO</categories><acm-class>F.4.1; I.2.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper defines the (first-order) conflict resolution calculus: an
extension of the resolution calculus inspired by techniques used in modern
SAT-solvers. The resolution inference is restricted to (first-order)
unit-propagation and the calculus is extended with a mechanism for assuming
decision literals and a new inference rule for clause learning, which is a
first-order generalization of the propositional conflict-driven clause learning
(CDCL) procedure. The calculus is sound (because it can be simulated by natural
deduction) and refutationally complete (because it can simulate resolution),
and these facts are proven in detail here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04572</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04572</id><created>2016-02-15</created><authors><author><keyname>Ha-Thuc</keyname><forenames>Viet</forenames></author><author><keyname>Venkataraman</keyname><forenames>Ganesh</forenames></author><author><keyname>Rodriguez</keyname><forenames>Mario</forenames></author><author><keyname>Sinha</keyname><forenames>Shakti</forenames></author><author><keyname>Sundaram</keyname><forenames>Senthil</forenames></author><author><keyname>Guo</keyname><forenames>Lin</forenames></author></authors><title>Personalized Expertise Search at LinkedIn</title><categories>cs.IR cs.LG cs.SI</categories><comments>2015 IEEE International Conference on Big Data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LinkedIn is the largest professional network with more than 350 million
members. As the member base increases, searching for experts becomes more and
more challenging. In this paper, we propose an approach to address the problem
of personalized expertise search on LinkedIn, particularly for exploratory
search queries containing {\it skills}. In the offline phase, we introduce a
collaborative filtering approach based on matrix factorization. Our approach
estimates expertise scores for both the skills that members list on their
profiles as well as the skills they are likely to have but do not explicitly
list. In the online phase (at query time) we use expertise scores on these
skills as a feature in combination with other features to rank the results. To
learn the personalized ranking function, we propose a heuristic to extract
training data from search logs while handling position and sample selection
biases. We tested our models on two products - LinkedIn homepage and LinkedIn
recruiter. A/B tests showed significant improvements in click through rates -
31% for CTR@1 for recruiter (18% for homepage) as well as downstream messages
sent from search - 37% for recruiter (20% for homepage). As of writing this
paper, these models serve nearly all live traffic for skills search on LinkedIn
homepage as well as LinkedIn recruiter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04577</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04577</id><created>2016-02-15</created><authors><author><keyname>Sakai</keyname><forenames>Yuta</forenames></author><author><keyname>Iwata</keyname><forenames>Ken-ichi</forenames></author></authors><title>Relations Between Conditional Shannon Entropy and Expectation of
  $\ell_{\alpha}$-Norm</title><categories>cs.IT math.IT</categories><comments>a short version was submitted to ISIT'2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper examines relationships between the conditional Shannon entropy and
the expectation of $\ell_{\alpha}$-norm for joint probability distributions.
More precisely, we investigate the tight bounds of the expectation of
$\ell_{\alpha}$-norm with a fixed conditional Shannon entropy, and vice versa.
As applications of the results, we derive the tight bounds between the
conditional Shannon entropy and several information measures which are
determined by the expectation of $\ell_{\alpha}$-norm, e.g., the conditional
R\'{e}nyi entropy and the conditional $R$-norm information. Moreover, we apply
these results to discrete memoryless channels under a uniform input
distribution. Then, we show the tight bounds of Gallager's $E_{0}$ functions
with a fixed mutual information under a uniform input distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04579</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04579</id><created>2016-02-15</created><authors><author><keyname>Takada</keyname><forenames>Toshiyuki</forenames></author><author><keyname>Hanada</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Yamada</keyname><forenames>Yoshiji</forenames></author><author><keyname>Sakuma</keyname><forenames>Jun</forenames></author><author><keyname>Takeuchi</keyname><forenames>Ichiro</forenames></author></authors><title>Secure Approximation Guarantee for Cryptographically Private Empirical
  Risk Minimization</title><categories>stat.ML cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy concern has been increasingly important in many machine learning (ML)
problems. We study empirical risk minimization (ERM) problems under secure
multi-party computation (MPC) frameworks. Main technical tools for MPC have
been developed based on cryptography. One of limitations in current
cryptographically private ML is that it is computationally intractable to
evaluate non-linear functions such as logarithmic functions or exponential
functions. Therefore, for a class of ERM problems such as logistic regression
in which non-linear function evaluations are required, one can only obtain
approximate solutions. In this paper, we introduce a novel cryptographically
private tool called secure approximation guarantee (SAG) method. The key
property of SAG method is that, given an arbitrary approximate solution, it can
provide a non-probabilistic assumption-free bound on the approximation quality
under cryptographically secure computation framework. We demonstrate the
benefit of the SAG method by applying it to several problems including a
practical privacy-preserving data analysis task on genomic and clinical
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04581</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04581</id><created>2016-02-15</created><authors><author><keyname>Kartynnik</keyname><forenames>Yury</forenames></author><author><keyname>Ryzhikov</keyname><forenames>Andrew</forenames></author></authors><title>On Minimum Maximal Distance-k Matchings</title><categories>cs.DM cs.CC math.CO</categories><comments>19 pages, 3 figures</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of several problems connected with the
problems of finding a maximal distance-$k$ matching of minimum cardinality or
minimum weight. We introduce the class of $k$-equimatchable graphs which is an
edge analogue of $k$-equipackable graphs. We prove that the recognition of
$k$-equimatchable graphs is co-NP-complete for any fixed $k \ge 2$. We provide
a simple characterization for the class of graphs with equal $k$-packing and
$k$-domination numbers. We also prove that the problem of finding a minimum
weight maximal distance-$2l$ matching in chordal graphs is hard to approximate
within a factor of $\varepsilon \ln |V(G)|$ for a fixed $\varepsilon$ unless
P=NP. Finally, we show NP-hardness of the minimum maximal induced matching
problem in several restricted graph classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04584</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04584</id><created>2016-02-15</created><updated>2016-02-16</updated><authors><author><keyname>Tsuda</keyname><forenames>Hirofumi</forenames></author><author><keyname>Umeno</keyname><forenames>Ken</forenames></author></authors><title>Weyl Spreading Sequence Optimizing CDMA</title><categories>cs.IT math.IT nlin.CD</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the new spreading sequence obtained by the Weyl sequence is
proposed for CDMA systems. Its cross-correlation function follows
$O(\frac{1}{N})$, where $N$ is the code length of the spreading sequence. In
this paper, we optimize the Weyl sequence code design to assign to each user
for CDMA systems and we analytically calculate its theoretical SIR (Signal to
Interference Noise Ratio). It is theoretically proven that the CDMA systems
with spreading sequence has about 2.5 times larger capacity of users than the
CDMA systems with the Gold code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04589</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04589</id><created>2016-02-15</created><authors><author><keyname>Garivier</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>IMT</affiliation></author><author><keyname>Kaufmann</keyname><forenames>Emilie</forenames><affiliation>CRIStAL, SEQUEL</affiliation></author></authors><title>Optimal Best Arm Identification with Fixed Confidence</title><categories>math.ST cs.LG stat.ML stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a complete characterization of the complexity of best-arm
identification in one-parameter bandit problems. We prove a new, tight lower
bound on the sample complexity. We propose the 'Track-and-Stop' strategy, which
is proved to be asymptotically optimal. It consists in a new sampling rule
(which tracks the optimal proportions of arm draws highlighted by the lower
bound) and in a stopping rule named after Chernoff, for which we give a new
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04591</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04591</id><created>2016-02-15</created><authors><author><keyname>Mao</keyname><forenames>Yuyi</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>ARQ with Adaptive Feedback for Energy Harvesting Receivers</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures, accepted to IEEE Wireless Communications and
  Networking Conference (WCNC) 2016, Doha, Qatar</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic repeat request (ARQ) is widely used in modern communication systems
to improve transmission reliability. In conventional ARQ protocols developed
for systems with energy-unconstrained receivers, an
acknowledgement/negative-acknowledgement (ACK/NACK) message is fed back when
decoding succeeds/fails. Such kind of non-adaptive feedback consumes
significant amount of energy, and thus will limit the performance of systems
with energy harvesting (EH) receivers. In order to overcome this limitation and
to utilize the harvested energy more efficiently, we propose a novel ARQ
protocol for EH receivers, where the ACK feedback can be adapted based upon the
receiver's EH state. Two conventional ARQ protocols are also considered. By
adopting the packet drop probability (PDP) as the performance metric, we
formulate the throughput constrained PDP minimization problem for a
communication link with a non-EH transmitter and an EH receiver. Optimal
reception policies including the sampling, decoding and feedback strategies,
are developed for different ARQ protocols. Simulation results will show that
the proposed ARQ protocol not only outperforms the conventional ARQs in terms
of PDP, but can also achieve a higher throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04593</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04593</id><created>2016-02-15</created><authors><author><keyname>James</keyname><forenames>Alex Pappachen</forenames></author></authors><title>Edge Detection for Pattern Recognition: A Survey</title><categories>cs.CV</categories><comments>Int. J. of Applied Pattern recognition, Vol 3, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This review provides an overview of the literature on the edge detection
methods for pattern recognition that inspire from the understanding of human
vision. We note that edge detection is one of the most fundamental process
within the low level vision and provides the basis for the higher level visual
intelligence in primates. The recognition of the patterns within the images
relate closely to the spatiotemporal processes of edge formations, and its
implementation needs a crossdisciplanry approach in neuroscience, computing and
pattern recognition. In this review, the edge detectors are grouped in as edge
features, gradients and sketch models, and some example applications are
provided for reference. We note a significant increase in the amount of
published research in the last decade that utilizes edge features in a wide
range of problems in computer vision and image understanding having a direct
implication to pattern recognition with images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04605</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04605</id><created>2016-02-15</created><authors><author><keyname>Pichler</keyname><forenames>Georg</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author><author><keyname>Matz</keyname><forenames>Gerald</forenames></author></authors><title>Distributed Information-Theoretic Biclustering</title><categories>cs.IT cs.LG math.IT</categories><comments>submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a novel multi-terminal source coding setup motivated by the
biclustering problem. Two separate encoders observe two stationary, memoryless
sources $X^n$ and $Z^n$, respectively. The goal is to find rate-limited
encodings $f(x^n)$ and $g(z^n)$ that maximize the mutual information
$I(f(X^n);g(Z^n))/n$. We present non-trivial outer and inner bounds on the
achievable region for this problem. These bounds are also generalized to an
arbitrary collection of stationary, memoryless sources. The considered problem
is intimately connected to distributed hypothesis testing against independence
under communication constraints, and hence our results are expected to apply to
that setting as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04613</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04613</id><created>2016-02-15</created><authors><author><keyname>Naouali</keyname><forenames>Sami</forenames></author><author><keyname>Salem</keyname><forenames>Semeh Ben</forenames></author></authors><title>Towards reducing the multidimensionality of OLAP cubes using the
  Evolutionary Algorithms and Factor Analysis Methods</title><categories>cs.AI</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Warehouses are structures with large amount of data collected from
heterogeneous sources to be used in a decision support system. Data Warehouses
analysis identifies hidden patterns initially unexpected which analysis
requires great memory and computation cost. Data reduction methods were
proposed to make this analysis easier. In this paper, we present a hybrid
approach based on Genetic Algorithms (GA) as Evolutionary Algorithms and the
Multiple Correspondence Analysis (MCA) as Analysis Factor Methods to conduct
this reduction. Our approach identifies reduced subset of dimensions from the
initial subset p where p'&lt;p where it is proposed to find the profile fact that
is the closest to reference. GAs identify the possible subsets and the Khi
formula of the ACM evaluates the quality of each subset. The study is based on
a distance measurement between the reference and n facts profile extracted from
the Warehouses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04620</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04620</id><created>2016-02-15</created><authors><author><keyname>Onywoki</keyname><forenames>Benson M.</forenames></author><author><keyname>Opiyo</keyname><forenames>Elisha T.</forenames></author></authors><title>A Framework for the Adoption of Biometric ATM Authentication in the
  Kenyan Banks</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The use of ATMs has become fundamental in the banking industry owing to the
values transacted using these systems and their 24/7 usage. Although several
researchers have studied the role of biometrics in security applications for
financial institutions, no systematic empirical research has been applied to
studying the role of organizational characteristics and contextual factors in
the Kenyan financial sector. This study sought to develop a framework for the
adoption of biometric ATMs in the Kenyan banking sector, apply the developed
framework to study factors influencing adoption of biometric ATM authentication
and validate the developed conceptual framework. A survey was used to collect
quantitative data from the ATM users which was then analysed using factor
analysis and multiple regression analysis. The study established that
performance expectancy, effort expectancy, social influence and user privacy
were key determinants for biometric ATMs acceptance, adoption and usage. The
study further demonstrated that age, gender and experience were moderating
factors on effort expectancy with experience further moderating performance
expectancy, effort expectancy, social influence and user privacy. Key words:
Key words: framework, adoption, authentication, financial institutions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04621</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04621</id><created>2016-02-15</created><authors><author><keyname>Osband</keyname><forenames>Ian</forenames></author><author><keyname>Blundell</keyname><forenames>Charles</forenames></author><author><keyname>Pritzel</keyname><forenames>Alexander</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>Deep Exploration via Bootstrapped DQN</title><categories>cs.LG cs.AI cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient exploration in complex environments remains a major challenge for
reinforcement learning. We propose bootstrapped DQN, a simple algorithm that
explores in a computationally and statistically efficient manner through use of
randomized value functions. Unlike dithering strategies such as epsilon-greedy
exploration, bootstrapped DQN carries out temporally-extended (or deep)
exploration; this can lead to exponentially faster learning. We demonstrate
these benefits in complex stochastic MDPs and in the large-scale Arcade
Learning Environment. Bootstrapped DQN substantially improves learning times
and performance across most Atari games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04629</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04629</id><created>2016-02-15</created><authors><author><keyname>Gomez-Miguelez</keyname><forenames>Ismael</forenames></author><author><keyname>Garcia-Saavedra</keyname><forenames>Andres</forenames></author><author><keyname>Sutton</keyname><forenames>Paul D.</forenames></author><author><keyname>Serrano</keyname><forenames>Pablo</forenames></author><author><keyname>Cano</keyname><forenames>Cristina</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>srsLTE: An Open-Source Platform for LTE Evolution and Experimentation</title><categories>cs.NI</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testbeds are essential for experimental evaluation as well as for product
development. In the context of LTE networks, existing testbed platforms are
limited either in functionality and/or extensibility or are too complex to
modify and customise. In this work we present srsLTE, an open-source platform
for LTE experimentation designed for maximum modularity and code reuse and
fully compliant with LTE Release 8. We show the potential of the srsLTE library
by extending the baseline code to allow LTE transmissions in the unlicensed
bands and coexistence with WiFi. We also expand previous results on this
emerging research area by showing how different vendor-specific mechanisms in
WiFi cards might affect coexistence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04630</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04630</id><created>2016-02-15</created><authors><author><keyname>Ghorbel</keyname><forenames>Asma</forenames></author><author><keyname>Kobayashi</keyname><forenames>Mari</forenames></author><author><keyname>Yang</keyname><forenames>Sheng</forenames></author></authors><title>Content Delivery in Erasure Broadcast Channels with Cache and Feedback</title><categories>cs.IT math.IT</categories><comments>29 pages, 7 figures. A short version has been submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a content delivery problem in a K-user erasure broadcast channel
such that a content providing server wishes to deliver requested files to
users, each equipped with a cache of a finite memory. Assuming that the
transmitter has state feedback and user caches can be filled during off-peak
hours reliably by the decentralized content placement, we characterize the
achievable rate region as a function of the memory sizes and the erasure
probabilities. The proposed delivery scheme, based on the broadcasting scheme
by Wang and Gatzianas et al., exploits the receiver side information
established during the placement phase. Our results can be extended to
centralized content placement as well as multi-antenna broadcast channels with
state feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04650</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04650</id><created>2016-02-15</created><authors><author><keyname>Metzler</keyname><forenames>Saskia</forenames></author><author><keyname>G&#xfc;nnemann</keyname><forenames>Stephan</forenames></author><author><keyname>Miettinen</keyname><forenames>Pauli</forenames></author></authors><title>Hyperbolae Are No Hyperbole: Modelling Communities That Are Not Cliques</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cliques (or quasi-cliques) are frequently used to model communities: a set of
nodes where each pair is (equally) likely to be connected. However, when
observing real-world communities, we see that most communities have more
structure than that. In particular, the nodes can be ordered in such a way that
(almost) all edges in the community lie below a hyperbola. In this paper we
present three new models for communities that capture this phenomenon. Our
models explain the structure of the communities differently, but we also prove
that they are identical in their expressive power. Our models fit to real-world
data much better than traditional block models, and allow for more in-depth
understanding of the structure of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04652</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04652</id><created>2016-02-15</created><updated>2016-02-28</updated><authors><author><keyname>Frieze</keyname><forenames>Alan</forenames></author><author><keyname>Johansson</keyname><forenames>Tony</forenames></author></authors><title>On the insertion time of random walk cuckoo hashing</title><categories>cs.DS math.CO</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that if the number of hash functions $d=O(1)$ is sufficiently large,
then the expected insertion time of Random Walk Cuckoo Hashing is $O(1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04661</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04661</id><created>2016-02-15</created><authors><author><keyname>Harada</keyname><forenames>Masaaki</forenames></author><author><keyname>Novak</keyname><forenames>Ethan</forenames></author><author><keyname>Tonchev</keyname><forenames>Vladimir D.</forenames></author></authors><title>The weight distribution of the self-dual $[128,64]$ polarity design code</title><categories>math.CO cs.IT math.IT</categories><comments>10 pages</comments><msc-class>05B05, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The weight distribution of the binary self-dual $[128,64]$ code being the
extended code $C^{*}$ of the code $C$ spanned by the incidence vectors of the
blocks of the polarity design in $PG(6,2)$ [11] is computed. It is shown also
that $R(3,7)$ and $C^{*}$ have no self-dual $[128,64,d]$ neighbor with $d \in
\{ 20, 24 \}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04667</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04667</id><created>2016-02-15</created><updated>2016-03-06</updated><authors><author><keyname>Els&#xe4;sser</keyname><forenames>Robert</forenames></author><author><keyname>Friedetzky</keyname><forenames>Tom</forenames></author><author><keyname>Kaaser</keyname><forenames>Dominik</forenames></author><author><keyname>Mallmann-Trenn</keyname><forenames>Frederik</forenames></author><author><keyname>Trinker</keyname><forenames>Horst</forenames></author></authors><title>Efficient k-Party Voting with Two Choices</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed $k$-party voting with two choices as
well as a simple modification of this protocol in complete graphs. In the
standard version, we are given a graph in which every node possesses one of $k$
different opinions at the beginning. In each step, every node chooses two
neighbors uniformly at random. If the opinions of the two neighbors coincide,
then this opinion is adopted. It is known that if $k=2$ and the difference
between the two opinions is $\Omega(\sqrt{n \log n})$, then after
$\mathcal{O}(\log n)$ steps, every node will possess the largest initial
opinion, with high probability.
  We show that if $k =\mathcal{O}(n^\epsilon)$ for some small $\epsilon$, then
this protocol converges to the initial majority within $\mathcal{O}(k+\log{n})$
steps, with high probability, as long as the initial difference between the
largest and second largest opinion is $\Omega(\sqrt{n \log n})$. Furthermore,
there exist initial configurations where the $\Theta(k)$ bound on the run time
is matched. If the initial difference is $\mathcal{O}(\sqrt{n})$, then the
largest opinion may loose the vote with constant probability. To speed up our
process, we consider the following variant of the two-choices protocol. The
process is divided into several phases, and in the first step of a phase every
node applies the two choices protocol. If a new opinion is adopted, the node
remembers it by setting a certain bit to true. In the subsequent steps of that
phase, each node samples one neighbor, and if the bit of this neighbor is set
to true, then the node takes the opinion of this neighbor and sets its bit to
true as well. At the end of the phase, the bits are reset to false. Then, the
phases are repeated several times. We show that this modified protocol improves
significantly over the standard two-choices protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04676</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04676</id><created>2016-02-15</created><authors><author><keyname>Garivier</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>IMT</affiliation></author><author><keyname>Kaufmann</keyname><forenames>Emilie</forenames><affiliation>CRIStAL, SEQUEL</affiliation></author><author><keyname>Koolen</keyname><forenames>Wouter</forenames><affiliation>CWI</affiliation></author></authors><title>Maximin Action Identification: A New Bandit Framework for Games</title><categories>math.ST cs.GT stat.ML stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an original problem of pure exploration in a strategic bandit model
motivated by Monte Carlo Tree Search. It consists in identifying the best
action in a game, when the player may sample random outcomes of sequentially
chosen pairs of actions. We propose two strategies for the fixed-confidence
setting: Maximin-LUCB, based on lower-and upper-confidence bounds; and
Maximin-Racing, which operates by successively eliminating the sub-optimal
actions. We discuss the sample complexity of both methods and compare their
performance empirically. We sketch a lower bound analysis, and possible
connections to an optimal algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04679</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04679</id><created>2016-02-15</created><authors><author><keyname>Ottaviano</keyname><forenames>Stefania</forenames></author><author><keyname>De Pellegrini</keyname><forenames>Francesco</forenames></author><author><keyname>Bonaccorsi</keyname><forenames>Stefano</forenames></author></authors><title>Heterogeneous SIS model for directed networks and optimal immunization</title><categories>math.OC cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the influence of a contact network structure over the spread
of epidemics in an heterogeneous population. Basically the epidemics spreads
over a directed weighted graph. We describe the epidemic process as a
continuous-time individual-based susceptible-infected-susceptible (SIS) model
using a first-order mean-field approximation. First we consider a network
without a specific topology, investigating the epidemic threshold and the
stability properties of the system. Then we analyze the case of a community
network, relying on the graph-theoretical notion of equitable partition, and
using a lower-dimensional dynamical system in order to individuate the epidemic
threshold. Moreover we prove that the positive steady-state of the original
system, that appears above the threshold, can be computed by this
lower-dimensional system. In the second part of the paper we treat the
important issue of the infectious disease control. Taking into account the
connectivity of the network, we provide a cost-optimal distribution of
resources to prevent the disease from persisting indefinitely in the
population; for a particular case of two-level immunization problem we report
on the construction of a polynomial time complexity algorithm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04693</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04693</id><created>2016-02-15</created><updated>2016-02-21</updated><authors><author><keyname>Alam</keyname><forenames>Shahid</forenames></author><author><keyname>Qu</keyname><forenames>Zhengyang</forenames></author><author><keyname>Riley</keyname><forenames>Ryan</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Rastogi</keyname><forenames>Vaibhav</forenames></author></authors><title>DroidNative: Semantic-Based Detection of Android Native Code Malware</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the Symantec and F-Secure threat reports, mobile malware
development in 2013 and 2014 has continued to focus almost exclusively ~99% on
the Android platform. Malware writers are applying stealthy mutations
(obfuscations) to create malware variants, thwarting detection by signature
based detectors. In addition, the plethora of more sophisticated detectors
making use of static analysis techniques to detect such variants operate only
at the bytecode level, meaning that malware embedded in native code goes
undetected. A recent study shows that 86% of the most popular Android
applications contain native code, making this a plausible threat. This paper
proposes DroidNative, an Android malware detector that uses specific control
flow patterns to reduce the effect of obfuscations, provides automation and
platform independence, and as far as we know is the first system that operates
at the Android native code level, allowing it to detect malware embedded in
both native code and bytecode. When tested with traditional malware variants it
achieves a detection rate (DR) of 99.48%, compared to academic and commercial
tools' DRs that range from 8.33% -- 93.22%. When tested with a dataset of 2240
samples DroidNative achieves a DR of 99.16%, a false positive rate of 0.4% and
an average detection time of 26.87 sec/sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04701</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04701</id><created>2016-02-15</created><authors><author><keyname>Moed</keyname><forenames>Henk F.</forenames></author></authors><title>Iran's scientific dominance and the emergence of South-East Asian
  countries in the Arab Gulf Region</title><categories>cs.DL</categories><comments>Version 15 Feb 2016 submitted to Scientometrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A longitudinal bibliometric analysis of publications indexed in Thomson
Reuters' Incites and Elsevier's Scopus, and published from the Arab Gulf States
and neighbouring countries, shows clear effects of major political events
during the past 35 years. Predictions made in 2006 by the US diplomat Richard
N. Haass on political changes in the Middle East have come true in the Gulf
States' national scientific research systems, to the extent that Iran has
become in 2015 by far the leading country in the Arab Gulf, and South-East
Asian countries including China, Malaysia and South Korea have become major
scientific collaborators, displacing the USA and other large Western countries.
But collaborations patterns among Gulf States show no apparent relationship
with differences in Islam denominations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04706</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04706</id><created>2016-02-15</created><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author><author><keyname>Lee</keyname><forenames>Sanghyuk</forenames></author><author><keyname>Lim</keyname><forenames>Eng Gee</forenames></author></authors><title>Energy-Efficient Time Synchronization Based on Asynchronous Source Clock
  Frequency Recovery and Reverse Two-Way Message Exchanges in Wireless Sensor
  Networks</title><categories>cs.NI stat.AP</categories><comments>11 pages, 8 figures, submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider energy-efficient time synchronization in a wireless sensor
network where a head node (i.e., a gateway between wired and wireless networks
and a center of data fusion) is equipped with a powerful processor and supplied
power from outlet, and sensor nodes (i.e., nodes measuring data and connected
only through wireless channels) are limited in processing and battery-powered.
It is this asymmetry that our study focuses on; unlike most existing schemes to
save the power of all network nodes, we concentrate on battery-powered sensor
nodes in minimizing energy consumption for time synchronization. We present a
time synchronization scheme based on asynchronous source clock frequency
recovery and reverse two-way message exchanges combined with measurement data
report messages, where we minimize the number of message transmissions from
sensor nodes, and carry out the performance analysis of the estimation of both
measurement time and clock frequency with lower bounds for the latter.
Simulation results verify that the proposed scheme outperforms the schemes
based on conventional two-way message exchanges with and without clock
frequency recovery in terms of the accuracy of measurement time estimation and
the number of message transmissions and receptions at sensor nodes as an
indirect measure of energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04707</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04707</id><created>2016-02-11</created><authors><author><keyname>Sinclair</keyname><forenames>David</forenames></author></authors><title>A 3D Sweep Hull Algorithm for computing Convex Hulls and Delaunay
  Triangulation</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new O(nlog(n)) algorithm for computing the convex hull
of a set of 3 dimensional points. The algorithm first sorts the point in
(x,y,z) then incrementally adds sorted points to the convex hull using the
constraint that each new point added to the hull can 'see' at least one facet
touching the last point added. The reduces the search time for adding new
points. The algorithm belongs to the family of swept hull algorithms.
  While slower than q-hull for the general case it significantly outperforms
q-hull for the pathological case where all of the points are on the 3D hull (as
is the case for Delaunay triangulation). The algorithm has been named the
'Newton Apple Wrapper algorithm' and has been released under GPL in C++.
keywords: Delaunay triangulation, 3D convex hull.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04709</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04709</id><created>2016-02-12</created><authors><author><keyname>Crocetti</keyname><forenames>Giancarlo</forenames></author><author><keyname>Delay</keyname><forenames>Amir A.</forenames></author><author><keyname>Seyedmendhi</keyname><forenames>Fatemeh</forenames></author></authors><title>Identifying Structures in Social Conversations in NSCLC Patients through
  the Semi-Automatic extraction of Topical Taxonomies</title><categories>cs.IR cs.AI cs.CL</categories><comments>7 pages, 7 figures, 1 table</comments><acm-class>H.3.1; H.3.3</acm-class><journal-ref>Journal of Engineering Research and Applications, Vol. 6, Issue 1,
  January 2016, pp.20-26</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exploration of social conversations for addressing patient's needs is an
important analytical task in which many scholarly publications are contributing
to fill the knowledge gap in this area. The main difficulty remains the
inability to turn such contributions into pragmatic processes the
pharmaceutical industry can leverage in order to generate insight from social
media data, which can be considered as one of the most challenging source of
information available today due to its sheer volume and noise. This study is
based on the work by Scott Spangler and Jeffrey Kreulen and applies it to
identify structure in social media through the extraction of a topical taxonomy
able to capture the latent knowledge in social conversations in health-related
sites. The mechanism for automatically identifying and generating a taxonomy
from social conversations is developed and pressured tested using public data
from media sites focused on the needs of cancer patients and their families.
Moreover, a novel method for generating the category's label and the
determination of an optimal number of categories is presented which extends
Scott and Jeffrey's research in a meaningful way. We assume the reader is
familiar with taxonomies, what they are and how they are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04712</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04712</id><created>2016-02-12</created><authors><author><keyname>Aguilar</keyname><forenames>Jeffrey</forenames></author><author><keyname>Zhang</keyname><forenames>Tingnan</forenames></author><author><keyname>Qian</keyname><forenames>Feifei</forenames></author><author><keyname>Kingsbury</keyname><forenames>Mark</forenames></author><author><keyname>McInroe</keyname><forenames>Benjamin</forenames></author><author><keyname>Mazouchova</keyname><forenames>Nicole</forenames></author><author><keyname>Li</keyname><forenames>Chen</forenames></author><author><keyname>Maladen</keyname><forenames>Ryan</forenames></author><author><keyname>Gong</keyname><forenames>Chaohui</forenames></author><author><keyname>Travers</keyname><forenames>Matt</forenames></author><author><keyname>Hatton</keyname><forenames>Ross L.</forenames></author><author><keyname>Choset</keyname><forenames>Howie</forenames></author><author><keyname>Umbanhowar</keyname><forenames>Paul B.</forenames></author><author><keyname>Goldman</keyname><forenames>Daniel I.</forenames></author></authors><title>A review on locomotion robophysics: the study of movement at the
  intersection of robotics, soft matter and dynamical systems</title><categories>cs.RO cond-mat.soft physics.bio-ph</categories><comments>61 pages, 18 figures, IOPScience journal: Reports on Progress in
  Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this review we argue for the creation of a physics of moving systems -- a
locomotion &quot;robophysics&quot; -- which we define as the pursuit of the discovery of
principles of self generated motion. Robophysics can provide an important
intellectual complement to the discipline of robotics, largely the domain of
researchers from engineering and computer science. The essential idea is that
we must complement study of complex robots in complex situations with
systematic study of simplified robophysical devices in controlled laboratory
settings and simplified theoretical models. We must thus use the methods of
physics to examine successful and failed locomotion in simplified (abstracted)
devices using parameter space exploration, systematic control, and techniques
from dynamical systems. Using examples from our and other's research, we will
discuss how such robophysical studies have begun to aid engineers in the
creation of devices that begin to achieve life-like locomotor abilities on and
within complex environments, have inspired interesting physics questions in low
dimensional dynamical systems, geometric mechanics and soft matter physics, and
have been useful to develop models for biological locomotion in complex
terrain. The rapidly decreasing cost of constructing sophisticated robot models
with easy access to significant computational power bodes well for scientists
and engineers to engage in a discipline which can readily integrate experiment,
theory and computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04713</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04713</id><created>2016-02-11</created><authors><author><keyname>Basnet</keyname><forenames>Subarna</forenames></author><author><keyname>Magee</keyname><forenames>Christopher L.</forenames></author></authors><title>Modeling of technological performance trends using design theory</title><categories>cs.CY</categories><comments>43 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional technical performance usually follows an exponential dependence on
time but the rate of change (the exponent) varies greatly among technological
domains. This paper presents a simple model that provides an explanatory
foundation for these phenomena based upon the inventive design process.
  The model assumes that invention - novel and useful design- arises through
probabilistic analogical transfers that combine existing knowledge by combining
existing individual operational ideas to arrive at new individual operating
ideas. The continuing production of individual operating ideas relies upon
injection of new basic individual operating ideas that occurs through coupling
of science and technology simulations.
  The individual operational ideas that result from this process are then
modeled as being assimilated in components of artifacts characteristic of a
technological domain. According to the model, two effects (differences in
interactions among components for different domains and differences in scaling
laws for different domains) account for the differences found in improvement
rates among domains whereas the analogical transfer process is the source of
the exponential behavior. The model is supported by a number of known empirical
facts: further empirical research is suggested to independently assess further
predictions made by the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04716</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04716</id><created>2016-02-15</created><authors><author><keyname>Xu</keyname><forenames>Shixiong</forenames></author><author><keyname>Gregg</keyname><forenames>David</forenames></author></authors><title>Customizable Precision of Floating-Point Arithmetic with Bitslice Vector
  Types</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Customizing the precision of data can provide attractive trade-offs between
accuracy and hardware resources. We propose a novel form of vector computing
aimed at arrays of custom-precision floating point data. We represent these
vectors in bitslice format. Bitwise instructions are used to implement
arithmetic circuits in software that operate on customized bit-precision.
Experiments show that this approach can be efficient for vectors of
low-precision custom floating point types, while providing arbitrary bit
precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04723</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04723</id><created>2016-02-15</created><authors><author><keyname>Basri</keyname><forenames>Ronen</forenames></author><author><keyname>Jacobs</keyname><forenames>David</forenames></author></authors><title>Efficient Representation of Low-Dimensional Manifolds using Deep
  Networks</title><categories>cs.NE cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the ability of deep neural networks to represent data that lies
near a low-dimensional manifold in a high-dimensional space. We show that deep
networks can efficiently extract the intrinsic, low-dimensional coordinates of
such data. We first show that the first two layers of a deep network can
exactly embed points lying on a monotonic chain, a special type of piecewise
linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,
the network can do this using an almost optimal number of parameters. We also
show that this network projects nearby points onto the manifold and then embeds
them with little error. We then extend these results to more general manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04727</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04727</id><created>2016-02-15</created><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author></authors><title>Tangles and Connectivity in Graphs</title><categories>cs.DM math.CO</categories><comments>Preliminary version of a paper that will appear in Proceedings of the
  10th International Conference on Language and Automata Theory and
  Applications, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a short introduction to the theory of tangles, both in graphs
and general connectivity systems. An emphasis is put on the correspondence
between tangles of order k and k-connected components. In particular, we prove
that there is a one-to-one correspondence between the triconnected components
of a graph and its tangles of order 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04732</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04732</id><created>2016-02-15</created><updated>2016-02-26</updated><authors><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames></author><author><keyname>Yakary&#x131;lmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Affine computation and affine automaton</title><categories>cs.FL cs.CC quant-ph</categories><comments>23 pages. New results were added. Accepted to CSR2016!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a quantum-like classical computational model, called affine
computation, as a generalization of probabilistic computation. After giving the
basics of affine computation, we define affine finite automata (AfA) and
compare it with quantum and probabilistic finite automata (QFA and PFA,
respectively) with respect to three basic language recognition modes. We show
that, in the cases of bounded and unbounded error, AfAs are more powerful than
QFAs and PFAs, and, in the case of nondeterministic computation, AfAs are more
powerful than PFAs but equivalent to QFAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04741</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04741</id><created>2016-02-15</created><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicolo'</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Minora</keyname><forenames>Alberto</forenames></author></authors><title>Delay and Cooperation in Nonstochastic Bandits</title><categories>cs.LG</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study networks of communicating learning agents that cooperate to solve a
common nonstochastic bandit problem. Agents use an underlying communication
network to get messages about actions selected by other agents, and drop
messages that took more than $d$ hops to arrive, where $d$ is a delay
parameter. We introduce \textsc{Exp3-Coop}, a cooperative version of the {\sc
Exp3} algorithm and prove that with $K$ actions and $N$ agents the average
per-agent regret after $T$ rounds is at most of order $\sqrt{\bigl(d+1 +
\tfrac{K}{N}\alpha_{\le d}\bigr)(T\ln K)}$, where $\alpha_{\le d}$ is the
independence number of the $d$-th power of the connected communication graph
$G$. We then show that for any connected graph, for $d=\sqrt{K}$ the regret
bound is $K^{1/4}\sqrt{T}$, strictly better than the minimax regret $\sqrt{KT}$
for noncooperating agents. More informed choices of $d$ lead to bounds which
are arbitrarily close to the full information minimax regret $\sqrt{T\ln K}$
when $G$ is dense. When $G$ has sparse components, we show that a variant of
\textsc{Exp3-Coop}, allowing agents to choose their parameters according to
their centrality in $G$, strictly improves the regret. Finally, as a by-product
of our analysis, we provide the first characterization of the minimax regret
for bandit learning with delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04742</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04742</id><created>2016-02-15</created><authors><author><keyname>Sinyavskiy</keyname><forenames>Oleg Y.</forenames></author></authors><title>Training of spiking neural networks based on information theoretic costs</title><categories>cs.NE q-bio.NC</categories><comments>A doctoral thesis, 111 pages, 55 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking neural network is a type of artificial neural network in which
neurons communicate between each other with spikes. Spikes are identical
Boolean events characterized by the time of their arrival. A spiking neuron has
internal dynamics and responds to the history of inputs as opposed to the
current inputs only. Because of such properties a spiking neural network has
rich intrinsic capabilities to process spatiotemporal data. However, because
the spikes are discontinuous 'yes or no' events, it is not trivial to apply
traditional training procedures such as gradient descend to the spiking
neurons. In this thesis we propose to use stochastic spiking neuron models in
which probability of a spiking output is a continuous function of parameters.
We formulate several learning tasks as minimization of certain
information-theoretic cost functions that use spiking output probability
distributions. We develop a generalized description of the stochastic spiking
neuron and a new spiking neuron model that allows to flexibly process rich
spatiotemporal data. We formulate and derive learning rules for the following
tasks:
  - a supervised learning task of detecting a spatiotemporal pattern as a
minimization of the negative log-likelihood (the surprisal) of the neuron's
output
  - an unsupervised learning task of increasing the stability of neurons output
as a minimization of the entropy
  - a reinforcement learning task of controlling an agent as a modulated
optimization of filtered surprisal of the neuron's output.
  We test the derived learning rules in several experiments such as
spatiotemporal pattern detection, spatiotemporal data storing and recall with
autoassociative memory, combination of supervised and unsupervised learning to
speed up the learning process, adaptive control of simple virtual agents in
changing environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04744</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04744</id><created>2016-02-15</created><authors><author><keyname>Backens</keyname><forenames>Miriam</forenames></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames></author><author><keyname>Wang</keyname><forenames>Quanlong</forenames></author></authors><title>A Simplified Stabilizer ZX-calculus</title><categories>quant-ph cs.LO</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stabilizer ZX-calculus is a rigorous graphical language for reasoning
about stabilizer quantum mechanics. This language has been proved to be
complete in two steps: first in a setting where scalars (diagrams with no
inputs or outputs) are ignored and then in a more general setting where a new
symbol and three additional rules have been added to keep track of scalars.
Here, we introduce a simplified version of the stabilizer ZX-calculus: we give
a smaller set of axioms and prove that meta-rules like `only the topology
matters', `colour symmetry' and `upside-down symmetry', which were considered
as axioms in previous versions of the stabilizer ZX-calculus, can in fact be
derived. In particular, we show that the additional symbol and one of the rules
introduced for proving the completeness of the scalar stabilizer ZX-calculus
are not necessary. We furthermore show that the remaining two rules dedicated
to scalars cannot be derived from the other rules, i.e. they are necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04747</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04747</id><created>2016-02-15</created><authors><author><keyname>Hassoun</keyname><forenames>Youssef</forenames></author></authors><title>Secure symmetric ciphers over the real field</title><categories>cs.CR</categories><comments>Extension of a paper published in NUMAN2014 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most cryptosystems are defined over finite algebraic structures where
arithmetic operations are performed modulo natural numbers. This applies to
private key as well as to public key ciphers. No secure cryptosystems defined
over the field of real numbers are known. In this work, we demonstrate the
feasibility of constructing secure symmetric key ciphers defined over the field
of real numbers. We consider the security of ciphers introduced in a previous
work and based on solving linear and non-linear equations numerically. We
complement the design of those ciphers to satisfy the requirements of secure
systems and, consequently, extend them into composite ciphers with multiple
encryptions. We show security enhancements by estimating the uncertainty in
finding the keys using a measure based on Shannon's entropy function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04754</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04754</id><created>2016-02-15</created><authors><author><keyname>Paxton</keyname><forenames>Chris</forenames></author><author><keyname>Kobilarov</keyname><forenames>Marin</forenames></author><author><keyname>Hager</keyname><forenames>Gregory D.</forenames></author></authors><title>Towards Robot Task Planning From Probabilistic Models of Human Skills</title><categories>cs.RO</categories><comments>8 pages, presented at AAAI 2016 PlanHS workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm for motion planning based on expert demonstrations
of a skill. In order to teach robots to perform complex object manipulation
tasks that can generalize robustly to new environments, we must (1) learn a
representation of the effects of a task and (2) find an optimal trajectory that
will reproduce these effects in a new environment. We represent robot skills in
terms of a probability distribution over features learned from multiple expert
demonstrations. When utilizing a skill in a new environment, we compute feature
expectations over trajectory samples in order to stochastically optimize the
likelihood of a trajectory in the new environment. The purpose of this method
is to enable execution of complex tasks based on a library of probabilistic
skill models. Motions can be combined to accomplish complex tasks in hybrid
domains. Our approach is validated in a variety of case studies, including an
Android game, simulated assembly task, and real robot experiment with a UR5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04762</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04762</id><created>2016-02-15</created><updated>2016-02-18</updated><authors><author><keyname>Sunberg</keyname><forenames>Zachary N.</forenames></author><author><keyname>Kochenderfer</keyname><forenames>Mykel J.</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Optimized and Trusted Collision Avoidance for Unmanned Aerial Vehicles
  using Approximate Dynamic Programming (Technical Report)</title><categories>cs.RO</categories><comments>An abbreviated version was submitted to ICRA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safely integrating unmanned aerial vehicles into civil airspace is contingent
upon development of a trustworthy collision avoidance system. This paper
proposes an approach whereby a parameterized resolution logic that is
considered trusted for a given range of its parameters is adaptively tuned
online. Specifically, to address the potential conservatism of the resolution
logic with static parameters, we present a dynamic programming approach for
adapting the parameters dynamically based on the encounter state. We compute
the adaptation policy offline using a simulation-based approximate dynamic
programming method that accommodates the high dimensionality of the problem.
Numerical experiments show that this approach improves safety and operational
performance compared to the baseline resolution logic, while retaining
trustworthiness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04781</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04781</id><created>2016-02-15</created><authors><author><keyname>Hauptmann</keyname><forenames>Mathias</forenames></author></authors><title>On Alternation and the Union Theorem</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under the assumption $P=\Sigma_2^p$, we prove a new variant of the Union
Theorem of McCreight and Meyer for the class $\Sigma_2^p$. This yields a union
function $F$ which is computable in time $F(n)^c$ for some constant $c$ and
satisfies $P=DTIME(F)=\Sigma_2(F)=\Sigma_2^p$ with respect to a subfamily
$(\tilde{S}_i)$ of $\Sigma_2$-machines. We show that this subfamily does not
change the complexity classes $P$ and $\Sigma_2^p$. Moreover, a padding
construction shows that this also implies $DTIME(F^c)=\Sigma_2(F^c)$. On the
other hand, we prove a variant of Gupta's result who showed that
$DTIME(t)\subsetneq\Sigma_2(t)$ for time-constructible functions $t(n)$. Our
variant of this result holds with respect to the subfamily $(\tilde{S}_i)$ of
$\Sigma_2$-machines. We show that these two results contradict each other.
Hence the assumption $P=\Sigma_2^p$ cannot hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04792</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04792</id><created>2016-02-15</created><updated>2016-02-16</updated><authors><author><keyname>Rastegari</keyname><forenames>Baharak</forenames></author><author><keyname>Goldberg</keyname><forenames>Paul</forenames></author><author><keyname>Manlove</keyname><forenames>David</forenames></author></authors><title>Preference Elicitation in Matching Markets via Interviews: A Study of
  Offline Benchmarks</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stable marriage problem and its extensions have been extensively studied,
with much of the work in the literature assuming that agents fully know their
own preferences over alternatives. This assumption however is not always
practical (especially in large markets) and agents usually need to go through
some costly deliberation process in order to learn their preferences. In this
paper we assume that such deliberations are carried out via interviews, where
an interview involves a man and a woman, each of whom learns information about
the other as a consequence. If everybody interviews everyone else, then clearly
agents can fully learn their preferences. But interviews are costly, and we may
wish to minimize their use. It is often the case, especially in practical
settings, that due to correlation between agents' preferences, it is
unnecessary for all potential interviews to be carried out in order to obtain a
stable matching. Thus the problem is to find a good strategy for interviews to
be carried out in order to minimize their use, whilst leading to a stable
matching. One way to evaluate the performance of an interview strategy is to
compare it against a naive algorithm that conducts all interviews. We argue
however that a more meaningful comparison would be against an optimal offline
algorithm that has access to agents' preference orderings under complete
information. We show that, unless P=NP, no offline algorithm can compute the
optimal interview strategy in polynomial time. If we are additionally aiming
for a particular stable matching (perhaps one with certain desirable
properties), we provide restricted settings under which efficient optimal
offline algorithms exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04799</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04799</id><created>2016-02-15</created><authors><author><keyname>Wiebe</keyname><forenames>Nathan</forenames></author><author><keyname>Kapoor</keyname><forenames>Ashish</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M</forenames></author></authors><title>Quantum Perceptron Models</title><categories>quant-ph cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate how quantum computation can provide non-trivial improvements
in the computational and statistical complexity of the perceptron model. We
develop two quantum algorithms for perceptron learning. The first algorithm
exploits quantum information processing to determine a separating hyperplane
using a number of steps sublinear in the number of data points $N$, namely
$O(\sqrt{N})$. The second algorithm illustrates how the classical mistake bound
of $O(\frac{1}{\gamma^2})$ can be further improved to
$O(\frac{1}{\sqrt{\gamma}})$ through quantum means, where $\gamma$ denotes the
margin. Such improvements are achieved through the application of quantum
amplitude amplification to the version space interpretation of the perceptron
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04800</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04800</id><created>2016-02-15</created><authors><author><keyname>Hauer</keyname><forenames>Florian</forenames></author><author><keyname>Tsiotras</keyname><forenames>Panagiotis</forenames></author></authors><title>Reduced Complexity Multi-Scale Path-Planning on Probabilistic Maps</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several modifications to the previously proposed MSPP algorithm
that can speed-up its execution considerably. The MSPP algorithm leverages a
multiscale representation of the environment in $n$ dimensions. The information
of the environment is stored in a tree data structure representing a recursive
dyadic partitioning of the search space. The information used by the algorithm
is the probability that a node in the tree corresponds to an obstacle in the
search space. Such trees are often created from mainstream perception
algorithms, and correspond to quadtrees and octrees in two and three
dimensions, respectively. We first present a new method to compute the graph
neighbors in order to reduce the complexity of each iteration, from $O(| V|^2)$
to $O(| V| \log |V|)$. We then show how to delay expensive intermediate
computations until we know that new information will be required, hence saving
time by not operating on information that is never used during the search.
Finally, we present a way to remove the very expensive need to calculate a full
multi-scale map with the use of sampling and derive an theoretical upperbound
of the probability of failure as a function of the number of samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04805</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04805</id><created>2016-02-15</created><authors><author><keyname>Mitrovic</keyname><forenames>Jovana</forenames></author><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution
  Regression</title><categories>stat.ML cs.LG stat.CO stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing exact posterior inference in complex generative models is often
difficult or impossible due to an expensive to evaluate or intractable
likelihood function. Approximate Bayesian computation (ABC) is an inference
framework that constructs an approximation to the true likelihood based on the
similarity between the observed and simulated data as measured by a predefined
set of summary statistics. Although the choice of appropriate problem-specific
summary statistics crucially influences the quality of the likelihood
approximation and hence also the quality of the posterior sample in ABC, there
are only few principled general-purpose approaches to the selection or
construction of such summary statistics. In this paper, we develop a novel
framework for this task using kernel-based distribution regression. We model
the functional relationship between data distributions and the optimal choice
(with respect to a loss function) of summary statistics using kernel-based
distribution regression. We show that our approach can be implemented in a
computationally and statistically efficient way using the random Fourier
features framework for large-scale kernel learning. In addition to that, our
framework shows superior performance when compared to related methods on toy
and real-world problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04806</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04806</id><created>2016-02-13</created><authors><author><keyname>Dongale</keyname><forenames>T. D.</forenames></author><author><keyname>Gaikwad</keyname><forenames>P. K.</forenames></author><author><keyname>Kamat</keyname><forenames>R. K.</forenames></author></authors><title>State Space Analysis of Memristor Based Series and Parallel RLCM
  Circuits</title><categories>cs.ET nlin.CD</categories><comments>9 pages, 4 figures and 1 table</comments><msc-class>94C05</msc-class><acm-class>J.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The present paper investigates state space analysis of memristor based series
and parallel RLCM circuits. The stability analysis is carried out with the help
of eigenvalues formulation method, pole-zero plot and transient response of
system. The state space analysis is successfully applied and eigenvalues of the
two circuits are calculated. It is found that the, system follows negative real
part of eigenvalues. The result clearly shows that addition of memristor in
circuits will not alter the stability of system. It is found that systems poles
located at left hand side of the S plane, which indicates stable performance of
system. It clearly evident that eigenvalues has negative real part hence two
systems are internally stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04835</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04835</id><created>2016-02-15</created><updated>2016-02-23</updated><authors><author><keyname>Reyes</keyname><forenames>Matthew G.</forenames></author><author><keyname>Neuhoff</keyname><forenames>David L.</forenames></author></authors><title>Cutset Width and Spacing for Reduced Cutset Coding of Markov Random
  Fields</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore tradeoffs, regarding coding performance, between the
thickness and spacing of the cutset used in Reduced Cutset Coding (RCC) of a
Markov random field image model. Considering MRF models on a square lattice of
sites, we show that under a stationarity condition, increasing the thickness of
the cutset reduces coding rate for the cutset, increasing the spacing between
components of the cutset increases the coding rate of the non-cutset pixels,
though the coding rate of the latter is always strictly less than that of the
former. We show that the redundancy of RCC can be decomposed into two terms, a
correlation redundancy due to coding the components of the cutset
independently, and a distribution redundancy due to coding the cutset as a
reduced MRF. We provide analysis of these two sources of redundancy. We present
results from numerical simulations with a homogeneous Ising model that bear out
the analytical results. We also present a consistent estimation algorithm for
the moment-matching reduced MRF on the cutset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04841</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04841</id><created>2016-02-15</created><authors><author><keyname>Manero</keyname><forenames>R. B. Ribas</forenames></author><author><keyname>Grewal</keyname><forenames>J.</forenames></author><author><keyname>Michael</keyname><forenames>B.</forenames></author><author><keyname>Shafti</keyname><forenames>A.</forenames></author><author><keyname>Althoefer</keyname><forenames>K.</forenames></author><author><keyname>Fernandez</keyname><forenames>J. Ll. Ribas</forenames></author><author><keyname>Howard</keyname><forenames>M. J.</forenames></author></authors><title>Wearable Embroidered Muscle Activity Sensing Device for the Human Upper
  Leg</title><categories>cs.HC</categories><comments>Preprint submitted to IEEE-EMBC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the last decade, running has become one of the most popular physical
activities in the world. Although the benefits of running are numerous, there
is a risk of Running Related Injuries (RRI) of the lower extremities.
Electromyography (EMG) techniques have previously been used to study causes of
RRIs, but the complexity of this technology limits its use to a laboratory
setting. As running is primarily an outdoors activity, this lack of technology
acts as a barrier to the study of RRIs in natural environments. This study
presents a minimally invasive wearable muscle sensing device consisting of
jogging leggings with embroidered surface EMG (sEMG) electrodes capable of
recording muscle activity data of the quadriceps group. To test the use of the
device, a proof of concept study consisting of $N=2$ runners performing a set
of $5km$ running trials is presented in which the effect of running surfaces on
muscle fatigue, a potential cause of RRIs, is evaluated. Results show that
muscle fatigue can be analysed from the sEMG data obtained through the wearable
device, and that running on soft surfaces (such as sand) may increase the
likelihood of suffering from RRIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04844</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04844</id><created>2016-02-15</created><updated>2016-02-22</updated><authors><author><keyname>Manzoor</keyname><forenames>Emaad A.</forenames></author><author><keyname>Momeni</keyname><forenames>Sadegh</forenames></author><author><keyname>Venkatakrishnan</keyname><forenames>Venkat N.</forenames></author><author><keyname>Akoglu</keyname><forenames>Leman</forenames></author></authors><title>Fast Memory-efficient Anomaly Detection in Streaming Heterogeneous
  Graphs</title><categories>cs.SI</categories><comments>10 pages, 2 tables, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a stream of heterogeneous graphs containing different types of nodes
and edges, how can we spot anomalous ones in real-time while consuming bounded
memory? This problem is motivated by and generalizes from its application in
security to host-level advanced persistent threat (APT) detection. We propose
StreamSpot, a clustering based anomaly detection approach that addresses
challenges in two key fronts: (1) heterogeneity, and (2) streaming nature. We
introduce a new similarity function for heterogeneous graphs that compares two
graphs based on their relative frequency of local substructures, represented as
short strings. This function lends itself to a vector representation of a
graph, which is (a) fast to compute, and (b) amenable to a sketched version
with bounded size that preserves similarity. StreamSpot exhibits desirable
properties that a streaming application requires---it is (i) fully-streaming;
processing the stream one edge at a time as it arrives, (ii) memory-efficient;
requiring constant space for the sketches and the clustering, (iii) fast;
taking constant time to update the graph sketches and the cluster summaries
that can process over 100K edges per second, and (iv) online; scoring and
flagging anomalies in real time. Experiments on datasets containing simulated
system-call flow graphs from normal browser activity and various attack
scenarios (ground truth) show that our proposed StreamSpot is high-performance;
achieving above 95% detection accuracy with small delay, as well as competitive
time and memory usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04845</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04845</id><created>2016-02-15</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Maxwell</keyname><forenames>Gregory</forenames></author><author><keyname>Terriberry</keyname><forenames>Timothy B.</forenames></author><author><keyname>Vos</keyname><forenames>Koen</forenames></author></authors><title>High-Quality, Low-Delay Music Coding in the Opus Codec</title><categories>cs.MM cs.SD</categories><comments>10 pages, 135th AES Convention. Proceedings of the 135th AES
  Convention, October 2013</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The IETF recently standardized the Opus codec as RFC6716. Opus targets a wide
range of real-time Internet applications by combining a linear prediction coder
with a transform coder. We describe the transform coder, with particular
attention to the psychoacoustic knowledge built into the format. The result
out-performs existing audio codecs that do not operate under real-time
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04847</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04847</id><created>2016-02-15</created><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Lee</keyname><forenames>Yin-Tat</forenames></author></authors><title>Black-box optimization with a politician</title><categories>math.OC cs.DS cs.LG cs.NA</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new framework for black-box convex optimization which is
well-suited for situations where gradient computations are expensive. We derive
a new method for this framework which leverages several concepts from convex
optimization, from standard first-order methods (e.g. gradient descent or
quasi-Newton methods) to analytical centers (i.e. minimizers of self-concordant
barriers). We demonstrate empirically that our new technique compares favorably
with state of the art algorithms (such as BFGS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04851</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04851</id><created>2016-02-15</created><authors><author><keyname>Fournier</keyname><forenames>Ga&#xeb;tan</forenames></author></authors><title>General distribution of consumers in pure Hotelling games</title><categories>math.OC cs.GT</categories><comments>26 pages,5 figures</comments><msc-class>91A06, 91A43</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pure Hotelling game is a competition between a finite number of players who
select simultaneously a location in order to attract as many consumers as
possible. In this paper, we study the case of a general distribution of
consumers on a network generated by a metric graph. Because players do not
compete on price, the continuum of consumers shop at the closest player's
location. Under regularity hypothesis on the distribution we prove the
existence of an epsilon-equilibrium in pure strategies and we construct it,
provided that the number of players is larger than a lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04853</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04853</id><created>2016-02-04</created><authors><author><keyname>Holovatch</keyname><forenames>Yurij</forenames></author><author><keyname>Palchykov</keyname><forenames>Vasyl</forenames></author></authors><title>Complex Networks of Words in Fables</title><categories>physics.soc-ph cs.CL</categories><comments>16 pages, 4 figures and 2 tables. To appear in: &quot;Maths Meets Myths:
  Complexity-science approaches to folktales, myths, sagas, and histories.&quot;
  Editors: R. Kenna, M. Mac Carron, P. Mac Carron. (Springer, 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter we give an overview of the application of complex network
theory to quantify some properties of language. Our study is based on two
fables in Ukrainian, Mykyta the Fox and Abu-Kasym's slippers. It consists of
two parts: the analysis of frequency-rank distributions of words and the
application of complex-network theory. The first part shows that the text sizes
are sufficiently large to observe statistical properties. This supports their
selection for the analysis of typical properties of the language networks in
the second part of the chapter. In describing language as a complex network,
while words are usually associated with nodes, there is more variability in the
choice of links and different representations result in different networks.
Here, we examine a number of such representations of the language network and
perform a comparative analysis of their characteristics. Our results suggest
that, irrespective of link representation, the Ukrainian language network used
in the selected fables is a strongly correlated, scale-free, small world. We
discuss how such empirical approaches may help form a useful basis for a
theoretical description of language evolution and how they may be used in
analyses of other textual narratives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04854</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04854</id><created>2016-02-15</created><authors><author><keyname>Mahdizadehaghdam</keyname><forenames>Shahin</forenames></author><author><keyname>Wang</keyname><forenames>Han</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author><author><keyname>Dai</keyname><forenames>Liyi</forenames></author></authors><title>Information Diffusion of Topic Propagation in Social Media</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world social and/or operational networks consist of agents with
associated states, whose connectivity forms complex topologies. This complexity
is further compounded by interconnected information layers, consisting, for
instance, documents/resources of the agents which mutually share topical
similarities. Our goal in this work is to predict the specific states of the
agents, as their observed resources evolve in time and get updated. The
information diffusion among the agents and the publications themselves
effectively result in a dynamic process which we capture by an interconnected
system of networks (i.e. layered). More specifically, we use a notion of a
supra-Laplacian matrix to address such a generalized diffusion of an
interconnected network starting with the classical &quot;graph Laplacian&quot;. The
auxiliary and external input update is modeled by a multidimensional Brownian
process, yielding two contributions to the variations in the states of the
agents: one that is due to the intrinsic interactions in the network system,
and the other due to the external inputs or innovations. A variation on this
theme, a priori knowledge of a fraction of the agents' states is shown to lead
to a Kalman predictor problem. This helps us refine the predicted states
exploiting the error in estimating the states of agents. Three real-world
datasets are used to evaluate and validate the information diffusion process in
this novel layered network approach. Our results demonstrate a lower prediction
error when using the interconnected network rather than the single connectivity
layer between the agents. The prediction error is further improved by using the
estimated diffusion connection and by applying the Kalman approach with partial
observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04860</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04860</id><created>2016-02-15</created><authors><author><keyname>Kavvos</keyname><forenames>G. A.</forenames></author></authors><title>System K: a simple modal {\lambda}-calculus</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce System K, a modal {\lambda}-calculus, which - under the
Curry-Howard isomorphism - corresponds to a constructive variant of the logic
K, the weakest normal modal logic. We investigate its metatheory and its
categorical semantics. Finally, we propound some ideas regarding its
computational interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04868</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04868</id><created>2016-02-15</created><authors><author><keyname>Sarkar</keyname><forenames>Sayantan</forenames></author><author><keyname>Patel</keyname><forenames>Vishal M.</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Deep Feature-based Face Detection on Mobile Devices</title><categories>cs.CV</categories><comments>ISBA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a deep feature-based face detector for mobile devices to detect
user's face acquired by the front facing camera. The proposed method is able to
detect faces in images containing extreme pose and illumination variations as
well as partial faces. The main challenge in developing deep feature-based
algorithms for mobile devices is the constrained nature of the mobile platform
and the non-availability of CUDA enabled GPUs on such devices. Our
implementation takes into account the special nature of the images captured by
the front-facing camera of mobile devices and exploits the GPUs present in
mobile devices without CUDA-based frameorks, to meet these challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04873</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04873</id><created>2016-02-15</created><authors><author><keyname>Morgan</keyname><forenames>Hannah</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author><author><keyname>Sanan</keyname><forenames>Patrick</forenames></author><author><keyname>Scott</keyname><forenames>L. Ridgway</forenames></author></authors><title>A Stochastic Performance Model for Pipelined Krylov Methods</title><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pipelined Krylov methods seek to ameliorate the latency due to inner products
necessary for projection by overlapping it with the computation associated with
sparse matrix-vector multiplication. We clarify a folk theorem that this can
only result in a speedup of $2\times$ over the naive implementation. Examining
many repeated runs, we show that stochastic noise also contributes to the
latency, and we model this using an analytical probability distribution. Our
analysis shows that speedups greater than $2\times$ are possible with these
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04874</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04874</id><created>2016-02-15</created><authors><author><keyname>Yao</keyname><forenames>Yushi</forenames></author><author><keyname>Huang</keyname><forenames>Zheng</forenames></author></authors><title>Bi-directional LSTM Recurrent Neural Network for Chinese Word
  Segmentation</title><categories>cs.LG cs.CL</categories><comments>2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural network(RNN) has been broadly applied to natural language
processing(NLP) problems. This kind of neural network is designed for modeling
sequential data and has been testified to be quite efficient in sequential
tagging tasks. In this paper, we propose to use bi-directional RNN with long
short-term memory(LSTM) units for Chinese word segmentation, which is a crucial
preprocess task for modeling Chinese sentences and articles. Classical methods
focus on designing and combining hand-craft features from context, whereas
bi-directional LSTM network(BLSTM) does not need any prior knowledge or
pre-designing, and it is expert in keeping the contextual information in both
directions. Experiment result shows that our approach gets state-of-the-art
performance in word segmentation on both traditional Chinese datasets and
simplified Chinese datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04875</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04875</id><created>2016-02-15</created><updated>2016-02-23</updated><authors><author><keyname>Chen</keyname><forenames>Min</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author><author><keyname>Hsu</keyname><forenames>David</forenames></author><author><keyname>Lee</keyname><forenames>Wee Sun</forenames></author></authors><title>POMDP-lite for Robust Robot Planning under Uncertainty</title><categories>cs.AI</categories><comments>In Proc. IEEE International Conference on Robotics &amp; Automation
  (ICRA) 2016, with supplementary materials</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The partially observable Markov decision process (POMDP) provides a
principled general model for planning under uncertainty. However, solving a
general POMDP is computationally intractable in the worst case. This paper
introduces POMDP-lite, a subclass of POMDPs in which the hidden state variables
are constant or only change deterministically. We show that a POMDP-lite is
equivalent to a set of fully observable Markov decision processes indexed by a
hidden parameter and is useful for modeling a variety of interesting robotic
tasks. We develop a simple model-based Bayesian reinforcement learning
algorithm to solve POMDP-lite models. The algorithm performs well on
large-scale POMDP-lite models with up to $10^{20}$ states and outperforms the
state-of-the-art general-purpose POMDP algorithms. We further show that the
algorithm is near-Bayesian-optimal under suitable conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04876</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04876</id><created>2016-02-15</created><authors><author><keyname>Brand&#xe3;o</keyname><forenames>Filipe</forenames></author></authors><title>VPSolver 3: Multiple-choice Vector Packing Solver</title><categories>math.OC cs.DS</categories><comments>8 pages. arXiv admin note: text overlap with arXiv:1310.6887</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VPSolver is a vector packing solver based on an arc-flow formulation with
graph compression. In this paper, we present the algorithm introduced in
VPSolver 3.0.0 for building compressed arc-flow models for the multiple-choice
vector packing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04877</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04877</id><created>2016-02-15</created><authors><author><keyname>Wei</keyname><forenames>Peng</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Xiao</keyname><forenames>Yue</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>Fast DGT Based Receivers for GFDM in Broadband Channels</title><categories>cs.IT math.IT</categories><comments>28 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized frequency division multiplexing (GFDM) is a recent multicarrier
5G waveform candidate with flexibility of pulse shaping filters. However, the
flexibility of choosing a pulse shaping filter may result in inter carrier
interference (ICI) and inter symbol interference (ISI), which becomes more
severe in a broadband channel. In order to eliminate the ISI and ICI, based on
discrete Gabor transform (DGT), in this paper, a transmit GFDM signal is first
treated as an inverse DGT (IDGT), and then a frequency-domain DGT is formulated
to recover (as a receiver) the GFDM signal. Furthermore, to reduce the
complexity, a suboptimal frequency-domain DGT called local DGT (LDGT) is
developed. Some analyses are also given for the proposed DGT based receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04878</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04878</id><created>2016-02-15</created><authors><author><keyname>Davis</keyname><forenames>Clayton A</forenames></author><author><keyname>Heiman</keyname><forenames>Julia</forenames></author><author><keyname>Janssen</keyname><forenames>Erick</forenames></author><author><keyname>Sanders</keyname><forenames>Stephanie</forenames></author><author><keyname>Garcia</keyname><forenames>Justin</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Kinsey Reporter: Citizen Science for Sex Research</title><categories>cs.CY</categories><comments>Let's Talk About Sex (Apps) Workshop at CSCW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kinsey Reporter is a global mobile app to share, explore, and visualize
anonymous data about sex. Reports are submitted via smartphone, then visualized
on a website or downloaded for offline analysis. In this paper we present the
major features of the Kinsey Reporter citizen science platform designed to
preserve the anonymity of its contributors, and preliminary data analyses that
suggest questions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04881</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04881</id><created>2016-02-15</created><updated>2016-02-18</updated><authors><author><keyname>Flocchini</keyname><forenames>Paola</forenames></author><author><keyname>Santoro</keyname><forenames>Nicola</forenames></author><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author><author><keyname>Yamashita</keyname><forenames>Masafumi</forenames></author></authors><title>Implicit Function Computation by Oblivious Mobile Robots</title><categories>cs.DC cs.DM math.CO</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An oblivious mobile robot is a stateless computational entity located in a
spatial universe, capable of moving in that universe. When activated, the robot
observes the universe and the location of the other robots, chooses a
destination, and moves there. The computation of the destination is made by
executing an algorithm, the same for all robots, whose sole input is the
current observation. No memory of all these actions is retained after the move.
When the universe is a graph, distributed computations by oblivious mobile
robots have been intensively studied focusing on the conditions for feasibility
of basic problems (e.g., gathering, exploration) in specific classes of graphs
under different schedulers. In this paper, we embark on a different, more
general, type of investigation.
  With their movements from vertices to neighboring vertices, the robots make
the system transition from one configuration to another. Viewing this
transition as the computation of an abstract function, we ask which functions
are computed by which systems. Our notion of computation is defined via an
ad-hoc relation between graphs: the &quot;pseudo-minor&quot; relation.
  Our main interest is on identifying sets of systems that are &quot;universal&quot;, in
the sense that they can collectively compute all finite functions. We are able
to identify several such classes of fully synchronous systems. In particular,
among other results, we prove the universality of the family of arbitrary
graphs with at least one robot, of graphs with at least two robots whose
quotient graphs contain arbitrarily long paths, and of graphs with at least
three robots and arbitrarily large finite girths.
  We then focus on the minimum size that a network must have for the robots to
be able to compute all functions on a given finite set. We are able to
approximate the minimum size of such a network up to a factor that tends to 2
as $n$ goes to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04886</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04886</id><created>2016-02-15</created><authors><author><keyname>Jaegle</keyname><forenames>Andrew</forenames></author><author><keyname>Phillips</keyname><forenames>Stephen</forenames></author><author><keyname>Daniilidis</keyname><forenames>Kostas</forenames></author></authors><title>Fast, Robust, Continuous Monocular Egomotion Computation</title><categories>cs.CV cs.RO</categories><comments>Accepted as a conference paper at ICRA 2016. Main paper: 8 pages, 7
  figures. Supplement: 4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose robust methods for estimating camera egomotion in noisy,
real-world monocular image sequences in the general case of unknown observer
rotation and translation with two views and a small baseline. This is a
difficult problem because of the nonconvex cost function of the perspective
camera motion equation and because of non-Gaussian noise arising from noisy
optical flow estimates and scene non-rigidity. To address this problem, we
introduce the expected residual likelihood method (ERL), which estimates
confidence weights for noisy optical flow data using likelihood distributions
of the residuals of the flow field under a range of counterfactual model
parameters. We show that ERL is effective at identifying outliers and
recovering appropriate confidence weights in many settings. We compare ERL to a
novel formulation of the perspective camera motion equation using a lifted
kernel, a recently proposed optimization framework for joint parameter and
confidence weight estimation with good empirical properties. We incorporate
these strategies into a motion estimation pipeline that avoids falling into
local minima. We find that ERL outperforms the lifted kernel method and
baseline monocular egomotion estimation strategies on the challenging KITTI
dataset, while adding almost no runtime cost over baseline egomotion methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04889</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04889</id><created>2016-02-15</created><updated>2016-02-17</updated><authors><author><keyname>Ash</keyname><forenames>Jordan T.</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>Multi-Source Domain Adaptation Using Approximate Label Matching</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain adaptation, and transfer learning more generally, seeks to remedy the
problem created when training and testing datasets are generated by different
distributions. In this work, we introduce a new unsupervised domain adaptation
algorithm for when there are multiple sources available to a learner. Our
technique assigns a rough labeling on the target samples, then uses it to learn
a transformation that aligns the two datasets before final classification. In
this article we give a convenient implementation of our method, show several
experiments using it, and compare it to other methods commonly used in the
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04900</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04900</id><created>2016-02-15</created><authors><author><keyname>Shan</keyname><forenames>Hangguan</forenames></author><author><keyname>Zhang</keyname><forenames>Yani</forenames></author><author><keyname>Zhuang</keyname><forenames>Weihua</forenames></author><author><keyname>Huang</keyname><forenames>Aiping</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author></authors><title>Simulation Results of User Behavior-Aware Scheduling Based on
  Time-Frequency Resource Conversion</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating time-frequency resource conversion (TFRC), a new network resource
allocation strategy, with call admission control can not only increase the cell
capacity but also reduce network congestion effectively. However, the optimal
setting of TFRC-oriented call admission control suffers from the curse of
dimensionality, due to Markov chain-based optimization in a high-dimensional
space. To address the scalability issue of TFRC, in [1] we extend the study of
TFRC into the area of scheduling. Specifically, we study downlink scheduling
based on TFRC for an LTE-type cellular network, to maximize service delivery.
The service scheduling of interest is formulated as a joint request, channel
and slot allocation problem which is NP-hard. An offline deflation and
sequential fixing based algorithm (named DSFRB) with only polynomial-time
complexity is proposed to solve the problem. For practical online
implementation, two TFRC-enabled low-complexity algorithms, modified Smith
ratio algorithm (named MSR) and modified exponential capacity algorithm (named
MEC), are proposed as well. In this report, we present detailed numerical
results of the proposed offline and online algorithms, which not only show the
effectiveness of the proposed algorithms but also corroborate the advantages of
the proposed TFRC-based schedule techniques in terms of quality-of-service
(QoS) provisioning for each user and revenue improvement for a service
operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04906</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04906</id><created>2016-02-15</created><authors><author><keyname>Wang</keyname><forenames>Junyan</forenames></author><author><keyname>Yeung</keyname><forenames>Sai-kit</forenames></author><author><keyname>Wang</keyname><forenames>Jue</forenames></author><author><keyname>Zhou</keyname><forenames>Kun</forenames></author></authors><title>Segmentation Rectification for Video Cutout via One-Class Structured
  Learning</title><categories>cs.CV cs.GR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works on interactive video object cutout mainly focus on designing
dynamic foreground-background (FB) classifiers for segmentation propagation.
However, the research on optimally removing errors from the FB classification
is sparse, and the errors often accumulate rapidly, causing significant errors
in the propagated frames. In this work, we take the initial steps to addressing
this problem, and we call this new task \emph{segmentation rectification}. Our
key observation is that the possibly asymmetrically distributed false positive
and false negative errors were handled equally in the conventional methods. We,
alternatively, propose to optimally remove these two types of errors. To this
effect, we propose a novel bilayer Markov Random Field (MRF) model for this new
task. We also adopt the well-established structured learning framework to learn
the optimal model from data. Additionally, we propose a novel one-class
structured SVM (OSSVM) which greatly speeds up the structured learning process.
Our method naturally extends to RGB-D videos as well. Comprehensive experiments
on both RGB and RGB-D data demonstrate that our simple and effective method
significantly outperforms the segmentation propagation methods adopted in the
state-of-the-art video cutout systems, and the results also suggest the
potential usefulness of our method in image cutout system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04915</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04915</id><created>2016-02-16</created><updated>2016-03-04</updated><authors><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Simchowitz</keyname><forenames>Max</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Gradient Descent Converges to Minimizers</title><categories>stat.ML cs.LG math.OC</categories><comments>Submitted to COLT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that gradient descent converges to a local minimizer, almost surely
with random initialization. This is proved by applying the Stable Manifold
Theorem from dynamical systems theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04918</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04918</id><created>2016-02-16</created><authors><author><keyname>Li</keyname><forenames>Yinxiao</forenames></author><author><keyname>Hu</keyname><forenames>Xiuhan</forenames></author><author><keyname>Xu</keyname><forenames>Danfei</forenames></author><author><keyname>Yue</keyname><forenames>Yonghao</forenames></author><author><keyname>Grinspun</keyname><forenames>Eitan</forenames></author><author><keyname>Allen</keyname><forenames>Peter</forenames></author></authors><title>Multi-Sensor Surface Analysis for Robotic Ironing</title><categories>cs.RO</categories><comments>7 pages, 6 figures in IEEE International Conference on Robotics and
  Automation (ICRA), Stockholm, May 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotic manipulation of deformable objects remains a challenging task. One
such task is to iron a piece of cloth autonomously. Given a roughly flattened
cloth, the goal is to have an ironing plan that can iteratively apply a regular
iron to remove all the major wrinkles by a robot. We present a novel solution
to analyze the cloth surface by fusing two surface scan techniques: a curvature
scan and a discontinuity scan. The curvature scan can estimate the height
deviation of the cloth surface, while the discontinuity scan can effectively
detect sharp surface features, such as wrinkles. We use this information to
detect the regions that need to be pulled and extended before ironing, and the
other regions where we want to detect wrinkles and apply ironing to remove the
wrinkles. We demonstrate that our hybrid scan technique is able to capture and
classify wrinkles over the surface robustly. Given detected wrinkles, we enable
a robot to iron them using shape features. Experimental results show that using
our wrinkle analysis algorithm, our robot is able to iron the cloth surface and
effectively remove the wrinkles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04921</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04921</id><created>2016-02-16</created><authors><author><keyname>Lin</keyname><forenames>Weiyao</forenames></author><author><keyname>Mi</keyname><forenames>Yang</forenames></author><author><keyname>Wang</keyname><forenames>Weiyue</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author><author><keyname>Mei</keyname><forenames>Tao</forenames></author></authors><title>A diffusion and clustering-based approach for finding coherent motions
  and understanding crowd scenes</title><categories>cs.CV cs.AI cs.MM</categories><comments>This manuscript is the accepted version for TIP (IEEE Transactions on
  Image Processing), 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of detecting coherent motions in crowd
scenes and presents its two applications in crowd scene understanding: semantic
region detection and recurrent activity mining. It processes input motion
fields (e.g., optical flow fields) and produces a coherent motion filed, named
as thermal energy field. The thermal energy field is able to capture both
motion correlation among particles and the motion trends of individual
particles which are helpful to discover coherency among them. We further
introduce a two-step clustering process to construct stable semantic regions
from the extracted time-varying coherent motions. These semantic regions can be
used to recognize pre-defined activities in crowd scenes. Finally, we introduce
a cluster-and-merge process which automatically discovers recurrent activities
in crowd scenes by clustering and merging the extracted coherent motions.
Experiments on various videos demonstrate the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04922</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04922</id><created>2016-02-16</created><authors><author><keyname>Savi&#x107;</keyname><forenames>Marko</forenames></author><author><keyname>Stojakovi&#x107;</keyname><forenames>Milo&#x161;</forenames></author></authors><title>Faster Bottleneck Non-crossing Matchings of Points in Convex Position</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an even number of points in a plane, we are interested in matching all
the points by straight line segments so that the segments do not cross.
Bottleneck matching is a matching that minimizes the length of the longest
segment. For points in convex position, we present a quadratic-time algorithm
for finding a bottleneck non-crossing matching, improving upon the best
previously known algorithm of cubic time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04924</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04924</id><created>2016-02-16</created><authors><author><keyname>Arya</keyname><forenames>Dhruv</forenames></author><author><keyname>Ha-Thuc</keyname><forenames>Viet</forenames></author><author><keyname>Sinha</keyname><forenames>Shakti</forenames></author></authors><title>Personalized Federated Search at LinkedIn</title><categories>cs.IR cs.LG</categories><comments>in Proceedings of the 24th ACM International on Conference on
  Information and Knowledge Management (CIKM 2015)</comments><doi>10.1145/2806416.2806615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LinkedIn has grown to become a platform hosting diverse sources of
information ranging from member profiles, jobs, professional groups, slideshows
etc. Given the existence of multiple sources, when a member issues a query like
&quot;software engineer&quot;, the member could look for software engineer profiles, jobs
or professional groups. To tackle this problem, we exploit a data-driven
approach that extracts searcher intents from their profile data and recent
activities at a large scale. The intents such as job seeking, hiring, content
consuming are used to construct features to personalize federated search
experience. We tested the approach on the LinkedIn homepage and A/B tests show
significant improvements in member engagement. As of writing this paper, the
approach powers all of federated search on LinkedIn homepage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04930</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04930</id><created>2016-02-16</created><authors><author><keyname>Xu</keyname><forenames>Yi-Zhi</forenames></author><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author></authors><title>Generalized minimum dominating set and application in automatic text
  summarization</title><categories>cs.IR cond-mat.stat-mech cs.CL physics.soc-ph</categories><comments>11 pages, including 4 figures and 2 tables. To be published in
  Journal of Physics: Conference Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph formed by vertices and weighted edges, a generalized minimum
dominating set (MDS) is a vertex set of smallest cardinality such that the
summed weight of edges from each outside vertex to vertices in this set is
equal to or larger than certain threshold value. This generalized MDS problem
reduces to the conventional MDS problem in the limiting case of all the edge
weights being equal to the threshold value. We treat the generalized MDS
problem in the present paper by a replica-symmetric spin glass theory and
derive a set of belief-propagation equations. As a practical application we
consider the problem of extracting a set of sentences that best summarize a
given input text document. We carry out a preliminary test of the statistical
physics-inspired method to this automatic text summarization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04933</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04933</id><created>2016-02-16</created><authors><author><keyname>Kenekayoro</keyname><forenames>Patrick</forenames></author><author><keyname>Zipamone</keyname><forenames>Godswill</forenames></author></authors><title>Greedy Ants Colony Optimization Strategy for Solving the Curriculum
  Based University Course Timetabling Problem</title><categories>cs.NE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Timetabling is a problem faced in all higher education institutions. The
International Timetabling Competition (ITC) has published a dataset that can be
used to test the quality of methods used to solve this problem. A number of
meta-heuristic approaches have obtained good results when tested on the ITC
dataset, however few have used the ant colony optimization technique,
particularly on the ITC 2007 curriculum based university course timetabling
problem. This study describes an ant system that solves the curriculum based
university course timetabling problem and the quality of the algorithm is
tested on the ITC 2007 dataset. The ant system was able to find feasible
solutions in all instances of the dataset and close to optimal solutions in
some instances. The ant system performs better than some published approaches,
however results obtained are not as good as those obtained by the best
published approaches. This study may be used as a benchmark for ant based
algorithms that solve the curriculum based university course timetabling
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04934</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04934</id><created>2016-02-16</created><authors><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Ramanujan</keyname><forenames>M. S.</forenames></author><author><keyname>Schindler</keyname><forenames>Irena</forenames></author></authors><title>Strong Backdoors for Linear Temporal Logic</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper we introduce the notion of strong backdoors into the
field of temporal logic for the CNF-fragment of linear temporal logic
introduced by Fisher. We study the parameterised complexity of the
satisfiability problem parameterised by the size of the backdoor. We
distinguish between backdoor detection and evaluation of backdoors into the
fragments of horn and krom formulas. Here we classify the operator fragments of
globally-operators for past or future, and the combination of both. Detection
is shown to be in FPT whereas the complexity of evaluation behaves different.
We show that for krom formulas the problem is paraNP-complete. For horn
formulas the complexity is shown to be either fixed parameter tractable or
paraNP-complete depending on the considered operator fragment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04936</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04936</id><created>2016-02-16</created><authors><author><keyname>Sethy</keyname><forenames>Harshit</forenames></author><author><keyname>Patel</keyname><forenames>Amit</forenames></author></authors><title>Reinforcement Learning approach for Real Time Strategy Games Battle city
  and S3</title><categories>cs.AI</categories><comments>13 pages, vol 9 issue 4 of IJIP</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we proposed reinforcement learning algorithms with the
generalized reward function. In our proposed method we use Q-learning and SARSA
algorithms with generalised reward function to train the reinforcement learning
agent. We evaluated the performance of our proposed algorithms on two real-time
strategy games called BattleCity and S3. There are two main advantages of
having such an approach as compared to other works in RTS. (1) We can ignore
the concept of a simulator which is often game specific and is usually hard
coded in any type of RTS games (2) our system can learn from interaction with
any opponents and quickly change the strategy according to the opponents and do
not need any human traces as used in previous works. Keywords : Reinforcement
learning, Machine learning, Real time strategy, Artificial intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04938</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04938</id><created>2016-02-16</created><authors><author><keyname>Ribeiro</keyname><forenames>Marco Tulio</forenames></author><author><keyname>Singh</keyname><forenames>Sameer</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author></authors><title>&quot;Why Should I Trust You?&quot;: Explaining the Predictions of Any Classifier</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust in a model. Trust is fundamental if one plans to
take action based on a prediction, or when choosing whether or not to deploy a
new model. Such understanding further provides insights into the model, which
can be used to turn an untrustworthy model or prediction into a trustworthy
one.
  In this work, we propose LIME, a novel explanation technique that explains
the predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We further
propose a method to explain models by presenting representative individual
predictions and their explanations in a non-redundant way, framing the task as
a submodular optimization problem. We demonstrate the flexibility of these
methods by explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). The usefulness of explanations is shown
via novel experiments, both simulated and with human subjects. Our explanations
empower users in various scenarios that require trust: deciding if one should
trust a prediction, choosing between models, improving an untrustworthy
classifier, and detecting why a classifier should not be trusted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04951</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04951</id><created>2016-02-16</created><authors><author><keyname>Harutyunyan</keyname><forenames>Anna</forenames></author><author><keyname>Bellemare</keyname><forenames>Marc G.</forenames></author><author><keyname>Stepleton</keyname><forenames>Tom</forenames></author><author><keyname>Munos</keyname><forenames>Remi</forenames></author></authors><title>Q($\lambda$) with Off-Policy Corrections</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and analyze an alternate approach to off-policy multi-step
temporal difference learning, in which off-policy returns are corrected with
the current Q-function in terms of rewards, rather than with the target policy
in terms of transition probabilities. We prove that such approximate
corrections are sufficient for off-policy convergence both in policy evaluation
and control, provided certain conditions. These conditions relate the distance
between the target and behavior policies, the eligibility trace parameter and
the discount factor, and formalize an underlying tradeoff in off-policy
TD($\lambda$). We illustrate this theoretical relationship empirically on a
continuous-state control task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04952</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04952</id><created>2016-02-16</created><authors><author><keyname>Korman</keyname><forenames>Amos</forenames></author><author><keyname>Rodeh</keyname><forenames>Yoav</forenames></author></authors><title>Parallel Linear Search with no Coordination for a Randomly Placed
  Treasure</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In STOC'16, Fraigniaud et al. consider the problem of finding a treasure
hidden in one of many boxes that are ordered by importance. That is, if a
treasure is in a more important box, then one would like to find it faster.
Assuming there are many searchers, the authors suggest that using an algorithm
that requires no coordination between searchers can be highly beneficial.
Indeed, besides saving the need for a communication and coordination mechanism,
such algorithms enjoy inherent robustness. The authors proceed to solve this
linear search problem in the case of countably many boxes and an adversary
placed treasure, and prove that the best speed-up possible by $k$
non-coordinating searchers is precisely $\frac{k}{4}(1+1/k)^2$. In particular,
this means that asymptotically, the speed-up is four times worse compared to
the case of full coordination.
  We suggest an important variant of the problem, where the treasure is placed
uniformly at random in one of a finite, large, number of boxes. We devise
non-coordinating algorithms that achieve a speed-up of $6/5$ for two searchers,
a speed-up of $3/2$ for three searchers, and in general, a speed-up of
$k(k+1)/(3k-1)$ for any $k \geq 1$ searchers. Thus, as $k$ grows to infinity,
the speed-up approaches three times worse compared to the case of full
coordination. Moreover, these bounds are tight in a strong sense as no
non-coordinating search algorithm for $k$ searchers can achieve better
speed-ups. We also devise non-coordinating algorithms that use only logarithmic
memory in the size of the search domain, and yet, asymptotically, achieve the
optimal speed-up. Finally, we note that all our algorithms are extremely simple
and hence applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04955</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04955</id><created>2016-02-16</created><authors><author><keyname>Abdelwahab</keyname><forenames>Elnaserledinellah Mahmood</forenames></author></authors><title>Constructive Patterns of Logical Truth</title><categories>cs.CC</categories><comments>Originally received May 11 2015 accepted January 20 2016 - published
  February 15 2016 J.Acad.(N.Y.)6,1:3-96 (94 pages 79 figures) - Theoretical
  Computer Science</comments><msc-class>Complexity of Computation</msc-class><acm-class>D.1.6; F.2; F.3; F.4; I.2; F.1.3</acm-class><journal-ref>J.Acad.(N.Y.)6,1:3-96, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simplified linguistic relation between syntax and semantics as intrinsic
property of classic Arabic motivates a dedicated look at P vs. NP in light of
efforts and solutions presented by ancient Arab- and Muslim scholars to
facilitate logical- and mathematical deduction. In Islamic Jurisprudence (Fikh)
it has recently been shown [Abdelwahab et al. 2014] that if a formal system
expressing Fikh is chosen in such a way that it is both, logically complete and
decidable, the question of a complete and consistent legislation is decidable.
If this formal Fikh-system is additionally chosen to be at least as expressive
as propositional logic, the deduction of detailed sentences is efficient while
the deduction of general rules is NP-complete. Further investigation reveals
that ancient scholars adopted a very efficient approach for checking the
validity of assertions with regard to both, language and logical argument which
was mainly characterized by the extensive use of syntactical patterns already
existent in input-variables. Accordingly, this paper introduces efficient
pattern-oriented procedures for 3-SAT as well as 2-approximation algorithms for
MinFBDD. It opens up the possibility of constructing polynomial sized FBDDs for
all Boolean functions expressed in a compact way. Eventually, an application of
this new 3-SAT-solver is shown to enable polynomial upper bounds of the number
of nodes in FBDDs constructed for finite projective planes problems overhauling
the currently known exponential lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04967</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04967</id><created>2016-02-16</created><updated>2016-03-06</updated><authors><author><keyname>Boykett</keyname><forenames>Tim</forenames></author><author><keyname>Kari</keyname><forenames>Jarkko</forenames></author><author><keyname>Salo</keyname><forenames>Ville</forenames></author></authors><title>Strongly Universal Reversible Gate Sets</title><categories>cs.ET math.CO</categories><comments>Submitted to Rev Comp 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the Toffoli gate and the negation gate together yield a
universal gate set, in the sense that every permutation of $\{0,1\}^n$ can be
implemented as a composition of these gates. Since every bit operation that
does not use all of the bits performs an even permutation, we need to use at
least one auxiliary bit to perform every permutation, and it is known that one
bit is indeed enough. Without auxiliary bits, all even permutations can be
implemented. We generalize these results to non-binary logic: If $A$ is a
finite set of odd cardinality then a finite gate set can generate all
permutations of $A^n$ for all $n$, without any auxiliary symbols. If the
cardinality of $A$ is even then, by the same argument as above, only even
permutations of $A^n$ can be implemented for large $n$, and we show that indeed
all even permutations can be obtained from a finite universal gate set. We also
consider the conservative case, that is, those permutations of $A^n$ that
preserve the weight of the input word. The weight is the vector that records
how many times each symbol occurs in the word. It turns out that no finite
conservative gate set can, for all $n$, implement all conservative even
permutations of $A^n$ without auxiliary bits. But we provide a finite gate set
that can implement all those conservative permutations that are even within
each weight class of $A^n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04974</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04974</id><created>2016-02-16</created><authors><author><keyname>Awad</keyname><forenames>Alaa</forenames></author><author><keyname>Elsayed</keyname><forenames>Medhat H. M.</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>Encoding Distortion Modeling For DWT-Based Wireless EEG Monitoring
  System</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in wireless body area sensor net- works leverage wireless and
mobile communication technologies to facilitate development of innovative
medical applications that can significantly enhance healthcare services and
improve quality of life. Specifically, Electroencephalography (EEG)-based
applications lie at the heart of these promising technologies. However, the
design and operation of such applications is challenging. Power consumption
requirements of the sensor nodes may turn some of these applications
impractical. Hence, implementing efficient encoding schemes are essential to
reduce power consumption in such applications. In this paper, we propose an
analytical distortion model for the EEG-based encoding systems. Using this
model, the encoder can effectively reconfigure its complexity by adjusting its
control parameters to satisfy application constraints while maintaining
reconstruction accuracy at the receiver side. The simulation results illustrate
that the main parameters that affect the distortion are compression ratio and
filter length of the considered DWT-based encoder. Furthermore, it is found
that the wireless channel variations have a significant influence on the
estimated distortion at the receiver side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04976</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04976</id><created>2016-02-16</created><authors><author><keyname>Contal</keyname><forenames>Emile</forenames></author><author><keyname>Vayatis</keyname><forenames>Nicolas</forenames></author></authors><title>Stochastic Process Bandits: Upper Confidence Bounds Algorithms via
  Generic Chaining</title><categories>stat.ML cs.LG</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers the problem of global optimization in the setup of
stochastic process bandits. We introduce an UCB algorithm which builds a
cascade of discretization trees based on generic chaining in order to render
possible his operability over a continuous domain. The theoretical framework
applies to functions under weak probabilistic smoothness assumptions and also
extends significantly the spectrum of application of UCB strategies. Moreover
generic regret bounds are derived which are then specialized to Gaussian
processes indexed on infinite-dimensional spaces as well as to quadratic forms
of Gaussian processes. Lower bounds are also proved in the case of Gaussian
processes to assess the optimality of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04980</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04980</id><created>2016-02-16</created><authors><author><keyname>Guravaiah</keyname><forenames>Koppala</forenames></author><author><keyname>Velusamy</keyname><forenames>R. Leela</forenames></author></authors><title>Performance Analysis of RFDMRP:River Formation Dynamics based Multi-Hop
  Routing Protocol in WSNs</title><categories>cs.NI</categories><comments>11 pages, 12 figures</comments><journal-ref>International Journal of Information Processing, 9(4), 22-33, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Wireless sensor networks, sensor nodes sense the data from environment
according to its functionality and forwards to its base station. This process
is called Data collection. The Data collection process is done either directly
or by multi-hop routing. In direct routing, every sensor node directly
transfers its sensed data to base station which has an impact on energy
consumption from sensor node due to the far distance between the sensor node
and base station. In multi-hop routing, the sensed data is relayed through
multiple nodes to the base station, it consumes less energy. This paper
presents and analyzes the performance of a data collection routing protocol
called RFDMRP: River Formation Dynamics based multi-hop routing protocol. The
performance of RFDMRP is tested and analyzed for network parameters such as
Network lifetime, Energy usage, and Node density &amp; data aggregation impact on
network lifetime. The simulated results are compared with two algorithms LEACH
and MOD_LEACH. The comparison reveals that the proposed algorithm performs
better than LEACH and MOD_LEACH with respect to Network lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04981</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04981</id><created>2016-02-16</created><authors><author><keyname>V&#xe4;lim&#xe4;ki</keyname><forenames>Tuomas</forenames></author><author><keyname>Ritala</keyname><forenames>Risto</forenames></author></authors><title>Optimizing Gaze Direction in a Visual Navigation Task</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigation in an unknown environment consists of multiple separable subtasks,
such as collecting information about the surroundings and navigating to the
current goal. In the case of pure visual navigation, all these subtasks need to
utilize the same vision system, and therefore a way to optimally control the
direction of focus is needed. We present a case study, where we model the
active sensing problem of directing the gaze of a mobile robot with three
machine vision cameras as a partially observable Markov decision process
(POMDP) using a mutual information (MI) based reward function. The key aspect
of the solution is that the cameras are dynamically used either in monocular or
stereo configuration. The benefits of using the proposed active sensing
implementation are demonstrated with simulations and experiments on a real
robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04983</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04983</id><created>2016-02-16</created><authors><author><keyname>Chowdhury</keyname><forenames>Sreyasi Nag</forenames></author><author><keyname>Malinowski</keyname><forenames>Mateusz</forenames></author><author><keyname>Bulling</keyname><forenames>Andreas</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>Contextual Media Retrieval Using Natural Language Queries</title><categories>cs.IR cs.AI cs.CL cs.CV cs.HC</categories><comments>8 pages, 9 figures, 1 table</comments><acm-class>H.3.3; H.5.1; I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread integration of cameras in hand-held and head-worn devices as
well as the ability to share content online enables a large and diverse visual
capture of the world that millions of users build up collectively every day. We
envision these images as well as associated meta information, such as GPS
coordinates and timestamps, to form a collective visual memory that can be
queried while automatically taking the ever-changing context of mobile users
into account. As a first step towards this vision, in this work we present
Xplore-M-Ego: a novel media retrieval system that allows users to query a
dynamic database of images and videos using spatio-temporal natural language
queries. We evaluate our system using a new dataset of real user queries as
well as through a usability study. One key finding is that there is a
considerable amount of inter-user variability, for example in the resolution of
spatial relations in natural language utterances. We show that our retrieval
system can cope with this variability using personalisation through an online
learning-based retrieval formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04984</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04984</id><created>2016-02-16</created><updated>2016-02-18</updated><authors><author><keyname>Kim</keyname><forenames>Hyo-Eun</forenames></author><author><keyname>Hwang</keyname><forenames>Sangheum</forenames></author></authors><title>Scale-Invariant Feature Learning using Deconvolutional Neural Networks
  for Weakly-Supervised Semantic Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A weakly-supervised semantic segmentation framework using tied
deconvolutional neural networks is proposed for scale-invariant feature
learning. Each deconvolution layer in the proposed framework consists of
unpooling and deconvolution operations. 'Unpooling' upsamples the input feature
map based on unpooling switches defined by corresponding convolution layer's
pooling operation. 'Deconvolution' convolves the input unpooled features by
using convolutional weights tied with the corresponding convolution layer's
convolution operation. This unpooling-deconvolution combination results in
reduction of false positives, since output features of the deconvolution layer
are reconstructed from the most discriminative unpooled features instead of the
raw one. All the feature maps restored from the entire deconvolution layers can
constitute a rich feature set according to different abstraction levels. Those
features are selectively used for generating class-specific activation maps.
Under the weak supervision (image-level labels), the proposed framework shows
promising results on medical images (chest X-rays) and achieves
state-of-the-art performance on the PASCAL VOC segmentation dataset in the same
experimental condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04995</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04995</id><created>2016-02-16</created><updated>2016-02-23</updated><authors><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Raftopoulou</keyname><forenames>Chrysanthi N.</forenames></author></authors><title>On the Density of 3-Planar Graphs</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $k$-planar graph is one that can be drawn in the plane such that every edge
is crossed at most $k$ times. For $k \leq 4$, Pach and T\'oth proved a bound of
$(k+3)(n-2)$ on the total number of edges of a $k$-planar graph, which is tight
for $k=1,2$. For $k=3$, the bound of $6n-12$ has been improved to
$\frac{11}{2}n-11$ and has been shown to be optimal up to an additive constant.
We give an alternative proof of this result, which contains more structural
insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05010</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05010</id><created>2016-02-16</created><authors><author><keyname>Widemann</keyname><forenames>Baltasar Tranc&#xf3;n y</forenames></author></authors><title>Higher-Order Recursion Abstraction: How to Make Ackermann, Knuth and
  Conway Look Like a Bunch of Primitives, Figuratively Speaking</title><categories>cs.LO cs.PL</categories><acm-class>D.1.1; D.3.3; F.3.3; F.4.1</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The Ackermann function is a famous total recursive binary function on the
natural numbers. It is the archetypal example of such a function that is not
primitive recursive, in the sense of classical recursion theory. However, and
in seeming contradiction, there are generalized notions of total recursion, for
which the Ackermann function is in fact primitive recursive, and often featured
as a witness for the additional power gained by the generalization. Here, we
investigate techniques for finding and analyzing the primitive form of
complicated recursive functions, namely also Knuth's and Conway's arrow
notations, in particular by recursion abstraction, in a framework of functional
program transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05012</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05012</id><created>2016-02-16</created><authors><author><keyname>Fowkes</keyname><forenames>Jaroslav</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>A Subsequence Interleaving Model for Sequential Pattern Mining</title><categories>stat.ML cs.AI cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent sequential pattern mining methods have used the minimum description
length (MDL) principle to define an encoding scheme which describes an
algorithm for mining the most compressing patterns in a database. We present a
novel subsequence interleaving model based on a probabilistic model of the
sequence database, which allows us to search for the most compressing set of
patterns without designing a specific encoding scheme. Our proposed algorithm
is able to efficiently mine the most relevant sequential patterns and rank them
using an associated measure of interestingness. The efficient inference in our
model is a direct result of our use of a structural expectation-maximization
framework, in which the expectation-step takes the form of a submodular
optimization problem subject to a coverage constraint. We show on both
synthetic and real world datasets that our model mines a set of sequential
patterns with low spuriousness and redundancy, high interpretability and
usefulness in real-world applications. Furthermore, we demonstrate that the
quality of the patterns from our approach is comparable to, if not better than,
existing state of the art sequential pattern mining algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05016</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05016</id><created>2016-02-16</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovnev</keyname><forenames>Alexander</forenames></author><author><keyname>Kulikov</keyname><forenames>Alexander S.</forenames></author><author><keyname>Mihajlin</keyname><forenames>Ivan</forenames></author><author><keyname>Pachocki</keyname><forenames>Jakub</forenames></author><author><keyname>Soca&#x142;a</keyname><forenames>Arkadiusz</forenames></author></authors><title>Tight Lower Bounds on Graph Embedding Problems</title><categories>cs.DS</categories><comments>23 pages. arXiv admin note: substantial text overlap with
  arXiv:1502.05447, arXiv:1507.03738</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that unless the Exponential Time Hypothesis (ETH) fails, deciding if
there is a homomorphism from graph $G$ to graph $H$ cannot be done in time
$|V(H)|^{o(|V(G)|)}$. We also show an exponential-time reduction from Graph
Homomorphism to Subgraph Isomorphism. This rules out (subject to ETH) a
possibility of $|V(H)|^{o(|V(H)|)}$-time algorithm deciding if graph $G$ is a
subgraph of $H$. For both problems our lower bounds asymptotically match the
running time of brute-force algorithms trying all possible mappings of one
graph into another. Thus, our work closes the gap in the known complexity of
these fundamental problems.
  Moreover, as a consequence of our reductions conditional lower bounds follow
for other related problems such as Locally Injective Homomorphism, Graph
Minors, Topological Graph Minors, Minimum Distortion Embedding and Quadratic
Assignment Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05028</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05028</id><created>2016-02-16</created><updated>2016-02-17</updated><authors><author><keyname>Ulyantsev</keyname><forenames>Vladimir</forenames></author><author><keyname>Zakirzyanov</keyname><forenames>Ilya</forenames></author><author><keyname>Shalyto</keyname><forenames>Anatoly</forenames></author></authors><title>Symmetry Breaking Predicates for SAT-based DFA Identification</title><categories>cs.FL cs.AI cs.LO</categories><comments>14 pages, 9 figures, 5 tables, submitted to Journal of Computer and
  System Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was shown before that the NP-hard problem of deterministic finite automata
(DFA) identification can be effectively translated to Boolean satisfiability
(SAT). Modern SAT-solvers can tackle hard DFA identification instances
efficiently. We present a technique to reduce the problem search space by
enforcing an enumeration of DFA states in depth-first search (DFS) or
breadth-first search (BFS) order. We propose symmetry breaking predicates,
which can be added to Boolean formulae representing various DFA identification
problems. We show how to apply this technique to DFA identification from both
noiseless and noisy data. Also we propose a method to identify all automata of
the desired size. The proposed approach outperforms the current
state-of-the-art DFASAT method for DFA identification from noiseless data. A
big advantage of the proposed approach is that it allows to determine exactly
the existence or non-existence of a solution of the noisy DFA identification
problem unlike metaheuristic approaches such as genetic algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05032</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05032</id><created>2016-02-16</created><authors><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author><author><keyname>Diab</keyname><forenames>Nuha</forenames></author><author><keyname>Kawar</keyname><forenames>Shada R.</forenames></author><author><keyname>Shahla</keyname><forenames>Robert J.</forenames></author></authors><title>Enumerating all the Irreducible Polynomials over Finite Field</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a deterministic algorithm that enumerates all the irreducible
polynomials of degree $n$ over a finite field and their roots in the extension
field in quasilinear time cost per element.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05038</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05038</id><created>2016-02-16</created><authors><author><keyname>Orden</keyname><forenames>David</forenames></author><author><keyname>Marsa-Maestre</keyname><forenames>Ivan</forenames></author><author><keyname>Gimenez-Guzman</keyname><forenames>Jose Manuel</forenames></author><author><keyname>de la Hoz</keyname><forenames>Enrique</forenames></author></authors><title>Spectrum graph coloring and applications to WiFi channel assignment</title><categories>cs.DM math.CO</categories><comments>22 pages, 6 figures, 6 tables, submitted</comments><msc-class>05C15, 90C35, 94C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by WiFi channel assignment, we propose and explore two
vertex-coloring problems for graphs, where the spectrum of colors is endorsed
with a matrix of interferences between each pair of colors. The Threshold
Spectrum Coloring problem fixes the number of colors available and aims to
minimize the interference threshold, i.e., the maximum of the interferences at
the vertices. The Chromatic Spectrum Coloring problem fixes a threshold and
aims to minimize the number of colors for which respecting that threshold is
possible. As theoretical results, we show that both problems are NP-hard and we
prove upper bounds for the solutions to each problem, with interesting
applications to the design and planning of wireless network infrastructures. We
complete the scene with experimental results, proposing a DSATUR-based
heuristic for each problem and comparing them with the nonlinear optimizer
ALHSO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05040</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05040</id><created>2016-02-16</created><authors><author><keyname>L&#xfc;ck</keyname><forenames>Martin</forenames></author></authors><title>The Axioms of Team Logic</title><categories>cs.LO</categories><msc-class>03B60</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework is developed that extends calculi for propositional, modal and
predicate logics to calculi for team-based logics. This method is applied to
classical and quantified propositional logic, first-order logic and the modal
logic K. Complete axiomatizations for propositional team logic PTL, quantified
propositional team logic QPTL, modal team logic MTL and the
dependence-atom-free fragment of first-order team logic TL are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05045</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05045</id><created>2016-02-16</created><authors><author><keyname>Klein</keyname><forenames>Felix</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Prompt Delay</title><categories>cs.GT cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay games are two-player games of infinite duration in which one player may
delay her moves to obtain a lookahead on her opponent's moves. Recently, such
games with quantitative winning conditions in weak MSO with the unbounding
quantifier were studied, but their properties turned out to be unsatisfactory.
In particular, unbounded lookahead might be necessary.
  Here, we study delay games with winning conditions given by PROMPT-LTL,
Linear Temporal Logic equipped with a parameterized eventually operator whose
scope is bounded. Our main result shows that solving PROMPT-LTL delay games is
complete for triply-exponential time. Furthermore, we give tight
triply-exponential bounds on the necessary lookahead and on the scope of the
parameterized eventually operator. Thus, we identify PROMPT-LTL as the first
known class of well-behaved quantitative winning conditions for delay games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05056</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05056</id><created>2016-02-16</created><authors><author><keyname>Widemann</keyname><forenames>Baltasar Tranc&#xf3;n y</forenames></author><author><keyname>Bogner</keyname><forenames>Christina</forenames></author></authors><title>Comments on &quot;the return of information theory&quot;</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We criticize the article &quot;Quantifying sustainability: Resilience, efficiency
and the return of information theory&quot; (Ulanowicz et al., Ecological Complexity,
2009) for its presentation of information theory to an audience of ecologists.
We find that the formulation and use of basic IT concepts are unnecessarily
obscure, deliberately incompatible with standard terminology, and in places
just mathematically wrong. We lift some of the confusion by correcting errors,
dissecting ambiguities and using standard terminology wherever possible. We
believe that clear and rigorous presentations are necessary in order to enable
information theoreticians and ecologists to engage in a fruitful dialogue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05059</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05059</id><created>2016-02-16</created><authors><author><keyname>Gavinsky</keyname><forenames>Dmitry</forenames></author></authors><title>Entangled simultaneity versus classical interactivity in communication
  complexity</title><categories>cs.CC quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1999 Raz demonstrated a partial function that had an efficient quantum
two-way communication protocol but no efficient classical two-way protocol and
asked, whether there existed a function with an efficient quantum one-way
protocol, but still no efficient classical two-way protocol. In 2010 Klartag
and Regev demonstrated such a function and asked, whether there existed a
function with an efficient quantum simultaneous-messages protocol, but still no
efficient classical two-way protocol.
  In this work we answer the latter question affirmatively and present a
partial function Shape, which can be computed by a protocol sending entangled
simultaneous messages of poly-logarithmic size, and whose classical two-way
complexity is lower bounded by a polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05063</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05063</id><created>2016-02-16</created><authors><author><keyname>Ince</keyname><forenames>Robin A. A.</forenames></author></authors><title>Measuring multivariate redundant information with pointwise common
  change in surprisal</title><categories>cs.IT math.IT math.ST q-bio.NC q-bio.QM stat.ME stat.TH</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The problem of how to properly quantify redundant information is an open
question that has been the subject of much recent research. Redundant
information refers to information about a target variable S that is common to
two or more predictor variables Xi. It can be thought of as quantifying
overlapping information content or similarities in the representation of S
between the Xi. We present a new measure of redundancy which measures the
common change in surprisal shared between variables at the local or pointwise
level. We demonstrate how this redundancy measure can be used within the
framework of the Partial Information Decomposition (PID) to give an intuitive
decomposition of the multivariate mutual information for a range of example
systems, including continuous Gaussian variables. We also propose a
modification of the PID in which we normalise partial information terms from
non-disjoint sets of sources within the same level of the redundancy lattice,
to prevent negative terms resulting from over-counting dependent partial
information values. Our redundancy measure is easy to compute, and Matlab code
implementing the measure, together with all considered examples, is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05067</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05067</id><created>2016-02-16</created><authors><author><keyname>Younis</keyname><forenames>Mahdi Mohammed</forenames></author><author><keyname>Baban</keyname><forenames>Miran Hikmat Mohammed</forenames></author></authors><title>Skill Evaluation for Newly Graduated Students Via Online Test</title><categories>cs.CY</categories><comments>Article Published in International Journal of Advanced Computer
  Science and Applications(IJACSA), Volume 6 Issue 9, 2015</comments><doi>10.14569/IJACSA.2015.060937</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Every year in each university many students are graduated holding a first
university degree. For example Bachelor degree in Computer Science. Most of
those students have a motivation to continue with further studies to get higher
education level degree in universities. However, some other students may have
enthusiasm towards working based on their skills that they got during their
life of studies in university. In both cases, it is required that applicant
must pass a test that comprise the entire subject that they learned before. For
this reasons, this research is proposing a new technique to evaluate graduate
students skills.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05069</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05069</id><created>2016-02-16</created><authors><author><keyname>Kang</keyname><forenames>Bosung</forenames></author></authors><title>Robust Covariance Matrix Estimation for Radar Space-Time Adaptive
  Processing (STAP)</title><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the disturbance or clutter covariance is a centrally important
problem in radar space time adaptive processing (STAP). The disturbance
covariance matrix should be inferred from training sample observations in
practice. Large number of homogeneous training samples are generally not
available because of difficulty of guaranteeing target free disturbance
observation, practical limitations imposed by the spatio-temporal
nonstationarity, and system considerations. In this dissertation, we look to
address the aforementioned challenges by exploiting physically inspired
constraints into ML estimation. While adding constraints is beneficial to
achieve satisfactory performance in the practical regime of limited training,
it leads to a challenging problem. We focus on breaking this classical
trade-off between computational tractability and desirable performance
measures, particularly in training starved regimes. In particular, we exploit
both the structure of the disturbance covariance and importantly the knowledge
of the clutter rank to yield a new rank constrained maximum likelihood (RCML)
estimator. In addition, we derive a new covariance estimator for STAP that
jointly considers a Toeplitz structure and a rank constraint on the clutter
component.
  Finally, we address the problem of working with inexact physical radar
parameters under a practical radar environment. We propose a robust covariance
estimation method via an expected likelihood (EL) approach. We analyze
covariance estimation algorithms under three different cases of imperfect
constraints: 1) only rank constraint, 2) both rank and noise power constraint,
and 3) condition number constraint. For each case, we formulate estimation of
the constraint as an optimization problem with the EL criterion and formally
derive and prove a significant analytical result such as uniqueness of the
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05072</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05072</id><created>2016-02-16</created><authors><author><keyname>Vakilinia</keyname><forenames>Kasra</forenames></author><author><keyname>Ranganathan</keyname><forenames>Sudarsan V. S.</forenames></author><author><keyname>Divsalar</keyname><forenames>Dariush</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Optimizing Transmission Lengths for Limited Feedback with Non-Binary
  LDPC Examples</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a general approach for optimizing the number of symbols
in increments (packets of incremental redundancy) in a feedback communication
system with a limited number of increments. This approach is based on a tight
normal approximation on the rate for successful decoding. Applying this
approach to a variety of feedback systems using non-binary (NB) low-density
parity-check (LDPC) codes shows that greater than 90% of capacity can be
achieved with average blocklengths fewer than 500 transmitted bits. One result
is that the performance with ten increments closely approaches the performance
with an infinite number of increments. The paper focuses on binary- input
additive-white Gaussian noise (BI-AWGN) channels but also demonstrates that the
normal approximation works well on examples of fading channels as well as
high-SNR AWGN channels that require larger QAM constellations. The paper
explores both variable-length feedback codes with termination (VLFT) and the
more practical variable length feedback (VLF) codes without termination that
require no assumption of noiseless transmitter confirmation. For VLF we
consider both a two-phase scheme and CRC-based scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05100</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05100</id><created>2016-02-06</created><authors><author><keyname>Asudeh</keyname><forenames>Abolfazl</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Query Reranking As A Service</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ranked retrieval model has rapidly become the de facto way for search
query processing in client-server databases, especially those on the web.
Despite of the extensive efforts in the database community on designing better
ranking functions/mechanisms, many such databases in practice still fail to
address the diverse and sometimes contradicting preferences of users on tuple
ranking, perhaps (at least partially) due to the lack of expertise and/or
motivation for the database owner to design truly effective ranking functions.
This paper takes a different route on addressing the issue by defining a novel
{\em query reranking problem}, i.e., we aim to design a third-party service
that uses nothing but the public search interface of a client-server database
to enable the on-the-fly processing of queries with any user-specified ranking
functions (with or without selection conditions), no matter if the ranking
function is supported by the database or not. We analyze the worst-case
complexity of the problem and introduce a number of ideas, e.g., on-the-fly
indexing, domination detection and virtual tuple pruning, to reduce the
average-case cost of the query reranking algorithm. We also present extensive
experimental results on real-world datasets, in both offline and live online
systems, that demonstrate the effectiveness of our proposed techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05103</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05103</id><created>2016-02-16</created><authors><author><keyname>Lorenzo</keyname><forenames>Beatriz</forenames></author><author><keyname>Gonzalez-Castano</keyname><forenames>F. Javier</forenames></author></authors><title>A Matching Game for Data Trading in Operator-Supervised User-Provided
  Networks</title><categories>cs.NI cs.GT</categories><comments>accepted for presentation at ICC 2016. arXiv admin note: text overlap
  with arXiv:1307.2763 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a recent cellular network connection paradigm,
known as user-provided network (UPN), where users share their connectivity and
act as an access point for other users. To incentivize user participation in
this network, we allow the users to trade their data plan and obtain some
profits by selling and buying leftover data capacities (caps) from each other.
We formulate the buyers and sellers association for data trading as a matching
game. In this game, buyers and sellers rank one another based on preference
functions that capture buyers' data demand and QoS requirements, sellers'
available data and energy resources. We show that these preferences are
interdependent and influenced by the existing network-wide matching. For this
reason, the game can be classified as a one-to-many matching game with
externalities. To solve this game, a distributed algorithm that combines
notions from matching theory and market equilibrium is proposed. The algorithm
enables the players to self-organize into a stable matching and dynamic
adaptation of price to data demand and supply. The properties of the resulting
matching are discussed. Moreover, the price benchmark for the users to join the
UPN and the operator gain are also determined. Simulation results show that the
proposed algorithm yields an improvement of the average utility per user up to
25% and 50% relative to random matching and worst case utility, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05106</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05106</id><created>2016-02-16</created><authors><author><keyname>Cotret</keyname><forenames>Pascal</forenames></author><author><keyname>Gogniat</keyname><forenames>Guy</forenames></author><author><keyname>Florez</keyname><forenames>Martha Johanna Sepulveda</forenames></author></authors><title>Protection of heterogeneous architectures on FPGAs: An approach based on
  hardware firewalls</title><categories>cs.CR</categories><comments>in Elsevier Microprocessors and Microsystems, 2016</comments><doi>10.1016/j.micpro.2016.01.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded systems are parts of our daily life and used in many fields. They
can be found in smartphones or in modern cars including GPS, light/rain sensors
and other electronic assistance mechanisms. These systems may handle sensitive
data (such as credit card numbers, critical information about the host system
and so on) which must be protected against external attacks as these data may
be transmitted through a communication link where attackers can connect to
extract sensitive information or inject malicious code within the system. This
work presents an approach to protect communications in multiprocessor
architectures. This approach is based on hardware security enhancements acting
as firewalls. These firewalls filter all data going through the system
communication bus and an additional flexible cryptographic block aims to
protect external memory from attacks. Benefits of our approach are demonstrated
using a case study and some custom software applications implemented in a
Field-Programmable Gate Array (FPGA). Firewalls implemented in the target
architecture allow getting a low-latency security layer with flexible
cryptographic features. To illustrate the benefit of such a solution,
implementations are discussed for different MPSoCs implemented on Xilinx
Virtex-6 FPGAs. Results demonstrate a reduction up to 33% in terms of latency
overhead compared to existing efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05109</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05109</id><created>2016-02-15</created><authors><author><keyname>Belaidouni</keyname><forenames>Somia</forenames></author><author><keyname>Miraoui</keyname><forenames>Moeiz</forenames></author><author><keyname>Tadj</keyname><forenames>Chakib</forenames></author></authors><title>Towards an Efficient Smart Space Architecture</title><categories>cs.CY cs.HC</categories><journal-ref>International Journal of Advanced Studies in Computer Science and
  Engineering, IJASCSE, Volume 5, Issue 1, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A smart space offers entirely new opportunities for end users by adapting
services accordingly to make life easy. A number of architectural designs have
been proposed to design context awareness systems and adaptation behavior.
However, the quality of the system depends on the degree of satisfaction of the
initials needs. In this paper, we discuss three main indicators of quality
design for smart spaces that are strongly related to the context modules and
reasoning process: functionality, reusability and changeability. A general
layered architecture system is presented to define the principal components
that should constitute any context aware adaptive system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05110</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05110</id><created>2016-02-16</created><updated>2016-02-17</updated><authors><author><keyname>Im</keyname><forenames>Daniel Jiwoong</forenames></author><author><keyname>Kim</keyname><forenames>Chris Dongjoo</forenames></author><author><keyname>Jiang</keyname><forenames>Hui</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author></authors><title>Generating images with recurrent adversarial networks</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gatys et al. (2015) showed that optimizing pixels to match features in a
convolutional network with respect reference image features is a way to render
images of high visual quality. We show that unrolling this gradient-based
optimization yields a recurrent computation that creates images by
incrementally adding onto a visual &quot;canvas&quot;. We propose a recurrent generative
model inspired by this view, and show that it can be trained using adversarial
training to generate very good image samples. We also propose a way to
quantitatively compare adversarial networks by having the generators and
discriminators of these networks compete against each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05112</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05112</id><created>2016-02-14</created><authors><author><keyname>Xu</keyname><forenames>Hongteng</forenames></author><author><keyname>Wu</keyname><forenames>Weichang</forenames></author><author><keyname>Nemati</keyname><forenames>Shamim</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author></authors><title>ICU Patient Flow Prediction via Discriminative Learning of
  Mutually-Correcting Processes</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade the rate of intensive care unit (ICU) use in the United
States has been increasing, with a recent study reporting almost one in three
Medicare beneficiaries experiencing an ICU visit during the last month of their
lives. With an aging population and ever-growing demand for critical care,
effective management of patient flow and transition among different care
facilities will prove indispensible for shortening lengths of hospital stays,
improving patient outcomes, allocating critical resources, and reducing
preventable re-admissions. In this paper, we focus on a new problem of
predicting the so-called ICU patient flow from longitudinal electronic health
records (EHRs), which is not explored via existing machine learning techniques.
By treating a sequence of transition events as a point process, we develop a
novel framework for modeling patient flow through various ICU care units and
predict patients' destination ICUs and duration days jointly. Instead of
learning a generative point process model via maximum likelihood estimation, we
propose a novel discriminative learning algorithm aiming at improving the
prediction of transition events. By parameterizing the proposed model as a
mutually-correcting process, we formulate the problem as a generalized linear
model, i.e., multinomial logistic regression, which yields itself to efficient
learning via alternating direction method of multipliers (ADMM). Furthermore,
we achieve simultaneous feature selection and learning by adding a group-lasso
regularizer to the ADMM algorithm. Using real-world data of ICU patients, we
show that our method obtains superior performance in terms of accuracy of
predicting the destination ICU transition and duration of each ICU occupancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05113</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05113</id><created>2016-02-16</created><authors><author><keyname>Quatmann</keyname><forenames>Tim</forenames></author><author><keyname>Dehnert</keyname><forenames>Christian</forenames></author><author><keyname>Jansen</keyname><forenames>Nils</forenames></author><author><keyname>Junges</keyname><forenames>Sebastian</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author></authors><title>Parameter Synthesis for Markov Models: Faster Than Ever</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple technique for verifying probabilistic models whose
transition probabilities are parametric. The key is to replace parametric
transitions by nondeterministic choices of extremal values. Analysing the
resulting parameter-free model using off-the-shelf means yields (refinable)
lower and upper bounds on probabilities of regions in the parameter space. The
technique outperforms the existing analysis of parametric Markov chains by
several orders of magnitude regarding both run-time and scalability. Its beauty
is its applicability to various probabilistic models. It in particular provides
the first sound and feasible method for performing parameter synthesis of
Markov decision processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05124</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05124</id><created>2016-02-16</created><authors><author><keyname>Hartmann</keyname><forenames>Alexander K.</forenames></author></authors><title>Practical Introduction to Clustering Data</title><categories>physics.data-an astro-ph.IM cond-mat.stat-mech cs.LG</categories><comments>22 pages. All source code in anc directory included. Section 8.5.6 of
  book: A.K. Hartmann, Big Practical Guide to Computer Simulations,
  World-Scientifc, Singapore (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data clustering is an approach to seek for structure in sets of complex data,
i.e., sets of &quot;objects&quot;. The main objective is to identify groups of objects
which are similar to each other, e.g., for classification. Here, an
introduction to clustering is given and three basic approaches are introduced:
the k-means algorithm, neighbour-based clustering, and an agglomerative
clustering method. For all cases, C source code examples are given, allowing
for an easy implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05127</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05127</id><created>2016-02-16</created><authors><author><keyname>Kuang</keyname><forenames>Da</forenames></author><author><keyname>Shi</keyname><forenames>Zuoqiang</forenames></author><author><keyname>Osher</keyname><forenames>Stanley</forenames></author><author><keyname>Bertozzi</keyname><forenames>Andrea</forenames></author></authors><title>A Harmonic Extension Approach for Collaborative Ranking</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new perspective on graph-based methods for collaborative ranking
for recommender systems. Unlike user-based or item-based methods that compute a
weighted average of ratings given by the nearest neighbors, or low-rank
approximation methods using convex optimization and the nuclear norm, we
formulate matrix completion as a series of semi-supervised learning problems,
and propagate the known ratings to the missing ones on the user-user or
item-item graph globally. The semi-supervised learning problems are expressed
as Laplace-Beltrami equations on a manifold, or namely, harmonic extension, and
can be discretized by a point integral method. We show that our approach does
not impose a low-rank Euclidean subspace on the data points, but instead
minimizes the dimension of the underlying manifold. Our method, named LDM (low
dimensional manifold), turns out to be particularly effective in generating
rankings of items, showing decent computational efficiency and robust ranking
quality compared to state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05134</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05134</id><created>2016-02-16</created><authors><author><keyname>Rotella</keyname><forenames>Nicholas</forenames></author><author><keyname>Mason</keyname><forenames>Sean</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author><author><keyname>Righetti</keyname><forenames>Ludovic</forenames></author></authors><title>Inertial Sensor-Based Humanoid Joint State Estimation</title><categories>cs.RO</categories><comments>Accepted to International Conference on Robotics and Automation
  (ICRA) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents methods for the determination of a humanoid robot's joint
velocities and accelerations directly from link-mounted Inertial Measurement
Units (IMUs) each containing a three-axis gyroscope and a three-axis
accelerometer. No information about the global pose of the floating base or its
links is required and precise knowledge of the link IMU poses is not necessary
due to presented calibration routines. Additionally, a filter is introduced to
fuse gyroscope angular velocities with joint position measurements and
compensate the computed joint velocities for time-varying gyroscope biases. The
resulting joint velocities are subject to less noise and delay than filtered
velocities computed from numerical differentiation of joint potentiometer
signals, leading to superior performance in joint feedback control as
demonstrated in experiments performed on a SARCOS hydraulic humanoid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05136</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05136</id><created>2016-02-16</created><authors><author><keyname>Caissy</keyname><forenames>David</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Exploration of Faulty Hamiltonian Graphs</title><categories>cs.DS</categories><comments>17 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of exploration of networks, some of whose edges are
faulty. A mobile agent, situated at a starting node and unaware of which edges
are faulty, has to explore the connected fault-free component of this node by
visiting all of its nodes. The cost of the exploration is the number of edge
traversals. For a given network and given starting node, the overhead of an
exploration algorithm is the worst-case ratio (taken over all fault
con?gurations) of its cost to the cost of an optimal algorithm which knows
where faults are situated. An exploration algorithm, for a given network and
given starting node, is called perfectly competitive if its overhead is the
smallest among all exploration algorithms not knowing the location of faults.
We design a perfectly competitive exploration algorithm for any ring, and show
that, for networks modeled by hamiltonian graphs, the overhead of any DFS
exploration is at most 10/9 times larger than that of a perfectly competitive
algorithm. Moreover, for hamiltonian graphs of size at least 24, this overhead
is less than 6% larger than that of a perfectly competitive algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05142</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05142</id><created>2016-02-13</created><authors><author><keyname>Wai</keyname><forenames>Larry</forenames></author></authors><title>Data Science at Udemy: Agile Experimentation with Algorithms</title><categories>cs.CY</categories><comments>6 pages, submitted to KDD 2016</comments><msc-class>68U35</msc-class><acm-class>K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe the data science framework at Udemy, which
currently supports the recommender and search system. We explain the
motivations behind the framework and review the approach, which allows multiple
individual data scientists to all become 'full stack', taking control of their
own destinies from the exploration and research phase, through algorithm
development, experiment setup, and deep experiment analytics. We describe
algorithms tested and deployed in 2015, as well as some key insights obtained
from experiments leading to the launch of the new recommender system at Udemy.
Finally, we outline the current areas of research, which include search,
personalization, and algorithmic topic generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05150</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05150</id><created>2016-02-16</created><authors><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author><author><keyname>Narrins</keyname><forenames>Lothar</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author><author><keyname>Rote</keyname><forenames>G&#xfc;nter</forenames></author><author><keyname>Thomas</keyname><forenames>Antonis</forenames></author><author><keyname>Uno</keyname><forenames>Takeaki</forenames></author></authors><title>Tight Exact and Approximate Algorithmic Results on Token Swapping</title><categories>cs.CC</categories><comments>22 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G=(V,E)$ with $V=\{1,\ldots,n\}$, we place on every vertex a
token $T_1,\ldots,T_n$. A swap is an exchange of tokens on adjacent vertices.
We consider the algorithmic question of finding a shortest sequence of swaps
such that token $T_i$ is on vertex $i$. We are able to achieve essentially
matching upper and lower bounds, for exact algorithms and approximation
algorithms. For exact algorithms, we rule out $2^{o(n)}$ algorithm under ETH.
This is matched with a simple $2^{O(n\log n)}$ algorithm based on dynamic
programming. We give a general $4$-approximation algorithm and show
APX-hardness. Thus, there is a small constant $\delta&gt;1$ such that every
polynomial time approximation algorithm has approximation factor at least
$\delta$.
  Our results also hold for a generalized version, where tokens and vertices
are colored. In this generalized version each token must go to a vertex with
the same color.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05151</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05151</id><created>2016-02-16</created><authors><author><keyname>Jancar</keyname><forenames>Petr</forenames></author></authors><title>Branching Bisimilarity of Normed BPA Processes as a Rational Monoid</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim is to highlight and elaborate the structural result for branching
bisimilarity on normed BPA (Basic Process Algebra) processes that was the crux
of a conference paper by Czerwinski and Jancar (arxiv 7/2014 and LiCS 2015
[CJ-2014,2015]). That paper focused on the computational complexity, and a
NEXPTIME-upper bound has been derived; the authors built on the ideas by Fu
(ICALP 2013), and strengthened his decidability result. Later He and Huang
announced the EXPTIME-completeness of this problem (arxiv 1/2015, and LiCS
2015), giving a technical proof for the EXPTIME membership. He and Huang
indirectly acknowledge the decomposition ideas in [CJ-2014] on which they also
built, but it is difficult for the reader to understand what was their starting
point and what are the crucial new ideas. One aim here is to present the
previous decomposition result of [CJ-2014,2015] in a technically new framework,
noting that branching bisimulation equivalence on normed BPA processes
corresponds to a rational monoid (in the sense of Sakarovitch); in particular
it is shown that the mentioned equivalence can be decided by normal-form
computing deterministic finite transducers. Another aim is to provide a
complete description, including an informal overview that should also make
clear how Fu's ideas were used, and to give all proofs in a form that should be
both rigorous and readable. The paper finishes by some remarks on the
computational complexity of the problem, also noting that no rigorous proof for
the EXPTIME-hardness has been given so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05152</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05152</id><created>2016-02-16</created><authors><author><keyname>Sumi</keyname><forenames>R&#xf3;bert</forenames></author><author><keyname>Varga</keyname><forenames>Melinda</forenames></author><author><keyname>Toroczkai</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Ercsey-Ravasz</keyname><forenames>M&#xe1;ria</forenames></author></authors><title>Order-to-chaos transition in the hardness of random Boolean
  satisfiability problems</title><categories>cs.CC cond-mat.stat-mech</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transient chaos is an ubiquitous phenomenon characterizing the dynamics of
phase space trajectories evolving towards a steady state attractor in physical
systems as diverse as fluids, chemical reactions and condensed matter systems.
Here we show that transient chaos also appears in the dynamics of certain
efficient algorithms searching for solutions of constraint satisfaction
problems that include scheduling, circuit design, routing, database problems or
even Sudoku. In particular, we present a study of the emergence of hardness in
Boolean satisfiability ($k$-SAT), a canonical class of constraint satisfaction
problems, by using an analog deterministic algorithm based on a system of
ordinary differential equations. Problem hardness is defined through the escape
rate $\kappa$, an invariant measure of transient chaos of the dynamical system
corresponding to the analog algorithm, and it expresses the rate at which the
trajectory approaches a solution.We show that for a given density of
constraints and fixed number of Boolean variables $N$, the hardness of formulas
in random $k$-SAT ensembles has a wide variation, approximable by a lognormal
distribution. We also show that when increasing the density of constraints
$\alpha$, hardness appears through a second-order phase transition at
$\alpha_{\chi}$ in the random 3-SAT ensemble where dynamical trajectories
become transiently chaotic. A similar behavior is found in 4-SAT as well,
however, such transition does not occur for 2-SAT. This behavior also implies a
novel type of transient chaos in which the escape rate has an
exponential-algebraic dependence on the critical parameter $\kappa \sim
N^{B|\alpha - \alpha_{\chi}|^{1-\gamma}}$ with $0&lt; \gamma &lt; 1$. We demonstrate
that the transition is generated by the appearance of metastable basins in the
solution space as the density of constraints $\alpha$ is increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05157</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05157</id><created>2016-02-16</created><updated>2016-02-18</updated><authors><author><keyname>Liu</keyname><forenames>Gangli</forenames></author></authors><title>A Ranking Algorithm for Re-finding</title><categories>cs.IR</categories><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Re-finding files from a personal computer is a frequent demand to users. When
encountered a difficult re-finding task, people may not recall the attributes
used by conventional re-finding methods, such as a file's path, file name,
keywords etc., the re-finding would fail.
  We proposed a method to support difficult re-finding tasks. By asking the
user a list of questions about the target, such as a document's pages, author
numbers, accumulated reading time, last reading location etc. Then use the
user's answers to filter out the target.
  After the user answered a list of questions about the target file, we
evaluate the user's familiar degree about the target file based on the answers.
We devise a ranking algorithm which sorts the candidates by comparing the
user's familiarity degree about the target and the candidates.
  We also propose a method to generate re-finding tasks artificially based on
the user's own document corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05161</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05161</id><created>2016-02-16</created><authors><author><keyname>Raz</keyname><forenames>Ran</forenames></author></authors><title>Fast Learning Requires Good Memory: A Time-Space Lower Bound for Parity
  Learning</title><categories>cs.LG cs.CC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that any algorithm for learning parities requires either a memory of
quadratic size or an exponential number of samples. This proves a recent
conjecture of Steinhardt, Valiant and Wager and shows that for some learning
problems a large storage space is crucial.
  More formally, in the problem of parity learning, an unknown string $x \in
\{0,1\}^n$ was chosen uniformly at random. A learner tries to learn $x$ from a
stream of samples $(a_1, b_1), (a_2, b_2) \ldots$, where each~$a_t$ is
uniformly distributed over $\{0,1\}^n$ and $b_t$ is the inner product of $a_t$
and $x$, modulo~2. We show that any algorithm for parity learning, that uses
less than $\frac{n^2}{25}$ bits of memory, requires an exponential number of
samples.
  Previously, there was no non-trivial lower bound on the number of samples
needed, for any learning problem, even if the allowed memory size is $O(n)$
(where $n$ is the space needed to store one sample).
  We also give an application of our result in the field of bounded-storage
cryptography. We show an encryption scheme that requires a private key of
length $n$, as well as time complexity of $n$ per encryption/decription of each
bit, and is provenly and unconditionally secure as long as the attacker uses
less than $\frac{n^2}{25}$ memory bits and the scheme is used at most an
exponential number of times. Previous works on bounded-storage cryptography
assumed that the memory size used by the attacker is at most linear in the time
needed for encryption/decription.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05163</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05163</id><created>2016-02-16</created><authors><author><keyname>Harper</keyname><forenames>K. Eric</forenames></author><author><keyname>de Gooijer</keyname><forenames>Thijmen</forenames></author><author><keyname>Smiley</keyname><forenames>Karen</forenames></author></authors><title>Composable Industrial Internet Applications for Tiered Architectures</title><categories>cs.CY cs.DC</categories><comments>10 pages, 10 figures, 30 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A single vendor cannot provide complete IIoT end-to-end solutions because
cooperation is required from multiple parties. Interoperability is a key
architectural quality. Composability of capabilities, information and
configuration is the prerequisite for interoperability, supported by a data
storage infrastructure and defined set of interfaces to build applications.
Secure collection, transport and storage of data and algorithms are
expectations for collaborative participation in any IIoT solution. Participants
require control of their data ownership and confidentiality. We propose an
Internet of Things, Services and People (IoTSP) application development and
management framework which includes components for data storage, algorithm
design and packaging, and computation execution. Applications use clusters of
platform services, organized in tiers, and local access to data to reduce
complexity and enhance reliable data exchange. Since communication is less
reliable across tiers, data is synchronized between storage replicas when
communication is available. The platform services provide a common ecosystem to
exchange data uniting data storage, applications, and components that process
the data. Configuration and orchestration of the tiers are managed using shared
tools and facilities. The platform promotes the data storage components to be
peers of the applications where each data owner is in control of when and how
much information is shared with a service provider. The service components and
applications are securely integrated using local event and data exchange
communication channels. This tiered architecture reduces the cyber attack
surface and enables individual tiers to operate autonomously, while addressing
interoperability concerns. We present our framework using predictive
maintenance as an example, and evaluate compatibility of our vision with an
emerging set of standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05168</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05168</id><created>2016-02-16</created><authors><author><keyname>Chaudhary</keyname><forenames>Rashi</forenames></author><author><keyname>Dasgupta</keyname><forenames>Himanshu</forenames></author></authors><title>An Approach for Noise Removal on Depth Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image based rendering is a fundamental problem in computer vision and
graphics. Modern techniques often rely on depth image for the 3D construction.
However for most of the existing depth cameras, the large and unpredictable
noises can be problematic, which can cause noticeable artifacts in the rendered
results. In this paper, we proposed an efficacious method for depth image noise
removal that can be applied for most RGBD systems. The proposed solution will
benefit many subsequent vision problems such as 3D reconstruction, novel view
rendering, object recognition. Our experimental results demonstrate the
efficacy and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05170</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05170</id><created>2016-02-09</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author></authors><title>Applied Logic in Engineering</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logic not only helps to solve complicated and safety-critical problems, but
also disciplines the mind and helps to develop abstract thinking, which is very
important for any area of Engineering. In this technical report, we present an
overview of common challenges in teaching of formal methods and discuss our
experiences from the course Applied Logic in Engineering. This course was
taught at TU Munich, Germany, in Winter Semester 2012/2013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05179</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05179</id><created>2016-02-16</created><updated>2016-02-24</updated><authors><author><keyname>Scellier</keyname><forenames>Benjamin</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Towards a Biologically Plausible Backprop</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work follows Bengio and Fischer (2015) in which theoretical foundations
were laid to show how iterative inference can backpropagate error signals.
Neurons move their activations towards configurations corresponding to lower
energy and smaller prediction error: a new observation creates a perturbation
at visible neurons that propagates into hidden layers, with these propagated
perturbations corresponding to the back-propagated gradient. This avoids the
need for a lengthy relaxation in the positive phase of training (when both
inputs and targets are observed), as was believed with previous work on
fixed-point recurrent networks. We show experimentally that energy-based neural
networks with several hidden layers can be trained at discriminative tasks by
using iterative inference and an STDP-like learning rule. The main result of
this paper is that we can train neural networks with 1, 2 and 3 hidden layers
on the permutation-invariant MNIST task and get the training error down to
0.00%. The results presented here make it more biologically plausible that a
mechanism similar to back-propagation may take place in brains in order to
achieve credit assignment in deep networks. The paper also discusses some of
the remaining open problems to achieve a biologically plausible implementation
of backprop in brains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05181</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05181</id><created>2016-02-16</created><authors><author><keyname>Biswas</keyname><forenames>Arindam</forenames></author></authors><title>A Simple Condition for the Existence of Transversals</title><categories>cs.DM math.CO</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Hall's Theorem is a basic result in Combinatorics which states that the
obvious necesssary condition for a finite family of sets to have a transversal
is also sufficient. We present a sufficient (but not necessary) condition on
the sizes of the sets in the family and the sizes of their intersections so
that a transversal exists. Using this, we prove that in a bipartite graph $G$
(bipartition $\{A, B\}$), without 4-cycles, if $\deg(v) \geq \sqrt{2e|A|}$ for
all $v \in A$, then $G$ has a matching of size $|A|$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05183</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05183</id><created>2016-02-16</created><authors><author><keyname>Rousseau</keyname><forenames>Ronald</forenames></author><author><keyname>Rahman</keyname><forenames>A. I. M. Jakaria</forenames></author><author><keyname>Guns</keyname><forenames>Raf</forenames></author><author><keyname>Engels</keyname><forenames>Tim C. E.</forenames></author></authors><title>Similarity adapted publication vectors: A note and a correction on
  measuring cognitive distance in multiple dimensions</title><categories>cs.DL cs.IR</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous article (Rahman, Guns, Rousseau, and Engels, 2015) we described
several approaches to determine the cognitive distance between two units. One
of these approaches was based on what we called barycenters in N dimensions.
The present note corrects this terminology and introduces the more adequate
term similarity-adapted publication vectors. Furthermore, we correct an error
in normalization and explain the importance of scale invariance in determining
cognitive distance. Overall, we find rather strong correlations between
cognitive distances obtained by the 2-dimensional (barycenters) and the
N-dimensional (similarity-adapted publication vectors) approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05205</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05205</id><created>2016-02-16</created><authors><author><keyname>D&#xfc;nner</keyname><forenames>Celestine</forenames></author><author><keyname>Forte</keyname><forenames>Simone</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Jaggi</keyname><forenames>Martin</forenames></author></authors><title>Primal-Dual Rates and Certificates</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm-independent framework to equip existing optimization
methods with primal-dual certificates. Such certificates and corresponding rate
of convergence guarantees are important for practitioners to diagnose progress,
in particular in machine learning applications. We obtain new primal-dual
convergence rates e.g. for the Lasso as well as many L1, Elastic-Net and
group-lasso-regularized problems. The theory applies to any norm-regularized
generalized linear model. Our approach provides efficiently computable duality
gaps which are globally defined, without modifying the original problems in the
region of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05209</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05209</id><created>2016-02-16</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Terriberry</keyname><forenames>Timothy B.</forenames></author></authors><title>Perceptual Vector Quantization For Video Coding</title><categories>cs.MM</categories><comments>11 pages, Proceedings of SPIE Visual Information Processing and
  Communication, 2015</comments><journal-ref>Proc. SPIE 9410, Visual Information Processing and Communication
  VI, 941009 (March 4, 2015)</journal-ref><doi>10.1117/12.2080529</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper applies energy conservation principles to the Daala video codec
using gain-shape vector quantization to encode a vector of AC coefficients as a
length (gain) and direction (shape). The technique originates from the CELT
mode of the Opus audio codec, where it is used to conserve the spectral
envelope of an audio signal. Conserving energy in video has the potential to
preserve textures rather than low-passing them. Explicitly quantizing a gain
allows a simple contrast masking model with no signaling cost. Vector
quantizing the shape keeps the number of degrees of freedom the same as scalar
quantization, avoiding redundancy in the representation. We demonstrate how to
predict the vector by transforming the space it is encoded in, rather than
subtracting off the predictor, which would make energy conservation impossible.
We also derive an encoding of the vector-quantized codewords that takes
advantage of their non-uniform distribution. We show that the resulting
technique outperforms scalar quantization by an average of 0.90 dB on still
images, equivalent to a 24.8% reduction in bitrate at equal quality, while for
videos, the improvement averages 0.83 dB, equivalent to a 13.7% reduction in
bitrate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05217</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05217</id><created>2016-02-16</created><authors><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author><author><keyname>Schneider</keyname><forenames>Reinhold</forenames></author><author><keyname>Stojanac</keyname><forenames>Zeljka</forenames></author></authors><title>Low rank tensor recovery via iterative hard thresholding</title><categories>cs.IT math.IT math.NA math.PR</categories><comments>34 pages</comments><msc-class>15A69, 15B52, 65Fxx, 94A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study extensions of compressive sensing and low rank matrix recovery
(matrix completion) to the recovery of low rank tensors of higher order from a
small number of linear measurements. While the theoretical understanding of low
rank matrix recovery is already well-developed, only few contributions on the
low rank tensor recovery problem are available so far. In this paper, we
introduce versions of the iterative hard thresholding algorithm for several
tensor decompositions, namely the higher order singular value decomposition
(HOSVD), the tensor train format (TT), and the general hierarchical Tucker
decomposition (HT). We provide a partial convergence result for these
algorithms which is based on a variant of the restricted isometry property of
the measurement operator adapted to the tensor decomposition at hand that
induces a corresponding notion of tensor rank. We show that subgaussian
measurement ensembles satisfy the tensor restricted isometry property with high
probability under a certain almost optimal bound on the number of measurements
which depends on the corresponding tensor format. These bounds are extended to
partial Fourier maps combined with random sign flips of the tensor entries.
Finally, we illustrate the performance of iterative hard thresholding methods
for tensor recovery via numerical experiments where we consider recovery from
Gaussian random measurements, tensor completion (recovery of missing entries),
and Fourier measurements for third order tensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05220</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05220</id><created>2016-02-16</created><authors><author><keyname>Eliasmith</keyname><forenames>Chris</forenames></author><author><keyname>Gosmann</keyname><forenames>Jan</forenames></author><author><keyname>Choo</keyname><forenames>Xuan</forenames></author></authors><title>BioSpaun: A large-scale behaving brain model with complex neurons</title><categories>q-bio.NC cs.AI</categories><comments>17 pages 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a large-scale functional brain model that includes detailed,
conductance-based, compartmental models of individual neurons. We call the
model BioSpaun, to indicate the increased biological plausibility of these
neurons, and because it is a direct extension of the Spaun model
\cite{Eliasmith2012b}. We demonstrate that including these detailed
compartmental models does not adversely affect performance across a variety of
tasks, including digit recognition, serial working memory, and counting. We
then explore the effects of applying TTX, a sodium channel blocking drug, to
the model. We characterize the behavioral changes that result from this
molecular level intervention. We believe this is the first demonstration of a
large-scale brain model that clearly links low-level molecular interventions
and high-level behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05231</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05231</id><created>2016-02-16</created><authors><author><keyname>Shubbak</keyname><forenames>Mahmood H.</forenames></author><author><keyname>Thorne</keyname><forenames>Simon</forenames></author></authors><title>Development and Experimentation of a Software Tool for Identifying High
  Risk Spreadsheets for Auditing</title><categories>cs.SE</categories><comments>22 pages, 11 Colour Figures, 4 Tables</comments><proxy>Grenville Croll</proxy><journal-ref>Proc. 16th EuSpRIG Conf. &quot;Spreadsheet Risk Management&quot; (2015)
  pp47-78 ISBN: 978-1-905404-52-0</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heavy use of spreadsheets by organisations bears many potential risks such as
errors, ambiguity, data loss, duplication, and fraud. In this paper these risks
are briefly outlined along with their available mitigation methods such as:
documentation, centralisation, auditing and user training. However, because of
the large quantities of spreadsheets used in organisations, applying these
methods on all spreadsheets is impossible. This fact is considered as a
deficiency in these methods, a gap which is addressed in this paper.
  In this paper a new software tool for managing spreadsheets and identifying
the risk levels they include is proposed, developed and tested. As an add-in
for Microsoft Excel application, &quot;Risk Calculator&quot; can automatically collect
and record spreadsheet properties in an inventory database and assign risk
scores based on their importance, use and complexity. Consequently, auditing
processes can be targeted to high risk spreadsheets. Such a method saves time,
effort, and money.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05232</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05232</id><created>2016-02-16</created><authors><author><keyname>Simsiri</keyname><forenames>Natcha</forenames></author><author><keyname>Tangwongsan</keyname><forenames>Kanat</forenames></author><author><keyname>Tirthapura</keyname><forenames>Srikanta</forenames></author><author><keyname>Wu</keyname><forenames>Kun-Lung</forenames></author></authors><title>Work-Efficient Parallel and Incremental Graph Connectivity</title><categories>cs.DS cs.DC</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On an evolving graph that is continuously updated by a high-velocity stream
of edges, how can one efficiently maintain if two vertices are connected? This
is the connectivity problem, a fundamental and widely studied problem on
graphs. We present the first shared-memory parallel algorithm for incremental
graph connectivity that is both provably work-efficient and has polylogarithmic
parallel depth. We also present a simpler algorithm with slightly worse
theoretical properties, but which is easier to implement and has good practical
performance. Our experiments show a throughput of hundreds of millions of edges
per second on a $20$-core machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05237</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05237</id><created>2016-02-16</created><authors><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author><author><keyname>Irfan</keyname><forenames>Mohammad T.</forenames></author></authors><title>FPTAS for Mixed-Strategy Nash Equilibria in Tree Graphical Games and
  Their Generalizations</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first FPTAS for computing an approximate mixed-strategy Nash
Equilibrium (MSNE) in graphical multi-hypermatrix games, which are
generalizations of normal-form games, graphical games (GGs), graphical
polymatrix games, and hypergraphical games. The computational complexity of
graphical polymatrix games, or polymatrix GGs for short, has been of great
interest in the computational/algorithmic game theory community. The exact-MSNE
formulation of the problem for polymatrix GGs is PPAD-compete, thus generally
believed intractable, even for binary-action games with tree graphs. In
contrast, to the best of our knowledge, we are the first to establish a
corollary FPTAS (or quasi-PTAS) for tree polymatrix and normal-form GGs with
the number of actions bounded by a constant (or a logarithm of the number of
players, respectively).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05240</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05240</id><created>2016-02-16</created><authors><author><keyname>He</keyname><forenames>Xinran</forenames></author><author><keyname>Kempe</keyname><forenames>David</forenames></author></authors><title>Robust Influence Maximization</title><categories>cs.SI physics.soc-ph</categories><report-no>usc2016</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncertainty about models and data is ubiquitous in the computational social
sciences, and it creates a need for robust social network algorithms, which can
simultaneously provide guarantees across a spectrum of models and parameter
settings. We begin an investigation into this broad domain by studying robust
algorithms for the Influence Maximization problem, in which the goal is to
identify a set of k nodes in a social network whose joint influence on the
network is maximized.
  We define a Robust Influence Maximization framework wherein an algorithm is
presented with a set of influence functions, typically derived from different
influence models or different parameter settings for the same model. The
different parameter settings could be derived from observed cascades on
different topics, under different conditions, or at different times. The
algorithm's goal is to identify a set of k nodes who are simultaneously
influential for all influence functions, compared to the (function-specific)
optimum solutions.
  We show strong approximation hardness results for this problem unless the
algorithm gets to select at least a logarithmic factor more seeds than the
optimum solution. However, when enough extra seeds may be selected, we show
that techniques of Krause et al. can be used to approximate the optimum robust
influence to within a factor of 1 - 1/e. We evaluate this bicriteria
approximation algorithm against natural heuristics on several real-world data
sets. Our experiments indicate that the worst-case hardness does not
necessarily translate into bad performance on real-world data sets; all
algorithms perform fairly well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05242</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05242</id><created>2016-02-16</created><updated>2016-02-17</updated><authors><author><keyname>Anari</keyname><forenames>Nima</forenames></author><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Rezaei</keyname><forenames>Alireza</forenames></author></authors><title>Monte Carlo Markov Chains Algorithms for Sampling Strongly Rayleigh
  Distributions and Determinantal Point Processes</title><categories>cs.LG cs.DS math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strongly Rayleigh distributions are natural generalizations of product and
determinantal probability distributions and satisfy strongest form of negative
dependence properties. We show that the &quot;natural&quot; Monte Carlo Markov Chain
(MCMC) is rapidly mixing in the support of a {\em homogeneous} strongly
Rayleigh distribution. As a byproduct, our proof implies Markov chains can be
used to efficiently generate approximate samples of a $k$-determinantal point
process. This answers an open question raised by Deshpande and Rademacher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05248</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05248</id><created>2016-02-16</created><authors><author><keyname>Pereira</keyname><forenames>Toby</forenames></author></authors><title>Proportional Approval Method using Squared loads, Approval removal and
  Coin-flip approval transformation (PAMSAC) - a new system of proportional
  representation using approval voting</title><categories>cs.GT</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several multi-winner systems that use approval voting have been developed but
they each suffer from various problems. Six of these methods are discussed in
this paper. They are Satisfaction Approval Voting, Minimax Approval Voting,
Proportional Approval Voting, Monroe's Fully Proportional Representation,
Chamberlin-Courant's Rule, and Ebert's method. They all fail at least one of
Proportional Representation (PR), strong PR, monotonicity or positive support.
However, the new method described in this paper - Proportional Approval Method
using Squared loads, Approval removal and Coin-flip approval transformation
(PAMSAC) - passes them all. PAMSAC uses the squared loads of Ebert's method,
but removes non-beneficial approvals to restore monotonicity. It also uses the
Coin-Flip Approval Transformation (CFAT), where voters are &quot;split&quot; into two for
each candidate they approve, and where one half of this split voter approves
and the other half does not approve each candidate approved on the ballot. This
restores positive support, and also makes the method equivalent to the D'Hondt
party-list method for party voting. PAMSAC reduces to simple approval voting in
the single-winner case. A score voting version is described that also reduces
to simple score voting in the single-winner case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05256</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05256</id><created>2016-02-16</created><authors><author><keyname>Shanklin</keyname><forenames>Wichai</forenames></author></authors><title>2D SEM images turn into 3D object models</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scanning electron microscopy (SEM) is probably one the most fascinating
examination approach that has been used since more than two decades to detailed
inspection of micro scale objects. Most of the scanning electron microscopes
could only produce 2D images that could not assist operational analysis of
microscopic surface properties. Computer vision algorithms combined with very
advanced geometry and mathematical approaches turn any SEM into a full 3D
measurement device. This work focuses on a methodical literature review for
automatic 3D surface reconstruction of scanning electron microscope images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05257</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05257</id><created>2016-02-16</created><authors><author><keyname>Kakde</keyname><forenames>Deovrat</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Arin</forenames></author><author><keyname>Kong</keyname><forenames>Seunghyun</forenames></author><author><keyname>Jahja</keyname><forenames>Maria</forenames></author><author><keyname>Jiang</keyname><forenames>Hansi</forenames></author><author><keyname>Silva</keyname><forenames>Jorge</forenames></author></authors><title>Peak Criterion for Choosing Gaussian Kernel Bandwidth in Support Vector
  Data Description</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support Vector Data Description (SVDD) is a machine-learning technique used
for single class classification and outlier detection. SVDD formulation with
kernel function provides a flexible boundary around data. The value of kernel
function parameters affects the nature of the data boundary. For example, it is
observed that with a Gaussian kernel, as the value of kernel bandwidth is
lowered, the data boundary changes from spherical to wiggly. The spherical data
boundary leads to underfitting, and an extremely wiggly data boundary leads to
overfitting. In this paper, we propose empirical criterion to obtain good
values of the Gaussian kernel bandwidth parameter. This criterion provides a
smooth boundary that captures the essential geometric features of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05263</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05263</id><created>2016-02-16</created><authors><author><keyname>Fotakis</keyname><forenames>Dimitris</forenames></author><author><keyname>Milis</keyname><forenames>Ioannis</forenames></author><author><keyname>Papadigenopoulos</keyname><forenames>Orestis</forenames></author><author><keyname>Vassalos</keyname><forenames>Vasilis</forenames></author><author><keyname>Zois</keyname><forenames>Georgios</forenames></author></authors><title>Scheduling MapReduce Jobs under Multi-Round Precedences</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider non-preemptive scheduling of MapReduce jobs with multiple tasks
in the practical scenario where each job requires several map-reduce rounds. We
seek to minimize the average weighted completion time and consider scheduling
on identical and unrelated parallel processors. For identical processors, we
present LP-based O(1)-approximation algorithms. For unrelated processors, the
approximation ratio naturally depends on the maximum number of rounds of any
job. Since the number of rounds per job in typical MapReduce algorithms is a
small constant, our scheduling algorithms achieve a small approximation ratio
in practice. For the single-round case, we substantially improve on previously
best known approximation guarantees for both identical and unrelated
processors. Moreover, we conduct an experimental analysis and compare the
performance of our algorithms against a fast heuristic and a lower bound on the
optimal solution, thus demonstrating their promising practical performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05264</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05264</id><created>2016-02-16</created><authors><author><keyname>Chhabra</keyname><forenames>Puneet S</forenames></author><author><keyname>Wallace</keyname><forenames>Andrew M</forenames></author><author><keyname>Hopgood</keyname><forenames>James R</forenames></author></authors><title>Anomaly Detection in Clutter using Spectrally Enhanced Ladar</title><categories>physics.optics cs.LG physics.ins-det stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete return (DR) Laser Detection and Ranging (Ladar) systems provide a
series of echoes that reflect from objects in a scene. These can be first, last
or multi-echo returns. In contrast, Full-Waveform (FW)-Ladar systems measure
the intensity of light reflected from objects continuously over a period of
time. In a camouflaged scenario, e.g., objects hidden behind dense foliage, a
FW-Ladar penetrates such foliage and returns a sequence of echoes including
buried faint echoes. The aim of this paper is to learn local-patterns of
co-occurring echoes characterised by their measured spectra. A deviation from
such patterns defines an abnormal event in a forest/tree depth profile. As far
as the authors know, neither DR or FW-Ladar, along with several spectral
measurements, has not been applied to anomaly detection. This work presents an
algorithm that allows detection of spectral and temporal anomalies in FW-Multi
Spectral Ladar (FW-MSL) data samples. An anomaly is defined as a full waveform
temporal and spectral signature that does not conform to a prior expectation,
represented using a learnt subspace (dictionary) and set of coefficients that
capture co-occurring local-patterns using an overlapping temporal window. A
modified optimization scheme is proposed for subspace learning based on
stochastic approximations. The objective function is augmented with a
discriminative term that represents the subspace's separability properties and
supports anomaly characterisation. The algorithm detects several man-made
objects and anomalous spectra hidden in a dense clutter of vegetation and also
allows tree species classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05281</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05281</id><created>2016-02-16</created><authors><author><keyname>Liu</keyname><forenames>Kun</forenames></author><author><keyname>Fridman</keyname><forenames>Emilia</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author><author><keyname>Xia</keyname><forenames>Yuanqing</forenames></author></authors><title>Generalized Jensen Inequalities with Application to Stability Analysis
  of Systems with Distributed Delays over Infinite Time-Horizons</title><categories>cs.SY</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Jensen inequality has been recognized as a powerful tool to deal with the
stability of time-delay systems. Recently, a new inequality that encompasses
the Jensen inequality was proposed for the stability analysis of systems with
finite delays. In this paper, we first present a generalized integral
inequality and its double integral extension. It is shown how these
inequalities can be applied to improve the stability result for linear
continuous-time systems with gamma-distributed delays. Then, for the
discrete-time counterpart we provide an extended Jensen summation inequality
with infinite sequences, which leads to less conservative stability conditions
for linear discrete-time systems with poisson-distributed delays. The
improvements obtained thanks to the introduced generalized inequalities are
demonstrated by examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05285</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05285</id><created>2016-02-16</created><authors><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Choice by Elimination via Deep Neural Networks</title><categories>stat.ML cs.IR cs.LG</categories><comments>PAKDD workshop on Biologically Inspired Techniques for Data Mining
  (BDM'16)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Neural Choice by Elimination, a new framework that integrates
deep neural networks into probabilistic sequential choice models for learning
to rank. Given a set of items to chose from, the elimination strategy starts
with the whole item set and iteratively eliminates the least worthy item in the
remaining subset. We prove that the choice by elimination is equivalent to
marginalizing out the random Gompertz latent utilities. Coupled with the choice
model is the recently introduced Neural Highway Networks for approximating
arbitrarily complex rank functions. We evaluate the proposed framework on a
large-scale public dataset with over 425K items, drawn from the Yahoo! learning
to rank challenge. It is demonstrated that the proposed method is competitive
against state-of-the-art learning to rank methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05286</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05286</id><created>2016-02-16</created><authors><author><keyname>Jia</keyname><forenames>Songwei</forenames></author><author><keyname>Gao</keyname><forenames>Lin</forenames></author><author><keyname>Gao</keyname><forenames>Yong</forenames></author><author><keyname>Nastos</keyname><forenames>James</forenames></author><author><keyname>Wen</keyname><forenames>Xiao</forenames></author><author><keyname>Zhang</keyname><forenames>Xindong</forenames></author><author><keyname>Wang</keyname><forenames>Haiyang</forenames></author></authors><title>Exploring triad-rich substructures by graph-theoretic characterizations
  in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>41 pages, 14 figures, and now underreviewing by Journal of
  Statistical Mechanics: Theory and Experiment</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important problems in complex networks is how to detect
metadata groups accurately. The main challenge lies in the fact that
traditional structural communities do not always capture the intrinsic features
of metadata groups. Motivated by the observation that metadata groups in PPI
networks tend to consist of an abundance of interacting triad motifs, we define
a 2-club substructure with diameter 2 which possessing triad-rich property to
describe a metadata group. Based on the triad-rich substructure, we design a
DIVision Algorithm using our proposed edge Niche Centrality DIVANC to detect
metadata groups effectively in complex networks. We also extend DIVANC to
detect overlapping metadata groups by proposing a simple 2-hop overlapping
strategy. To verify the effectiveness of triad-rich substructures, we compare
DIVANC with existing algorithms on PPI networks, LFR synthetic networks and
football networks. The experimental results show that DIVANC outperforms most
other algorithms significantly and, in particular, can detect sparse metadata
groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05287</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05287</id><created>2016-02-16</created><authors><author><keyname>Shirani</keyname><forenames>F.</forenames></author><author><keyname>Pradhan</keyname><forenames>S. S.</forenames></author></authors><title>Trade-off between Communication and Cooperation in the Interference
  Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of coding over the multi-user Interference Channel
(IC). It is well-known that aligning the interfering signals results in
improved achievable rates in certain setups involving more than two users. We
argue that in the general interference problem, senders face a tradeoff between
communicating their message to their corresponding decoder or cooperating with
other users by aligning their signals. Traditionally, interference alignment is
carried out using structured codes such as linear codes and group codes. We
show through an example that the usual structured coding schemes used for
interference neutralization lack the necessary flexibility to optimize this
tradeoff. Based on this intuition, we propose a new class of codes for this
problem. We use the example to show that the application of these codes gives
strict improvements in terms of achievable rates. Finally, we derive a new
achievable region for the three user IC which strictly improves upon the
previously known inner bounds for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05292</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05292</id><created>2016-02-16</created><authors><author><keyname>Ge</keyname><forenames>Zhenhao</forenames></author><author><keyname>Sun</keyname><forenames>Yufang</forenames></author><author><keyname>Smith</keyname><forenames>Mark J. T.</forenames></author></authors><title>Authorship Attribution Using a Neural Network Language Model</title><categories>cs.CL cs.AI</categories><comments>Proceedings of the 30th AAAI Conference on Artificial Intelligence
  (AAAI'16)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practice, training language models for individual authors is often
expensive because of limited data resources. In such cases, Neural Network
Language Models (NNLMs), generally outperform the traditional non-parametric
N-gram models. Here we investigate the performance of a feed-forward NNLM on an
authorship attribution problem, with moderate author set size and relatively
limited data. We also consider how the text topics impact performance. Compared
with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the
proposed method achieves nearly 2:5% reduction in perplexity and increases
author classification accuracy by 3:43% on average, given as few as 5 test
sentences. The performance is very competitive with the state of the art in
terms of accuracy and demand on test data. The source code, preprocessed
datasets, a detailed description of the methodology and results are available
at https://github.com/zge/authorship-attribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05305</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05305</id><created>2016-02-17</created><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author><author><keyname>Lee</keyname><forenames>Sanghyuk</forenames></author><author><keyname>Lim</keyname><forenames>Eng Gee</forenames></author></authors><title>Simulation Study of an Energy-Efficient Time Synchronization Scheme
  based on Source Clock Frequency Recovery in Asymmetric Wireless Sensor
  Networks</title><categories>cs.NI stat.AP</categories><comments>4 pages, 2 figures, 6th International Symposium on Advanced
  Engineering (ISAE 2015), Pukyong National University, Busan, Korea, 22-24
  Oct. 2015. arXiv admin note: substantial text overlap with arXiv:1508.02708</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we report preliminary results of a simulation study on an
energy-efficient time synchronization scheme based on source clock frequency
recovery (SCFR) at sensor nodes in asymmetric wireless sensor networks (WSNs),
where a head node --- serving as a gateway between wired and wireless networks
--- is equipped with a powerful processor and supplied power from outlet, and
sensor nodes --- connected only through wireless channels --- are limited in
processing and battery-powered. In the SCFR-based WSN time synchronization
scheme, we concentrate on battery-powered sensor nodes and reduce their energy
consumption by minimizing the number of message transmissions from sensor nodes
to the head node. Through simulation experiments we analyze the performance of
the SCFR-based WSN time synchronization scheme, including the impact of SCFR on
time synchronization based on two-way message exchange, and demonstrate the
feasibility of the proposed time synchronization scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05307</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05307</id><created>2016-02-17</created><authors><author><keyname>Ren</keyname><forenames>Xiang</forenames></author><author><keyname>He</keyname><forenames>Wenqi</forenames></author><author><keyname>Qu</keyname><forenames>Meng</forenames></author><author><keyname>Voss</keyname><forenames>Clare R.</forenames></author><author><keyname>Ji</keyname><forenames>Heng</forenames></author><author><keyname>Han</keyname><forenames>Jiawei</forenames></author></authors><title>Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label
  Embedding</title><categories>cs.CL cs.LG</categories><comments>Submitted to KDD 2016. 11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current systems of fine-grained entity typing use distant supervision in
conjunction with existing knowledge bases to assign categories (type labels) to
entity mentions. However, the type labels so obtained from knowledge bases are
often noisy (i.e., incorrect for the entity mention's local context). We define
a new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic
identification of correct type labels (type-paths) for training examples, given
the set of candidate type labels obtained by distant supervision with a given
type hierarchy. The unknown type labels for individual entity mentions and the
semantic similarity between entity types pose unique challenges for solving the
LNR task. We propose a general framework, called PLE, to jointly embed entity
mentions, text features and entity types into the same low-dimensional space
where, in that space, objects whose types are semantically close have similar
representations. Then we estimate the type-path for each training example in a
top-down manner using the learned embeddings. We formulate a global objective
for learning the embeddings from text corpora and knowledge bases, which adopts
a novel margin-based loss that is robust to noisy labels and faithfully models
type correlation derived from knowledge bases. Our experiments on three public
typing datasets demonstrate the effectiveness and robustness of PLE, with an
average of 25% improvement in accuracy compared to next best method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05310</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05310</id><created>2016-02-17</created><authors><author><keyname>Tu</keyname><forenames>Stephen</forenames></author><author><keyname>Roelofs</keyname><forenames>Rebecca</forenames></author><author><keyname>Venkataraman</keyname><forenames>Shivaram</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Large Scale Kernel Learning using Block Coordinate Descent</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that distributed block coordinate descent can quickly solve
kernel regression and classification problems with millions of data points.
Armed with this capability, we conduct a thorough comparison between the full
kernel, the Nystr\&quot;om method, and random features on three large classification
tasks from various domains. Our results suggest that the Nystr\&quot;om method
generally achieves better statistical accuracy than random features, but can
require significantly more iterations of optimization. Lastly, we derive new
rates for block coordinate descent which support our experimental findings when
specialized to kernel methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05311</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05311</id><created>2016-02-17</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Terriberry</keyname><forenames>Timothy B.</forenames></author><author><keyname>Maxwell</keyname><forenames>Gregory</forenames></author></authors><title>A Full-Bandwidth Audio Codec With Low Complexity And Very Low Delay</title><categories>cs.MM cs.SD</categories><comments>5 pages, Proceedings of EUSIPCO 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an audio codec that addresses the low-delay requirements of some
applications such as network music performance. The codec is based on the
modified discrete cosine transform (MDCT) with very short frames and uses
gain-shape quantization to preserve the spectral envelope. The short frame
sizes required for low delay typically hinder the performance of transform
codecs. However, at 96 kbit/s and with only 4 ms algorithmic delay, the
proposed codec out-performs the ULD codec operating at the same rate. The total
complexity of the codec is small, at only 17 WMOPS for real-time operation at
48 kHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05312</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05312</id><created>2016-02-17</created><authors><author><keyname>Zaman</keyname><forenames>Faisal</forenames></author><author><keyname>Wong</keyname><forenames>Ya Ping</forenames></author><author><keyname>Ng</keyname><forenames>Boon Yian</forenames></author></authors><title>Density-based Denoising of Point Cloud</title><categories>cs.CV</categories><comments>9 pages, 5 figures, to be appeared in the Proceeding of 9th
  International Conference on Robotics, Vision, Signal Processing &amp; Power
  Applications (ROVISP), 2-3 Feb 2016, Penang, Malaysia</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Point cloud source data for surface reconstruction is usually contaminated
with noise and outliers. To overcome this deficiency, a density-based point
cloud denoising method is presented to remove outliers and noisy points. First,
particle-swam optimization technique is employed for automatically
approximating optimal bandwidth of multivariate kernel density estimation to
ensure the robust performance of density estimation. Then, mean-shift based
clustering technique is used to remove outliers through a thresholding scheme.
After removing outliers from the point cloud, bilateral mesh filtering is
applied to smooth the remaining points. The experimental results show that this
approach, comparably, is robust and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05314</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05314</id><created>2016-02-17</created><authors><author><keyname>Weyand</keyname><forenames>Tobias</forenames></author><author><keyname>Kostrikov</keyname><forenames>Ilya</forenames></author><author><keyname>Philbin</keyname><forenames>James</forenames></author></authors><title>PlaNet - Photo Geolocation with Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it possible to build a system to determine the location where a photo was
taken using just its pixels? In general, the problem seems exceptionally
difficult: it is trivial to construct situations where no location can be
inferred. Yet images often contain informative cues such as landmarks, weather
patterns, vegetation, road markings, and architectural details, which in
combination may allow one to determine an approximate location and occasionally
an exact location. Websites such as GeoGuessr and View from your Window suggest
that humans are relatively good at integrating these cues to geolocate images,
especially en-masse. In computer vision, the photo geolocation problem is
usually approached using image retrieval methods. In contrast, we pose the
problem as one of classification by subdividing the surface of the earth into
thousands of multi-scale geographic cells, and train a deep network using
millions of geotagged images. While previous approaches only recognize
landmarks or perform approximate matching using global image descriptors, our
model is able to use and integrate multiple visible cues. We show that the
resulting model, called PlaNet, outperforms previous approaches and even
attains superhuman levels of accuracy in some cases. Moreover, we extend our
model to photo albums by combining it with a long short-term memory (LSTM)
architecture. By learning to exploit temporal coherence to geolocate uncertain
photos, we demonstrate that this model achieves a 50% performance improvement
over the single-image model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05318</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05318</id><created>2016-02-17</created><authors><author><keyname>Chandrasekharan</keyname><forenames>Sathyanarayanan</forenames></author><author><keyname>Gomez</keyname><forenames>Karina</forenames></author><author><keyname>Al-Hourani</keyname><forenames>Akram</forenames></author><author><keyname>Kandeepan</keyname><forenames>Sithamparanathan</forenames></author><author><keyname>Rasheed</keyname><forenames>Tinku</forenames></author><author><keyname>Goratti</keyname><forenames>Leonardo</forenames></author><author><keyname>Reynaud</keyname><forenames>Laurent</forenames></author><author><keyname>Grace</keyname><forenames>David</forenames></author><author><keyname>Bucaille</keyname><forenames>Isabelle</forenames></author><author><keyname>Wirth</keyname><forenames>Thomas</forenames></author><author><keyname>Allsopp</keyname><forenames>Sandy</forenames></author></authors><title>Designing and Implementing Future Aerial Communication Networks</title><categories>cs.NI cs.SY</categories><comments>IEEE Communications Magazine 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing &quot;connectivity from the sky&quot; is the new innovative trend in wireless
communications. High and low altitude platforms, drones, aircrafts and airships
are being considered as the candidates for deploying wireless communications
complementing the terrestrial communication infrastructure. In this article, we
report the detailed account of the design and implementation challenges of an
aerial network consisting of LTE Advanced (LTE-A) base stations. In particular,
we review achievements and innovations harnessed by an aerial network composed
of Helikite platforms. Helikites can be raised in the sky to bring Internet
access during special events and in the aftermath of an emergency. The trial
phase of the system mounting LTE-A technology onboard Helikites to serve users
on the ground showed not only to be very encouraging but that such a system
could offer even a longer lasting solution provided that inefficiency in
powering the radio frequency equipment in the Helikite can be overcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05332</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05332</id><created>2016-02-17</created><authors><author><keyname>Dong</keyname><forenames>Bin</forenames></author><author><keyname>Shen</keyname><forenames>Zuowei</forenames></author><author><keyname>Xie</keyname><forenames>Peichu</forenames></author></authors><title>Image Restoration: A General Wavelet Frame Based Model and Its
  Asymptotic Analysis</title><categories>math.FA cs.CV</categories><msc-class>42C40, 68U10, 65D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image restoration is one of the most important areas in imaging science.
Mathematical tools have been widely used in image restoration, where wavelet
frame based approach is one of the successful examples. In this paper, we
introduce a generic wavelet frame based image restoration model, called the
&quot;general model&quot;, which includes most of the existing wavelet frame based models
as special cases. Moreover, the general model also includes examples that are
new to the literature. Motivated by our earlier studies [1-3], We provide an
asymptotic analysis of the general model as image resolution goes to infinity,
which establishes a connection between the general model in discrete setting
and a new variatonal model in continuum setting. The variational model also
includes some of the existing variational models as special cases, such as the
total generalized variational model proposed by [4]. In the end, we introduce
an algorithm solving the general model and present one numerical simulation as
an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05333</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05333</id><created>2016-02-17</created><authors><author><keyname>Lautenschlaeger</keyname><forenames>Wolfram</forenames></author><author><keyname>Francini</keyname><forenames>Andrea</forenames></author></authors><title>Global Synchronization Protection for Bandwidth Sharing TCP Flows in
  High-Speed Links</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a congested network link, synchronization effects between
bandwidth-sharing TCP flows cause wide queue length oscillations, which may
translate into poor link utilization if insufficiently buffered. We introduce
global synchronization protection (GSP), a simple extension to the ordinary
operation of a tail-drop queue that safely suppresses the flow synchronization.
Our minimalistic solution is well suited for scaling with leading-edge link
rates: it adds only few extra operations in the fast path and does not require
accelerated memory access compared to the line rate. GSP makes it easier to
provide advanced control of TCP congestion in high-speed links and in low-power
packet processing hardware. Using experiments with a Linux prototype of GSP, we
show that, despite its exclusive focus on removing global synchronization, the
new scheme performs as well as far more complex active queue management (AQM)
schemes like CoDel and PIE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05335</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05335</id><created>2016-02-17</created><authors><author><keyname>Koh</keyname><forenames>Jing Yang</forenames></author><author><keyname>Nevat</keyname><forenames>Ido</forenames></author><author><keyname>Leong</keyname><forenames>Derek</forenames></author><author><keyname>Wong</keyname><forenames>Wai-Choong</forenames></author></authors><title>Geo-spatial Location Spoofing Detection for Internet of Things</title><categories>cs.CR</categories><comments>A shorten version of this work has been accepted to the IEEE IoT
  Journal (IoT-J) on 08-Feb-2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new location spoofing detection algorithm for geo-spatial
tagging and location-based services in the Internet of Things (IoT), called
Enhanced Location Spoofing Detection using Audibility (ELSA) which can be
implemented at the backend server without modifying existing legacy IoT
systems. ELSA is based on a statistical decision theory framework and uses
two-way time-of-arrival (TW-TOA) information between the user's device and the
anchors. In addition to the TW-TOA information, ELSA exploits the implicit
available audibility information to improve detection rates of location
spoofing attacks. Given TW-TOA and audibility information, we derive the
decision rule for the verification of the device's location, based on the
generalized likelihood ratio test. We develop a practical threat model for
delay measurements spoofing scenarios, and investigate in detail the
performance of ELSA in terms of detection and false alarm rates. Our extensive
simulation results on both synthetic and real-world datasets demonstrate the
superior performance of ELSA compared to conventional non-audibility-aware
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05342</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05342</id><created>2016-02-17</created><authors><author><keyname>Igarashi</keyname><forenames>Ayumi</forenames></author><author><keyname>Elkind</keyname><forenames>Edith</forenames></author></authors><title>Hedonic Games with Graph-restricted Communication</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study hedonic coalition formation games in which cooperation among the
players is restricted by a graph structure: a subset of players can form a
coalition if and only if they are connected in the given graph. We investigate
the complexity of finding stable outcomes in such games, for several notions of
stability. In particular, we provide an efficient algorithm that finds an
individually stable partition for an arbitrary hedonic game on an acyclic
graph. We also introduce a new stability concept -in-neighbor stability- which
is tailored for our setting. We show that the problem of finding an in-neighbor
stable outcome admits a polynomial-time algorithm if the underlying graph is a
path, but is NP-hard for arbitrary trees even for additively separable hedonic
games; for symmetric additively separable games we obtain a PLS-hardness
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05350</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05350</id><created>2016-02-17</created><authors><author><keyname>Chen</keyname><forenames>Di</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author></authors><title>Relative Error Embeddings for the Gaussian Kernel Distance</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A reproducing kernel can define an embedding of a data point into an infinite
dimensional reproducing kernel Hilbert space (RKHS). The norm in this space
describes a distance, which we call the kernel distance. The random Fourier
features (of Rahimi and Recht) describe an oblivious approximate mapping into
finite dimensional Euclidean space that behaves similar to the RKHS. We show in
this paper that for the Gaussian kernel the Euclidean norm between these mapped
to features has $(1+\epsilon)$-relative error with respect to the kernel
distance. When there are $n$ data points, we show that $O((1/\epsilon^2)
\log(n))$ dimensions of the approximate feature space are sufficient and
necessary.
  Without a bound on $n$, but when the original points lie in $\mathbb{R}^d$
and have diameter bounded by $\mathcal{M}$, then we show that $O((d/\epsilon^2)
\log(\mathcal{M}))$ dimensions are sufficient, and that this many are required,
up to $\log(1/\epsilon)$ factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05351</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05351</id><created>2016-02-17</created><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Yu</keyname><forenames>Yuling</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author></authors><title>Energy-Efficient Joint Congestion Control and Resource Optimization in
  Heterogeneous Cloud Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>13 pages, 7 figures, accepted by IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heterogeneous cloud radio access network (HCRAN) is a promising paradigm
which integrates the advantages of cloud radio access network (C-RAN) and
heterogeneous network (HetNet). In this paper, we study the joint congestion
control and resource optimization to explore the energy efficiency
(EE)-guaranteed tradeoff between throughput utility and delay performance in a
downlink slotted H-CRAN. We formulate the considered problem as a stochastic
optimization problem, which maximizes the utility of average throughput and
maintains the network stability subject to required EE constraint and transmit
power consumption constraints by traffic admission control, user association,
resource block allocation and power allocation. Leveraging on the Lyapunov
optimization technique, the stochastic optimization problem can be transformed
and decomposed into three separate subproblems which can be solved concurrently
at each slot. The third mixed-integer nonconvex subproblem is efficiently
solved utilizing the continuity relaxation of binary variables and the Lagrange
dual decomposition method. Theoretical analysis shows that the proposal can
quantitatively control the throughput-delay performance tradeoff with required
EE performance. Simulation results consolidate the theoretical analysis and
demonstrate the advantages of the proposal from the prospective of queue
stability and power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05352</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05352</id><created>2016-02-17</created><authors><author><keyname>Schnabel</keyname><forenames>Tobias</forenames></author><author><keyname>Swaminathan</keyname><forenames>Adith</forenames></author><author><keyname>Singh</keyname><forenames>Ashudeep</forenames></author><author><keyname>Chandak</keyname><forenames>Navin</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author></authors><title>Recommendations as Treatments: Debiasing Learning and Evaluation</title><categories>cs.LG cs.AI cs.IR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most data for evaluating and training recommender systems is subject to
selection biases, either through self-selection by the users or through the
actions of the recommendation system itself. In this paper, we provide a
principled approach to handling selection biases, adapting models and
estimation techniques from causal inference. The approach leads to unbiased
performance estimators despite biased data, and to a matrix factorization
method that provides substantially improved prediction performance on
real-world data. We theoretically and empirically characterize the robustness
of the approach, finding that it is highly practical and scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05365</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05365</id><created>2016-02-17</created><authors><author><keyname>Miculan</keyname><forenames>Marino</forenames></author><author><keyname>Peressotti</keyname><forenames>Marco</forenames></author></authors><title>A Specification of Open Transactional Memory for Haskell</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transactional memory (TM) has emerged as a promising abstraction for
concurrent programming alternative to lock-based synchronizations. However,
most TM models admit only isolated transactions, which are not adequate in
multi-threaded programming where transactions have to interact via shared data
before committing. In this paper, we present Open Transactional Memory (OTM), a
programming abstraction supporting safe, data-driven interactions between
composable memory transactions. This is achieved by relaxing isolation between
transactions, still ensuring atomicity: threads of different transactions can
interact by accessing shared variables, but then their transactions have to
commit together-actually, these transactions are transparently merged. This
model allows for loosely-coupled interactions since transaction merging is
driven only by accesses to shared data, with no need to specify participants
beforehand. In this paper we provide a specification of the OTM in the setting
of Concurrent Haskell, showing that it is a conservative extension of current
STM abstraction. In particular, we provide a formal semantics, which allows us
to prove that OTM satisfies the \emph{opacity} criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05372</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05372</id><created>2016-02-17</created><authors><author><keyname>Binu</keyname><forenames>V P</forenames></author><author><keyname>Nair</keyname><forenames>Divya G</forenames></author><author><keyname>Sreekumar</keyname><forenames>A</forenames></author></authors><title>Secret Sharing Homomorphism and Secure E-voting</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure E-voting is a challenging protocol. Several approaches based on
homomorphic crypto systems, mix-nets blind signatures are proposed in the
literature .But most of them need complicated homomorphic encryption which
involves complicated encryption decryption process and key management which is
not efficient. In this paper we propose a secure and efficient E-voting scheme
based on secret sharing homomorphism. Here E-voting is viewed as special case
of multi party computation where several voters jointly compute the result
without revealing his vote. Secret sharing schemes are good alternative for
secure multi party computation and are computationally efficient and secure
compared with the cryptographic techniques. It is the first proposal, which
makes use of the additive homomorphic property of the Shamir secret sharing
scheme and the encoding decoding of votes to obtain the individual votes
obtained by each candidate apart from the election result. We have achieved
integrity and privacy while keeping the efficiency of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05388</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05388</id><created>2016-02-17</created><authors><author><keyname>Imran</keyname><forenames>Muhammad</forenames></author><author><keyname>Mitra</keyname><forenames>Prasenjit</forenames></author><author><keyname>Srivastava</keyname><forenames>Jaideep</forenames></author></authors><title>Cross-Language Domain Adaptation for Classifying Crisis-Related Short
  Messages</title><categories>cs.CL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid crisis response requires real-time analysis of messages. After a
disaster happens, volunteers attempt to classify tweets to determine needs,
e.g., supplies, infrastructure damage, etc. Given labeled data, supervised
machine learning can help classify these messages. Scarcity of labeled data
causes poor performance in machine training. Can we reuse old tweets to train
classifiers? How can we choose labeled tweets for training? Specifically, we
study the usefulness of labeled data of past events. Do labeled tweets in
different language help? We observe the performance of our classifiers trained
using different combinations of training sets obtained from past disasters. We
perform extensive experimentation on real crisis datasets and show that the
past labels are useful when both source and target events are of the same type
(e.g. both earthquakes). For similar languages (e.g., Italian and Spanish),
cross-language domain adaptation was useful, however, when for different
languages (e.g., Italian and English), the performance decreased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05391</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05391</id><created>2016-02-17</created><authors><author><keyname>Yin</keyname><forenames>Yitong</forenames></author></authors><title>Simple average-case lower bounds for approximate near-neighbor from
  isoperimetric inequalities</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove an $\Omega(d/\log \frac{sw}{nd})$ lower bound for the average-case
cell-probe complexity of deterministic or Las Vegas randomized algorithms
solving approximate near-neighbor (ANN) problem in $d$-dimensional Hamming
space in the cell-probe model with $w$-bit cells, using a table of size $s$.
This lower bound matches the highest known worst-case cell-probe lower bounds
for any static data structure problems.
  This average-case cell-probe lower bound is proved in a general framework
that relates the cell-probe complexity of ANN to isoperimetric inequalities
regarding an expansion property of the underlying metric space. This connection
between ANN lower bounds and isoperimetric inequalities is established by a
stronger version of the richness lemma which we prove by the cell-sampling
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05394</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05394</id><created>2016-02-17</created><authors><author><keyname>Jenatton</keyname><forenames>Rodolphe</forenames></author><author><keyname>Huang</keyname><forenames>Jim</forenames></author><author><keyname>Archambeau</keyname><forenames>Cedric</forenames></author></authors><title>Online optimization and regret guarantees for non-additive long-term
  constraints</title><categories>stat.ML cs.LG math.OC math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online optimization in the 1-lookahead setting, where the
objective does not decompose additively over the rounds of the online game. The
resulting formulation enables us to deal with non-stationary and/or long-term
constraints , which arise, for example, in online display advertising problems.
We propose an on-line primal-dual algorithm for which we obtain dynamic
cumulative regret guarantees. They depend on the convexity and the smoothness
of the non-additive penalty, as well as terms capturing the smoothness with
which the residuals of the non-stationary and long-term constraints vary over
the rounds. We conduct experiments on synthetic data to illustrate the benefits
of the non-additive penalty and show vanishing regret convergence on live
traffic data collected by a display advertising platform in production.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05395</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05395</id><created>2016-02-17</created><authors><author><keyname>Naumchev</keyname><forenames>Alexandr</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author><author><keyname>Rivera</keyname><forenames>Victor</forenames></author></authors><title>Unifying Requirements and Code: an Example</title><categories>cs.SE</categories><comments>13 pages; 7 figures; to appear in Ershov Informatics Conference, PSI,
  Kazan, Russia (LNCS), 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Requirements and code, in conventional software engineering wisdom, belong to
entirely different worlds. Is it possible to unify these two worlds? A unified
framework could help make software easier to change and reuse. To explore the
feasibility of such an approach, the case study reported here takes a classic
example from the requirements engineering literature and describes it using a
programming language framework to express both domain and machine properties.
The paper describes the solution, discusses its benefits and limitations, and
assesses its scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05400</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05400</id><created>2016-02-17</created><authors><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author><author><keyname>Power</keyname><forenames>John</forenames></author></authors><title>Category theoretic semantics for theorem proving in logic programming:
  embracing the laxness</title><categories>cs.LO</categories><comments>20 pages, CMCS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A propositional logic program $P$ may be identified with a $P_fP_f$-coalgebra
on the set of atomic propositions in the program. The corresponding
$C(P_fP_f)$-coalgebra, where $C(P_fP_f)$ is the cofree comonad on $P_fP_f$,
describes derivations by resolution. Using lax semantics, that correspondence
may be extended to a class of first-order logic programs without existential
variables. The resulting extension captures the proofs by term-matching
resolution in logic programming. Refining the lax approach, we further extend
it to arbitrary logic programs. We also exhibit a refinement of Bonchi and
Zanasi's saturation semantics for logic programming that complements lax
semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05404</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05404</id><created>2016-02-17</created><authors><author><keyname>Uiterwijk</keyname><forenames>Jos W. H. M.</forenames></author></authors><title>11 x 11 Domineering is Solved: The first player wins</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed a program called MUDoS (Maastricht University Domineering
Solver) that solves Domineering positions in a very efficient way. This enables
the solution of known positions so far (up to the 10 x 10 board) much quicker
(measured in number of investigated nodes).
  More importantly, it enables the solution of the 11 x 11 Domineering board, a
board up till now far out of reach of previous Domineering solvers. The
solution needed the investigation of 259,689,994,008 nodes, using almost half a
year of computation time on a single simple desktop computer. The results show
that under optimal play the first player wins the 11 x 11 Domineering game,
irrespective if Vertical or Horizontal starts the game.
  In addition, several other boards hitherto unsolved were solved. Using the
convention that Vertical starts, the 8 x 15, 11 x 9, 12 x 8, 12 x 15, 14 x 8,
and 17 x 6 boards are all won by Vertical, whereas the 6 x 17, 8 x 12, 9 x 11,
and 11 x 10 boards are all won by Horizontal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05409</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05409</id><created>2016-02-17</created><authors><author><keyname>Dawar</keyname><forenames>Anuj</forenames></author><author><keyname>Wang</keyname><forenames>Pengming</forenames></author></authors><title>Lasserre Lower Bounds and Definability of Semidefinite Programming</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  For a large class of optimization problems, namely those that can be
expressed as maximization problems over a constraint language, we establish
strong lower bounds on the number of levels of the Lasserre hierarchy of
semi-definite programs (SDPs) that are required to solve the problem exactly.
The lower bounds are established through logical undefinability results. That
is, we show that the linear programming relaxation of the problem, as well as
the SDP corresponding to any fixed level of the Lasserre hierarchy is
interpretable in a MAXCSP instance by means of formulas of FPC (fixed-point
logic with counting). We also show that the solution of an SDP can be expressed
in this logic. Together, these results give a way of translating lower bounds
on the number of variables required in counting logic to express a constraint
satisfaction problem into lower bounds on the number of levels required in the
Lasserre hierarchy to eliminate the integrality gap in the corresponding MAXCSP
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05413</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05413</id><created>2016-02-17</created><authors><author><keyname>Fagnani</keyname><forenames>Fabio</forenames></author><author><keyname>Zino</keyname><forenames>Lorenzo</forenames></author></authors><title>Diffusion of innovation in large scale graphs</title><categories>cs.SI math.PR</categories><comments>27 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Will a new smartphone application diffuse deeply in the population or will it
sink into oblivion soon? To predict this, we argue that common models of spread
of innovations based on cascade dynamics or epidemics may not be fully
adequate. Therefore we propose a novel stochastic network dynamics modeling the
spread of a new technological asset, whose adoption is based on the
word-of-mouth and the persuasion strength increases the more the product is
diffused. In this paper we carry on an analysis on large scale graphs to show
off how the parameters of the model, the topology of the graph and, possibly,
the initial diffusion of the asset, determine whether the spread of the asset
is successful or not. In particular, by means of stochastic dominations and
deterministic approximations, we provide some general results for a large class
of expansive graphs. Finally we present numerical simulations trying to expand
the analytical results we proved to even more general topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05419</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05419</id><created>2016-02-17</created><updated>2016-02-24</updated><authors><author><keyname>Dieuleveut</keyname><forenames>Aymeric</forenames><affiliation>SIERRA, LIENS</affiliation></author><author><keyname>Flammarion</keyname><forenames>Nicolas</forenames><affiliation>LIENS, SIERRA</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>SIERRA, LIENS</affiliation></author></authors><title>Harder, Better, Faster, Stronger Convergence Rates for Least-Squares
  Regression</title><categories>math.OC cs.LG stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the optimization of a quadratic objective function whose
gradients are only accessible through a stochastic oracle that returns the
gradient at any given point plus a zero-mean finite variance random error. We
present the first algorithm that achieves jointly the optimal prediction error
rates for least-squares regression, both in terms of forgetting of initial
conditions in O(1/n 2), and in terms of dependence on the noise and dimension d
of the problem, as O(d/n). Our new algorithm is based on averaged accelerated
regularized gradient descent, and may also be analyzed through finer
assumptions on initial conditions and the Hessian matrix, leading to
dimension-free quantities that may still be small while the &quot; optimal &quot; terms
above are large. In order to characterize the tightness of these new bounds, we
consider an application to non-parametric regression and use the known lower
bounds on the statistical performance (without computational limits), which
happen to match our bounds obtained from a single pass on the data and thus
show optimality of our algorithm in a wide variety of particular trade-offs
between bias and variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05432</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05432</id><created>2016-02-17</created><authors><author><keyname>Villagra</keyname><forenames>Marcos</forenames></author><author><keyname>Yakary&#x131;lmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Language recognition power and succintness of affine automata</title><categories>cs.FL cs.CC quant-ph</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study a non-linear generalization based on affine
transformations of probabilistic and quantum automata proposed recently by
D\'iaz-Caro and Yakary{\i}lmaz [CSR2016] referred as affine automata. First, we
present efficient simulations of probabilistic and quantum automata by means of
affine automata which allows us to characterized the class of exclusive
stochastic languages. Then, we initiate a study on the succintness of affine
automata. In particular, we show that an infinite family of unary regular
languages can be recognized by 2-state affine automata but the state numbers of
quantum and probabilistic automata cannot be bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05436</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05436</id><created>2016-02-17</created><authors><author><keyname>Gartrell</keyname><forenames>Mike</forenames></author><author><keyname>Paquet</keyname><forenames>Ulrich</forenames></author><author><keyname>Koenigstein</keyname><forenames>Noam</forenames></author></authors><title>Low-Rank Factorization of Determinantal Point Processes for
  Recommendation</title><categories>stat.ML cs.LG</categories><comments>10 pages, 4 figures. Submitted to KDD 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determinantal point processes (DPPs) have garnered attention as an elegant
probabilistic model of set diversity. They are useful for a number of subset
selection tasks, including product recommendation. DPPs are parametrized by a
positive semi-definite kernel matrix. In this work we present a new method for
learning the DPP kernel from observed data using a low-rank factorization of
this kernel. We show that this low-rank factorization enables a learning
algorithm that is nearly an order of magnitude faster than previous approaches,
while also providing for a method for computing product recommendation
predictions that is far faster (up to 20x faster or more for large item
catalogs) than previous techniques that involve a full-rank DPP kernel.
Furthermore, we show that our method provides equivalent or sometimes better
predictive performance than prior full-rank DPP approaches, and better
performance than several other competing recommendation methods in many cases.
We conduct an extensive experimental evaluation using several real-world
datasets in the domain of product recommendation to demonstrate the utility of
our method, along with its limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05437</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05437</id><created>2016-02-17</created><authors><author><keyname>Elkin</keyname><forenames>Michael</forenames></author><author><keyname>Neiman</keyname><forenames>Ofer</forenames></author></authors><title>Distributed Strong Diameter Network Decomposition</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a pair of positive parameters $D,\chi$, a partition ${\cal P}$ of the
vertex set $V$ of an $n$-vertex graph $G = (V,E)$ into disjoint clusters of
diameter at most $D$ each is called a $(D,\chi)$ network decomposition, if the
supergraph ${\cal G}({\cal P})$, obtained by contracting each of the clusters
of ${\cal P}$, can be properly $\chi$-colored. The decomposition ${\cal P}$ is
said to be strong (resp., weak) if each of the clusters has strong (resp.,
weak) diameter at most $D$, i.e., if for every cluster $C \in {\cal P}$ and
every two vertices $u,v \in C$, the distance between them in the induced graph
$G(C)$ of $C$ (resp., in $G$) is at most $D$.
  Network decomposition is a powerful construct, very useful in distributed
computing and beyond. It was shown by Awerbuch \etal \cite{AGLP89} and
Panconesi and Srinivasan \cite{PS92}, that strong $(2^{O(\sqrt{\log
n})},2^{O(\sqrt{\log n})})$ network decompositions can be computed in
$2^{O(\sqrt{\log n})}$ distributed time. Linial and Saks \cite{LS93} devised an
ingenious randomized algorithm that constructs {\em weak} $(O(\log n),O(\log
n))$ network decompositions in $O(\log^2 n)$ time. It was however open till now
if {\em strong} network decompositions with both parameters $2^{o(\sqrt{\log
n})}$ can be constructed in distributed $2^{o(\sqrt{\log n})}$ time.
  In this paper we answer this long-standing open question in the affirmative,
and show that strong $(O(\log n),O(\log n))$ network decompositions can be
computed in $O(\log^2 n)$ time. We also present a tradeoff between parameters
of our network decomposition. Our work is inspired by and relies on the
&quot;shifted shortest path approach&quot;, due to Blelloch \etal \cite{BGKMPT11}, and
Miller \etal \cite{MPX13}. These authors developed this approach for PRAM
algorithms for padded partitions. We adapt their approach to network
decompositions in the distributed model of computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05439</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05439</id><created>2016-02-17</created><authors><author><keyname>Browet</keyname><forenames>Arnaud</forenames></author><author><keyname>De Vleeschouwer</keyname><forenames>Christophe</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Mathiah</keyname><forenames>Navrita</forenames></author><author><keyname>Saykali</keyname><forenames>Bechara</forenames></author><author><keyname>Migeotte</keyname><forenames>Isabelle</forenames></author></authors><title>Cell segmentation with random ferns and graph-cuts</title><categories>cs.CV cs.LG</categories><comments>submitted to ICIP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The progress in imaging techniques have allowed the study of various aspect
of cellular mechanisms. To isolate individual cells in live imaging data, we
introduce an elegant image segmentation framework that effectively extracts
cell boundaries, even in the presence of poor edge details. Our approach works
in two stages. First, we estimate pixel interior/border/exterior class
probabilities using random ferns. Then, we use an energy minimization framework
to compute boundaries whose localization is compliant with the pixel class
probabilities. We validate our approach on a manually annotated dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05445</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05445</id><created>2016-02-17</created><authors><author><keyname>Chergui</keyname><forenames>Hatim</forenames></author><author><keyname>Benjillali</keyname><forenames>Mustapha</forenames></author><author><keyname>Saoudi</keyname><forenames>Samir</forenames></author></authors><title>Performance Analysis of Project-and-Forward Relaying in Mixed
  MIMO-Pinhole and Rayleigh Dual-Hop Channel</title><categories>cs.IT math.IT</categories><comments>4 pages, IEEE Communications Letters, 2016</comments><doi>10.1109/LCOMM.2016.2514348</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we present an end-to-end performance analysis of dual-hop
project-and-forward relaying in a realistic scenario, where the source-relay
and the relay-destination links are experiencing MIMO-pinhole and Rayleigh
channel conditions, respectively. We derive the probability density function of
both the relay post-processing and the end-to-end signal-to-noise ratios, and
the obtained expressions are used to derive the outage probability of the
analyzed system as well as its end-to-end ergodic capacity in terms of
generalized functions. Applying then the residue theory to Mellin-Barnes
integrals, we infer the system asymptotic behavior for different channel
parameters. As the bivariate Meijer-G function is involved in the analysis, we
propose a new and fast MATLAB implementation enabling an automated definition
of the complex integration contour. Extensive Monte-Carlo simulations are
invoked to corroborate the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05448</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05448</id><created>2016-02-17</created><authors><author><keyname>Schwarz</keyname><forenames>S.</forenames></author><author><keyname>Stefanov</keyname><forenames>A.</forenames></author><author><keyname>Wolf</keyname><forenames>S.</forenames></author><author><keyname>Montina</keyname><forenames>A.</forenames></author></authors><title>Optimal measurements for nonlocal correlations</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem in quantum information theory is to find the experimental setup
that maximizes the nonlocality of correlations with respect to some suitable
measure such as the violation of Bell inequalities. The latter has however some
drawbacks. First and foremost it is unfeasible to determine the whole set of
Bell inequalities already for a few measurements and thus unfeasible to find
the experimental setup maximizing their violation. Second, the Bell violation
suffers from an ambiguity stemming from the choice of the normalization of the
Bell coefficients. An alternative measure of nonlocality with a direct
information-theoretic interpretation is the minimal amount of classical
communication required for simulating nonlocal correlations. In the case of
many instances simulated in parallel, the minimal communication cost per
instance is called nonlocal capacity, and its computation can be reduced to a
convex-optimization problem. This quantity can be computed for a higher number
of measurements and turns out to be useful for finding the optimal experimental
setup. In this paper, we present a simple method for maximizing the nonlocal
capacity over a given configuration space and, in particular, over a set of
possible measurements, yielding the corresponding optimal setup. Furthermore,
we show that there is a functional relationship between Bell violation and
nonlocal capacity. The method is illustrated with numerical tests and compared
with the maximization of the violation of CGLMP-type Bell inequalities on the
basis of entangled two-qubit as well as two-qutrit states. Remarkably, the
anomaly of nonlocality displayed by qutrits turns out to be even stronger if
the nonlocal capacity is employed as a measure of nonlocality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05450</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05450</id><created>2016-02-17</created><authors><author><keyname>&#x160;o&#x161;i&#x107;</keyname><forenames>Adrian</forenames></author><author><keyname>KhudaBukhsh</keyname><forenames>Wasiur R.</forenames></author><author><keyname>Zoubir</keyname><forenames>Abdelhak M.</forenames></author><author><keyname>Koeppl</keyname><forenames>Heinz</forenames></author></authors><title>Inverse Reinforcement Learning in Swarm Systems</title><categories>stat.ML cs.AI cs.MA cs.SY</categories><comments>9 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inverse reinforcement learning (IRL) is the problem of recovering a system's
latent reward function from observed system behavior. In this paper, we
concentrate on IRL in homogeneous large-scale systems, which we refer to as
swarms. We show that, by exploiting the inherent homogeneity of a swarm, the
IRL objective can be reduced to an equivalent single-agent formulation of
constant complexity, which allows us to decompose a global system objective
into local subgoals at the agent-level. Based on this finding, we reformulate
the corresponding optimal control problem as a fix-point problem pointing
towards a symmetric Nash equilibrium, which we solve using a novel
heterogeneous learning scheme particularly tailored to the swarm setting.
Results on the Vicsek model and the Ising model demonstrate that the proposed
framework is able to produce meaningful reward models from which we can learn
near-optimal local controllers that replicate the observed system dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05456</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05456</id><created>2016-02-17</created><authors><author><keyname>Salem</keyname><forenames>Maha</forenames></author><author><keyname>Weiss</keyname><forenames>Astrid</forenames></author><author><keyname>Baxter</keyname><forenames>Paul</forenames></author><author><keyname>Dautenhahn</keyname><forenames>Kerstin</forenames></author></authors><title>5th International Symposium on New Frontiers in Human-Robot Interaction
  2016 (NF-HRI 2016)</title><categories>cs.RO</categories><comments>Index for conference proceedings NF-HRI 2016</comments><msc-class>68T40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume is the proceedings of the 5th International Symposium on New
Frontiers in Human-Robot Interaction, held at the AISB Convention 2016, which
took place on the 5th and 6th of April 2016, in Sheffield, U.K.
  Organised by Organised by Maha Salem (Google U.K.), Astrid Weiss (Vienna
University of Technology, Austria), Paul Baxter (Lincoln University, U.K.), and
Kerstin Dautenhahn (University of Hertfordshire, U.K.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05457</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05457</id><created>2016-02-17</created><authors><author><keyname>Fasino</keyname><forenames>Dario</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Modularity bounds for clusters located by leading eigenvectors of the
  normalized modularity matrix</title><categories>math.SP cs.SI math.NA physics.soc-ph</categories><msc-class>05C50, 15A18, 15B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nodal theorems for generalized modularity matrices ensure that the cluster
located by the positive entries of the leading eigenvector of various
modularity matrices induces a connected subgraph. In this paper we obtain lower
bounds for the modularity of that set of nodes showing that, under certain
conditions, the nodal domains induced by eigenvectors corresponding to highly
positive eigenvalues of the normalized modularity matrix have indeed positive
modularity, that is they can be recognized as modules inside the network.
Moreover we establish Cheeger-type inequalities for the cut-modularity of the
graph, providing a theoretical support to the common understanding that highly
positive eigenvalues of modularity matrices are related with the possibility of
subdividing a network into communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05459</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05459</id><created>2016-02-17</created><authors><author><keyname>Fasino</keyname><forenames>Dario</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Localization of dominant eigenpairs and planted communities by means of
  Frobenius inner products</title><categories>math.SP cs.SI math.NA</categories><msc-class>15A18, 15B48</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new localization result for the leading eigenvalue and
eigenvector of a symmetric matrix $A$. The result exploits the Frobenius inner
product between $A$ and a given rank-one landmark matrix $X$. Different choices
for $X$ may be used, depending upon the problem under investigation. In
particular, we show that the choice where $X$ is the all-ones matrix allows to
estimate the signature of the leading eigenvector of $A$, generalizing previous
results on Perron-Frobenius properties of matrices with some negative entries.
As another application we consider the problem of community detection in graphs
and networks. The problem is solved by means of modularity-based spectral
techniques, following the ideas pioneered by Miroslav Fiedler in mid 70s. We
show that a suitable choice of $X$ can be used to provide new quality
guarantees of those techniques, when the network follows a stochastic block
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05460</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05460</id><created>2016-02-17</created><authors><author><keyname>Solovey</keyname><forenames>Kiril</forenames></author><author><keyname>Salzman</keyname><forenames>Oren</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>New perspective on sampling-based motion planning via random geometric
  graphs</title><categories>cs.RO cs.CG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Roadmaps constructed by many sampling-based motion planners coincide, in the
absence of obstacles, with standard models of random geometric graphs (RGGs).
Those models have been studied for several decades and by now a rich body of
literature exists analyzing various properties and types of RGGs. In their
seminal work on optimal motion planning Karaman and Frazzoli (2011) conjectured
that a sampling-based planner has a certain property if the underlying RGG has
this property as well. In this paper we settle this conjecture and leverage it
for the development of a general framework for the analysis of sampling-based
planners. Our framework, which we call localization-tessellation, allows for
easy transfer of arguments on RGGs from the free unit-hypercube to spaces
punctured by obstacles, which are geometrically and topologically much more
complex. We demonstrate its power by providing alternative and (arguably)
simple proofs for probabilistic completeness and asymptotic (near-)optimality
of probabilistic roadmaps (PRMs). Furthermore, we introduce several variants of
PRMs, analyze them using our framework, and discuss the implications of the
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05462</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05462</id><created>2016-02-17</created><authors><author><keyname>Stein</keyname><forenames>Manuel</forenames></author><author><keyname>Barb&#xe9;</keyname><forenames>Kurt</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author></authors><title>DOA Parameter Estimation with 1-bit Quantization - Bounds, Methods and
  the Exponential Replacement</title><categories>cs.IT math.IT</categories><comments>20th International ITG Workshop on Smart Antennas (WSA), March 2016,
  Munich, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While 1-bit analog-to-digital conversion (ADC) allows to significantly reduce
the analog complexity of wireless receive systems, using the exact likelihood
function of the hard-limiting system model in order to obtain efficient
algorithms in the digital domain can make 1-bit signal processing challenging.
If the signal model before the quantizer consists of correlated Gaussian random
variables, the tail probability for a multivariate Gaussian distribution with N
dimensions (general orthant probability) is required in order to formulate the
likelihood function of the quantizer output. As a closed-form expression for
the general orthant probability is an open mathematical problem, formulation of
efficient processing methods for correlated and quantized data and an
analytical performance assessment have, despite their high practical relevance,
only found limited attention in the literature on quantized estimation theory.
Here we review the approach of replacing the original system model by an
equivalent distribution within the exponential family. For 1-bit signal
processing, this allows to circumvent calculation of the general orthant
probability and gives access to a conservative approximation of the receive
likelihood. For the application of blind direction-of-arrival (DOA) parameter
estimation with an array of K sensors, each performing 1-bit quantization, we
demonstrate how the exponential replacement enables to formulate a pessimistic
version of the Cram\'er-Rao lower bound (CRLB) and to derive an asymptotically
achieving conservative maximum-likelihood estimator (CMLE). The 1-bit DOA
performance analysis based on the pessimistic CRLB points out that a
low-complexity radio front-end design with 1-bit ADC is in particular suitable
for blind wireless DOA estimation with a large number of array elements
operating in the medium SNR regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05473</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05473</id><created>2016-02-17</created><authors><author><keyname>Maal&#xf8;e</keyname><forenames>Lars</forenames></author><author><keyname>S&#xf8;nderby</keyname><forenames>Casper Kaae</forenames></author><author><keyname>S&#xf8;nderby</keyname><forenames>S&#xf8;ren Kaae</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author></authors><title>Auxiliary Deep Generative Models</title><categories>stat.ML cs.AI cs.LG</categories><comments>Under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep generative models parameterized by neural networks have recently
achieved state-of-the-art performance in unsupervised and semi-supervised
learning. We extend deep generative models with auxiliary variables which
improves the variational approximation. The auxiliary variables leave the
generative model unchanged but make the variational distribution more
expressive. Inspired by the structure of the auxiliary variable we also propose
a model with two stochastic layers and skip connections. Our findings suggest
that more expressive and properly specified deep generative models converge
faster with better results. We show state-of-the-art performance within
semi-supervised learning on MNIST (0.96%), SVHN (16.61%) and NORB (9.40%)
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05480</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05480</id><created>2016-02-17</created><authors><author><keyname>Tomasi</keyname><forenames>Beatrice</forenames></author><author><keyname>Guillaud</keyname><forenames>Maxime</forenames></author></authors><title>Pilot Length Optimization for Spatially Correlated Multi-User MIMO
  Channel Estimation</title><categories>cs.IT math.IT</categories><comments>presented at Asilomar Conference on Signals, Systems, and Computers,
  Nov. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the design of pilot sequences for channel estimation in the
context of multiple-user Massive MIMO; considering the presence of channel
correlation, and assuming that the statistics are known, we seek to exploit the
spatial correlation of the channels to minimize the length of the pilot
sequences, and specifically the fact that the users can be separated either
through their spatial signature (low-rank channel covariance matrices), or
through the use of different training sequences. We introduce an algorithm to
design short training sequences for a given set of user covariance matrices.
The obtained pilot sequences are in general non-orthogonal, however they ensure
that the channel estimation error variance is uniformly upper-bounded by a
chosen constant over all channel dimensions. We show through simulations using
a realistic scenario based on the one-ring channel model that the proposed
technique can yield pilot sequences of length significantly smaller than the
number of users in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05481</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05481</id><created>2016-02-17</created><authors><author><keyname>Damian</keyname><forenames>Mirela</forenames></author><author><keyname>Nelavalli</keyname><forenames>Naresh</forenames></author></authors><title>Improved Bounds on the Stretch Factor of $Y_4$</title><categories>cs.CG</categories><comments>12 pages, 12 figures</comments><acm-class>F.2.2; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish an upper bound of 4.94 on the stretch factor of the Yao graph
$Y_4^\infty$ defined in the $L_\infty$-metric, improving upon the best
previously known upper bound of 6.31. We also establish an upper bound of 54.62
on the stretch factor of the Yao graph $Y_4$ defined in the Euclidean metric,
improving upon the best previously known upper bound of 662.16.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05507</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05507</id><created>2016-02-12</created><authors><author><keyname>Klan</keyname><forenames>Petr</forenames></author></authors><title>Cybernetic Interpretation of the Riemann Zeta Function</title><categories>cs.SY math.NT</categories><comments>9 pages, 5 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses cybernetic approach to study behavior of the Riemann zeta
function. It is based on the elementary cybernetic concepts like feedback,
transfer functions, time delays, PI (Proportional--Integral) controllers or
FOPDT (First Order Plus Dead Time) models, respectively. An unusual dynamic
interpretation of the Riemann zeta function is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05510</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05510</id><created>2016-02-17</created><authors><author><keyname>Rey</keyname><forenames>Anton</forenames></author><author><keyname>Igual</keyname><forenames>Francisco D.</forenames></author><author><keyname>Prieto-Mat&#xed;as</keyname><forenames>Manuel</forenames></author></authors><title>HeSP: a simulation framework for solving the task
  scheduling-partitioning problem on heterogeneous architectures</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe HeSP, a complete simulation framework to study a
general task scheduling-partitioning problem on heterogeneous architectures,
which treats recursive task partitioning and scheduling decisions on equal
footing. Considering recursive partitioning as an additional degree of freedom,
tasks can be dynamically partitioned or merged at runtime for each available
processor type, exposing additional or reduced degrees of parallelism as
needed. Our simulations reveal that, for a specific class of dense linear
algebra algorithms taken as a driving example, simultaneous decisions on task
scheduling and partitioning yield significant performance gains on two
different heterogeneous platforms: a highly heterogeneous CPU-GPU system and a
low-power asymmetric big.LITTLE ARM platform. The insights extracted from the
framework can be further applied to actual runtime task schedulers in order to
improve performance on current or future architectures and for different
task-parallel codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05511</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05511</id><created>2016-02-17</created><authors><author><keyname>Fan</keyname><forenames>Bing</forenames></author><author><keyname>Thapar</keyname><forenames>Hemant K.</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author></authors><title>Multihead Multitrack Detection with Reduced-State Sequence Estimation</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve ultra-high storage capacity, the data tracks are squeezed more and
more on the magnetic recording disks, causing severe intertrack interference
(ITI). The multihead multitrack (MHMT) detector is proposed to better combat
ITI. Such a detector, however, has prohibitive implementation complexity. In
this paper we propose to use the reduced-state sequence estimation (RSSE)
algorithm to significantly reduce the complexity, and render MHMT practical. We
first consider a commonly used symmetric two-head two-track (2H2T) channel
model. The effective distance between two input symbols is redefined. It
provides a better distance measure and naturally leads to an unbalanced set
partition tree. Different trellis configurations are obtained based on the
desired performance/complexity tradeoff. Simulation results show that the
reduced MHMT detector can achieve near maximum-likelihood (ML) performance with
a small fraction of the original number of trellis states. Error event analysis
is given to explain the behavior of RSSE algorithm on 2H2T channel. Search
results of dominant RSSE error events for different channel targets are
presented. We also study an asymmetric 2H2T system. The simulation results and
error event analysis show that RSSE is applicable to the asymmetric channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05513</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05513</id><created>2016-02-17</created><authors><author><keyname>Fan</keyname><forenames>Bing</forenames></author><author><keyname>Thapar</keyname><forenames>Hemant K.</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author></authors><title>Multihead Multitrack Detection with ITI Estimation in Next Generation
  Magnetic Recording System</title><categories>cs.IT math.IT</categories><comments>12 pages, submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multitrack detection with array-head reading is a promising technique
proposed for next generation magnetic storage systems. The multihead multitrack
(MHMT) system is characterized by intersymbol interference (ISI) in the
downtrack direction and intertrack interference (ITI) in the crosstrack
direction. Constructing the trellis of a MHMT maximum likelihood (ML) detector
requires knowledge of the ITI, which is generally unknown at the receiver. In
addition, to retain efficiency, the ML detector requires a static estimate of
the ITI, whose true value may in reality vary. In this paper we propose a
modified ML detector on the $n$-head, $n$-track ($n$H$n$T) channel which could
efficiently track the change of ITI, and adapt to new estimates. The trellis
used in the proposed detector is shown to be independent of the ITI level. A
gain loop structure is used to estimate the ITI. Simulation results show that
the proposed detector offers a performance advantage in settings where
complexity constraints limit the traditional ML detector to use a static ITI
estimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05521</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05521</id><created>2016-02-17</created><authors><author><keyname>Cheung</keyname><forenames>Kent Tsz Kan</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Joint Transmit and Receive Beamforming for Multi-Relay MIMO-OFDMA
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, 1 table, 2 algorithms, accepted by IEEE ICC 2016,
  23-27 May 2016, Kuala Lumpur, Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel transmission protocol is conceived for a multi-user, multi-relay,
multiple-input--multiple-output orthogonal frequency-division multiple-access
(MIMO-OFDMA) cellular network based on joint transmit and receive beamforming.
More specifically, the network's MIMO channels are mathematically decomposed
into several effective multiple-input--single-output (MISO) channels, which are
spatially multiplexed for transmission. For the sake of improving the
attainable capacity, these MISO channels are grouped using a pair of novel
grouping algorithms, which are then evaluated in terms of their performance
versus complexity trade-off\footnote{This paper concisely focuses on the
transmission protocol proposed in our previous work [1]. For more details,
please refer to [1].}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05526</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05526</id><created>2016-02-17</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Terriberry</keyname><forenames>Timothy B.</forenames></author><author><keyname>Montgomery</keyname><forenames>Christopher</forenames></author><author><keyname>Maxwell</keyname><forenames>Gregory</forenames></author></authors><title>A High-Quality Speech and Audio Codec With Less Than 10 ms Delay</title><categories>cs.SD cs.MM</categories><comments>10 pages</comments><journal-ref>IEEE Transactions on Audio, Speech and Language Processing, Vol.
  18, No. 1, pp. 58-67, 2010</journal-ref><doi>10.1109/TASL.2009.2023186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increasing quality requirements for multimedia communications, audio
codecs must maintain both high quality and low delay. Typically, audio codecs
offer either low delay or high quality, but rarely both. We propose a codec
that simultaneously addresses both these requirements, with a delay of only 8.7
ms at 44.1 kHz. It uses gain-shape algebraic vector quantisation in the
frequency domain with time-domain pitch prediction. We demonstrate that the
proposed codec operating at 48 kbit/s and 64 kbit/s out-performs both G.722.1C
and MP3 and has quality comparable to AAC-LD, despite having less than one
fourth of the algorithmic delay of these codecs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05531</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05531</id><created>2016-02-17</created><updated>2016-02-18</updated><authors><author><keyname>Bianco</keyname><forenames>Simone</forenames></author><author><keyname>Celona</keyname><forenames>Luigi</forenames></author><author><keyname>Napoletano</keyname><forenames>Paolo</forenames></author><author><keyname>Schettini</keyname><forenames>Raimondo</forenames></author></authors><title>On the Use of Deep Learning for Blind Image Quality Assessment</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the use of deep learning for distortion-generic
blind image quality assessment. We report on different design choices, ranging
from the use of features extracted from pre-trained Convolutional Neural
Networks (CNNs) as a generic image description, to the use of features
extracted from a CNN fine-tuned for the image quality task. Our best proposal,
named DeepBIQ, estimates the image quality by aver- age pooling the scores
predicted on multiple sub-regions of the original image. The score of each
sub-region is computed using a Support Vector Regression (SVR) machine taking
as input features extracted using a CNN fine-tuned for image quality
assessment. Experimental results on the LIVE In the Wild Image Quality
Challenge Database show that DeepBIQ outperforms the state-of-the-art methods
compared, having a Linear Correlation Coefficient (LCC) with human subjective
scores of almost 0.91. Furthermore, in many cases, the quality score
predictions of DeepBIQ are closer to the average observer than those of a
generic human observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05536</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05536</id><created>2016-02-17</created><updated>2016-02-22</updated><authors><author><keyname>Chen</keyname><forenames>Di</forenames></author><author><keyname>Schedler</keyname><forenames>Stephan</forenames></author><author><keyname>Kuehn</keyname><forenames>Volker</forenames></author></authors><title>Backhaul Traffic Balancing and Dynamic Content-Centric Clustering for
  the Downlink of Fog Radio Access Network</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, an evolution of the Cloud Radio Access Network (C-RAN) has been
proposed, named as Fog Radio Access Network (F-RAN). Compared to C-RAN, the
Radio Units (RUs) in F-CAN are equipped with local caches, which can store some
frequently requested files. In the downlink, users requesting the same file
form a multicast group, and are cooperatively served by a cluster of RUs. The
requested file is either available locally in the cache of this cluster or
fetched from the Central Processor (CP) via backhauls. Thus caching some
frequently requested files can greatly reduce the burden on backhaul links.
Whether a specific RU should be involved in a cluster to serve a multicast
group depends on its backhaul capacity, requested files, cached files and the
channel. Therefore it is subject to optimization. In this paper we investigate
the joint design of multicast beamforming, dynamic clustering and backhaul
traffic balancing. Beamforming and clustering are jointly optimized in order to
minimize the power consumed, while QoS of each user is to be met and the
traffic on each backhaul link is balanced according to its capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05537</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05537</id><created>2016-02-17</created><authors><author><keyname>Karangelos</keyname><forenames>Efthymios</forenames></author><author><keyname>Panciatici</keyname><forenames>Patrick</forenames></author><author><keyname>Wehenkel</keyname><forenames>Louis</forenames></author></authors><title>Whither probabilistic security management for real-time operation of
  power systems ?</title><categories>cs.SY</categories><comments>Presented at the 2013 IREP Symposium-Bulk Power Systems Dynamics and
  Control-IX (IREP), August 25-30,2013, Rethymnon, Crete, Greece</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the stakes of introducing probabilistic approaches
for the management of power system's security. In real-time operation, the aim
is to arbitrate in a rational way between preventive and corrective control,
while taking into account i) the prior probabilities of contingencies, ii) the
possible failure modes of corrective control actions, iii) the socio-economic
consequences of service interruptions. This work is a first step towards the
construction of a globally coherent decision making framework for security
management from long-term system expansion, via mid-term asset management,
towards short-term operation planning and real-time operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05546</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05546</id><created>2016-02-17</created><authors><author><keyname>D&#xe9;fago</keyname><forenames>Xavier</forenames><affiliation>JAIST</affiliation></author><author><keyname>Potop-Butucaru</keyname><forenames>Maria Gradinariu</forenames><affiliation>LIP6</affiliation></author><author><keyname>Cl&#xe9;ment</keyname><forenames>Julien</forenames><affiliation>LRI</affiliation></author><author><keyname>Messika</keyname><forenames>St&#xe9;phane</forenames><affiliation>LRI</affiliation></author><author><keyname>Raipin-Parv&#xe9;dy</keyname><forenames>Philippe</forenames></author><author><keyname>Raipin-Parv&#xe9;dy</keyname><forenames>P</forenames></author></authors><title>Fault and Byzantine Tolerant Self-stabilizing Mobile Robots Gathering -
  Feasibility Study -</title><categories>cs.RO cs.DC</categories><comments>A first version of this paper have been submitted to Distributed
  Computing in February 2012 (the extended abstract has been published in
  2006). The current version is the revised version sent in 2014. The most
  important results of this paper have been diffused in MAC 2010 held in
  Ottawa, Canada</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gathering is a fundamental coordination problem in cooperative mobile
robotics. In short, given a set of robots with arbitrary initial locations and
no initial agreement on a global coordinate system, gathering requires that all
robots, following their algorithm, reach the exact same but not predetermined
location. Gathering is particularly challenging in networks where robots are
oblivious (i.e., stateless) and direct communication is replaced by
observations on their respective locations. Interestingly any algorithm that
solves gathering with oblivious robots is inherently self-stabilizing if no
specific assumption is made on the initial distribution of the robots. In this
paper, we significantly extend the studies of de-terministic gathering
feasibility under different assumptions This manuscript considerably extends
preliminary results presented as an extended abstract at the DISC 2006
conference [7]. The current version is under review at Distributed Computing
Journal since February 2012 (in a previous form) and since 2014 in the current
form. The most important results have been also presented in MAC 2010 organized
in Ottawa from August 15th to 17th 2010 related to synchrony and faults (crash
and Byzantine). Unlike prior work, we consider a larger set of scheduling
strategies, such as bounded schedulers. In addition, we extend our study to the
feasibility of probabilistic self-stabilizing gathering in both fault-free and
fault-prone environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05547</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05547</id><created>2016-02-17</created><authors><author><keyname>G&#xf6;ller</keyname><forenames>Stefan</forenames></author><author><keyname>Haase</keyname><forenames>Christoph</forenames></author><author><keyname>Lazi&#x107;</keyname><forenames>Ranko</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>A Polynomial-Time Algorithm for Reachability in Branching VASS in
  Dimension One</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Branching VASS (BVASS) generalise vector addition systems with states by
allowing for special branching transitions that can non-deterministically
distribute a counter value between two control states. A run of a BVASS
consequently becomes a tree, and reachability is to decide whether a given
configuration is the root of a reachability tree. This paper shows
P-completeness of reachability in BVASS in dimension one, the first
decidability result for reachability in a subclass of BVASS known so far.
Moreover, we show that coverability and boundedness in BVASS in dimension one
are P-complete as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05548</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05548</id><created>2016-01-05</created><authors><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Yu</keyname><forenames>Yuling</forenames></author><author><keyname>Xiang</keyname><forenames>Hongyu</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Energy-Efficient Resource Allocation Optimization for Multimedia
  Heterogeneous Cloud Radio Access Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heterogeneous cloud radio access network (H-CRAN) is a promising paradigm
which incorporates the cloud computing into heterogeneous networks (HetNets),
thereby taking full advantage of cloud radio access networks (C-RANs) and
HetNets. Characterizing the cooperative beamforming with fronthaul capacity and
queue stability constraints is critical for multimedia applications to
improving energy efficiency (EE) in H-CRANs. An energy-efficient optimization
objective function with individual fronthaul capacity and inter-tier
interference constraints is presented in this paper for queue-aware multimedia
H-CRANs. To solve this non-convex objective function, a stochastic optimization
problem is reformulated by introducing the general Lyapunov optimization
framework. Under the Lyapunov framework, this optimization problem is
equivalent to an optimal network-wide cooperative beamformer design algorithm
with instantaneous power, average power and inter-tier interference
constraints, which can be regarded as the weighted sum EE maximization problem
and solved by a generalized weighted minimum mean square error approach. The
mathematical analysis and simulation results demonstrate that a tradeoff
between EE and queuing delay can be achieved, and this tradeoff strictly
depends on the fronthaul constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05551</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05551</id><created>2016-02-17</created><authors><author><keyname>Xiang</keyname><forenames>Yu</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Chen</keyname><forenames>Yih-Farn R.</forenames></author><author><keyname>Lan</keyname><forenames>Tian</forenames></author></authors><title>Differentiated latency in data center networks with erasure coded files
  through traffic engineering</title><categories>cs.DC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an algorithm to minimize weighted service latency for
different classes of tenants (or service classes) in a data center network
where erasure-coded files are stored on distributed disks/racks and access
requests are scattered across the network. Due to limited bandwidth available
at both top-of-the-rack and aggregation switches and tenants in different
service classes need differentiated services, network bandwidth must be
apportioned among different intra- and inter-rack data flows for different
service classes in line with their traffic statistics. We formulate this
problem as weighted queuing and employ a class of probabilistic request
scheduling policies to derive a closed-form upper-bound of service latency for
erasure-coded storage with arbitrary file access patterns and service time
distributions. The result enables us to propose a joint weighted latency (over
different service classes) optimization over three entangled &quot;control knobs&quot;:
the bandwidth allocation at top-of-the-rack and aggregation switches for
different service classes, dynamic scheduling of file requests, and the
placement of encoded file chunks (i.e., data locality). The joint optimization
is shown to be a mixed-integer problem. We develop an iterative algorithm which
decouples and solves the joint optimization as 3 sub-problems, which are either
convex or solvable via bipartite matching in polynomial time. The proposed
algorithm is prototyped in an open-source, distributed file system, {\em
Tahoe}, and evaluated on a cloud testbed with 16 separate physical hosts in an
Openstack cluster using 48-port Cisco Catalyst switches. Experiments validate
our theoretical latency analysis and show significant latency reduction for
diverse file access patterns. The results provide valuable insights on
designing low-latency data center networks with erasure coded storage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05555</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05555</id><created>2016-02-17</created><updated>2016-02-23</updated><authors><author><keyname>Burghardt</keyname><forenames>Jochen</forenames></author></authors><title>Repetition-Free Derivability from a Regular Grammar is NP-Hard</title><categories>cs.FL cs.CC</categories><comments>Technical report; 13 pages; 6 figures</comments><msc-class>68Q25, 68Q45</msc-class><acm-class>F.1.1; F.1.3; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the NP-hardness of the problem whether a given word can be derived
from a given regular grammar without repeated occurrence of any nonterminal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05556</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05556</id><created>2016-02-17</created><authors><author><keyname>Abukharis</keyname><forenames>Salim</forenames></author><author><keyname>Alzubi</keyname><forenames>Jafar A.</forenames></author><author><keyname>Alzubi</keyname><forenames>Omar A.</forenames></author><author><keyname>Alamri</keyname><forenames>Saeed</forenames></author><author><keyname>O'Farrell</keyname><forenames>Tim</forenames></author></authors><title>Packet Error Rate Performance of IEEE802.11g under Bluetooth Interface</title><categories>cs.NI</categories><comments>5 pages, 2 figures, Journal paper</comments><journal-ref>Research Journal of Applied Sciences, Engineering and Technology
  8(12): 1419-1423, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study approaches the issues concerning the coexistence of the IEEE
802.11g wireless networks and the adhoc Bluetooth networks that operate within
the same 2.4 GHz band. The performance of 802.11g is evaluated by simulating
the PER (Packet Error Rate) parameter and coverage area. The simulation
experiments are based on the worst case scenario presumption, which entails the
transmission of HV1 packet which HV1 link requires transmission on 100% of the
Bluetooth (BT) time slots using the maximum hop rate of 1600 hops per sec. The
paper suggests a practical approach to mitigate the interference using symbol
erasures technique through the assessment of the PER parameter at various Eb/No
values. The results show that the symbol erasures can effectively mitigate the
interference and enhance the performance of the 802.11g.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05559</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05559</id><created>2016-02-17</created><authors><author><keyname>Hamdoun</keyname><forenames>Hassan</forenames></author><author><keyname>Alzubi</keyname><forenames>Jafar A.</forenames></author><author><keyname>Alzubi</keyname><forenames>Omar A.</forenames></author><author><keyname>Mangeni</keyname><forenames>Solomon</forenames></author></authors><title>Key Economic and Environmental Perspectives on Sustainability in the ICT
  Sector</title><categories>cs.CY</categories><comments>5 pages, 4 figures, Journal paper</comments><journal-ref>Middle-East Journal of Scientific Research 21 (8): 1341-1345, 2014</journal-ref><doi>10.5829/idosi.mejsr.2014.21.08.21709</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Telecommunication networks have become as critical to the 21st century
development as were railways, roads and canals, to the 19th Century
developments and is now seen as enabler to a more sustained business,
environment and society as a whole. Still fascinating has been and is the
exponential rate of growth in this industry. This is one sector where the next
revolution is always just around the corner whether known or unknown. The
telecoms industry is categorized by high rates of innovation in a rapidly
changing technological environment. This in turn is associated with an immense
range of sustainability concerns and challenges for the Telecoms service
providers, the service users and the whole industry and its far reaching
influence on other industries. This paper discusses three key aspects of such
challenges namely; the question of sustainable power/energy supply for the
industry when the change is resulting in increasing energy and operational
cost, the exploitation of technologies advancement for sustainability and their
business and environmental benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05561</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05561</id><created>2016-02-17</created><updated>2016-03-03</updated><authors><author><keyname>Siyari</keyname><forenames>Payam</forenames></author><author><keyname>Dilkina</keyname><forenames>Bistra</forenames></author><author><keyname>Dovrolis</keyname><forenames>Constantine</forenames></author></authors><title>Lexis: An Optimization Framework for Discovering the Hierarchical
  Structure of Sequential Data</title><categories>cs.AI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data represented as strings abounds in biology, linguistics, document mining,
web search and many other fields. Such data often have a hierarchical
structure, either because they were artificially designed and composed in a
hierarchical manner or because there is an underlying evolutionary process that
creates repeatedly more complex strings from simpler substrings. We propose a
framework, referred to as &quot;Lexis&quot;, that produces an optimized hierarchical
representation of a given set of &quot;target&quot; strings. The resulting hierarchy,
&quot;Lexis-DAG&quot;, shows how to construct each target through the concatenation of
intermediate substrings, minimizing the total number of such concatenations or
DAG edges. The Lexis optimization problem is related to the smallest grammar
problem. After we prove its NP-Hardness for two cost formulations, we propose
an efficient greedy algorithm for the construction of Lexis-DAGs. We also
consider the problem of identifying the set of intermediate nodes (substrings)
that collectively form the &quot;core&quot; of a Lexis-DAG, which is important in the
analysis of Lexis-DAGs. We show that the Lexis framework can be applied in
diverse applications such as optimized synthesis of DNA fragments in genomic
libraries, hierarchical structure discovery in protein sequences,
dictionary-based text compression, and feature extraction from a set of
documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05566</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05566</id><created>2016-02-17</created><authors><author><keyname>Tameling</keyname><forenames>Daniel</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Ismail</keyname><forenames>Ahmed E.</forenames><affiliation>AICES, RWTH Aachen</affiliation><affiliation>Aachener Verfahrenstechnik - Molecular Simulations and Transformations, RWTH Aachen</affiliation></author></authors><title>A Note on Time Measurements in LAMMPS</title><categories>cond-mat.mtrl-sci cs.CE physics.chem-ph physics.comp-ph</categories><report-no>AICES-2016/02-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the issue of assessing the efficiency of components of a parallel
program at the example of the MD package LAMMPS. In particular, we look at how
LAMMPS deals with the issue and explain why the approach adopted might lead to
inaccurate conclusions. The misleading nature of this approach is subsequently
verified experimentally with a case study. Afterwards, we demonstrate how one
should correctly determine the efficiency of the components and show what
changes to the code base of LAMMPS are necessary in order to get the correct
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05567</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05567</id><created>2016-02-17</created><authors><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author></authors><title>Nodal domain theorem for the graph $p$-Laplacian</title><categories>math.SP cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the nonlinear graph $p$-Laplacian and the set of
eigenvalues and associated eigenvectors of this operator defined by a
variational principle. We prove a unifying nodal domain theorem for the graph
$p$-Laplacian for any $p\geq 1$. While for $p&gt;1$ the bounds on the number of
weak and strong nodal domains are the same as for the linear graph Laplacian
($p=2$), the behavior changes for $p=1$. We show that the bounds are tight for
$p\geq 1$ by studying the eigenvectors of the graph $p$-Laplacian for two
example graphs where the bounds on the nodal domains are attained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05568</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05568</id><created>2016-02-17</created><authors><author><keyname>Choi</keyname><forenames>Edward</forenames></author><author><keyname>Bahadori</keyname><forenames>Mohammad Taha</forenames></author><author><keyname>Searles</keyname><forenames>Elizabeth</forenames></author><author><keyname>Coffey</keyname><forenames>Catherine</forenames></author><author><keyname>Sun</keyname><forenames>Jimeng</forenames></author></authors><title>Multi-layer Representation Learning for Medical Concepts</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning efficient representations for concepts has been proven to be an
important basis for many applications such as machine translation or document
classification. Proper representations of medical concepts such as diagnosis,
medication, procedure codes and visits will have broad applications in
healthcare analytics. However, in Electronic Health Records (EHR) the visit
sequences of patients include multiple concepts (diagnosis, procedure, and
medication codes) per visit. This structure provides two types of relational
information, namely sequential order of visits and co-occurrence of the codes
within each visit. In this work, we propose Med2Vec, which not only learns
distributed representations for both medical codes and visits from a large EHR
dataset with over 3 million visits, but also allows us to interpret the learned
representations confirmed positively by clinical experts. In the experiments,
Med2Vec displays significant improvement in key medical applications compared
to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while
providing clinically meaningful interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05572</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05572</id><created>2016-02-16</created><authors><author><keyname>Huzurbazar</keyname><forenames>S.</forenames></author><author><keyname>Lee</keyname><forenames>Long</forenames></author><author><keyname>Kuang</keyname><forenames>Dongyang</forenames></author></authors><title>A landmark-based algorithm for automatic pattern recognition and
  abnormality detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of mathematical and statistical algorithms with the aim of
establishing a computer-based framework for fast and reliable automatic pattern
recognition and abnormality detection. Under this framework, we propose a
numerical algorithm for finding group averages where an average of a group is
an estimator that is said to best represent the properties of interest of that
group. A novelty of the proposed landmark-based algorithm is that the algorithm
tracks information of the momentum field through the geodesic shooting process.
The momentum field provides a local template-based coordinate system and is
linear in nature. It is also a dual of the velocity field with respect to an
assigned base template, yielding advantages for statistical analyses. We apply
this framework to a small brain image database for detecting structure
abnormality. The brain structure changes identified by our framework are highly
consistent with studies in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05573</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05573</id><created>2016-02-17</created><authors><author><keyname>Lee</keyname><forenames>Hyung Mok</forenames></author><author><keyname>Bigot</keyname><forenames>Eric-Olivier Le</forenames></author><author><keyname>Du</keyname><forenames>ZhiHui</forenames></author><author><keyname>Lin</keyname><forenames>ZhangXi</forenames></author><author><keyname>Guo</keyname><forenames>XiangYu</forenames></author><author><keyname>Wen</keyname><forenames>LinQing</forenames></author><author><keyname>Phukon</keyname><forenames>Khun Sang</forenames></author><author><keyname>Pandey</keyname><forenames>Vihan</forenames></author><author><keyname>Bose</keyname><forenames>Sukanta</forenames></author><author><keyname>Fan</keyname><forenames>Xi-Long</forenames></author><author><keyname>Hendry</keyname><forenames>Martin</forenames></author></authors><title>Gravitational wave astrophysics, data analysis and multimessenger
  astronomy</title><categories>astro-ph.IM cs.DC gr-qc physics.data-an</categories><journal-ref>Sci China-Phys Mech Astron, 2015, 58(12): 120403</journal-ref><doi>10.1007/s11433-015-5740-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews gravitational wave sources and their detection. One of the
most exciting potential sources of gravitational waves are coalescing binary
black hole systems. They can occur on all mass scales and be formed in numerous
ways, many of which are not understood. They are generally invisible in
electromagnetic waves, and they provide opportunities for deep investigation of
Einstein's general theory of relativity. Sect. 1 of this paper considers ways
that binary black holes can be created in the universe, and includes the
prediction that binary black hole coalescence events are likely to be the first
gravitational wave sources to be detected. The next parts of this paper address
the detection of chirp waveforms from coalescence events in noisy data. Such
analysis is computationally intensive. Sect. 2 reviews a new and powerful
method of signal detection based on the GPU-implemented summed parallel
infinite impulse response filters. Such filters are intrinsically real time
alorithms, that can be used to rapidly detect and localise signals. Sect. 3 of
the paper reviews the use of GPU processors for rapid searching for
gravitational wave bursts that can arise from black hole births and
coalescences. In sect. 4 the use of GPU processors to enable fast efficient
statistical significance testing of gravitational wave event candidates is
reviewed. Sect. 5 of this paper addresses the method of multimessenger
astronomy where the discovery of electromagnetic counterparts of gravitational
wave events can be used to identify sources, understand their nature and obtain
much greater science outcomes from each identified event.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05608</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05608</id><created>2016-02-17</created><authors><author><keyname>Kowalik</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Lauri</keyname><forenames>Juho</forenames></author><author><keyname>Soca&#x142;a</keyname><forenames>Arkadiusz</forenames></author></authors><title>On the fine-grained complexity of rainbow coloring</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rainbow k-Coloring problem asks whether the edges of a given graph can be
colored in $k$ colors so that every pair of vertices is connected by a rainbow
path, i.e., a path with all edges of different colors. Our main result states
that for any $k\ge 2$, there is no algorithm for Rainbow k-Coloring running in
time $2^{o(n^{3/2})}$, unless ETH fails.
  Motivated by this negative result we consider two parameterized variants of
the problem. In Subset Rainbow k-Coloring problem, introduced by Chakraborty et
al. [STACS 2009, J. Comb. Opt. 2009], we are additionally given a set $S$ of
pairs of vertices and we ask if there is a coloring in which all the pairs in
$S$ are connected by rainbow paths. We show that Subset Rainbow k-Coloring is
FPT when parameterized by $|S|$. We also study Maximum Rainbow k-Coloring
problem, where we are additionally given an integer $q$ and we ask if there is
a coloring in which at least $q$ anti-edges are connected by rainbow paths. We
show that the problem is FPT when parameterized by $q$ and has a kernel of size
$O(q)$ for every $k\ge 2$ (thus proving that the problem is FPT), extending the
result of Ananth et al. [FSTTCS 2011].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05618</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05618</id><created>2016-02-17</created><authors><author><keyname>Larsson</keyname><forenames>Jacob</forenames></author><author><keyname>Borg</keyname><forenames>Markus</forenames></author><author><keyname>Olsson</keyname><forenames>Thomas</forenames></author></authors><title>Testing Quality Requirements of a System-of-Systems in the Public Sector
  - Challenges and Potential Remedies</title><categories>cs.SE</categories><comments>15 pages, accepted for publication in the Proceedings of the 3rd
  International Workshop on Requirements Engineering and Testing (RET'16)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality requirements is a difficult concept in software projects, and testing
software qualities is a well-known challenge. Without proper management of
quality requirements, there is an increased risk that the software product
under development will not meet the expectations of its future users. In this
paper, we share experiences from testing quality requirements when developing a
large system-of-systems in the public sector in Sweden. We complement the
experience reporting by analyzing documents from the case under study. As a
final step, we match the identified challenges with solution proposals from the
literature. We report five main challenges covering inadequate requirements
engineering and disconnected test managers. Finally, we match the challenges to
solutions proposed in the scientific literature, including integrated
requirements engineering, the twin peaks model, virtual plumblines, the QUPER
model, and architecturally significant requirements. Our experiences are
valuable to other large development projects struggling with testing of quality
requirements. Furthermore, the report could be used by as input to process
improvement activities in the case under study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05620</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05620</id><created>2016-02-17</created><authors><author><keyname>Hamkins</keyname><forenames>Jon</forenames></author></authors><title>The Golay Code Outperforms the Extended Golay Code Under Hard-Decision
  Decoding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the binary Golay code is slightly more power efficient than the
extended binary Golay code under maximum-likelihood (ML), hard-decision
decoding. In fact, if a codeword from the extended code is transmitted, one
cannot achieve a higher probability of correct decoding than by simply ignoring
the 24th symbol and using an ML decoder for the non-extended code on the first
23 symbols. This is so, despite the fact that using that last symbol would
allow one to sometimes correct error patterns with weight four. To our
knowledge the worse performance of the extended Golay code has not been
previously noted, but it is noteworthy considering that it is the extended
version of the code that has been preferred in many deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05622</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05622</id><created>2016-02-17</created><authors><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author><author><keyname>Buchin</keyname><forenames>Maike</forenames></author><author><keyname>Gudmundsson</keyname><forenames>Joachim</forenames></author><author><keyname>Horton</keyname><forenames>Michael</forenames></author><author><keyname>Sijben</keyname><forenames>Stef</forenames></author></authors><title>Compact Flow Diagrams for State Sequences</title><categories>cs.DS cs.CC</categories><comments>18 pages, 8 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the concept of compactly representing a large number of state
sequences, e.g., sequences of activities, as a flow diagram. We argue that the
flow diagram representation gives an intuitive summary that allows the user to
detect patterns among large sets of state sequences. Simplified, our aim is to
generate a small flow diagram that models the flow of states of all the state
sequences given as input. For a small number of state sequences we present
efficient algorithms to compute a minimal flow diagram. For a large number of
state sequences we show that it is unlikely that efficient algorithms exist.
More specifically, the problem is W[1]-hard if the number of state sequences is
taken as a parameter. We thus introduce several heuristics for this problem. We
argue about the usefulness of the flow diagram by applying the algorithms to
two problems in sports analysis. We evaluate the performance of our algorithms
on a football data set and generated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05629</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05629</id><created>2016-02-17</created><authors><author><keyname>McMahan</keyname><forenames>H. Brendan</forenames></author><author><keyname>Moore</keyname><forenames>Eider</forenames></author><author><keyname>Ramage</keyname><forenames>Daniel</forenames></author><author><keyname>Arcas</keyname><forenames>Blaise Ag&#xfc;era y</forenames></author></authors><title>Federated Learning of Deep Networks using Model Averaging</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern mobile devices have access to a wealth of data suitable for learning
models, which in turn can greatly improve the user experience on the device.
For example, language models can improve speech recognition and text entry, and
image models can automatically select good photos. However, this rich data is
often privacy sensitive, large in quantity, or both, which may preclude logging
to the data-center and training there using conventional approaches. We
advocate an alternative that leaves the training data distributed on the mobile
devices, and learns a shared model by aggregating locally-computed updates. We
term this decentralized approach Federated Learning.
  We present a practical method for the federated learning of deep networks
that proves robust to the unbalanced and non-IID data distributions that
naturally arise. This method allows high-quality models to be trained in
relatively few rounds of communication, the principal constraint for federated
learning. The key insight is that despite the non-convex loss functions we
optimize, parameter averaging over updates from multiple clients produces
surprisingly good results, for example decreasing the communication needed to
train an LSTM language model by two orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05635</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05635</id><created>2016-02-17</created><authors><author><keyname>Alrahman</keyname><forenames>Yehia Abd</forenames></author><author><keyname>De Nicola</keyname><forenames>Rocco</forenames></author><author><keyname>Loreti</keyname><forenames>Michele</forenames></author></authors><title>On the Power of Attribute-based Communication</title><categories>cs.LO</categories><comments>Extended Technical Report, All proofs are included in the appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In open systems, i.e. systems operating in an environment that they cannot
control and with components that may join or leave, behaviors can arise as side
effects of intensive components interaction. Finding ways to understand and
design these systems and, most of all, to model the interactions of their
components, is a difficult but important endeavor. To tackle these issues, we
present AbC, a calculus for attribute-based communication. An AbC system
consists of a set of parallel agents each of which is equipped with a set of
attributes. Communication takes place in an implicit multicast fashion, and
interactions among agents are dynamically established by taking into account
&quot;connections&quot; as determined by predicates over the attributes of agents. First,
the syntax and the semantics of the calculus are presented, then expressiveness
and effectiveness of AbC are demonstrated both in terms of modeling scenarios
featuring collaboration, reconfiguration, and adaptation and of the possibility
of encoding channel-based interactions and other interaction patterns.
Behavioral equivalences for AbC are introduced for establishing formal
relationships between different descriptions of the same system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05638</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05638</id><created>2016-02-17</created><authors><author><keyname>Baxter</keyname><forenames>Paul</forenames></author></authors><title>Memory-Centred Cognitive Architectures for Robots Interacting Socially
  with Humans</title><categories>cs.RO</categories><comments>Presented at 2nd Workshop on Cognitive Architectures for Social
  Human-Robot Interaction 2016 (arXiv:1602.01868)&quot;</comments><report-no>CogArch4sHRI/2016/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Memory-Centred Cognition perspective places an active association
substrate at the heart of cognition, rather than as a passive adjunct.
Consequently, it places prediction and priming on the basis of prior experience
to be inherent and fundamental aspects of processing. Social interaction is
taken here to minimally require contingent and co-adaptive behaviours from the
interacting parties. In this contribution, I seek to show how the
memory-centred cognition approach to cognitive architectures can provide an
means of addressing these functions. A number of example implementations are
briefly reviewed, particularly focusing on multi-modal alignment as a function
of experience-based priming. While there is further refinement required to the
theory, and implementations based thereon, this approach provides an
interesting alternative perspective on the foundations of cognitive
architectures to support robots engage in social interactions with humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05642</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05642</id><created>2016-02-17</created><authors><author><keyname>Abisheva</keyname><forenames>Adiya</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>When the Filter Bubble Bursts: Collective Evaluation Dynamics in Online
  Communities</title><categories>cs.SI physics.data-an physics.soc-ph stat.AP</categories><comments>20 pages, 4 figures, Submitted to the 8th International ACM Web
  Science Conference 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze online collective evaluation processes through positive and
negative votes in various social media. We find two modes of collective
evaluations that stem from the existence of filter bubbles. Above a threshold
of collective attention, negativity grows faster with positivity, as a sign of
the burst of a filter bubble when information reaches beyond the local social
context of a user. We analyze how collectively evaluated content can reach
large social contexts and create polarization, showing that emotions expressed
through text play a key role in collective evaluation processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05643</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05643</id><created>2016-02-17</created><authors><author><keyname>Long</keyname><forenames>Fan</forenames></author><author><keyname>Rinard</keyname><forenames>Martin</forenames></author></authors><title>An Analysis of the Search Spaces for Generate and Validate Patch
  Generation Systems</title><categories>cs.SE</categories><acm-class>D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first systematic analysis of the characteristics of patch
search spaces for automatic patch generation systems. We analyze the search
spaces of two current state-of-the-art systems, SPR and Prophet, with 16
different search space configurations. Our results are derived from an analysis
of 1104 different search spaces and 768 patch generation executions. Together
these experiments consumed over 9000 hours of CPU time on Amazon EC2.
  The analysis shows that 1) correct patches are sparse in the search spaces
(typically at most one correct patch per search space per defect), 2) incorrect
patches that nevertheless pass all of the test cases in the validation test
suite are typically orders of magnitude more abundant, and 3) leveraging
information other than the test suite is therefore critical for enabling the
system to successfully isolate correct patches.
  We also characterize a key tradeoff in the structure of the search spaces.
Larger and richer search spaces that contain correct patches for more defects
can actually cause systems to find fewer, not more, correct patches. We
identify two reasons for this phenomenon: 1) increased validation times because
of the presence of more candidate patches and 2) more incorrect patches that
pass the test suite and block the discovery of correct patches. These
fundamental properties, which are all characterized for the first time in this
paper, help explain why past systems often fail to generate correct patches and
help identify challenges, opportunities, and productive future directions for
the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05656</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05656</id><created>2016-02-17</created><authors><author><keyname>Wang</keyname><forenames>Zheng</forenames></author></authors><title>An Estimation Method Using Periodic Inspection of Indicators</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new approach for estimating the failure time
distribution using the indicator data. The indicators, which are checked by
periodic inspection of a standby redundant system, only convey whether at least
one failure occurs per interval. The estimation procedure first obtains the
estimation of the forward recurrence time using the indicator data. Then the
mean is estimated based on its relationship with the forward recurrence time.
And the estimation of the sampled Cdf is thus derived based on its relationship
with the forward recurrence time and the mean. Finally, the Cdf function is
estimated using interpolation method. The simulation results showed that the
estimation method performed well for the four Weibull distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05657</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05657</id><created>2016-02-17</created><authors><author><keyname>Matsubara</keyname><forenames>Shunichi</forenames></author></authors><title>The Computational Complexity of the Frobenius Problem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove that the decision version of the Frobenius problem is
Sigma_2^P-complete as a main theorem. Given a set A = {a_1, ..., a_n} of
coprime integers such that 2 &lt;= a_1 &lt; ... &lt; a_n, we call the greatest integer
that has no nonnegative integer combination of A the Frobenius number, and
denote it by g(A). We call the decision problem determining whether g(A) &gt;= k
for a given pair (A, k) the Frobenius problem, and denote it by Frobenius
Problem, where A = {a_1, ..., a_n} is a set of coprime integers such that 2 &lt;=
a_1 &lt; ... &lt; a_n and k is a positive integer. Sigma_2^P is a complexity class at
the second level of the polynomial hierarchy found by Meyer and Stockmeyer.
Pi_2^P is the class of the complements of problems in Sigma_2^P. In this paper,
we construct two polynomial-time reductions. First, we construct a reduction
from a Pi_2^P-complete problem Pi_2(3DM) to a problem Pi_2(AIK), and then
construct a reduction from Pi_2(AIK) to the complement problem of Frobenius
Problem. Finally, by proving that Frobenius Problem is in Sigma_2^P, we obtain
the main theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05659</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05659</id><created>2016-02-17</created><updated>2016-02-29</updated><authors><author><keyname>Liu</keyname><forenames>Fuqiang</forenames></author><author><keyname>Bi</keyname><forenames>Fukun</forenames></author><author><keyname>Yang</keyname><forenames>Yiding</forenames></author><author><keyname>Chen</keyname><forenames>Liang</forenames></author></authors><title>Boost Picking: A Universal Method on Converting Supervised
  Classification to Semi-supervised Classification</title><categories>cs.CV cs.LG</categories><comments>9 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes a universal method, Boost Picking, to train supervised
classification models mainly by un-labeled data. Boost Picking only adopts two
weak classifiers to estimate and correct the error. It is theoretically proved
that Boost Picking could train a supervised model mainly by un-labeled data as
effectively as the same model trained by 100% labeled data, only if recalls of
the two weak classifiers are all greater than zero and the sum of precisions is
greater than one. Based on Boost Picking, we present &quot;Test along with Training
(TawT)&quot; to improve the generalization of supervised models. Both Boost Picking
and TawT are successfully tested in varied little data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05660</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05660</id><created>2016-02-17</created><authors><author><keyname>Liu</keyname><forenames>Fuqiang</forenames></author><author><keyname>Bi</keyname><forenames>Fukun</forenames></author><author><keyname>Chen</keyname><forenames>Liang</forenames></author><author><keyname>Shi</keyname><forenames>Hao</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author></authors><title>Feature-Area Optimization: A Novel SAR Image Registration Method</title><categories>cs.CV</categories><comments>5 pages, 5 figures</comments><journal-ref>IEEE Geoscience and Remote Sensing Letter, Year: 2016, Volume: 13,
  Pages: 242 - 246</journal-ref><doi>10.1109/LGRS.2015.2507982</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This letter proposes a synthetic aperture radar (SAR) image registration
method named Feature-Area Optimization (FAO). First, the traditional area-based
optimization model is reconstructed and decomposed into three key but uncertain
factors: initialization, slice set and regularization. Next, structural
features are extracted by scale invariant feature transform (SIFT) in
dual-resolution space (SIFT-DRS), a novel SIFT-Like method dedicated to FAO.
Then, the three key factors are determined based on these features. Finally,
solving the factor-determined optimization model can get the registration
result. A series of experiments demonstrate that the proposed method can
register multi-temporal SAR images accurately and efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05671</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05671</id><created>2016-02-17</created><authors><author><keyname>Shirvanimoghaddam</keyname><forenames>Mahyar</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>Massive Multiple Access Based on Superposition Raptor Codes for M2M
  Communications</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine-to-machine (M2M) wireless systems aim to provide ubiquitous
connectivity between machine type communication (MTC) devices without any human
intervention. Given the exponential growth of MTC traffic, it is of utmost
importance to ensure that future wireless standards are capable of handling
this traffic. In this paper, we focus on the design of a very efficient massive
access strategy for highly dense cellular networks with M2M communications.
Several MTC devices are allowed to simultaneously transmit at the same resource
block by incorporating Raptor codes and superposition modulation. This
significantly reduces the access delay and improves the achievable system
throughput. A simple yet efficient random access strategy is proposed to only
detect the selected preambles and the number of devices which have chosen them.
No device identification is needed in the random access phase which
significantly reduces the signalling overhead. The proposed scheme is analyzed
and the maximum number of MTC devices that can be supported in a resource block
is characterized as a function of the message length, number of available
resources, and the number of preambles. Simulation results show that the
proposed scheme can effectively support a massive number of MTC devices for a
limited number of available resources, when the message size is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05681</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05681</id><created>2016-02-18</created><authors><author><keyname>Barthe</keyname><forenames>Gilles</forenames></author><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Gr&#xe9;goire</keyname><forenames>Benjamin</forenames></author><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Strub</keyname><forenames>Pierre-Yves</forenames></author></authors><title>A program logic for union bounds</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a probabilistic Hoare logic aHL based on the union bound, a basic
fact from probability theory. While this principle is simple, it is an
extremely common tool in the analysis of randomized algorithms. From a formal
verification perspective, the union bound allows compositional reasoning over
possible ways an algorithm may go wrong. The union bound is also a flexible
abstraction for expressing complex facts about probabilities---even though our
target programs and properties are probabilistic, assertions in our logic are
standard first-order formulas and do not involve probabilities or
distributions.
  Our logic can also prove accuracy properties for interactive programs, where
the program must produce intermediate outputs as soon as pieces of the input
arrive, rather than accessing the entire input at once. This setting also
enables adaptivity, where later inputs may depend on earlier intermediate
outputs.
  We implement our logic and show how to prove accuracy for several examples
from the differential privacy literature, both interactive and non-interactive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05682</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05682</id><created>2016-02-18</created><authors><author><keyname>Qi</keyname><forenames>Simeng</forenames></author><author><keyname>Huang</keyname><forenames>Zheng</forenames></author></authors><title>Identification of Audio Recording Devices From Background Noise</title><categories>cs.SD cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a study on identification of audio recording devices
from background noise, thus providing a method for forensics and copyright
disputes related. The audio signal is the sum of speech signal and noise
signal. Since usually, it's the speech which is regarded as the information to
be passed that people care about, a great amount of researches have been
dedicated to getting higher SNR. So there are many speech enhancement
algorithms to improve the quality of the speech signals, which is generally can
be seen much like reducing the noise. However, we make the underlying
hypothesis that the noise can be regarded as the intrinsic fingerprint traces
of an audio recording device in a way. These digital traces can be
characterized and identified by new machine learning techniques. Therefore, the
noise can serve as the intrinsic features for the identification. As for the
classifier, a Softmax classifier and an MLP classifier are used and compared.
The identification accuracy is up to 93\% among nine different devices, and
shows the method of getting feature vector from the noise of each device and
identifying with deeplearning techniques is viable, and well-preformed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05699</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05699</id><created>2016-02-18</created><authors><author><keyname>Wan</keyname><forenames>Hai</forenames></author><author><keyname>Zhang</keyname><forenames>Heng</forenames></author><author><keyname>Xiao</keyname><forenames>Peng</forenames></author><author><keyname>Huang</keyname><forenames>Haoran</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author></authors><title>Query Answering with Inconsistent Existential Rules under Stable Model
  Semantics</title><categories>cs.AI</categories><comments>Accepted by AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional inconsistency-tolerent query answering in ontology-based data
access relies on selecting maximal components of an ABox/database which are
consistent with the ontology. However, some rules in ontologies might be
unreliable if they are extracted from ontology learning or written by
unskillful knowledge engineers. In this paper we present a framework of
handling inconsistent existential rules under stable model semantics, which is
defined by a notion called rule repairs to select maximal components of the
existential rules. Surprisingly, for R-acyclic existential rules with
R-stratified or guarded existential rules with stratified negations, both the
data complexity and combined complexity of query answering under the rule
{repair semantics} remain the same as that under the conventional query
answering semantics. This leads us to propose several approaches to handle the
rule {repair semantics} by calling answer set programming solvers. An
experimental evaluation shows that these approaches have good scalability of
query answering under rule repairs on realistic cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05702</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05702</id><created>2016-02-18</created><authors><author><keyname>Van Eyndhoven</keyname><forenames>Simon</forenames></author><author><keyname>Francart</keyname><forenames>Tom</forenames></author><author><keyname>Bertrand</keyname><forenames>Alexander</forenames></author></authors><title>EEG-informed attended speaker extraction from recorded speech mixtures
  with application in neuro-steered hearing prostheses</title><categories>cs.SD cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy,
two-speaker acoustic scenario, relying on microphone array recordings from a
binaural hearing aid, which are complemented with electroencephalography (EEG)
recordings to infer the speaker of interest. METHODS: In this study, we propose
a modular processing flow that first extracts the two speech envelopes from the
microphone recordings, then selects the attended speech envelope based on the
EEG, and finally uses this envelope to inform a multi-channel speech separation
and denoising algorithm. RESULTS: Strong suppression of interfering
(unattended) speech and background noise is achieved, while the attended speech
is preserved. Furthermore, EEG-based auditory attention detection (AAD) is
shown to be robust to the use of noisy speech signals. CONCLUSIONS: Our results
show that AAD-based speaker extraction from microphone array recordings is
feasible and robust, even in noisy acoustic environments, and without access to
the clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Current
research on AAD always assumes the availability of the clean speech signals,
which limits the applicability in real settings. We have extended this research
to detect the attended speaker even when only microphone recordings with noisy
speech mixtures are available. This is an enabling ingredient for new
brain-computer interfaces and effective filtering schemes in neuro-steered
hearing prostheses. Here, we provide a first proof of concept for EEG-informed
attended speaker extraction and denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05703</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05703</id><created>2016-02-18</created><updated>2016-02-19</updated><authors><author><keyname>Di Lorenzo</keyname><forenames>Paolo</forenames></author><author><keyname>Barbarossa</keyname><forenames>Sergio</forenames></author><author><keyname>Banelli</keyname><forenames>Paolo</forenames></author><author><keyname>Sardellitti</keyname><forenames>Stefania</forenames></author></authors><title>Least Mean Squares Estimation of Graph Signals</title><categories>cs.LG cs.SY</categories><comments>Submitted to IEEE Transactions on Signal and Information Processing
  over Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications spanning from sensor to social networks, transportation
systems, gene regulatory networks or big data, the signals of interest are
defined over the vertices of a graph. The aim of this paper is to propose a
least mean square (LMS) strategy for adaptive estimation of signals defined
over graphs. Assuming the graph signal to be band-limited, over a known
bandwidth, the method enables reconstruction, with guaranteed performance in
terms of mean-square error, and tracking from a limited number of observations
over a subset of vertices. A detailed mean square analysis provides the
performance of the proposed method, and leads to several insights for designing
useful sampling strategies for graph signals. Numerical results validate our
theoretical findings, and illustrate the performance of the proposed method.
Furthermore, to cope with the case where the bandwidth is not known beforehand,
we propose a method that performs a sparse online estimation of the signal
support in the (graph) frequency domain, which enables online adaptation of the
graph sampling strategy. Finally, we apply the proposed method to build the
power spatial density cartography of a given operational region in a cognitive
network environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05705</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05705</id><created>2016-02-18</created><authors><author><keyname>Nix</keyname><forenames>Jonathan Darren</forenames></author></authors><title>Applying Boolean discrete methods in the production of a real-valued
  probabilistic programming model</title><categories>cs.AI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the application of some notable Boolean methods,
namely the Disjunctive Normal Form representation of logic table expansions,
and apply them to a real-valued logic model which utilizes quantities on the
range [0,1] to produce a probabilistic programming of a game character's logic
in mathematical form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05712</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05712</id><created>2016-02-18</created><authors><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author><author><keyname>Keusch</keyname><forenames>Ralph</forenames></author><author><keyname>Lengler</keyname><forenames>Johannes</forenames></author></authors><title>Average Distance in a General Class of Scale-Free Networks with
  Underlying Geometry</title><categories>cs.DM cs.SI</categories><comments>25 pages. arXiv admin note: text overlap with arXiv:1511.00576</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Chung-Lu random graphs, a classic model for real-world networks, each
vertex is equipped with a weight drawn from a power-law distribution (for which
we fix an exponent $2 &lt; \beta &lt; 3$), and two vertices form an edge
independently with probability proportional to the product of their weights.
Modern, more realistic variants of this model also equip each vertex with a
random position in a specific underlying geometry, which is typically
Euclidean, such as the unit square, circle, or torus. The edge probability of
two vertices then depends, say, inversely polynomial on their distance.
  We show that specific choices, such as the underlying geometry being
Euclidean or the dependence on the distance being inversely polynomial, do not
significantly influence the average distance, by studying a generic augmented
version of Chung-Lu random graphs. Specifically, we analyze a model where the
edge probability of two vertices can depend arbitrarily on their positions, as
long as the marginal probability of forming an edge (for two vertices with
fixed weights, one fixed position, and one random position) is as in Chung-Lu
random graphs, i.e., proportional to the product of their weights. The
resulting class contains Chung-Lu random graphs, hyperbolic random graphs, and
geometric inhomogeneous random graphs as special cases. Our main result is that
this general model has the same average distance as Chung-Lu random graphs, up
to a factor $1+o(1)$. The proof also yields that our model has a giant
component and polylogarithmic diameter with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05719</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05719</id><created>2016-02-18</created><authors><author><keyname>B&#x142;asiok</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author></authors><title>An improved analysis of the ER-SpUD dictionary learning algorithm</title><categories>cs.LG cs.DS cs.IT math.IT math.PR</categories><acm-class>I.2.6; F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In &quot;dictionary learning&quot; we observe $Y = AX + E$ for some
$Y\in\mathbb{R}^{n\times p}$, $A \in\mathbb{R}^{m\times n}$, and
$X\in\mathbb{R}^{m\times p}$. The matrix $Y$ is observed, and $A, X, E$ are
unknown. Here $E$ is &quot;noise&quot; of small norm, and $X$ is column-wise sparse. The
matrix $A$ is referred to as a {\em dictionary}, and its columns as {\em
atoms}. Then, given some small number $p$ of samples, i.e.\ columns of $Y$, the
goal is to learn the dictionary $A$ up to small error, as well as $X$. The
motivation is that in many applications data is expected to sparse when
represented by atoms in the &quot;right&quot; dictionary $A$ (e.g.\ images in the Haar
wavelet basis), and the goal is to learn $A$ from the data to then use it for
other applications.
  Recently, [SWW12] proposed the dictionary learning algorithm ER-SpUD with
provable guarantees when $E = 0$ and $m = n$. They showed if $X$ has
independent entries with an expected $s$ non-zeroes per column for $1 \lesssim
s \lesssim \sqrt{n}$, and with non-zero entries being subgaussian, then for
$p\gtrsim n^2\log^2 n$ with high probability ER-SpUD outputs matrices $A', X'$
which equal $A, X$ up to permuting and scaling columns (resp.\ rows) of $A$
(resp.\ $X$). They conjectured $p\gtrsim n\log n$ suffices, which they showed
was information theoretically necessary for {\em any} algorithm to succeed when
$s \simeq 1$. Significant progress was later obtained in [LV15].
  We show that for a slight variant of ER-SpUD, $p\gtrsim n\log(n/\delta)$
samples suffice for successful recovery with probability $1-\delta$. We also
show that for the unmodified ER-SpUD, $p\gtrsim n^{1.99}$ samples are required
even to learn $A, X$ with polynomially small success probability. This resolves
the main conjecture of [SWW12], and contradicts the main result of [LV15],
which claimed that $p\gtrsim n\log^4 n$ guarantees success whp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05721</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05721</id><created>2016-02-18</created><authors><author><keyname>Chatterjee</keyname><forenames>Kingshuk</forenames></author><author><keyname>Ray</keyname><forenames>Kumar Sankar</forenames></author></authors><title>Restricted deterministic Watson-Crick automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new model of deterministic Watson-Crick
automaton namely restricted deterministic Watson- Crick automaton which is a
deterministic Watson-Crick automaton where the complementarity string in the
lower strand is restricted to a language L. We examine the computational power
of the restricted model with respect to L being in different language classes
such as regular, unary regular, finite, context free and context sensitive. We
also show that computational power of restricted deterministic Watson- Crick
automata with L in regular languages is same as that of deterministic
Watson-Crick automata and that the set of all languages accepted by restricted
deterministic Watson-Crick automata with L in unary regular languages is a
proper subset of context free languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05723</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05723</id><created>2016-02-18</created><authors><author><keyname>Moustakas</keyname><forenames>Aristides</forenames></author></authors><title>The effects of marine protected areas over time and species dispersal
  potential: A quantitative conservation conflict attempt</title><categories>q-bio.PE cs.MA q-bio.QM stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protected areas are an important conservation measure. However, there are
controversial findings regarding whether closed areas are beneficial for
species and habitat conservation as well as landings. Species dispersal is
acknowledged as a key factor for the design and impacts of closed areas. A
series of agent based models using random diffusion to model fish dispersal
were run before and after habitat protection. All results were normalised
without the protected habitat in each scenario to detect the relative
difference after closing an area, all else being equal. Results show that
landings of species with short dispersal ranges will take longer to reach the
levels of pre Marine Protected Areas (MPAs) establishment than landings of
species with long dispersal ranges. Further the establishment of an MPA
generates a higher relative population source within the MPA for species with
low dispersal abilities than for species with high dispersal abilities. Results
derived here show that there exists a win-win feasible scenario that maximises
both fish biomass as well as fish catches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05741</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05741</id><created>2016-02-18</created><authors><author><keyname>Decurninge</keyname><forenames>Alexis</forenames></author><author><keyname>Guillaud</keyname><forenames>Maxime</forenames></author><author><keyname>Slock</keyname><forenames>Dirk</forenames></author></authors><title>Channel Covariance Estimation in Massive MIMO Frequency Division Duplex
  Systems</title><categories>cs.IT math.IT stat.AP</categories><comments>6 pages in Globecom, &quot;From theory to practice&quot; workshop, December
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel covariance is emerging as a critical ingredient of the acquisition of
instantaneous channel state information (CSI) in multi-user Massive MIMO
systems operating in frequency division duplex (FDD) mode. In this context,
channel reciprocity does not hold, and it is generally expected that covariance
information about the downlink channel must be estimated and fed back by the
user equipment (UE). As an alternative CSI acquisition technique, we propose to
infer the downlink covariance based on the observed uplink covariance. This
inference process relies on a dictionary of uplink/downlink covariance
matrices, and on interpolation in the corresponding Riemannian space; once the
dictionary is known, the estimation does not rely on any form of feedback from
the UE. In this article, we present several variants of the interpolation
method, and benchmark them through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05744</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05744</id><created>2016-02-18</created><authors><author><keyname>Bramson</keyname><forenames>Aaron</forenames></author><author><keyname>Vandermarliere</keyname><forenames>Benjamin</forenames></author></authors><title>Benchmarking Measures of Network Influence</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying key agents for the transmission of diseases (ideas, technology,
etc.) across social networks has predominantly relied on measures of centrality
on a static base network or a temporally flattened graph of agent interactions.
Various measures have been proposed as the best trackers of influence, such as
degree centrality, betweenness, and $k$-shell, depending on the structure of
the connectivity. We consider SIR and SIS propagation dynamics on a
temporally-extruded network of observed interactions and measure the
conditional marginal spread as the change in the magnitude of the infection
given the removal of each agent at each time: its temporal knockout (TKO)
score. We argue that the exhaustive approach of the TKO score makes it an
effective benchmark measure for evaluating the accuracy of other, often more
practical, measures of influence. We find that none of the common network
measures applied to the induced flat graphs are accurate predictors of network
propagation influence on the systems studied; however, temporal networks and
the TKO measure provide the requisite targets for the hunt for effective
predictive measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05751</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05751</id><created>2016-02-18</created><authors><author><keyname>Cabitza</keyname><forenames>Federico</forenames></author><author><keyname>Locoro</keyname><forenames>Angela</forenames></author></authors><title>Human-Data Interaction in Healthcare: Acknowledging Use-related Chasms
  to Design for Better Health Information</title><categories>cs.HC</categories><comments>8 pages, 2 Figures</comments><msc-class>H.5.3</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we focus on an emerging strand of IT-oriented research, namely
Human-Data Interaction (HDI) and how this can be applied to healthcare. HDI
regards both how humans create and use data by means of interactive systems,
which can both assist and constrain them, as well as to passively collect and
proactively generate data. Healthcare provides a challenging arena to test the
potential of HDI to provide a new, user-centered perspective on how data work
should be supported and assessed, especially in the light of the fact that data
are becoming increasingly big and that many tools are now available for the lay
people, including doctors and nurses, to interact with health-related data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05753</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05753</id><created>2016-02-18</created><authors><author><keyname>Finlayson</keyname><forenames>Mark A.</forenames></author><author><keyname>Erjavec</keyname><forenames>Toma&#x17e;</forenames></author></authors><title>Overview of Annotation Creation: Processes &amp; Tools</title><categories>cs.CL cs.HC</categories><comments>To appear in: James Pustejovsky and Nancy Ide (eds.) &quot;Handbook of
  Linguistic Annotation.&quot; 2016. New York: Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creating linguistic annotations requires more than just a reliable annotation
scheme. Annotation can be a complex endeavour potentially involving many
people, stages, and tools. This chapter outlines the process of creating
end-to-end linguistic annotations, identifying specific tasks that researchers
often perform. Because tool support is so central to achieving high quality,
reusable annotations with low cost, the focus is on identifying capabilities
that are necessary or useful for annotation tools, as well as common problems
these tools present that reduce their utility. Although examples of specific
tools are provided in many cases, this chapter concentrates more on abstract
capabilities and problems because new tools appear continuously, while old
tools disappear into disuse or disrepair. The two core capabilities tools must
have are support for the chosen annotation scheme and the ability to work on
the language under study. Additional capabilities are organized into three
categories: those that are widely provided; those that often useful but found
in only a few tools; and those that have as yet little or no available tool
support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05765</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05765</id><created>2016-02-18</created><authors><author><keyname>Jameel</keyname><forenames>Shoaib</forenames></author><author><keyname>Schockaert</keyname><forenames>Steven</forenames></author></authors><title>Entity Embeddings with Conceptual Subspaces as a Basis for Plausible
  Reasoning</title><categories>cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conceptual spaces are geometric representations of conceptual knowledge, in
which entities correspond to points, natural properties correspond to convex
regions, and the dimensions of the space correspond to salient features. While
conceptual spaces enable elegant models of various cognitive phenomena, the
lack of automated methods for constructing such representations have so far
limited their application in artificial intelligence. To address this issue, we
propose a method which learns a vector-space embedding of entities from
Wikipedia and constrains this embedding such that entities of the same semantic
type are located in some lower-dimensional subspace. We experimentally
demonstrate the usefulness of these subspaces as (approximate) conceptual space
representations by showing, among others, that important features can be
modelled as directions and that natural properties tend to correspond to convex
regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05768</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05768</id><created>2016-02-18</created><authors><author><keyname>Cooper</keyname><forenames>Colin</forenames></author><author><keyname>Radzik</keyname><forenames>Tomasz</forenames></author><author><keyname>Rivera</keyname><forenames>Nicolas</forenames></author></authors><title>The coalescing-branching random walk on expanders and the dual epidemic
  process</title><categories>cs.DC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information propagation on graphs is a fundamental topic in distributed
computing. One of the simplest model of information propagation is the push
protocol in which at each round each agent independently pushes the current
knowledge to a random neighbour. In this paper we study the so-called
coalescing-branching random walk (COBRA), in which each vertex pushes the
information to $k$ randomly selected neighbours and then it stops passing
information until it receives the information again. The aim of COBRA is to
propagate information fast but with limiting number of transmissions per vertex
per step. In this paper we study the cover time of the COBRA process defined as
the minimum time until each vertex has received the information at least once.
Our main result says that if $G$ is a $n$-vertex $r$-regular graph whose
transition matrix has second eigenvalue $\lambda$, then the cover time of $G$
is $O(\log n )$, if $1-\lambda &gt;0$ is constant, and $O((\log
n)/(1-\lambda)^3))$, if $1-\lambda \geq 1/\sqrt{r}$ and $r = o(n/\log n)$. This
improves the previous bound of $O(\log^2 n)$ for expander graphs. Our main new
tool in analysing the COBRA process is a duality relation between this process
and a discrete epidemic process called biased infection with persistent source
(BIPS). This duality says that the time to infect the whole graph in the BIPS
processes is roughly the same as the cover time of the COBRA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05772</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05772</id><created>2016-02-18</created><authors><author><keyname>Gerdjikov</keyname><forenames>Stefan</forenames></author><author><keyname>Schulz</keyname><forenames>Klaus U.</forenames></author></authors><title>Corpus analysis without prior linguistic knowledge - unsupervised mining
  of phrases and subphrase structure</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When looking at the structure of natural language, &quot;phrases&quot; and &quot;words&quot; are
central notions. We consider the problem of identifying such &quot;meaningful
subparts&quot; of language of any length and underlying composition principles in a
completely corpus-based and language-independent way without using any kind of
prior linguistic knowledge. Unsupervised methods for identifying &quot;phrases&quot;,
mining subphrase structure and finding words in a fully automated way are
described. This can be considered as a step towards automatically computing a
&quot;general dictionary and grammar of the corpus&quot;. We hope that in the long run
variants of our approach turn out to be useful for other kind of sequence data
as well, such as, e.g., speech, genom sequences, or music annotation. Even if
we are not primarily interested in immediate applications, results obtained for
a variety of languages show that our methods are interesting for many practical
tasks in text mining, terminology extraction and lexicography, search engine
technology, and related fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05819</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05819</id><created>2016-02-18</created><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author><author><keyname>Pinsker</keyname><forenames>Michael</forenames></author><author><keyname>Pongr&#xe1;cz</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>Constraint satisfaction problems for reducts of homogeneous graphs</title><categories>cs.LO cs.CC math.LO</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For $n\geq 3$, let $(H_n, E)$ denote the $n$-th Henson graph, i.e., the
unique countable homogeneous graph with exactly those finite graphs as induced
subgraphs that do not embed the complete graph on $n$ vertices. We show that
for all structures $\Gamma$ with domain $H_n$ whose relations are first-order
definable in $(H_n,E)$ the constraint satisfaction problem for $\Gamma$ is
either in P or is NP-complete.
  We moreover show a similar complexity dichotomy for all structures whose
relations are first-order definable in a homogeneous graph whose reflexive
closure is an equivalence relation.
  Together with earlier results, in particular for the random graph, this
completes the complexity classification of constraint satisfaction problems of
structures first-order definable in countably infinite homogeneous graphs: all
such problems are either in P or NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05828</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05828</id><created>2016-02-18</created><authors><author><keyname>Baget</keyname><forenames>Jean Francois</forenames></author><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Bouraoui</keyname><forenames>Zied</forenames></author><author><keyname>Croitoru</keyname><forenames>Madalina</forenames></author><author><keyname>Mugnier</keyname><forenames>Marie-Laure</forenames></author><author><keyname>Papini</keyname><forenames>Odile</forenames></author><author><keyname>Rocher</keyname><forenames>Swan</forenames></author><author><keyname>Tabia</keyname><forenames>Karim</forenames></author></authors><title>A General Modifier-based Framework for Inconsistency-Tolerant Query
  Answering</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general framework for inconsistency-tolerant query answering
within existential rule setting. This framework unifies the main semantics
proposed by the state of art and introduces new ones based on cardinality and
majority principles. It relies on two key notions: modifiers and inference
strategies. An inconsistency-tolerant semantics is seen as a composite modifier
plus an inference strategy. We compare the obtained semantics from a
productivity point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05829</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05829</id><created>2016-02-18</created><authors><author><keyname>Goldberg</keyname><forenames>Eugene</forenames></author></authors><title>Property Checking Without Invariant Generation</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce ProveProp, a procedure for proving safety properties. ProveProp
is based on a technique called Partial Quantifier Elimination (PQE). In
contrast to complete quantifier elimination, in PQE, only a part of the formula
is taken out of the scope of quantifiers. So PQE can be dramatically more
efficient than complete quantifier elimination. The appeal of ProveProp is
twofold. First, it can prove a property without generating an inductive
invariant. This is an implication of the fact that computing the reachability
diameter of a system reduces to PQE. Second, PQE enables depth-first search, so
ProveProp can be used to find very deep bugs. To prove property true, ProveProp
has to consider traces of length up to the reachability diameter. This may slow
down property checking for systems with a large diameter. We describe a
variation of ProveProp that can prove a property without generation of long
traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05830</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05830</id><created>2016-02-02</created><authors><author><keyname>Cazenille</keyname><forenames>Leo</forenames></author><author><keyname>Bredeche</keyname><forenames>Nicolas</forenames></author><author><keyname>Halloy</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>Automated optimisation of multi-level models of collective behaviour in
  a mixed society of animals and robots</title><categories>nlin.AO cs.MA cs.RO q-bio.QM</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Animal and robotic collective behaviours can exhibit complex dynamics that
require multi-level descriptions. Here, we are interested in developing a
multi-level modeling framework for the use of robots in studies about animal
collective decision-making. In this context, using robots can be useful for
validating models in silico, inducing calibrated repetitive stimuli to trigger
animal responses or modulating and controlling animal collective behaviour.
However, designing appropriate biomimetic robotic behaviour faces a major
challenge: how to go from the collective decision dynamics observed with
animals to an actual algorithmic implementation in robots. In previous work,
this was mainly done by hand, often by taking inspiration from human-designed
models. Typically, models of behaviour are either macroscopic, differential
equations of the population dynamics, or microscopic,explicit spatio-temporal
state of each individual. Only microscopic models can easily be implemented as
robot controllers. Here, we address the problem of automating the design of
lower level description models that can be implemented in robots and exhibit
the same collective dynamics as a given higher level model. We apply
evolutionary algorithms to simultaneously optimise the parameters of models
accounting for different levels of description. This methodology is applied to
an experimentally validated shelter-selection problem solved by gregarious
insects and robots. We successfully design and calibrate automatically both a
microscopic and a hybrid model exhibiting the same dynamics as a macroscopic
one. Our framework can be used for multi-level modeling of collective behaviour
in animal or robot populations and bio-hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05831</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05831</id><created>2016-02-18</created><authors><author><keyname>Urbat</keyname><forenames>Henning</forenames></author><author><keyname>Ad&#xe1;mek</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Chen</keyname><forenames>Liang-Ting</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author></authors><title>One Eilenberg Theorem to Rule Them All</title><categories>cs.FL math.CT</categories><acm-class>F.4.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Eilenberg-type correspondences, relating varieties of languages (e.g. of
finite words, infinite words, or trees) to pseudovarieties of finite algebras,
form the backbone of algebraic language theory. Numerous such correspondences
are known in the literature. We demonstrate that they all arise from the same
recipe: one models languages and the algebras recognizing them by monads on an
algebraic category, and applies a Stone-type duality. Our main contribution is
a generic variety theorem that covers e.g. Wilke's and Pin's work on
$\infty$-languages, the variety theorem for cost functions of Daviaud,
Kuperberg, and Pin, and unifies the two previous categorical approaches of
Boja\'nczyk and of Ad\'amek et al. In addition it gives a number of new
results, such as an extension of the local variety theorem of Gehrke,
Grigorieff, and Pin from finite to infinite words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05835</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05835</id><created>2016-02-18</created><authors><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Zou</keyname><forenames>Yulong</forenames></author></authors><title>Cognitive Network Cooperation for Green Cellular Networks</title><categories>cs.IT math.IT</categories><comments>8 pages, 5 figures in IEEE Access, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been a growing interest in green cellular networks
for the sake of reducing the energy dissipated by communications and networking
devices, including the base stations (BSs) and battery-powered user terminals
(UTs). This paper investigates the joint employment of cognition and
cooperation techniques invoked for improving the energy efficiency of cellular
networks. To be specific, the cellular devices first have to identify the
unused spectral bands (known as spectrum holes) using their spectrum sensing
functionality. Then, they cooperate for exploiting the detected spectrum holes
to support energy-efficient cellular communications. Considering the fact that
contemporary terminals (e.g., smart phones) support various wireless access
interfaces, we exploit either the Bluetooth or the Wi-Fi network operating
within the spectrum holes for supporting cellular communications with the
intention of achieving energy savings. This approach is termed as
\emph{cognitive network cooperation}, since different wireless access networks
cognitively cooperate with cellular networks. In order to illustrate the energy
efficiency benefits of using both cognition and cooperation, we study the
cooperation between television stations (TVSs) and BSs in transmitting to UTs
relying on an opportunistic exploitation of the TV spectrum, where the unused
TV spectral band is utilized in an opportunistic way, depending on whether it
is detected to be idle (or not). It is shown that for a given number of
information bits to be transmitted, the total energy consumed is significantly
reduced, when both cognition and cooperation are supported in cellular
networks, as compared to the conventional direct transmission, pure cognition
and pure cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05837</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05837</id><created>2016-02-18</created><updated>2016-03-02</updated><authors><author><keyname>Backurs</keyname><forenames>Arturs</forenames></author><author><keyname>Dikkala</keyname><forenames>Nishanth</forenames></author><author><keyname>Tzamos</keyname><forenames>Christos</forenames></author></authors><title>Tight Hardness Results for Maximum Weight Rectangles</title><categories>cs.DS cs.CC cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ weighted points (positive or negative) in $d$ dimensions, what is
the axis-aligned box which maximizes the total weight of the points it
contains?
  The best known algorithm for this problem is based on a reduction to a
related problem, the Weighted Depth problem [T. M. Chan, FOCS'13], and runs in
time $O(n^d)$. It was conjectured [Barbay et al., CCCG'13] that this runtime is
tight up to subpolynomial factors. We answer this conjecture affirmatively by
providing a matching conditional lower bound. We also provide conditional lower
bounds for the special case when points are arranged in a grid (a well studied
problem known as Maximum Subarray problem) as well as for other related
problems.
  All our lower bounds are based on assumptions that the best known algorithms
for the All-Pairs Shortest Paths problem (APSP) and for the Max-Weight k-Clique
problem in edge-weighted graphs are essentially optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05838</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05838</id><created>2016-02-18</created><authors><author><keyname>Brandstadt</keyname><forenames>Andreas</forenames></author><author><keyname>Mosca</keyname><forenames>Raffaele</forenames></author></authors><title>Maximum Weight Independent Set in lClaw-Free Graphs in Polynomial Time</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Maximum Weight Independent Set (MWIS) problem is a well-known NP-hard
problem. For graphs $G_1, G_2$, $G_1+G_2$ denotes the disjoint union of $G_1$
and $G_2$, and for a constant $l \ge 2$, $lG$ denotes the disjoint union of $l$
copies of $G$. A {\em claw} has vertices $a,b,c,d$, and edges $ab,ac,ad$. MWIS
can be solved for claw-free graphs in polynomial time; the first two polynomial
time algorithms were introduced in 1980 by \cite{Minty1980,Sbihi1980}, then
revisited by \cite{NakTam2001}, and recently improved by
\cite{FaeOriSta2011,FaeOriSta2014}, and by \cite{NobSas2011,NobSas2015} with
the best known time bound in \cite{NobSas2015}. Furthermore MWIS can be solved
for the following extensions of claw-free graphs in polynomial time: fork-free
graphs \cite{LozMil2008}, $K_2$+claw-free graphs \cite{LozMos2005}, and
apple-free graphs \cite{BraLozMos2010,BraKleLozMos2008}.
  This manuscript shows that for any constant $l$, MWIS can be solved for
$l$claw-free graphs in polynomial time. Our approach is based on Farber's
approach showing that every $2K_2$-free graph has ${\cal O}(n^2)$ maximal
independent sets \cite{Farbe1989}, which directly leads to a polynomial time
algorithm for MWIS on $2K_2$-free graphs by dynamic programming.
  Solving MWIS for $l$claw-free graphs in polynomial time extends known results
for claw-free graphs, for $lK_2$-free graphs for any constant $l$
\cite{Aleks1991,FarHujTuz1993,Prisn1995,TsuIdeAriShi1977}, for $K_2$+claw-free
graphs, for $2P_3$-free graphs \cite{LozMos2012}, and solves the open questions
for $2K_2+P_3$-free graphs and for $P_3$+claw-free graphs being two of the
minimal graph classes, defined by forbidding one induced subgraph, for which
the complexity of MWIS was an open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05852</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05852</id><created>2016-02-18</created><authors><author><keyname>Winkler</keyname><forenames>Kyrill</forenames></author><author><keyname>Schwarz</keyname><forenames>Manfred</forenames></author><author><keyname>Schmid</keyname><forenames>Ulrich</forenames></author></authors><title>Consensus in Directed Dynamic Networks with Short-Lived Stability</title><categories>cs.DC cs.DS</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of deterministically solving consensus in a
synchronous dynamic network with unreliable, unidirectional point-to-point
links, which are under the control of a message adversary. As the nodes in
practical dynamic networks typically start in a more or less uncoordinated way,
and only eventually reach normal operating conditions, we focus on eventually
stabilizing message adversaries here: Whereas the communication graphs of an
unknown number of initial rounds may be quite arbitrary, a &quot;stable period&quot; of x
consecutive rounds with a single common root component must eventually occur.
Earlier work has established that such strongly connected components without
incoming edges, which consist of the same set of nodes (with possibly varying
interconnect topology) during at least x=2D+1 rounds allow to solve consensus.
Herein, D is an upper bound on the number of rounds required by any root member
to reach every other node in the system. In this paper, we complete the
characterization of consensus solvability in this model by also considering
short-lived stabilization and non-uniform algorithms: We prove that it is
impossible to solve consensus for 0 &lt; x &lt; D+1 and D+1 &lt; x &lt; 2D+1 if at most a
constant-factor upper bound on the number of nodes n is known. Surprisingly,
though, consensus can be solved for x=D+1 by means of a novel non-uniform
algorithm presented and proved correct in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05853</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05853</id><created>2016-02-18</created><authors><author><keyname>Antikainen</keyname><forenames>Markku</forenames></author><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Trossen</keyname><forenames>Dirk</forenames></author><author><keyname>Sathiaseelan</keyname><forenames>Arjuna</forenames></author></authors><title>XBF: Scaling up Bloom-filter-based Source Routing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well known drawback of IP-multicast is that it requires per-group state to
be stored in the routers. Bloom-filter based source-routed multicast remedies
this problem by moving the state from the routers to the packets. However, a
fixed sized Bloom-filter can only store a limited number of items before the
false positive ratio grows too high implying scalability issues. Several
proposals have tried to address these scalability issues in Bloom-filter
forwarding. These proposals, however, unnecessarily increase the forwarding
complexity.
  In this paper, we present Extensible-Bloom-filter (XBF), a new framing and
forwarding solution which effectively circumvents the aforementioned drawbacks.
XBF partitions a network into sub-networks that reflect the network topology
and traffic patterns, and uses a separate fixed-length Bloom-filter in each of
these. We formulate this partition assignment problem into a balanced edge
partitioning problem, and evaluate it with simulations on realistic topologies.
Our results show that XBF scales to very large networks with minimal overhead
and completely eliminates the false-positives that have plagued the traditional
Bloom-filter-based forwarding protocols. It furthermore integrates with SDN
environments, making it highly suitable for deployments in off-the-shelf
SDN-based networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05856</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05856</id><created>2016-02-18</created><authors><author><keyname>Minkin</keyname><forenames>Ilia</forenames></author><author><keyname>Pham</keyname><forenames>Son</forenames></author><author><keyname>Medvedev</keyname><forenames>Paul</forenames></author></authors><title>TwoPaCo: An efficient algorithm to build the compacted de Bruijn graph
  from many complete genomes</title><categories>cs.DS q-bio.GN</categories><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: De Bruijn graphs have been proposed as a data structure to
facilitate the analysis of related whole genome sequences, in both a population
and comparative genomic settings. However, current approaches do not scale well
to many genomes of large size (such as mammalian genomes). Results: In this
paper, we present TwoPaCo, a simple and scalable low memory algorithm for the
direct construction of the compacted de Bruijn graph from a set of complete
genomes. We demonstrate that it can construct the graph for 100 simulated human
genomes in less then a day and eight real primates in less than two hours, on a
typical shared-memory machine. We believe that this progress will enable novel
biological analyses of hundreds of mammalian-sized genomes. Availability: Our
code and data is available for download from github.com/medvedevgroup/TwoPaCo
Contact: ium125@psu.edu
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05866</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05866</id><created>2016-02-18</created><authors><author><keyname>Riondato</keyname><forenames>Matteo</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>ABRA: Approximating Betweenness Centrality in Static and Dynamic Graphs
  with Rademacher Averages</title><categories>cs.DS</categories><acm-class>G.2.2; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ABRA, a suite of algorithms that compute and maintain
probabilistically-guaranteed, high-quality, approximations of the betweenness
centrality of all nodes (or edges) on both static and fully dynamic graphs. Our
algorithms rely on random sampling and their analysis leverages on Rademacher
averages and pseudodimension, fundamental concepts from statistical learning
theory. To our knowledge, this is the first application of these concepts to
the field of graph analysis. The results of our experimental evaluation show
that our approach is much faster than exact methods, and vastly outperforms, in
both speed and number of samples, current state-of-the-art algorithms with the
same quality guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05874</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05874</id><created>2015-04-26</created><authors><author><keyname>Kaur</keyname><forenames>Navdeep</forenames></author><author><keyname>Singh</keyname><forenames>Prabhsimran</forenames></author></authors><title>Delving into the Security Issues of Mobile Cloud Computing</title><categories>cs.CR</categories><comments>5 Pages, 1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Looking at the last decade, progress in technology has made a huge impact on
our lifestyles. Enhanced use of mobile phones has provided a technological
breakthrough, with the latest smartphones capturing the market. The word
smartphone is enough for everyone to understand the tremendous potential it
brought to the market in terms of economics as well as usability. Not only
this, this ever growing mobile mania has a lot more to offer. The familiarity
of applications like dropbox etc is a clear indication of the popularity of
mobile and cloud computing. But where we get all the benefits from this
computing platform, there are some of the challenges too. However, with the
enhanced facilities and luxuries, some challenges are always accompanied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05875</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05875</id><created>2016-02-18</created><updated>2016-03-01</updated><authors><author><keyname>Keren</keyname><forenames>Gil</forenames></author><author><keyname>Schuller</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Convolutional RNN: an Enhanced Model for Extracting Features from
  Sequential Data</title><categories>stat.ML cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional convolutional layers extract features from patches of data by
applying a non-linearity on an affine function of the input. We propose a model
that enhances this feature extraction process for the case of sequential data,
by feeding patches of the data into a recurrent neural network and using the
outputs or hidden states of the recurrent units to compute the extracted
features. By doing so, we exploit the fact that a window containing a few
frames of the sequential data is a sequence itself and this additional
structure might encapsulate valuable information. In addition, we allow for
more steps of computation in the feature extraction process, which is
potentially beneficial as an affine function followed by a non-linearity can
result in too simple features. Using our convolutional recurrent layers we
obtain an improvement in performance in two audio classification tasks,
compared to traditional convolutional layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05888</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05888</id><created>2016-02-18</created><authors><author><keyname>Alaca</keyname><forenames>&#x15e;aban</forenames></author><author><keyname>Millar</keyname><forenames>Goldwyn</forenames></author></authors><title>Character Values of the Sidelnikov-Lempel-Cohn-Eastman Sequences</title><categories>cs.IT math.CO math.IT math.NT</categories><msc-class>05B10, 94A55, 11T23, 11T71, 11B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary sequences with good autocorrelation properties and large linear
complexity are useful in stream cipher cryptography. The
Sidelnikov-Lempel-Cohn-Eastman (SLCE) sequences have nearly optimal
autocorrelation. However, the problem of determining the linear complexity of
the SLCE sequences is still open. Our approach is to exploit the fact that
character values associated with the SLCE sequences can be expressed in terms
of a certain type of Jacobi sum. By making use of known evaluations of Gauss
and Jacobi sums in the &quot;pure&quot; and &quot;small index&quot; cases, we are able to obtain
new insight into the linear complexity of the SLCE sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05889</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05889</id><created>2016-02-18</created><authors><author><keyname>Duda</keyname><forenames>Jarek</forenames></author></authors><title>Distortion-Resistant Hashing for rapid search of similar DNA subsequence</title><categories>cs.DS cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the basic tasks in bioinformatics is localizing a short subsequence
$S$, read while sequencing, in a long reference sequence $R$, like the human
geneome. A natural rapid approach would be finding a hash value for $S$ and
compare it with a prepared database of hash values for each of length $|S|$
subsequences of $R$. The problem with such approach is that it would only spot
a perfect match, while in reality there are lots of small changes:
substitutions, deletions and insertions.
  This issue could be repaired if having a hash function designed to tolerate
some small distortion accordingly to an alignment metric (like
Needleman-Wunch): designed to make that two similar sequences should most
likely give the same hash value. This paper discusses construction of
Distortion-Resistant Hashing (DRH) to generate such fingerprints for rapid
search of similar subsequences. The proposed approach is based on the rate
distortion theory: in a nearly uniform subset of length $|S|$ sequences, the
hash value represents the closest sequence to $S$. This gives some control of
the distance of collisions: sequences having the same hash value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05891</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05891</id><created>2016-02-18</created><authors><author><keyname>Silva</keyname><forenames>Leonardo Humberto</forenames></author><author><keyname>Hovadick</keyname><forenames>Daniel</forenames></author><author><keyname>Valente</keyname><forenames>Marco Tulio</forenames></author><author><keyname>Bergel</keyname><forenames>Alexandre</forenames></author><author><keyname>Anquetil</keyname><forenames>Nicolas</forenames></author><author><keyname>Etien</keyname><forenames>Anne</forenames></author></authors><title>JSClassFinder: A Tool to Detect Class-like Structures in JavaScript</title><categories>cs.SE cs.PL</categories><comments>VI Brazilian Conference on Software: Theory and Practice (Tools
  Track), p. 1-8, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing usage of JavaScript in web applications, there is a great
demand to write JavaScript code that is reliable and maintainable. To achieve
these goals, classes can be emulated in the current JavaScript standard
version. In this paper, we propose a reengineering tool to identify such
class-like structures and to create an object-oriented model based on
JavaScript source code. The tool has a parser that loads the AST (Abstract
Syntax Tree) of a JavaScript application to model its structure. It is also
integrated with the Moose platform to provide powerful visualization, e.g., UML
diagram and Distribution Maps, and well-known metric values for software
analysis. We also provide some examples with real JavaScript applications to
evaluate the tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05897</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05897</id><created>2016-02-18</created><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Frostig</keyname><forenames>Roy</forenames></author><author><keyname>Singer</keyname><forenames>Yoram</forenames></author></authors><title>Toward Deeper Understanding of Neural Networks: The Power of
  Initialization and a Dual View on Expressivity</title><categories>cs.LG cs.AI cs.CC cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general duality between neural networks and compositional
kernels, striving towards a better understanding of deep learning. We show that
initial representations generated by common random initializations are
sufficiently rich to express all functions in the dual kernel space. Hence,
though the training objective is hard to optimize in the worst case, the
initial weights form a good starting point for optimization. Our dual view also
reveals a pragmatic and aesthetic perspective of neural networks and
underscores their expressive power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05899</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05899</id><created>2016-02-18</created><authors><author><keyname>Adar</keyname><forenames>Ron</forenames></author><author><keyname>Epstein</keyname><forenames>Leah</forenames></author></authors><title>An algorithm for the weighted metric dimension of two-dimensional grids</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-dimensional grid consists of vertices of the form (i,j) for 1 \leq i
\leq m and 1 \leq j \leq n, for fixed m,n &gt; 1. Two vertices are adjacent if the
\ell_1 distance between their vectors is equal to 1. A landmark set is a subset
of vertices L \subseteq V, such that for any distinct pair of vertices u,v \in
V, there exists a vertex of L whose distances to u and v are not equal. We
design an efficient algorithm for finding a minimum landmark set with respect
to total cost in a grid graph with non-negative costs defined on the vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05900</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05900</id><created>2016-02-17</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Smith</keyname><forenames>Daniel V.</forenames></author><author><keyname>Montgomery</keyname><forenames>Christopher</forenames></author><author><keyname>Terriberry</keyname><forenames>Timothy B.</forenames></author></authors><title>An Iterative Linearised Solution to the Sinusoidal Parameter Estimation
  Problem</title><categories>cs.SD</categories><comments>23 pages</comments><journal-ref>Computers and Electrical Engineering (Elsevier), Vol. 36, No. 4,
  pp. 603-616, 2010</journal-ref><doi>10.1016/j.compeleceng.2008.11.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal processing applications use sinusoidal modelling for speech synthesis,
speech coding, and audio coding. Estimation of the model parameters involves
non-linear optimisation methods, which can be very costly for real-time
applications. We propose a low-complexity iterative method that starts from
initial frequency estimates and converges rapidly. We show that for N sinusoids
in a frame of length L, the proposed method has a complexity of O(LN), which is
significantly less than the matching pursuits method. Furthermore, the proposed
method is shown to be more accurate than the matching pursuits and
time-frequency reassignment methods in our experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05901</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05901</id><created>2016-02-18</created><authors><author><keyname>Liu</keyname><forenames>Hui</forenames></author><author><keyname>Wang</keyname><forenames>Kun</forenames></author><author><keyname>Luo</keyname><forenames>Jia</forenames></author><author><keyname>Chen</keyname><forenames>Zhangxin</forenames></author></authors><title>Development of A Platform for Large-scale Reservoir Simulations on
  Parallel computers</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our work on designing a platform for large-scale
reservoir simulations. Detailed components, such as grid and linear solver, and
data structures are introduced, which can serve as a guide to parallel
reservoir simulations and other parallel applications. The main objective of
platform is to support implementation of various parallel reservoir simulators
on distributed-memory parallel systems, where MPI (Message Passing Interface)
is employed for communications among computation nodes. It provides structured
grid due to its simplicity and cell-centered data is applied for each cell. The
platform has a distributed matrix and vector module and a map module. The
matrix and vector module is the base of our parallel linear systems. The map
connects grid and linear system modules, which defines various mappings between
grid and linear systems. Commonly-used Krylov subspace linear solvers are
implemented, including the restarted GMRES method and the BiCGSTAB method. It
also has an interface to a parallel algebraic multigrid solver, BoomerAMG from
HYPRE. Parallel general-purpose preconditioners and special preconditioners for
reservoir simulations are also developed. Various data structures are designed,
such as grid, cell, data, linear solver and preconditioner, and some key
default parameters are presented in this paper. The numerical experiments show
that our platform has excellent scalability and it can simulate giant reservoir
models with hundreds of millions of grid cells using thousands of CPU cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05908</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05908</id><created>2016-02-18</created><authors><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author></authors><title>Efficient approaches for escaping higher order saddle points in
  non-convex optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local search heuristics for non-convex optimizations are popular in applied
machine learning. However, in general it is hard to guarantee that such
algorithms even converge to a local minimum, due to the existence of
complicated saddle point structures in high dimensions. Many functions have
degenerate saddle points such that the first and second order derivatives
cannot distinguish them with local optima. In this paper we use higher order
derivatives to escape these saddle points: we design the first efficient
algorithm guaranteed to converge to a third order local optimum (while existing
techniques are at most second order). We also show that it is NP-hard to extend
this further to finding fourth order local optima.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05909</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05909</id><created>2016-02-18</created><authors><author><keyname>Deligkas</keyname><forenames>Argyrios</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author></authors><title>On the Complexity of Weighted Greedy Matchings</title><categories>cs.DM</categories><comments>19 pages, 5 figures</comments><acm-class>F.2.2; F.1.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the fact that in several cases a matching in a graph is stable
if and only if it is produced by a greedy algorithm, we study the problem of
computing a maximum weight greedy matching on edge-weighted graphs, denoted
GreedyMatching. We prove that GreedyMatching is hard even to approximate; in
particular, it is APX-complete, even on bipartite graphs of maximum degree 3
and only 5 different weight values. Our results imply that, unless P=NP, there
is no polynomial time approximation algorithm with ratio better than 0.9474 for
graphs with integer weights and 0.889 for graphs with rational weights. By
slightly modifying our reduction we prove that also the two decision variations
of the problem, where a specific edge or vertex needs to be matched, are
strongly NP-complete, even on bipartite graphs with 7 different weight values.
On the positive side, we consider a simple randomized greedy matching
algorithm, which we call RGMA, and we study its performance on the special
class of bush graphs, i.e. on edge-weighted graphs where all edges with equal
weight form a star (bush). We prove that RGMA achieves a
$\frac{2}{3}$-approximation on bush graphs, where the each bush has at most two
leafs, and we conjecture that the same approximation ratio is achieved even
when applied to general bush graphs. The importance of the performance of RGMA
on general bush graphs is highlighted by the fact that, as our results show, a
$\rho$-approximation ratio of RGMA for GreedyMatching on general bush graphs
implies that, for every $\epsilon&lt;1$, the randomized algorithm MRG (first
defined in \cite{ADFS95}) achieves a $(\rho-\epsilon)$-approximation of the
maximum cardinality matching in unweighted graphs. Thus, an affirmative answer
to our conjecture that $\rho=\frac{2}{3}$ would solve a longstanding open
problem \cite{ADFS95,PS12}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05914</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05914</id><created>2016-02-18</created><authors><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author></authors><title>Breaking the Logarithmic Barrier for Truthful Combinatorial Auctions
  with Submodular Bidders</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a central problem in Algorithmic Mechanism Design: constructing
truthful mechanisms for welfare maximization in combinatorial auctions with
submodular bidders. Dobzinski, Nisan, and Schapira provided the first mechanism
that guarantees a non-trivial approximation ratio of $O(\log^2 m)$ [STOC'06],
where $m$ is the number of items. This was subsequently improved to $O(\log
m\log \log m)$ [Dobzinski, APPROX'07] and then to $O(\log m)$ [Krysta and
Vocking, ICALP'12].
  In this paper we develop the first mechanism that breaks the logarithmic
barrier. Specifically, the mechanism provides an approximation ratio of
$O(\sqrt {\log m})$. Similarly to previous constructions, our mechanism uses
polynomially many value and demand queries, and in fact provides the same
approximation ratio for the larger class of XOS (a.k.a. fractionally
subadditive) valuations.
  We also develop a computationally efficient implementation of the mechanism
for combinatorial auctions with budget additive bidders. Although in general
computing a demand query is NP-hard for budget additive valuations, we observe
that the specific form of demand queries that our mechanism uses can be
efficiently computed when bidders are budget additive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05916</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05916</id><created>2016-02-18</created><authors><author><keyname>Yousefi</keyname><forenames>Niloofar</forenames></author><author><keyname>Lei</keyname><forenames>Yunwen</forenames></author><author><keyname>Kloft</keyname><forenames>Marius</forenames></author><author><keyname>Mollaghasemi</keyname><forenames>Mansooreh</forenames></author><author><keyname>Anagnastapolous</keyname><forenames>Georgios</forenames></author></authors><title>Local Rademacher Complexity-based Learning Guarantees for Multi-Task
  Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a Talagrand-type of concentration inequality for Multi-Task Learning
(MTL), using which we establish sharp excess risk bounds for MTL in terms of
distribution- and data-dependent versions of the Local Rademacher Complexity
(LRC). We also give a new bound on the LRC for strongly convex hypothesis
classes, which applies not only to MTL but also to the standard i.i.d. setting.
Combining both results, one can now easily derive fast-rate bounds on the
excess risk for many prominent MTL methods, including---as we
demonstrate---Schatten-norm, group-norm, and graph-regularized MTL. The derived
bounds reflect a relationship akeen to a conservation law of asymptotic
convergence rates. This very relationship allows for trading o? slower rates
w.r.t. the number of tasks for faster rates with respect to the number of
available samples per task, when compared to the rates obtained via a
traditional, global Rademacher analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05920</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05920</id><created>2016-02-18</created><authors><author><keyname>Kowsari</keyname><forenames>Kamran</forenames></author><author><keyname>Alassaf</keyname><forenames>Manal H.</forenames></author></authors><title>Weighted Unsupervised Learning for 3D Object Detection</title><categories>cs.CV</categories><comments>Weighted Unsupervised Learning, Object Detection, RGB-D camera,
  Kinect</comments><acm-class>I.4.8; I.5.4; I.2.10; I.5</acm-class><doi>10.14569/IJACSA.2016.070180</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces a novel weighted unsupervised learning for object
detection using an RGB-D camera. This technique is feasible for detecting the
moving objects in the noisy environments that are captured by an RGB-D camera.
The main contribution of this paper is a real-time algorithm for detecting each
object using weighted clustering as a separate cluster. In a preprocessing
step, the algorithm calculates the pose 3D position X, Y, Z and RGB color of
each data point and then it calculates each data point's normal vector using
the point's neighbor. After preprocessing, our algorithm calculates k-weights
for each data point; each weight indicates membership. Resulting in clustered
objects of the scene.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05922</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05922</id><created>2016-02-18</created><authors><author><keyname>Cioab&#x103;</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Gu</keyname><forenames>Xiaofeng</forenames></author></authors><title>Connectivity, toughness, spanning trees of bounded degree, and the
  spectrum of regular graphs</title><categories>math.CO cs.DM</categories><comments>13 pages; accepted to Czechoslovak Mathematical Journal, special
  issue dedicated to Professor Miroslav Fiedler</comments><msc-class>05C50, 05C40, 05C42, 05E99, 05C05, 15A18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present some new results describing connections between the
spectrum of a regular graph and its generalized connectivity, toughness, and
the existence of spanning trees with bounded degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05925</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05925</id><created>2016-02-18</created><authors><author><keyname>Purdy</keyname><forenames>Scott</forenames></author></authors><title>Encoding Data for HTM Systems</title><categories>cs.NE q-bio.NC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hierarchical Temporal Memory (HTM) is a biologically inspired machine
intelligence technology that mimics the architecture and processes of the
neocortex. In this white paper we describe how to encode data as Sparse
Distributed Representations (SDRs) for use in HTM systems. We explain several
existing encoders, which are available through the open source project called
NuPIC, and we discuss requirements for creating encoders for new types of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05928</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05928</id><created>2016-02-18</created><authors><author><keyname>Bouyer</keyname><forenames>Patricia</forenames></author><author><keyname>Markey</keyname><forenames>Nicolas</forenames></author><author><keyname>Randour</keyname><forenames>Mickael</forenames></author><author><keyname>Sangnier</keyname><forenames>Arnaud</forenames></author><author><keyname>Stan</keyname><forenames>Daniel</forenames></author></authors><title>Reachability in Networks of Register Protocols under Stochastic
  Schedulers</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the almost-sure reachability problem in a distributed system
obtained as the asynchronous composition of N copies (called processes) of the
same automaton (called protocol), that can communicate via a shared register
with finite domain. The automaton has two types of transitions:
write-transitions update the value of the register, while read-transitions move
to a new state depending on the content of the register. Non-determinism is
resolved by a stochastic scheduler. Given a protocol, we focus on almost-sure
reachability of a target state by one of the processes. The answer to this
problem naturally depends on the number N of processes. However, we prove that
our setting has a cut-off property: the answer to the almost-sure reachability
problem is constant when N is large enough; we then develop an EXPSPACE
algorithm deciding whether this constant answer is positive or negative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05931</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05931</id><created>2016-02-18</created><updated>2016-03-02</updated><authors><author><keyname>Cohen</keyname><forenames>Joseph Paul</forenames></author><author><keyname>Lo</keyname><forenames>Henry Z.</forenames></author><author><keyname>Ding</keyname><forenames>Wei</forenames></author></authors><title>RandomOut: Using a convolutional gradient norm to win The Filter Lottery</title><categories>cs.CV</categories><comments>Submitted to ICLR 2016 workshop track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks are sensitive to the random initialization of
filters. We call this The Filter Lottery (TFL) because the random numbers used
to initialize the network determine if you will &quot;win&quot; and converge to a
satisfactory local minimum. This issue forces networks to contain more filters
(be wider) to achieve higher accuracy because they have better odds of being
transformed into highly discriminative features at the risk of introducing
redundant features. To deal with this, we propose to evaluate and replace
specific convolutional filters that have little impact on the prediction. We
use the gradient norm to evaluate the impact of a filter on error, and
re-initialize filters when the gradient norm of its weights falls below a
specific threshold. This consistently improves accuracy across two datasets by
up to 1.8%. Our scheme RandomOut allows us to increase the number of filters
explored without increasing the size of the network. This yields more compact
networks which can train and predict with less computation, thus allowing more
powerful CNNs to run on mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05940</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05940</id><created>2016-02-18</created><authors><author><keyname>Bollig</keyname><forenames>Benedikt</forenames></author></authors><title>One-Counter Automata with Counter Visibility</title><categories>cs.FL cs.LO</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a one-counter automaton (OCA), one can read a letter from some finite
alphabet, increment and decrement the counter by one, or test it for zero. It
is well-known that universality and language inclusion for OCAs are
undecidable. We consider here OCAs with counter visibility: Whenever the
automaton produces a letter, it outputs the current counter value along with
it. Hence, its language is now a set of words over an infinite alphabet. We
show that universality and inclusion for that model are in PSPACE, thus no
harder than the corresponding problems for finite automata, which can actually
be considered as a special case. In fact, we show that OCAs with counter
visibility are effectively determinizable and closed under all boolean
operations. As a strict generalization, we subsequently extend our model by
registers. The general nonemptiness problem being undecidable, we impose a
bound on the number of register comparisons and show that the corresponding
nonemptiness problem is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05941</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05941</id><created>2016-02-18</created><authors><author><keyname>Gonzalez</keyname><forenames>Adriana</forenames></author><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Huang</keyname><forenames>Gang</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author></authors><title>Multi-resolution Compressive Sensing Reconstruction</title><categories>cs.CV</categories><comments>5 pages; 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reconstructing an image from compressive
measurements using a multi-resolution grid. In this context, the reconstructed
image is divided into multiple regions, each one with a different resolution.
This problem arises in situations where the image to reconstruct contains a
certain region of interest (RoI) that is more important than the rest. Through
a theoretical analysis and simulation experiments we show that the
multi-resolution reconstruction provides a higher quality of the RoI compared
to the traditional single-resolution approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05944</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05944</id><created>2016-02-18</created><authors><author><keyname>Grant</keyname><forenames>Erin</forenames></author><author><keyname>Nematzadeh</keyname><forenames>Aida</forenames></author><author><keyname>Stevenson</keyname><forenames>Suzanne</forenames></author></authors><title>The Interaction of Memory and Attention in Novel Word Generalization: A
  Computational Investigation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People exhibit a tendency to generalize a novel noun to the basic-level in a
hierarchical taxonomy -- a cognitively salient category such as &quot;dog&quot; -- with
the degree of generalization depending on the number and type of exemplars.
Recently, a change in the presentation timing of exemplars has also been shown
to have an effect, surprisingly reversing the prior observed pattern of
basic-level generalization. We explore the precise mechanisms that could lead
to such behavior by extending a computational model of word learning and word
generalization to integrate cognitive processes of memory and attention. Our
results show that the interaction of forgetting and attention to novelty, as
well as sensitivity to both type and token frequencies of exemplars, enables
the model to replicate the empirical results from different presentation
timings. Our results reinforce the need to incorporate general cognitive
processes within word learning models to better understand the range of
observed behaviors in vocabulary acquisition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05950</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05950</id><created>2016-02-18</created><authors><author><keyname>Anderson</keyname><forenames>David G.</forenames></author><author><keyname>Gu</keyname><forenames>Ming</forenames></author></authors><title>An Efficient, Sparsity-Preserving, Online Algorithm for Data
  Approximation</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Singular Value Decomposition (SVD) is a longstanding standard for data
approximation because it is optimal in the 2 and Frobenius norms. The SVD,
nevertheless, suffers from many setbacks, including computational cost, loss of
sparsity in the decomposition, and the inability to be updated easily when new
information arrives. Additionally, the SVD provides limited information on data
features and variables that best represent the data. In this work, we present a
truncated LU factorization called {\bf Spectrum-Revealing LU} (SRLU) for
effective low-rank matrix approximation, and develop the first algorithm to
compute an SRLU factorization, which is both efficient and reliable. Our
algorithm uses randomization and a novel LU updating technique with partial
pivoting, which is more stable than any other known LU updating algorithm. We
provide both approximation error bounds and singular value bounds for the SRLU
approximation computed by our algorithm. Our analysis suggests that SRLU is
competitive with the best low-rank matrix approximation methods, deterministic
or randomized, in both computational complexity and approximation quality.
Numeric experiments illustrate that SRLU preserves sparsity, highlights
important data features and variables, can be efficiently updated, and
calculates data approximations nearly as accurately as the SVD. To the best of
our knowledge this is the first practical variant of the LU decomposition for
efficient and effective low-rank matrix approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05973</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05973</id><created>2016-02-18</created><authors><author><keyname>Kaplan</keyname><forenames>Marc</forenames></author><author><keyname>Leurent</keyname><forenames>Ga&#xeb;tan</forenames></author><author><keyname>Leverrier</keyname><forenames>Anthony</forenames></author><author><keyname>Naya-Plasencia</keyname><forenames>Mar&#xed;a</forenames></author></authors><title>Breaking Symmetric Cryptosystems using Quantum Period Finding</title><categories>quant-ph cs.CR</categories><comments>31 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to Shor's algorithm, quantum computers are a severe threat for public key
cryptography. This motivated the cryptographic community to search for
quantum-safe solutions. On the other hand, the impact of quantum computing on
secret key cryptography is much less understood. In this paper, we consider
attacks where an adversary can query an oracle implementing a cryptographic
primitive in a quantum superposition of different states. This model gives a
lot of power to the adversary, but recent results show that it is nonetheless
possible to build secure cryptosystems in it.
  We study applications of a quantum procedure called Simon's algorithm (the
simplest quantum period finding algorithm) in order to attack symmetric
cryptosystems in this model. Following previous works in this direction, we
show that several classical attacks based on finding collisions can be
dramatically sped up using Simon's algorithm: finding a collision requires
$\Omega(2^{n/2})$ queries in the classical setting, but when collisions happen
with some hidden periodicity, they can be found with only $O(n)$ queries in the
quantum model.
  We obtain attacks with very strong implications. First, we show that the most
widely used modes of operation for authentication and authenticated encryption
(e.g. CBC-MAC, PMAC, GMAC, GCM, and OCB) are completely broken in this security
model. Our attacks are also applicable to many CAESAR candidates: CLOC, AEZ,
COPA, OTR, POET, OMD, and Minalpher. This is quite surprising compared to the
situation with encryption modes: Anand et al. show that standard modes are
secure when using a quantum-secure PRF.
  Second, we show that slide attacks can also be sped up using Simon's
algorithm. This is the first exponential speed up of a classical symmetric
cryptanalysis technique in the quantum model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05975</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05975</id><created>2016-02-18</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author></authors><title>The Daala Directional Deringing Filter</title><categories>cs.MM</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents the deringing filter used in the Daala royalty-free video
codec. The filter is based on a non-linear conditional replacement filter and
is designed for vectorization efficiency. It takes into account the direction
of edges and patterns by operating, on a block-by-block basis, along the
direction detected in each block. Objective results show a bit-rate reduction
between 4% and 8% on video sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05980</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05980</id><created>2016-02-18</created><authors><author><keyname>Xu</keyname><forenames>Bing</forenames></author><author><keyname>Huang</keyname><forenames>Ruitong</forenames></author><author><keyname>Li</keyname><forenames>Mu</forenames></author></authors><title>Revise Saturated Activation Functions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been generally believed that training deep neural networks is hard
with saturated activation functions, including Sigmoid and Tanh. Recent works
shows that deep Tanh networks are able to converge with careful model
initialization while deep Sigmoid networks still fail. In this paper, we
propose a re-scaled Sigmoid function which is able to maintain the gradient in
a stable scale. In addition, we break the symmetry of Tanh by penalizing the
negative part. Our preliminary results on deep convolution networks shown that,
even without stabilization technologies such as batch normalization and
sophisticated initialization, the &quot;re-scaled Sigmoid&quot; converges to local
optimality robustly. Furthermore the &quot;leaky Tanh&quot; is comparable or even
outperforms the state-of-the-art non-saturated activation functions such as
ReLU and leaky ReLU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05990</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05990</id><created>2016-02-18</created><authors><author><keyname>Cardoso</keyname><forenames>Jo&#xe3;o R.</forenames></author><author><keyname>Miraldo</keyname><forenames>Pedro</forenames></author><author><keyname>Araujo</keyname><forenames>Helder</forenames></author></authors><title>Pl\&quot;ucker Correction Problem: Analysis and Improvements in Efficiency</title><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A given six dimensional vector represents a 3D straight line in Plucker
coordinates if its coordinates satisfy the Klein quadric constraint. In many
problems aiming to find the Plucker coordinates of lines, noise in the data
and other type of errors contribute for obtaining 6D vectors that do not
correspond to lines, because of that constraint. A common procedure to overcome
this drawback is to find the Plucker coordinates of the lines that are closest
to those vectors. This is known as the Plucker correction problem. In this
article we propose a simple, closed-form, and global solution for this problem.
When compared with the state-of-the-art method, one can conclude that our
algorithm is easier and requires much less operations than previous techniques
(it does not require Singular Value Decomposition techniques).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.05996</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.05996</id><created>2016-02-18</created><authors><author><keyname>Neopane</keyname><forenames>Ojash</forenames></author><author><keyname>Das</keyname><forenames>Srinjoy</forenames></author><author><keyname>Arias-Castro</keyname><forenames>Ery</forenames></author><author><keyname>Kreutz-Delgado</keyname><forenames>Kenneth</forenames></author></authors><title>A Nonparametric Framework for Quantifying Generative Inference on
  Neuromorphic Systems</title><categories>cs.NE</categories><comments>Accepted for lecture presentation at ISCAS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann Machines and Deep Belief Networks have been successfully
used in probabilistic generative model applications such as image occlusion
removal, pattern completion and motion synthesis. Generative inference in such
algorithms can be performed very efficiently on hardware using a Markov Chain
Monte Carlo procedure called Gibbs sampling, where stochastic samples are drawn
from noisy integrate and fire neurons implemented on neuromorphic substrates.
Currently, no satisfactory metrics exist for evaluating the generative
performance of such algorithms implemented on high-dimensional data for
neuromorphic platforms. This paper demonstrates the application of
nonparametric goodness-of-fit testing to both quantify the generative
performance as well as provide decision-directed criteria for choosing the
parameters of the neuromorphic Gibbs sampler and optimizing usage of hardware
resources used during sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06005</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06005</id><created>2016-02-18</created><authors><author><keyname>Ausavarungnirun</keyname><forenames>Rachata</forenames></author><author><keyname>Fallin</keyname><forenames>Chris</forenames></author><author><keyname>Yu</keyname><forenames>Xiangyao</forenames></author><author><keyname>Chang</keyname><forenames>Kevin Kai-Wei</forenames></author><author><keyname>Nazario</keyname><forenames>Greg</forenames></author><author><keyname>Das</keyname><forenames>Reetuparna</forenames></author><author><keyname>Loh</keyname><forenames>Gabriel H.</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>Achieving both High Energy Efficiency and High Performance in On-Chip
  Communication using Hierarchical Rings with Deflection Routing</title><categories>cs.DC</categories><acm-class>C.1.2; B.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical ring networks, which hierarchically connect multiple levels of
rings, have been proposed in the past to improve the scalability of ring
interconnects, but past hierarchical ring designs sacrifice some of the key
benefits of rings by introducing more complex in-ring buffering and buffered
flow control. Our goal in this paper is to design a new hierarchical ring
interconnect that can maintain most of the simplicity of traditional ring
designs (no in-ring buffering or buffered flow control) while achieving high
scalability as more complex buffered hierarchical ring designs. Our design,
called HiRD (Hierarchical Rings with Deflection), includes features that allow
us to mostly maintain the simplicity of traditional simple ring topologies
while providing higher energy efficiency and scalability. First, HiRD does not
have any buffering or buffered flow control within individual rings, and
requires only a small amount of buffering between the ring hierarchy levels.
When inter-ring buffers are full, our design simply deflects flits so that they
circle the ring and try again, which eliminates the need for in-ring buffering.
Second, we introduce two simple mechanisms that provides an end-to-end delivery
guarantee within the entire network without impacting the critical path or
latency of the vast majority of network traffic. HiRD attains equal or better
performance at better energy efficiency than multiple versions of both a
previous hierarchical ring design and a traditional single ring design. We also
analyze our design's characteristics and injection and delivery guarantees. We
conclude that HiRD can be a compelling design point that allows higher energy
efficiency and scalability while retaining the simplicity and appeal of
conventional ring-based designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06006</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06006</id><created>2016-02-18</created><authors><author><keyname>Qi</keyname><forenames>Minglong</forenames></author><author><keyname>Xiong</keyname><forenames>Shengwu</forenames></author><author><keyname>Yuan</keyname><forenames>Jingling</forenames></author><author><keyname>Rao</keyname><forenames>Wenbi</forenames></author><author><keyname>Zhong</keyname><forenames>Luo</forenames></author></authors><title>On a Class of Almost Difference Sets Constructed by Using the
  Ding-Helleseth-Martinsens Constructions</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pseudorandom binary sequences with optimal balance and autocorrelation have
many applications in stream cipher, communication, coding theory, etc. It is
known that binary sequences with three-level autocorrelation should have an
almost difference set as their characteristic sets. How to construct new
families of almost difference set is an important research topic in such fields
as communication, coding theory and cryptography. In a work of Ding, Helleseth,
and Martinsen in 2001, the authors developed a new method, known as the
Ding-Helleseth-Martinsens Constructions in literature, of constructing an
almost difference set from product sets of GF(2) and the union of two
cyclotomic classes of order four. In the present paper, we have constructed two
classes of almost difference set with product sets between GF(2) and union sets
of the cyclotomic classes of order 12 using that method. In addition, we could
find there do not exist the Ding-Helleseth-Martinsens Constructions for the
cyclotomic classes of order six and eight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06007</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06007</id><created>2016-02-18</created><updated>2016-02-26</updated><authors><author><keyname>Qi</keyname><forenames>Minglong</forenames></author><author><keyname>Xiong</keyname><forenames>Shengwu</forenames></author><author><keyname>Yuan</keyname><forenames>Jinbgling</forenames></author><author><keyname>Rao</keyname><forenames>Wenbi</forenames></author><author><keyname>Zhong</keyname><forenames>Luo</forenames></author></authors><title>On the Nonexistence of the Ding-Helleseth-Martinsens Constructions of
  Almost Difference Set for Cyclotomic Classes of Order 6</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pseudorandom sequences with optimal three-level autocorrelation have
important applications in CDMA communication systems. Constructing the
sequences with three-level autocorrelation is equivalent to finding cyclic
almost difference sets as their supports. In a paper of Ding, Helleseth, and
Martinsen, the authors developed a new method known as the
Ding-Helleseth-Martinsens Constructions in literature to construct the almost
difference set using product set between GF(2) and union sets of cyclotomic
classes of order 4. In this correspondence, we show that there do not exist
such constructions for cyclotomic classes of order 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06009</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06009</id><created>2016-02-18</created><authors><author><keyname>Rahman</keyname><forenames>Mehnaz</forenames></author><author><keyname>Choi</keyname><forenames>Gwan S.</forenames></author></authors><title>Hardware Architecture of Complex K-best MIMO Decoder</title><categories>cs.IT math.IT</categories><comments>13 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a hardware architecture of complex K-best Multiple Input
Multiple Output (MIMO) decoder reducing the complexity of Maximum Likelihood
(ML) detector. We develop a novel low-power VLSI design of complex K-best
decoder for 8x8 MIMO and 64 QAM modulation scheme. Use of Schnorr-Euchner (SE)
enumeration and a new parameter, Rlimit in the design reduce the complexity of
calculating K-best nodes to a certain level with increased performance. The
total word length of only 16 bits has been adopted for the hardware design
limiting the bit error rate (BER) degradation to 0.3 dB with list size, K and
Rlimit equal to 4. The proposed VLSI architecture is modeled in Verilog HDL
using Xilinx and synthesized using Synopsys Design Vision in 45 nm CMOS
technology. According to the synthesize result, it achieves 1090.8 Mbps
throughput with power consumption of 782 mW and latency of 0.044 us. The
maximum frequency the design proposed is 181.8 MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06012</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06012</id><created>2016-02-18</created><authors><author><keyname>Burke</keyname><forenames>Kyle</forenames></author><author><keyname>Hearn</keyname><forenames>Bob</forenames></author></authors><title>PSPACE-Complete Two-Color Placement Games</title><categories>cs.CC</categories><comments>16 Pages, 16 figures</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that three placement games, Col, NoGo, and Fjords, are
PSPACE-complete on planar graphs. The hardness of Col and Fjords is shown via a
reduction from Bounded 2-Player Constraint Logic and NoGo is shown to be hard
directly from Col.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06023</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06023</id><created>2016-02-18</created><authors><author><keyname>Nallapati</keyname><forenames>Ramesh</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Sequence-to-Sequence RNNs for Text Summarization</title><categories>cs.CL</categories><comments>4 pages with references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we cast text summarization as a sequence-to-sequence problem
and apply the attentional encoder-decoder RNN that has been shown to be
successful for Machine Translation (Bahdanau et al. (2014)). Our experiments
show that the proposed architecture significantly outperforms the state-of-the
art model of Rush et al. (2015) on the Gigaword dataset without any additional
tuning. We also propose additional extensions to the standard architecture,
which we show contribute to further improvement in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06025</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06025</id><created>2016-02-18</created><authors><author><keyname>Ren</keyname><forenames>Yong</forenames></author><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author></authors><title>Spectral Learning for Supervised Topic Models</title><categories>cs.LG cs.CL cs.IR stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised topic models simultaneously model the latent topic structure of
large collections of documents and a response variable associated with each
document. Existing inference methods are based on variational approximation or
Monte Carlo sampling, which often suffers from the local minimum defect.
Spectral methods have been applied to learn unsupervised topic models, such as
latent Dirichlet allocation (LDA), with provable guarantees. This paper
investigates the possibility of applying spectral methods to recover the
parameters of supervised LDA (sLDA). We first present a two-stage spectral
method, which recovers the parameters of LDA followed by a power update method
to recover the regression model parameters. Then, we further present a
single-phase spectral algorithm to jointly recover the topic distribution
matrix as well as the regression weights. Our spectral algorithms are provably
correct and computationally efficient. We prove a sample complexity bound for
each algorithm and subsequently derive a sufficient condition for the
identifiability of sLDA. Thorough experiments on synthetic and real-world
datasets verify the theory and demonstrate the practical effectiveness of the
spectral algorithms. In fact, our results on a large-scale review rating
dataset demonstrate that our single-phase spectral algorithm alone gets
comparable or even better performance than state-of-the-art methods, while
previous work on spectral methods has rarely reported such promising
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06033</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06033</id><created>2016-02-18</created><updated>2016-02-22</updated><authors><author><keyname>Rizoiu</keyname><forenames>Marian-Andrei</forenames></author><author><keyname>Xie</keyname><forenames>Lexing</forenames></author><author><keyname>Sanner</keyname><forenames>Scott</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author><author><keyname>Yu</keyname><forenames>Honglin</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>Can this video be promoted? - Endogenous and exogenous popularity
  processes in social media</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study what drives the popularity of cultural items, by computationally
quantifying a video's potential to become viral. We propose to model popularity
lifecycles as the endogenous response to exogenous stimuli, dubbed Hawkes
intensity process. This model can explain the complex, multi-phase,
longitudinal popularity of a Youtube video. Moreover, it can also forecast the
future popularity, outperforming the state-of-the-art method. The model is
validated on a large dataset of the most Tweeted and shared videos over 6
months. A direct implication of this work is a prototype description for viral
videos - those that simultaneously exhibit a high sensitivity to external
promotions and a high endogenous response have the highest potential to become
viral.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06038</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06038</id><created>2016-02-18</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Feng</keyname><forenames>Wenlong</forenames></author><author><keyname>Huang</keyname><forenames>Mengxing</forenames></author></authors><title>Automatic Generation of High-Coverage Tests for RTL Designs using
  Software Techniques and Tools</title><categories>cs.SE cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Register Transfer Level (RTL) design validation is a crucial stage in the
hardware design process. We present a new approach to enhancing RTL design
validation using available software techniques and tools. Our approach converts
the source code of a RTL design into a C++ software program. Then a powerful
symbolic execution engine is employed to execute the converted C++ program
symbolically to generate test cases. To better generate efficient test cases,
we limit the number of cycles to guide symbolic execution. Moreover, we add
bit-level symbolic variable support into the symbolic execution engine.
Generated test cases are further evaluated by simulating the RTL design to get
accurate coverage. We have evaluated the approach on a floating point unit
(FPU) design. The preliminary results show that our approach can deliver
high-quality tests to achieve high coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06042</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06042</id><created>2016-02-18</created><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Rao</keyname><forenames>Nikhil</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit</forenames></author></authors><title>Structured Sparse Regression via Greedy Hard-Thresholding</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several learning applications require solving high-dimensional regression
problems where the relevant features belong to a small number of (overlapping)
groups. For very large datasets, hard thresholding methods have proven to be
extremely efficient under standard sparsity assumptions, but such methods
require NP hard projections when dealing with overlapping groups. In this
paper, we propose a simple and efficient method that avoids NP-hard projections
by using greedy approaches. Our proposed methods come with strong theoretical
guarantees even in the presence of poorly conditioned data, exhibit an
interesting computation-accuracy trade-off and can be extended to significantly
harder problems such as sparse overlapping groups. Experiments on both real and
synthetic data validate our claims and demonstrate that the proposed methods
are significantly faster than the best known greedy and convex relaxation
techniques for learning with structured sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06043</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06043</id><created>2016-02-18</created><authors><author><keyname>Horne</keyname><forenames>Ross</forenames></author><author><keyname>Tiu</keyname><forenames>Alwen</forenames></author><author><keyname>Aman</keyname><forenames>Bogdan</forenames></author><author><keyname>Ciobanu</keyname><forenames>Gabriel</forenames></author></authors><title>Private Names in Non-Commutative Logic</title><categories>cs.LO</categories><comments>Submitted for review 18/2/2016</comments><acm-class>F.4.1; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an expressive but decidable first-order system (named MAV1)
defined by using the calculus of structures, a generalisation of the sequent
calculus. In addition to first-order universal and existential quantifiers the
system incorporates a pair of nominal quantifiers called `new' and `wen',
distinct from the self-dual Gabbay-Pitts and Miller-Tiu nominal quantifiers.
The novelty of these operators is they are polarised in the sense that `new'
distributes over positive operators while `wen' distributes over negative
operators. This greater control of bookkeeping enables private names to be
modelled as required in applications such as the verification of security
protocols. Due to the presence of a self-dual non-commutative operator, a
direct proof of cut elimination for first-order quantifiers in the calculus of
structures is necessary; contrasting to related results for first-order
quantifiers in the calculus of structures which rely on a correspondence with
the sequent calculus. As a consequence of cut elimination, the complexity
classes for provability in MAV1 and several sub-systems are established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06045</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06045</id><created>2016-02-18</created><authors><author><keyname>Sivaraman</keyname><forenames>Anirudh</forenames></author><author><keyname>Subramanian</keyname><forenames>Suvinay</forenames></author><author><keyname>Agrawal</keyname><forenames>Anurag</forenames></author><author><keyname>Chole</keyname><forenames>Sharad</forenames></author><author><keyname>Chuang</keyname><forenames>Shang-Tse</forenames></author><author><keyname>Edsall</keyname><forenames>Tom</forenames></author><author><keyname>Alizadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Katti</keyname><forenames>Sachin</forenames></author><author><keyname>McKeown</keyname><forenames>Nick</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Hari</forenames></author></authors><title>Programmable Packet Scheduling</title><categories>cs.NI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Switches today provide a small set of scheduling algorithms. While we can
tweak scheduling parameters, we cannot modify algorithmic logic, or add a
completely new algorithm, after the switch has been designed. This paper
presents a design for a programmable packet scheduler, which allows scheduling
algorithms---potentially algorithms that are unknown today---to be programmed
into a switch without requiring hardware redesign.
  Our design builds on the observation that scheduling algorithms make two
decisions: in what order to schedule packets and when to schedule them.
Further, in many scheduling algorithms these decisions can be made when packets
are enqueued. We leverage this observation to build a programmable scheduler
using a single abstraction: the push-in first-out queue (PIFO), a priority
queue that maintains the scheduling order and time for such algorithms.
  We show that a programmable scheduler using PIFOs lets us program a wide
variety of scheduling algorithms. We present a detailed hardware design for
this scheduler for a 64-port 10 Gbit/s shared-memory switch with &lt;4% chip area
overhead on a 16-nm standard-cell library. Our design lets us program many
sophisticated algorithms, such as a 5-level hierarchical scheduler with
programmable scheduling algorithms at each level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06052</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06052</id><created>2016-02-19</created><authors><author><keyname>Fichte</keyname><forenames>Johannes K.</forenames></author><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Schindler</keyname><forenames>Irina</forenames></author></authors><title>Strong Backdoors for Default Logic</title><categories>cs.LO cs.AI cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a notion of backdoors to Reiter's propositional
default logic and study structural properties of it. Also we consider the
problems of backdoor detection (parameterised by the solution size) as well as
backdoor evaluation (parameterised by the size of the given backdoor), for
various kinds of target classes (cnf, horn, krom, monotone, identity). We show
that backdoor detection is fixed-parameter tractable for the considered target
classes, and backdoor evaluation is either fixed-parameter tractable, in
para-DP2 , or in para-NP, depending on the target class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06053</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06053</id><created>2016-02-19</created><authors><author><keyname>Zhang</keyname><forenames>Hongyi</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author></authors><title>First-order Methods for Geodesically Convex Optimization</title><categories>math.OC cs.LG stat.ML</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geodesic convexity generalizes the notion of (vector space) convexity to
nonlinear metric spaces. But unlike convex optimization, geodesically convex
(g-convex) optimization is much less developed. In this paper we contribute to
the understanding of g-convex optimization by developing iteration complexity
analysis for several first-order algorithms on Hadamard manifolds.
Specifically, we prove upper bounds for the global complexity of deterministic
and stochastic (sub)gradient methods for optimizing smooth and nonsmooth
g-convex functions, both with and without strong g-convexity. Our analysis also
reveals how the manifold geometry, especially \emph{sectional curvature},
impacts convergence rates. To the best of our knowledge, our work is the first
to provide global complexity analysis for first-order algorithms for general
g-convex optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06056</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06056</id><created>2016-02-19</created><authors><author><keyname>Zhou</keyname><forenames>Jiaji</forenames></author><author><keyname>Paolini</keyname><forenames>Robert</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author><author><keyname>Mason</keyname><forenames>Matthew T.</forenames></author></authors><title>A Convex Polynomial Force-Motion Model for Planar Sliding:
  Identification and Application</title><categories>cs.RO</categories><comments>IEEE International Conference on Robotics and Automation (ICRA) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a polynomial force-motion model for planar sliding. The set of
generalized friction loads is the 1-sublevel set of a polynomial whose gradient
directions correspond to generalized velocities. Additionally, the polynomial
is confined to be convex even-degree homogeneous in order to obey the maximum
work inequality, symmetry, shape invariance in scale, and fast invertibility.
We present a simple and statistically-efficient model identification procedure
using a sum-of-squares convex relaxation. Simulation and robotic experiments
validate the accuracy and efficiency of our approach. We also show practical
applications of our model including stable pushing of objects and free sliding
dynamic simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06057</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06057</id><created>2016-02-19</created><authors><author><keyname>Singh</keyname><forenames>Raghavendra</forenames></author></authors><title>Uniresolution representations of white-matter data from CoCoMac</title><categories>cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracing data as collated by CoCoMac, a seminal neuroinformatics database, is
at multiple resolutions -- white matter tracts were studied for areas and their
subdivisions by different reports. Network theoretic analysis of this
multi-resolution data often assumes that the data at various resolutions is
equivalent, which may not be correct. In this paper we propose three methods to
resolve the multi-resolution issue such that the resultant networks have
connectivity data at only one resolution. The different resultant networks are
compared in terms of their network analysis metrics and degree distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06058</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06058</id><created>2016-02-19</created><authors><author><keyname>Pae</keyname><forenames>Sung-il</forenames></author></authors><title>Binarizations in Random Number Generation</title><categories>cs.DS cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting procedures produce unbiased random bits from biased coin flips.
Binarizations take inputs from an m-faced dice and produce bit sequences to be
fed into a (binary) extracting procedure to obtain random bits, and this can be
done in an entropy-preserving manner, without loss of information. Such a
procedure has been proposed by Zhou and Bruck [1]. We discuss a family of such
entropy-preserving processes that we call complete binarizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06063</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06063</id><created>2016-02-19</created><authors><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Chen</keyname><forenames>Youjia</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Pricing and Resource Allocation via Game Theory for a Small-Cell Video
  Caching System</title><categories>cs.IT cs.GT cs.NI math.IT</categories><comments>Accepted to appear in IEEE Journal on Selected Areas in
  Communications, special issue on Video Distribution over Future Internet</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidence indicates that downloading on-demand videos accounts for a dramatic
increase in data traffic over cellular networks. Caching popular videos in the
storage of small-cell base stations (SBS), namely, small-cell caching, is an
efficient technology for reducing the transmission latency whilst mitigating
the redundant transmissions of popular videos over back-haul channels. In this
paper, we consider a commercialized small-cell caching system consisting of a
network service provider (NSP), several video retailers (VR), and mobile users
(MU). The NSP leases its SBSs to the VRs for the purpose of making profits, and
the VRs, after storing popular videos in the rented SBSs, can provide faster
local video transmissions to the MUs, thereby gaining more profits. We conceive
this system within the framework of Stackelberg game by treating the SBSs as a
specific type of resources. We first model the MUs and SBSs as two independent
Poisson point processes, and develop, via stochastic geometry theory, the
probability of the specific event that an MU obtains the video of its choice
directly from the memory of an SBS. Then, based on the probability derived, we
formulate a Stackelberg game to jointly maximize the average profit of both the
NSP and the VRs. Also, we investigate the Stackelberg equilibrium by solving a
non-convex optimization problem. With the aid of this game theoretic framework,
we shed light on the relationship between four important factors: the optimal
pricing of leasing an SBS, the SBSs allocation among the VRs, the storage size
of the SBSs, and the popularity distribution of the VRs. Monte-Carlo
simulations show that our stochastic geometry-based analytical results closely
match the empirical ones. Numerical results are also provided for quantifying
the proposed game-theoretic framework by showing its efficiency on pricing and
resource allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06064</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06064</id><created>2016-02-19</created><updated>2016-02-24</updated><authors><author><keyname>He</keyname><forenames>Tianxing</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Droppo</keyname><forenames>Jasha</forenames></author><author><keyname>Yu</keyname><forenames>Kai</forenames></author></authors><title>On Training Bi-directional Neural Network Language Model with Noise
  Contrastive Estimation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to train bi-directional neural network language model(NNLM) with
noise contrastive estimation(NCE). Experiments are conducted on a rescore task
on the PTB data set. It is shown that NCE-trained bi-directional NNLM
outperformed the one trained by conventional maximum likelihood training. But
still(regretfully), it did not out-perform the baseline uni-directional NNLM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06070</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06070</id><created>2016-02-19</created><authors><author><keyname>Singh</keyname><forenames>Raghavendra</forenames></author></authors><title>Vertex-disjoint Cycle Cover for graph signal processing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eigenvectors of the Laplacian of a cycle graph exhibit the sinusoidal
characteristics of the standard DFT basis, and signals defined on such graphs
are amenable to linear shift invariant (LSI) operations. In this paper we
propose to reduce a generic graph to its vertex-disjoint cycle cover, i.e., a
set of subgraphs that are cycles, that together contain all vertices of the
graph, and no two subgraphs have any vertices in common. Additionally if the
weight of an edge in the graph is a function of the variation in the signals on
its vertices, then maximally smooth cycles can be found, such that the
resulting DFT does not have high frequency components. We show that an image
graph can be reduced to such low-frequency cycles, and use that to propose a
simple image denoising algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06073</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06073</id><created>2016-02-19</created><authors><author><keyname>Morimae</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author><author><keyname>Gall</keyname><forenames>Francois Le</forenames></author></authors><title>Modified group non-membership is in AWPP</title><categories>quant-ph cs.CC</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the group non-membership problem is in QMA relative to any
group oracle and in ${\rm SPP}\cap{\rm BQP}$ relative to group oracles for
solvable groups. We consider a modified version of the group non-membership
problem where the order of the group is also given as an additional input. We
show that the problem is in AWPP relative to any group oracle. To show the
result, we use the idea of the postselected quantum computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06079</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06079</id><created>2016-02-19</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Arkadev</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Sagnik</forenames></author></authors><title>Tribes Is Hard in the Message Passing Model</title><categories>cs.CC</categories><doi>10.4230/LIPIcs.STACS.2015.224</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the point-to-point message passing model of communication in
which there are $k$ processors with individual private inputs, each $n$-bit
long. Each processor is located at the node of an underlying undirected graph
and has access to private random coins. An edge of the graph is a private
channel of communication between its endpoints. The processors have to compute
a given function of all their inputs by communicating along these channels.
While this model has been widely used in distributed computing, strong lower
bounds on the amount of communication needed to compute simple functions have
just begun to appear. In this work, we prove a tight lower bound of
$\Omega(kn)$ on the communication needed for computing the Tribes function,
when the underlying graph is a star of $k+1$ nodes that has $k$ leaves with
inputs and a center with no input. Lower bound on this topology easily implies
comparable bounds for others. Our lower bounds are obtained by building upon
the recent information theoretic techniques of Braverman et.al (FOCS'13) and
combining it with the earlier work of Jayram, Kumar and Sivakumar (STOC'03).
This approach yields information complexity bounds that is of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06091</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06091</id><created>2016-02-19</created><authors><author><keyname>Burgess</keyname><forenames>Mark</forenames></author></authors><title>On the scaling of functional spaces, from smart cities to cloud
  computing</title><categories>cs.CY physics.soc-ph</categories><comments>Physics and Society (physics.soc-ph)</comments><acm-class>C.0; C.1.3; D.4.8; G.2.2; G.3; H.1.1; I.2.11; I.6.1; J.2; J.7;
  K.4.0; K.6.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of spacetime, and its role in understanding functional systems has
received little attention in information science. Recent work, on the origin of
universal scaling in cities and biological systems, provides an intriguing
insight into the functional use of space, and its measurable effects. Cities
are large information systems, with many similarities to other technological
infrastructures, so the results shed new light indirectly on the scaling the
expected behaviour of smart pervasive infrastructures and the communities that
make use of them.
  Using promise theory, I derive and extend the scaling laws for cities to
expose what may be extrapolated to technological systems. From the promise
model, I propose an explanation for some anomalous exponents in the original
work, and discuss what changes may be expected due to technological
advancement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06093</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06093</id><created>2016-02-19</created><authors><author><keyname>de Menibus</keyname><forenames>Benjamin Hellouin</forenames><affiliation>I2M</affiliation></author><author><keyname>Sablik</keyname><forenames>Mathieu</forenames><affiliation>I2M</affiliation></author></authors><title>Self-organisation in cellular automata with coalescent particles:
  Qualitative and quantitative approaches</title><categories>math.DS cs.MA math.PR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces new tools to study self-organisation in a family of
simple cellular automata which contain some particle-like objects with good
collision properties (coalescence) in their time evolution. We draw an initial
configuration at random according to some initial $\sigma$-ergodic measure, and
use the limit measure to descrbe the asymptotic behaviour of the automata. We
first take a qualitative approach, i.e. we obtain information on the limit
measure(s). We prove that only particles moving in one particular direction can
persist asymptotically. This provides some previously unknown information on
the limit measures of various deterministic and probabilistic cellular
automata: 3 and 4-cyclic cellular automata (introduced in [Fis90b]), one-sided
captive cellular automata (introduced in [The04]), N. Fat\`es' candidate to
solve the density classification problem [Fat13], self stabilization process
toward a discrete line [RR15]... In a second time we restrict our study to to a
subclass, the gliders cellular automata. For this class we show quantitative
results, consisting in the asymptotic law of some parameters: the entry times
(generalising [KFD11]), the density of particles and the rate of convergence to
the limit measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06095</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06095</id><created>2016-02-19</created><authors><author><keyname>Aubrun</keyname><forenames>Nathalie</forenames><affiliation>LIGM</affiliation></author><author><keyname>Sablik</keyname><forenames>Mathieu</forenames><affiliation>LATP</affiliation></author></authors><title>Simulation of Effective Subshifts by Two-dimensional Subshifts of Finite
  Type</title><categories>math.DS cs.DM cs.FL</categories><proxy>ccsd</proxy><journal-ref>Acta Applicandae Mathematicae, Springer Verlag, 2013</journal-ref><doi>10.1007/s10440-013-9808-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study how a subshift can simulate another one, where the
notion of simulation is given by operations on subshifts inspired by the
dynamical systems theory (factor, projective subaction...). There exists a
correspondence between the notion of simulation and the set of forbidden
patterns. The main result of this paper states that any effective subshift of
dimension d -- that is a subshift whose set of forbidden patterns can be
generated by a Turing machine -- can be obtained by applying dynamical
operations on a subshift of finite type of dimension d + 1 -- a subshift that
can be defined by a finite set of forbidden patterns. This result improves
Hochman's [Hoc09].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06097</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06097</id><created>2016-02-19</created><authors><author><keyname>Boyer</keyname><forenames>Brice</forenames></author><author><keyname>Eder</keyname><forenames>Christian</forenames></author><author><keyname>Faug&#xe8;re</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Lachartre</keyname><forenames>Sylvian</forenames></author><author><keyname>Martani</keyname><forenames>Fayssal</forenames></author></authors><title>GBLA -- Gr\&quot;obner Basis Linear Algebra Package</title><categories>cs.SC</categories><comments>24 pages, 2 figures, 8 tables</comments><msc-class>13P10</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a system paper about a new GPLv2 open source C library GBLA
implementing and improving the idea of Faug\`ere and Lachartre (GB reduction).
We further exploit underlying structures in matrices generated during Gr\&quot;obner
basis computations in algorithms like F4 or F5 taking advantage of block
patterns by using a special data structure called multilines. Moreover, we
discuss a new order of operations for the reduction process. In various
different experimental results we show that GBLA performs better than GB
reduction or Magma in sequential computations (up to 40% faster) and scales
much better than GB reduction for a higher number of cores: On 32 cores we
reach a scaling of up to 26. GBLA is up to 7 times faster than GB reduction.
Further, we compare different parallel schedulers GBLA can be used with. We
also developed a new advanced storage format that exploits the fact that our
matrices are coming from Gr\&quot;obner basis computations, shrinking storage by a
factor of up to 4. A huge database of our matrices is freely available with
GBLA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06133</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06133</id><created>2016-02-19</created><authors><author><keyname>Atzeni</keyname><forenames>Italo</forenames></author><author><keyname>Maso</keyname><forenames>Marco</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Optimal Low-Complexity Self-Interference Cancellation for Full-Duplex
  MIMO Small Cells</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-interference (SI) significantly limits the performance of full-duplex
(FD) radio devices if not properly cancelled. State-of-the-art SI cancellation
(SIC) techniques at the receive chain implicitly set an upper bound on the
transmit power of the device. This paper starts from this observation and
proposes a transmit beamforming design for FD multiple-antenna radios that: i)
leverages the inherent SIC capabilities at the receiver and the channel state
information; and ii) exploits the potential of multiple antennas in terms of
spatial SIC. The proposed solution not only maximizes the throughput while
complying with the SIC requirements of the FD device, but also enjoys a very
low complexity that allows it to outperform state-of-the-art counterparts in
terms of processing time and power requirements. Numerical results show that
our transmit beamforming design achieves significant gains with respect to
applying zero-forcing to the SI channel when the number of transmit antennas is
small to moderate, which makes it particularly appealing for FD small-cell base
stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06136</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06136</id><created>2016-02-19</created><authors><author><keyname>Alsarem</keyname><forenames>Mazen</forenames><affiliation>DRIM</affiliation></author><author><keyname>Portier</keyname><forenames>Pierre-Edouard</forenames><affiliation>DRIM</affiliation></author><author><keyname>Calabretto</keyname><forenames>Sylvie</forenames><affiliation>DRIM</affiliation></author><author><keyname>Kosch</keyname><forenames>Harald</forenames></author></authors><title>Ordonnancement d'entit\'es pour la rencontre du web des documents et du
  web des donn\'ees</title><categories>cs.IR cs.AI</categories><comments>in French, Revue des Sciences et Technologies de l'Information -
  S{\'e}rie Document Num\'erique, Lavoisier, 2015, Nouvelles approches en
  recherche d'information, 18 (2-3/2015 ), pp.123-154</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advances of the Linked Open Data (LOD) initiative are giving rise to a
more structured web of data. Indeed, a few datasets act as hubs (e.g., DBpedia)
connecting many other datasets. They also made possible new web services for
entity detection inside plain text (e.g., DBpedia Spotlight), thus allowing for
new applications that will benefit from a combination of the web of documents
and the web of data. To ease the emergence of these new use-cases, we propose a
query-biased algorithm for the ranking of entities detected inside a web page.
Our algorithm combine link analysis with dimensionality reduction. We use
crowdsourcing for building a publicly available and reusable dataset on which
we compare our algorithm to the state of the art. Finally, we use this
algorithm for the construction of semantic snippets for which we evaluate the
usability and the usefulness with a crowdsourcing-based approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06149</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06149</id><created>2016-02-19</created><authors><author><keyname>Bianco</keyname><forenames>Simone</forenames></author></authors><title>Large age-gap face verification by feature injection in deep networks</title><categories>cs.CV</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new method for face verification across large age
gaps and also a dataset containing variations of age in the wild, the Large
Age-Gap (LAG) dataset, with images ranging from child/young to adult/old. The
proposed method exploits a deep convolutional neural network (DCNN) pre-trained
for the face recognition task on a large dataset and then fine-tuned for the
large age-gap face verification task. Finetuning is performed in a Siamese
architecture using a contrastive loss function. A feature injection layer is
introduced to boost verification accuracy, showing the ability of the DCNN to
learn a similarity metric leveraging external features. Experimental results on
the LAG dataset show that our method is able to outperform the face
verification solutions in the state of the art considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06155</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06155</id><created>2016-02-19</created><updated>2016-02-24</updated><authors><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Montalto</keyname><forenames>Alessandro</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Nollo</keyname><forenames>Giandomenico</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author></authors><title>Multiscale Analysis of Information Dynamics for Linear Multivariate
  Processes</title><categories>cs.IT math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of complex physical and physiological systems represented by
multivariate time series, an issue of great interest is the description of the
system dynamics over a range of different temporal scales. While
information-theoretic approaches to the multiscale analysis of complex dynamics
are being increasingly used, the theoretical properties of the applied measures
are poorly understood. This study introduces for the first time a framework for
the analytical computation of information dynamics for linear multivariate
stochastic processes explored at different time scales. After showing that the
multiscale processing of a vector autoregressive (VAR) process introduces a
moving average (MA) component, we describe how to represent the resulting VARMA
process using state-space (SS) models and how to exploit the SS model
parameters to compute analytical measures of information storage and
information transfer for the original and rescaled processes. The framework is
then used to quantify multiscale information dynamics for simulated
unidirectionally and bidirectionally coupled VAR processes, showing that
rescaling may lead to insightful patterns of information storage and transfer
but also to potentially misleading behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06156</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06156</id><created>2016-02-19</created><updated>2016-02-22</updated><authors><author><keyname>Frederic</keyname><forenames>Gabry</forenames></author><author><keyname>Valerio</keyname><forenames>Bioglio</forenames></author><author><keyname>Ingmar</keyname><forenames>Land</forenames></author></authors><title>On Edge Caching with Secrecy Constraints</title><categories>cs.IT math.IT</categories><comments>to appear in ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the problem of optimal cache placement under
secrecy constraints in heterogeneous networks, where small-cell base stations
are equipped with caches to reduce the overall backhaul load. For two models
for eavesdropping attacks, we formally derive the necessary conditions for
secrecy and we derive the corresponding achievable backhaul rate. In particular
we formulate the optimal caching schemes with secrecy constraints as a convex
optimization problem. We then thoroughly investigate the backhaul rate
performance of the heterogeneous network with secrecy constraints using
numerical simulations. We compare the system performance with and without
secrecy constraints and we analyze the influence of the system parameters, such
as the file popularity, size of the library files and the capabilities of the
small-cell base stations, on the overall performance of our optimal caching
strategy. Our results highlight the considerable impact of the secrecy
requirements on the overall caching performance of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06157</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06157</id><created>2016-02-19</created><authors><author><keyname>Issac</keyname><forenames>Jan</forenames></author><author><keyname>W&#xfc;thrich</keyname><forenames>Manuel</forenames></author><author><keyname>Cifuentes</keyname><forenames>Cristina Garcia</forenames></author><author><keyname>Bohg</keyname><forenames>Jeannette</forenames></author><author><keyname>Trimpe</keyname><forenames>Sebastian</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author></authors><title>Depth-Based Object Tracking Using a Robust Gaussian Filter</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of model-based 3D-tracking of objects given dense
depth images as input. Two difficulties preclude the application of a standard
Gaussian filter to this problem. First of all, depth sensors are characterized
by fat-tailed measurement noise. To address this issue, we show how a recently
published robustification method for Gaussian filters can be applied to the
problem at hand. Thereby, we avoid using heuristic outlier detection methods
that simply reject measurements if they do not match the model. Secondly, the
computational cost of the standard Gaussian filter is prohibitive due to the
high-dimensional measurement, i.e. the depth image. To address this problem, we
propose an approximation to reduce the computational complexity of the filter.
In quantitative experiments on real data we show how our method clearly
outperforms the standard Gaussian filter. Furthermore, we compare its
performance to a particle-filter-based tracking method, and observe comparable
computational efficiency and improved accuracy and smoothness of the estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06159</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06159</id><created>2016-02-19</created><updated>2016-02-22</updated><authors><author><keyname>Even</keyname><forenames>Guy</forenames></author><author><keyname>Levi</keyname><forenames>Reut</forenames></author><author><keyname>Medina</keyname><forenames>Moti</forenames></author><author><keyname>Rosen</keyname><forenames>Adi</forenames></author></authors><title>Sublinear Random Access Generators for Preferential Attachment Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of generating random graphs in evolving random graph
models. In the standard approach, the whole graph is chosen randomly according
to the distribution of the model before answering queries to the adjacency
lists of the graph. Instead, we propose to answer queries by generating the
graphs on-the-fly while respecting the probability space of the random graph
model.
  We focus on two random graph models: the Barab{\'{a}}si-Albert Preferential
Attachment model (BA-graphs) and the random recursive tree model. We present
sublinear randomized generating algorithms for both models. Per query, the
running time, the increase in space, and the number of random bits consumed are
$\poly\log(n)$ with probability $1-1/\poly(n)$, where $n$ denotes the number of
vertices.
  This result shows that, although the BA random graph model is defined
sequentially, random access is possible without chronological evolution. In
addition to a conceptual contribution, on-the-fly generation of random graphs
can serve as a tool for simulating sublinear algorithms over large BA-graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06166</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06166</id><created>2016-02-19</created><authors><author><keyname>Gangloff</keyname><forenames>Silv&#xe8;re</forenames></author><author><keyname>de Menibus</keyname><forenames>Benjamin Hellouin</forenames></author></authors><title>Computing the entropy of one-dimensional decidable subshifts</title><categories>math.DS cs.CC cs.DM</categories><comments>12 pages, 1 table, submitted to ICALP 2016</comments><msc-class>37B10</msc-class><acm-class>G.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterise the diffculty of computing topological entropy of
one-dimensional subshifts under various dynamical restrictions. Entropy is
known to be computable from above for general one-dimensional subshifts, and
the aim of the article is to distinguish under which dynamical properties it
becomes computable. The considered properties are minimality, unique
ergodicity, and various forms of topological mixing. In each case, we are also
able to characterise by computability conditions the set of reals that can
appear as topological entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06167</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06167</id><created>2016-02-19</created><authors><author><keyname>Xu</keyname><forenames>Xiangxiang</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Zhang</keyname><forenames>Xiujun</forenames></author><author><keyname>Xiao</keyname><forenames>Limin</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author></authors><title>Deployment of 5G Networking Infrastructure with Machine Type
  Communication Considerations</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures. This paper has been accepted by ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing optimal strategies to deploy small cell stations is crucial to meet
the quality-of-service requirements in next-generation cellular networks with
constrained deployment costs. In this paper, a general deployment framework is
proposed to jointly optimize the locations of backhaul aggregate nodes, small
base stations, machine aggregators, and multi-hop wireless backhaul links to
accommodate both human-type and machine-type communications. The goal is to
provide deployment solutions with best coverage performance under cost
constraints. The formulated problem is shown to be a multi-objective integer
programming for which it is challenging to obtain the optimal solutions. To
solve the problem, a heuristic algorithm is proposed by combining Lagrangian
relaxation, the weighted sum method, the $\epsilon$-constraint method and tabu
search to obtain both the solutions and bounds, for the objective function.
Simulation results show that the proposed framework can provide solutions with
better performance compared with conventional deployment models in scenarios
where available fiber connections are scarce. Furthermore, the gap between
obtained solutions and the lower bounds is quite tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06169</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06169</id><created>2016-02-19</created><authors><author><keyname>Even</keyname><forenames>Guy</forenames></author><author><keyname>Medina</keyname><forenames>Moti</forenames></author><author><keyname>Patt-Shamir</keyname><forenames>Boaz</forenames></author></authors><title>Competitive Path Computation and Function Placement in SDNs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a task of serving requests that arrive in an online fashion in
Software-Defined Networks (SDNs) with network function virtualization (NFV).
Each request specifies an abstract routing and processing &quot;plan&quot; for a flow.
Each processing function can be performed by a specified subset of servers in
the system. The algorithm needs to either reject the request or admit it and
return detailed routing (a.k.a. &quot;path computation&quot;) and processing assignment
(&quot;function placement&quot;). Each request also specifies the communication bandwidth
and the processing load it requires. Components in the system (links and
processors) have bounded capacity; a feasible solution may not violate the
capacity constraints. Requests have benefits and the goal is to maximize the
total benefit of accepted requests.
  In this paper we first formalize the problem, and propose a new service model
that allows us to cope with requests with unknown duration. The new service
model augments the traditional accept/reject schemes with a new possible
response of &quot;stand by.&quot; Our main result is an online algorithm for path
computation and function placement that guarantees, in each time step,
throughput of at least $\Omega\left(\frac{\text{OPT}^*}{\log n}\right)$, where
$n$ is the system size and $\text{OPT}^*$ is an upper bound on the maximal
possible throughput. The guarantee holds assuming that requests ask for at most
an $O\left(1/{\log n}\right)$-fraction of the capacity of any component in the
system. Furthermore, the guarantee holds even though our algorithm serves
requests in an all-or-nothing fashion using a single path and never preempts
accepted flows, while $\text{OPT}^*$ may serve fractional requests, may split
the allocation over multiple paths, and may arbitrarily preempt and resume
service of requests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06174</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06174</id><created>2016-02-19</created><authors><author><keyname>Even</keyname><forenames>Guy</forenames></author><author><keyname>Medina</keyname><forenames>Moti</forenames></author><author><keyname>Ros&#xe9;n</keyname><forenames>Adi</forenames></author></authors><title>A Constant Approximation Algorithm for Scheduling Packets on Line
  Networks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of scheduling packets on directed line
networks. Each node in the network has a local buffer of bounded size $B$, and
each edge (or link) can transmit a limited number $c$ of packets in every time
unit. The input to the problems consists of $n$ - the size of the network, $B$
- the node buffer sizes, $c$ - the links capacities, and a set of
source-destination packet requests with a release times. A solution for this
problem is a schedule that delivers packets to their destination without
violating the capacity constraints of the network (buffers or edges). Our goal
is to design an algorithm that computes a schedule that maximizes the number of
packets that arrive to their destination.
  We present a randomized approximation algorithm with constant approximation
ratio in expectation for the case where the buffer-size to link-capacity ratio
$B/c$ is constant. This improves over the $O(\log^* n)$-approximation algorithm
of R\&quot;{a}cke-Ros\'{e}n (SPAA 2009).
  The algorithm supports &quot;soft&quot; deadlines in the following sense. Packets may
have deadlines, and the algorithm delivers the accepted packets no later than
$\log n$ time units after their respective deadlines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06183</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06183</id><created>2016-02-19</created><authors><author><keyname>Wu</keyname><forenames>Ke</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>Node-By-Node Greedy Deep Learning for Interpretable Features</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilayer networks have seen a resurgence under the umbrella of deep
learning. Current deep learning algorithms train the layers of the network
sequentially, improving algorithmic performance as well as providing some
regularization. We present a new training algorithm for deep networks which
trains \emph{each node in the network} sequentially. Our algorithm is orders of
magnitude faster, creates more interpretable internal representations at the
node level, while not sacrificing on the ultimate out-of-sample performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06199</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06199</id><created>2016-02-19</created><authors><author><keyname>Rosnes</keyname><forenames>Eirik</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author></authors><title>Asymptotic Analysis and Spatial Coupling of Counter Braids</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. Parts of this
  paper have been presented at the IEEE Information Theory Workshop, Hobart,
  Australia, Nov. 2014, and at the IEEE Information Theory Workshop, Jeju
  Island, Korea, Oct. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A counter braid (CB) is a novel counter architecture introduced by Lu et al.
in 2007 for per-flow measurements on high-speed links. CBs achieve an
asymptotic compression rate (under optimal decoding) that matches the entropy
lower bound of the flow size distribution. In this paper, we apply the concept
of spatial coupling to CBs to improve their belief propagation (BP) threshold,
and analyze the performance of the resulting spatially-coupled CBs (SC-CBs). We
introduce an equivalent bipartite graph representation of CBs with identical
iteration-by-iteration finite-length and asymptotic performance. Based on this
equivalent representation, we then analyze the asymptotic performance of
single-layer CBs and SC-CBs. In particular, we show that the potential
threshold and the area threshold are equal. We also derive the Maxwell decoder
for CBs and prove that the area threshold is an upper bound on the Maxwell
decoding threshold, which, in turn, is a lower bound on the maximum a
posteriori (MAP) decoding threshold. We then show that the area under the
extended BP extrinsic information transfer curve (defined for the equivalent
graph), computed for the expected residual CB graph when a peeling decoder
equivalent to the BP decoder stops, is equal to zero precisely at the area
threshold. This, combined with the analysis of the Maxwell decoder and
simulation results, leads us to the conjecture that the area threshold is in
fact equal to the Maxwell decoding threshold and hence a lower bound on the MAP
decoding threshold. Interestingly, SC-CBs do not show the well-known phenomenon
of threshold saturation of the BP decoding threshold to the MAP decoding
threshold characteristic of spatially-coupled low-density parity-check codes
and other coupled systems. However, SC-CBs yield better BP decoding thresholds
than their uncoupled counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06209</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06209</id><created>2016-02-19</created><authors><author><keyname>Li</keyname><forenames>Qianrui</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Gresset</keyname><forenames>Nicolas</forenames></author></authors><title>Cooperative Channel Estimation for Coordinated Transmission with Limited
  Backhaul</title><categories>cs.IT math.IT</categories><comments>26 pages, 7 figures, submitted to IEEE transaction on Wireless
  Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obtaining accurate global channel state information (CSI) at multiple
transmitter devices is critical to the performance of many coordinated
transmission schemes. Practical CSI local feedback often leads to noisy and
partial CSI estimates at each transmitter. With rate-limited bi-directional
backhaul, transmitters have the opportunity to exchange a few CSI-related bits
towards establishing global CSIT. This work investigates the possible
strategies towards this goal. We propose a novel decentralized algorithm that
produces MMSE-optimal global channel estimates at each device from combining
local feedback and backhaul-exchanged information. The method adapts to
arbitrary initial information topologies and feedback noise statistics. The
advantage over conventional CSI exchange mechanisms in a network MIMO (CoMP)
setting are highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06213</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06213</id><created>2016-02-19</created><authors><author><keyname>Wang</keyname><forenames>Li-Xin</forenames></author></authors><title>Modeling Stock Price Dynamics with Fuzzy Opinion Networks</title><categories>q-fin.TR cs.SI cs.SY q-fin.CP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a mathematical model for the word-of-mouth communications among
stock investors through social networks and explore how the changes of the
investors' social networks influence the stock price dynamics and vice versa.
An investor is modeled as a Gaussian fuzzy set (a fuzzy opinion) with the
center and standard deviation as inputs and the fuzzy set itself as output.
Investors are connected in the following fashion: the center input of an
investor is taken as the average of the neighbors' outputs, where two investors
are neighbors if their fuzzy opinions are close enough to each other, and the
standard deviation (uncertainty) input is taken with local, global or external
reference schemes to model different scenarios of how investors define
uncertainties. The centers and standard deviations of the fuzzy opinions are
the expected prices and their uncertainties, respectively, that are used as
inputs to the price dynamic equation. We prove that with the local reference
scheme the investors converge to different groups in finite time, while with
the global or external reference schemes all investors converge to a consensus
within finite time and the consensus may change with time in the external
reference case. We show how to model trend followers, contrarians and
manipulators within this mathematical framework and prove that the biggest
enemy of a manipulator is the other manipulators. We perform Monte Carlo
simulations to show how the model parameters influence the price dynamics, and
we apply a modified version of the model to the daily closing prices of fifteen
top banking and real estate stocks in Hong Kong for the recent two years from
Dec. 5, 2013 to Dec. 4, 2015 and discover that a sharp increase of the combined
uncertainty is a reliable signal to predict the reversal of the current price
trend.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06215</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06215</id><created>2016-02-19</created><authors><author><keyname>Ba&#x15f;tu&#x11f;</keyname><forenames>Ejder</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Zeydan</keyname><forenames>Engin</forenames></author><author><keyname>Kader</keyname><forenames>Manhal Abdel</forenames></author><author><keyname>Karatepe</keyname><forenames>Alper</forenames></author><author><keyname>Er</keyname><forenames>Ahmet Salih</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Big Data Meets Telcos: A Proactive Caching Perspective</title><categories>cs.IT cs.NI math.IT</categories><comments>8 pages, 5 figures</comments><journal-ref>IEEE/KICS Journal of Communications and Networks, vol. 17, no. 6,
  pp. 549-557, December 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile cellular networks are becoming increasingly complex to manage while
classical deployment/optimization techniques and current solutions (i.e., cell
densification, acquiring more spectrum, etc.) are cost-ineffective and thus
seen as stopgaps. This calls for development of novel approaches that leverage
recent advances in storage/memory, context-awareness, edge/cloud computing, and
falls into framework of big data. However, the big data by itself is yet
another complex phenomena to handle and comes with its notorious 4V: velocity,
voracity, volume and variety. In this work, we address these issues in
optimization of 5G wireless networks via the notion of proactive caching at the
base stations. In particular, we investigate the gains of proactive caching in
terms of backhaul offloadings and request satisfactions, while tackling the
large-amount of available data for content popularity estimation. In order to
estimate the content popularity, we first collect users' mobile traffic data
from a Turkish telecom operator from several base stations in hours of time
interval. Then, an analysis is carried out locally on a big data platform and
the gains of proactive caching at the base stations are investigated via
numerical simulations. It turns out that several gains are possible depending
on the level of available information and storage size. For instance, with 10%
of content ratings and 15.4 Gbyte of storage size (87% of total catalog size),
proactive caching achieves 100% of request satisfaction and offloads 98% of the
backhaul when considering 16 base stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06219</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06219</id><created>2016-02-19</created><authors><author><keyname>Morales</keyname><forenames>Alfredo J.</forenames></author><author><keyname>Vavilala</keyname><forenames>Vaibhav</forenames></author><author><keyname>Benito</keyname><forenames>Rosa M.</forenames></author><author><keyname>Bar-Yam</keyname><forenames>Yaneer</forenames></author></authors><title>Global Patterns of Human Synchronization</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media are transforming global communication and coordination and
provide unprecedented opportunities for studying socio-technical domains. Here
we study global dynamical patterns of communication on Twitter across many
scales. Underlying the observed patterns is both the diurnal rotation of the
earth, day and night, and the synchrony required for contingency of actions
between individuals. We find that urban areas show a cyclic contraction and
expansion that resembles heartbeats linked to social rather than natural
cycles. Different urban areas have characteristic signatures of daily
collective activities. We show that the differences detected are consistent
with a new emergent global synchrony that couples behavior in distant regions
across the world. Although local synchrony is the major force that shapes the
collective behavior in cities, a larger-scale synchronization is beginning to
occur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06220</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06220</id><created>2016-02-19</created><authors><author><keyname>Kavvos</keyname><forenames>G. A.</forenames></author></authors><title>Kleene's Two Kinds of Recursion</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is an elementary expository article regarding the application of
Kleene's Recursion Theorems in making definitions by recursion. Whereas the
Second Recursion Theorem (SRT) is applicable in a first-order setting, the
First Recursion Theorem (FRT) requires a higher-order setting. In some cases
both theorems are applicable, but one is stronger than the other: the FRT
always produces least fixed points, but this is not always the case with the
SRT. Nevertheless, an old result by Rogers allows us to bridge this gap by
subtly redefining the implementation of a higher-order functional in order to
bring it to a `standard form.'
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06221</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06221</id><created>2016-02-19</created><authors><author><keyname>Peressotti</keyname><forenames>Marco</forenames></author></authors><title>Endofunctors modelling higher-order behaviours</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how the abstract behaviours of higher-order systems can
be modelled as final coalgebras of suitable behavioural functors. These
functors have the challenging peculiarity to be circularly defined with their
own final coalgebra. Our main contribution is a general construction for
defining these functors, solving this circularity which is the essence of
higher-order behaviours. This characterisation is syntax agnostic. To achieve
this property, we shift from term passing to behaviour passing: in the former
higher-order is expressed by passing around syntactic objects (such as terms or
processes) as representations of behaviours whereas the former ditches the
syntactic encoding altogether and works directly with behaviours i.e. semantic
objects. From this perspective, the former can be seen as syntactic
higher-order whereas the later as semantic higher-order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06223</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06223</id><created>2016-02-19</created><updated>2016-02-22</updated><authors><author><keyname>Jones</keyname><forenames>Shawn M.</forenames></author><author><keyname>Shankar</keyname><forenames>Harihar</forenames></author></authors><title>Rules of Acquisition for Mementos and Their Content</title><categories>cs.DL</categories><comments>16 pages, 6 figures, 13 listings</comments><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text extraction from web pages has many applications, including web crawling
optimization and document clustering. Though much has been written about the
acquisition of content from live web pages, content acquisition of archived web
pages, known as mementos, remains a relatively new enterprise. In the course of
conducting a study with almost 700,000 web pages, we encountered issues
acquiring mementos and extracting text from them. The acquisition of memento
content via HTTP is expected to be a relatively painless exercise, but we have
found cases to the contrary. We also find that the parsing of HTML, already
known to be problematic, can be more complex when one attempts to extract the
text of mementos across many web archives, due to issues involving different
memento presentation behaviors, as well as the age of the HTML in their
mementos. For the benefit of others acquiring mementos across many web
archives, we document those experiences here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06225</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06225</id><created>2016-02-19</created><authors><author><keyname>Ndiaye</keyname><forenames>Eugene</forenames></author><author><keyname>Fercoq</keyname><forenames>Olivier</forenames></author><author><keyname>Gramfort</keyname><forenames>Alexandre</forenames></author><author><keyname>Salmon</keyname><forenames>Joseph</forenames></author></authors><title>GAP Safe Screening Rules for Sparse-Group-Lasso</title><categories>stat.ML cs.LG math.OC stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In high dimensional settings, sparse structures are crucial for efficiency,
either in term of memory, computation or performance. In some contexts, it is
natural to handle more refined structures than pure sparsity, such as for
instance group sparsity. Sparse-Group Lasso has recently been introduced in the
context of linear regression to enforce sparsity both at the feature level and
at the group level. We adapt to the case of Sparse-Group Lasso recent safe
screening rules that discard early in the solver irrelevant features/groups.
Such rules have led to important speed-ups for a wide range of iterative
methods. Thanks to dual gap computations, we provide new safe screening rules
for Sparse-Group Lasso and show significant gains in term of computing time for
a coordinate descent implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06229</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06229</id><created>2016-02-19</created><authors><author><keyname>Mitchell</keyname><forenames>Chris J</forenames></author></authors><title>On the security of 2-key triple DES</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reconsiders the security offered by 2-key triple DES, an
encryption technique that remains widely used despite recently being
de-standardised by NIST. A generalisation of the 1990 van Oorschot-Wiener
attack is described, constituting the first advance in cryptanalysis of 2-key
triple DES since 1990. We give further attack enhancements that together imply
that the widely used estimate that 2-key triple DES provides 80 bits of
security can no longer be regarded as conservative; the widely stated assertion
that the scheme is secure as long as the key is changed regularly is also
challenged. The main conclusion is that, whilst not completely broken, the
margin of safety for 2-key triple DES is slim, and efforts to replace it, at
least with its 3-key variant, should be pursued with some urgency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06230</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06230</id><created>2016-02-19</created><updated>2016-03-07</updated><authors><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Sparse Signal Detection with Compressive Measurements via Partial
  Support Set Estimation</title><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of sparse signal detection based on
partial support set estimation with compressive measurements in a distributed
network. Multiple nodes in the network are assumed to observe sparse signals
which share a common but unknown support. In the traditional compressive
sensing (CS) framework, estimation of the complete support set is required
since the goal is to reconstruct the sparse signal. However, in sparse signal
detection, a reliable detection decision can be made using a partially or
inaccurately estimated support set with a small number of measurements and
reduced computational complexity. We discuss how to determine the minimum
fraction of the support to be estimated so that a desired detection performance
is achieved in a centralized setting. When the raw compressed observations are
not available at the central fusion center, we develop two distributed
algorithms for sparse signal detection. In these algorithms, the final decision
statistic is computed based on locally estimated partial support sets via
orthogonal matching pursuit (OMP) at individual nodes. The proposed distributed
algorithms with less communication overhead are shown to provide comparable
performance (sometimes better) to the centralized approach when the size of the
estimated partial support set is very small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06236</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06236</id><created>2016-02-19</created><authors><author><keyname>Beame</keyname><forenames>Paul</forenames></author><author><keyname>Koutris</keyname><forenames>Paraschos</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Communication Cost in Parallel Query Processing</title><categories>cs.DB</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.5972</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing conjunctive queries over large databases on
parallel architectures without shared storage. Using the structure of such a
query $q$ and the skew in the data, we study tradeoffs between the number of
processors, the number of rounds of communication, and the per-processor load
-- the number of bits each processor can send or can receive in a single round
-- that are required to compute $q$.
  When the data is free of skew, we obtain essentially tight upper and lower
bounds for one round algorithms and we show how the bounds degrade when there
is skew in the data. In the case of skewed data, we show how to improve the
algorithms when approximate degrees of the heavy-hitter elements are available,
obtaining essentially optimal algorithms for queries such as simple joins and
triangle join queries.
  For queries that we identify as tree-like, we also prove nearly matching
upper and lower bounds for multi-round algorithms for a natural class of
skew-free databases. One consequence of these latter lower bounds is that for
any $\varepsilon&gt;0$, using $p$ processors to compute the connected components
of a graph, or to output the path, if any, between a specified pair of vertices
of a graph with $m$ edges and per-processor load that is
$O(m/p^{1-\varepsilon})$ requires $\Omega(\log p)$ rounds of communication.
  Our upper bounds are given by simple structured algorithms using MapReduce.
Our one-round lower bounds are proved in a very general model, which we call
the Massively Parallel Communication (MPC) model, that allows processors to
communicate arbitrary bits. Our multi-round lower bounds apply in a restricted
version of the MPC model in which processors in subsequent rounds after the
first communication round are only allowed to send tuples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06239</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06239</id><created>2016-02-04</created><authors><author><keyname>Rudolph-Lilith</keyname><forenames>Michelle</forenames></author></authors><title>On a recursive construction of circular paths and the search for $\pi$
  on the integer lattice $\mathbb{Z}^2$</title><categories>cs.GR cs.CG</categories><msc-class>97N70, 68R10, 52C05, 11H06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital circles not only play an important role in various technological
settings, but also provide a lively playground for more fundamental
number-theoretical questions. In this paper, we present a new recursive
algorithm for the construction of digital circles on the integer lattice
$\mathbb{Z}^2$, which makes sole use of the signum function. By briefly
elaborating on the nature of discretization of circular paths, we then find
that this algorithm recovers, in a space endowed with $\ell^1$-norm, the
defining constant $\pi$ of a circle in $\mathbb{R}^2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06245</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06245</id><created>2016-02-19</created><updated>2016-02-27</updated><authors><author><keyname>Bendich</keyname><forenames>Paul</forenames></author><author><keyname>Gasparovic</keyname><forenames>Ellen</forenames></author><author><keyname>Tralie</keyname><forenames>Christopher J.</forenames></author><author><keyname>Harer</keyname><forenames>John</forenames></author></authors><title>Scaffoldings and Spines: Organizing High-Dimensional Data Using Cover
  Trees, Local Principal Component Analysis, and Persistent Homology</title><categories>cs.CG</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a flexible and multi-scale method for organizing, visualizing, and
understanding datasets sampled from or near stratified spaces. The first part
of the algorithm produces a cover tree using adaptive thresholds based on a
combination of multi-scale local principal component analysis and topological
data analysis. The resulting cover tree nodes consist of points within or near
the same stratum of the stratified space. They are then connected to form a
\emph{scaffolding} graph, which is then simplified and collapsed down into a
\emph{spine} graph. From this latter graph the stratified structure becomes
apparent. We demonstrate our technique on several synthetic point cloud
examples and we use it to understand song structure in musical audio data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06246</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06246</id><created>2016-02-19</created><authors><author><keyname>Aazam</keyname><forenames>Mohammad</forenames></author><author><keyname>StHilaire</keyname><forenames>Marc</forenames></author><author><keyname>Huh</keyname><forenames>EuiNam</forenames></author></authors><title>Towards Media Intercloud Standardization Evaluating Impact of Cloud
  Storage Heterogeneity</title><categories>cs.NI</categories><comments>13 pages. 14 figures, Springer Journal of Grid Computing, 2016</comments><doi>10.1007/s10723-015-9339-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital media has been increasing very rapidly, resulting in cloud
computing's popularity gain. Cloud computing provides ease of management of
large amount of data and resources. With a lot of devices communicating over
the Internet and with the rapidly increasing user demands, solitary clouds have
to communicate to other clouds to fulfill the demands and discover services
elsewhere. This scenario is called intercloud computing or cloud federation.
Intercloud computing still lacks standard architecture. Prior works discuss
some of the architectural blueprints, but none of them highlight the key issues
involved and their impact, so that a valid and reliable architecture could be
envisioned. In this paper, we discuss the importance of intercloud computing
and present in detail its architectural components. Intercloud computing also
involves some issues. We discuss key issues as well and present impact of
storage heterogeneity. We have evaluated some of the most noteworthy cloud
storage services, namely Dropbox, Amazon CloudDrive, GoogleDrive, Microsoft
OneDrive (formerly SkyDrive), Box, and SugarSync in terms of Quality of
Experience (QoE), Quality of Service (QoS), and storage space efficiency.
Discussion on the results shows the acceptability level of these storage
services and the shortcomings in their design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06258</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06258</id><created>2016-02-18</created><authors><author><keyname>Angelopoulos</keyname><forenames>Spyros</forenames></author><author><keyname>D&#xfc;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Lidbetter</keyname><forenames>Thomas</forenames></author></authors><title>The expanding search ratio of a graph</title><categories>math.OC cs.DM</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of searching for a hidden target in an environment that
is modeled by an edge-weighted graph. A sequence of edges is chosen starting
from a given {\em root} vertex such that each edge is adjacent to a previously
chosen edge. This search paradigm, known as {\em expanding search} was recently
introduced for modeling problems such as coal mining in which the cost of
re-exploration is negligible. We define the {\em search ratio} of an expanding
search as the maximum over all vertices of the ratio of the time taken to reach
the vertex and the shortest-path cost to it from the root. Similar objectives
have previously been studied in the context of conventional (pathwise) search.
  In this paper we address algorithmic and computational issues of minimizing
the search ratio over all expanding searches, for a variety of search
environments, including general graphs, trees and star-like graphs. Our main
results focus on the problem of finding the {\em randomized expanding search}
with minimum {\em expected search ratio}, which is equivalent to solving a
zero-sum game between a {\em Searcher} and a {\em Hider}. We solve these
problems for certain classes of graphs, and obtain constant-factor
approximations for others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06260</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06260</id><created>2016-02-19</created><authors><author><keyname>Comin</keyname><forenames>Carlo</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>Checking Dynamic Consistency of Conditional Hyper Temporal Networks via
  Mean Payoff Games (Hardness and (pseudo) Singly-Exponential Time Algorithm)</title><categories>cs.CC</categories><comments>arXiv admin note: text overlap with arXiv:1505.00828</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we introduce the \emph{Conditional Hyper Temporal Network
(CHyTN)} model, which is a natural extension and generalization of both the
\CSTN and the \HTN model. Our contribution goes as follows. We show that
deciding whether a given \CSTN or CHyTN is dynamically consistent is
\coNP-hard. Then, we offer a proof that deciding whether a given CHyTN is
dynamically consistent is \PSPACE-hard, provided that the input instances are
allowed to include both multi-head and multi-tail hyperarcs. In light of this,
we continue our study by focusing on CHyTNs that allow only multi-head or only
multi-tail hyperarcs, and we offer the first deterministic (pseudo)
singly-exponential time algorithm for the problem of checking the
dynamic-consistency of such CHyTNs, also producing a dynamic execution strategy
whenever the input CHyTN is dynamically consistent. Since \CSTN{s} are a
special case of CHyTNs, this provides as a byproduct the first
sound-and-complete (pseudo) singly-exponential time algorithm for checking
dynamic-consistency in CSTNs. The proposed algorithm is based on a novel
connection between CSTN{s}/CHyTN{s} and Mean Payoff Games. The presentation of
the connection between \CSTN{s}/CHyTNs and \MPG{s} is mediated by the \HTN
model. In order to analyze the algorithm, we introduce a refined notion of
dynamic-consistency, named $\epsilon$-dynamic-consistency, and present a sharp
lower bounding analysis on the critical value of the reaction time
$\hat{\varepsilon}$ where a \CSTN/CHyTN transits from being, to not being,
dynamically consistent. The proof technique introduced in this analysis of
$\hat{\varepsilon}$ is applicable more generally when dealing with linear
difference constraints which include strict inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06283</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06283</id><created>2016-02-19</created><authors><author><keyname>Comin</keyname><forenames>Carlo</forenames></author><author><keyname>Labarre</keyname><forenames>Anthony</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author><author><keyname>Vialette</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Sorting With Forbidden Intermediates</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of applications, most notably in comparative genomics, involve
the computation of a shortest sorting sequence of operations for a given
permutation, where the set of allowed operations is fixed beforehand. Such
sequences are useful for instance when reconstructing potential scenarios of
evolution between species, or when trying to assess their similarity. We
revisit those problems by adding a new constraint on the sequences to be
computed: they must \emph{avoid} a given set of \emph{forbidden intermediates},
which correspond to species that cannot exist because the mutations that would
be involved in their creation are lethal. We initiate this study by focusing on
the case where the only mutations that can occur are exchanges of any two
elements in the permutations, and give a polynomial time algorithm for solving
that problem when the permutation to sort is an involution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06284</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06284</id><created>2016-02-19</created><authors><author><keyname>Tull</keyname><forenames>Sean</forenames></author></authors><title>Operational Theories of Physics as Categories</title><categories>math-ph cs.LO math.CT math.MP quant-ph</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to the study of operational theories of physics
using category theory. We define a generalisation of the (causal)
operational-probabilistic theories of Chiribella et al. and establish their
correspondence with our new notion of an operational category. Our work is
based on effectus theory, a recently developed area of categorical logic, to
which we give an operational interpretation, demonstrating its relevance to the
modelling of general probabilistic theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06289</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06289</id><created>2016-02-19</created><authors><author><keyname>Jastrz&#x119;bski</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Le&#x15b;niak</keyname><forenames>Damian</forenames></author><author><keyname>Czarnecki</keyname><forenames>Wojciech Marian</forenames></author></authors><title>Learning to SMILE(S)</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how one can directly apply natural language processing (NLP)
methods to classification problems in cheminformatics. Connection between these
seemingly separate fields is shown by considering standard textual
representation of compound, SMILES. The problem of activity prediction against
a target protein is considered, which is a crucial part of computer aided drug
design process. Conducted experiments show that this way one can not only
outrank state of the art results of hand crafted representations but also gets
direct structural insights into the way decisions are made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06291</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06291</id><created>2016-02-19</created><authors><author><keyname>Ghosh</keyname><forenames>Shalini</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Strope</keyname><forenames>Brian</forenames></author><author><keyname>Roy</keyname><forenames>Scott</forenames></author><author><keyname>Dean</keyname><forenames>Tom</forenames></author><author><keyname>Heck</keyname><forenames>Larry</forenames></author></authors><title>Contextual LSTM (CLSTM) models for Large scale NLP tasks</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Documents exhibit sequential structure at multiple levels of abstraction
(e.g., sentences, paragraphs, sections). These abstractions constitute a
natural hierarchy for representing the context in which to infer the meaning of
words and larger fragments of text. In this paper, we present CLSTM (Contextual
LSTM), an extension of the recurrent neural network LSTM (Long-Short Term
Memory) model, where we incorporate contextual features (e.g., topics) into the
model. We evaluate CLSTM on three specific NLP tasks: word prediction, next
sentence selection, and sentence topic prediction. Results from experiments run
on two corpora, English documents in Wikipedia and a subset of articles from a
recent snapshot of English Google News, indicate that using both words and
topics as features improves performance of the CLSTM models over baseline LSTM
models for these tasks. For example on the next sentence selection task, we get
relative accuracy improvements of 21% for the Wikipedia dataset and 18% for the
Google News dataset. This clearly demonstrates the significant benefit of using
context appropriately in natural language (NL) tasks. This has implications for
a wide variety of NL applications like question answering, sentence completion,
paraphrase generation, and next utterance prediction in dialog systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06294</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06294</id><created>2016-02-19</created><authors><author><keyname>Zitlau</keyname><forenames>Roman</forenames></author><author><keyname>Hoyle</keyname><forenames>Ben</forenames></author><author><keyname>Paech</keyname><forenames>Kerstin</forenames></author><author><keyname>Weller</keyname><forenames>Jochen</forenames></author><author><keyname>Rau</keyname><forenames>Markus Michael</forenames></author><author><keyname>Seitz</keyname><forenames>Stella</forenames></author></authors><title>Stacking for machine learning redshifts applied to SDSS galaxies</title><categories>astro-ph.IM astro-ph.CO cs.LG</categories><comments>13 pages, 3 tables, 7 figures submitted to MNRAS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an analysis of a general machine learning technique called
'stacking' for the estimation of photometric redshifts. Stacking techniques can
feed the photometric redshift estimate, as output by a base algorithm, back
into the same algorithm as an additional input feature in a subsequent learning
round. We shown how all tested base algorithms benefit from at least one
additional stacking round (or layer). To demonstrate the benefit of stacking,
we apply the method to both unsupervised machine learning techniques based on
self-organising maps (SOMs), and supervised machine learning methods based on
decision trees. We explore a range of stacking architectures, such as the
number of layers and the number of base learners per layer. Finally we explore
the effectiveness of stacking even when using a successful algorithm such as
AdaBoost. We observe a significant improvement of between 1.9% and 21% on all
computed metrics when stacking is applied to weak learners (such as SOMs and
decision trees). When applied to strong learning algorithms (such as AdaBoost)
the ratio of improvement shrinks, but still remains positive and is between
0.4% and 2.5% for the explored metrics and comes at almost no additional
computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06295</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06295</id><created>2016-02-19</created><authors><author><keyname>Join</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Fliess</keyname><forenames>Michel</forenames></author><author><keyname>Voyant</keyname><forenames>Cyril</forenames></author><author><keyname>Chaxel</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Solar energy production: Short-term forecasting and risk management</title><categories>q-fin.GN cs.DS</categories><comments>8th IFAC Conference on Manufacturing Modelling, Management &amp; Control
  (Troyes, France, June 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electricity production via solar energy is tackled via short-term forecasts
and risk management. Our main tool is a new setting on time series. It allows
the definition of &quot;confidence bands&quot; where the Gaussian assumption, which is
not satisfied by our concrete data, may be abandoned. Those bands are quite
convenient and easily implementable. Numerous computer simulations are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06323</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06323</id><created>2016-02-19</created><authors><author><keyname>Fulla</keyname><forenames>Peter</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>On Planar Valued CSPs</title><categories>cs.CC cs.DM</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of planar valued constraint
satisfaction problems (VCSPs). First, we show that intractable Boolean VCSPs
have to be self-complementary to be tractable in the planar setting, thus
extending a corresponding result of Dvorak and Kupec [ICALP'15] from CSPs to
VCSPs. Second, we give a complete complexity classification of conservative
planar VCSPs on arbitrary finite domains. As it turns out, in this case
planarity does not lead to any new tractable cases, and thus our classification
is a sharpening of the classification of conservative VCSPs by Kolmogorov and
Zivny [JACM'13].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06333</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06333</id><created>2016-02-17</created><authors><author><keyname>De Micheli</keyname><forenames>Enrico</forenames></author><author><keyname>Viano</keyname><forenames>Giovanni Alberto</forenames></author></authors><title>Fredholm integral equations of the first kind and topological
  information theory</title><categories>math.NA cs.IT math-ph math.CA math.IT math.MP</categories><comments>17 pages</comments><msc-class>45B05, 47A52, 94A05</msc-class><journal-ref>Integral Equations and Operator Theory 73, 553-571, 2012</journal-ref><doi>10.1007/s00020-012-1970-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fredholm integral equations of the first kind are a classical example of
ill-posed problem in the sense of Hadamard. If the integral operator is
self-adjoint and admits a set of eigenfunctions, then a formal solution can be
written in terms of eigenfunction expansions. One of the possible methods of
regularization consists in truncating this formal expansion after restricting
the class of admissible solutions through a-priori global bounds. In this paper
we reconsider various possible methods of truncation from the viewpoint of the
$\varepsilon$-coverings of compact sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06346</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06346</id><created>2016-02-19</created><authors><author><keyname>Pires</keyname><forenames>Bernardo &#xc1;vila</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author></authors><title>Policy Error Bounds for Model-Based Reinforcement Learning with Factored
  Linear Models</title><categories>stat.ML cs.LG</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a model-based approach to calculating approximately
optimal policies in Markovian Decision Processes. In particular, we derive
novel bounds on the loss of using a policy derived from a factored linear
model, a class of models which generalize virtually all previous models that
come with strong computational guarantees. For the first time in the
literature, we derive performance bounds for model-based techniques where the
model inaccuracy is measured in weighted norms. Moreover, our bounds show a
decreased sensitivity to the discount factor and, unlike similar bounds derived
for other approaches, they are insensitive to measure mismatch. Similarly to
previous works, our proofs are also based on contraction arguments, but with
the main differences that we use carefully constructed norms building on Banach
lattices, and the contraction property is only assumed for operators acting on
&quot;compressed&quot; spaces, thus weakening previous assumptions, while strengthening
previous results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06347</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06347</id><created>2016-02-19</created><authors><author><keyname>Fioretto</keyname><forenames>Ferdinando</forenames></author><author><keyname>Pontelli</keyname><forenames>Enrico</forenames></author><author><keyname>Yeoh</keyname><forenames>William</forenames></author></authors><title>Distributed Constraint Optimization Problems and Applications: A Survey</title><categories>cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of Multi-Agent System (MAS) is an active area of research within
Artificial Intelligence, with an increasingly important impact in industrial
and other real-world applications. Within a MAS, autonomous agents interact to
pursue personal interests and/or to achieve common objectives. Distributed
Constraint Optimization Problems (DCOPs) have emerged as one of the prominent
agent architectures to govern the agents' autonomous behavior, where both
algorithms and communication models are driven by the structure of the specific
problem. During the last decade, several extensions to the DCOP model have
enabled them to support MAS in complex, real-time, and uncertain environments.
  This survey aims at providing an overview of the DCOP model, giving a
classification of its multiple extensions and addressing both resolution
methods and applications that find a natural mapping within each class of
DCOPs. The proposed classification suggests several future perspectives for
DCOP extensions, and identifies challenges in the design of efficient
resolution algorithms, possibly through the adaptation of strategies from
different areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06359</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06359</id><created>2016-02-19</created><authors><author><keyname>Pang</keyname><forenames>Liang</forenames></author><author><keyname>Lan</keyname><forenames>Yanyan</forenames></author><author><keyname>Guo</keyname><forenames>Jiafeng</forenames></author><author><keyname>Xu</keyname><forenames>Jun</forenames></author><author><keyname>Wan</keyname><forenames>Shengxian</forenames></author><author><keyname>Cheng</keyname><forenames>Xueqi</forenames></author></authors><title>Text Matching as Image Recognition</title><categories>cs.CL cs.AI</categories><comments>Accepted by AAAI-2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matching two texts is a fundamental problem in many natural language
processing tasks. An effective way is to extract meaningful matching patterns
from words, phrases, and sentences to produce the matching score. Inspired by
the success of convolutional neural network in image recognition, where neurons
can capture many complicated patterns based on the extracted elementary visual
patterns such as oriented edges and corners, we propose to model text matching
as the problem of image recognition. Firstly, a matching matrix whose entries
represent the similarities between words is constructed and viewed as an image.
Then a convolutional neural network is utilized to capture rich matching
patterns in a layer-by-layer way. We show that by resembling the compositional
hierarchies of patterns in image recognition, our model can successfully
identify salient signals such as n-gram and n-term matchings. Experimental
results demonstrate its superiority against the baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06365</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06365</id><created>2016-02-19</created><authors><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Ma</keyname><forenames>Yuanye</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Distributed Power Control in Interference Channels with QoS Constraints
  and RF Energy Harvesting: A Game-Theoretic Approach</title><categories>cs.IT math.IT</categories><comments>Accepted to appear in IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a new distributed power control scheme for a power
splitting-based interference channel (IFC) with simultaneous wireless
information and power transfer (SWIPT). The considered IFC consists of multiple
source-destination pairs. Each destination splits its received signal into two
parts for information decoding and energy harvesting (EH), respectively. Each
pair adjusts its transmit power and power splitting ratio to meet both the
signal-to-interference-plus-noise ratio (SINR) and EH constraints at its
corresponding destination. To characterize rational behaviors of
source-destination pairs, we formulate a non-cooperative game for the
considered system, where each pair is modeled as a strategic player who aims to
minimize its own transmit power under both SINR and EH constraints at the
destination. We derive a sufficient and necessary condition for the existence
and uniqueness of the Nash equilibrium (NE) of the formulated game. The best
response strategy of each player is derived and then the NE can be achieved
iteratively. Numerical results show that the proposed game-theoretic approach
can achieve a near-optimal performance under various SINR and EH constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06373</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06373</id><created>2016-02-20</created><authors><author><keyname>Zhang</keyname><forenames>Jinye</forenames></author><author><keyname>Chen</keyname><forenames>Laming</forenames></author><author><keyname>Boufounos</keyname><forenames>Petros T.</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Cross Validation in Compressive Sensing and its Application of OMP-CV
  Algorithm</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) is a data acquisition technique that measures sparse
or compressible signals at a sampling rate lower than their Nyquist rate.
Results show that sparse signals can be reconstructed using greedy algorithms,
often requiring prior knowledge such as the signal sparsity or the noise level.
As a substitute to prior knowledge, cross validation (CV), a statistical method
that examines whether a model overfits its data, has been proposed to determine
the stopping condition of greedy algorithms. This paper first analyzes cross
validation in a general compressive sensing framework and developed general
cross validation techniques which could be used to understand CV-based sparse
recovery algorithms. Furthermore, we provide theoretical analysis for OMP-CV, a
cross validation modification of orthogonal matching pursuit, which has very
good sparse recovery performance. Finally, numerical experiments are given to
validate our theoretical results and investigate the behaviors of OMP-CV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06375</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06375</id><created>2016-02-20</created><authors><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Power-Distortion Metrics for Path Planning over Gaussian Sensor Networks</title><categories>cs.IT cs.RO math.IT math.OC stat.AP</categories><comments>will appear in Trans. on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path planning is an important component of au- tonomous mobile sensing
systems. This paper studies upper and lower bounds of communication performance
over Gaussian sen- sor networks, to drive power-distortion metrics for path
planning problems. The Gaussian multiple-access channel is employed as a
channel model and two source models are considered. In the first setting, the
underlying source is estimated with minimum mean squared error, while in the
second, reconstruction of a random spatial field is considered. For both
problem settings, the upper and the lower bounds of sensor power-distortion
curve are derived. For both settings, the upper bounds follow from the
amplify-and-forward scheme and the lower bounds admit a unified derivation
based on data processing inequality and tensorization property of the maximal
correlation measure. Next, closed-form solutions of the optimal power
allocation problems are obtained under a weighted sum-power constraint. The gap
between the upper and the lower bounds is analyzed for both weighted sum and
individual power constrained settings. Finally, these metrics are used to drive
a path planning algorithm and the effects of power-distortion metrics, network
parameters, and power optimization on the optimized path selection are
analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06395</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06395</id><created>2016-02-20</created><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author><author><keyname>Lewis-Pye</keyname><forenames>Andrew</forenames></author></authors><title>Computing halting probabilities from other halting probabilities</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The halting probability of a Turing machine is the probability that the
machine will halt if it starts with a random stream written on its one-way
input tape. When the machine is universal, this probability is referred to as
Chaitin's omega number, and is the most well known example of a real which is
random in the sense of Martin-L\&quot;{o}f. Although omega numbers depend on the
underlying universal Turing machine, they are robust in the sense that they all
have the same Turing degree, namely the degree of the halting problem. In this
paper we give precise bounds on the redundancy growth rate that is generally
required for the computation of an omega number from another omega number. We
show that for each $\epsilon &gt;1$, any pair of omega numbers compute each other
with redundancy $\epsilon \log n$. On the other hand, this is not true for
$\epsilon=1$. In fact, we show that for each omega number there exists another
omega number which is not computable from the first one with redundancy $\log
n$. This latter result improves an older result of Frank Stephan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06399</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06399</id><created>2016-02-20</created><authors><author><keyname>Lin</keyname><forenames>Junhong</forenames></author><author><keyname>Li</keyname><forenames>Song</forenames></author></authors><title>Restricted $q$-Isometry Properties Adapted to Frames for Nonconvex
  $l_q$-Analysis</title><categories>cs.IT math.IT</categories><comments>40 pages, 1 figure, under revision for a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses reconstruction of signals from few measurements in the
situation that signals are sparse or approximately sparse in terms of a general
frame via the $l_q$-analysis optimization with $0&lt;q\leq 1$. We first introduce
a notion of restricted $q$-isometry property ($q$-RIP) adapted to a dictionary,
which is a natural extension of the standard $q$-RIP, and establish a
generalized $q$-RIP condition for approximate reconstruction of signals via the
$l_q$-analysis optimization. We then determine how many random, Gaussian
measurements are needed for the condition to hold with high probability. The
resulting sufficient condition is met by fewer measurements for smaller $q$
than when $q=1$.
  The introduced generalized $q$-RIP is also useful in compressed data
separation. In compressed data separation, one considers the problem of
reconstruction of signals' distinct subcomponents, which are (approximately)
sparse in morphologically different dictionaries, from few measurements. With
the notion of generalized $q$-RIP, we show that under an usual assumption that
the dictionaries satisfy a mutual coherence condition, the $l_q$ split analysis
with $0&lt;q\leq1 $ can approximately reconstruct the distinct components from
fewer random Gaussian measurements with small $q$ than when $q=1$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06401</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06401</id><created>2016-02-20</created><authors><author><keyname>Bikakis</keyname><forenames>Nikos</forenames></author><author><keyname>Liagouris</keyname><forenames>John</forenames></author><author><keyname>Krommyda</keyname><forenames>Maria</forenames></author><author><keyname>Papastefanatos</keyname><forenames>George</forenames></author><author><keyname>Sellis</keyname><forenames>Timos</forenames></author></authors><title>graphVizdb: A Scalable Platform for Interactive Large Graph
  Visualization</title><categories>cs.HC cs.DB cs.DS</categories><comments>32nd IEEE International Conference on Data Engineering (ICDE '16)</comments><msc-class>97R50, 68P05, 68P15</msc-class><acm-class>E.1; H.2.8; H.5.2; H.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel platform for the interactive visualization of very large
graphs. The platform enables the user to interact with the visualized graph in
a way that is very similar to the exploration of maps at multiple levels. Our
approach involves an offline preprocessing phase that builds the layout of the
graph by assigning coordinates to its nodes with respect to a Euclidean plane.
The respective points are indexed with a spatial data structure, i.e., an
R-tree, and stored in a database. Multiple abstraction layers of the graph
based on various criteria are also created offline, and they are indexed
similarly so that the user can explore the dataset at different levels of
granularity, depending on her particular needs. Then, our system translates
user operations into simple and very efficient spatial operations (i.e., window
queries) in the backend. This technique allows for a fine-grained access to
very large graphs with extremely low latency and memory requirements and
without compromising the functionality of the tool. Our web-based prototype
supports three main operations: (1) interactive navigation, (2) multi-level
exploration, and (3) keyword search on the graph metadata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06406</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06406</id><created>2016-02-20</created><authors><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Langbort</keyname><forenames>Cedric</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>On the Role of Side Information In Strategic Communication</title><categories>cs.IT cs.GT cs.SI math.IT</categories><comments>submitted to ISIT'16. arXiv admin note: text overlap with
  arXiv:1510.00764</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the fundamental limits of strate- gic communication in
network settings. Strategic communication differs from the conventional
communication paradigms in in- formation theory since it involves different
objectives for the encoder and the decoder, which are aware of this mismatch
and act accordingly. This leads to a Stackelberg game where both agents commit
to their mappings ex-ante. Building on our prior work on the point-to-point
setting, this paper studies the compression and communication problems with the
receiver and/or transmitter side information setting. The equilibrium
strategies and associated costs are characterized for the Gaussian variables
with quadratic cost functions. Several questions on the benefit of side
information in source and joint source-channel coding in such strategic
settings are analyzed. Our analysis has uncovered an interesting result on
optimality of uncoded mappings in strategic source-channel coding in networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06407</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06407</id><created>2016-02-20</created><authors><author><keyname>Brandenburg</keyname><forenames>Franz J.</forenames></author></authors><title>A Reduction System for Optimal 1-Planar Graphs</title><categories>cs.CG</categories><comments>24 pages, 22 figures</comments><msc-class>68R10 68Q42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph with n vertices is 1-planar if it can be drawn in the plane such that
each edge is crossed at most once, and is optimal if it has the maximum of 4n-8
edges. There is a two-rule graph reduction system that reduces every optimal
1-planar graph to an extended wheel graph which is irreducible. The application
of a reduction is constraint, and the use of one reduction can exclude the use
of another one. The reductions are de?fined on 1-planar embeddings of graphs.
  We show how to use the reductions e?ciently on graphs, which results in a
simple quadratic-time recognition algorithm for optimal 1-planar graphs. In
addition, we prove that every optimal 1-planar graph G, which is
non-irreducible, can be reduced to every extended wheel graph whose size is in
a range from the minimum extended wheel graph to some upper bound. The minimum
extended wheel graph is obtained if and only if G has a separating 4-cycle. In
consequence, the reduction system is non-confluent. Moreover, we show that the
optimal 1-planar graphs constitute a single equivalence class for the
reductions and their inverse with the minimum extended wheel graph as a
representative, and that the equivalence problem is decidable in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06410</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06410</id><created>2016-02-20</created><authors><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Semidefinite Programs for Exact Recovery of a Hidden Community</title><categories>stat.ML cs.IT cs.SI math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a semidefinite programming (SDP) relaxation of the maximum
likelihood estimation for exactly recovering a hidden community of cardinality
$K$ from an $n \times n$ symmetric data matrix $A$, where for distinct indices
$i,j$, $A_{ij} \sim P$ if $i, j$ are both in the community and $A_{ij} \sim Q$
otherwise, for two known probability distributions $P$ and $Q$. We identify a
sufficient condition and a necessary condition for the success of SDP for the
general model. For both the Bernoulli case ($P={\rm Bern}(p)$ and $Q={\rm
Bern}(q)$ with $p&gt;q$) and the Gaussian case ($P=\mathcal{N}(\mu,1)$ and
$Q=\mathcal{N}(0,1)$ with $\mu&gt;0$), which correspond to the problem of planted
dense subgraph recovery and submatrix localization respectively, the general
results lead to the following findings: (1) If $K=\omega( n /\log n)$, SDP
attains the information-theoretic recovery limits with sharp constants; (2) If
$K=\Theta(n/\log n)$, SDP is order-wise optimal, but strictly suboptimal by a
constant factor; (3) If $K=o(n/\log n)$ and $K \to \infty$, SDP is order-wise
suboptimal. A key ingredient in the proof of the necessary condition is a
construction of a primal feasible solution based on random perturbation of the
true cluster matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06411</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06411</id><created>2016-02-20</created><authors><author><keyname>Axiotis</keyname><forenames>Kyriakos</forenames></author><author><keyname>Fotakis</keyname><forenames>Dimitris</forenames></author></authors><title>On the Size and the Approximability of Minimum Temporally Connected
  Subgraphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider temporal graphs with discrete time labels and investigate the
size and the approximability of minimum temporally connected spanning
subgraphs. We present a family of minimally connected temporal graphs with $n$
vertices and $\Omega(n^2)$ edges, thus resolving an open question of (Kempe,
Kleinberg, Kumar, JCSS 64, 2002) about the existence of sparse temporal
connectivity certificates. Next, we consider the problem of computing a minimum
weight subset of temporal edges that preserve connectivity of a given temporal
graph either from a given vertex r (r-MTC problem) or among all vertex pairs
(MTC problem). We show that the approximability of r-MTC is closely related to
the approximability of Directed Steiner Tree and that r-MTC can be solved in
polynomial time if the underlying graph has bounded treewidth. We also show
that the best approximation ratio for MTC is at least $O(2^{\log^{1-\epsilon}
n})$ and at most $O(\min\{n^{1+\epsilon}, (\Delta M)^{2/3+\epsilon}\})$, for
any constant $\epsilon &gt; 0$, where $M$ is the number of temporal edges and
$\Delta$ is the maximum degree of the underlying graph. Furthermore, we prove
that the unweighted version of MTC is APX-hard and that MTC is efficiently
solvable in trees and $2$-approximable in cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06417</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06417</id><created>2016-02-20</created><authors><author><keyname>Tran</keyname><forenames>Hoang-Dung</forenames></author><author><keyname>Nguyen</keyname><forenames>Luan Viet</forenames></author><author><keyname>Xiang</keyname><forenames>Weiming</forenames></author><author><keyname>Johnson</keyname><forenames>Taylor T.</forenames></author></authors><title>Order-Reduction Abstractions for Safety Verification of High-Dimensional
  Linear Systems</title><categories>cs.SY</categories><comments>Preliminary version under review for Discrete Event Dynamic Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Order-reduction is a standard automated approximation technique for
computer-aided design, analysis, and simulation of many classes of systems,
from circuits to buildings. For a given system, these methods produce a
reduced-order system where the dimension of the state-space is smaller, while
attempting to preserve behaviors similar to those of the full-order original
system. To be used as a sound abstraction for formal verification, a measure of
the similarity of behavior must be formalized and computed, which we develop in
a computational way for a class of linear systems and periodically-switched
systems as the main contributions of this paper. We have implemented the
order-reduction as a sound abstraction process through a source-to-source model
transformation in the HyST tool and use SpaceEx to compute sets of reachable
states to verify properties of the full-order system through analysis of the
reduced-order system. Our experimental results suggest systems with on the
order of a thousand state variables can be reduced to systems with tens of
state variables such that the order-reduction overapproximation error is small
enough to prove or disprove safety properties of interest using current
reachability analysis tools. Our results illustrate this approach is effective
to alleviate the state-space explosion problem for verification of
high-dimensional linear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06420</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06420</id><created>2016-02-20</created><updated>2016-02-23</updated><authors><author><keyname>Warrell</keyname><forenames>Jonathan H.</forenames></author></authors><title>A Probabilistic Dependent Type System based on Non-Deterministic Beta
  Reduction</title><categories>cs.LO cs.PL</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Probabilistic Dependent Type Systems (PDTS) via a functional
language based on a subsystem of intuitionistic type theory including dependent
sums and products, which is expanded to include stochastic functions. We
provide a sampling-based semantics for the language based on non-deterministic
beta reduction. Further, we derive a probabilistic logic from the PDTS
introduced as a direct result of the Curry-Howard isomorphism. The
probabilistic logic derived is shown to provide a universal representation for
finite discrete distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06426</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06426</id><created>2016-02-20</created><authors><author><keyname>Neri</keyname><forenames>Cassio</forenames></author></authors><title>A loopless and branchless $O(1)$ algorithm to generate the next Dyck
  word</title><categories>cs.DS</categories><comments>First published on 19 July 2014 at https://github.com/cassioneri/Dyck</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Let integer be any C/C++ unsigned integer type up to 64-bits long. Given a
Dyck word the following code returns the next Dyck word of the same size,
provided it exists.
  integer next_dyck_word(integer w) {
  integer const a = w &amp; -w;
  integer const b = w + a;
  integer c = w ^ b;
  c = (c / a &gt;&gt; 2) + 1;
  c = ((c * c - 1) &amp; 0xaaaaaaaaaaaaaaaa) | b;
  return c;
  }
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06431</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06431</id><created>2016-02-20</created><authors><author><keyname>Alves</keyname><forenames>Rodrigo A S</forenames></author><author><keyname>Assun&#xe7;&#xe3;o</keyname><forenames>Renato</forenames></author><author><keyname>de Melo</keyname><forenames>Pedro O S Vaz</forenames></author></authors><title>Burstiness Scale: a highly parsimonious model for characterizing random
  series of events</title><categories>stat.ML cs.SI</categories><acm-class>H.2.8; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem to accurately and parsimoniously characterize random series of
events (RSEs) present in the Web, such as e-mail conversations or Twitter
hashtags, is not trivial. Reports found in the literature reveal two apparent
conflicting visions of how RSEs should be modeled. From one side, the
Poissonian processes, of which consecutive events follow each other at a
relatively regular time and should not be correlated. On the other side, the
self-exciting processes, which are able to generate bursts of correlated events
and periods of inactivities. The existence of many and sometimes conflicting
approaches to model RSEs is a consequence of the unpredictability of the
aggregated dynamics of our individual and routine activities, which sometimes
show simple patterns, but sometimes results in irregular rising and falling
trends. In this paper we propose a highly parsimonious way to characterize
general RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSE
as a mix of two independent process: a Poissonian and a self-exciting one. Here
we describe a fast method to extract the two parameters of BuSca that,
together, gives the burstyness scale, which represents how much of the RSE is
due to bursty and viral effects. We validated our method in eight diverse and
large datasets containing real random series of events seen in Twitter, Yelp,
e-mail conversations, Digg, and online forums. Results showed that, even using
only two parameters, BuSca is able to accurately describe RSEs seen in these
diverse systems, what can leverage many applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06434</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06434</id><created>2016-02-20</created><authors><author><keyname>Sadjina</keyname><forenames>Severin</forenames></author><author><keyname>Kyllingstad</keyname><forenames>Lars T.</forenames></author><author><keyname>Pedersen</keyname><forenames>Eilif</forenames></author><author><keyname>Skjong</keyname><forenames>Stian</forenames></author></authors><title>Energy Conservation and Power Bonds in Co-Simulations: Non-Iterative
  Adaptive Step Size Control and Error Estimation</title><categories>cs.SY cs.CE cs.DC</categories><comments>15 pages, 12 figures, 8 tables</comments><acm-class>I.6.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we study the flow of energy between coupled simulators in a
co-simulation environment using the concept of power bonds. We introduce energy
residuals which are a direct expression of the coupling errors and hence the
accuracy of co-simulation results. We propose a novel Energy-Conservation-based
Co-Simulation method (ECCO) for adaptive macro step size control to improve
accuracy and efficiency. In contrast to other co-simulation algorithms, this
method is non-iterative and only requires knowledge of the coupling variable
values. Consequently, it allows for significant speed ups and the protection of
sensitive information contained within simulator models. A quarter car model
with linear and nonlinear damping serves as a co-simulation benchmark and
demonstrates the capabilities of the energy residual concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06439</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06439</id><created>2016-02-20</created><authors><author><keyname>Kim</keyname><forenames>Kwang In</forenames></author><author><keyname>Tompkin</keyname><forenames>James</forenames></author><author><keyname>Pfister</keyname><forenames>Hanspeter</forenames></author><author><keyname>Theobalt</keyname><forenames>Christian</forenames></author></authors><title>Context-guided diffusion for label propagation on graphs</title><categories>cs.CV</categories><doi>10.1109/ICCV.2015.318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing approaches for diffusion on graphs, e.g., for label propagation, are
mainly focused on isotropic diffusion, which is induced by the commonly-used
graph Laplacian regularizer. Inspired by the success of diffusivity tensors for
anisotropic diffusion in image processing, we presents anisotropic diffusion on
graphs and the corresponding label propagation algorithm. We develop positive
definite diffusivity operators on the vector bundles of Riemannian manifolds,
and discretize them to diffusivity operators on graphs. This enables us to
easily define new robust diffusivity operators which significantly improve
semi-supervised learning performance over existing diffusion algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06442</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06442</id><created>2016-02-20</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Yamamoto</keyname><forenames>Shun'ichi</forenames></author><author><keyname>Rouat</keyname><forenames>Jean</forenames></author><author><keyname>Michaud</keyname><forenames>Francois</forenames></author><author><keyname>Nakadai</keyname><forenames>Kazuhiro</forenames></author><author><keyname>Okuno</keyname><forenames>Hiroshi G.</forenames></author></authors><title>Robust Recognition of Simultaneous Speech By a Mobile Robot</title><categories>cs.RO cs.SD</categories><comments>12 pages</comments><journal-ref>IEEE Transactions on Robotics, Vol. 23, No. 4, pp. 742-752, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a system that gives a mobile robot the ability to
perform automatic speech recognition with simultaneous speakers. A microphone
array is used along with a real-time implementation of Geometric Source
Separation and a post-filter that gives a further reduction of interference
from other sources. The post-filter is also used to estimate the reliability of
spectral features and compute a missing feature mask. The mask is used in a
missing feature theory-based speech recognition system to recognize the speech
from simultaneous Japanese speakers in the context of a humanoid robot.
Recognition rates are presented for three simultaneous speakers located at 2
meters from the robot. The system was evaluated on a 200 word vocabulary at
different azimuths between sources, ranging from 10 to 90 degrees. Compared to
the use of the microphone array source separation alone, we demonstrate an
average reduction in relative recognition error rate of 24% with the
post-filter and of 42% when the missing features approach is combined with the
post-filter. We demonstrate the effectiveness of our multi-source microphone
array post-filter and the improvement it provides when used in conjunction with
the missing features theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06453</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06453</id><created>2016-02-20</created><authors><author><keyname>Mireault</keyname><forenames>Paul</forenames></author></authors><title>Developing a Repeating Model Using the Structured Spreadsheet Modelling
  and Implementation Methodology</title><categories>cs.SE</categories><comments>13 Pages, 17 Colour Figures, 2 Tables</comments><proxy>Grenville Croll</proxy><journal-ref>Proc. 16th EuSpRIG Conf. (2015) &quot;Spreadsheet Risk Management&quot;,
  pp15-28, ISBN: 978-1-905404-52-0</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreadsheets often have variables and formulas that are similar, differing
only by the fact that they refer to different instances of an entity. For
example, the calculation of the sales revenues of the South and East regions
are Revenues South = Price * Quantity Sold South and Revenues East = Price *
Quantity Sold East. In this paper, we present a conceptual modelling approach
that takes advantage of these similarities and leads the spreadsheet developer
to the formula Revenues = Price * Quantity. We then present simple but strict
rules to implement the spreadsheet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06454</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06454</id><created>2016-02-20</created><authors><author><keyname>Nazi</keyname><forenames>Azade</forenames></author><author><keyname>Das</keyname><forenames>Mahashweta</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Web Item Reviewing Made Easy By Leveraging Available User Feedback</title><categories>cs.SI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread use of online review sites over the past decade has motivated
businesses of all types to possess an expansive arsenal of user feedback to
mark their reputation. Though a significant proportion of purchasing decisions
are driven by average rating, detailed reviews are critical for activities like
buying expensive digital SLR camera. Since writing a detailed review for an
item is usually time-consuming, the number of reviews available in the Web is
far from many. Given a user and an item our goal is to identify the top-$k$
meaningful phrases/tags to help her review the item easily. We propose
general-constrained optimization framework based on three measures - relevance
(how well the result set of tags describes an item), coverage (how well the
result set of tags covers the different aspects of an item), and polarity (how
well sentiment is attached to the result set of tags). By adopting different
definitions of coverage, we identify two concrete problem instances that enable
a wide range of real-world scenarios. We develop practical algorithms with
theoretical bounds to solve these problems efficiently. We conduct experiments
on synthetic and real data crawled from the web to validate the effectiveness
of our solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06456</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06456</id><created>2016-02-20</created><authors><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Gonzalez-Prelcic</keyname><forenames>Nuria</forenames></author><author><keyname>Daniels</keyname><forenames>Robert</forenames></author><author><keyname>Bhat</keyname><forenames>Chandra R.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Millimeter Wave Vehicular Communication to Support Massive Automotive
  Sensing</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures, 1 table, submitted to IEEE Communications
  Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As driving becomes more automated, vehicles are being equipped with more
sensors generating even higher data rates. Radars (RAdio Detection and Ranging)
are used for object detection, visual cameras as virtual mirrors, and LIDARs
(Light Detection and Ranging) for generating high resolution depth associated
range maps, all to enhance the safety and efficiency of driving. Connected
vehicles can use wireless communication to exchange sensor data, allowing them
to enlarge their sensing range and improve automated driving functions.
Unfortunately, conventional technologies, such as dedicated short-range
communication (DSRC) and 4G cellular communication, do not support the
gigabit-per-second data rates that would be required for raw sensor data
exchange between vehicles. This paper makes the case that millimeter wave
(mmWave) communication is the only viable approach for high bandwidth connected
vehicles. The motivations and challenges associated with using mmWave for
vehicle-to-vehicle and vehicle-to-infrastructure applications are highlighted.
A solution to one key challenge - the overhead of mmWave beam training - is
proposed. The critical feature of this solution is to leverage information
derived from the sensors or DSRC as side information for the mmWave
communication link configuration. Examples and simulation results show that the
beam alignment overhead can be reduced by using position information obtained
from DSRC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06458</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06458</id><created>2016-02-20</created><authors><author><keyname>Salimi</keyname><forenames>Babak</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author></authors><title>Causes for Query Answers from Databases, Datalog Abduction and
  View-Updates: The Presence of Integrity Constraints</title><categories>cs.DB cs.AI</categories><comments>To appear in Proceedings Flairs, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causality has been recently introduced in databases, to model, characterize
and possibly compute causes for query results (answers). Connections between
queryanswer causality, consistency-based diagnosis, database repairs (wrt.
integrity constraint violations), abductive diagnosis and the view-update
problem have been established. In this work we further investigate connections
between query-answer causality and abductive diagnosis and the view-update
problem. In this context, we also define and investigate the notion of
query-answer causality in the presence of integrity constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06460</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06460</id><created>2016-02-20</created><authors><author><keyname>Agrawal</keyname><forenames>Garima</forenames></author><author><keyname>Karlapalem</keyname><forenames>Kamalakar</forenames></author></authors><title>Wheeled Robots playing Chain Catch: Strategies and Evaluation</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robots playing games that humans are adept in is a challenge. We studied
robotic agents playing Chain Catch game as a Multi-Agent System (MAS). Our game
starts with a traditional Catch game similar to Pursuit evasion, and further
extends it to form a growing chain of predator agents to chase remaining preys.
Hence Chain Catch is a combination of two challenges - pursuit domain and
robotic chain formation. These are games that require team of robotic agents to
cooperate among themselves and to compete with other group of agents through
quick decision making. In this paper, we present a Chain Catch simulator that
allows us to incorporate game rules, design strategies and simulate the game
play. We developed cost model driven strategies for each of Escapee, Catcher
and Chain. Our results show that Sliding slope strategy is the best strategy
for Escapees whereas Tagging method is the best method for chain s movement in
Chain Catch. We also use production quality robots to implement the game play
in a physical environment and analyze game strategies on real robots. Our real
robots implementation in different scenarios shows that game strategies work as
expected and a complete chain formation takes place successfully in each game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06461</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06461</id><created>2016-02-20</created><authors><author><keyname>Mellon</keyname><forenames>Jonathan</forenames></author><author><keyname>Yoder</keyname><forenames>Jordan</forenames></author><author><keyname>Evans</keyname><forenames>Daniel</forenames></author></authors><title>Undermining and Strengthening Social Networks through Network
  Modification</title><categories>cs.SI physics.soc-ph stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks have well documented effects at the individual and aggregate
level. Consequently it is often useful to understand how an attempt to
influence a network will change its structure and consequently achieve other
goals. We develop a framework for network modification that allows for
arbitrary objective functions, types of modification (e.g. edge weight
addition, edge weight removal, node removal, and covariate value change), and
recovery mechanisms (i.e. how a network responds to interventions). The
framework outlined in this paper helps both to situate the existing work on
network interventions but also opens up many new possibilities for intervening
in networks. In particular use two case studies to highlight the potential
impact of empirically calibrating the objective function and network recovery
mechanisms as well as showing how interventions beyond node removal can be
optimised. First, we simulate an optimal removal of nodes from the Noordin
terrorist network in order to reduce the expected number of attacks (based on
empirically predicting the terrorist collaboration network from multiple types
of network ties). Second, we simulate optimally strengthening ties within
entrepreneurial ecosystems in six developing countries. In both cases we
estimate ERGM models to simulate how a network will endogenously evolve after
intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06462</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06462</id><created>2016-02-20</created><authors><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>The Singularity May Never Be Near</title><categories>cs.AI</categories><comments>Under review</comments><acm-class>I.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is both much optimism and pessimism around artificial intelligence (AI)
today. The optimists are investing millions of dollars, and even in some cases
billions of dollars into AI. The pessimists, on the other hand, predict that AI
will end many things: jobs, warfare, and even the human race. Both the
optimists and the pessimists often appeal to the idea of a technological
singularity, a point in time where machine intelligence starts to run away, and
a new, more intelligent species starts to inhabit the earth. If the optimists
are right, this will be a moment that fundamentally changes our economy and our
society. If the pessimists are right, this will be a moment that also
fundamentally changes our economy and our society. It is therefore very
worthwhile spending some time deciding if either of them might be right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06468</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06468</id><created>2016-02-20</created><authors><author><keyname>Zhang</keyname><forenames>Yuyu</forenames></author><author><keyname>Bahadori</keyname><forenames>Mohammad Taha</forenames></author><author><keyname>Su</keyname><forenames>Hang</forenames></author><author><keyname>Sun</keyname><forenames>Jimeng</forenames></author></authors><title>FLASH: Fast Bayesian Optimization for Data Analytic Pipelines</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern data science relies on data analytic pipelines to organize
interdependent computational steps. Such analytic pipelines often involve
different algorithms across multiple steps, each with its own hyperparameters.
To get the best performance, it is often critical to select optimal algorithms
and set appropriate hyperparameters, which requires large computational
efforts. Bayesian optimization provides a principled way for searching optimal
hyperparameters for a single algorithm. However, many challenges remain in
solving pipeline optimization problems with high-dimensional and highly
conditional search space. In this work, we propose Fast LineAr SearcH (FLASH),
an efficient method for tuning analytic pipelines. FLASH is a two-layer
Bayesian optimization framework, which firstly uses a parametric model to
select promising algorithms, then computes a nonparametric model to fine-tune
hyperparameters of the promising algorithms. FLASH also includes an effective
caching algorithm which can further accelerate the search process. Extensive
experiments on a number of benchmark datasets have demonstrated that FLASH
significantly outperforms previous state-of-the-art methods in both search
speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20%
improvement on test error rate compared to the baselines. Our method also
yields state-of-the-art performance on a real-world application for healthcare
predictive modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06483</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06483</id><created>2016-02-20</created><authors><author><keyname>Sonenberg</keyname><forenames>Liz</forenames></author><author><keyname>Miller</keyname><forenames>Tim</forenames></author><author><keyname>Pearce</keyname><forenames>Adrian</forenames></author><author><keyname>Felli</keyname><forenames>Paolo</forenames></author><author><keyname>Muise</keyname><forenames>Christian</forenames></author><author><keyname>Dignum</keyname><forenames>Frank</forenames></author></authors><title>Social planning for social HRI</title><categories>cs.RO cs.AI</categories><comments>Presented at &quot;2nd Workshop on Cognitive Architectures for Social
  Human-Robot Interaction 2016 (arXiv:1602.01868)&quot;</comments><report-no>CogArch4sHRI/2016/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making a computational agent 'social' has implications for how it perceives
itself and the environment in which it is situated, including the ability to
recognise the behaviours of others. We point to recent work on social planning,
i.e. planning in settings where the social context is relevant in the
assessment of the beliefs and capabilities of others, and in making appropriate
choices of what to do next.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06484</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06484</id><created>2016-02-20</created><authors><author><keyname>Riedl</keyname><forenames>Mark O.</forenames></author></authors><title>Computational Narrative Intelligence: A Human-Centered Goal for
  Artificial Intelligence</title><categories>cs.AI</categories><comments>5 pages, published in the CHI 2016 Workshop on Human-Centered Machine
  Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Narrative intelligence is the ability to craft, tell, understand, and respond
affectively to stories. We argue that instilling artificial intelligences with
computational narrative intelligence affords a number of applications
beneficial to humans. We lay out some of the machine learning challenges
necessary to solve to achieve computational narrative intelligence. Finally, we
argue that computational narrative is a practical step towards machine
enculturation, the teaching of sociocultural values to machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06488</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06488</id><created>2016-02-20</created><authors><author><keyname>Zeng</keyname><forenames>Xuezhi</forenames></author><author><keyname>Garg</keyname><forenames>Saurabh Kumar</forenames></author><author><keyname>Strazdins</keyname><forenames>Peter</forenames></author><author><keyname>Jayaraman</keyname><forenames>Prem</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author></authors><title>IOTSim: a Cloud based Simulator for Analysing IoT Applications</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A disruptive technology that is influencing not only computing paradigm but
every other business is the rise of big data. Internet of Things (IoT)
applications are considered to be a major source of big data. Such IoT
applications are in general supported through clouds where data is stored and
processed by big data processing systems. In order to improve the efficiency of
cloud infrastructure so that they can efficiently support IoT big data
applications, it is important to understand how these applications and the
corresponding big data processing systems will perform in cloud computing
environments. However, given the scalability and complex requirements of big
data processing systems, an empirical evaluation on actual cloud infrastructure
can hinder the development of timely and cost effective IoT solutions.
Therefore, a simulator supporting IoT applications in cloud environment is
highly demanded, but such work is still in its infancy. To fill this gap, we
have designed and implemented IOTSim which supports and enables simulation of
IoT big data processing using MapReduce model in cloud computing environment. A
real case study validates the efficacy of the simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06489</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06489</id><created>2016-02-20</created><authors><author><keyname>Li</keyname><forenames>Chencheng</forenames></author><author><keyname>Zhou</keyname><forenames>Pan</forenames></author><author><keyname>Zhou</keyname><forenames>Yingxue</forenames></author><author><keyname>Bian</keyname><forenames>Kaigui</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author><author><keyname>Rahardja</keyname><forenames>Susanto</forenames></author></authors><title>Distributed Private Online Learning for Social Big Data Computing over
  Data Center Networks</title><categories>cs.DC cs.LG cs.SI</categories><comments>ICC2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid growth of Internet technologies, cloud computing and social
networks have become ubiquitous. An increasing number of people participate in
social networks and massive online social data are obtained. In order to
exploit knowledge from copious amounts of data obtained and predict social
behavior of users, we urge to realize data mining in social networks. Almost
all online websites use cloud services to effectively process the large scale
of social data, which are gathered from distributed data centers. These data
are so large-scale, high-dimension and widely distributed that we propose a
distributed sparse online algorithm to handle them. Additionally,
privacy-protection is an important point in social networks. We should not
compromise the privacy of individuals in networks, while these social data are
being learned for data mining. Thus we also consider the privacy problem in
this article. Our simulations shows that the appropriate sparsity of data would
enhance the performance of our algorithm and the privacy-preserving method does
not significantly hurt the performance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06492</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06492</id><created>2016-02-20</created><authors><author><keyname>Gui</keyname><forenames>Haichao</forenames></author><author><keyname>Vukovich</keyname><forenames>George</forenames></author></authors><title>Global Finite-Time Attitude Tracking via Quaternion Feedback</title><categories>math.OC cs.SY</categories><comments>14 pages, 5 figures, submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the attitude tracking of a rigid body through quaternion
description. Global finite-time attitude controllers are designed respectively
with three types of measurements, namely, full states, attitude plus
constant-biased angular velocity, and only attitude. In all three scenarios
hybrid control techniques are utilized to overcome the well-known topological
constraint on the attitude manifold, while coupled nonsmooth feedback inputs
are designed via homogeneous theory to achieve finite-time stability.
Specially, a finite-time bias observer is derived in the second scenario and a
quaternion filter is constructed to provide damping in the absence of velocity
feedback. The proposed methods ensure bounded control torques a priori and, in
particular, include several existing attitude controllers as special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06498</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06498</id><created>2016-02-20</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Directly Coupled Observers for Quantum Harmonic Oscillators with
  Discounted Mean Square Cost Functionals and Penalized Back-action</title><categories>cs.SY math.OC math.PR quant-ph</categories><comments>11 pages, a brief version to be submitted to the IEEE 2016 Conference
  on Norbert Wiener in the 21st Century, 13-15 July, Melbourne, Australia</comments><msc-class>81Q93, 93E11, 49K15, 49N10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with quantum harmonic oscillators consisting of a
quantum plant and a directly coupled coherent quantum observer. We employ
discounted quadratic performance criteria in the form of exponentially weighted
time averages of second-order moments of the system variables. A coherent
quantum filtering (CQF) problem is formulated as the minimization of the
discounted mean square of an estimation error, with which the dynamic variables
of the observer approximate those of the plant. The cost functional also
involves a quadratic penalty on the plant-observer coupling matrix in order to
mitigate the back-action of the observer on the covariance dynamics of the
plant. For the discounted mean square optimal CQF problem with penalized
back-action, we establish first-order necessary conditions of optimality in the
form of algebraic matrix equations. By using the Hamiltonian structure of the
Heisenberg dynamics and related Lie-algebraic techniques, we represent this set
of equations in a more explicit form in the case of equally dimensioned plant
and observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06500</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06500</id><created>2016-02-21</created><authors><author><keyname>Wu</keyname><forenames>Sissi Xiaoxiao</forenames></author><author><keyname>So</keyname><forenames>Anthony Man-Cho</forenames></author><author><keyname>Pan</keyname><forenames>Jiaxian</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author></authors><title>Some Proof Derivations and Further Simulation Results for &quot;Semidefinite
  Relaxation and Approximation Analysis of a Beamformed Alamouti Scheme for
  Relay Beamforming Networks&quot;</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a companion technical report of the main manuscript &quot;Semidefinite
Relaxation and Approximation Analysis of a Beamformed Alamouti Scheme for Relay
Beamforming Networks&quot;. The report serves to give detailed derivations of Lemma
1-2 in the main manuscript, which are too long to be included in the latter. In
addition, more simulation results are presented to verify the viability of the
BF Alamouti AF schemes developed in the main manuscript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06506</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06506</id><created>2016-02-21</created><authors><author><keyname>Sakata</keyname><forenames>Ayaka</forenames></author></authors><title>Evaluation of Generalized Degrees of Freedom for Sparse Estimation by
  Replica Method</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a method to evaluate the generalized degrees of freedom (GDF),
which is a key quantity of a model selection criterion, for linear regression
with sparse regularization. Using the replica method, GDF is expressed by the
variables that characterize the saddle point of the free energy without
depending on the form of the regularization. Within the framework of replica
symmetric (RS) analysis, GDF is provided with a physical meaning as the
effective density of non-zero components. The validity of our method in the RS
phase is supported by the consistency of our results with previous mathematical
results. The analytical results in the RS phase are numerically achieved by the
belief propagation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06508</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06508</id><created>2016-02-21</created><authors><author><keyname>Wang</keyname><forenames>Li-Xin</forenames></author><author><keyname>Mendel</keyname><forenames>Jerry M.</forenames></author></authors><title>Fuzzy Opinion Networks: A Mathematical Framework for the Evolution of
  Opinions and Their Uncertainties Across Social Networks</title><categories>cs.SI cs.SY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new mathematical framework for the evolution and propagation of
opinions, called Fuzzy Opinion Network, which is the connection of a number of
Gaussian Nodes, possibly through some weighted average, time-delay or logic
operators, where a Gaussian Node is a Gaussian fuzzy set with the center and
the standard deviation being the node inputs and the fuzzy set itself being the
node output. In this framework an opinion is modeled as a Gaussian fuzzy set
with the center representing the opinion itself and the standard deviation
characterizing the uncertainty about the opinion. We study the basic
connections of Fuzzy Opinion Networks, including basic center, basic standard
deviation (sdv), basic center-sdv, chain-in-center and chain-in-sdv
connections, and we analyze a number of dynamic connections to show how
opinions and their uncertainties propagate and evolve across different network
structures and scenarios. We explain what insights we might gain from these
mathematical results about the formation and evolution of human opinions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06509</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06509</id><created>2016-02-21</created><authors><author><keyname>Ma</keyname><forenames>Junjie</forenames></author><author><keyname>Ping</keyname><forenames>Li</forenames></author></authors><title>Orthogonal AMP</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate message passing (AMP) is a low-cost iterative signal recovery
algorithm for compressed sensing. AMP consists of two constituent estimators,
one linear and one non-linear. For sensing matrices with independent
identically distributed (IID) Gaussian entries, the performance of AMP can be
asymptotically characterized by a simple scaler recursion called state
evolution (SE). SE analysis shows that AMP can potentially approach the optimal
minimum mean squared-error (MMSE) limit. However, SE may become unreliable for
other matrix ensembles, especially for ill-conditioned ones. This imposes
limits on the applications of AMP.
  In this paper, we propose an orthogonal AMP (OAMP) algorithm based on
de-correlated linear estimation (LE) and divergence-free non-linear estimation
(NLE). The Onsager term in standard AMP vanishes as a result of the
divergence-free constraint on NLE. We develop an SE procedure for OAMP and show
numerically that the SE for OAMP is accurate for a wide range of sensing
matrices, including IID Gaussian matrices, partial orthogonal matrices, and
general unitarily-invariant matrices. We discuss an orthogonality property for
the errors during the iterative process in OAMP and show intuitively that this
property is key to the validity of SE. We further derive optimized options for
OAMP and show that the corresponding SE fixed point coincides with the optimal
performance obtained via the replica method. Our numerical results demonstrate
the advantage of OAMP over AMP, in terms of final MSE or convergence speed, for
ill-conditioned matrices and partial orthogonal matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06510</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06510</id><created>2016-02-21</created><authors><author><keyname>Ramalingam</keyname><forenames>Rajasekar</forenames></author><author><keyname>Khan</keyname><forenames>Shimaz</forenames></author><author><keyname>Mohammed</keyname><forenames>Shameer</forenames></author></authors><title>The need for effective information security awareness practices in Oman
  higher educational institutions</title><categories>cs.CR cs.CY</categories><comments>1st Symposium on Communication, Information Technology and
  Biotechnology: Current Trends and Future Scope, Sur College of Applied
  Sciences, Ministry of Higher education, Sultanate of Oman, 12th and 13th May,
  2015</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The revolution of internet technology and its usage have led a significant
increase in the number of online transactions and electronic data transfer,
parallely increased the number of cybercrime incidents around the world. Steady
economic growth in the Sultanate of Oman accelerated the volume of online
utilization for e-commerce, banking, communication, education and so forth.
Normally attackers target the users who ignore security practices due to the
lack of information security awareness. Unawareness of information security
practices, user negligence, lack of awareness programs and trainings are the
root cause for information security threats. Earlier studies reveal there is a
considerable and continuous cybercrime incident in Oman which compromises the
security policy of the organizations, affecting the business continuity and the
economic growth. In this study, a survey was performed among the educational
institutions in Oman to investigate the level of information security awareness
and based on the study, a security awareness model is proposed to enable
information security practices in the educational institutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06516</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06516</id><created>2016-02-21</created><updated>2016-03-06</updated><authors><author><keyname>Ghoshdastidar</keyname><forenames>Debarghya</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling
  Techniques</title><categories>cs.LG stat.ML</categories><comments>Added new experimental results to the previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph partitioning plays a central role in machine learning, and the
development of graph partitioning algorithms is still an active area of
research. The immense demand for such algorithms arises due to the abundance of
applications that involve pairwise interactions or similarities among entities.
Recent studies in computer vision and databases systems have emphasized on the
necessity of considering multi-way interactions, and has led to the study of a
more general problem in the form of hypergraph partitioning.
  This paper focuses on the problem of partitioning uniform hypergraphs, which
arises in applications such as subspace clustering, motion segmentation etc. We
show that uniform hypergraph partitioning is equivalent to a tensor trace
maximization problem, and hence, a tensor based method is a natural answer to
this problem. We also propose a tensor spectral method that extends the widely
known spectral clustering algorithm to the case of uniform hypergraphs. While
the theoretical guarantees of spectral clustering have been extensively
studied, very little is known about the statistical properties of tensor based
methods. To this end, we prove the consistency of the proposed algorithm under
a planted partition model.
  The computational complexity of tensorial approaches has resulted in the use
of various tensor sampling strategies. We present the first theoretical study
on the effect of sampling in tensor based hypergraph partitioning. Our result
justifies the empirical success of iterative sampling techniques often used in
practice. We also present an iteratively sampled variant of the proposed
algorithm for the purpose of subspace clustering, and demonstrate the
performance of this method on a benchmark problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06518</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06518</id><created>2016-02-21</created><authors><author><keyname>Pentina</keyname><forenames>Anastasia</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>Active Task Selection for Multi-Task Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of multi-task learning, in which a
learner is given a collection of prediction tasks that need to be solved. In
contrast to previous work, we give up on the assumption that labeled training
data is available for all tasks. Instead, we propose an active task selection
framework, where based only on the unlabeled data, the learner can choose a,
typically small, subset of tasks for which he gets some labeled examples. For
the remaining tasks, which have no available annotation, solutions are found by
transferring information from the selected tasks. We analyze two transfer
strategies and develop generalization bounds for each of them. Based on this
theoretical analysis we propose two algorithms for making the choice of labeled
tasks in a principled way and show their effectiveness on synthetic and real
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06522</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06522</id><created>2016-02-21</created><authors><author><keyname>Thomas</keyname><forenames>Josephine Maria</forenames></author><author><keyname>Muscoloni</keyname><forenames>Alessandro</forenames></author><author><keyname>Ciucci</keyname><forenames>Sara</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author><author><keyname>Cannistraci</keyname><forenames>Carlo Vittorio</forenames></author></authors><title>Machine learning meets network science: dimensionality reduction for
  fast and efficient embedding of networks in the hyperbolic space</title><categories>cond-mat.dis-nn cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex network topologies and hyperbolic geometry seem specularly connected,
and one of the most fascinating and challenging problems of recent complex
network theory is to map a given network to its hyperbolic space. The
Popularity Similarity Optimization (PSO) model represents - at the moment - the
climax of this theory. It suggests that the trade-off between node popularity
and similarity is a mechanism to explain how complex network topologies emerge
- as discrete samples - from the continuous world of hyperbolic geometry. The
hyperbolic space seems appropriate to represent real complex networks. In fact,
it preserves many of their fundamental topological properties, and can be
exploited for real applications such as, among others, link prediction and
community detection. Here, we observe for the first time that a
topological-based machine learning class of algorithms - for nonlinear
unsupervised dimensionality reduction - can directly approximate the network's
node angular coordinates of the hyperbolic model into a two-dimensional space,
according to a similar topological organization that we named angular
coalescence. On the basis of this phenomenon, we propose a new class of
algorithms that offers fast and accurate coalescent embedding of networks in
the hyperbolic space even for graphs with thousands of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06529</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06529</id><created>2016-02-21</created><authors><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Robust Resource Allocation for Full-Duplex Cognitive Radio Systems</title><categories>cs.IT math.IT</categories><comments>submit to EUSIPCO 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate resource allocation algorithm design for
full-duplex (FD) cognitive radio systems. The secondary network employs a FD
base station for serving multiple half-duplex downlink and uplink users
simultaneously. We study the resource allocation design for minimizing the
maximum interference leakage to primary users while providing quality of
service for secondary users. The imperfectness of the channel state information
of the primary users is taken into account for robust resource allocation
algorithm design. The algorithm design is formulated as a non-convex
optimization problem and solved optimally by applying semidefinite programming
(SDP) relaxation. Simulation results not only show the significant reduction in
interference leakage compared to baseline schemes, but also confirm the
robustness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06531</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06531</id><created>2016-02-21</created><authors><author><keyname>Pentina</keyname><forenames>Anastasia</forenames></author><author><keyname>Ben-David</keyname><forenames>Shai</forenames></author></authors><title>Multi-task and Lifelong Learning of Kernels</title><categories>stat.ML cs.LG</categories><comments>Appears in Proceedings of ALT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a problem of learning kernels for use in SVM classification in
the multi-task and lifelong scenarios and provide generalization bounds on the
error of a large margin classifier. Our results show that, under mild
conditions on the family of kernels used for learning, solving several related
tasks simultaneously is beneficial over single task learning. In particular, as
the number of observed tasks grows, assuming that in the considered family of
kernels there exists one that yields low approximation error on all tasks, the
overhead associated with learning such a kernel vanishes and the complexity
converges to that of learning when this good kernel is given to the learner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06539</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06539</id><created>2016-02-21</created><authors><author><keyname>Liu</keyname><forenames>Liangchen</forenames></author><author><keyname>Wiliem</keyname><forenames>Arnold</forenames></author><author><keyname>Chen</keyname><forenames>Shaokang</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Determining the best attributes for surveillance video keywords
  generation</title><categories>cs.LG cs.AI</categories><comments>7 pages, ISBA 2016. arXiv admin note: text overlap with
  arXiv:1602.01940</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic video keyword generation is one of the key ingredients in reducing
the burden of security officers in analyzing surveillance videos. Keywords or
attributes are generally chosen manually based on expert knowledge of
surveillance. Most existing works primarily aim at either supervised learning
approaches relying on extensive manual labelling or hierarchical probabilistic
models that assume the features are extracted using the bag-of-words approach;
thus limiting the utilization of the other features. To address this, we turn
our attention to automatic attribute discovery approaches. However, it is not
clear which automatic discovery approach can discover the most meaningful
attributes. Furthermore, little research has been done on how to compare and
choose the best automatic attribute discovery methods. In this paper, we
propose a novel approach, based on the shared structure exhibited amongst
meaningful attributes, that enables us to compare between different automatic
attribute discovery approaches.We then validate our approach by comparing
various attribute discovery methods such as PiCoDeS on two attribute datasets.
The evaluation shows that our approach is able to select the automatic
discovery approach that discovers the most meaningful attributes. We then
employ the best discovery approach to generate keywords for videos recorded
from a surveillance system. This work shows it is possible to massively reduce
the amount of manual work in generating video keywords without limiting
ourselves to a particular video feature descriptor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06541</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06541</id><created>2016-02-21</created><authors><author><keyname>Thoma</keyname><forenames>Martin</forenames></author></authors><title>A Survey of Semantic Segmentation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This survey gives an overview over different techniques used for pixel-level
semantic segmentation. Metrics and datasets for the evaluation of segmentation
algorithms and traditional approaches for segmentation such as unsupervised
methods, Decision Forests and SVMs are described and pointers to the relevant
papers are given. Recently published approaches with convolutional neural
networks are mentioned and typical problematic situations for segmentation
algorithms are examined. A taxonomy of segmentation algorithms is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06550</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06550</id><created>2016-02-21</created><updated>2016-02-28</updated><authors><author><keyname>Melnyk</keyname><forenames>Igor</forenames></author><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author><author><keyname>Matthews</keyname><forenames>Bryan</forenames></author><author><keyname>Oza</keyname><forenames>Nikunj</forenames></author></authors><title>Semi-Markov Switching Vector Autoregressive Model-based Anomaly
  Detection in Aviation Systems</title><categories>cs.LG stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the problem of anomaly detection in heterogeneous,
multivariate, variable-length time series datasets. Our focus is on the
aviation safety domain, where data objects are flights and time series are
sensor readings and pilot switches. In this context the goal is to detect
anomalous flight segments, due to mechanical, environmental, or human factors
in order to identifying operationally significant events and provide insights
into the flight operations and highlight otherwise unavailable potential safety
risks and precursors to accidents. For this purpose, we propose a framework
which represents each flight using a semi-Markov switching vector
autoregressive (SMS-VAR) model. Detection of anomalies is then based on
measuring dissimilarities between the model's prediction and data observation.
The framework is scalable, due to the inherent parallel nature of most
computations, and can be used to perform online anomaly detection. Extensive
experimental results on simulated and real datasets illustrate that the
framework can detect various types of anomalies along with the key parameters
involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06561</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06561</id><created>2016-02-21</created><updated>2016-02-23</updated><authors><author><keyname>Heaton</keyname><forenames>J. B.</forenames></author><author><keyname>Polson</keyname><forenames>N. G.</forenames></author><author><keyname>Witte</keyname><forenames>J. H.</forenames></author></authors><title>Deep Learning in Finance</title><categories>cs.LG</categories><comments>20 Pages, 5 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the use of deep learning hierarchical models for problems in
financial prediction and classification. Financial prediction problems -- such
as those presented in designing and pricing securities, constructing
portfolios, and risk management -- often involve large data sets with complex
data interactions that currently are difficult or impossible to specify in a
full economic model. Applying deep learning methods to these problems can
produce more useful results than standard methods in finance. In particular,
deep learning can detect and exploit interactions in the data that are, at
least currently, invisible to any existing financial economic theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06564</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06564</id><created>2016-02-21</created><authors><author><keyname>Yuan</keyname><forenames>Jiangye</forenames></author></authors><title>Automatic Building Extraction in Aerial Scenes Using Convolutional
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic building extraction from aerial and satellite imagery is highly
challenging due to extremely large variations of building appearances. To
attack this problem, we design a convolutional network with a final stage that
integrates activations from multiple preceding stages for pixel-wise
prediction, and introduce the signed distance function of building boundaries
as the output representation, which has an enhanced representation power. We
leverage abundant building footprint data available from geographic information
systems (GIS) to compile training data. The trained network achieves superior
performance on datasets that are significantly larger and more complex than
those used in prior work, demonstrating that the proposed method provides a
promising and scalable solution for automating this labor-intensive task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06566</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06566</id><created>2016-02-21</created><authors><author><keyname>Maiti</keyname><forenames>Dipayan</forenames></author><author><keyname>Islam</keyname><forenames>Mohammad Raihanul</forenames></author><author><keyname>Leman</keyname><forenames>Scotland</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Interactive Storytelling over Document Collections</title><categories>cs.AI cs.LG stat.ML</categories><comments>This paper has been submitted to a conference for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Storytelling algorithms aim to 'connect the dots' between disparate documents
by linking starting and ending documents through a series of intermediate
documents. Existing storytelling algorithms are based on notions of coherence
and connectivity, and thus the primary way by which users can steer the story
construction is via design of suitable similarity functions. We present an
alternative approach to storytelling wherein the user can interactively and
iteratively provide 'must use' constraints to preferentially support the
construction of some stories over others. The three innovations in our approach
are distance measures based on (inferred) topic distributions, the use of
constraints to define sets of linear inequalities over paths, and the
introduction of slack and surplus variables to condition the topic distribution
to preferentially emphasize desired terms over others. We describe experimental
results to illustrate the effectiveness of our interactive storytelling
approach over multiple text datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06568</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06568</id><created>2016-02-21</created><authors><author><keyname>Berger</keyname><forenames>Martin</forenames></author><author><keyname>Tratt</keyname><forenames>Laurence</forenames></author><author><keyname>Urban</keyname><forenames>Christian</forenames></author></authors><title>Modelling homogeneous generative meta-programming</title><categories>cs.PL</categories><acm-class>D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homogeneous generative meta-programming (HGMP) enables the generation of
program fragments at compile-time or run-time. We present the first
foundational calculus which can model powerful HGMP languages such as Template
Haskell. The calculus is designed such that we can gradually enhance it with
the features needed to model many of the advanced features of real languages.
As a demonstration of the flexibility of our approach, we also provide a simple
type system for the calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06571</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06571</id><created>2016-02-21</created><authors><author><keyname>Yang</keyname><forenames>Pu</forenames></author><author><keyname>Iyer</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Frazier</keyname><forenames>Peter</forenames></author></authors><title>Mean Field Equilibria for Competitive Exploration in Resource Sharing
  Settings</title><categories>cs.GT</categories><comments>17 pages, 1 figure, 1 table, to appear in proceedings of the 25th
  International World Wide Web Conference(WWW2016)</comments><acm-class>J.4</acm-class><doi>10.1145/2872427.2883011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a model of nomadic agents exploring and competing for
time-varying location-specific resources, arising in crowdsourced
transportation services, online communities, and in traditional location based
economic activity. This model comprises a group of agents, and a set of
locations each endowed with a dynamic stochastic resource process. Each agent
derives a periodic reward determined by the overall resource level at her
location, and the number of other agents there. Each agent is strategic and
free to move between locations, and at each time decides whether to stay at the
same node or switch to another one. We study the equilibrium behavior of the
agents as a function of dynamics of the stochastic resource process and the
nature of the externality each agent imposes on others at the same location. In
the asymptotic limit with the number of agents and locations increasing
proportionally, we show that an equilibrium exists and has a threshold
structure, where each agent decides to switch to a different location based
only on their current location's resource level and the number of other agents
at that location. This result provides insight into how system structure
affects the agents' collective ability to explore their domain to find and
effectively utilize resource-rich areas. It also allows assessing the impact of
changing the reward structure through penalties or subsidies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06577</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06577</id><created>2016-02-21</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author></authors><title>2-Bit Random Projections, NonLinear Estimators, and Approximate Near
  Neighbor Search</title><categories>stat.ML cs.DS cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1403.8144</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method of random projections has become a standard tool for machine
learning, data mining, and search with massive data at Web scale. The effective
use of random projections requires efficient coding schemes for quantizing
(real-valued) projected data into integers. In this paper, we focus on a simple
2-bit coding scheme. In particular, we develop accurate nonlinear estimators of
data similarity based on the 2-bit strategy. This work will have important
practical applications. For example, in the task of near neighbor search, a
crucial step (often called re-ranking) is to compute or estimate data
similarities once a set of candidate data points have been identified by hash
table techniques. This re-ranking step can take advantage of the proposed
coding scheme and estimator.
  As a related task, in this paper, we also study a simple uniform quantization
scheme for the purpose of building hash tables with projected data. Our
analysis shows that typically only a small number of bits are needed. For
example, when the target similarity level is high, 2 or 3 bits might be
sufficient. When the target similarity level is not so high, it is preferable
to use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good
choice for the task of sublinear time approximate near neighbor search via hash
tables.
  Combining these results, we conclude that 2-bit random projections should be
recommended for approximate near neighbor search and similarity estimation.
Extensive experimental results are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06582</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06582</id><created>2016-02-21</created><authors><author><keyname>Levin</keyname><forenames>Dovid Y.</forenames></author><author><keyname>Habets</keyname><forenames>Emanu&#xeb;l A. P.</forenames></author><author><keyname>Gannot</keyname><forenames>Sharon</forenames></author></authors><title>Near-field signal acquisition for smartglasses using two acoustic
  vector-sensors</title><categories>cs.SD</categories><comments>The abstract displayed in the metadata field is slightly modified due
  to space limitations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smartglasses, in addition to their visual-output capabilities, often contain
acoustic sensors for receiving the user's voice. However, operation in noisy
environments may lead to significant degradation of the received signal. To
address this issue, we propose employing an acoustic sensor array which is
mounted on the eyeglasses frames. The signals from the array are processed by
an algorithm with the purpose of acquiring the user's desired near-filed speech
signal while suppressing noise signals originating from the environment. The
array is comprised of two AVSs which are located at the fore of the glasses'
temples. Each AVS consists of four collocated subsensors: one pressure sensor
(with an omnidirectional response) and three particle-velocity sensors (with
dipole responses) oriented in mutually orthogonal directions. The array
configuration is designed to boost the input power of the desired signal, and
to ensure that the characteristics of the noise at the different channels are
sufficiently diverse (lending towards more effective noise suppression). Since
changes in the array's position correspond to the desired speaker's movement,
the relative source-receiver position remains unchanged; hence, the need to
track fluctuations of the steering vector is avoided. Conversely, the spatial
statistics of the noise are subject to rapid and abrupt changes due to sudden
movement and rotation of the user's head. Consequently, the algorithm must be
capable of rapid adaptation. We propose an algorithm which incorporates
detection of the desired speech in the time-frequency domain, and employs this
information to adaptively update estimates of the noise statistics. Speech
detection plays a key role in ensuring the quality of the output signal. We
conduct controlled measurements of the array in noisy scenarios. The proposed
algorithm preforms favorably with respect to conventional algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06586</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06586</id><created>2016-02-21</created><updated>2016-02-29</updated><authors><author><keyname>Huang</keyname><forenames>Qingqing</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Kong</keyname><forenames>Weihao</forenames></author><author><keyname>Valiant</keyname><forenames>Gregory</forenames></author></authors><title>Recovering Structured Probability Matrices</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of accurately recovering a matrix B of size M by M ,
which represents a probability distribution over M^2 outcomes, given access to
an observed matrix of &quot;counts&quot; generated by taking independent samples from the
distribution B. How can structural properties of the underlying matrix B be
leveraged to yield computationally efficient and information theoretically
optimal reconstruction algorithms? When can accurate reconstruction be
accomplished in the sparse data regime? This basic problem lies at the core of
a number of questions that are currently being considered by different
communities, including community detection in sparse random graphs, learning
structured models such as topic models or hidden Markov models, and the efforts
from the natural language processing community to compute &quot;word embeddings&quot;.
  Our results apply to the setting where B has a rank 2 structure. For this
setting, we propose an efficient (and practically viable) algorithm that
accurately recovers the underlying M by M matrix using Theta(M) samples. This
result easily translates to Theta(M) sample algorithms for learning topic
models with two topics over dictionaries of size M, and learning hidden Markov
Models with two hidden states and observation distributions supported on M
elements. These linear sample complexities are optimal, up to constant factors,
in an extremely strong sense: even testing basic properties of the underlying
matrix (such as whether it has rank 1 or 2) requires Omega(M) samples. This
impossibility of sublinear-sample property testing in these settings is
intriguing and underscores the significant differences between these structured
settings and the standard setting of drawing i.i.d samples from an unstructured
distribution of support size M.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06589</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06589</id><created>2016-02-21</created><authors><author><keyname>Di Napoli</keyname><forenames>Edoardo</forenames><affiliation>J&#xfc;lich Supercomputing Centre</affiliation><affiliation>J&#xfc;lich Aachen Research Alliance -- High-performance Computing</affiliation></author><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen University</affiliation></author><author><keyname>Hrywniak</keyname><forenames>Markus</forenames><affiliation>GRS, RWTH Aachen University</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen University</affiliation></author></authors><title>High-performance generation of the Hamiltonian and Overlap matrices in
  FLAPW methods</title><categories>cs.CE cs.DS cs.PF physics.comp-ph</categories><comments>26 pages, 2 figures and one table. Submitted to Computer Physics
  Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the greatest effort of computational scientists is to translate the
mathematical model describing a class of physical phenomena into large and
complex codes. Many of these codes face the difficulty of implementing the
mathematical operations in the model in terms of low level optimized kernels
offering both performance and portability. Legacy codes suffers from the
additional curse of rigid design choices based on outdated performance metrics
(e.g. minimization of memory footprint). Using a representative code from the
Materials Science community, we propose a methodology to restructure the most
expensive operations in terms of an optimized combination of dense linear
algebra kernels. The resulting algorithm guarantees an increased performance
and an extended life span of this code enabling larger scale simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06594</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06594</id><created>2016-02-21</created><authors><author><keyname>Chong</keyname><forenames>Michelle S.</forenames></author><author><keyname>Kuijper</keyname><forenames>Margreta</forenames></author></authors><title>Vulnerability of linear systems against sensor attacks--a system's
  security index</title><categories>cs.SY cs.CR</categories><comments>Submitted</comments><msc-class>93B35, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The `security index' of a discrete-time LTI system under sensor attacks is
introduced as a quantitative measure on the security of an observable system.
We derive ideas from error control coding theory to provide sufficient
conditions for attack detection and correction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06598</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06598</id><created>2016-02-21</created><authors><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Nam</keyname><forenames>Young-Han</forenames></author><author><keyname>Rahman</keyname><forenames>Md Saifur</forenames></author><author><keyname>Zhang</keyname><forenames>Jianzhong</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Initial Beam Association in Millimeter Wave Cellular Systems: Analysis
  and Design Insights</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enabling the high data rates of millimeter wave (mmWave) cellular systems
requires deploying large antenna arrays at both the basestations and mobile
users. The beamforming weights of these large arrays need to be tuned to
guarantee sufficient beamforming gains. Prior work on coverage and rate of
mmWave cellular networks focused mainly on the case when basestations and
mobile users beamfomring vectors are perfectly designed for maximum beamforming
gains. Designing beamforming/combining vectors, though, requires training which
may impact both the SINR coverage and rate of mmWave cellular systems. This
paper characterizes and evaluates the performance of mmWave cellular networks
while accounting for the beam training/association overhead. First, a model for
the initial beam association is developed based on beam sweeping and downlink
control pilot reuse. To incorporate the impact of beam training into system
performance, a new metric, called the effective reliable rate, is defined and
adopted. Using stochastic geometry, the effective reliable rate of mmWave
cellular networks is derived for two special cases: with near-orthogonal
control pilots and with full pilot reuse. Analytical and simulation results
provide insights into the answers of three important questions: (i) What is the
impact of beam association on mmWave network performance? (ii) Should
orthogonal or reused control pilots be employed in the initial beam association
phase? (iii) Should exhaustive or hierarchical search be adopted for the beam
training phase? The results show that unless the employed beams are very wide
or the system coherence block length is very small, exhaustive search with full
pilot reuse is nearly as good as perfect beam alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06599</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06599</id><created>2016-02-21</created><authors><author><keyname>Mercan</keyname><forenames>Hanefi</forenames></author><author><keyname>Yilmaz</keyname><forenames>Cemal</forenames></author></authors><title>Towards Unified Combinatorial Interaction Testing</title><categories>cs.SE</categories><comments>5 pages, 1 algorithm, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We believe that we can exploit the benefits of combinatorial interaction
testing (CIT) on many &quot;non-traditional&quot; combinatorial spaces using many
&quot;non-traditional&quot; coverage criteria. However, this requires truly flexible CIT
approaches. To this end, we introduce Unified Combinatorial Interaction Testing
(U-CIT), which enables practitioners to define their own combinatorial spaces
and coverage criteria for testing, and present a unified construction approach
to compute specific instances of U-CIT objects. We, furthermore, argue that
most (if not all) existing CIT objects are a special case of U-CIT and
demonstrate the flexibility of U-CIT on a simple, yet realistic scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06604</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06604</id><created>2016-02-21</created><authors><author><keyname>Lokhov</keyname><forenames>Andrey Y.</forenames></author><author><keyname>Lemons</keyname><forenames>Nathan</forenames></author><author><keyname>McAndrew</keyname><forenames>Thomas C.</forenames></author><author><keyname>Hagberg</keyname><forenames>Aric</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author></authors><title>Detection of faults and intrusions in cyber-physical systems from
  physical correlations</title><categories>cs.SY cs.SI physics.data-an physics.soc-ph stat.AP</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-physical systems are critical infrastructures crucial to the reliable
delivery of energy and other resources, and to the stable functioning of
automatic and control architectures. These systems are composed of
interdependent physical, control and communications networks described by
disparate mathematical models creating scientific challenges that go well
beyond the modeling and analysis of the individual networks. A key challenge in
cyber-physical defense is a fast online detection and localization of faults
and intrusions without a prior knowledge of the failure type. We describe a set
of techniques for an efficient identification of faults from correlations in
physical signals, assuming that the minimal amount of information on the system
is available. The performance of detection method is illustrated on data
collected from a large building automation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06612</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06612</id><created>2016-02-21</created><authors><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Villar</keyname><forenames>Soledad</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>Clustering subgaussian mixtures by semidefinite programming</title><categories>stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model-free, parameter-free relax-and-round algorithm for
$k$-means clustering, based on a semidefinite relaxation due to Peng and Wei.
The algorithm interprets the SDP output as a denoised version of the original
data and then rounds this output to a hard clustering.
  We provide a generic method for proving performance guarantees for this
algorithm, and we analyze the algorithm in the context of subgaussian mixture
models. We also study the fundamental limits for estimating Gaussian centers by
$k$-means clustering in order to compare our approximation guarantee to the
theoretically optimal $k$-means clustering solution. In particular, our
approximation guarantee has no dependence on the number of points, and for
equidistant clusters at $O(k)$ separation, is optimal up to a factor of $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06620</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06620</id><created>2016-02-21</created><authors><author><keyname>Stinson</keyname><forenames>Kerrek</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Constantine</keyname><forenames>Paul G.</forenames></author></authors><title>A randomized algorithm for enumerating zonotope vertices</title><categories>math.NA cs.NA math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a randomized algorithm for enumerating the vertices of a zonotope,
which is a low-dimensional linear projection of a hypercube. The algorithm
produces a pair of the zonotope's vertices by sampling a random linear
combination of the zonotope generators, where the combination's weights are the
signs of the product between the zonotope's generator matrix and random vectors
with normally distributed entries. We study the probability of recovering
particular vertices and relate it to the vertices' normal cones. This study
shows that if we terminate the randomized algorithm before all vertices are
recovered, then the convex hull of the resulting vertex set approximates the
zonotope. In high dimensions, we expect the enumeration algorithm to be most
appropriate as an approximation algorithm---particularly for cases when
existing methods are not practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06627</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06627</id><created>2016-02-21</created><authors><author><keyname>Lin</keyname><forenames>Chengyu</forenames></author><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>Sensitivity Conjecture and Log-rank Conjecture for functions with small
  alternating numbers</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sensitivity Conjecture and the Log-rank Conjecture are among the most
important and challenging problems in concrete complexity. Incidentally, the
Sensitivity Conjecture is known to hold for monotone functions, and so is the
Log-rank Conjecture for $f(x \wedge y)$ and $f(x\oplus y)$ with monotone
functions $f$, where $\wedge$ and $\oplus$ are bit-wise AND and XOR,
respectively. In this paper, we extend these results to functions $f$ which
alternate values for a relatively small number of times on any monotone path
from $0^n$ to $1^n$. These deepen our understandings of the two conjectures,
and contribute to the recent line of research on functions with small
alternating numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06632</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06632</id><created>2016-02-21</created><updated>2016-02-22</updated><authors><author><keyname>Bhamre</keyname><forenames>Tejal</forenames></author><author><keyname>Zhang</keyname><forenames>Teng</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Denoising and Covariance Estimation of Single Particle Cryo-EM Images</title><categories>cs.CV q-bio.BM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of image restoration in cryo-EM entails correcting for the
effects of the Contrast Transfer Function (CTF) and noise. Popular methods for
image restoration include `phase flipping', which corrects only for the Fourier
phases but not amplitudes, and Wiener filtering, which requires the spectral
signal to noise ratio. We propose a new image restoration method which we call
`Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of the
projection images is used within the classical Wiener filtering framework for
solving the image restoration deconvolution problem. Our estimation procedure
for the covariance matrix is new and successfully corrects for the CTF. We
demonstrate the efficacy of CWF by applying it to restore both simulated and
experimental cryo-EM images. Results with experimental datasets demonstrate
that CWF provides a good way to evaluate the particle images and to see what
the dataset contains even without 2D classification and averaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06634</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06634</id><created>2016-02-21</created><authors><author><keyname>Suzuki</keyname><forenames>Ryo</forenames></author><author><keyname>Salehi</keyname><forenames>Niloufar</forenames></author><author><keyname>Lam</keyname><forenames>Michelle S.</forenames></author><author><keyname>Marroquin</keyname><forenames>Juan C.</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael S.</forenames></author></authors><title>Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships</title><categories>cs.HC</categories><comments>CHI 2016</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expert crowdsourcing marketplaces have untapped potential to empower workers'
career and skill development. Currently, many workers cannot afford to invest
the time and sacrifice the earnings required to learn a new skill, and a lack
of experience makes it difficult to get job offers even if they do. In this
paper, we seek to lower the threshold to skill development by repurposing
existing tasks on the marketplace as mentored, paid, real-world work
experiences, which we refer to as micro-internships. We instantiate this idea
in Atelier, a micro-internship platform that connects crowd interns with crowd
mentors. Atelier guides mentor-intern pairs to break down expert crowdsourcing
tasks into milestones, review intermediate output, and problem-solve together.
We conducted a field experiment comparing Atelier's mentorship model to a
non-mentored alternative on a real-world programming crowdsourcing task,
finding that Atelier helped interns maintain forward progress and absorb best
practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06643</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06643</id><created>2016-02-21</created><authors><author><keyname>Speranzon</keyname><forenames>Alberto</forenames></author><author><keyname>Bopardikar</keyname><forenames>Shaunak D.</forenames></author></authors><title>An Algebraic Topological Approach to Privacy: Numerical and Categorical
  Data</title><categories>cs.DB cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we cast the classic problem of achieving k-anonymity for a
given database as a problem in algebraic topology. Using techniques from this
field of mathematics, we propose a framework for k-anonymity that brings new
insights and algorithms to anonymize a database. We begin by addressing the
simpler case when the data lies in a metric space. This case is instrumental to
introduce the main ideas and notation. Specifically, by mapping a database to
the Euclidean space and by considering the distance between datapoints, we
introduce a simplicial representation of the data and show how concepts from
algebraic topology, such as the nerve complex and persistent homology, can be
applied to efficiently obtain the entire spectrum of k-anonymity of the
database for various values of k and levels of generalization. For this
representation, we provide an analytic characterization of conditions under
which a given representation of the dataset is k-anonymous. We introduce a
weighted barcode diagram which, in this context, becomes a computational tool
to tradeoff data anonymity with data loss expressed as level of generalization.
Some simulations results are used to illustrate the main idea of the paper. We
conclude the paper with a discussion on how to extend this method to address
the general case of a mix of categorical and metric data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06645</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06645</id><created>2016-02-21</created><authors><author><keyname>Liu</keyname><forenames>Song</forenames></author><author><keyname>Li</keyname><forenames>Wanqing</forenames></author><author><keyname>Ogunbona</keyname><forenames>Philip</forenames></author><author><keyname>Chow</keyname><forenames>Yang-Wai</forenames></author></authors><title>Creating Simplified 3D Models with High Quality Textures</title><categories>cs.GR cs.CV</categories><comments>2015 International Conference on Digital Image Computing: Techniques
  and Applications (DICTA), Page 1 - 8</comments><doi>10.1109/DICTA.2015.7371249</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an extension to the KinectFusion algorithm which allows
creating simplified 3D models with high quality RGB textures. This is achieved
through (i) creating model textures using images from an HD RGB camera that is
calibrated with Kinect depth camera, (ii) using a modified scheme to update
model textures in an asymmetrical colour volume that contains a higher number
of voxels than that of the geometry volume, (iii) simplifying dense polygon
mesh model using quadric-based mesh decimation algorithm, and (iv) creating and
mapping 2D textures to every polygon in the output 3D model. The proposed
method is implemented in real-time by means of GPU parallel processing.
Visualization via ray casting of both geometry and colour volumes provides
users with a real-time feedback of the currently scanned 3D model. Experimental
results show that the proposed method is capable of keeping the model texture
quality even for a heavily decimated model and that, when reconstructing small
objects, photorealistic RGB textures can still be reconstructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06647</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06647</id><created>2016-02-21</created><authors><author><keyname>Liu</keyname><forenames>Song</forenames></author><author><keyname>Li</keyname><forenames>Wanqing</forenames></author><author><keyname>Davis</keyname><forenames>Stephen</forenames></author><author><keyname>Ritz</keyname><forenames>Christian</forenames></author><author><keyname>Tian</keyname><forenames>Hongda</forenames></author></authors><title>Planogram Compliance Checking Based on Detection of Recurring Patterns</title><categories>cs.CV</categories><comments>Accepted by MM (IEEE Multimedia Magazine) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel method for automatic planogram compliance checking in
retail chains is proposed without requiring product template images for
training. Product layout is extracted from an input image by means of
unsupervised recurring pattern detection and matched via graph matching with
the expected product layout specified by a planogram to measure the level of
compliance. A divide and conquer strategy is employed to improve the speed.
Specifically, the input image is divided into several regions based on the
planogram. Recurring patterns are detected in each region respectively and then
merged together to estimate the product layout. Experimental results on real
data have verified the efficacy of the proposed method. Compared with a
template-based method, higher accuracies are achieved by the proposed method
over a wide range of products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06648</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06648</id><created>2016-02-22</created><authors><author><keyname>Hwang</keyname><forenames>Sung-Ha</forenames></author><author><keyname>Rey-Bellet</keyname><forenames>Luc</forenames></author></authors><title>Strategic Decompositions of Normal Form Games: Zero-sum Games and
  Potential Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study new classes of games, called zero-sum equivalent games and zero-sum
equivalent potential games, and prove decomposition theorems involving these
classes of games. We say that two games are &quot;strategically equivalent&quot; if, for
every player, the payoff differences between two strategies (holding other
players' strategies fixed) are identical. A zero-sum equivalent game is a game
that is strategically equivalent to a zero-sum game; a zero-sum equivalent
potential game is a zero-sum equivalent game that is strategically equivalent
to a common interest game. We also call a game &quot;normalized&quot; if the sum of one
player's payoffs, given the other players' strategies, is always zero. We show
that any normal form game can be uniquely decomposed into either (i) a zero-sum
equivalent game and a normalized common interest game, or (ii) a zero-sum
equivalent potential game, a normalized zero-sum game, and a normalized common
interest game, each with distinctive equilibrium properties. For example, we
show that two-player zero-sum equivalent games with finite strategy sets
generically have a unique Nash equilibrium and that two-player zero-sum
equivalent potential games with finite strategy sets generically have a
strictly dominant Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06652</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06652</id><created>2016-02-22</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author></authors><title>Auditory System for a Mobile Robot</title><categories>cs.RO cs.SD</categories><comments>120 pages, PhD thesis, University of Sherbrooke, 2005</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this thesis, we propose an artificial auditory system that gives a robot
the ability to locate and track sounds, as well as to separate simultaneous
sound sources and recognising simultaneous speech. We demonstrate that it is
possible to implement these capabilities using an array of microphones, without
trying to imitate the human auditory system. The sound source localisation and
tracking algorithm uses a steered beamformer to locate sources, which are then
tracked using a multi-source particle filter. Separation of simultaneous sound
sources is achieved using a variant of the Geometric Source Separation (GSS)
algorithm, combined with a multi-source post-filter that further reduces noise,
interference and reverberation. Speech recognition is performed on separated
sources, either directly or by using Missing Feature Theory (MFT) to estimate
the reliability of the speech features.
  The results obtained show that it is possible to track up to four
simultaneous sound sources, even in noisy and reverberant environments.
Real-time control of the robot following a sound source is also demonstrated.
The sound source separation approach we propose is able to achieve a 13.7 dB
improvement in signal-to-noise ratio compared to a single microphone when three
speakers are present. In these conditions, the system demonstrates more than
80% accuracy on digit recognition, higher than most human listeners could
obtain in our small case study when recognising only one of these sources. All
these new capabilities will allow humans to interact more naturally with a
mobile robot in real life settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06653</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06653</id><created>2016-02-22</created><authors><author><keyname>Poojary</keyname><forenames>Sudheer</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Analysis of Multiple Flows using Different High Speed TCP protocols on a
  General Network</title><categories>cs.NI</categories><comments>Submitted to Performance Evaluation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop analytical tools for performance analysis of multiple TCP flows
(which could be using TCP CUBIC, TCP Compound, TCP New Reno) passing through a
multi-hop network. We first compute average window size for a single TCP
connection (using CUBIC or Compound TCP) under random losses. We then consider
two techniques to compute steady state throughput for different TCP flows in a
multi-hop network. In the first technique, we approximate the queues as M/G/1
queues. In the second technique, we use an optimization program whose solution
approximates the steady state throughput of the different flows. Our results
match well with ns2 simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06654</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06654</id><created>2016-02-22</created><authors><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Liu</keyname><forenames>Fayao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author></authors><title>Structured Learning of Binary Codes with Column Generation</title><categories>cs.LG</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hashing methods aim to learn a set of hash functions which map the original
features to compact binary codes with similarity preserving in the Hamming
space. Hashing has proven a valuable tool for large-scale information
retrieval. We propose a column generation based binary code learning framework
for data-dependent hash function learning. Given a set of triplets that encode
the pairwise similarity comparison information, our column generation based
method learns hash functions that preserve the relative comparison relations
within the large-margin learning framework. Our method iteratively learns the
best hash functions during the column generation procedure. Existing hashing
methods optimize over simple objectives such as the reconstruction error or
graph Laplacian related loss functions, instead of the performance evaluation
criteria of interest---multivariate performance measures such as the AUC and
NDCG. Our column generation based method can be further generalized from the
triplet loss to a general structured learning based framework that allows one
to directly optimize multivariate performance measures. For optimizing general
ranking measures, the resulting optimization problem can involve exponentially
or infinitely many variables and constraints, which is more challenging than
standard structured output learning. We use a combination of column generation
and cutting-plane techniques to solve the optimization problem. To speed-up the
training we further explore stage-wise training and propose to use a simplified
NDCG loss for efficient inference. We demonstrate the generality of our method
by applying it to ranking prediction and image retrieval, and show that it
outperforms a few state-of-the-art hashing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06657</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06657</id><created>2016-02-22</created><authors><author><keyname>Sarkar</keyname><forenames>Kaushik</forenames></author><author><keyname>Sundaram</keyname><forenames>Hari</forenames></author></authors><title>Influencing Busy People in a Social Network</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: text overlap with arXiv:1303.5903</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify influential early adopters in a social network, where individuals
are resource constrained, to maximize the spread of multiple, costly behaviors.
A solution to this problem is especially important for viral marketing. The
problem of maximizing influence in a social network is challenging since it is
computationally intractable. We make three contributions. First, propose a new
model of collective behavior that incorporates individual intent, knowledge of
neighbors actions and resource constraints. Second, we show that the multiple
behavior influence maximization is NP-hard. Furthermore, we show that the
problem is submodular, implying the existence of a greedy solution that
approximates the optimal solution to within a constant. However, since the
greedy algorithm is expensive for large networks, we propose efficient
heuristics to identify the influential individuals, including heuristics to
assign behaviors to the different early adopters. We test our approach on
synthetic and real-world topologies with excellent results. We evaluate the
effectiveness under three metrics: unique number of participants, total number
of active behaviors and network resource utilization. Our heuristics produce
15-51% increase in expected resource utilization over the naive approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06659</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06659</id><created>2016-02-22</created><authors><author><keyname>Liu</keyname><forenames>Fu-Hong</forenames></author><author><keyname>Liu</keyname><forenames>Hsiang-Hsuan</forenames></author><author><keyname>Wong</keyname><forenames>Prudence W. H.</forenames></author></authors><title>Optimal Nonpreemptive Scheduling in a Smart Grid Model</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a scheduling problem arising in demand response management in smart
grid. Consumers send in power requests with a flexible feasible time interval
during which their requests can be served. The grid controller, upon receiving
power requests, schedules each request within the specified interval. The
electricity cost is measured by a convex function of the load in each timeslot.
The objective is to schedule all requests with the minimum total electricity
cost. Previous work has studied cases where jobs have unit power requirement
and unit duration, and offline setting was considered. We extend the study to
arbitrary power requirement and duration, which has been shown to be NP-hard.
We give the first online algorithm for the problem, and prove the competitive
ratio is optimal. We also prove that the problem is fixed parameter tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06662</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06662</id><created>2016-02-22</created><authors><author><keyname>Henaff</keyname><forenames>Mikael</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Orthogonal RNNs and Long-Memory Tasks</title><categories>cs.NE cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although RNNs have been shown to be powerful tools for processing sequential
data, finding architectures or optimization strategies that allow them to model
very long term dependencies is still an active area of research. In this work,
we carefully analyze two synthetic datasets originally outlined in (Hochreiter
and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store
information over many time steps. We explicitly construct RNN solutions to
these problems, and using these constructions, illuminate both the problems
themselves and the way in which RNNs store different types of information in
their hidden states. These constructions furthermore explain the success of
recent methods that specify unitary initializations or constraints on the
transition matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06664</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06664</id><created>2016-02-22</created><updated>2016-03-01</updated><authors><author><keyname>Sun</keyname><forenames>Ju</forenames></author><author><keyname>Qu</keyname><forenames>Qing</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>A Geometric Analysis of Phase Retrieval</title><categories>cs.IT math.IT math.OC stat.ML</categories><comments>55 pages, 5 figures. A short version can be found here
  http://sunju.org/docs/PR_G4_16.pdf . This is a minor revised version based on
  feedback from other researchers. Submitted to the Journal of Foundation of
  Computational Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can we recover a complex signal from its Fourier magnitudes? More generally,
given a set of $m$ measurements, $y_k = |\mathbf a_k^* \mathbf x|$ for $k = 1,
\dots, m$, is it possible to recover $\mathbf x \in \mathbb{C}^n$ (i.e.,
length-$n$ complex vector)? This **generalized phase retrieval** (GPR) problem
is a fundamental task in various disciplines, and has been the subject of much
recent investigation. Natural nonconvex heuristics often work remarkably well
for GPR in practice, but lack clear theoretical explanations. In this paper, we
take a step towards bridging this gap. We prove that when the measurement
vectors $\mathbf a_k$'s are generic (i.i.d. complex Gaussian) and the number of
measurements is large enough ($m \ge C n \log^3 n$), with high probability, a
natural least-squares formulation for GPR has the following benign geometric
structure: (1) there are no spurious local minimizers, and all global
minimizers are equal to the target signal $\mathbf x$, up to a global phase;
and (2) the objective function has a negative curvature around each saddle
point. This structure allows a number of iterative optimization methods to
efficiently find a global minimizer, without special initialization. To
corroborate the claim, we describe and analyze a second-order trust-region
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06665</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06665</id><created>2016-02-22</created><authors><author><keyname>Mukherjee</keyname><forenames>Sudarshan</forenames></author><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author></authors><title>How Much Bandpass Filtering is Required in Massive MIMO Basestations?</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the impact of aliased adjacent band blocker signal
(AABS) on the information sum-rate of massive multiple-input multiple-output
(MIMO) uplink, with both perfect and imperfect channel estimates, in order to
determine the required adjacent band attenuation in RF bandpass filters (BPFs).
With imperfect channel estimates, our study reveals that as the number of
base-station (BS) antennas ($M$) increases, with $M \to \infty$, the required
attenuation at the BPFs increases as $\mathcal{O}(\sqrt{M})$, provided the
desired information sum-rate (both in the presence and absence of AABS) remains
fixed. This implies a practical limit on the number of BS antennas due to the
increase in BPF design complexity and power consumption with increasing $M$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06667</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06667</id><created>2016-02-22</created><authors><author><keyname>Ramanagopal</keyname><forenames>Manikandasriram Srinivasan</forenames></author><author><keyname>Ny</keyname><forenames>Jerome Le</forenames></author></authors><title>Motion Planning Strategies for Autonomously Mapping 3D Structures</title><categories>cs.RO cs.AI cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a system capable of autonomously mapping the visible part
of a bounded three-dimensional structure using a mobile ground robot equipped
with a depth sensor. We describe motion planning strategies to determine
appropriate successive viewpoints and attempt to fill holes automatically in a
point cloud produced by the sensing and perception layer. We develop a local
motion planner using potential fields to maintain a desired distance from the
structure. The emphasis is on accurately reconstructing a 3D model of a
structure of moderate size rather than mapping large open environments, with
applications for example in architecture, construction and inspection. The
proposed algorithms do not require any initialization in the form of a mesh
model or a bounding box. We compare via simulations the performance of our
policies to the classic frontier based exploration algorithm. We illustrate the
efficacy of our approach for different structure sizes, levels of localization
accuracy and range of the depth sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06675</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06675</id><created>2016-02-22</created><authors><author><keyname>Evestedt</keyname><forenames>Niclas</forenames></author><author><keyname>Ljungqvist</keyname><forenames>Oskar</forenames></author><author><keyname>Axehill</keyname><forenames>Daniel</forenames></author></authors><title>Path tracking and stabilization for a reversing general 2-trailer
  configuration using a cascaded control approach</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a cascaded approach for stabilization and path tracking of a
general 2-trailer vehicle configuration with an off-axle hitching is presented.
A low level Linear Quadratic controller is used for stabilization of the
internal angles while a pure pursuit path tracking controller is used on a
higher level to handle the path tracking. Piecewise linearity is the only
requirement on the control reference which makes the design of reference paths
very general. A Graphical User Interface is designed to make it easy for a user
to design control references for complex manoeuvres given some representation
of the surroundings. The approach is demonstrated with challenging path
following scenarios both in simulation and on a small scale test platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06678</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06678</id><created>2016-02-22</created><updated>2016-02-25</updated><authors><author><keyname>Scavo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Houidi</keyname><forenames>Zied Ben</forenames></author><author><keyname>Traverso</keyname><forenames>Stefano</forenames></author><author><keyname>Teixeira</keyname><forenames>Renata</forenames></author><author><keyname>Mellia</keyname><forenames>Marco</forenames></author></authors><title>WeBrowse: Mining HTTP logs online for network-based content
  recommendation</title><categories>cs.HC cs.SI</categories><comments>13 pages, 10 figures, 4 tables, 1 algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A powerful means to help users discover new content in the overwhelming
amount of information available today is sharing in online communities such as
social networks or crowdsourced platforms. This means comes short in the case
of what we call communities of a place: people who study, live or work at the
same place. Such people often share common interests but either do not know
each other or fail to actively engage in submitting and relaying information.
To counter this effect, we propose passive crowdsourced content discovery, an
approach that leverages the passive observation of web-clicks as an indication
of users' interest in a piece of content. We design, implement, and evaluate
WeBrowse , a passive crowdsourced system which requires no active user
engagement to promote interesting content to users of a community of a place.
Instead, it extracts the URLs users visit from traffic traversing a network
link to identify popular and interesting pieces of information. We first
prototype WeBrowse and evaluate it using both ground-truths and real traces
from a large European Internet Service Provider. Then, we deploy WeBrowse in a
campus of 15,000 users, and in a neighborhood. Evaluation based on our
deployments shows the feasibility of our approach. The majority of WeBrowse's
users welcome the quality of content it promotes. Finally, our analysis of
popular topics across different communities confirms that users in the same
community of a place share common interests, compared to users from different
communities, thus confirming the promise of WeBrowse's approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06683</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06683</id><created>2016-02-22</created><authors><author><keyname>Busnel</keyname><forenames>Yann</forenames></author><author><keyname>Gashi</keyname><forenames>Ilir</forenames></author></authors><title>EDCC 2015 - Fast Abstracts &amp; Student Forum Proceedings</title><categories>cs.DC cs.SE</categories><comments>The Tenth European Dependable Computing Conference - EDCC 2015 -
  Paris, France, September 7-11, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Fast Abstracts are short presentations of work in progress or opinion pieces
and aim to serve as a rapid and flexible mechanism to (i) Report on current
work that may or may not be complete; (ii) Introduce new ideas to the
community; (iii) State positions on controversial issues or open problems.
  On the other hand, the goal of the Student Forum is to encourage students to
attend EDCC and present their work, exchange ideas with researchers and
practitioners, and get early feedback on their research efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06686</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06686</id><created>2016-02-22</created><updated>2016-02-23</updated><authors><author><keyname>Xie</keyname><forenames>An</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoliang</forenames></author><author><keyname>Maier</keyname><forenames>Guido</forenames></author><author><keyname>Lu</keyname><forenames>Sanglu</forenames></author></authors><title>Designing a Disaster-resilient Network with Software Defined Networking</title><categories>cs.NI cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the wide deployment of network facilities and the increasing requirement
of network reliability, the disruptive event like natural disaster, power
outage or malicious attack has become a non-negligible threat to the current
communication network. Such disruptive event can simultaneously destroy all
devices in a specific geographical area and affect many network based
applications for a long time. Hence, it is essential to build
disaster-resilient network for future highly survivable communication services.
In this paper, we consider the problem of designing a highly resilient network
through the technique of SDN (Software Defined Networking). In contrast to the
conventional idea of handling all the failures on the control plane (the
controller), we focus on an integrated design to mitigate disaster risks by
adding some redundant functions on the data plane. Our design consists of a
sub-graph based proactive protection approach on the data plane and a splicing
approach at the controller for effective restoration on the control plane. Such
a systematic design is implemented in the OpenFlow framework through the
Mininet emulator and Nox controller. Numerical results show that our approach
can achieve high robustness with low control overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06687</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06687</id><created>2016-02-22</created><authors><author><keyname>Ackerman</keyname><forenames>Margareta</forenames></author><author><keyname>Adolfsson</keyname><forenames>Andreas</forenames></author><author><keyname>Brownstein</keyname><forenames>Naomi</forenames></author></authors><title>An Effective and Efficient Approach for Clusterability Evaluation</title><categories>cs.LG stat.ML</categories><comments>10 pages, 2 tables, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is an essential data mining tool that aims to discover inherent
cluster structure in data. As such, the study of clusterability, which
evaluates whether data possesses such structure, is an integral part of cluster
analysis. Yet, despite their central role in the theory and application of
clustering, current notions of clusterability fall short in two crucial aspects
that render them impractical; most are computationally infeasible and others
fail to classify the structure of real datasets.
  In this paper, we propose a novel approach to clusterability evaluation that
is both computationally efficient and successfully captures the structure of
real data. Our method applies multimodality tests to the (one-dimensional) set
of pairwise distances based on the original, potentially high-dimensional data.
We present extensive analyses of our approach for both the Dip and Silverman
multimodality tests on real data as well as 17,000 simulations, demonstrating
the success of our approach as the first practical notion of clusterability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06688</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06688</id><created>2016-02-22</created><authors><author><keyname>Takabatake</keyname><forenames>Yoshimasa</forenames></author><author><keyname>Nakashima</keyname><forenames>Kenta</forenames></author><author><keyname>Kuboyama</keyname><forenames>Tetsuji</forenames></author><author><keyname>Tabei</keyname><forenames>Yasuo</forenames></author><author><keyname>Sakamoto</keyname><forenames>Hiroshi</forenames></author></authors><title>siEDM: an efficient string index and search algorithm for edit distance
  with moves</title><categories>cs.DS</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although several self-indexes for highly repetitive text collections exist,
developing an index and search algorithm with editing operations remains a
challenge. Edit distance with moves (EDM) is a string-to-string distance
measure that includes substring moves in addition to ordinal editing operations
to turn one string into another. Although the problem of computing EDM is
intractable, it has a wide range of potential applications, especially in
approximate string retrieval. Despite the importance of computing EDM, there
has been no efficient method for indexing and searching large text collections
based on the EDM measure. We propose the first algorithm, named string index
for edit distance with moves (siEDM), for indexing and searching strings with
EDM. The siEDM algorithm builds an index structure by leveraging the idea
behind the edit sensitive parsing (ESP), an efficient algorithm enabling
approximately computing EDM with guarantees of upper and lower bounds for the
exact EDM. siEDM efficiently prunes the space for searching query strings by
the proposed method, which enables fast query searches with the same guarantee
as ESP. We experimentally tested the ability of siEDM to index and search
strings on benchmark datasets, and we showed siEDM's efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06690</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06690</id><created>2016-02-22</created><updated>2016-03-04</updated><authors><author><keyname>Nasser</keyname><forenames>Rajai</forenames></author></authors><title>Erasure Schemes Using Generalized Polar Codes: Zero-Undetected-Error
  Capacity and Performance Trade-offs</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of generalized polar (GP) codes when they are used
for coding schemes involving erasure. GP codes are a family of codes which
contains, among others, the standard polar codes of Ar{\i}kan and Reed-Muller
codes. We derive a closed formula for the zero-undetected-error capacity
$I_0^{GP}(W)$ of GP codes for a given binary memoryless symmetric (BMS) channel
$W$ under the low complexity successive cancellation decoder with erasure. We
show that for every $R&lt;I_0^{GP}(W)$, there exists a generalized polar code of
blocklength $N$ and of rate at least $R$ where the undetected-error probability
is zero and the erasure probability is less than
$2^{-N^{\frac{1}{2}-\epsilon}}$. On the other hand, for any GP code of rate
$I_0^{GP}(W)&lt;R&lt;I(W)$ and blocklength $N$, the undetected error probability
cannot be made less than $2^{-N^{\frac{1}{2}+\epsilon}}$ unless the erasure
probability is close to $1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06695</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06695</id><created>2016-02-22</created><authors><author><keyname>Liu</keyname><forenames>Yuan</forenames></author></authors><title>Optimal Mode Selection in D2D-Enabled Multi-Base Station Systems</title><categories>cs.IT math.IT</categories><comments>To be published in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider device-to-device (D2D) communication underlaying
uplink cellular networks with multiple base stations (BSs), where each user can
switch between traditional cellular mode (through BS) and D2D mode (by
connecting proximity user), namely mode selection. We impose load balancing
constraints on BSs to efficient resource usage. The joint problem of mode
selection and user association is formulated as a combinatorial problem and
NP-complete. We adopt a graph-based approach to solve the problem globally
optimally in polynomial time. To further reduce complexity, we also propose a
distributed algorithm based on dual method. We show that the proposed
distributed algorithm achieves nearly the same performance as the proposed
optimal graph based algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06697</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06697</id><created>2016-02-22</created><authors><author><keyname>Cao</keyname><forenames>Yue</forenames></author><author><keyname>Long</keyname><forenames>Mingsheng</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author></authors><title>Correlation Hashing Network for Efficient Cross-Modal Retrieval</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the storage and retrieval efficiency, hashing has been widely deployed
to approximate nearest neighbor search for large-scale multimedia retrieval.
Cross-modal hashing, which improves the quality of hash coding by exploiting
the semantic correlation across different modalities, has received increasing
attention recently. For most existing cross-modal hashing methods, an object is
first represented as of vector of hand-crafted or machine-learned features,
followed by another separate quantization step that generates binary codes.
However, suboptimal hash coding may be produced, because the quantization error
is not statistically minimized and the feature representation is not optimally
compatible with the binary coding. In this paper, we propose a novel
Correlation Hashing Network (CHN) architecture for cross-modal hashing, in
which we jointly learn good data representation tailored to hash coding and
formally control the quantization error. The CHN model is a hybrid deep
architecture constituting four key components: (1) an image network with
multiple convolution-pooling layers to extract good image representations, and
a text network with several fully-connected layers to extract good text
representations; (2) a fully-connected hashing layer to generate
modality-specific compact hash codes; (3) a squared cosine loss layer for
capturing both cross-modal correlation and within-modal correlation; and (4) a
new cosine quantization loss for controlling the quality of the binarized hash
codes. Extensive experiments on standard cross-modal retrieval datasets show
the proposed CHN model yields substantial boosts over latest state-of-the-art
hashing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06698</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06698</id><created>2016-02-22</created><authors><author><keyname>Liu</keyname><forenames>Yuan</forenames></author></authors><title>Wireless Information and Power Transfer for Multi-Relay Assisted
  Cooperative Communication</title><categories>cs.IT math.IT</categories><comments>To be published in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider simultaneous wireless information and power
transfer (SWIPT) in multi-relay assisted two-hop relay system, where multiple
relay nodes simultaneously assist the transmission from source to destination
using the concept of distributed space-time coding. Each relay applies power
splitting protocol to coordinate the received signal energy for information
decoding and energy harvesting. The optimization problems of power splitting
ratios at the relays are formulated for both decode-and-forward (DF) and
amplify-and-forward (AF) relaying protocols. Efficient algorithms are proposed
to find the optimal solutions. Simulations verify the effectiveness of the
proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06700</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06700</id><created>2016-02-22</created><authors><author><keyname>Kaptein</keyname><forenames>Maurits</forenames></author><author><keyname>Kruijswijk</keyname><forenames>Jules</forenames></author></authors><title>StreamingBandit: Developing Adaptive Persuasive Systems</title><categories>cs.HC cs.CY</categories><comments>13 pages, 2 figures. Shorter version of this paper is presented as a
  poster during http://persuasive2016.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces StreamingBandit, a (back-end) solution for developing
adaptive and personalized persuasive systems. Creating successful persuasive
applications requires a combination of design, social science, and technology.
StreamingBandit contributes to the required technology by providing a platform
that can be used to adapt persuasive technologies in real-time and at large
scales. We first introduce the design philosophy of StreamingBandit using a
running example and highlight how a large number of adaptive persuasive systems
can be regarded as solutions to (contextual) multi-armed bandit problems: a
type of problem that StreamingBandit was built to address. Subsequently, we
detail several scenarios of the use of StreamingBandit to create adaptive
persuasive systems and detail its future developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06703</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06703</id><created>2016-02-22</created><authors><author><keyname>Jacq</keyname><forenames>Alexis</forenames></author><author><keyname>Johal</keyname><forenames>Wafa</forenames></author><author><keyname>Dillenbourg</keyname><forenames>Pierre</forenames></author><author><keyname>Paiva</keyname><forenames>Ana</forenames></author></authors><title>Cognitive Architecture for Mutual Modelling</title><categories>cs.RO cs.CY</categories><comments>Presented at &quot;2nd Workshop on Cognitive Architectures for Social
  Human-Robot Interaction 2016 (arXiv:1602.01868)</comments><report-no>CogArch4sHRI/2016/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social robotics, robots needs to be able to be understood by humans.
Especially in collaborative tasks where they have to share mutual knowledge.
For instance, in an educative scenario, learners share their knowledge and they
must adapt their behaviour in order to make sure they are understood by others.
Learners display behaviours in order to show their understanding and teachers
adapt in order to make sure that the learners' knowledge is the required one.
This ability requires a model of their own mental states perceived by others:
\textit{&quot;has the human understood that I(robot) need this object for the task
or should I explain it once again ?&quot;} In this paper, we discuss the importance
of a cognitive architecture enabling second-order Mutual Modelling for
Human-Robot Interaction in educative contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06705</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06705</id><created>2016-02-22</created><authors><author><keyname>Dahlgaard</keyname><forenames>S&#xf8;ren</forenames></author></authors><title>On the Hardness of Partially Dynamic Graph Problems and Connections to
  Diameter</title><categories>cs.DS</categories><comments>16 pages, 4 figures, abstract has been truncated to fit arXiv limits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional lower bounds for dynamic graph problems has received a great deal
of attention in recent years. While many results are now known for the
fully-dynamic case and such bounds often imply worst-case bounds for the
partially dynamic setting, it seems much more difficult to prove amortized
bounds for incremental and decremental algorithms. In this paper we consider
partially dynamic versions of three classic problems in graph theory. Based on
popular conjectures we show that:
  -- No algorithm with amortized update time $O(n^{1-\varepsilon})$ exists for
incremental or decremental maximum cardinality bipartite matching. This
significantly improves on the $O(m^{1/2-\varepsilon})$ bound for sparse graphs
of Henzinger et al. [STOC'15] and $O(n^{1/3-\varepsilon})$ bound of Kopelowitz,
Pettie and Porat. Our linear bound also appears more natural. In addition, the
result we present separates the node-addition model from the edge insertion
model, as an algorithm with total update time $O(m\sqrt{n})$ exists for the
former by Bosek et al. [FOCS'14].
  -- No algorithm with amortized update time $O(m^{1-\varepsilon})$ exists for
incremental or decremental maximum flow in directed and weighted sparse graphs.
No such lower bound was known for partially dynamic maximum flow previously.
This also slightly strengthens the $O(n^{1-\varepsilon})$ bound for
fully-dynamic algorithms of Abboud et al. [STOC'15].
  -- No algorithm with amortized update time $O(n^{1/2 - \varepsilon})$ exists
for incremental or decremental $(4/3-\varepsilon')$-approximating the diameter
of an unweighted graph. We also show a slightly stronger bound if node
additions are allowed. [...]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06708</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06708</id><created>2016-02-22</created><authors><author><keyname>Boyar</keyname><forenames>Joan</forenames></author><author><keyname>Epstein</keyname><forenames>Leah</forenames></author><author><keyname>Favrholdt</keyname><forenames>Lene M.</forenames></author><author><keyname>Larsen</keyname><forenames>Kim S.</forenames></author><author><keyname>Levin</keyname><forenames>Asaf</forenames></author></authors><title>Online Bounded Analysis</title><categories>cs.DS</categories><comments>IMADA-preprint-cs</comments><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though competitive analysis is often a very good tool for the analysis of
online algorithms, sometimes it does not give any insight and sometimes it
gives counter-intuitive results. Much work has gone into exploring other
performance measures, in particular targeted at what seems to be the core
problem with competitive analysis: the comparison of the performance of an
online algorithm is made to a too powerful adversary. We consider a new
approach to restricting the power of the adversary, by requiring that when
judging a given online algorithm, the optimal offline algorithm must perform as
well as the online algorithm, not just on the entire final request sequence,
but also on any prefix of that sequence. This is limiting the adversary's usual
advantage of being able to exploit that it knows the sequence is continuing
beyond the current request. Through a collection of online problems, including
machine scheduling, bin packing, dual bin packing, and seat reservation, we
investigate the significance of this particular offline advantage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06709</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06709</id><created>2016-02-22</created><authors><author><keyname>Das</keyname><forenames>Dipankar</forenames></author><author><keyname>Avancha</keyname><forenames>Sasikanth</forenames></author><author><keyname>Mudigere</keyname><forenames>Dheevatsa</forenames></author><author><keyname>Vaidynathan</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Sridharan</keyname><forenames>Srinivas</forenames></author><author><keyname>Kalamkar</keyname><forenames>Dhiraj</forenames></author><author><keyname>Kaul</keyname><forenames>Bharat</forenames></author><author><keyname>Dubey</keyname><forenames>Pradeep</forenames></author></authors><title>Distributed Deep Learning Using Synchronous Stochastic Gradient Descent</title><categories>cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design and implement a distributed multinode synchronous SGD algorithm,
without altering hyper parameters, or compressing data, or altering algorithmic
behavior. We perform a detailed analysis of scaling, and identify optimal
design points for different networks. We demonstrate scaling of CNNs on 100s of
nodes, and present what we believe to be record training throughputs. A 512
minibatch VGG-A CNN training run is scaled 90X on 128 nodes. Also 256 minibatch
VGG-A and OverFeat-FAST networks are scaled 53X and 42X respectively on a 64
node cluster. We also demonstrate the generality of our approach via
best-in-class 6.5X scaling for a 7-layer DNN on 16 nodes. Thereafter we attempt
to democratize deep-learning by training on an Ethernet based AWS cluster and
show ~14X scaling on 16 nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06722</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06722</id><created>2016-02-22</created><authors><author><keyname>Biswas</keyname><forenames>Sinchan</forenames></author><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author></authors><title>Sensing Throughput Optimization in Fading Cognitive Multiple Access
  Channels With Energy Harvesting Secondary Transmitters</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates the problem of maximizing expected sum throughput in a
fading multiple access cognitive radio network when secondary user (SU)
transmitters have energy harvesting capability, and perform cooperative
spectrum sensing. We formulate the problem as maximization of sum-capacity of
the cognitive multiple access network over a finite time horizon subject to a
time averaged interference constraint at the primary user (PU) and almost sure
energy causality constraints at the SUs. The problem is a mixed integer
non-linear program with respect to two decision variables namely spectrum
access decision and spectrum sensing decision, and the continuous variables
sensing time and transmission power. In general, this problem is known to be NP
hard. For optimization over these two decision variables, we use an exhaustive
search policy when the length of the time horizon is small, and a heuristic
policy for longer horizons. For given values of the decision variables, the
problem simplifies into a joint optimization on SU \textit{transmission power}
and \textit{sensing time}, which is non-convex in nature. We solve the
resulting optimization problem as an alternating convex optimization problem
for both non-causal and causal channel state information and harvested energy
information patterns at the SU base station (SBS) or fusion center (FC). We
present an analytic solution for the non-causal scenario with infinite battery
capacity for a general finite horizon problem.We formulate the problem with
causal information and finite battery capacity as a stochastic control problem
and solve it using the technique of dynamic programming. Numerical results are
presented to illustrate the performance of the various algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06725</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06725</id><created>2016-02-22</created><authors><author><keyname>Mnih</keyname><forenames>Andriy</forenames></author><author><keyname>Rezende</keyname><forenames>Danilo J.</forenames></author></authors><title>Variational inference for Monte Carlo objectives</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in deep latent variable models has largely been driven by the
development of flexible and scalable variational inference methods. Variational
training of this type involves maximizing a lower bound on the log-likelihood,
using samples from the variational posterior to compute the required gradients.
Recently, Burda et al. (2015) have derived a tighter lower bound using a
multi-sample importance sampling estimate of the likelihood and showed that
optimizing it yields models that use more of their capacity and achieve higher
likelihoods. This development showed the importance of such multi-sample
objectives and explained the success of several related approaches.
  We extend the multi-sample approach to discrete latent variables and analyze
the difficulty encountered when estimating the gradients involved. We then
develop the first unbiased gradient estimator designed for importance-sampled
objectives and evaluate it at training generative and structured output
prediction models. The resulting estimator, which is based on low-variance
per-sample learning signals, is both simpler and more effective than the NVIL
estimator proposed for the single-sample variational objective, and is
competitive with the currently used biased estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06727</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06727</id><created>2016-02-22</created><authors><author><keyname>Wu</keyname><forenames>Zhizheng</forenames></author><author><keyname>King</keyname><forenames>Simon</forenames></author></authors><title>Improving Trajectory Modelling for DNN-based Speech Synthesis by using
  Stacked Bottleneck Features and Minimum Trajectory Error Training</title><categories>cs.SD cs.CL cs.NE</categories><comments>submitted to IEEE/ACM Transactions on Audio, Speech and Language
  Processing 2016 (Under second round review)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose two novel techniques --- stacking bottleneck features and minimum
trajectory error training criterion --- to improve the performance of deep
neural network (DNN)-based speech synthesis. The techniques address the related
issues of frame-by-frame independence and ignorance of the relationship between
static and dynamic features, within current typical DNN-based synthesis
frameworks. Stacking bottleneck features, which are an acoustically--informed
linguistic representation, provides an efficient way to include more detailed
linguistic context at the input. The proposed minimum trajectory error training
criterion minimises overall output trajectory error across an utterance, rather
than minimising the error per frame independently, and thus takes into account
the interaction between static and dynamic features. The two techniques can be
easily combined to further improve performance. We present both objective and
subjective results that demonstrate the effectiveness of the proposed
techniques. The subjective results show that combining the two techniques leads
to significantly more natural synthetic speech than from conventional DNN or
long short-term memory (LSTM) recurrent neural network (RNN) systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06731</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06731</id><created>2016-02-22</created><authors><author><keyname>Alechina</keyname><forenames>Natasha</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Kash</keyname><forenames>Ian A.</forenames></author><author><keyname>Logan</keyname><forenames>Brian</forenames></author></authors><title>Decentralised Norm Monitoring in Open Multi-Agent Systems</title><categories>cs.MA cs.GT</categories><comments>24 pages, 7 pdf figures</comments><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of detecting norm violations in open multi-agent
systems (MAS). We show how, using ideas from scrip systems, we can design
mechanisms where the agents comprising the MAS are incentivised to monitor the
actions of other agents for norm violations. The cost of providing the
incentives is not borne by the MAS and does not come from fines charged for
norm violations (fines may be impossible to levy in a system where agents are
free to leave and rejoin again under a different identity). Instead, monitoring
incentives come from (scrip) fees for accessing the services provided by the
MAS. In some cases, perfect monitoring (and hence enforcement) can be achieved:
no norms will be violated in equilibrium. In other cases, we show that, while
it is impossible to achieve perfect enforcement, we can get arbitrarily close;
we can make the probability of a norm violation in equilibrium arbitrarily
small. We show using simulations that our theoretical results hold for
multi-agent systems with as few as 1000 agents---the system rapidly converges
to the steady-state distribution of scrip tokens necessary to ensure monitoring
and then remains close to the steady state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06737</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06737</id><created>2016-02-22</created><authors><author><keyname>Bennati</keyname><forenames>Stefano</forenames></author></authors><title>Necessity is the mother of invention. The role of collective sensing in
  group formation</title><categories>q-bio.PE cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, the fields of Sociology and Biology explain how groups could
form and remain together if external factors are present, for example an
environment that favors group behavior, e.g. avoid predation, or between-group
competition. This work investigates the causes of group formation when these
external factors are not present and individuals compete for the same
resources, i.e. within-group competition.
  A motivating example for our research is a recent anthropological study that
found evidence for an increase in social tolerance among Homo Sapiens starting
from the Middle Pleistocene. Social tolerance is a prerequisite for the
creation of large social groups that include unrelated individuals, but at the
same time it becomes evolutionarily successful only if interactions with
unrelated individuals are frequent.
  We argue that lack of information about resource location could have induced
frequent interactions between unrelated individuals, as it would enable
collective sensing and provide an evolutionary advantage. Collective sensing
refers to the ability of a group to sense what is beyond the capabilities of
the individual. Collective sensing is present in nature but its role in group
formation has never been studied.
  We test our hypothesis by means of an agent-based evolutionary model of a
foraging task, with which we compare the fitness individuals using different
navigation strategies: random walk and herding behavior, i.e. moving towards
crowded areas. Although agents are unable to perceive resources at the
individual level, interactions between them allow the group of herding agents
to collectively locate resources.
  Our findings suggest that evolution favors the spontaneous formation of
groups, if resources become scarce and information about their location is not
available at the individual level but can be inferred from the dynamics of the
population.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06746</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06746</id><created>2016-02-22</created><authors><author><keyname>Shcherbatyi</keyname><forenames>Iaroslav</forenames></author><author><keyname>Andres</keyname><forenames>Bjoern</forenames></author></authors><title>Convexification of Learning from Constraints</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regularized empirical risk minimization with constrained labels (in contrast
to fixed labels) is a remarkably general abstraction of learning. For common
loss and regularization functions, this optimization problem assumes the form
of a mixed integer program (MIP) whose objective function is non-convex. In
this form, the problem is resistant to standard optimization techniques. We
construct MIPs with the same solutions whose objective functions are convex.
Specifically, we characterize the tightest convex extension of the objective
function, given by the Legendre-Fenchel biconjugate. Computing values of this
tightest convex extension is NP-hard. However, by applying our characterization
to every function in an additive decomposition of the objective function, we
obtain a class of looser convex extensions that can be computed efficiently.
For some decompositions, common loss and regularization functions, we derive a
closed form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06763</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06763</id><created>2016-02-22</created><authors><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>Recursive Algorithms for Dense Linear Algebra: The ReLAPACK Collection</title><categories>cs.MS cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To exploit both memory locality and the full performance potential of highly
tuned kernels, dense linear algebra libraries such as LAPACK commonly implement
operations as blocked algorithms. However, to achieve next-to-optimal
performance with such algorithms, significant tuning is required. On the other
hand, recursive algorithms are virtually tuning free, and yet attain similar
performance. In this paper, we first analyze and compare blocked and recursive
algorithms in terms of performance, and then introduce ReLAPACK, an open-source
library of recursive algorithms to seamlessly replace most of LAPACK's blocked
algorithms. In many scenarios, ReLAPACK clearly outperforms reference LAPACK,
and even improves upon the performance of optimizes libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06771</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06771</id><created>2016-02-22</created><updated>2016-02-23</updated><authors><author><keyname>Bonchi</keyname><forenames>Filippo</forenames></author><author><keyname>Gadducci</keyname><forenames>Fabio</forenames></author><author><keyname>Kissinger</keyname><forenames>Aleks</forenames></author><author><keyname>Sobocinski</keyname><forenames>Pawel</forenames></author><author><keyname>Zanasi</keyname><forenames>Fabio</forenames></author></authors><title>Rewriting modulo symmetric monoidal structure</title><categories>math.CT cs.LO</categories><comments>preprint. 32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  String diagrams are a powerful and intuitive graphical syntax for terms of
symmetric monoidal categories (SMCs). They find many applications in computer
science and are becoming increasingly relevant in other fields such as physics
and control theory.
  An important role in many such approaches is played by equational theories of
diagrams, typically oriented and applied as rewrite rules. This paper lays a
comprehensive foundation of this form of rewriting. We interpret diagrams
combinatorially as typed hypergraphs and establish the precise correspondence
between diagram rewriting modulo the laws of SMCs on the one hand and double
pushout (DPO) rewriting of hypergraphs, subject to a soundness condition called
convexity, on the other. This result rests on a more general characterisation
theorem in which we show that typed hypergraph DPO rewriting amounts to diagram
rewriting modulo the laws of SMCs with a chosen special Frobenius structure.
  We illustrate our approach with a proof of termination for the theory of
non-commutative bimonoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06778</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06778</id><created>2016-02-22</created><authors><author><keyname>Hubard</keyname><forenames>Alfredo</forenames></author><author><keyname>Kalu&#x17e;a</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>de Mesmay</keyname><forenames>Arnaud</forenames></author><author><keyname>Tancer</keyname><forenames>Martin</forenames></author></authors><title>Shortest path embeddings of graphs on surfaces</title><categories>cs.CG cs.DM math.CO math.GT</categories><acm-class>F.2.2; G.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical theorem of F\'{a}ry states that every planar graph can be
represented by an embedding in which every edge is represented by a straight
line segment. We consider generalizations of F\'{a}ry's theorem to surfaces
equipped with Riemannian metrics. In this setting, we require that every edge
is drawn as a shortest path between its two endpoints and we call an embedding
with this property a shortest path embedding. The main question addressed in
this paper is whether given a closed surface S, there exists a Riemannian
metric for which every topologically embeddable graph admits a shortest path
embedding. This question is also motivated by various problems regarding
crossing numbers on surfaces.
  We observe that the round metrics on the sphere and the projective plane have
this property. We provide flat metrics on the torus and the Klein bottle which
also have this property.
  Then we show that for the unit square flat metric on the Klein bottle there
exists a graph without shortest path embeddings. We show, moreover, that for
large g, there exist graphs G embeddable into the orientable surface of genus
g, such that with large probability a random hyperbolic metric does not admit a
shortest path embedding of G, where the probability measure is proportional to
the Weil-Petersson volume on moduli space.
  Finally, we construct a hyperbolic metric on every orientable surface S of
genus g, such that every graph embeddable into S can be embedded so that every
edge is a concatenation of at most O(g) shortest paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06787</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06787</id><created>2016-02-22</created><authors><author><keyname>Klidbary</keyname><forenames>Sajad Haghzad</forenames><affiliation>IEEE</affiliation></author><author><keyname>Shouraki</keyname><forenames>Saeed Bagheri</forenames></author><author><keyname>Afrakoti</keyname><forenames>Iman Esmaili Pain</forenames></author></authors><title>Fast IDS Computing System Method and its Memristor Crossbar-based
  Hardware Implementation</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active Learning Method (ALM) is one of the powerful tools in soft computing
that is inspired by human brain capabilities in processing complicated
information. ALM, which is in essence an adaptive fuzzy learning method, models
a Multi-Input Single-Output (MISO) system with several Single-Input
Single-Output (SISO) subsystems. Ink Drop Spread (IDS) operator, which is the
main processing engine of this method, extracts useful features from the data
without complicated computations and provides stability and convergence as
well. Despite great performance of ALM in applications such as classification,
clustering, and modelling, an efficient hardware implementation has remained a
challenging problem. Large amount of memory required to store the information
of IDS planes as well as the high computational cost of the IDS computing
system are two main barriers to ALM becoming more popular. In this paper, a
novel learning method is proposed based on the idea of IDS, but with a novel
approach that eliminates the computational cost of IDS operator. Unlike
traditional approaches, our proposed method finds functions to describe the IDS
plane that eliminates the need for large amount of memory to a great extent.
Narrow Path and Spread, which are two main features used in the inference
engine of ALM, are then extracted from IDS planes with minimum amount of memory
usage and power consumption. Our proposed algorithm is fully compatible with
memristor-crossbar implementation that leads to a significant decrease in the
number of required memristors (from O(n^2) to O(3n)). Simpler algorithm and
higher speed make our algorithm suitable for applications where real-time
process, low-cost and small implementation are paramount. Applications in
clustering and function approximation are provided, which reveals the effective
performance of our proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06797</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06797</id><created>2016-02-22</created><authors><author><keyname>Wang</keyname><forenames>Zhiguo</forenames></author><author><keyname>Mi</keyname><forenames>Haitao</forenames></author><author><keyname>Ittycheriah</keyname><forenames>Abraham</forenames></author></authors><title>Semi-supervised Clustering for Short Text via Deep Representation
  Learning</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we propose a semi-supervised method for short text clustering,
where we represent texts as distributed vectors with neural networks, and use a
small amount of labeled data to specify our intention for clustering. We design
a novel objective to combine the representation learning process and the
k-means clustering process together, and optimize the objective with both
labeled data and unlabeled data iteratively until convergence through three
steps: (1) assign each short text to its nearest centroid based on its
representation from the current neural networks; (2) re-estimate the cluster
centroids based on cluster assignments from step (1); (3) update neural
networks according to the objective by keeping centroids and cluster
assignments fixed. Experimental results on four datasets show that our method
works significantly better than several other text clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06807</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06807</id><created>2016-02-19</created><authors><author><keyname>Zheng</keyname><forenames>Ren</forenames></author><author><keyname>Lu</keyname><forenames>Wenlian</forenames></author><author><keyname>Xu</keyname><forenames>Shouhuai</forenames></author></authors><title>Preventive and Reactive Cyber Defense Dynamics Is Globally Stable</title><categories>cs.CR math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed {\em cybersecurity dynamics} approach aims to
understand cybersecurity from a holistic perspective by modeling the evolution
of the global cybersecurity state. These models describe the interactions
between the various kinds of cyber defenses and the various kinds of cyber
attacks. We study a particular kind of cybersecurity dynamics caused by the
interactions between preventive and reactive defenses (e.g., filtering and
malware detection) against push- and pull-based cyber attacks (e.g., malware
spreading and &quot;drive-by download&quot; attacks). The dynamics was previously shown
to be globally stable in a {\em special} regime of the parameter universe, but
little is known beyond this special regime. In this paper, we resolve an open
problem in this domain by proving that the dynamics is globally stable in the
{\em entire} parameter universe (i.e., the dynamics always converges to a
unique equilibrium). We discuss the cybersecurity meanings and implications of
this theoretic result. We also prove that the dynamics converges {\em
exponentially} to the equilibrium except for a special parameter regime, in
which case the dynamics converges {\em polynomially}. Since it is often
difficult to compute the equilibrium, we propose new bounds of the equilibrium
and numerically show that these bounds are tighter than those proposed in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06817</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06817</id><created>2016-02-22</created><authors><author><keyname>Maffray</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Pastor</keyname><forenames>Lucas</forenames></author></authors><title>The maximum weight stable set problem in $(P_6,\mbox{bull})$-free graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial-time algorithm that finds a maximum weight stable set
in a graph that does not contain as an induced subgraph an induced path on six
vertices or a bull (the graph with vertices $a, b, c, d, e$ and edges $ab, bc,
cd, be, ce$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06818</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06818</id><created>2016-02-22</created><updated>2016-03-07</updated><authors><author><keyname>Sun</keyname><forenames>Yubao</forenames></author><author><keyname>Hang</keyname><forenames>Renlong</forenames></author><author><keyname>Liu</keyname><forenames>Qingshan</forenames></author><author><keyname>Zhu</keyname><forenames>Fuping</forenames></author><author><keyname>Pei</keyname><forenames>Hucheng</forenames></author></authors><title>Graph Regularized Low Rank Representation for Aerosol Optical Depth
  Retrieval</title><categories>cs.LG</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel data-driven regression model for aerosol
optical depth (AOD) retrieval. First, we adopt a low rank representation (LRR)
model to learn a powerful representation of the spectral response. Then, graph
regularization is incorporated into the LRR model to capture the local
structure information and the nonlinear property of the remote-sensing data.
Since it is easy to acquire the rich satellite-retrieval results, we use them
as a baseline to construct the graph. Finally, the learned feature
representation is feeded into support vector machine (SVM) to retrieve AOD.
Experiments are conducted on two widely used data sets acquired by different
sensors, and the experimental results show that the proposed method can achieve
superior performance compared to the physical models and other state-of-the-art
empirical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06819</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06819</id><created>2016-02-22</created><authors><author><keyname>Debatty</keyname><forenames>Thibault</forenames></author><author><keyname>Michiardi</keyname><forenames>Pietro</forenames></author><author><keyname>Mees</keyname><forenames>Wim</forenames></author></authors><title>Fast Online k-nn Graph Building</title><categories>cs.DS</categories><comments>Submitted to ACM SIGKDD 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an online approximate k-nn graph building algorithm,
which is able to quickly update a k-nn graph using a flow of data points. One
very important step of the algorithm consists in using the current distributed
graph to search for the neighbors of a new node. Hence we also propose a
distributed partitioning method based on balanced k-medoids clustering, that we
use to optimize the distributed search process. Finally, we present the
improved sequential search procedure that is used inside each partition.
  We also perform an experimental evaluation of the different algorithms, where
we study the influence of the parameters and compare the result of our
algorithms to existing state of the art. This experimental evaluation confirms
that the fast online k-nn graph building algorithm produces a graph that is
highly similar to the graph produced by an offline exhaustive algorithm, while
it requires less similarity computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06820</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06820</id><created>2016-02-22</created><authors><author><keyname>Schoeny</keyname><forenames>Clayton</forenames></author><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author><author><keyname>Gabrys</keyname><forenames>Ryan</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author></authors><title>Codes for Correcting a Burst of Deletions or Insertions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies codes which correct bursts of deletions. Namely, a binary
code will be called a $b$-burst-correcting code if it can correct a deletion of
any $b$ consecutive bits. While the lower bound on the redundancy of such codes
was shown by Levenshtein to be asymptotically $\log(n)+b-1$, the redundancy of
the best code construction by Cheng et al. is $b(\log (n/b+1))$. In this paper,
we close this gap and provide a construction of codes with redundancy at most
$\log(n) + (b-1)\log(\log(n)) +b -\log(b)$. We also derive a non-asymptotic
upper bound on the size of $b$-burst-correcting codes and extend the burst
deletion model to two more cases: 1. a deletion burst of at most $b$
consecutive bits and 2. a deletion burst of size at most $b$ (not necessarily
consecutive). We extend our code construction of $b$-burst-correcting codes for
the first case and study the second case for $b=3,4$. The equivalent models for
insertions are also studied and are shown to be equivalent to correcting the
corresponding burst of deletions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06822</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06822</id><created>2016-02-22</created><authors><author><keyname>Whitney</keyname><forenames>William F.</forenames></author><author><keyname>Chang</keyname><forenames>Michael</forenames></author><author><keyname>Kulkarni</keyname><forenames>Tejas</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Understanding Visual Concepts with Continuation Learning</title><categories>cs.LG</categories><comments>Under review as a workshop paper for ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a neural network architecture and a learning algorithm to
produce factorized symbolic representations. We propose to learn these concepts
by observing consecutive frames, letting all the components of the hidden
representation except a small discrete set (gating units) be predicted from the
previous frame, and let the factors of variation in the next frame be
represented entirely by these discrete gated units (corresponding to symbolic
representations). We demonstrate the efficacy of our approach on datasets of
faces undergoing 3D transformations and Atari 2600 games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06823</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06823</id><created>2016-02-22</created><authors><author><keyname>Tchitchigin</keyname><forenames>Alexander</forenames></author><author><keyname>Safina</keyname><forenames>Larisa</forenames></author><author><keyname>Mazzara</keyname><forenames>Manuel</forenames></author><author><keyname>Elwakil</keyname><forenames>Mohamed</forenames></author><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Rivera</keyname><forenames>Victor</forenames></author></authors><title>Refinement types in Jolie</title><categories>cs.SE cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jolie is the first language for microservices and it is currently dynamically
type checked. This paper considers the opportunity to integrate dynamic and
static type checking with the introduction of refinement types, verified via
SMT solver. The integration of the two aspects allows a scenario where the
static verification of internal services and the dynamic verification of
(potentially malicious) external services cooperates in order to reduce testing
effort and enhancing security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06832</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06832</id><created>2016-02-22</created><authors><author><keyname>Baskin</keyname><forenames>Mehmet</forenames></author><author><keyname>Leblebicioglu</keyname><forenames>Kemal</forenames></author></authors><title>Robust stabilization loop design for gimbaled electro-optical imaging
  system</title><categories>cs.SY</categories><comments>13 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For electro-optical imaging systems, line-of-sight stabilization against
different disturbances created by mobile platforms is crucial property. The
development of high resolution sensors and the demand in increased operating
distances have recently increased the expectations from stabilization loops.
For that reason, higher gains and larger bandwidths become necessary. As the
stabilization loop satisfies these requirements for good disturbance
attenuation, it must also satisfy sufficient loop stability. In gimbaled
imaging systems, the main difficulties in satisfying sufficient loop stability
are structural resonances and model uncertainties. Therefore, satisfying high
stabilization performance in the presence of model uncertainties or modeling
errors requires utilization of robust control methods. In this paper, robust
LQG/LTR controller design is described for a two-axis gimbal. First, the
classical LQG/LTR method is modified such that it becomes very powerful loop
shaping method. Next, using this method, controller is synthesized. Robust
stability and robust performance of stabilization loop is investigated by using
singular value tests. The report is concluded with the experimental validation
of the designed robust controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06844</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06844</id><created>2016-02-22</created><updated>2016-02-25</updated><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Ning</keyname><forenames>Yue</forenames></author><author><keyname>Chakraborty</keyname><forenames>Prithwish</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author><author><keyname>Tatti</keyname><forenames>Nikolaj</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Generating Realistic Synthetic Population Datasets</title><categories>cs.DB</categories><comments>The conference version of the paper is submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern studies of societal phenomena rely on the availability of large
datasets capturing attributes and activities of synthetic, city-level,
populations. For instance, in epidemiology, synthetic population datasets are
necessary to study disease propagation and intervention measures before
implementation. In social science, synthetic population datasets are needed to
understand how policy decisions might affect preferences and behaviors of
individuals. In public health, synthetic population datasets are necessary to
capture diagnostic and procedural characteristics of patient records without
violating confidentialities of individuals. To generate such datasets over a
large set of categorical variables, we propose the use of the maximum entropy
principle to formalize a generative model such that in a statistically
well-founded way we can optimally utilize given prior information about the
data, and are unbiased otherwise. An efficient inference algorithm is designed
to estimate the maximum entropy model, and we demonstrate how our approach is
adept at estimating underlying data distributions. We evaluate this approach
against both simulated data and on US census datasets, and demonstrate its
feasibility using an epidemic simulation application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06847</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06847</id><created>2016-02-22</created><authors><author><keyname>Li</keyname><forenames>Lingxiang</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Fang</keyname><forenames>Jun</forenames></author></authors><title>Improving Wireless Physical Layer Security via Exploiting Co-Channel
  Interference</title><categories>cs.IT math.IT</categories><comments>13 pages and 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a scenario in which a source-destination pair needs to
establish a confidential connection against an external eavesdropper, aided by
the interference generated by another source-destination pair that exchanges
public messages. The goal is to compute the maximum achievable secrecy degrees
of freedom (S.D.o.F) region of a MIMO two-user wiretap network. First, a
cooperative secrecy transmission scheme is proposed, whose feasible set is
shown to achieve all S.D.o.F. pairs on the S.D.o.F. region boundary. In this
way, the determination of the S.D.o.F. region is reduced to a problem of
maximizing the S.D.o.F. pair over the proposed transmission scheme. The maximum
achievable S.D.o.F. region boundary points are obtained in closed form, and the
construction of the precoding matrices achieving the maximum S.D.o.F. region
boundary is provided. The obtained analytical expressions clearly show the
relation between the maximum achievable S.D.o.F. region and the number of
antennas at each terminal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06850</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06850</id><created>2016-02-22</created><authors><author><keyname>Xu</keyname><forenames>Tianhua</forenames></author></authors><title>Differential Carrier Phase Estimation in Optical Fiber Communication
  Systems Based on One-Tap Normalized Least-Mean-Square Algorithm</title><categories>physics.optics cs.IT math.IT</categories><comments>7 pages</comments><msc-class>78A55</msc-class><acm-class>C.2.5</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The theoretical analysis of the one-tap normalized least-mean-square carrier
phase estimation (CPE) is carried out in long-haul high speed coherent optical
fiber communication systems. It is found that the one-tap normalized
least-mean-square equalizer shows a similar performance compared to the
traditional differential detection in the carrier phase recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06863</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06863</id><created>2016-02-22</created><authors><author><keyname>Rabusseau</keyname><forenames>Guillaume</forenames></author><author><keyname>Kadri</keyname><forenames>Hachem</forenames></author></authors><title>Higher-Order Low-Rank Regression</title><categories>cs.LG</categories><comments>submitted to ICML 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an efficient algorithm (HOLRR) to handle regression tasks
where the outputs have a tensor structure. We formulate the regression problem
as the minimization of a least square criterion under a multilinear rank
constraint, a difficult non convex problem. HOLRR computes efficiently an
approximate solution of this problem, with solid theoretical guarantees. A
kernel extension is also presented. Experiments on synthetic and real data show
that HOLRR outperforms multivariate and multilinear regression methods and is
considerably faster than existing tensor methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06865</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06865</id><created>2016-02-22</created><authors><author><keyname>Deligkas</keyname><forenames>Argyrios</forenames></author><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Igwe</keyname><forenames>Tobenna Peter</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author></authors><title>An Empirical Study on Computing Equilibria in Polymatrix Games</title><categories>cs.GT</categories><comments>To appear at AAMAS 2016 (without the appendix)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Nash equilibrium is an important benchmark for behaviour in systems of
strategic autonomous agents. Polymatrix games are a succinct and expressive
representation of multiplayer games that model pairwise interactions between
players. The empirical performance of algorithms to solve these games has
received little attention, despite their wide-ranging applications. In this
paper we carry out a comprehensive empirical study of two prominent algorithms
for computing a sample equilibrium in these games, Lemke's algorithm that
computes an exact equilibrium, and a gradient descent method that computes an
approximate equilibrium. Our study covers games arising from a number of
interesting applications. We find that Lemke's algorithm can compute exact
equilibria in relatively large games in a reasonable amount of time. If we are
willing to accept (high-quality) approximate equilibria, then we can deal with
much larger games using the descent method. We also report on which games are
most challenging for each of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06866</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06866</id><created>2016-02-22</created><updated>2016-03-08</updated><authors><author><keyname>Shao</keyname><forenames>Huijuan</forenames></author><author><keyname>Hossain</keyname><forenames>K. S. M. Tozammel</forenames></author><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Khan</keyname><forenames>Maleq</forenames></author><author><keyname>Vullikanti</keyname><forenames>Anil</forenames></author><author><keyname>Prakash</keyname><forenames>B. Aditya</forenames></author><author><keyname>Marathe</keyname><forenames>Madhav</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Forecasting the Flu: Designing Social Network Sensors for Epidemics</title><categories>cs.SI physics.soc-ph</categories><comments>The conference version of the paper is submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early detection and modeling of a contagious epidemic can provide important
guidance about quelling the contagion, controlling its spread, or the effective
design of countermeasures. A topic of recent interest has been to design social
network sensors, i.e., identifying a small set of people who can be monitored
to provide insight into the emergence of an epidemic in a larger population. We
formally pose the problem of designing social network sensors for flu epidemics
and identify two different objectives that could be targeted in such sensor
design problems. Using the graph theoretic notion of dominators we develop an
efficient and effective heuristic for forecasting epidemics at lead time. Using
six city-scale datasets generated by extensive microscopic epidemiological
simulations involving millions of individuals, we illustrate the practical
applicability of our methods and show significant benefits (up to twenty-two
days more lead time) compared to other competitors. Most importantly, we
demonstrate the use of surrogates or proxies for policy makers for designing
social network sensors that require from nonintrusive knowledge of people to
more information on the relationship among people. The results show that the
more intrusive information we obtain, the longer lead time to predict the flu
outbreak up to nine days.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06867</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06867</id><created>2016-02-19</created><updated>2016-03-07</updated><authors><author><keyname>Barr&#xf3;n-Romero</keyname><forenames>Carlos</forenames></author></authors><title>Lower bound for the Complexity of the Boolean Satisfiability Problem</title><categories>cs.CC</categories><comments>arXiv admin note: text overlap with arXiv:1510.02682</comments><msc-class>68Q10, 68Q12, 68Q19, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper depicts algorithms for solving the decision Boolean Satisfiability
Problem. An extreme problem is formulated to analyze the complexity of
algorithms and the complexity for solving it. A novel and easy reformulation as
a lottery for an extreme case is presented to determine a stable complexity
around $2^n$. The reformulation point out that the decision Boolean
Satisfiability Problem can only be solved in exponential time. This implies
there is not an efficient algorithm for the NP Class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06871</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06871</id><created>2016-02-19</created><authors><author><keyname>Sari</keyname><forenames>Intan Okta</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Wardhani</keyname><forenames>Kiky Rizky Nova</forenames></author></authors><title>Application Location Based Service (LBS) Location Search Palembang
  Nature-Based Android</title><categories>cs.CY</categories><comments>The 5th International Conference on Information Technology and
  Engineering Application (ICIBA2016), Paper presented at the The 5th
  ICIBA2016, Bina Darma University, Palembang</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of information systems to make the operating system more
diverse mobile devices, the emergence of the Android operating system that is
open allows users to search for and acquire various information easily and
quickly. Application Search Nature Places is an application that can help bring
information on nearby Places Nature is all around. Can be used in the Android
Operating System and Global Positioning System (GPS). To be able to use this
application, users must be connected to the Internet because it requires data
taken from Google Maps. The main facilities contained in this application is a
feature that makes it Map and Route users in finding the intended location.
With the LBS application is expected to provide information that is accurate,
clear and precise to determine location points Nature Palembang, and can
facilitate local and foreign tourists and the public, especially the city of
Palembang.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06872</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06872</id><created>2016-02-22</created><authors><author><keyname>Frostig</keyname><forenames>Roy</forenames></author><author><keyname>Musco</keyname><forenames>Cameron</forenames></author><author><keyname>Musco</keyname><forenames>Christopher</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>Principal Component Projection Without Principal Component Analysis</title><categories>cs.DS cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to efficiently project a vector onto the top principal components
of a matrix, without explicitly computing these components. Specifically, we
introduce an iterative algorithm that provably computes the projection using
few calls to any black-box routine for ridge regression.
  By avoiding explicit principal component analysis (PCA), our algorithm is the
first with no runtime dependence on the number of top principal components. We
show that it can be used to give a fast iterative method for the popular
principal component regression problem, giving the first major runtime
improvement over the naive method of combining PCA with regression.
  To achieve our results, we first observe that ridge regression can be used to
obtain a &quot;smooth projection&quot; onto the top principal components. We then sharpen
this approximation to true projection using a low-degree polynomial
approximation to the matrix step function. Step function approximation is a
topic of long-term interest in scientific computing. We extend prior theory by
constructing polynomials with simple iterative structure and rigorously
analyzing their behavior under limited precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06885</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06885</id><created>2016-02-22</created><updated>2016-02-27</updated><authors><author><keyname>Ollier</keyname><forenames>Virginie</forenames></author><author><keyname>Korso</keyname><forenames>Mohammed Nabil El</forenames></author><author><keyname>Boyer</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Larzabal</keyname><forenames>Pascal</forenames></author><author><keyname>Pesavento</keyname><forenames>Marius</forenames></author></authors><title>Joint ML calibration and DOA estimation with separated arrays</title><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates parametric direction-of-arrival (DOA) estimation in a
particular context: i) each sensor is characterized by an unknown complex gain
and ii) the array consists of a collection of subarrays which are substantially
separated from each other leading to a structured noise covariance matrix. We
propose two iterative algorithms based on the maximum likelihood (ML)
estimation method adapted to the context of joint array calibration and DOA
estimation. Numerical simulations reveal that the two proposed schemes, the
iterativeML (IML) and the modified iterativeML (MIML) algorithms for joint
array calibration and DOA estimation, outperform the state of the art methods
and the MIML algorithm reaches the Cram\'er-Rao bound for a low number of
iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06886</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06886</id><created>2016-02-22</created><authors><author><keyname>Srivastava</keyname><forenames>Akash</forenames></author><author><keyname>Zou</keyname><forenames>James</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>Clustering with a Reject Option: Interactive Clustering as Bayesian
  Prior Elicitation</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A good clustering can help a data analyst to explore and understand a data
set, but what constitutes a good clustering may depend on domain-specific and
application-specific criteria. These criteria can be difficult to formalize,
even when it is easy for an analyst to know a good clustering when they see
one. We present a new approach to interactive clustering for data exploration
called TINDER, based on a particularly simple feedback mechanism, in which an
analyst can reject a given clustering and request a new one, which is chosen to
be different from the previous clustering while fitting the data well. We
formalize this interaction in a Bayesian framework as a method for prior
elicitation, in which each different clustering is produced by a prior
distribution that is modified to discourage previously rejected clusterings. We
show that TINDER successfully produces a diverse set of clusterings, each of
equivalent quality, that are much more diverse than would be obtained by
randomized restarts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06888</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06888</id><created>2016-02-22</created><authors><author><keyname>Patterson</keyname><forenames>Maria T</forenames></author><author><keyname>Anderson</keyname><forenames>Nikolas</forenames></author><author><keyname>Bennett</keyname><forenames>Collin</forenames></author><author><keyname>Bruggemann</keyname><forenames>Jacob</forenames></author><author><keyname>Grossman</keyname><forenames>Robert</forenames></author><author><keyname>Handy</keyname><forenames>Matthew</forenames></author><author><keyname>Ly</keyname><forenames>Vuong</forenames></author><author><keyname>Mandl</keyname><forenames>Dan</forenames></author><author><keyname>Pederson</keyname><forenames>Shane</forenames></author><author><keyname>Pivarski</keyname><forenames>Jim</forenames></author><author><keyname>Powell</keyname><forenames>Ray</forenames></author><author><keyname>Spring</keyname><forenames>Jonathan</forenames></author><author><keyname>Wells</keyname><forenames>Walt</forenames></author></authors><title>The Matsu Wheel: A Cloud-based Framework for Efficient Analysis and
  Reanalysis of Earth Satellite Imagery</title><categories>cs.DC astro-ph.IM</categories><comments>10 pages, accepted for presentation to IEEE BigDataService 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Project Matsu is a collaboration between the Open Commons Consortium and NASA
focused on developing open source technology for the cloud-based processing of
Earth satellite imagery. A particular focus is the development of applications
for detecting fires and floods to help support natural disaster detection and
relief. Project Matsu has developed an open source cloud-based infrastructure
to process, analyze, and reanalyze large collections of hyperspectral satellite
image data using OpenStack, Hadoop, MapReduce, Storm and related technologies.
  We describe a framework for efficient analysis of large amounts of data
called the Matsu &quot;Wheel.&quot; The Matsu Wheel is currently used to process incoming
hyperspectral satellite data produced daily by NASA's Earth Observing-1 (EO-1)
satellite. The framework is designed to be able to support scanning queries
using cloud computing applications, such as Hadoop and Accumulo. A scanning
query processes all, or most of the data, in a database or data repository.
  We also describe our preliminary Wheel analytics, including an anomaly
detector for rare spectral signatures or thermal anomalies in hyperspectral
data and a land cover classifier that can be used for water and flood
detection. Each of these analytics can generate visual reports accessible via
the web for the public and interested decision makers. The resultant products
of the analytics are also made accessible through an Open Geospatial Compliant
(OGC)-compliant Web Map Service (WMS) for further distribution. The Matsu Wheel
allows many shared data services to be performed together to efficiently use
resources for processing hyperspectral satellite image data and other, e.g.,
large environmental datasets that may be analyzed for many purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06889</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06889</id><created>2016-02-22</created><authors><author><keyname>Lu</keyname><forenames>Gongyuan</forenames></author><author><keyname>Zhou</keyname><forenames>Xuesong</forenames></author><author><keyname>Peng</keyname><forenames>Qiyuan</forenames></author><author><keyname>He</keyname><forenames>Bisheng</forenames></author><author><keyname>Mahmoudi</keyname><forenames>Monirehalsadat</forenames></author><author><keyname>Zhao</keyname><forenames>Jun</forenames></author></authors><title>Solving Resource Recharging Station Location-routing Problem through a
  Resource-space-time Network Representation</title><categories>math.OC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The resource recharging station location routing problem is a generalization
of the location routing problem with sophisticated and critical resource
consumption and recharging constraints. Based on a representation of
discretized acyclic resource-space-time networks, we propose a generic
formulation to optimize dynamic infrastructure location and routes decisions.
The proposed integer linear programming formulation could greatly simplify the
modeling representation of time window, resource change, and sub-tour
constraints through a well-structured multi-dimensional network. An
approximation solution framework based on the Lagrangian relaxation is
developed to decompose the problem to a knapsack sub-problem for selecting
recharging stations and a vehicle routing sub-problem in a space-time network.
Both sub-problems can be solved through dynamic programming algorithms to
obtain optimal solution. A number of experiments are used to demonstrate the
Lagrangian multiplier adjustment-based location routing decision making, as
well as the effectiveness of the developed algorithm in large-scale networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06894</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06894</id><created>2016-02-22</created><authors><author><keyname>Padrol</keyname><forenames>Arnau</forenames></author></authors><title>Extension complexity of polytopes with few vertices or facets</title><categories>math.CO cs.DM math.MG</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the extension complexity of polytopes with few vertices or facets.
On the one hand, we provide a complete classification of $d$-polytopes with at
most $d+4$ vertices according to their extension complexity: Out of the
super-exponentially many $d$-polytopes with $d+4$ vertices, all have extension
complexity $d+4$ except for some families of size $\theta(d^2)$. On the other
hand, we show that generic realizations of simplicial/simple $d$-polytopes with
$d+1+\alpha$ vertices/facets have extension complexity at least $2
\sqrt{d(d+\alpha)} -d + 1$, which shows that for all $d&gt;(\frac{\alpha-1}{2})^2$
there are $d$-polytopes with $d+1+\alpha$ vertices or facets and extension
complexity $d+1+\alpha$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06897</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06897</id><created>2016-02-22</created><authors><author><keyname>Cabalar</keyname><forenames>Pedro</forenames></author><author><keyname>Fandinno</keyname><forenames>Jorge</forenames></author></authors><title>Enablers and Inhibitors in Causal Justifications of Logic Programs</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To appear in Theory and Practice of Logic Programming (TPLP). In this paper
we propose an extension of logic programming (LP) where each default literal
derived from the well-founded model is associated to a justification
represented as an algebraic expression. This expression contains both causal
explanations (in the form of proof graphs built with rule labels) and terms
under the scope of negation that stand for conditions that enable or disable
the application of causal rules. Using some examples, we discuss how these new
conditions, we respectively call &quot;enablers&quot; and &quot;inhibitors&quot;, are intimately
related to default negation and have an essentially different nature from
regular cause-effect relations. The most important result is a formal
comparison to the recent algebraic approaches for justifications in LP:
&quot;Why-not Provenance&quot; (WnP) and &quot;Causal Graphs&quot; (CG). We show that the current
approach extends both WnP and CG justifications under the Well-Founded
Semantics and, as a byproduct, we also establish a formal relation between
these two approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06902</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06902</id><created>2016-02-22</created><authors><author><keyname>Vellambi</keyname><forenames>Badri N</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu</forenames></author></authors><title>Lossy Compression with Near-uniform Encoder Outputs</title><categories>cs.IT math.IT</categories><comments>Submitted to the 2016 IEEE International Symposium on Information
  Theory (11 Pages, 3 Figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that lossless compression of a discrete memoryless source at
a rate just above entropy with near-uniform encoder output is possible if and
only if the encoder and decoder share a common random seed. This work focuses
on deriving conditions for near-uniform lossy compression in the Wyner-Ziv and
the distributed lossy compression problems. We show that in the Wyner-Ziv case,
near-uniform encoder output and operation close to the WZ-rate limit is
simultaneously possible, while in the distributed lossy compression problem,
jointly near-uniform outputs is achievable at any rate point in the interior of
the rate region provided the sources share non-trivial common randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06904</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06904</id><created>2016-02-18</created><authors><author><keyname>Lal</keyname><forenames>Amit</forenames></author><author><keyname>Shan</keyname><forenames>Chunyan</forenames></author><author><keyname>Xi</keyname><forenames>Peng</forenames></author></authors><title>Structured illumination microscopy image reconstruction algorithm</title><categories>cs.CV</categories><comments>OpenSIM code may be downloaded from:
  https://github.com/LanMai/OpenSIM</comments><doi>10.1109/JSTQE.2016.2521542</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured illumination microscopy (SIM) is a very important super-resolution
microscopy technique, which provides high speed super-resolution with about
two-fold spatial resolution enhancement. Several attempts aimed at improving
the performance of SIM reconstruction algorithm have been reported. However,
most of these highlight only one specific aspect of the SIM reconstruction --
such as the determination of the illumination pattern phase shift accurately --
whereas other key elements -- such as determination of modulation factor,
estimation of object power spectrum, Wiener filtering frequency components with
inclusion of object power spectrum information, translocating and the merging
of the overlapping frequency components -- are usually glossed over
superficially. In addition, most of the work reported lie scattered throughout
the literature and a comprehensive review of the theoretical background is
found lacking. The purpose of the present work is two-fold: 1) to collect the
essential theoretical details of SIM algorithm at one place, thereby making
them readily accessible to readers for the first time; and 2) to provide an
open source SIM reconstruction code (named OpenSIM), which enables users to
interactively vary the code parameters and study it's effect on reconstructed
SIM image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06910</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06910</id><created>2016-02-19</created><authors><author><keyname>Sauerbier</keyname><forenames>Charles</forenames></author></authors><title>Prime Factoring and The Complexity Of</title><categories>cs.DM</categories><comments>6 pages</comments><msc-class>11Y05, 11Y16, 65Q10, 65Y20, 68Q17, 03D17</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A difference equation based method of determining two factors of a composite
is presented. The feasibility of P-complexity is shown. Presentation of
material is non-theoretical; intended to be accessible to a broader audience of
non academic and theoretical practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06913</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06913</id><created>2016-02-22</created><authors><author><keyname>Zhou</keyname><forenames>Fuhui</forenames></author><author><keyname>Li</keyname><forenames>Zan</forenames></author><author><keyname>Cheng</keyname><forenames>Julian</forenames></author><author><keyname>Li</keyname><forenames>Qunwei</forenames></author><author><keyname>Si</keyname><forenames>Jiangbo</forenames></author></authors><title>Robust AN-Aided Beamforming and Power Splitting Design for Secure MISO
  Cognitive Radio With SWIPT</title><categories>cs.IT math.IT</categories><comments>30 pages, 8 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiple-input single-output cognitive radio downlink network is studied
with simultaneous wireless information and power transfer. In this network, a
secondary user coexists with multiple primary users and multiple energy
harvesting receivers. In order to guarantee secure communication and energy
harvesting, the problem of robust secure artificial noise-aided beamforming and
power splitting design is investigated under imperfect channel state
information (CSI). Specifically, the transmit power minimization problem and
the max-min fairness energy harvesting problem are formulated for both the
bounded CSI error model and the probabilistic CSI error model. These problems
are non-convex and challenging to solve. A one-dimensional search algorithm is
proposed to solve these problems based on ${\cal S}\text{-Procedure} $ under
the bounded CSI error model and based on Bernstein-type inequalities under the
probabilistic CSI error model. It is shown that the optimal robust secure
beamforming can be achieved under the bounded CSI error model, whereas a
suboptimal beamforming solution can be obtained under the probabilistic CSI
error model. A tradeoff is elucidated between the secrecy rate of the secondary
user receiver and the energy harvested by the energy harvesting receivers under
a max-min fairness criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06915</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06915</id><created>2016-02-22</created><authors><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author><author><keyname>Smith</keyname><forenames>Taylor J.</forenames></author></authors><title>Periodicity in Rectangular Arrays</title><categories>cs.DM math.CO</categories><msc-class>68R15 (Primary), 68W32, 68W40, 05A15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss two-dimensional generalizations of the familiar
Lyndon-Sch\&quot;utzenberger theorem for words. We define the notion of primitive
array (as one that cannot be expressed as the repetition of smaller arrays). We
count the number of m x n arrays that are primitive. We show that one can test
primitivity and compute the primitive root in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06916</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06916</id><created>2016-02-22</created><authors><author><keyname>Hashemi</keyname><forenames>Abolfazl</forenames></author><author><keyname>Vikalo</keyname><forenames>Haris</forenames></author></authors><title>Sparse Linear Regression via Generalized Orthogonal Least-Squares</title><categories>stat.ML cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse linear regression, which entails finding a sparse solution to an
underdetermined system of linear equations, can formally be expressed as an
$l_0$-constrained least-squares problem. The Orthogonal Least-Squares (OLS)
algorithm sequentially selects the features (i.e., columns of the coefficient
matrix) to greedily find an approximate sparse solution. In this paper, a
generalization of Orthogonal Least-Squares which relies on a recursive relation
between the components of the optimal solution to select L features at each
step and solve the resulting overdetermined system of equations is proposed.
Simulation results demonstrate that the generalized OLS algorithm is
computationally efficient and achieves performance superior to that of existing
greedy algorithms broadly used in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06920</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06920</id><created>2016-02-22</created><updated>2016-03-04</updated><authors><author><keyname>Cura</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Perret</keyname><forenames>Julien</forenames></author><author><keyname>Paparoditis</keyname><forenames>Nicolas</forenames></author></authors><title>Implicit LOD for processing, visualisation and classification in Point
  Cloud Servers</title><categories>cs.CG cs.CV cs.SE</categories><comments>to be submitted, 18 pages, 24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new paradigm to effortlessly get a portable geometric Level Of
Details (LOD) for a point cloud inside a Point Cloud Server. The point cloud is
divided into groups of points (patch), then each patch is reordered (MidOc
ordering) so that reading points following this order provides more and more
details on the patch. This LOD have then multiple applications: point cloud
size reduction for visualisation (point cloud streaming) or speeding of slow
algorithm, fast density peak detection and correction as well as safeguard for
methods that may be sensible to density variations. The LOD method also embeds
information about the sensed object geometric nature, and thus can be used as a
crude multi-scale dimensionality descriptor, enabling fast classification and
on-the-fly filtering for basic classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06922</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06922</id><created>2016-02-22</created><authors><author><keyname>Kennedy</keyname><forenames>Christopher</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>Fast Cross-Polytope Locality-Sensitive Hashing</title><categories>cs.DS cs.CG</categories><comments>13 pages, 3 figures</comments><msc-class>68Wxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a variant of cross-polytope locality sensitive hashing with
respect to angular distance which is both optimal in asymptotic sensitivity and
provably fast. Precisely, we substitute the random rotation in the standard
cross-polytope scheme for a fast Johnson-Lindenstrauss transform followed by
lifted rotation to reduce the number of hash computations from
$\mathcal{O}(d^2)$ to $\mathcal{O}(d \ln d)$. This builds on a recent result in
(Andoni, Indyk, Laarhoven, Razenshteyn, Schmidt, 2015) by providing an LSH
scheme for angular distance which is not only optimal in asymptotic
sensitivity, but also fast. Finally, we present a discretized version of the
scheme which reduces the number of random bits to $\mathcal{O}(d)$ while still
retaining asymptotic optimality and efficient hash computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06924</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06924</id><created>2016-02-07</created><authors><author><keyname>Costa</keyname><forenames>Regivaldo</forenames></author><author><keyname>Ramos</keyname><forenames>Fernando M. V.</forenames></author></authors><title>An SDN-based approach to enhance BGP security</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  BGP is vulnerable to a series of attacks. Many solutions have been proposed
in the past two decades, but the most effective remain largely undeployed. This
is due to three fundamental reasons: the solutions are too computationally
expensive for current routers, they require changes to BGP, and/or they do not
give the right incentives to promote deployment.
  In this abstract we propose a Software-Defined Networking (SDN) architecture
to secure BGP routing. Our solution, BGPSecX, targets an IXP and it includes
techniques to allow different IXPs to collaborate. With SDN we remove the
computational burden from routers and do not make changes to BGP. Targeting
IXPs and promoting inter-IXP collaboration enables the creation of incentives
to foster adoption of BGP security services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06925</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06925</id><created>2016-02-22</created><authors><author><keyname>Ford</keyname><forenames>Russell</forenames></author><author><keyname>Zhang</keyname><forenames>Menglei</forenames></author><author><keyname>Mezzavilla</keyname><forenames>Marco</forenames></author><author><keyname>Dutta</keyname><forenames>Sourjya</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Achieving Ultra-Low Latency in 5G Millimeter Wave Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IMT 2020 requirements of 20 Gbps peak data rate and 1 millisecond latency
present significant engineering challenges for the design of 5G cellular
systems. Use of the millimeter wave (mmWave) bands above 10 GHz --- where vast
quantities of spectrum are available --- is a promising 5G candidate that may
be able to rise to the occasion.
  However, while the mmWave bands can support massive peak data rates,
delivering these data rates on end-to-end service while maintaining reliability
and ultra-low latency performance will require rethinking all layers of the
protocol stack. This papers surveys some of the challenges and possible
solutions for delivering end-to-end, reliable, ultra-low latency services in
mmWave cellular systems in terms of the Medium Access Control (MAC) layer,
congestion control and core network architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06929</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06929</id><created>2016-02-22</created><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Jin</keyname><forenames>Chi</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Netrapalli</keyname><forenames>Praneeth</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>Matching Matrix Bernstein with Little Memory: Near-Optimal Finite Sample
  Guarantees for Oja's Algorithm</title><categories>cs.LG cs.DS cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work provides improved guarantees for streaming principle component
analysis (PCA). Given $A_1, \ldots, A_n\in \mathbb{R}^{d\times d}$ sampled
independently from distributions satisfying $\mathbb{E}[A_i] = \Sigma$ for
$\Sigma \succeq \mathbf{0}$, this work provides an $O(d)$-space linear-time
single-pass streaming algorithm for estimating the top eigenvector of $\Sigma$.
The algorithm nearly matches (and in certain cases improves upon) the accuracy
obtained by the standard batch method that computes top eigenvector of the
empirical covariance $\frac{1}{n} \sum_{i \in [n]} A_i$ as analyzed by the
matrix Bernstein inequality. Moreover, to achieve constant accuracy, our
algorithm improves upon the best previous known sample complexities of
streaming algorithms by either a multiplicative factor of $O(d)$ or
$1/\mathrm{gap}$ where $\mathrm{gap}$ is the relative distance between the top
two eigenvalues of $\Sigma$.
  These results are achieved through a novel analysis of the classic Oja's
algorithm, one of the oldest and most popular algorithms for streaming PCA. In
particular, this work shows that simply picking a random initial point $w_0$
and applying the update rule $w_{i + 1} = w_i + \eta_i A_i w_i$ suffices to
accurately estimate the top eigenvector, with a suitable choice of $\eta_i$. We
believe our result sheds light on how to efficiently perform streaming PCA both
in theory and in practice and we hope that our analysis may serve as the basis
for analyzing many variants and extensions of streaming PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06932</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06932</id><created>2016-02-22</created><authors><author><keyname>Ford</keyname><forenames>Russell</forenames></author><author><keyname>Zhang</keyname><forenames>Menglei</forenames></author><author><keyname>Dutta</keyname><forenames>Sourjya</forenames></author><author><keyname>Mezzavilla</keyname><forenames>Marco</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>A Framework for Cross-Layer Evaluation of 5G mmWave Cellular Networks in
  ns-3</title><categories>cs.NI</categories><acm-class>I.6.5; I.6.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing demand for ubiquitous mobile data services along with the
scarcity of spectrum in the sub-6 GHz bands has given rise to the recent
interest in developing wireless systems that can exploit the large amount of
spectrum available in the millimeter wave (mmWave) frequency range. Due to its
potential for multi-gigabit and ultra-low latency links, mmWave technology is
expected to play a central role in 5th Generation (5G) cellular networks.
Overcoming the poor radio propagation and sensitivity to blockages at higher
frequencies presents major challenges, which is why much of the current
research is focused at the physical layer. However, innovations will be
required at all layers of the protocol stack to effectively utilize the large
air link capacity and provide the end-to-end performance required by future
networks.
  Discrete-event network simulation is a useful tool for researchers and will
no doubt be invaluable for evaluating novel 5G protocols and systems from an
end-to-end perspective. In this work, we present the first-of-its-kind,
open-source framework for modeling mmWave cellular networks in the ns-3
simulator. Channel models are provided along with a configurable physical and
MAC-layer implementation, which can be interfaced with the higher-layer
protocols and core network model from the ns-3 LTE module for simulating
end-to-end connectivity. The framework is demonstrated through several example
simulations showing the performance of our custom mmWave stack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06935</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06935</id><created>2016-02-22</created><updated>2016-02-23</updated><authors><author><keyname>Carro</keyname><forenames>Adri&#xe1;n</forenames></author><author><keyname>Toral</keyname><forenames>Ra&#xfa;l</forenames></author><author><keyname>Miguel</keyname><forenames>Maxi San</forenames></author></authors><title>The noisy voter model on complex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-fin.GN</categories><comments>28 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new analytical method to study stochastic, binary-state models
on complex networks. Moving beyond the usual mean-field theories, this
alternative approach is based on the introduction of an uncorrelated network
approximation, allowing to deal with the network structure as parametric
heterogeneity. As an illustration, we study the noisy voter model, a
modification of the original voter model including random changes of state. The
proposed method is able to unfold the dependence of the model not only on the
mean degree (the mean-field prediction) but also on more complex averages over
the degree distribution. In particular, we find that the degree heterogeneity
---variance of the underlying degree distribution--- has a strong influence on
the location of the critical point of a noise-induced, finite-size transition
occurring in the model, on the local ordering of the system, and on the
functional form of its temporal correlations. Finally, we show how this latter
point opens the possibility of inferring the degree heterogeneity of the
underlying network by observing only the aggregate behavior of the system as a
whole, an issue of interest for systems where only macroscopic, population
level variables can be measured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06940</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06940</id><created>2016-02-22</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Bouveret</keyname><forenames>Sylvain</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Mackenzie</keyname><forenames>Simon</forenames></author></authors><title>Complexity of Manipulating Sequential Allocation</title><categories>cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential allocation is a simple allocation mechanism in which agents are
given pre-specified turns and each agents gets the most preferred item that is
still available. It has long been known that sequential allocation is not
strategyproof.
  Bouveret and Lang (2014) presented a polynomial-time algorithm to compute a
best response of an agent with respect to additively separable utilities and
claimed that (1) their algorithm correctly finds a best response, and (2) each
best response results in the same allocation for the manipulator. We show that
both claims are false via an example. We then show that in fact the problem of
computing a best response is NP-complete. On the other hand, the insights and
results of Bouveret and Lang (2014) for the case of two agents still hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06963</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06963</id><created>2016-02-22</created><authors><author><keyname>Farris</keyname><forenames>Amy Voss</forenames></author><author><keyname>Dickes</keyname><forenames>Amanda Catherine</forenames></author><author><keyname>Sengupta</keyname><forenames>Pratim</forenames></author></authors><title>Development of Disciplined Interpretation Using Computational Modeling
  in the Elementary Science Classroom</title><categories>physics.ed-ph cs.CY</categories><comments>in Proceedings of the 12th International Conference of the Learning
  Sciences (ICLS 2016)</comments><acm-class>K.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies of scientists building models show that the development of scientific
models involves a great deal of subjectivity. However, science as experienced
in school settings typically emphasizes an overly objective and rationalistic
view. In this paper, we argue for focusing on the development of disciplined
interpretation as an epistemic and representational practice that progressively
deepens students' computational modeling in science by valuing, rather than
deemphasizing, the subjective nature of the experience of modeling. We report
results from a study in which fourth grade children engaged in computational
modeling throughout the academic year. We present three salient themes that
characterize the development of students' disciplined interpretations in terms
of their development of computational modeling as a way of seeing and doing
science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06967</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06967</id><created>2016-02-22</created><authors><author><keyname>Doroshin</keyname><forenames>Danila</forenames></author><author><keyname>Lubimov</keyname><forenames>Nikolay</forenames></author><author><keyname>Nastasenko</keyname><forenames>Marina</forenames></author><author><keyname>Kotov</keyname><forenames>Mikhail</forenames></author></authors><title>Blind score normalization method for PLDA based speaker recognition</title><categories>cs.CL cs.LG cs.SD</categories><comments>4 pages, 1 figure, presented at the Interspeech 2015. In Sixteenth
  Annual Conference of the International Speech Communication Association 2015</comments><acm-class>I.5.1; I.5.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art
method for modeling $i$-vector space in speaker recognition task. However the
performance degradation is observed if enrollment data size differs from one
speaker to another. This paper presents a solution to such problem by
introducing new PLDA scoring normalization technique. Normalization parameters
are derived in a blind way, so that, unlike traditional \textit{ZT-norm}, no
extra development data is required. Moreover, proposed method has shown to be
optimal in terms of detection cost function. The experiments conducted on NIST
SRE 2014 database demonstrate an improved accuracy in a mixed enrollment number
condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06977</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06977</id><created>2016-02-22</created><updated>2016-02-25</updated><authors><author><keyname>Fast</keyname><forenames>Ethan</forenames></author><author><keyname>McGrath</keyname><forenames>William</forenames></author><author><keyname>Rajpurkar</keyname><forenames>Pranav</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael</forenames></author></authors><title>Augur: Mining Human Behaviors from Fiction to Power Interactive Systems</title><categories>cs.HC cs.AI cs.IR</categories><comments>CHI: ACM Conference on Human Factors in Computing Systems 2016</comments><doi>10.1145/2858036.2858528</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From smart homes that prepare coffee when we wake, to phones that know not to
interrupt us during important conversations, our collective visions of HCI
imagine a future in which computers understand a broad range of human
behaviors. Today our systems fall short of these visions, however, because this
range of behaviors is too large for designers or programmers to capture
manually. In this paper, we instead demonstrate it is possible to mine a broad
knowledge base of human behavior by analyzing more than one billion words of
modern fiction. Our resulting knowledge base, Augur, trains vector models that
can predict many thousands of user activities from surrounding objects in
modern contexts: for example, whether a user may be eating food, meeting with a
friend, or taking a selfie. Augur uses these predictions to identify actions
that people commonly take on objects in the world and estimate a user's future
activities given their current situation. We demonstrate Augur-powered,
activity-based systems such as a phone that silences itself when the odds of
you answering it are low, and a dynamic music player that adjusts to your
present activity. A field deployment of an Augur-powered wearable camera
resulted in 96% recall and 71% precision on its unsupervised predictions of
common daily activities. A second evaluation where human judges rated the
system's predictions over a broad set of input images found that 94% were rated
sensible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06979</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06979</id><created>2016-02-22</created><authors><author><keyname>Fast</keyname><forenames>Ethan</forenames></author><author><keyname>Chen</keyname><forenames>Binbin</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael</forenames></author></authors><title>Empath: Understanding Topic Signals in Large-Scale Text</title><categories>cs.CL cs.AI</categories><comments>CHI: ACM Conference on Human Factors in Computing Systems 2016</comments><doi>10.1145/2858036.2858535</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human language is colored by a broad range of topics, but existing text
analysis tools only focus on a small number of them. We present Empath, a tool
that can generate and validate new lexical categories on demand from a small
set of seed terms (like &quot;bleed&quot; and &quot;punch&quot; to generate the category violence).
Empath draws connotations between words and phrases by deep learning a neural
embedding across more than 1.8 billion words of modern fiction. Given a small
set of seed words that characterize a category, Empath uses its neural
embedding to discover new related terms, then validates the category with a
crowd-powered filter. Empath also analyzes text across 200 built-in,
pre-validated categories we have generated from common topics in our web
dataset, like neglect, government, and social media. We show that Empath's
data-driven, human validated categories are highly correlated (r=0.906) with
similar categories in LIWC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06986</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06986</id><created>2016-02-22</created><authors><author><keyname>Meyer</keyname><forenames>Evgeny</forenames></author><author><keyname>Peet</keyname><forenames>Matthew M.</forenames></author></authors><title>A Convex Approach for Stability Analysis of Coupled PDEs using Lyapunov
  Functionals</title><categories>math.OC cs.SY</categories><comments>IFAC Workshop on Time Delay Systems, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an algorithm for stability analysis of systems
described by coupled linear Partial Differential Equations (PDEs) with constant
coefficients and mixed boundary conditions. Our approach uses positive matrices
to parameterize functionals which are positive or negative on certain function
spaces. Applying this parameterization to construct Lyapunov functionals with
negative derivative allows us to express stability conditions as a set of LMI
constraints which can be constructed using SOSTOOLS and tested using standard
SDP solvers such as SeDuMi. The results are tested using a simple numerical
example and compared results obtained from simulation using a standard form of
discretization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06987</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06987</id><created>2016-02-22</created><authors><author><keyname>Baumeler</keyname><forenames>&#xc4;min</forenames></author><author><keyname>Wolf</keyname><forenames>Stefan</forenames></author></authors><title>Causality - Complexity - Consistency: Can Space-Time Be Based on Logic
  and Computation?</title><categories>quant-ph cs.LO</categories><comments>17 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The difficulty of explaining non-local correlations in a fixed causal
structure sheds new light on the old debate on whether space and time are to be
seen as fundamental. Refraining from assuming space-time as given a priori has
a number of consequences. First, the usual definitions of randomness depend on
a causal structure and turn meaningless. So motivated, we propose an intrinsic,
physically motivated measure for the randomness of a string of bits: its length
minus its normalized work value, a quantity we closely relate to its Kolmogorov
complexity (the length of the shortest program making a universal Turing
machine generate this string). We test this alternative concept of randomness
for the example of non-local correlations: We end up with a reasoning that
leads to similar conclusions but is more direct than in the probabilistic view
since only the outcomes of measurements that can actually all be carried out
together are put into relation to each other. In the same context-free spirit,
we connect the logical reversibility of an evolution to the second law of
thermodynamics and the arrow of time. Refining this, we end up with a
speculation on the emergence of a space-time structure on bit strings in terms
of data-compressibility relations. Finally, we show that logical consistency,
by which we replace the abandoned causality, it strictly weaker a constraint
than the latter in the multi-party case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06989</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06989</id><created>2016-02-22</created><authors><author><keyname>de Amorim</keyname><forenames>Renato Cordeiro</forenames></author><author><keyname>Hennig</keyname><forenames>Christian</forenames></author></authors><title>Recovering the number of clusters in data sets with noise features using
  feature rescaling factors</title><categories>stat.ML cs.LG</categories><journal-ref>Information Sciences 324 (2015), 126-145</journal-ref><doi>10.1016/j.ins.2015.06.039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce three methods for re-scaling data sets aiming at
improving the likelihood of clustering validity indexes to return the true
number of spherical Gaussian clusters with additional noise features. Our
method obtains feature re-scaling factors taking into account the structure of
a given data set and the intuitive idea that different features may have
different degrees of relevance at different clusters.
  We experiment with the Silhouette (using squared Euclidean, Manhattan, and
the p$^{th}$ power of the Minkowski distance), Dunn's, Calinski-Harabasz and
Hartigan indexes on data sets with spherical Gaussian clusters with and without
noise features. We conclude that our methods indeed increase the chances of
estimating the true number of clusters in a data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06994</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06994</id><created>2016-02-22</created><authors><author><keyname>Gudmundsson</keyname><forenames>Joachim</forenames></author><author><keyname>Horton</keyname><forenames>Michael</forenames></author></authors><title>Spatio-Temporal Analysis of Team Sports -- A Survey</title><categories>cs.OH</categories><comments>42 pages, 11 figures</comments><acm-class>A.1; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Team-based invasion sports such as football, basketball and hockey are
similar in the sense that the players are able to move freely around the
playing area; and that player and team performance cannot be fully analysed
without considering the movements and interactions of all players as a group.
State of the art object tracking systems now produce spatio-temporal traces of
player trajectories with high definition and high frequency, and this, in turn,
has facilitated a variety of research efforts, across many disciplines, to
extract insight from the trajectories. We survey recent research efforts that
use spatio-temporal data from team sports as input, and involve non-trivial
computation. This article categorises the research efforts in a coherent
framework and identifies a number of open research questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.06997</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.06997</id><created>2016-02-22</created><authors><author><keyname>Kokoris-Kogias</keyname><forenames>Eleftherios</forenames></author><author><keyname>Jovanovic</keyname><forenames>Philipp</forenames></author><author><keyname>Gailly</keyname><forenames>Nicolas</forenames></author><author><keyname>Khoffi</keyname><forenames>Ismail</forenames></author><author><keyname>Gasser</keyname><forenames>Linus</forenames></author><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Enhancing Bitcoin Security and Performance with Strong Consistency via
  Collective Signing</title><categories>cs.CR</categories><comments>17 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While showing great promise, Bitcoin requires users to wait tens of minutes
for transactions to commit - even then offering only probabilistic guarantees.
This paper introduces ByzCoin, a novel Byzantine consensus protocol that
leverages scalable collective signing to commit Bitcoin transactions
irreversibly within seconds. ByzCoin achieves Byzantine consensus while
preserving Bitcoin's open membership by dynamically forming hash
power-proportionate consensus groups representing recently-successful block
miners. ByzCoin employs communication trees to optimize transaction commitment
and verification under normal operation while guaranteeing safety and liveness
under Byzantine faults, up to a near-optimal tolerance of f faulty group
members among 3f+2 total. ByzCoin mitigates double spending and selfish mining
attacks by producing collectively signed transaction blocks within one minute
of transaction submission. Tree-structured communication further reduces this
latency to less than 30 seconds. Thanks to these optimizations ByzCoin achieves
a throughput higher than Paypal currently handles, with confirmation latencies
of 15-20 seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07008</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07008</id><created>2016-02-22</created><authors><author><keyname>Dourbal</keyname><forenames>Pavel</forenames></author></authors><title>Synthesis of fast multiplication algorithms for arbitrary tensors</title><categories>cs.DS</categories><comments>79 pages</comments><msc-class>15A04, 15A23, 65F50, 68W10, 68W15, 68W35</msc-class><acm-class>F.2.1; G.1.3; G.4; I.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method of fast linear transform algorithm synthesis for an arbitrary
tensor, matrix, or vector is proposed. The method is based on factorization of
a tensor and using the factors for building computational structures performing
fast tensor - vector multiplication on a computer or dedicated hardware
platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07009</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07009</id><created>2016-02-22</created><authors><author><keyname>Li</keyname><forenames>Zhigang</forenames></author><author><keyname>Qiu</keyname><forenames>Feng</forenames></author><author><keyname>Wang</keyname><forenames>Jianhui</forenames></author></authors><title>Data-Driven Real-Time Power Dispatch for Maximizing Variable Renewable
  Generation</title><categories>cs.SY</categories><comments>12 pages, 10 figures</comments><doi>10.1016/j.apenergy.2016.02.125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional power dispatch methods have difficulties in accommodating
large-scale variable renewable generation (VRG) and have resulted in
unnecessary VRG spillage in the practical industry. The recent
dispatchable-interval-based methods have the potential to reduce VRG
curtailment, but the dispatchable intervals are not allocated effectively due
to the lack of exploiting historical dispatch records of VRG units. To bridge
this gap, this paper proposes a novel data-driven real-time dispatch approach
to maximize VRG utili-zation by using do-not-exceed (DNE) limits. This approach
defines the maximum generation output ranges that the system can ac-commodate
without compromising reliability. The DNE limits of VRG units and operating
base points of conventional units are co-optimized by hybrid stochastic and
robust optimization, and the decision models are formulated as mixed-integer
linear programs by the sample average approximation technique exploiting
historical VRG data. A strategy for selecting historical data samples is also
proposed to capture the VRG uncertainty more accurately under variant
prediction output levels. Computational experiments show the effectiveness of
the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07013</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07013</id><created>2016-02-22</created><authors><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Karczmarz</keyname><forenames>Adam</forenames></author></authors><title>Improved Bounds for Shortest Paths in Dense Distance Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing shortest paths in so-called dense distance
graphs. Every planar graph $G$ on $n$ vertices can be partitioned into a set of
$O(n/r)$ edge-disjoint regions (called an $r$-division) with $O(r)$ vertices
each, such that each region has $O(\sqrt{r})$ vertices (called boundary
vertices) in common with other regions. A dense distance graph of a region is a
complete graph containing all-pairs distances between its boundary nodes. A
dense distance graph of an $r$-division is the union of the $O(n/r)$ dense
distance graphs of the individual pieces. Since the introduction of dense
distance graphs by Fakcharoenphol and Rao, computing single-source shortest
paths in dense distance graphs has found numerous applications in fundamental
planar graph algorithms.
  Fakcharoenphol and Rao proposed an algorithm (later called FR-Dijkstra) for
computing single-source shortest paths in a dense distance graph in
$O\left(\frac{n}{\sqrt{r}}\log{n}\log{r}\right)$ time. We show an
$O\left(\frac{n}{\sqrt{r}}\left(\left(\frac{\log{r}}{\log\log{r}}\right)^2+\log^{1+\epsilon}{n}\right)\right)$
time algorithm for this problem, which is the first improvement to date over
FR-Dijkstra for the important case when $r$ is polynomial in $n$. In this case,
our algorithm is faster by a factor of $O(\log^2{\log{n}})$ and implies
improved upper bounds for such problems as multiple-source multiple-sink
maximum flows, single-source all-sinks maximum flow and (dynamic) exact
distance oracles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07017</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07017</id><created>2016-02-22</created><authors><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author><author><keyname>Xu</keyname><forenames>Yong</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>A survey of sparse representation: algorithms and applications</title><categories>cs.CV cs.LG</categories><comments>Published on IEEE Access, Vol. 3, pp. 490-530, 2015</comments><doi>10.1109/ACCESS.2015.2430359</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representation has attracted much attention from researchers in fields
of signal processing, image processing, computer vision and pattern
recognition. Sparse representation also has a good reputation in both
theoretical research and practical applications. Many different algorithms have
been proposed for sparse representation. The main purpose of this article is to
provide a comprehensive study and an updated review on sparse representation
and to supply a guidance for researchers. The taxonomy of sparse representation
methods can be studied from various viewpoints. For example, in terms of
different norm minimizations used in sparsity constraints, the methods can be
roughly categorized into five groups: sparse representation with $l_0$-norm
minimization, sparse representation with $l_p$-norm (0$&lt;$p$&lt;$1) minimization,
sparse representation with $l_1$-norm minimization and sparse representation
with $l_{2,1}$-norm minimization. In this paper, a comprehensive overview of
sparse representation is provided. The available sparse representation
algorithms can also be empirically categorized into four groups: greedy
strategy approximation, constrained optimization, proximity algorithm-based
optimization, and homotopy algorithm-based sparse representation. The
rationales of different algorithms in each category are analyzed and a wide
range of sparse representation applications are summarized, which could
sufficiently reveal the potential nature of the sparse representation theory.
Specifically, an experimentally comparative study of these sparse
representation algorithms was presented. The Matlab code used in this paper can
be available at: http://www.yongxu.org/lunwen.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07019</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07019</id><created>2016-02-22</created><authors><author><keyname>Wang</keyname><forenames>Zhiguo</forenames></author><author><keyname>Mi</keyname><forenames>Haitao</forenames></author><author><keyname>Ittycheriah</keyname><forenames>Abraham</forenames></author></authors><title>Sentence Similarity Learning by Lexical Decomposition and Composition</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Most conventional sentence similarity methods only focus on similar parts of
two input sentences, and simply ignore the dissimilar parts, which usually give
us some clues and semantic meanings about the sentences. In this work, we
propose a model to take into account both the similarities and dissimilarities
by decomposing and composing lexical semantics over sentences. The model
represents each word as a vector, and calculates a semantic matching vector for
each word based on all words in the other sentence. Then, each word vector is
decomposed into a similar component and a dissimilar component based on the
semantic matching vector. After this, a two-channel CNN model is employed to
capture features by composing the similar and dissimilar components. Finally, a
similarity score is estimated over the composed feature vectors. Experimental
results show that our model gets the state-of-the-art performance on the answer
sentence selection task, and achieves a comparable result on the paraphrase
identification task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07022</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07022</id><created>2016-02-22</created><authors><author><keyname>Zhang</keyname><forenames>Jiayi</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Masouros</keyname><forenames>Christos</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author></authors><title>On the Spectral Efficiency of Space-Constrained Massive MIMO with Linear
  Receivers</title><categories>cs.IT math.IT</categories><comments>6pages, 5 figures, accepted by IEEE ICC 2016, 23-27 May 2016, Kuala
  Lumpur, Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the spectral efficiency (SE) of massive
multiple-input multiple-output (MIMO) systems with a large number of antennas
at the base station (BS) accounting for physical space constraints. In contrast
to the vast body of related literature, which considers fixed inter-element
spacing, we elaborate on a practical topology in which an increase in the
number of antennas in a fixed total space induces an inversely proportional
decrease in the inter-antenna distance. For this scenario, we derive exact and
approximate expressions, as well as simplified upper/lower bounds, for the SE
of maximum-ratio combining (MRC), zero-forcing (ZF) and minimum mean-squared
error receivers (MMSE) receivers. In particular, our analysis shows that the
MRC receiver is non-optimal for space-constrained massive MIMO topologies. On
the other hand, ZF and MMSE receivers can still deliver an increasing SE as the
number of BS antennas grows large. Numerical results corroborate our analysis
and show the effect of the number of antennas, the number of users, and the
total antenna array space on the sum SE performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07024</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07024</id><created>2016-02-22</created><authors><author><keyname>Vadlamudi</keyname><forenames>Satya Gautam</forenames></author><author><keyname>Sengupta</keyname><forenames>Sailik</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author><author><keyname>Taguinod</keyname><forenames>Marthony</forenames></author><author><keyname>Zhao</keyname><forenames>Ziming</forenames></author><author><keyname>Doup&#xe9;</keyname><forenames>Adam</forenames></author><author><keyname>Ahn</keyname><forenames>Gail-Joon</forenames></author></authors><title>Moving Target Defense for Web Applications using Bayesian Stackelberg
  Games</title><categories>cs.CR cs.AI cs.GT cs.MA</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web applications form a critical component of cyber security systems as they
act as a gateway for many institutions. Vulnerabilities in web applications
allow malicious actors to access and/or modify restricted data. Here the
hackers have the opportunity to perform reconnaissance so as to gain knowledge
about the web application layout before launching an attack, whereas the
defender (administrator of the web application) must secure the application
even with its potential vulnerabilities. In order to mask such vulnerabilities
which are primarily associated with different individual configurations, Moving
Target Defense systems were proposed wherein the defender switches between
various configurations thereby making it difficult to attack with success,
while maintaining a seamless experience for the genuine users. However, the
design of good quality switching strategies is still an open problem which is
crucial for the effectiveness of the Moving Target Defense approach. In this
paper, we present a way to find effective switching strategies by modeling this
ecosystem as a Bayesian Stackelberg game with the administrator as the leader
and the hackers as the followers, which as we show succinctly captures various
aspects of the Moving Target Defense systems. Furthermore, we show how to
determine which vulnerability areas should be addressed first once the system
is deployed and which attacker type uncertainties should be calibrated with
high precision, for increasing the security of the web application. We present
experimental results on a representative web application system demonstrating
the utility of switching strategies obtained using the proposed method, and we
discuss various future directions that are unique to the web application
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07029</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07029</id><created>2016-02-22</created><authors><author><keyname>Reddy</keyname><forenames>Siddharth</forenames></author><author><keyname>Labutov</keyname><forenames>Igor</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author></authors><title>Latent Skill Embedding for Personalized Lesson Sequence Recommendation</title><categories>cs.LG cs.AI cs.CY</categories><comments>Under review by the ACM SIGKDD Conference on Knowledge Discovery and
  Data Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Students in online courses generate large amounts of data that can be used to
personalize the learning process and improve quality of education. In this
paper, we present the Latent Skill Embedding (LSE), a probabilistic model of
students and educational content that can be used to recommend personalized
sequences of lessons with the goal of helping students prepare for specific
assessments. Akin to collaborative filtering for recommender systems, the
algorithm does not require students or content to be described by features, but
it learns a representation using access traces. We formulate this problem as a
regularized maximum-likelihood embedding of students, lessons, and assessments
from historical student-content interactions. An empirical evaluation on
large-scale data from Knewton, an adaptive learning technology company, shows
that this approach predicts assessment results competitively with benchmark
models and is able to discriminate between lesson sequences that lead to
mastery and failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07031</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07031</id><created>2016-02-22</created><authors><author><keyname>Alsheikh</keyname><forenames>Mohammad Abu</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Lin</keyname><forenames>Shaowei</forenames></author><author><keyname>Tan</keyname><forenames>Hwee-Pink</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Mobile Big Data Analytics Using Deep Learning and Apache Spark</title><categories>cs.DC cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of mobile devices, such as smartphones and Internet of
Things (IoT) gadgets, results in the recent mobile big data (MBD) era.
Collecting MBD is unprofitable unless suitable analytics and learning methods
are utilized for extracting meaningful information and hidden patterns from
data. This article presents an overview and brief tutorial of deep learning in
MBD analytics and discusses a scalable learning framework over Apache Spark.
Specifically, a distributed deep learning is executed as an iterative MapReduce
computing on many Spark workers. Each Spark worker learns a partial deep model
on a partition of the overall MBD, and a master deep model is then built by
averaging the parameters of all partial models. This Spark-based framework
speeds up the learning of deep models consisting of many hidden layers and
millions of parameters. We use a context-aware activity recognition application
with a real-world dataset containing millions of samples to validate our
framework and assess its speedup effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07032</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07032</id><created>2016-02-22</created><authors><author><keyname>Reddy</keyname><forenames>Siddharth</forenames></author><author><keyname>Labutov</keyname><forenames>Igor</forenames></author><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author></authors><title>Unbounded Human Learning: Optimal Scheduling for Spaced Repetition</title><categories>cs.AI cs.CY</categories><comments>Under review by the ACM SIGKDD Conference on Knowledge Discovery and
  Data Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of human learning, there is broad evidence that our ability to
retain a piece of information improves with repeated exposure, and that it
decays with delay since the last exposure. This plays a crucial role in the
design of educational software, leading to a trade-off between teaching new
material and reviewing what has already been taught. A common way to balance
this trade-off is via spaced repetition -- using periodic review of content to
improve long-term retention. Though widely used in practice, there is little
formal understanding of the design of these systems. This paper addresses this
gap. First, we mine log data from a spaced repetition system to establish the
functional dependence of retention on reinforcement and delay. Second, based on
this memory model, we develop a mathematical framework for spaced repetition
systems using a queueing-network approach. This model formalizes the popular
Leitner Heuristic for spaced repetition, providing the first rigorous and
computationally tractable means of optimizing the review schedule. Finally, we
empirically confirm the validity of our formal model via a Mechanical Turk
experiment. In particular, we verify a key qualitative insight and prediction
of our model -- the existence of a sharp phase transition in learning outcomes
upon increasing the rate of new item introductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07038</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07038</id><created>2016-02-22</created><authors><author><keyname>Sober</keyname><forenames>Barak</forenames></author><author><keyname>Levin</keyname><forenames>David</forenames></author></authors><title>Computer Aided Restoration of Handwritten Character Strokes</title><categories>cs.GR cs.CV math.NA</categories><comments>11 pages, 17 figures</comments><msc-class>68U07, 68U10, 65D18, 94A08</msc-class><acm-class>I.7.5; I.5.4; I.4.5; J.6; I.3.3; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work suggests a new variational approach to the task of computer aided
restoration of incomplete characters, residing in a highly noisy document. We
model character strokes as the movement of a pen with a varying radius.
Following this model, a cubic spline representation is being utilized to
perform gradient descent steps, while maintaining interpolation at some initial
(manually sampled) points. The proposed algorithm was utilized in the process
of restoring approximately 1000 ancient Hebrew characters (dating to ca.
8th-7th century BCE), some of which are presented herein and show that the
algorithm yields plausible results when applied on deteriorated documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07040</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07040</id><created>2016-02-20</created><authors><author><keyname>Rozaki</keyname><forenames>Eleni</forenames></author></authors><title>Clustering Optimisation Techniques in Mobile Networks</title><categories>cs.NI cs.DS</categories><comments>8 pages, 4 figures</comments><journal-ref>(IJRITCC), February 2016, Volume 4, Issue 2, PP:22-29</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of mobile phones has exploded over the past years,abundantly through
the introduction of smartphones and the rapidly expanding use of mobile data.
This has resulted in a spiraling problem of ensuring quality of service for
users of mobile networks. Hence, mobile carriers and service providers need to
determine how to prioritise expansion decisions and optimise network faults to
ensure customer satisfaction and optimal network performance. To assist in that
decision-making process, this research employs data mining classification of
different Key Performance Indicator datasets to develop a monitoring scheme for
mobile networks as a means of identifying the causes of network malfunctions.
Then, the data are clustered to observe the characteristics of the technical
areas with the use of k-means clustering. The data output is further trained
with decision tree classification algorithms. The end result was that this
method of network optimisation allowed for significantly improved fault
detection performance
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07043</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07043</id><created>2016-02-22</created><authors><author><keyname>Adler</keyname><forenames>Philip</forenames></author><author><keyname>Falk</keyname><forenames>Casey</forenames></author><author><keyname>Friedler</keyname><forenames>Sorelle A.</forenames></author><author><keyname>Rybeck</keyname><forenames>Gabriel</forenames></author><author><keyname>Scheidegger</keyname><forenames>Carlos</forenames></author><author><keyname>Smith</keyname><forenames>Brandon</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Auditing Black-box Models by Obscuring Features</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-trained predictive models are widely used to assist in decision making.
But they are used as black boxes that output a prediction or score. It is
therefore hard to acquire a deeper understanding of model behavior: and in
particular how different attributes influence the model prediction. This is
very important when trying to interpret the behavior of complex models, or
ensure that certain problematic attributes (like race or gender) are not unduly
influencing decisions.
  In this paper, we present a technique for auditing black-box models: we can
study the extent to which existing models take advantage of particular features
in the dataset without knowing how the models work. We show how a class of
techniques originally developed for the detection and repair of disparate
impact in classification models can be used to study the sensitivity of any
model with respect to any feature subsets.
  Our approach does not require the black-box model to be retrained. This is
important if (for example) the model is only accessible via an API, and
contrasts our work with other methods that investigate feature influence like
feature selection. We present experimental evidence for the effectiveness of
our procedure using a variety of publicly available datasets and models. We
also validate our procedure using techniques from interpretable learning and
feature selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07046</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07046</id><created>2016-02-23</created><authors><author><keyname>Balcan</keyname><forenames>Maria Florina</forenames></author><author><keyname>Du</keyname><forenames>Simon S.</forenames></author><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Yu</keyname><forenames>Adams Wei</forenames></author></authors><title>An Improved Gap-Dependency Analysis of the Noisy Power Method</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the noisy power method algorithm, which has wide applications in
machine learning and statistics, especially those related to principal
component analysis (PCA) under resource (communication, memory or privacy)
constraints. Existing analysis of the noisy power method shows an
unsatisfactory dependency over the &quot;consecutive&quot; spectral gap
$(\sigma_k-\sigma_{k+1})$ of an input data matrix, which could be very small
and hence limits the algorithm's applicability. In this paper, we present a new
analysis of the noisy power method that achieves improved gap dependency for
both sample complexity and noise tolerance bounds. More specifically, we
improve the dependency over $(\sigma_k-\sigma_{k+1})$ to dependency over
$(\sigma_k-\sigma_{q+1})$, where $q$ is an intermediate algorithm parameter and
could be much larger than the target rank $k$. Our proofs are built upon a
novel characterization of proximity between two subspaces that differ from
canonical angle characterizations analyzed in previous works. Finally, we apply
our improved bounds to distributed private PCA and memory-efficient streaming
PCA and obtain bounds that are superior to existing results in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07048</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07048</id><created>2016-02-23</created><authors><author><keyname>Dong</keyname><forenames>Yuxiao</forenames></author><author><keyname>Johnson</keyname><forenames>Reid A.</forenames></author><author><keyname>Xu</keyname><forenames>Jian</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Structural Diversity and Homophily: A Study Across More than One Hundred
  Large-Scale Networks</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the ways in which local network structures are formed and
organized is a fundamental problem in network science. A widely recognized
organizing principle is structural homophily, which suggests that people with
more common neighbors are more likely to connect with each other. However, what
influence the diverse structures formed by common neighbors have on link
formation is much less well understood. To explore this problem, we begin by
formally defining the structural diversity of common neighborhoods. Using a
collection of 116 large-scale networks---the biggest with over 60 million nodes
and 1.8 billion edges---we then leverage this definition to develop a unique
network signature, which we use to uncover several distinct network
superfamilies not discoverable by conventional methods. We demonstrate that
structural diversity has a significant impact on link existence, and we
discover striking cases where it violates the principle of homophily. Our
findings suggest that structural diversity is an intrinsic network property,
giving rise to potential advances in the pursuit of theories of link formation
and network evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07053</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07053</id><created>2016-02-23</created><authors><author><keyname>Chau</keyname><forenames>Chi-Kin</forenames></author><author><keyname>Ho</keyname><forenames>Ivan W. H.</forenames></author><author><keyname>Situ</keyname><forenames>Zhenhui</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author><author><keyname>Zhang</keyname><forenames>Jialiang</forenames></author></authors><title>Effective Static and Adaptive Carrier Sensing Mechanisms for Dense CSMA
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasingly dense deployments of CSMA networks (e.g., WiFi, Zigbee)
arising from applications of Internet-of-things call for an improvement to
mitigate the interference among simultaneous transmitting wireless devices. For
cost efficiency and backward compatibility with legacy transceiver hardware, a
simple approach to address interference is by appropriately configuring the
carrier sensing thresholds in CSMA protocols, particularly in dense wireless
networks. Most prior studies of the configuration of carrier sensing thresholds
are based on a simplified conflict graph model, whereas this paper considers a
realistic signal-to-interference-and-noise ratio model. We provide a
comprehensive study for two effective CSMA protocols:
Cumulative-interference-Power Carrier Sensing and
Incremental-interference-Power Carrier Sensing, considering two aspects: (1)
static approach that sets a universal carrier sensing threshold for ensuring
interference-safe transmissions regardless of network topology, and (2)
adaptive approach that adjusts the carrier sensing thresholds dynamically based
on the feedback of nearby transmissions. We also provide simulation studies to
evaluate the starvation ratio, fairness, and goodput of our approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07057</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07057</id><created>2016-02-23</created><authors><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author><author><keyname>Shariat</keyname><forenames>Shahriar</forenames></author></authors><title>Finding Needle in a Million Metrics: Anomaly Detection in a Large-scale
  Computational Advertising Platform</title><categories>cs.AI cs.CY cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online media offers opportunities to marketers to deliver brand messages to a
large audience. Advertising technology platforms enables the advertisers to
find the proper group of audiences and deliver ad impressions to them in real
time. The recent growth of the real time bidding has posed a significant
challenge on monitoring such a complicated system. With so many components we
need a reliable system that detects the possible changes in the system and
alerts the engineering team. In this paper we describe the mechanism that we
invented for recovering the representative metrics and detecting the change in
their behavior. We show that this mechanism is able to detect the possible
problems in time by describing some incident cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07063</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07063</id><created>2016-02-23</created><authors><author><keyname>Chen</keyname><forenames>Bo-Wei</forenames></author><author><keyname>Ji</keyname><forenames>Wen</forenames></author></authors><title>Geo-Conquesting Based on Crowdsourced Metatrails from Mobile Sensing</title><categories>cs.HC</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article investigates approaches for intelligent marketing in smart
cities, where metatrails are crowdsourced by mobile sensing for marketing
strategies. Unlike most works that focused on client sides, this study is
intended for market planning, from the perspective of enterprises. Several
novel crowdsourced features based on metatrails, including hotspot networks,
crowd transitions, affinity subnetworks, and sequential visiting patterns, are
discussed in the article. These smart footprints can reflect crowd preferences
and the topology of a site of interest. Marketers can utilize such information
for commercial resource planning and deployment. Simulations were conducted to
demonstrate the performance. At the end, this study also discusses different
scenarios for practical geo-conquesting applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07064</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07064</id><created>2016-02-23</created><authors><author><keyname>Martinez-Gil</keyname><forenames>Jorge</forenames></author></authors><title>SIFT: An Algorithm for Extracting Structural Information From Taxonomies</title><categories>cs.DB cs.AI</categories><comments>12 pages</comments><msc-class>68T30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present SIFT, a 3-step algorithm for the analysis of the
structural information represented by means of a taxonomy. The major advantage
of this algorithm is the capability to leverage the information inherent to the
hierarchical structures of taxonomies to infer correspondences which can allow
to merge them in a later step. This method is particular relevant in scenarios
where taxonomy alignment techniques exploiting textual information from
taxonomy nodes cannot operate successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07065</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07065</id><created>2016-02-23</created><authors><author><keyname>Reich</keyname><forenames>Johannes</forenames></author></authors><title>Composition, Cooperation, and Coordination of Computational Systems</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system model is developed where the criterion to partition the world into a
system and a rest is based on the functional relation between its states. This
approach implies that the gestalt of systems becomes very dynamic. Especially
interactions between systems may create temporary super systems &quot;on the fly&quot;.
The reference to the function notion establishes the link to computation.
  Based on the distinction between the intended determinism versus
nondeterminism of the interaction, two classes of system interactions are
distinguished: composition and cooperation. Composition means that by
interaction super systems are created and the interacting systems become
subsystems. Cooperation means that systems sensibly interact loosely without
creating any identifiable super system function and therefore without super
system creation from the perspective of the interacting systems.
  Cooperative system interactions are described with the help of the protocol
notion based on shared states, interpreted as the stateful exchange of
characters via Shannon channels between roles - the projection of systems onto
the interaction channels. It is shown that roles can be internally coordinated
by a set of rules, leading to a very flexible process notion which unfolds its
full potential only in a new type of execution environment capable of executing
spontaneous transitions.
  The system model has immediate implications for componentization which can be
viewed as an attempt to either hide functional recursion and provide only
functionality whose composition behavior is easy to understand, or to provide
loosely coupled interactions via protocols. To be complete, component models
should therefore at least support three general classes of components: one for
loose coupling, one for hierarchical composition, and one for pipes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07067</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07067</id><created>2016-02-23</created><authors><author><keyname>Kasai</keyname><forenames>Hiroyuki</forenames></author></authors><title>Online Low-Rank Tensor Subspace Tracking from Incomplete Data by CP
  Decomposition using Recursive Least Squares</title><categories>cs.NA</categories><comments>IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an online tensor subspace tracking algorithm based on the CP
decomposition exploiting the recursive least squares (RLS), dubbed OnLine
Low-rank Subspace tracking by TEnsor CP Decomposition (OLSTEC). Numerical
evaluations show that the proposed OLSTEC algorithm gives faster convergence
per iteration comparing with the state-of-the-art online algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07085</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07085</id><created>2016-02-23</created><authors><author><keyname>Kaya</keyname><forenames>Abidin</forenames></author></authors><title>New extremal binary self-dual codes of length 68 via short kharaghani
  array over f_2 + uf_2</title><categories>math.CO cs.IT math.IT</categories><comments>10 pages, 5 tables</comments><msc-class>94B05, 94B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, new construction methods for self-dual codes are given. The
methods use the short Kharaghani array and a variation of it. These are
applicable to any commutative Frobenius ring. We apply the constructions over
the ring F_2 + uF_2 and self-dual Type I [64, 32, 12]_2-codes with various
weight enumerators obtained as Gray images. By the use of an extension theorem
for self-dual codes we were able to construct 27 new extremal binary self-dual
codes of length 68. The existence of the extremal binary self-dual codes with
these weight enumerators was previously unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07092</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07092</id><created>2016-02-23</created><authors><author><keyname>Tumyrkin</keyname><forenames>Rasul</forenames></author><author><keyname>Mazzara</keyname><forenames>Manuel</forenames></author><author><keyname>Kassab</keyname><forenames>Mohammad</forenames></author><author><keyname>Succi</keyname><forenames>Giancarlo</forenames></author><author><keyname>Lee</keyname><forenames>JooYoung</forenames></author></authors><title>Quality Attributes in Practice: Contemporary Data</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that the software process in place impacts the quality of
the resulting product. However, the specific way in which this effect occurs is
still mostly unknown and reported through anecdotes. To gather a better
understanding of such relationship, a very large survey has been conducted
during the last year and has been completed by more than 100 software
developers and engineers from 21 countries. We have used the percentage of
satisfied customers estimated by the software developers and engineers as the
main dependent variable. The results evidence some interesting patterns, like
that quality attribute of which customers are more satisfied appears
functionality, architectural styles may not have a significant influence on
quality, agile methodologies might result in happier customers, larger
companies and shorter projects seems to produce better products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07104</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07104</id><created>2016-02-23</created><authors><author><keyname>Karaca</keyname><forenames>Mehmet</forenames></author><author><keyname>Bastani</keyname><forenames>Saeed</forenames></author><author><keyname>Priyanto</keyname><forenames>Basuki Endah</forenames></author><author><keyname>Safavi</keyname><forenames>Mohammadhassan</forenames></author><author><keyname>Landfeldt</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Resource Allocation with Padding Overhead in OFDMA based Next Generation
  802.11 WLANs</title><categories>cs.NI cs.IT cs.PF math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, IEEE 802.11ax Task Group has proposed new guidelines for the use of
OFDMA-based medium access control. In this new framework, it has been decided
that the transmission for all the users in an multi-user OFDMA should end end
at the same time, and the users with insufficient data should transmit null
data (i.e. padding) to fill the duration. While this scheme offers strong
features such as resilience to Overlapping Basic Service Set (OBSS)
interference and ease of synchronization, it also poses major side issues of
degraded throughput performance and waste of devices' energy. We investigate
resource allocation problems where the scheduling duration (i.e., time) is
optimized through Lyapunov optimization techniques by taking into account the
padding overhead, airtime fairness and energy consumption of the users. Also,
being aware of the complexity of the existing OFDMA solutions, we propose
lightweight and agile algorithms with the consideration of their overhead and
implementation issues. We show that our resource allocation algorithms are
arbitrarily close to the optimal performance at the price of reduced
convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07106</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07106</id><created>2016-02-23</created><authors><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Scalable Generation of Scale-free Graphs</title><categories>cs.DS cs.DC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explain how massive instances of scale-free graphs following the
Barabasi-Albert model can be generated very quickly in an embarrassingly
parallel way. This makes this popular model available for studying big data
graph problems. As a demonstration, we generated a Petaedge graph in less than
an hour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07107</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07107</id><created>2016-02-23</created><authors><author><keyname>Bonald</keyname><forenames>Thomas</forenames></author><author><keyname>Combes</keyname><forenames>Richard</forenames></author></authors><title>A Streaming Algorithm for Crowdsourced Data Classification</title><categories>stat.ML cs.LG</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a streaming algorithm for the binary classification of data based
on crowdsourcing. The algorithm learns the competence of each labeller by
comparing her labels to those of other labellers on the same tasks and uses
this information to minimize the prediction error rate on each task. We provide
performance guarantees of our algorithm for a fixed population of independent
labellers. In particular, we show that our algorithm is optimal in the sense
that the cumulative regret compared to the optimal decision with known labeller
error probabilities is finite, independently of the number of tasks to label.
The complexity of the algorithm is linear in the number of labellers and the
number of tasks, up to some logarithmic factors. Numerical experiments
illustrate the performance of our algorithm compared to existing algorithms,
including simple majority voting and expectation-maximization algorithms, on
both synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07109</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07109</id><created>2016-02-23</created><authors><author><keyname>S&#xf6;lch</keyname><forenames>Maximilian</forenames></author><author><keyname>Bayer</keyname><forenames>Justin</forenames></author><author><keyname>Ludersdorfer</keyname><forenames>Marvin</forenames></author><author><keyname>van der Smagt</keyname><forenames>Patrick</forenames></author></authors><title>Variational Inference for On-line Anomaly Detection in High-Dimensional
  Time Series</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate variational inference has shown to be a powerful tool for
modeling unknown, complex probability distributions. Recent advances in the
field allow us to learn probabilistic sequence models. We apply a Stochastic
Recurrent Network (STORN) to learn robot time series data. Our evaluation
demonstrates that we can robustly detect anomalies both off- and on-line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07112</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07112</id><created>2016-02-23</created><authors><author><keyname>Combes</keyname><forenames>Richard</forenames></author><author><keyname>Sidi</keyname><forenames>Habib</forenames></author><author><keyname>Elayoubi</keyname><forenames>Salah-Eddine</forenames></author></authors><title>Multipath streaming: fundamental limits and efficient algorithms</title><categories>cs.NI cs.IT math.IT</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate streaming over multiple links. A file is split into small
units called chunks that may be requested on the various links according to
some policy, and received after some random delay. After a start-up time called
pre-buffering time, received chunks are played at a fixed speed. There is
starvation if the chunk to be played has not yet arrived. We provide lower
bounds (fundamental limits) on the starvation probability of any policy. We
further propose simple, order-optimal policies that require no feedback. For
general delay distributions, we provide tractable upper bounds for the
starvation probability of the proposed policies, allowing to select the
pre-buffering time appropriately. We specialize our results to: (i) links that
employ CSMA or opportunistic scheduling at the packet level, (ii) links shared
with a primary user (iii) links that use fair rate sharing at the flow level.
Our results also give insight into how to store parts of a streaming file in a
distributed manner (e.g in a data center) to guarantee maximal performance. Our
upper bounds are tighter than known results even for the single link case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07113</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07113</id><created>2016-02-23</created><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author><author><keyname>Lewis-Pye</keyname><forenames>Andrew</forenames></author><author><keyname>Teutsch</keyname><forenames>Jason</forenames></author></authors><title>Lower bounds on the redundancy in computations from random oracles via
  betting strategies with restricted wagers</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ku\v{c}era-G\'{a}cs theorem is a landmark result in algorithmic
randomness asserting that every real is computable from a Martin-L\&quot;{o}f random
real. If the computation of the first $n$ bits of a sequence requires $n+h(n)$
bits of the random oracle, then $h$ is the redundancy of the computation.
Ku\v{c}era implicitly achieved redundancy $n\log n$ while G\'{a}cs used a more
elaborate coding procedure which achieves redundancy $\sqrt{n}\log n$. A
similar bound is implicit in the later proof by Merkle and Mihailovi\'{c}. In
this paper we obtain strict lower bounds on the redundancy in computations from
Martin-L\&quot;{o}f random oracles. We show that any nondecreasing computable
function $g$ such that $\sum_n 2^{-g(n)}=\infty$ is not a general lower bound
on the redundancy in computations from Martin-L\&quot;{o}f random oracles. In fact,
there exists a real $X$ such that the redundancy $g$ of any computation of $X$
from a Martin-L\&quot;{o}f random oracle satisfies $\sum_n 2^{-g(n)}&lt;\infty$.
Moreover, the class of such reals is comeager and includes a $\Delta^0_2$ real
as well as all weakly 2-generic reals. We also show that every real which is
generalized non-low$_2$ computes a real with the above property. This excludes
many slow growing functions such as $\log n$ from bounding the redundancy in
computations from random oracles for a large class of reals. These results are
obtained as an application of a theory of effective betting strategies with
restricted wagers which we develop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07115</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07115</id><created>2016-02-23</created><authors><author><keyname>Sternagel</keyname><forenames>Christian</forenames></author><author><keyname>Sternagel</keyname><forenames>Thomas</forenames></author></authors><title>Level-Confluence of 3-CTRSs in Isabelle/HOL</title><categories>cs.LO</categories><comments>In Proceedings of the 4th International Workshop on Confluence (IWC
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an Isabelle/HOL formalization of an earlier result by Suzuki,
Middeldorp, and Ida; namely that a certain class of conditional rewrite systems
is level-confluent. Our formalization is basically along the lines of the
original proof, from which we deviate mostly in the level of detail as well as
concerning some basic definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07119</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07119</id><created>2016-02-23</created><authors><author><keyname>Mettes</keyname><forenames>Pascal</forenames></author><author><keyname>Koelma</keyname><forenames>Dennis C.</forenames></author><author><keyname>Snoek</keyname><forenames>Cees G. M.</forenames></author></authors><title>The ImageNet Shuffle: Reorganized Pre-training for Video Event Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper strives for video event detection using a representation learned
from deep convolutional neural networks. Different from the leading approaches,
who all learn from the 1,000 classes defined in the ImageNet Large Scale Visual
Recognition Challenge, we investigate how to leverage the complete ImageNet
hierarchy for pre-training deep networks. To deal with the problems of
over-specific classes and classes with few images, we introduce a bottom-up and
top-down approach for reorganization of the ImageNet hierarchy based on all its
21,814 classes and more than 14 million images. Experiments on the TRECVID
Multimedia Event Detection 2013 and 2015 datasets show that video
representations derived from the layers of a deep neural network pre-trained
with our reorganized hierarchy i) improves over standard pre-training, ii) is
complementary among different reorganizations, iii) maintains the benefits of
fusion with other modalities, and iv) leads to state-of-the-art event detection
results. The reorganized hierarchies and their derived Caffe models are
publicly available at http://tinyurl.com/imagenetshuffle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07120</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07120</id><created>2016-02-23</created><authors><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author></authors><title>Submodular Learning and Covering with Response-Dependent Costs</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider interactive learning and covering problems, in a setting where
actions may incur different costs, depending on the outcomes of the action. For
instance, in a clinical trial, selecting a patient for treatment might result
in improved health or adverse effects for this patients, and these two outcomes
have different costs. We consider a setting where these costs can be inferred
from observable responses to the actions. We generalize previous analyses of
interactive learning and covering to \emph{consistency aware} submodular
objectives, and propose a natural greedy algorithm for the setting of
response-dependent costs. We bound the approximation factor of this greedy
algorithm, for general submodular functions, as well as specifically for
\emph{learning objectives}, and show that a different property of the cost
function controls the approximation factor in each of these scenarios. We
further show that in both settings, the approximation factor of this greedy
algorithm is near-optimal in the class of greedy algorithms. Experiments
demonstrate the advantages of the proposed algorithm in the response-dependent
cost setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07125</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07125</id><created>2016-02-23</created><authors><author><keyname>Huttunen</keyname><forenames>Heikki</forenames></author><author><keyname>Yancheshmeh</keyname><forenames>Fatemeh Shokrollahi</forenames></author><author><keyname>Chen</keyname><forenames>Ke</forenames></author></authors><title>Car Type Recognition with Deep Neural Networks</title><categories>cs.CV</categories><comments>Submitted to IEEE Intelligent Vehicles Symposium 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study automatic recognition of cars of four types: Bus,
Truck, Van and Small car. For this problem we consider two data driven
frameworks: a deep neural network and a support vector machine using SIFT
features. The accuracy of the methods is validated with a database of over 6500
images, and the resulting prediction accuracy is over 97 %. This clearly
exceeds the accuracies of earlier studies that use manually engineered feature
extraction pipelines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07127</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07127</id><created>2016-02-23</created><authors><author><keyname>Br&#xe4;uer</keyname><forenames>Johannes</forenames></author></authors><title>Measuring Object-Oriented Design Principles</title><categories>cs.SE</categories><comments>4 pages, 1 figure, Proceeding of the Doctoral Symposium at the 30th
  ACM/IEEE International Conference on Automated Software Engineering (ASE
  2015), Lincoln, Nebraska, USA, November 9-13, 2015</comments><doi>10.1109/ASE.2015.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of automatizing the assessment of objectoriented design is not new.
Different approaches define and apply their own quality models, which are
composed of single metrics or combinations thereof, to operationalize software
design. However, single metrics are too fine-grained to identify core design
flaws and they cannot provide hints for making design improvements. In order to
deal with these weaknesses of metric-based models, rules-based approaches have
proven successful in the realm of source-code quality. Moreover, for developing
a well-designed software system, design principles play a key role, as they
define fundamental guidelines and help to avoid pitfalls. Therefore, this
thesis will enhance and complete a rule-based quality reference model for
operationalizing design principles and will provide a measuring tool that
implements these rules. The validation of the design quality model and the
measurement tool will be based on various industrial projects. Additionally,
quantitative and qualitative surveys will be conducted in order to get
validated results on the value of object-oriented design principles for
software development
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07128</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07128</id><created>2016-02-23</created><updated>2016-02-29</updated><authors><author><keyname>Nakibly</keyname><forenames>Gabi</forenames></author><author><keyname>Schcolnik</keyname><forenames>Jaime</forenames></author><author><keyname>Rubin</keyname><forenames>Yossi</forenames></author></authors><title>Website-Targeted False Content Injection by Network Operators</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that some network operators inject false content into users'
network traffic. Yet all previous works that investigate this practice focus on
edge ISPs (Internet Service Providers), namely, those that provide Internet
access to end users. Edge ISPs that inject false content affect their customers
only. However, in this work we show that not only edge ISPs may inject false
content, but also core network operators. These operators can potentially alter
the traffic of \emph{all} Internet users who visit predetermined websites. We
expose this practice by inspecting a large amount of traffic originating from
several networks. Our study is based on the observation that the forged traffic
is injected in an out-of-band manner: the network operators do not update the
network packets in-path, but rather send the forged packets \emph{without}
dropping the legitimate ones. This creates a race between the forged and the
legitimate packets as they arrive to the end user. This race can be identified
and analyzed. Our analysis shows that the main purpose of content injection is
to increase the network operators' revenue by inserting advertisements to
websites. Nonetheless, surprisingly, we have also observed numerous cases of
injected malicious content. We publish representative samples of the injections
to facilitate continued analysis of this practice by the security community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07154</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07154</id><created>2016-02-23</created><authors><author><keyname>D&#xfc;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Konrad</keyname><forenames>Christian</forenames></author><author><keyname>Renault</keyname><forenames>Marc</forenames></author></authors><title>On the Power of Advice and Randomization for Online Bipartite Matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While randomized online algorithms have access to a sequence of uniform
random bits, deterministic online algorithms with advice have access to a
sequence of \textit{advice bits}, i.e., bits that are set by an all powerful
oracle prior to the processing of the request sequence. Advice bits are at
least as helpful as random bits, but how helpful are they? In this work, we
investigate the power of advice bits and random bits for online maximum
bipartite matching (\textsc{MBM}).
  The well-known Karp-Vazirani-Vazirani algorithm is an optimal randomized
$(1-\frac{1}{e})$-competitive algorithm for \textsc{MBM} that requires access
to $\Theta(n \log n)$ uniform random bits. We show that
$\Omega(\log(\frac{1}{\epsilon}) n)$ advice bits are necessary and
$O(\frac{1}{\epsilon^5} n)$ sufficient in order to obtain a
$(1-\epsilon)$-competitive deterministic advice algorithm. Furthermore, for a
large natural class of deterministic advice algorithms, we prove that
$\Omega(\log \log \log n)$ advice bits are required in order to improve on the
$\frac{1}{2}$-competitiveness of the best deterministic online algorithm, while
it is known that $O(\log n)$ bits are sufficient.
  Last, we give a randomized online algorithm that uses $c n$ random bits, for
integers $c \ge 1$, and a competitive ratio that approaches $1-\frac{1}{e}$
very quickly as $c$ is increasing. For example if $c = 10$, then the difference
between $1-\frac{1}{e}$ and the achieved competitive ratio is less than
$0.0002$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07165</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07165</id><created>2016-02-23</created><authors><author><keyname>Keiren</keyname><forenames>Jeroen J. A.</forenames></author><author><keyname>Fontana</keyname><forenames>Peter</forenames></author><author><keyname>Cleaveland</keyname><forenames>Rance</forenames></author></authors><title>Corrections to A Menagerie of Timed Automata</title><categories>cs.FL</categories><comments>9 pages, corrects a technical error in the ACM Computing Surveys
  paper mentioned in the title, that can be found at
  http://dx.doi.org/10.1145/2518102</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note corrects a technical error in the ACM Computing Surveys paper
mentioned in the title. The flaw involved constructions for showing that timed
automata with urgent locations have the same expressiveness as timed automata
that allow false location invariants. Corrected con- structions are presented
in this note, and the affected results are reproved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07168</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07168</id><created>2016-02-23</created><updated>2016-03-08</updated><authors><author><keyname>Egorov</keyname><forenames>Michael</forenames></author><author><keyname>Wilkison</keyname><forenames>MacLane</forenames></author></authors><title>ZeroDB white paper</title><categories>cs.CR cs.DB</categories><comments>Website of the project: https://www.zerodb.io/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ZeroDB is an end-to-end encrypted database that enables clients to operate on
(search, sort, query, and share) encrypted data without exposing encryption
keys or cleartext data to the database server. The familiar client-server
architecture is unchanged, but query logic and encryption keys are pushed
client-side. Since the server has no insight into the nature of the data, the
risk of data being exposed via a server-side data breach is eliminated. Even if
the server is successfully infiltrated, adversaries would not have access to
the cleartext data and cannot derive anything useful out of disk or RAM
snapshots.
  ZeroDB provides end-to-end encryption while maintaining much of the
functionality expected of a modern database, such as full-text search, sort,
and range queries. Additionally, ZeroDB uses proxy re-encryption and/or delta
key technology to enable secure, granular sharing of encrypted data without
exposing keys to the server and without sharing the same encryption key between
users of the database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07171</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07171</id><created>2016-02-23</created><authors><author><keyname>Goedgebeur</keyname><forenames>Jan</forenames></author><author><keyname>Zamfirescu</keyname><forenames>Carol T.</forenames></author></authors><title>Improved bounds for hypohamiltonian graphs</title><categories>math.CO cs.DM</categories><comments>21 pages; submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is hypohamiltonian if $G$ is non-hamiltonian and $G - v$ is
hamiltonian for every $v \in V(G)$. In the following, every graph is assumed to
be hypohamiltonian. Aldred, Wormald, and McKay gave a list of all graphs of
order at most 17. In this article, we present an algorithm to generate all
graphs of a given order and apply it to prove that there exist exactly 14
graphs of order 18 and 34 graphs of order 19. We also extend their results in
the cubic case. Furthermore, we show that (i) the smallest graph of girth 6 has
order 25, (ii) the smallest planar graph has order at least 23, (iii) the
smallest cubic planar graph has order at least 54, and (iv) the smallest cubic
planar graph of girth 5 with non-trivial automorphism group has order 78.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07182</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07182</id><created>2016-02-23</created><authors><author><keyname>Garivier</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>IMT</affiliation></author><author><keyname>M&#xe9;nard</keyname><forenames>Pierre</forenames><affiliation>IMT</affiliation></author><author><keyname>Stoltz</keyname><forenames>Gilles</forenames><affiliation>GREGH</affiliation></author></authors><title>Explore First, Exploit Next: The True Shape of Regret in Bandit Problems</title><categories>math.ST cs.LG stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit lower bounds on the regret in the case of multi-armed bandit
problems. We obtain non-asymptotic bounds and provide straightforward proofs
based only on well-known properties of Kullback-Leibler divergences. These
bounds show that in an initial phase the regret grows almost linearly, and that
the well-known logarithmic growth of the regret only holds in a final phase.
The proof techniques come to the essence of the arguments used and they are
deprived of all unnecessary complications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07185</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07185</id><created>2016-02-23</created><authors><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Mejova</keyname><forenames>Yelena</forenames></author></authors><title>Crowdsourcing Health Labels: Inferring Body Weight from Profile Pictures</title><categories>cs.HC cs.CY</categories><comments>This is a preprint of an article appearing at ACM DigitalHealth 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To use social media for health-related analysis, one key step is the
detection of health-related labels for users. But unlike transient conditions
like flu, social media users are less vocal about chronic conditions such as
obesity, as users might not tweet &quot;I'm still overweight&quot;. As, however,
obesity-related conditions such as diabetes, heart disease, osteoarthritis, and
even cancer are on the rise, this obese-or-not label could be one of the most
useful for studies in public health.
  In this paper we investigate the feasibility of using profile pictures to
infer if a user is overweight or not. We show that this is indeed possible and
further show that the fraction of labeled-as-overweight users is higher in U.S.
counties with higher obesity rates. Going from public to individual health
analysis, we then find differences both in behavior and social networks, for
example finding users labeled as overweight to have fewer followers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07188</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07188</id><created>2016-02-23</created><authors><author><keyname>Nikulin</keyname><forenames>Yaroslav</forenames></author><author><keyname>Novak</keyname><forenames>Roman</forenames></author></authors><title>Exploring the Neural Algorithm of Artistic Style</title><categories>cs.CV</categories><comments>A short class project report (14 pages, 14 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the method of style transfer presented in the article &quot;A Neural
Algorithm of Artistic Style&quot; by Leon A. Gatys, Alexander S. Ecker and Matthias
Bethge (arXiv:1508.06576).
  We first demonstrate the power of the suggested style space on a few
examples. We then vary different hyper-parameters and program properties that
were not discussed in the original paper, among which are the recognition
network used, starting point of the gradient descent and different ways to
partition style and content layers. We also give a brief comparison of some of
the existing algorithm implementations and deep learning frameworks used.
  To study the style space further we attempt to generate synthetic images by
maximizing a single entry in one of the Gram matrices $\mathcal{G}_l$ and some
interesting results are observed. Next, we try to mimic the sparsity and
intensity distribution of Gram matrices obtained from a real painting and
generate more complex textures.
  Finally, we propose two new style representations built on top of network's
features and discuss how one could be used to achieve local and potentially
content-aware style transfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07194</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07194</id><created>2016-02-23</created><authors><author><keyname>Kleindessner</keyname><forenames>Matth&#xe4;us</forenames></author><author><keyname>von Luxburg</keyname><forenames>Ulrike</forenames></author></authors><title>Lens depth function and k-relative neighborhood graph: versatile tools
  for ordinal data analysis</title><categories>stat.ML cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years it has become popular to study machine learning problems
based on ordinal distance information rather than numerical distance
measurements. By ordinal distance information we refer to binary answers to
distance comparisons such as d(A,B) &lt; d(C,D). For many problems in machine
learning and statistics it is unclear how to solve them in such a scenario. Up
to now, the main approach is to explicitly construct an ordinal embedding of
the data points in the Euclidean space, an approach that has a number of
drawbacks. In this paper, we propose algorithms for the problems of medoid
estimation, outlier identification, classification and clustering when given
only ordinal distance comparisons. They are based on estimating the lens depth
function and the k-relative neighborhood graph on a data set. Our algorithms
are simple, can easily be parallelized, avoid many of the drawbacks of an
ordinal embedding approach and are much faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07195</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07195</id><created>2016-02-23</created><authors><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author><author><keyname>Moharir</keyname><forenames>Sharayu</forenames></author></authors><title>Paging with Multiple Caches</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern content delivery networks consist of one or more back-end servers
which store the entire content catalog, assisted by multiple front-end servers
with limited storage and service capacities located near the end-users.
Appropriate replication of content on the front-end servers is key to maximize
the fraction of requests served by the front-end servers. Motivated by this, a
multiple cache variant of the classical single cache paging problem is studied,
which is referred to as the Multiple Cache Paging (MCP) problem. In each
time-slot, a batch of content requests arrive that have to be served by a bank
of caches, and each cache can serve exactly one request. If a content is not
found in the bank, it is fetched from the back-end server, and one currently
stored content is ejected, and counted as fault. As in the classical paging
problem, the goal is to minimize the total number of faults. The competitive
ratio of any online algorithm for the MCP problem is shown to be unbounded for
arbitrary input, thus concluding that the MCP problem is fundamentally
different from the classical paging problem. Consequently, stochastic arrivals
setting is considered, where requests arrive according to a known/unknown
stochastic process. It is shown that near optimal performance can be achieved
with simple policies that require no co-ordination across the caches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07199</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07199</id><created>2016-02-23</created><updated>2016-03-01</updated><authors><author><keyname>Eide</keyname><forenames>Aslak Wegner</forenames></author><author><keyname>Pickering</keyname><forenames>J. Brian</forenames></author><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Bravos</keyname><forenames>George</forenames></author><author><keyname>F&#xf8;lstad</keyname><forenames>Asbj&#xf8;rn</forenames></author><author><keyname>Engen</keyname><forenames>Vegard</forenames></author><author><keyname>Tsvetkova</keyname><forenames>Milena</forenames></author><author><keyname>Meyer</keyname><forenames>Eric T.</forenames></author><author><keyname>Walland</keyname><forenames>Paul</forenames></author><author><keyname>L&#xfc;ders</keyname><forenames>Marika</forenames></author></authors><title>Human-Machine Networks: Towards a Typology and Profiling Framework</title><categories>cs.HC cs.CY cs.SI</categories><comments>Pre-print; To be presented at the 18th International Conference on
  Human-Computer Interaction International, Toronto, Canada, 17 - 22 July 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we outline an initial typology and framework for the purpose of
profiling human-machine networks, that is, collective structures where humans
and machines interact to produce synergistic effects. Profiling a human-machine
network along the dimensions of the typology is intended to facilitate access
to relevant design knowledge and experience. In this way the profiling of an
envisioned or existing human-machine network will both facilitate relevant
design discussions and, more importantly, serve to identify the network type.
We present experiences and results from two case trials: a crisis management
system and a peer-to-peer reselling network. Based on the lessons learnt from
the case trials we suggest potential benefits and challenges, and point out
needed future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07210</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07210</id><created>2016-02-23</created><authors><author><keyname>Droschinsky</keyname><forenames>Andre</forenames></author><author><keyname>Kriege</keyname><forenames>Nils M.</forenames></author><author><keyname>Mutzel</keyname><forenames>Petra</forenames></author></authors><title>Faster Algorithms for the Maximum Common Subtree Isomorphism Problem</title><categories>cs.DS cs.CC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum common subtree isomorphism problem asks for the largest possible
isomorphism between subtrees of two given input trees. This problem is a
natural restriction of the maximum common subgraph problem, which is NP-hard in
general graphs. Confining to trees renders polynomial time algorithms possible
and is of fundamental importance for approaches on more general graph classes.
Various variants of this problem in trees have been intensively studied. We
consider the general case, where trees are neither rooted nor ordered and the
isomorphism is maximum w.r.t. a weight function on the mapped vertices and
edges. For trees of order n and maximum degree $\Delta$ our algorithm achieves
a running time of $O(n^2\Delta)$ by exploiting the structure of the matching
instances arising as subproblems. Thus our algorithm outperforms the best
previously known approaches. We further show that for trees of bounded degree
the time complexity of the problem is $\Theta(n^2)$. For trees of unbounded
degree we show that a further reduction of the running time would directly
improve the best known approach to the assignment problem. Combining a
polynomial-delay algorithm for the enumeration of all maximum common subtree
isomorphisms with central ideas of our new algorithm leads to an improvement of
its running time from $O(n^6+Tn^2)$ to $O(n^3+Tn\Delta)$, where $n$ is the
order of the larger tree, $T$ is the number of different solutions, and
$\Delta$ is the minimum of the maximum node degrees of the input trees. Our
theoretical results are supplemented by an experimental evaluation on synthetic
and real-world instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07217</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07217</id><created>2016-02-23</created><authors><author><keyname>Guisado-G&#xe1;mez</keyname><forenames>Joan</forenames></author><author><keyname>Prat-P&#xe9;rez</keyname><forenames>Arnau</forenames></author><author><keyname>Larriba-Pey</keyname><forenames>Josep Llu&#xed;s</forenames></author></authors><title>Query Expansion via structural motifs in Wikipedia Graph</title><categories>cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The search for relevant information can be very frustrating for users who,
unintentionally, use too general or inappropriate keywords to express their
requests. To overcome this situation, query expansion techniques aim at
transforming the user request by adding new terms, referred as expansion
features, that better describe the real intent of the users. We propose a
method that relies exclusively on relevant structures (as opposed to the use of
semantics) found in knowledge bases (KBs) to extract the expansion features. We
call our method Structural Query Expansion (SQE). The structural analysis of
KBs takes us to propose a set of structural motifs that connect their strongly
related entries, which can be used to extract expansion features. In this paper
we use Wikipedia as our KB, which is probably one of the largest sources of
information. SQE is capable of achieving more than 150% improvement over non
expanded queries and is able to identify the expansion features in less than
0.2 seconds in the worst case scenario. Most significantly, we believe that we
are contributing to open new research directions in query expansion, proposing
a method that is orthogonal to many current systems. For example, SQE improves
pseudo-relevance feedback techniques up to 13%
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07225</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07225</id><created>2016-02-23</created><authors><author><keyname>Simmons</keyname><forenames>David</forenames></author><author><keyname>Coon</keyname><forenames>Justin</forenames></author></authors><title>Strictly Positive and Continuous Random Fibonacci Sequences and Network
  Theory Applications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We motivate the study of a certain class of random Fibonacci sequences -
which we call continuous random Fibonacci sequences - by demonstrating that
their exponential growth rate can be used to establish capacity and power
scaling laws for multihop cooperative amplify-and-forward (AF) relay networks.
With these laws, we show that it is possible to construct multihop cooperative
AF networks that simultaneously avoid 1) exponential capacity decay and 2)
exponential transmit power growth across the network. This is achieved by
ensuring the network's Lyapunov exponent is zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07235</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07235</id><created>2016-02-23</created><updated>2016-02-26</updated><authors><author><keyname>Wettstein</keyname><forenames>Manuel</forenames></author></authors><title>Trapezoidal Diagrams, Upward Triangulations, and Prime Catalan Numbers</title><categories>cs.CG math.CO</categories><comments>Added affiliation. Fixed typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The d-dimensional Catalan numbers form a well-known sequence of numbers which
count balanced bracket expressions over an alphabet of size d. In this paper,
we introduce and study what we call d-dimensional prime Catalan numbers, a
sequence of numbers which count only a very specific subset of indecomposable
balanced bracket expressions.
  We further introduce the notion of a trapezoidal diagram of a crossing-free
geometric graph, such as a triangulation or a crossing-free perfect matching.
In essence, such a diagram is obtained by augmenting the geometric graph in
question with its trapezoidal decomposition, and then forgetting about the
precise coordinates of individual vertices while preserving the vertical
visibility relations between vertices and segments. We note that trapezoidal
diagrams of triangulations are closely related to abstract upward
triangulations.
  We study the numbers of such diagrams in the cases of (i) perfect matchings
and (ii) triangulations. We give bijective proofs which establish relations
with 3-dimensional (prime) Catalan numbers. This allows us to determine the
corresponding exponential growth rates exactly as (i) 5.196^n and (ii) 23.459^n
(bases are rounded to 3 decimal places).
  Finally, we give lower bounds for the maximum number of embeddings of a
trapezoidal diagram on any given point set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07236</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07236</id><created>2016-02-23</created><authors><author><keyname>Norris</keyname><forenames>Clayton</forenames></author></authors><title>Petrarch 2 : Petrarcher</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PETRARCH 2 is the fourth generation of a series of Event-Data coders stemming
from research by Phillip Schrodt. Each iteration has brought new functionality
and usability, and this is no exception.Petrarch 2 takes much of the power of
the original Petrarch's dictionaries and redirects it into a faster and smarter
core logic. Earlier iterations handled sentences largely as a list of words,
incorporating some syntactic information here and there. Petrarch 2 now views
the sentence entirely on the syntactic level. It receives the syntactic parse
of a sentence from the Stanford CoreNLP software, and stores this data as a
tree structure of linked nodes, where each node is a Phrase object.
Prepositional, noun, and verb phrases each have their own version of this
Phrase class, which deals with the logic particular to those kinds of phrases.
Since this is an event coder, the core of the logic focuses around the verbs:
who is acting, who is being acted on, and what is happening. The theory behind
this new structure and its logic is founded in Generative Grammar, Information
Theory, and Lambda-Calculus Semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07241</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07241</id><created>2016-02-23</created><authors><author><keyname>Clifford</keyname><forenames>Raphael</forenames></author><author><keyname>Starikovskaya</keyname><forenames>Tatiana</forenames></author></authors><title>Approximate Hamming distance in a stream</title><categories>cs.DS</categories><comments>Submitted to ICALP' 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing a $(1+\epsilon)$-approximation of the
Hamming distance between a pattern of length $n$ and successive substrings of a
stream. We first look at the one-way randomised communication complexity of
this problem, giving Alice the first half of the stream and Bob the second
half. We show the following: (1) If Alice and Bob both share the pattern then
there is an $O(\epsilon^{-4} \log^2 n)$ bit randomised one-way communication
protocol. (2) If only Alice has the pattern then there is an
$O(\epsilon^{-2}\sqrt{n}\log n)$ bit randomised one-way communication protocol.
  We then go on to develop small space streaming algorithms for
$(1+\epsilon)$-approximate Hamming distance which give worst case running time
guarantees per arriving symbol. (1) For binary input alphabets there is an
$O(\epsilon^{-3} \sqrt{n} \log^{2} n)$ space and $O(\epsilon^{-2} \log{n})$
time streaming $(1+\epsilon)$-approximate Hamming distance algorithm. (2) For
general input alphabets there is an $O(\epsilon^{-5} \sqrt{n} \log^{4} n)$
space and $O(\epsilon^{-4} \log^3 {n})$ time streaming
$(1+\epsilon)$-approximate Hamming distance algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07249</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07249</id><created>2016-02-23</created><updated>2016-03-07</updated><authors><author><keyname>Fountalis</keyname><forenames>Ilias</forenames></author><author><keyname>Bracco</keyname><forenames>Annalisa</forenames></author><author><keyname>Dilkina</keyname><forenames>Bistra</forenames></author><author><keyname>Dovrolis</keyname><forenames>Constantine</forenames></author><author><keyname>Keilholz</keyname><forenames>Shella</forenames></author></authors><title>{\delta}-MAPS: From spatio-temporal data to a weighted and lagged
  network between functional domains</title><categories>cs.OH</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose {\delta}-MAPS, a method that analyzes spatio-temporal data to
first identify the distinct spatial components of the underlying system,
referred to as &quot;domains&quot;, and second to infer the connections between them. A
domain is a spatially contiguous region of highly correlated temporal activity.
The core of a domain is a point or subregion at which a metric of local
homogeneity is maximum across the entire domain. We compute a domain as the
maximum-sized set of spatially contiguous cells that include the detected core
and satisfy a homogeneity constraint, expressed in terms of the average
pairwise cross-correlation across all cells in the domain. Domains may be
spatially overlapping. Different domains may have correlated activity,
potentially at a lag, because of direct or indirect interactions. The proposed
edge inference method examines the statistical significance of each lagged
cross-correlation between two domains, infers a range of lag values for each
edge, and assigns a weight to each edge based on the covariance of the two
domains. We illustrate the application of {\delta}-MAPS on data from two
domains: climate science and neuroscience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07250</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07250</id><created>2016-02-23</created><authors><author><keyname>Ugur</keyname><forenames>Yigit</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ali Ozgur</forenames></author></authors><title>Reducing MIMO Detection Complexity via Hierarchical Modulation</title><categories>cs.IT math.IT</categories><comments>4 pages, 5 figures, submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers multiple-input multiple-output (MIMO) communication
systems using hierarchical modulation. A disadvantage of the maximum-likelihood
(ML) MIMO detector is that computational complexity increases exponentially
with the number of transmit antennas. To reduce complexity, we propose a
hierarchical modulation scheme to be used in MIMO trans- mission where base and
enhancement layers are incorporated. In the proposed receiver, the base layer
is detected first with a minimum mean square error (MMSE) detector which is
followed by ML detection of the enhancement layer. Our results indicate that
the proposed low complexity scheme does not compromise performance when design
parameters such as code rates and constellation ratio are chosen carefully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07255</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07255</id><created>2016-02-23</created><authors><author><keyname>You</keyname><forenames>Lei</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author></authors><title>Load Optimization with User Association in Cooperative and Load-Coupled
  LTE Networks</title><categories>cs.IT math.IT</categories><comments>13 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We extend the problem of optimizing user association for load balancing in
cellular networks along two dimensions. First, we consider joint transmission
(JT), which is one of the coordinated multipoint (CoMP) techniques, with which
a user may be simultaneously served by multiple base stations. Second, we
account for, mathematically, the coupling relation between the base stations'
load levels; these variables correspond to the amount of time-frequency
resource units (RUs) used for transmission, and they are dependent on each
other due to inter-cell interference. We formulate two optimization problems,
sum load minimization (MinSumL) and maximum load minimization (MinMaxL). We
prove that both MinSumL and MinMaxL are NP-hard. We propose a mixed integer
linear programming (MILP) based scheme by means of linearization. This approach
also leads to a bounding scheme to MinSumL and MinMaxL, for performance
benchmarking. As a second solution approach, we derive a set of partial
optimality conditions. Fulfillment of the conditions will guarantees
performance improvement for both MinSumL and MinMaxL. A solution algorithm is
then derived based on the conditions. Simulation results are provided to
demonstrate the effectiveness of the solution approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07259</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07259</id><created>2016-02-23</created><authors><author><keyname>Kulikov</keyname><forenames>Sergey B.</forenames></author></authors><title>Philosophical Fictionalism and Problem of Artificial Intelligence</title><categories>cs.OH</categories><comments>15 pages, in Russian</comments><msc-class>00A30</msc-class><journal-ref>Philosophical Problems of Information Technologies and Cyberspace.
  2015. 10 (2): 42-57</journal-ref><doi>10.17726/philIT.2015.10.2.811.93</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The artificial intelligence received broad interpretation as a literary
image. This approach did not have unambiguous refering to the scopes of logical
studies and mathematical investigations. An author applied methods peculiar to
the semiotic approach, offered by Boris Uspensky and Yury Lotman. In addition,
the article presented the criticism of modern versions of educational
technologies, which led to the unconditional expectations for possibilities of
information and telecommunication technologies. Methodological culture's
growth, which was described on the base of semiotics and functional approach to
word formation of new meanings for the description of the studied subjects,
provided the development of pupils' thought. As a result, the research opened
new prospects on understanding of artificial intelligence within educational
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07261</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07261</id><created>2016-02-23</created><authors><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author><author><keyname>Ioffe</keyname><forenames>Sergey</forenames></author><author><keyname>Vanhoucke</keyname><forenames>Vincent</forenames></author></authors><title>Inception-v4, Inception-ResNet and the Impact of Residual Connections on
  Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Very deep convolutional networks have been central to the largest advances in
image recognition performance in recent years. One example is the Inception
architecture that has been shown to achieve very good performance at relatively
low computational cost. Recently, the introduction of residual connections in
conjunction with a more traditional architecture has yielded state-of-the-art
performance in the 2015 ILSVRC challenge; its performance was similar to the
latest generation Inception-v3 network. This raises the question of whether
there are any benefit in combining the Inception architecture with residual
connections. Here we give clear empirical evidence that training with residual
connections accelerates the training of Inception networks significantly. There
is also some evidence of residual Inception networks outperforming similarly
expensive Inception networks without residual connections by a thin margin. We
also present several new streamlined architectures for both residual and
non-residual Inception networks. These variations improve the single-frame
recognition performance on the ILSVRC 2012 classification task significantly.
We further demonstrate how proper activation scaling stabilizes the training of
very wide residual Inception networks. With an ensemble of three residual and
one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the
ImageNet classification (CLS) challenge
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07264</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07264</id><created>2015-05-14</created><authors><author><keyname>Crocetti</keyname><forenames>Giancarlo</forenames></author><author><keyname>Coakley</keyname><forenames>Michael</forenames></author><author><keyname>Dressner</keyname><forenames>Phil</forenames></author><author><keyname>Kellum</keyname><forenames>Wanda</forenames></author><author><keyname>Lamin</keyname><forenames>Tamba</forenames></author></authors><title>A Multivariate Biomarker for Parkinson's Disease</title><categories>cs.LG</categories><comments>5 pages, 4 figures, 3 tables, published at the Research Day at Pace
  University, New York. Proceedings of 12th Annual Research Day, 2014 - Pace
  University</comments><acm-class>H.2.8; I.5.3; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we executed a genomic analysis with the objective of selecting
a set of genes (possibly small) that would help in the detection and
classification of samples from patients affected by Parkinson Disease. We
performed a complete data analysis and during the exploratory phase, we
selected a list of differentially expressed genes. Despite their association
with the diseased state, we could not use them as a biomarker tool. Therefore,
our research was extended to include a multivariate analysis approach resulting
in the identification and selection of a group of 20 genes that showed a clear
potential in detecting and correctly classify Parkinson Disease samples even in
the presence of other neurodegenerative disorders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07265</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07265</id><created>2016-02-23</created><authors><author><keyname>Beygelzimer</keyname><forenames>Alina</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Zhang</keyname><forenames>Chicheng</forenames></author></authors><title>Search Improves Label for Active Learning</title><categories>cs.LG stat.ML</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate active learning with access to two distinct oracles: Label
(which is standard) and Search (which is not). The Search oracle models the
situation where a human searches a database to seed or counterexample an
existing solution. Search is stronger than Label while being natural to
implement in many situations. We show that an algorithm using both oracles can
provide exponentially large problem-dependent improvements over Label alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07267</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07267</id><created>2016-02-23</created><authors><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author></authors><title>Towards a closure operator for enumeration of maximal tricliques in
  tripartite hypergraphs</title><categories>cs.DM</categories><comments>Draft for spec. issue of DAM (2015)</comments><msc-class>06B99, 06A15</msc-class><acm-class>G.2.2; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Triadic Formal Concept Analysis (3FCA) was introduced by Lehman and Wille
almost two decades ago, but up-to-date even though that different researchers
actively work on this FCA branch, a proper closure operator for enumeration of
triconcepts, i.e. maximal triadic cliques of tripartite hypergaphs, was not
introduced. In this paper we show that the previously introduced operators for
obtaining triconcepts and maximal connected and complete sets (MCCS) are not
always consistent and provide the reader with a definition of valid closure
operator and associated set system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07273</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07273</id><created>2016-02-23</created><authors><author><keyname>Beuchat</keyname><forenames>Paul</forenames></author><author><keyname>Georghiou</keyname><forenames>Angelos</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Approximate Dynamic Programming: a $\mathcal{Q}$-Function Approach</title><categories>cs.SY</categories><comments>15 pages, 5 figures, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study both the value function and $\mathcal{Q}$-function
formulation of the Linear Programming (LP) approach to ADP. The approach
selects from a restricted function space to fit an approximate solution to the
true optimal Value function and $\mathcal{Q}$-function. Working in the
discrete-time, continuous-space setting, we extend and prove guarantees for the
fitting error and online performance of the policy, providing tighter bounds.
We provide also a condition that allows the $\mathcal{Q}$-function approach to
be more efficiently formulated in many practical cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07275</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07275</id><created>2016-02-22</created><authors><author><keyname>Prado</keyname><forenames>Sandra D.</forenames></author><author><keyname>Dahmen</keyname><forenames>Silvio R.</forenames></author><author><keyname>Bazzan</keyname><forenames>Ana L. C.</forenames></author><author><keyname>Mac Carron</keyname><forenames>Padraig</forenames></author><author><keyname>Kenna</keyname><forenames>Ralph</forenames></author></authors><title>Temporal Network Analysis of Literary Texts</title><categories>physics.soc-ph cs.CL</categories><comments>17 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study temporal networks of characters in literature focusing on &quot;Alice's
Adventures in Wonderland&quot; (1865) by Lewis Carroll and the anonymous &quot;La Chanson
de Roland&quot; (around 1100). The former, one of the most influential pieces of
nonsense literature ever written, describes the adventures of Alice in a
fantasy world with logic plays interspersed along the narrative. The latter, a
song of heroic deeds, depicts the Battle of Roncevaux in 778 A.D. during
Charlemagne's campaign on the Iberian Peninsula. We apply methods recently
developed by Taylor and coworkers \cite{Taylor+2015} to find time-averaged
eigenvector centralities, Freeman indices and vitalities of characters. We show
that temporal networks are more appropriate than static ones for studying
stories, as they capture features that the time-independent approaches fail to
yield.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07280</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07280</id><created>2016-02-22</created><authors><author><keyname>Sengupta</keyname><forenames>Abhishek</forenames></author><author><keyname>Rajan</keyname><forenames>Vaibhav</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Sakyajit</forenames></author><author><keyname>Sarma</keyname><forenames>G R K</forenames></author></authors><title>A Statistical Model for Stroke Outcome Prediction and Treatment Planning</title><categories>stat.AP cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stroke is a major cause of mortality and long--term disability in the world.
Predictive outcome models in stroke are valuable for personalized treatment,
rehabilitation planning and in controlled clinical trials. In this paper we
design a new model to predict outcome in the short-term, the putative
therapeutic window for several treatments. Our regression-based model has a
parametric form that is designed to address many challenges common in medical
datasets like highly correlated variables and class imbalance. Empirically our
model outperforms the best--known previous models in predicting short--term
outcomes and in inferring the most effective treatments that improve outcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07285</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07285</id><created>2016-02-23</created><authors><author><keyname>Singh</keyname><forenames>Rohit</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author></authors><title>Automatic Generation of Formula Simplifiers based on Conditional Rewrite
  Rules</title><categories>cs.PL</categories><comments>Submitted for peer reviewed conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the problem of creating simplifiers for logic formulas
based on conditional term rewriting. In particular, the paper focuses on a
program synthesis application where formula simplifications have been shown to
have a significant impact. We show that by combining machine learning
techniques with constraint-based synthesis, it is possible to synthesize a
formula simplifier fully automatically from a corpus of representative
problems, making it possible to create formula simplifiers tailored to specific
problem domains. We demonstrate the benefits of our approach for synthesis
benchmarks from the SyGuS competition and automated grading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07291</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07291</id><created>2016-02-23</created><authors><author><keyname>Sadjadi</keyname><forenames>Seyed Omid</forenames></author><author><keyname>Ganapathy</keyname><forenames>Sriram</forenames></author><author><keyname>Pelecanos</keyname><forenames>Jason W.</forenames></author></authors><title>The IBM 2016 Speaker Recognition System</title><categories>cs.SD cs.CL stat.ML</categories><comments>Submitted to Odyssey 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe the recent advancements made in the IBM i-vector
speaker recognition system for conversational speech. In particular, we
identify key techniques that contribute to significant improvements in
performance of our system, and quantify their contributions. The techniques
include: 1) a nearest-neighbor discriminant analysis (NDA) approach that is
formulated to alleviate some of the limitations associated with the
conventional linear discriminant analysis (LDA) that assumes Gaussian
class-conditional distributions, 2) the application of speaker- and
channel-adapted features, which are derived from an automatic speech
recognition (ASR) system, for speaker recognition, and 3) the use of a deep
neural network (DNN) acoustic model with a large number of output units (~10k
senones) to compute the frame-level soft alignments required in the i-vector
estimation process. We evaluate these techniques on the NIST 2010 speaker
recognition evaluation (SRE) extended core conditions involving telephone and
microphone trials. Experimental results indicate that: 1) the NDA is more
effective (up to 35% relative improvement in terms of EER) than the traditional
parametric LDA for speaker recognition, 2) when compared to raw acoustic
features (e.g., MFCCs), the ASR speaker-adapted features provide gains in
speaker recognition performance, and 3) increasing the number of output units
in the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k)
provides consistent improvements in performance (for example from 37% to 57%
relative EER gains over our baseline GMM i-vector system). To our knowledge,
results reported in this paper represent the best performances published to
date on the NIST SRE 2010 extended core tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07320</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07320</id><created>2016-02-23</created><authors><author><keyname>Lipton</keyname><forenames>Zachary C.</forenames></author></authors><title>Stuck in a What? Adventures in Weight Space</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning researchers commonly suggest that converged models are stuck in
local minima. More recently, some researchers observed that under reasonable
assumptions, the vast majority of critical points are saddle points, not true
minima. Both descriptions suggest that weights converge around a point in
weight space, be it a local optima or merely a critical point. However, it's
possible that neither interpretation is accurate. As neural networks are
typically over-complete, it's easy to show the existence of vast continuous
regions through weight space with equal loss. In this paper, we build on recent
work empirically characterizing the error surfaces of neural networks. We
analyze training paths through weight space, presenting evidence that apparent
convergence of loss does not correspond to weights arriving at critical points,
but instead to large movements through flat regions of weight space. While it's
trivial to show that neural network error surfaces are globally non-convex, we
show that error surfaces are also locally non-convex, even after breaking
symmetry with a random initialization and also after partial training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07324</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07324</id><created>2016-02-23</created><authors><author><keyname>Lee</keyname><forenames>Joonbum</forenames></author><author><keyname>Mu&#xf1;oz</keyname><forenames>Mauricio</forenames></author><author><keyname>Fridman</keyname><forenames>Lex</forenames></author><author><keyname>Victor</keyname><forenames>Trent</forenames></author><author><keyname>Reimer</keyname><forenames>Bryan</forenames></author><author><keyname>Mehler</keyname><forenames>Bruce</forenames></author></authors><title>Investigating Drivers' Head and Glance Correspondence</title><categories>cs.OH</categories><comments>27 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relationship between a driver's glance pattern and corresponding head
rotation is highly complex due to its nonlinear dependence on the individual,
task, and driving context. This study explores the ability of head pose to
serve as an estimator for driver gaze by connecting head rotation data with
manually coded gaze region data using both a statistical analysis approach and
a predictive (i.e., machine learning) approach. For the latter, classification
accuracy increased as visual angles between two glance locations increased. In
other words, the greater the shift in gaze, the higher the accuracy of
classification. This is an intuitive but important concept that we make
explicit through our analysis. The highest accuracy achieved was 83% using the
method of Hidden Markov Models (HMM) for the binary gaze classification problem
of (1) the forward roadway versus (2) the center stack. Results suggest that
although there are individual differences in head-glance correspondence while
driving, classifier models based on head-rotation data may be robust to these
differences and therefore can serve as reasonable estimators for glance
location. The results suggest that driver head pose can be used as a surrogate
for eye gaze in several key conditions including the identification of
high-eccentricity glances. Inexpensive driver head pose tracking may be a key
element in detection systems developed to mitigate driver distraction and
inattention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07332</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07332</id><created>2016-02-23</created><authors><author><keyname>Krishna</keyname><forenames>Ranjay</forenames></author><author><keyname>Zhu</keyname><forenames>Yuke</forenames></author><author><keyname>Groth</keyname><forenames>Oliver</forenames></author><author><keyname>Johnson</keyname><forenames>Justin</forenames></author><author><keyname>Hata</keyname><forenames>Kenji</forenames></author><author><keyname>Kravitz</keyname><forenames>Joshua</forenames></author><author><keyname>Chen</keyname><forenames>Stephanie</forenames></author><author><keyname>Kalantidis</keyname><forenames>Yannis</forenames></author><author><keyname>Li</keyname><forenames>Li-Jia</forenames></author><author><keyname>Shamma</keyname><forenames>David A.</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael S.</forenames></author><author><keyname>Li</keyname><forenames>Fei-Fei</forenames></author></authors><title>Visual Genome: Connecting Language and Vision Using Crowdsourced Dense
  Image Annotations</title><categories>cs.CV cs.AI</categories><comments>44 pages, 37 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite progress in perceptual tasks such as image classification, computers
still perform poorly on cognitive tasks such as image description and question
answering. Cognition is core to tasks that involve not just recognizing, but
reasoning about our visual world. However, models used to tackle the rich
content in images for cognitive tasks are still being trained using the same
datasets designed for perceptual tasks. To achieve success at cognitive tasks,
models need to understand the interactions and relationships between objects in
an image. When asked &quot;What vehicle is the person riding?&quot;, computers will need
to identify the objects in an image as well as the relationships riding(man,
carriage) and pulling(horse, carriage) in order to answer correctly that &quot;the
person is riding a horse-drawn carriage&quot;.
  In this paper, we present the Visual Genome dataset to enable the modeling of
such relationships. We collect dense annotations of objects, attributes, and
relationships within each image to learn these models. Specifically, our
dataset contains over 100K images where each image has an average of 21
objects, 18 attributes, and 18 pairwise relationships between objects. We
canonicalize the objects, attributes, relationships, and noun phrases in region
descriptions and questions answer pairs to WordNet synsets. Together, these
annotations represent the densest and largest dataset of image descriptions,
objects, attributes, relationships, and question answers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07335</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07335</id><created>2015-07-25</created><authors><author><keyname>Mishra</keyname><forenames>Minati</forenames></author><author><keyname>Adhikary</keyname><forenames>M. C.</forenames></author></authors><title>Robust Detection of Intensity Variant Clones in Forged and JPEG
  Compressed Images</title><categories>cs.CV</categories><comments>page 48-60</comments><journal-ref>ANSVESA, 9(1), 48-60, 2014, ISSN-0974-715X</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digitization of images has made image editing easier. Ease of image editing
tempted users and professionals to manipulate digital images leading to digital
image forgeries. Today digital image forgery has posed a great threat to the
authenticity of the popular digital media, the digital images. A lot of
research is going on worldwide to detect image forgery and to separate the
forged images from their authentic counterparts. This paper provides a novel
intensity invariant detection model (IIDM) for detection of intensity variant
clones that is robust against JPEG compression, noise attacks and blurring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07337</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07337</id><created>2016-02-21</created><updated>2016-02-25</updated><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Deng</keyname><forenames>Xinwei</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Sparse Estimation of Multivariate Poisson Log-Normal Model and Inverse
  Covariance for Count Data</title><categories>stat.ME cs.LG</categories><comments>The conference version of the paper is submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling data with multivariate count responses is a challenge problem due to
the discrete nature of the responses. Such data are commonly observed in many
applications such as biology, epidemiology and social studies. The existing
methods for univariate count response cannot be easily extended to the
multivariate situation since the dependency among multiple responses needs to
be properly accommodated. In this paper, we propose a multivariate Poisson
Log-Normal regression model for multivariate data with count responses. An
efficient Monte Carlo EM algorithm is also developed to facilitate the model
estimation. By simultaneously estimating the regression coefficients and
inverse covariance matrix over the latent variables, the proposed regression
model takes advantages of association among multiple count responses to improve
the model prediction performance. Simulation studies are conducted to
systematically evaluate the performance of the proposed method in comparison
with conventional methods. The proposed method is further used to analyze a
real influenza-like illness data, showing accurate prediction and forecasting
with meaningful interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07338</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07338</id><created>2015-01-16</created><authors><author><keyname>Guo</keyname><forenames>Dongwei</forenames></author><author><keyname>Wang</keyname><forenames>Shasha</forenames></author><author><keyname>Wei</keyname><forenames>Zhibo</forenames></author><author><keyname>Wang</keyname><forenames>Siwen</forenames></author><author><keyname>Hong</keyname><forenames>Yan</forenames></author></authors><title>The effect of social welfare system based on the complex network</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the passage of time, the development of communication technology and
transportation broke the isolation among people. Relationship tends to be
complicated, pluralism, dynamism. In the network where interpersonal
relationship and evolved complex net based on game theory work serve
respectively as foundation architecture and theoretical model, with the
combination of game theory and regard public welfare as influencing factor, we
artificially initialize that closed network system. Through continual loop
operation of the program, we summarize the changing rule of the cooperative
behavior in the interpersonal relationship, so that we can analyze the policies
about welfare system about whole network and the relationship of frequency of
betrayal in cooperative behavior. Most analytical data come from some simple
investigations and some estimates based on internet and environment and the
study put emphasis on simulating social network and analyze influence of social
welfare system on Cooperative Behavio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07340</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07340</id><created>2015-04-16</created><authors><author><keyname>Davies</keyname><forenames>Krispin</forenames></author><author><keyname>Ramirez-Serrano</keyname><forenames>Alejandro</forenames></author></authors><title>A Reconfigurable USAR Robot Designed for Traversing Complex 3D Terrain</title><categories>cs.RO</categories><journal-ref>Proceedings of 22nd Canadian Congress of Applied Mechanics
  (CANCAM2009), vol. 1, pp. 209-210</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of robotics in Urban Search and Rescue (USAR) is growing steadily
from their initial inception during the 2001 World Trade Centre incident.
Despite years of progress, the core design of robots currently in use for USAR
purposes has deviated little, favoring software and control development and
optimization of the basic robot template to improve performance instead.
Presented here is a novel design description of the Cricket, an advanced robot
with a broader range of physical capabilities than traditional USAR robots. By
incorporating the tracked structure of earlier robots, appreciated for energy
efficiency and robustness, into a multi-limbed walking design, the Cricket
enables the use of advanced locomotion techniques. The ability to climb over
obstacles many times the height of the robot, ascend vertical shafts without
the assistance of a tether, and traverse rough and near vertical terrain
improves the Cricket's capability to successfully locate victims in confined
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07348</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07348</id><created>2016-02-23</created><authors><author><keyname>Sommars</keyname><forenames>Jeff</forenames></author><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author></authors><title>Computing Pretropisms for the Cyclic n-Roots Problem</title><categories>cs.CG</categories><comments>Accepted for EuroCG 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cyclic n-roots problem is an important benchmark problem for polynomial
system solvers. We consider the pruning of cone intersections for a polyhedral
method to compute series for the solution curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07349</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07349</id><created>2016-02-23</created><authors><author><keyname>Barfuss</keyname><forenames>Wolfram</forenames></author><author><keyname>Massara</keyname><forenames>Guido Previde</forenames></author><author><keyname>Di Matteo</keyname><forenames>T.</forenames></author><author><keyname>Aste</keyname><forenames>Tomaso</forenames></author></authors><title>Parsimonious modeling with Information Filtering Networks</title><categories>cs.IT math.IT stat.ML</categories><comments>11 pages, 4 figures, 3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a methodology to construct sparse models from data by using
information filtering networks as inference structure. This method is
computationally very efficient and statistically robust because it is based
{on} local, low-dimensional, inversions of the covariance matrix to generate a
global sparse inverse. Compared with state-of-the-art methodologies such as
lasso, our method is computationally more efficient producing in a fraction of
computation time models that have equivalent or better performances but with a
sparser and more meaningful inference structure. The local nature of this
approach allow{s} dynamical partial updating when the properties of some
variables change without the need of recomputing the whole model. We discuss
performances with financial data and financial applications to prediction,
stress testing and risk allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07351</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07351</id><created>2016-02-23</created><authors><author><keyname>Basu</keyname><forenames>Amitabh</forenames></author><author><keyname>Dinitz</keyname><forenames>Michael</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Computing approximate PSD factorizations</title><categories>cs.DS math.OC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm for computing approximate PSD factorizations of
nonnegative matrices. The running time of the algorithm is polynomial in the
dimensions of the input matrix, but exponential in the PSD rank and the
approximation error. The main ingredient is an exact factorization algorithm
when the rows and columns of the factors are constrained to lie in a general
polyhedron. This strictly generalizes nonnegative matrix factorizations which
can be captured by letting this polyhedron to be the nonnegative orthant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07360</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07360</id><created>2016-02-23</created><updated>2016-02-27</updated><authors><author><keyname>Iandola</keyname><forenames>Forrest N.</forenames></author><author><keyname>Moskewicz</keyname><forenames>Matthew W.</forenames></author><author><keyname>Ashraf</keyname><forenames>Khalid</forenames></author><author><keyname>Han</keyname><forenames>Song</forenames></author><author><keyname>Dally</keyname><forenames>William J.</forenames></author><author><keyname>Keutzer</keyname><forenames>Kurt</forenames></author></authors><title>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;1MB
  model size</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research on deep neural networks has focused primarily on improving
accuracy. For a given accuracy level, is typically possible to identify
multiple DNN architectures that achieve that accuracy level. With equivalent
accuracy, smaller DNN architectures offer at least three advantages: (1)
Smaller DNNs require less communication across servers during distributed
training. (2) Smaller DNNs require less bandwidth to export a new model from
the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on
FPGAs and other hardware with limited memory. To provide all of these
advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet
achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.
Additionally, with model compression techniques we are able to compress
SqueezeNet to less than 1MB (461x smaller than AlexNet).
  The SqueezeNet architecture is available for download here:
https://github.com/DeepScale/SqueezeNet
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07362</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07362</id><created>2016-02-23</created><authors><author><keyname>Cummings</keyname><forenames>Rachel</forenames></author><author><keyname>Pennock</keyname><forenames>David M.</forenames></author><author><keyname>Vaughan</keyname><forenames>Jennifer Wortman</forenames></author></authors><title>The Possibilities and Limitations of Private Prediction Markets</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the design of private prediction markets, financial markets
designed to elicit predictions about uncertain events without revealing too
much information about market participants' actions or beliefs. Our goal is to
design market mechanisms in which participants' trades or wagers influence the
market's behavior in a way that leads to accurate predictions, yet no single
participant has too much influence over what others are able to observe. We
study the possibilities and limitations of such mechanisms using tools from
differential privacy. We begin by designing a private one-shot wagering
mechanism in which bettors specify a belief about the likelihood of a future
event and a corresponding monetary wager. Wagers are redistributed among
bettors in a way that more highly rewards those with accurate predictions. We
provide a class of wagering mechanisms that are guaranteed to satisfy
truthfulness, budget balance in expectation, and other desirable properties
while additionally guaranteeing epsilon-joint differential privacy in the
bettors' reported beliefs, and analyze the trade-off between the achievable
level of privacy and the sensitivity of a bettor's payment to her own report.
We then ask whether it is possible to obtain privacy in dynamic prediction
markets, focusing our attention on the popular cost-function framework in which
securities with payments linked to future events are bought and sold by an
automated market maker. We show that under general conditions, it is impossible
for such a market maker to simultaneously achieve bounded worst-case loss and
epsilon-differential privacy without allowing the privacy guarantee to degrade
extremely quickly as the number of trades grows, making such markets
impractical in settings in which privacy is valued. We conclude by suggesting
several avenues for potentially circumventing this lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07364</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07364</id><created>2016-02-23</created><authors><author><keyname>Moll&#xe9;n</keyname><forenames>Christopher</forenames></author><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Performance of the Wideband Massive Uplink MIMO with One-Bit ADCs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analog-to-digital converters (ADCs) consume a significant part of the total
power in a massive MIMO base station. One-bit ADCs are one way to reduce power
consumption. This paper presents an analysis of the spectral efficiency of
single-carrier and OFDM transmission in massive MIMO systems that use one-bit
ADCs. A closed-form achievable rate is derived for a wideband system with a
large number of channel taps, assuming linear channel estimation and symbol
detection. Quantization results in two types of error in the symbol detection.
The circularly symmetric error becomes Gaussian in massive MIMO and vanishes as
the number of antennas grows. The amplitude distortion, which severely degrades
the performance of OFDM, is caused by variations in received interference
energy between symbol durations. As the number of channel taps grows, the
amplitude distortion vanishes and OFDM has the same performance as
single-carrier transmission. A main conclusion of this paper is that wideband
massive MIMO systems work well with one-bit ADCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07365</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07365</id><created>2016-02-23</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Constrained Generalized Delaunay Graphs Are Plane Spanners</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We look at generalized Delaunay graphs in the constrained setting by
introducing line segments which the edges of the graph are not allowed to
cross. Given an arbitrary convex shape $C$, a constrained Delaunay graph is
constructed by adding an edge between two vertices $p$ and $q$ if and only if
there exists a homothet of $C$ with $p$ and $q$ on its boundary that does not
contain any other vertices visible to $p$ and $q$. We show that, regardless of
the convex shape $C$ used to construct the constrained Delaunay graph, there
exists a constant $t$ (that depends on $C$) such that it is a plane
$t$-spanner. Furthermore, we reduce the upper bound on the spanning ratio for
the special case where the empty convex shape is an arbitrary rectangle to
$\sqrt{2} \cdot \left( 2 l/s + 1 \right)$, where $l$ and $s$ are the length of
the long and short side of the rectangle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07366</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07366</id><created>2016-02-23</created><authors><author><keyname>Wang</keyname><forenames>Weidong</forenames></author><author><keyname>Wang</keyname><forenames>Liqiang</forenames></author><author><keyname>Lu</keyname><forenames>Wei</forenames></author></authors><title>An Intelligent QoS Identification for Untrustworthy Web Services Via
  Two-phase Neural Networks</title><categories>cs.SE</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  QoS identification for untrustworthy Web services is critical in QoS
management in the service computing since the performance of untrustworthy Web
services may result in QoS downgrade. The key issue is to intelligently learn
the characteristics of trustworthy Web services from different QoS levels, then
to identify the untrustworthy ones according to the characteristics of QoS
metrics. As one of the intelligent identification approaches, deep neural
network has emerged as a powerful technique in recent years. In this paper, we
propose a novel two-phase neural network model to identify the untrustworthy
Web services. In the first phase, Web services are collected from the published
QoS dataset. Then, we design a feedforward neural network model to build the
classifier for Web services with different QoS levels. In the second phase, we
employ a probabilistic neural network (PNN) model to identify the untrustworthy
Web services from each classification. The experimental results show the
proposed approach has 90.5% identification ratio far higher than other
competing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07373</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07373</id><created>2016-02-23</created><authors><author><keyname>Wang</keyname><forenames>Song</forenames></author><author><keyname>Ren</keyname><forenames>Dongchun</forenames></author><author><keyname>Chen</keyname><forenames>Li</forenames></author><author><keyname>Fan</keyname><forenames>Wei</forenames></author><author><keyname>Sun</keyname><forenames>Jun</forenames></author><author><keyname>Naoi</keyname><forenames>Satoshi</forenames></author></authors><title>On Study of the Binarized Deep Neural Network for Image Classification</title><categories>cs.NE cs.CV cs.LG</categories><comments>9 pages, 6 figures. Rejected conference (CVPR 2015) submission.
  Submission date: November, 2014. This work is patented in China (NO.
  201410647710.3)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the deep neural network (derived from the artificial neural
network) has attracted many researchers' attention by its outstanding
performance. However, since this network requires high-performance GPUs and
large storage, it is very hard to use it on individual devices. In order to
improve the deep neural network, many trials have been made by refining the
network structure or training strategy. Unlike those trials, in this paper, we
focused on the basic propagation function of the artificial neural network and
proposed the binarized deep neural network. This network is a pure binary
system, in which all the values and calculations are binarized. As a result,
our network can save a lot of computational resource and storage. Therefore, it
is possible to use it on various devices. Moreover, the experimental results
proved the feasibility of the proposed network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07377</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07377</id><created>2016-02-23</created><authors><author><keyname>Khorrami</keyname><forenames>Pooya</forenames></author><author><keyname>Paine</keyname><forenames>Tom Le</forenames></author><author><keyname>Brady</keyname><forenames>Kevin</forenames></author><author><keyname>Dagli</keyname><forenames>Charlie</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>How Deep Neural Networks Can Improve Emotion Recognition on Video Data</title><categories>cs.CV</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of dimensional emotion recognition on video data using
deep learning. While several previous methods have shown the benefits of
training temporal neural network models such as recurrent neural networks
(RNNs) on hand-crafted features, few works have considered combining
convolutional neural networks (CNNs) with RNNs. In this work, we present a
system that performs emotion recognition on video data using both CNNs and
RNNs, and we also analyze how much each neural network component contributes to
the system's overall performance. We present our findings on videos from the
Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the
effects of several hyperparameters on overall performance while also achieving
superior performance to the baseline and other competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07383</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07383</id><created>2016-02-23</created><authors><author><keyname>Ding</keyname><forenames>Weiguang</forenames></author><author><keyname>Taylor</keyname><forenames>Graham</forenames></author></authors><title>Automatic Moth Detection from Trap Images for Pest Management</title><categories>cs.CV cs.LG cs.NE</categories><comments>Preprints accepted by Computers and electronics in agriculture</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monitoring the number of insect pests is a crucial component in
pheromone-based pest management systems. In this paper, we propose an automatic
detection pipeline based on deep learning for identifying and counting pests in
images taken inside field traps. Applied to a commercial codling moth dataset,
our method shows promising performance both qualitatively and quantitatively.
Compared to previous attempts at pest detection, our approach uses no
pest-specific engineering which enables it to adapt to other species and
environments with minimal human effort. It is amenable to implementation on
parallel hardware and therefore capable of deployment in settings where
real-time performance is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07387</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07387</id><created>2016-02-23</created><updated>2016-03-02</updated><authors><author><keyname>Kairouz</keyname><forenames>Peter</forenames></author><author><keyname>Bonawitz</keyname><forenames>Keith</forenames></author><author><keyname>Ramage</keyname><forenames>Daniel</forenames></author></authors><title>Discrete Distribution Estimation under Local Privacy</title><categories>stat.ML cs.LG</categories><comments>23 pages, 12 figures, submitted to ICML 2016 (under review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The collection and analysis of user data drives improvements in the app and
web ecosystems, but comes with risks to privacy. This paper examines discrete
distribution estimation under local privacy, a setting wherein service
providers can learn the distribution of a categorical statistic of interest
without collecting the underlying data. We present new mechanisms, including
hashed K-ary Randomized Response (KRR), that empirically meet or exceed the
utility of existing mechanisms at all privacy levels. New theoretical results
demonstrate the order-optimality of KRR and the existing RAPPOR mechanism at
different privacy regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07388</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07388</id><created>2016-02-23</created><authors><author><keyname>Burghardt</keyname><forenames>Keith</forenames></author><author><keyname>Alsina</keyname><forenames>Emanuel F.</forenames></author><author><keyname>Girvan</keyname><forenames>Michelle</forenames></author><author><keyname>Rand</keyname><forenames>William</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>The Myopia of Crowds: A Study of Collective Evaluation on Stack Exchange</title><categories>cs.HC cs.CY cs.LG physics.soc-ph</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowds can often make better decisions than individuals or small groups of
experts by leveraging their ability to aggregate diverse information. Question
answering sites, such as Stack Exchange, rely on the &quot;wisdom of crowds&quot; effect
to identify the best answers to questions asked by users. We analyze data from
250 communities on the Stack Exchange network to pinpoint factors affecting
which answers are chosen as the best answers. Our results suggest that, rather
than evaluate all available answers to a question, users rely on simple
cognitive heuristics to choose an answer to vote for or accept. These cognitive
heuristics are linked to an answer's salience, such as the order in which it is
listed and how much screen space it occupies. While askers appear to depend
more on heuristics, compared to voting users, when choosing an answer to accept
as the most helpful one, voters use acceptance itself as a heuristic: they are
more likely to choose the answer after it is accepted than before that very
same answer was accepted. These heuristics become more important in explaining
and predicting behavior as the number of available answers increases. Our
findings suggest that crowd judgments may become less reliable as the number of
answers grow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07393</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07393</id><created>2016-02-23</created><authors><author><keyname>Ge</keyname><forenames>Zhenhao</forenames></author><author><keyname>Sun</keyname><forenames>Yufang</forenames></author></authors><title>Domain Specific Author Attribution Based on Feedforward Neural Network
  Language Models</title><categories>cs.CL cs.LG cs.NE</categories><comments>International Conference on Pattern Recognition Application and
  Methods (ICPRAM) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authorship attribution refers to the task of automatically determining the
author based on a given sample of text. It is a problem with a long history and
has a wide range of application. Building author profiles using language models
is one of the most successful methods to automate this task. New language
modeling methods based on neural networks alleviate the curse of dimensionality
and usually outperform conventional N-gram methods. However, there have not
been much research applying them to authorship attribution. In this paper, we
present a novel setup of a Neural Network Language Model (NNLM) and apply it to
a database of text samples from different authors. We investigate how the NNLM
performs on a task with moderate author set size and relatively limited
training and test data, and how the topics of the text samples affect the
accuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of
fitness of a trained language model to the test data. Given 5 random test
sentences, it also increases the author classification accuracy by 3.43% on
average, compared with the N-gram methods using SRILM tools. An open source
implementation of our methodology is freely available at
https://github.com/zge/authorship-attribution/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07394</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07394</id><created>2016-02-23</created><authors><author><keyname>Ge</keyname><forenames>Zhenhao</forenames></author></authors><title>Improved Accent Classification Combining Phonetic Vowels with Acoustic
  Features</title><categories>cs.SD cs.CL</categories><comments>International Congress on Image and Signal Processing (CISP) 2015</comments><doi>10.1109/CISP.2015.7408064</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researches have shown accent classification can be improved by integrating
semantic information into pure acoustic approach. In this work, we combine
phonetic knowledge, such as vowels, with enhanced acoustic features to build an
improved accent classification system. The classifier is based on Gaussian
Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual
Linear Predictive (PLP) features. The features are further optimized by
Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant
Analysis (HLDA). Using 7 major types of accented speech from the Foreign
Accented English (FAE) corpus, the system achieves classification accuracy 54%
with input test data as short as 20 seconds, which is competitive to the state
of the art in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07396</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07396</id><created>2016-02-23</created><authors><author><keyname>Mongeon</keyname><forenames>Philippe</forenames></author><author><keyname>Brodeur</keyname><forenames>Christine</forenames></author><author><keyname>Beaudry</keyname><forenames>Catherine</forenames></author><author><keyname>Lariviere</keyname><forenames>Vincent</forenames></author></authors><title>Concentration of research funding leads to decreasing marginal returns</title><categories>cs.DL</categories><comments>Accepted for publication in Research Evaluation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most countries, basic research is supported by research councils that
select, after peer review, the individuals or teams that are to receive
funding. Unfortunately, the number of grants these research councils can
allocate is not infinite and, in most cases, a minority of the researchers
receive the majority of the funds. However, evidence as to whether this is an
optimal way of distributing available funds is mixed. The purpose of this study
is to measure the relation between the amount of funding provided to 12,720
researchers in Quebec over a fifteen year period (1998-2012) and their
scientific output and impact from 2000 to 2013. Our results show that both in
terms of the quantity of papers produced and of their scientific impact, the
concentration of research funding in the hands of a so-called &quot;elite&quot; of
researchers generally produces diminishing marginal returns. Also, we find that
the most funded researchers do not stand out in terms of output and scientific
impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07399</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07399</id><created>2016-02-23</created><authors><author><keyname>Huang</keyname><forenames>Qian</forenames></author><author><keyname>Zhang</keyname><forenames>Yuanzhi</forenames></author><author><keyname>Ge</keyname><forenames>Zhenhao</forenames></author><author><keyname>Lu</keyname><forenames>Chao</forenames></author></authors><title>Refining Wi-Fi Based Indoor Localization with Li-Fi Assisted Model
  Calibration in Smart Buildings</title><categories>cs.NI</categories><comments>International Conference on Computing in Civil and Building
  Engineering (ICCCBE) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been an increasing number of information
technologies utilized in buildings to advance the idea of &quot;smart buildings&quot;.
Among various potential techniques, the use of Wi-Fi based indoor positioning
allows to locate and track smartphone users inside a building, therefore,
location-aware intelligent solutions can be applied to control and of building
operations. These location-aware indoor services (e.g., path finding, internet
of things, location based advertising) demand real-time accurate indoor
localization, which is a key issue to guarantee high quality of service in
smart buildings. This paper presents a new Wi-Fi based indoor localization
technique that achieves significantly improvement of indoor positioning
accuracy with the help of Li-Fi assisted coefficient calibration. The proposed
technique leverages indoor existing Li-Fi lighting and Wi-Fi infrastructure,
and results in a cost-effective and user-convenient indoor accurate
localization framework. In this work, experimental study and measurements are
conducted to verify the performance of the proposed idea. The results
substantiate the concept of refining Wi-Fi based indoor localization with Li-Fi
assisted computation calibration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07407</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07407</id><created>2016-02-24</created><authors><author><keyname>Keshavarz-Kohjerdi</keyname><forenames>Fatemeh</forenames></author><author><keyname>Bagheri</keyname><forenames>Alireza</forenames></author></authors><title>Hamiltonian Paths in C-shaped Grid Graphs</title><categories>cs.CC</categories><comments>28 pages, 31 figures, and 20 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Hamiltonian path problem in C-shaped grid graphs, and present
the necessary and sufficient conditions for the existence of a Hamiltonian path
between two given vertices in these graphs. We also give a linear-time
algorithm for finding a Hamiltonian path between two given vertices of a
C-shaped grid graph, if it exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07415</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07415</id><created>2016-02-24</created><authors><author><keyname>De Sa</keyname><forenames>Christopher</forenames></author><author><keyname>Olukotun</keyname><forenames>Kunle</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gibbs sampling is a Markov Chain Monte Carlo technique commonly used for
estimating marginal distributions. To speed up Gibbs sampling, there has
recently been interest in parallelizing it by executing asynchronously. While
empirical results suggest that many models can be efficiently sampled
asynchronously, traditional Markov chain analysis does not apply to the
asynchronous case, and thus asynchronous Gibbs sampling is poorly understood.
In this paper, we derive a better understanding of the two main challenges of
asynchronous Gibbs: sampling bias and mixing time. We show experimentally that
our theoretical results match practical outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07416</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07416</id><created>2016-02-24</created><authors><author><keyname>Li</keyname><forenames>Chongxuan</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Learning to Generate with Memory</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memory units have been widely used to enrich the capabilities of deep
networks on capturing long-term dependencies in reasoning and prediction tasks,
but little investigation exists on deep generative models (DGMs) which are good
at inferring high-level invariant representations from unlabeled data. This
paper presents a deep generative model with a possibly large external memory
and an attention mechanism to capture the local detail information that is
often lost in the bottom-up abstraction process in representation learning. By
adopting a smooth attention model, the whole network is trained end-to-end by
optimizing a variational bound of data likelihood via auto-encoding variational
Bayesian methods, where an asymmetric recognition network is learnt jointly to
infer high-level invariant representations. The asymmetric architecture can
reduce the competition between bottom-up invariant feature extraction and
top-down generation of instance details. Our experiments on several datasets
demonstrate that memory can significantly boost the performance of DGMs and
even achieve state-of-the-art results on various tasks, including density
estimation, image generation, and missing value imputation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07422</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07422</id><created>2016-02-24</created><updated>2016-03-07</updated><authors><author><keyname>Hradovich</keyname><forenames>Mikita</forenames></author><author><keyname>Kasperski</keyname><forenames>Adam</forenames></author><author><keyname>Zielinski</keyname><forenames>Pawel</forenames></author></authors><title>The robust recoverable spanning tree problem with interval costs is
  polynomially solvable</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the robust recoverable spanning tree problem with interval edge
costs is considered. The complexity of this problem has remained open to date.
It is shown that the problem is polynomially solvable, by using an iterative
relaxation method. A generalization of this idea to the robust recoverable
matroid basis problem is also presented. Polynomial algorithms for both robust
recoverable problems are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07423</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07423</id><created>2016-02-24</created><authors><author><keyname>Liu</keyname><forenames>Lei</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Li</keyname><forenames>Ying</forenames></author></authors><title>Capacity-Achieving Iterative LMMSE Detection for MIMO-NOMA Systems</title><categories>cs.IT math.IT</categories><comments>6pages, 5 figures, accepted by IEEE ICC 2016, 23-27 May 2016, Kuala
  Lumpur, Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a iterative Linear Minimum Mean Square Error (LMMSE)
detection for the uplink Multiuser Multiple-Input and Multiple-Output (MU-MIMO)
systems with Non-Orthogonal Multiple Access (NOMA). The iterative LMMSE
detection greatly reduces the system computational complexity by departing the
overall processing into many low-complexity distributed calculations. However,
it is generally considered to be sub-optimal and achieves relatively poor
performance. In this paper, we firstly present the matching conditions and area
theorems for the iterative detection of the MIMO-NOMA systems. Based on the
proposed matching conditions and area theorems, the achievable rate region of
the iterative LMMSE detection is analysed. We prove that by properly design the
iterative LMMSE detection, it can achieve (i) the optimal sum capacity of
MU-MIMO systems, (ii) all the maximal extreme points in the capacity region of
MU-MIMO system, and (iii) the whole capacity region of two-user MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07424</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07424</id><created>2016-02-24</created><authors><author><keyname>De Stefani</keyname><forenames>Lorenzo</forenames></author><author><keyname>Epasto</keyname><forenames>Alessandro</forenames></author><author><keyname>Riondato</keyname><forenames>Matteo</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>TRI\`EST: Counting Local and Global Triangles in Fully-dynamic Streams
  with Fixed Memory Size</title><categories>cs.DS cs.DB</categories><acm-class>G.2.2, H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present TRI\`EST, a suite of one-pass streaming algorithms to compute
unbiased, low-variance, high-quality approximations of the global and local
(i.e., incident to each vertex) number of triangles in a fully-dynamic graph
represented as an adversarial stream of edge insertions and deletions. Our
algorithms use reservoir sampling and its variants to exploit the
user-specified memory space at all times. This is in contrast with previous
approaches which use hard-to-choose parameters (e.g., a fixed sampling
probability) and offer no guarantees on the amount of memory they will use. We
show a full analysis of the variance of the estimations and novel concentration
bounds for these quantities. Our experimental results on very large graphs show
that TRI\`EST outperforms state-of-the-art approaches in accuracy and exhibits
a small update time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07428</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07428</id><created>2016-02-24</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Song</keyname><forenames>Jiaming</forenames></author><author><keyname>Chen</keyname><forenames>Bei</forenames></author></authors><title>Max-Margin Nonparametric Latent Feature Models for Link Prediction</title><categories>cs.LG cs.SI stat.ME stat.ML</categories><comments>14 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction is a fundamental task in statistical network analysis. Recent
advances have been made on learning flexible nonparametric Bayesian latent
feature models for link prediction. In this paper, we present a max-margin
learning method for such nonparametric latent feature relational models. Our
approach attempts to unite the ideas of max-margin learning and Bayesian
nonparametrics to discover discriminative latent features for link prediction.
It inherits the advances of nonparametric Bayesian methods to infer the unknown
latent social dimension, while for discriminative link prediction, it adopts
the max-margin learning principle by minimizing a hinge-loss using the linear
expectation operator, without dealing with a highly nonlinear link likelihood
function. For posterior inference, we develop an efficient stochastic
variational inference algorithm under a truncated mean-field assumption. Our
methods can scale up to large-scale real networks with millions of entities and
tens of millions of positive links. We also provide a full Bayesian
formulation, which can avoid tuning regularization hyper-parameters.
Experimental results on a diverse range of real datasets demonstrate the
benefits inherited from max-margin learning and Bayesian nonparametric
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07435</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07435</id><created>2016-02-24</created><authors><author><keyname>Luo</keyname><forenames>Yuan</forenames></author><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Walrand</keyname><forenames>Jean</forenames></author></authors><title>Parametric Prediction from Parametric Agents</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a problem of prediction based on opinions elicited from
heterogeneous rational agents with private information. Making an accurate
prediction with a minimal cost requires a joint design of the incentive
mechanism and the prediction algorithm. Such a problem lies at the nexus of
statistical learning theory and game theory, and arises in many domains such as
consumer surveys and mobile crowdsourcing. In order to elicit heterogeneous
agents' private information and incentivize agents with different capabilities
to act in the principal's best interest, we design an optimal joint incentive
mechanism and prediction algorithm called COPE (COst and Prediction
Elicitation), the analysis of which offers several valuable engineering
insights. First, when the costs incurred by the agents are linear in the
exerted effort, COPE corresponds to a &quot;crowd contending&quot; mechanism, where the
principal only employs the agent with the highest capability. Second, when the
costs are quadratic, COPE corresponds to a &quot;crowd-sourcing&quot; mechanism that
employs multiple agents with different capabilities at the same time. Numerical
simulations show that COPE improves the principal's profit and the network
profit significantly (larger than 30% in our simulations), comparing to those
mechanisms that assume all agents have equal capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07449</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07449</id><created>2016-02-24</created><authors><author><keyname>Buzzi</keyname><forenames>Stefano</forenames></author><author><keyname>D'Andrea</keyname><forenames>Carmen</forenames></author><author><keyname>Foggi</keyname><forenames>Tommaso</forenames></author><author><keyname>Ugolini</keyname><forenames>Alessandro</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author></authors><title>Spectral Efficiency of MIMO Millimeter-Wave Links with Single-Carrier
  Modulation for 5G Networks</title><categories>cs.IT math.IT</categories><comments>8 pages, 8 figures, to appear in Proc. 20th International ITG
  Workshop on Smart Antennas (WSA2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future wireless networks will extensively rely upon bandwidths centered on
carrier frequencies larger than 10GHz. Indeed, recent research has shown that,
despite the large path-loss, millimeter wave (mmWave) frequencies can be
successfully exploited to transmit very large data-rates over short distances
to slowly moving users. Due to hardware complexity and cost constraints,
single-carrier modulation schemes, as opposed to the popular multi-carrier
schemes, are being considered for use at mmWave frequencies. This paper
presents preliminary studies on the achievable spectral efficiency on a
wireless MIMO link operating at mmWave in a typical 5G scenario. Two different
single-carrier modem schemes are considered, i.e. a traditional modulation
scheme with linear equalization at the receiver, and a single-carrier
modulation with cyclic prefix, frequency-domain equalization and FFT-based
processing at the receiver. Our results show that the former achieves a larger
spectral efficiency than the latter. Results also confirm that the spectral
efficiency increases with the dimension of the antenna array, as well as that
performance gets severely degraded when the link length exceeds 100 meters and
the transmit power falls below 0dBW. Nonetheless, mmWave appear to be very
suited for providing very large data-rates over short distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07453</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07453</id><created>2016-02-24</created><authors><author><keyname>Srinivasarao</keyname><forenames>B. K. N.</forenames></author><author><keyname>Gogineni</keyname><forenames>Vinay Chakravarthi</forenames></author><author><keyname>Mula</keyname><forenames>Subrahmanyam</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Indrajit</forenames></author></authors><title>VLSI Friendly Framework for Scalable Video Coding based on Compressed
  Sensing</title><categories>cs.MM cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new VLSI friendly framework for scalable video coding
based on Compressed Sensing (CS). It achieves scalability through 3-Dimensional
Discrete Wavelet Transform (3-D DWT) and better compression ratio by exploiting
the inherent sparsity of the high-frequency wavelet sub-bands through CS. By
using 3-D DWT and a proposed adaptive measurement scheme called AMS at the
encoder, one can succeed in improving the compression ratio and reducing the
complexity of the decoder. The proposed video codec uses only 7% of the total
number of multipliers needed in a conventional CS-based video coding system. A
codebook of Bernoulli matrices with different sizes corresponding to the
predefined sparsity levels is maintained at both the encoder and the decoder.
Based on the calculated l0-norm of the input vector, one of the sixteen
possible Bernoulli matrices will be selected for taking the CS measurements and
its index will be transmitted along with the measurements. Based on this index,
the corresponding Bernoulli matrix has been used in CS reconstruction algorithm
to get back the high-frequency wavelet sub-bands at the decoder. At the
decoder, a new Enhanced Approximate Message Passing (EAMP) algorithm has been
proposed to reconstruct the wavelet coefficients and apply the inverse wavelet
transform for restoring back the video frames. Simulation results have
established the superiority of the proposed framework over the existing schemes
and have increased its suitability for VLSI implementation. Moreover, the coded
video is found to be scalable with an increase in a number of levels of wavelet
decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07455</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07455</id><created>2016-02-24</created><authors><author><keyname>Yang</keyname><forenames>Li-An</forenames></author><author><keyname>Liu</keyname><forenames>Jui-Bin</forenames></author><author><keyname>Chen</keyname><forenames>Chao-Hong</forenames></author><author><keyname>Chen</keyname><forenames>Ying-ping</forenames></author></authors><title>Automatically Proving Mathematical Theorems with Evolutionary Algorithms
  and Proof Assistants</title><categories>cs.NE cs.LO</categories><comments>Submitted to 2016 IEEE Congress on Evolutionary Computation (CEC
  2016), part of 2016 IEEE World Congress on Computational Intelligence (IEEE
  WCCI 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical theorems are human knowledge able to be accumulated in the form
of symbolic representation, and proving theorems has been considered
intelligent behavior. Based on the BHK interpretation and the Curry-Howard
isomorphism, proof assistants, software capable of interacting with human for
constructing formal proofs, have been developed in the past several decades.
Since proofs can be considered and expressed as programs, proof assistants
simplify and verify a proof by computationally evaluating the program
corresponding to the proof. Thanks to the transformation from logic to
computation, it is now possible to generate or search for formal proofs
directly in the realm of computation. Evolutionary algorithms, known to be
flexible and versatile, have been successfully applied to handle a variety of
scientific and engineering problems in numerous disciplines for also several
decades. Examining the feasibility of establishing the link between
evolutionary algorithms, as the program generator, and proof assistants, as the
proof verifier, in order to automatically find formal proofs to a given logic
sentence is the primary goal of this study. In the article, we describe in
detail our first, ad-hoc attempt to fully automatically prove theorems as well
as the preliminary results. Ten simple theorems from various branches of
mathematics were proven, and most of these theorems cannot be proven by using
the tactic auto alone in Coq, the adopted proof assistant. The implication and
potential influence of this study are discussed, and the developed source code
with the obtained experimental results are released as open source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07464</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07464</id><created>2016-02-24</created><authors><author><keyname>Teisseyre</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Feature ranking for multi-label classification using Markov Networks</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple and efficient method for ranking features in multi-label
classification. The method produces a ranking of features showing their
relevance in predicting labels, which in turn allows to choose a final subset
of features. The procedure is based on Markov Networks and allows to model the
dependencies between labels and features in a direct way. In the first step we
build a simple network using only labels and then we test how much adding a
single feature affects the initial network. More specifically, in the first
step we use the Ising model whereas the second step is based on the score
statistic, which allows to test a significance of added features very quickly.
The proposed approach does not require transformation of label space, gives
interpretable results and allows for attractive visualization of dependency
structure. We give a theoretical justification of the procedure by discussing
some theoretical properties of the Ising model and the score statistic. We also
discuss feature ranking procedure based on fitting Ising model using $l_1$
regularized logistic regressions. Numerical experiments show that the proposed
methods outperform the conventional approaches on the considered artificial and
real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07466</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07466</id><created>2016-02-24</created><authors><author><keyname>Teisseyre</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Asymptotic consistency and order specification for logistic classifier
  chains in multi-label learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifier chains are popular and effective method to tackle a multi-label
classification problem. The aim of this paper is to study the asymptotic
properties of the chain model in which the conditional probabilities are of the
logistic form. In particular we find conditions on the number of labels and the
distribution of feature vector under which the estimated mode of the joint
distribution of labels converges to the true mode. Best of our knowledge, this
important issue has not yet been studied in the context of multi-label
learning. We also investigate how the order of model building in a chain
influences the estimation of the joint distribution of labels. We establish the
link between the problem of incorrect ordering in the chain and incorrect model
specification. We propose a procedure of determining the optimal ordering of
labels in the chain, which is based on using measures of correct specification
and allows to find the ordering such that the consecutive logistic models are
best possibly specified. The other important question raised in this paper is
how accurately can we estimate the joint posterior probability when the
ordering of labels is wrong or the logistic models in the chain are incorrectly
specified. The numerical experiments illustrate the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07467</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07467</id><created>2016-02-24</created><authors><author><keyname>Maatkamp</keyname><forenames>Marcel</forenames></author><author><keyname>van Delden</keyname><forenames>Martin</forenames></author><author><keyname>LeKhac</keyname><forenames>Nhien An</forenames></author></authors><title>Unidirectional Secure Information Transfer via RabbitMQ</title><categories>cs.NI cs.CR</categories><doi>10.13140/RG.2.1.1412.0720</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Protecting computer systems handling possible sensitive information is of the
utmost importance. Those systems are typically air-gapped with data diodes to
assure that no information can physically flow back. Traditional computer
protocols like HTTP or SOAP which are normally used to transport information
between computers are typical bi-directional communication protocols and are
thus unsuitable to be used over a data diode. Currently the only commercially
available protocols over a data diode sold by vendors are file-based protocols.
Other protocols can be custom made but are expensive and proprietary. There are
currently no open source solutions to stream data in a generic way over a data
diode other than those file-based solutions. Purpose of the dissertation is to
research if open source technology can be used to mirror the contents of a
messagebus over a data diode to get a cost effective security-proof and almost
maintenance-free solution. and to further research if this technology can be
used to transfer not only plain text data but also data sensitive by nature by
using end-to-end encryption so that this information could even be admitted as
evidence. Method used to validate the research is a practical case study that
shows how a sensor stream can send unencrypted and encrypted events over a data
diode of arbitrary size via a message bus which are transparently and securely
transferred and re-emitted internally without any kind of configuration
management. Results show that it is indeed possible to successfully mirror data
from a Message Bus over a data diode and it is thus worthwhile to further
invest in this technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07474</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07474</id><created>2016-02-24</created><authors><author><keyname>Sol&#xe9;-Ribalta</keyname><forenames>Albert</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Sergio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Congestion induced by the structure of multiplex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>5 pages, 5 figures. To appear in Physical Review Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplex networks are representations of multilayer interconnected complex
networks where the nodes are the same at every layer. They turn out to be good
abstractions of the intricate connectivity of multimodal transportation
networks, among other types of complex systems. One of the most important
critical phenomena arising in such networks is the emergence of congestion in
transportation flows. Here we prove analytically that the structure of
multiplex networks can induce congestion for flows that otherwise will be
decongested if the individual layers were not interconnected. We provide
explicit equations for the onset of congestion and approximations that allow to
compute this onset from individual descriptors of the individual layers. The
observed cooperative phenomenon reminds the Braess' paradox in which adding
extra capacity to a network when the moving entities selfishly choose their
route can in some cases reduce overall performance. Similarly, in the multiplex
structure, the efficiency in transportation can unbalance the transportation
loads resulting in unexpected congestion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07475</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07475</id><created>2016-02-24</created><authors><author><keyname>Gomez</keyname><forenames>Lluis</forenames></author><author><keyname>Karatzas</keyname><forenames>Dimosthenis</forenames></author></authors><title>A fine-grained approach to scene text script identification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the problem of script identification in unconstrained
scenarios. Script identification is an important prerequisite to recognition,
and an indispensable condition for automatic text understanding systems
designed for multi-language environments. Although widely studied for document
images and handwritten documents, it remains an almost unexplored territory for
scene text images.
  We detail a novel method for script identification in natural images that
combines convolutional features and the Naive-Bayes Nearest Neighbor
classifier. The proposed framework efficiently exploits the discriminative
power of small stroke-parts, in a fine-grained classification framework.
  In addition, we propose a new public benchmark dataset for the evaluation of
joint text detection and script identification in natural scenes. Experiments
done in this new dataset demonstrate that the proposed method yields state of
the art results, while it generalizes well to different datasets and variable
number of scripts. The evidence provided shows that multi-lingual scene text
recognition in the wild is a viable proposition. Source code of the proposed
method is made available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07480</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07480</id><created>2016-02-24</created><authors><author><keyname>Gomez</keyname><forenames>Lluis</forenames></author><author><keyname>Nicolaou</keyname><forenames>Anguelos</forenames></author><author><keyname>Karatzas</keyname><forenames>Dimosthenis</forenames></author></authors><title>Boosting patch-based scene text script identification with ensembles of
  conjoined networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the problem of script identification in scene text
images. Facing this problem with state of the art CNN classifiers is not
straightforward, as they fail to address a key characteristic of scene text
instances: their extremely variable aspect ratio. Instead of resizing input
images to a fixed aspect ratio as in the typical use of holistic CNN
classifiers, we propose here a patch-based classification framework in order to
preserve discriminative parts of the image that are characteristic of its
class.
  We describe a novel method based on the use of ensembles of conjoined
networks to jointly learn discriminative stroke-parts representations and their
relative importance in a patch-based classification scheme. Our experiments
with this learning procedure demonstrate state-of-the-art results in two public
script identification datasets.
  In addition, we propose a new public benchmark dataset for the evaluation of
multi-lingual scene text end-to-end reading systems. Experiments done in this
dataset demonstrate the key role of script identification in a complete
end-to-end system that combines our script identification method with a
previously published text detector and an off-the-shelf OCR engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07486</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07486</id><created>2016-02-24</created><updated>2016-02-25</updated><authors><author><keyname>Wang</keyname><forenames>Qing</forenames></author><author><keyname>Ferrarotti</keyname><forenames>Flavio</forenames></author><author><keyname>Schewe</keyname><forenames>Klaus-Dieter</forenames></author><author><keyname>Tec</keyname><forenames>Loredana</forenames></author></authors><title>A Complete Logic for Non-Deterministic Database Transformations</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database transformations provide a unifying framework for database queries
and updates. Recently, it was shown that non-deterministic database
transformations can be captured exactly by a variant of ASMs, the so-called
Database Abstract StateMachines (DB-ASMs). In this article we present a logic
for DB-ASMs, extending the logic of Nanchen and St\&quot;ark for ASMs. In
particular, we develop a rigorous proof system for the logic for DB-ASMs, which
is proven to be sound and complete. The most difficult challenge to be handled
by the extension is a proper formalisation capturing non-determinism of
database transformations and all its related features such as consistency,
update sets or multisets associated with DB-ASM rules. As the database part of
a state of database transformations is a finite structure and DB-ASMs are
restricted by allowing quantifiers only over the database part of a state, we
resolve this problem by taking update sets explicitly into the logic, i.e. by
using an additional modal operator [X], where X is interpreted as an update set
{\Delta} generated by a DB-ASM rule. The DB-ASM logic provides a powerful
verification tool to study properties of database transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07495</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07495</id><created>2016-02-24</created><authors><author><keyname>Ghasemi</keyname><forenames>Alireza</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author><author><keyname>Fadaee</keyname><forenames>Mohsen</forenames></author><author><keyname>Manzuri</keyname><forenames>Mohammad T.</forenames></author><author><keyname>Rohban</keyname><forenames>Mohammad H.</forenames></author></authors><title>Active Learning from Positive and Unlabeled Data</title><categories>cs.LG</categories><comments>6 pages, presented at IEEE ICDM 2011 Workshops</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During recent years, active learning has evolved into a popular paradigm for
utilizing user's feedback to improve accuracy of learning algorithms. Active
learning works by selecting the most informative sample among unlabeled data
and querying the label of that point from user. Many different methods such as
uncertainty sampling and minimum risk sampling have been utilized to select the
most informative sample in active learning. Although many active learning
algorithms have been proposed so far, most of them work with binary or
multi-class classification problems and therefore can not be applied to
problems in which only samples from one class as well as a set of unlabeled
data are available.
  Such problems arise in many real-world situations and are known as the
problem of learning from positive and unlabeled data. In this paper we propose
an active learning algorithm that can work when only samples of one class as
well as a set of unlabelled data are available. Our method works by separately
estimating probability desnity of positive and unlabeled points and then
computing expected value of informativeness to get rid of a hyper-parameter and
have a better measure of informativeness./ Experiments and empirical analysis
show promising results compared to other similar methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07504</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07504</id><created>2016-02-24</created><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Heggernes</keyname><forenames>Pinar</forenames></author><author><keyname>Kratsch</keyname><forenames>Dieter</forenames></author></authors><title>Enumeration and Maximum Number of Minimal Connected Vertex Covers in
  Graphs</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connected Vertex Cover is one of the classical problems of computer science,
already mentioned in the monograph of Garey and Johnson. Although the
optimization and decision variants of finding connected vertex covers of
minimum size or weight are well studied, surprisingly there is no work on the
enumeration or maximum number of minimal connected vertex covers of a graph. In
this paper we show that the maximum number of minimal connected vertex covers
of a graph is at most 1.8668^n, and these can be enumerated in time
O(1.8668^n). For graphs of chordality at most 5, we are able to give a better
upper bound, and for chordal graphs and distance-hereditary graphs we are able
to give tight bounds on the maximum number of minimal connected vertex covers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07507</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07507</id><created>2016-02-24</created><authors><author><keyname>Ghasemi</keyname><forenames>Alireza</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author><author><keyname>Manzuri</keyname><forenames>Mohammad T.</forenames></author><author><keyname>Rohban</keyname><forenames>M. H.</forenames></author></authors><title>A Bayesian Approach to the Data Description Problem</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of data description using a Bayesian
framework. The goal of data description is to draw a boundary around objects of
a certain class of interest to discriminate that class from the rest of the
feature space. Data description is also known as one-class learning and has a
wide range of applications.
  The proposed approach uses a Bayesian framework to precisely compute the
class boundary and therefore can utilize domain information in form of prior
knowledge in the framework. It can also operate in the kernel space and
therefore recognize arbitrary boundary shapes. Moreover, the proposed method
can utilize unlabeled data in order to improve accuracy of discrimination.
  We evaluate our method using various real-world datasets and compare it with
other state of the art approaches of data description. Experiments show
promising results and improved performance over other data description and
one-class learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07509</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07509</id><created>2016-02-24</created><authors><author><keyname>Brattka</keyname><forenames>Vasco</forenames></author></authors><title>Computability and Analysis, a Historical Approach</title><categories>math.LO cs.LO</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The histories of computability theory and analysis are surprisingly
intertwined since the beginning of the twentieth century. For one, \'Emil Borel
discussed his ideas on computable real number functions in his introduction to
measure theory. On the other hand, Alan Turing had computable real numbers in
mind when he introduced his now famous machine model. Here we want to focus on
a particular aspect of computability and analysis, namely on computability
properties of theorems from analysis. This is a topic that emerged already in
early work of Turing, Specker and other pioneers of computable analysis and
eventually leads to the very recent project of classifying the computational
content of theorems in the Weihrauch lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07527</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07527</id><created>2016-02-24</created><authors><author><keyname>Murray</keyname><forenames>Iain</forenames></author></authors><title>Differentiation of the Cholesky decomposition</title><categories>stat.CO cs.MS</categories><comments>18 pages, including 7 pages of code listings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review strategies for differentiating matrix-based computations, and
derive symbolic and algorithmic update rules for differentiating expressions
containing the Cholesky decomposition. We recommend new `blocked' algorithms,
based on differentiating the Cholesky algorithm DPOTRF in the LAPACK library,
which uses `Level 3' matrix-matrix operations from BLAS, and so is
cache-friendly and easy to parallelize. For large matrices, the resulting
algorithms are the fastest way to compute Cholesky derivatives, and are an
order of magnitude faster than the algorithms in common usage. In some
computing environments, symbolically-derived updates are faster for small
matrices than those based on differentiating Cholesky algorithms. The symbolic
and algorithmic approaches can be combined to get the best of both worlds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07533</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07533</id><created>2016-02-24</created><updated>2016-02-25</updated><authors><author><keyname>Haneda</keyname><forenames>Katsuyuki</forenames></author><author><keyname>Tian</keyname><forenames>Lei</forenames></author><author><keyname>Zheng</keyname><forenames>Yi</forenames></author><author><keyname>Asplund</keyname><forenames>Henrik</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Steer</keyname><forenames>David</forenames></author><author><keyname>Li</keyname><forenames>Clara</forenames></author><author><keyname>Balercia</keyname><forenames>Tommaso</forenames></author><author><keyname>Lee</keyname><forenames>Sunguk</forenames></author><author><keyname>Kim</keyname><forenames>YoungSuk</forenames></author><author><keyname>Ghosh</keyname><forenames>Amitava</forenames></author><author><keyname>Thomas</keyname><forenames>Timothy</forenames></author><author><keyname>Nakamura</keyname><forenames>Takehiro</forenames></author><author><keyname>Kakishima</keyname><forenames>Yuichi</forenames></author><author><keyname>Imai</keyname><forenames>Tetsuro</forenames></author><author><keyname>Papadopoulas</keyname><forenames>Haralabos</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author><author><keyname>MacCartney</keyname><forenames>George R.</forenames><suffix>Jr.</suffix></author><author><keyname>Samimi</keyname><forenames>Mathew K.</forenames></author><author><keyname>Sun</keyname><forenames>Shu</forenames></author><author><keyname>Koymen</keyname><forenames>Ozge</forenames></author><author><keyname>Hur</keyname><forenames>Sooyoung</forenames></author><author><keyname>Park</keyname><forenames>Jeongho</forenames></author><author><keyname>Zhang</keyname><forenames>Charlie</forenames></author><author><keyname>Mellios</keyname><forenames>Evangelos</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author><author><keyname>Ghassamzadah</keyname><forenames>Saeed S.</forenames></author><author><keyname>Ghosh</keyname><forenames>Arun</forenames></author></authors><title>5G 3GPP-like Channel Models for Outdoor Urban Microcellular and
  Macrocellular Environments</title><categories>cs.IT math.IT</categories><comments>To be published in 2016 IEEE 83rd Vehicular Technology Conference
  Spring (VTC 2016-Spring), Nanjing, China, May 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the development of new 5G systems to operate in bands up to 100 GHz,
there is a need for accurate radio propagation models at these bands that
currently are not addressed by existing channel models developed for bands
below 6 GHz. This document presents a preliminary overview of 5G channel models
for bands up to 100 GHz. These have been derived based on extensive measurement
and ray tracing results across a multitude of frequencies from 6 GHz to 100
GHz, and this document describes an initial 3D channel model which includes: 1)
typical deployment scenarios for urban microcells (UMi) and urban macrocells
(UMa), and 2) a baseline model for incorporating path loss, shadow fading, line
of sight probability, penetration and blockage models for the typical
scenarios. Various processing methodologies such as clustering and antenna
decoupling algorithms are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07535</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07535</id><created>2016-02-24</created><authors><author><keyname>Ghasemi</keyname><forenames>Alireza</forenames></author><author><keyname>Scholefield</keyname><forenames>Adam</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>SHAPE: Linear-Time Camera Pose Estimation With Quadratic Error-Decay</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel camera pose estimation or perspective-n-point (PnP)
algorithm, based on the idea of consistency regions and half-space
intersections. Our algorithm has linear time-complexity and a squared
reconstruction error that decreases at least quadratically, as the number of
feature point correspondences increase.
  Inspired by ideas from triangulation and frame quantisation theory, we define
consistent reconstruction and then present SHAPE, our proposed consistent pose
estimation algorithm. We compare this algorithm with state-of-the-art pose
estimation techniques in terms of accuracy and error decay rate. The
experimental results verify our hypothesis on the optimal worst-case quadratic
decay and demonstrate its promising performance compared to other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07536</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07536</id><created>2015-01-13</created><authors><author><keyname>Yampolskiy</keyname><forenames>Mark</forenames></author><author><keyname>Andel</keyname><forenames>Todd R.</forenames></author><author><keyname>McDonald</keyname><forenames>J. Todd</forenames></author><author><keyname>Glisson</keyname><forenames>William B.</forenames></author><author><keyname>Yasinsac</keyname><forenames>Alec</forenames></author></authors><title>Towards Security of Additive Layer Manufacturing</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Additive Layer Manufacturing (ALM), also broadly known as 3D printing, is a
new technology to produce 3D objects. As an opposite approach to the
conventional subtractive manufacturing process, 3D objects are created by
adding thin material layers over layers. Until recently, they have been used,
mainly, for plastic models. However, the technology has evolved making it
possible to use high-quality printing with metal alloys. Agencies and companies
like NASA, ESA, Boeing, Airbus, etc. are investigating various ALM technology
application areas. Recently, SpaceX used additive manufacturing to produce
engine chambers for the newest Dragon spacecraft. BAE System plans to print
on-demand a complete Unmanned Aerial Vehicle (UAV), depending on the
operational requirements. Companies expect the implementation of ALM technology
will bring a broad variety of technological and economic benefits. This
includes, but not limited to, the reduction of the time needed to produce
complex parts, reduction of wasted material and thus control of production
costs along with minimization of part storage space as companies implement
just-in-time and on-demand production solutions. The broad variety of
application areas and a high grade of computerization of the manufacturing
process will inevitably make ALM an attractive target for various attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07542</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07542</id><created>2016-02-24</created><authors><author><keyname>Ghasemi</keyname><forenames>Alireza</forenames></author><author><keyname>Scholefield</keyname><forenames>Adam</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>On the Accuracy of Point Localisation in a Circular Camera-Array</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although many advances have been made in light-field and camera-array image
processing, there is still a lack of thorough analysis of the localisation
accuracy of different multi-camera systems. By considering the problem from a
frame-quantisation perspective, we are able to quantify the point localisation
error of circular camera configurations. Specifically, we obtain closed form
expressions bounding the localisation error in terms of the parameters
describing the acquisition setup.
  These theoretical results are independent of the localisation algorithm and
thus provide fundamental limits on performance. Furthermore, the new
frame-quantisation perspective is general enough to be extended to more complex
camera configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07556</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07556</id><created>2016-02-24</created><authors><author><keyname>Gerencs&#xe9;r</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Gusev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author></authors><title>Primitive sets of nonnegative matrices and synchronizing automata</title><categories>cs.FL cs.DM</categories><comments>14 pages</comments><acm-class>F.1.1; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of nonnegative matrices $\mathcal{M}=\{M_1, M_2, \ldots, M_k\}$ is
called primitive if there exist indices $i_1, i_2, \ldots, i_m$ such that
$M_{i_1} M_{i_2} \ldots M_{i_m}$ is positive (i.e. has all its entries $&gt;0$).
The length of the shortest such product is called the exponent of
$\mathcal{M}$. The concept of primitive sets of matrices comes up in a number
of problems within control theory, non-homogeneous Markov chains, automata
theory etc. Recently, connections between synchronizing automata and primitive
sets of matrices were established. In the present paper, we significantly
strengthen these links by providing equivalence results, both in terms of
combinatorial characterization, and computational aspects. We study the maximal
exponent among all primitive sets of $n \times n$ matrices, which we denote by
$\exp(n)$. We prove that $\lim_{n\rightarrow\infty} \tfrac{\log \exp(n)}{n} =
\tfrac{\log 3}{3}$, and moreover, we establish that this bound leads to a
resolution of the \v{C}ern\'{y} problem for carefully synchronizing automata.
We also study the set of matrices with no zero rows and columns, denoted by
$\mathcal{NZ}$, due to its intriguing connections to the \v{C}ern\'{y}
conjecture and the recent generalization of Perron-Frobenius theory for this
class. We characterize computational complexity of different problems related
to the exponent of $\mathcal{NZ}$ matrix sets, and present a quadratic bound on
the exponents of sets belonging to a special subclass. Namely, we show that the
exponent of a set of matrices having total support is bounded by $2n^2 -5n +5$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07558</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07558</id><created>2016-02-23</created><authors><author><keyname>Alhubail</keyname><forenames>Maitham Makki</forenames></author><author><keyname>Wang</keyname><forenames>Qiqi</forenames></author><author><keyname>Williams</keyname><forenames>John</forenames></author></authors><title>The swept rule for breaking the latency barrier in time advancing
  two-dimensional PDEs</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes a method to accelerate parallel, explicit time
integration of two-dimensional unsteady PDEs. The method is motivated by our
observation that latency, not bandwidth, often limits how fast PDEs can be
solved in parallel. The method is called the swept rule of space-time domain
decomposition. Compared to conventional, space-only domain decomposition, it
communicates similar amount of data, but in fewer messages. The swept rule
achieves this by decomposing space and time among computing nodes in ways that
exploit the domains of influence and the domain of dependency, making it
possible to communicate once per many time steps with no redundant computation.
By communicating less often, the swept rule effectively breaks the latency
barrier, advancing on average more than one time step per ping-pong latency of
the network. The article presents simple theoretical analysis to the
performance of the swept rule in two spatial dimensions, and supports the
analysis with numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07563</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07563</id><created>2016-02-24</created><authors><author><keyname>Mozetic</keyname><forenames>Igor</forenames></author><author><keyname>Grcar</keyname><forenames>Miha</forenames></author><author><keyname>Smailovic</keyname><forenames>Jasmina</forenames></author></authors><title>Multilingual Twitter Sentiment Classification: The Role of Human
  Annotators</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What are the limits of automated Twitter sentiment classification? We analyze
a large set of manually labeled tweets in different languages, use them as
training data, and construct automated classification models. It turns out that
the quality of classification models depends much more on the quality and size
of training data than on the type of the model trained. Experimental results
indicate that there is no statistically significant difference between the
performance of the top classification models. We quantify the quality of
training data by applying various annotator agreement measures, and identify
the weakest points of different datasets. We show that the model performance
approaches the inter-annotator agreement when the size of the training set is
sufficiently large. However, it is crucial to regularly monitor the self- and
inter-annotator agreements since this improves the training datasets and
consequently the model performance. Finally, we show that there is strong
evidence that humans perceive the sentiment classes (negative, neutral, and
positive) as ordered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07565</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07565</id><created>2016-02-24</created><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Chmel&#xed;k</keyname><forenames>Martin</forenames></author><author><keyname>Gupta</keyname><forenames>Anchit</forenames></author><author><keyname>Novotn&#xfd;</keyname><forenames>Petr</forenames></author></authors><title>Stochastic Shortest Path with Energy Constraints in POMDPs</title><categories>cs.AI</categories><comments>Technical report accompanying a paper published in proceedings of
  AAMAS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider partially observable Markov decision processes (POMDPs) with a
set of target states and positive integer costs associated with every
transition. The traditional optimization objective (stochastic shortest path)
asks to minimize the expected total cost until the target set is reached. We
extend the traditional framework of POMDPs to model energy consumption, which
represents a hard constraint. The energy levels may increase and decrease with
transitions, and the hard constraint requires that the energy level must remain
positive in all steps till the target is reached. First, we present a novel
algorithm for solving POMDPs with energy levels, developing on existing POMDP
solvers and using RTDP as its main method. Our second contribution is related
to policy representation. For larger POMDP instances the policies computed by
existing solvers are too large to be understandable. We present an automated
procedure based on machine learning techniques that automatically extracts
important decisions of the policy allowing us to compute succinct human
readable policies. Finally, we show experimentally that our algorithm performs
well and computes succinct policies on a number of POMDP instances from the
literature that were naturally enhanced with energy levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07566</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07566</id><created>2016-02-24</created><authors><author><keyname>Polato</keyname><forenames>Mirko</forenames></author><author><keyname>Sperduti</keyname><forenames>Alessandro</forenames></author><author><keyname>Burattin</keyname><forenames>Andrea</forenames></author><author><keyname>de Leoni</keyname><forenames>Massimiliano</forenames></author></authors><title>Time and Activity Sequence Prediction of Business Process Instances</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to know in advance the trend of running process instances, with
respect to different features, such as the expected completion time, would
allow business managers to timely counteract to undesired situations, in order
to prevent losses. Therefore, the ability to accurately predict future features
of running business process instances would be a very helpful aid when managing
processes, especially under service level agreement constraints. However,
making such accurate forecasts is not easy: many factors may influence the
predicted features.
  Many approaches have been proposed to cope with this problem but all of them
assume that the underling process is stationary. However, in real cases this
assumption is not always true. In this work we present new methods for
predicting the remaining time of running cases. In particular we propose a
method, assuming process stationarity, which outperforms the state-of-the-art
and two other methods which are able to make predictions even with
non-stationary processes. We also describe an approach able to predict the full
sequence of activities that a running case is going to take. All these methods
are extensively evaluated on two real case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07567</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07567</id><created>2016-02-24</created><authors><author><keyname>Fridman</keyname><forenames>Emilia</forenames></author><author><keyname>Terushkin</keyname><forenames>Maria</forenames></author></authors><title>New Stability and Exact Observability Conditions for Semilinear Wave
  Equations</title><categories>cs.SY math.AP</categories><comments>12 pages, 1 figure, published in Automatica</comments><journal-ref>Automatica, 63, pp. 1-10, 2016</journal-ref><doi>10.1016/j.automatica.2015.10.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of estimating the initial state of 1-D wave equations with
globally Lipschitz nonlinearities from boundary measurements on a finite
interval was solved recently by using the sequence of forward and backward
observers, and deriving the upper bound for exact observability time in terms
of Linear Matrix Inequalities (LMIs) [5]. In the present paper, we generalize
this result to n-D wave equations on a hypercube. This extension includes new
LMI-based exponential stability conditions for n-D wave equations, as well as
an upper bound on the minimum exact observability time in terms of LMIs. For
1-D wave equations with locally Lipschitz nonlinearities, we find an estimate
on the region of initial conditions that are guaranteed to be uniquely
recovered from the measurements. The efficiency of the results is illustrated
by numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07570</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07570</id><created>2016-02-24</created><authors><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Bayesian Exploration: Incentivizing Exploration in Bayesian Games</title><categories>cs.GT cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a ubiquitous scenario in the Internet economy when individual
decision-makers (henceforth, agents) both produce and consume information as
they make strategic choices in an uncertain environment. This creates a
three-way tradeoff between exploration (trying out insufficiently explored
alternatives to help others in the future), exploitation (making optimal
decisions given the information discovered by other agents), and incentives of
the agents (who are myopically interested in exploitation, while preferring the
others to explore). We posit a principal who controls the flow of information
from agents that came before, and strives to coordinate the agents towards a
socially optimal balance between exploration and exploitation, not using any
monetary transfers. The goal is to design a recommendation policy for the
principal which respects agents' incentives and minimizes a suitable notion of
regret.
  We extend prior work in this direction to allow the agents to interact with
one another in a shared environment: at each time step, multiple agents arrive
to play a Bayesian game, receive recommendations, choose their actions, receive
their payoffs, and then leave the game forever. The agents now face two sources
of uncertainty: the actions of the other agents and the parameters of the
uncertain game environment.
  Our main contribution is to show that the principal can achieve constant
regret when the utilities are deterministic (where the constant depends on the
prior distribution, but not on the time horizon), and logarithmic regret when
the utilities are stochastic. As a key technical tool, we introduce the concept
of explorable actions, the actions which some incentive-compatible policy can
recommend with non-zero probability. We show how the principal can identify
(and explore) all explorable actions, and use the revealed information to
perform optimally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07572</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07572</id><created>2016-02-24</created><authors><author><keyname>Rothe</keyname><forenames>Sascha</forenames></author><author><keyname>Ebert</keyname><forenames>Sebastian</forenames></author><author><keyname>Sch&#xfc;tze</keyname><forenames>Hinrich</forenames></author></authors><title>Ultradense Word Embeddings by Orthogonal Transformation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embeddings are generic representations that are useful for many NLP tasks. In
this paper, we introduce DENSIFIER, a method that learns an orthogonal
transformation of the embedding space that focuses the information relevant for
a task in an ultradense subspace of a dimensionality that is smaller by a
factor of 100 than the original space. We show that ultradense embeddings
generated by DENSIFIER reach state of the art on a lexicon creation task in
which words are annotated with three types of lexical information - sentiment,
concreteness and frequency. On the SemEval2015 10B sentiment analysis task we
show that no information is lost when the ultradense subspace is used, but
training is an order of magnitude more efficient due to the compactness of the
ultradense space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07573</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07573</id><created>2015-08-08</created><authors><author><keyname>Chen</keyname><forenames>Fuhao</forenames></author><author><keyname>Chen</keyname><forenames>Jun</forenames></author><author><keyname>Huang</keyname><forenames>Feng</forenames></author></authors><title>A straightforward method to assess motion blur for different types of
  displays</title><categories>cs.CV</categories><comments>22 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simulation method based on the liquid crystal response and the human visual
system is suitable to characterize motion blur for LCDs but not other display
types. We propose a more straightforward and widely applicable method to
quantify motion blur based on the width of the moving object. We thus compare
various types of displays objectively. A perceptual experiment was conducted to
validate the proposed method. We test varying motion velocities for nine
commercial displays. We compare the three motion blur evaluation methods
(simulation, human perception, and our method) using z-scores. Our comparisons
indicate that our method accurately characterizes motion blur for various
display types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07576</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07576</id><created>2016-02-24</created><authors><author><keyname>Cohen</keyname><forenames>Taco S.</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Group Equivariant Convolutional Networks</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a
natural generalization of convolutional neural networks that reduces sample
complexity by exploiting symmetries. By convolving over groups larger than the
translation group, G-CNNs build representations that are equivariant to these
groups, which makes it possible to greatly increase the degree of parameter
sharing. We show how G-CNNs can be implemented with negligible computational
overhead for discrete groups such as the group of translations, reflections and
rotations by multiples of 90 degrees. G-CNNs achieve state of the art results
on rotated MNIST and significantly improve over a competitive baseline on
augmented and non-augmented CIFAR-10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07579</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07579</id><created>2016-02-24</created><authors><author><keyname>Liao</keyname><forenames>Yun</forenames></author><author><keyname>Wang</keyname><forenames>Tianyu</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Listen-and-Talk: Protocol Design and Analysis for Full-duplex Cognitive
  Radio Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In traditional cognitive radio networks, secondary users (SUs) typically
access the spectrum of primary users (PUs) by a two-stage &quot;listen-before-talk&quot;
(LBT) protocol, i.e., SUs sense the spectrum holes in the first stage before
transmitting in the second. However, there exist two major problems: 1)
transmission time reduction due to sensing, and 2) sensing accuracy impairment
due to data transmission. In this paper, we propose a &quot;listen-and-talk&quot; (LAT)
protocol with the help of full-duplex (FD) technique that allows SUs to
simultaneously sense and access the vacant spectrum. Spectrum utilization
performance is carefully analyzed, with the closed-form spectrum waste ratio
and collision ratio with the PU provided. Also, regarding the secondary
throughput, we report the existence of a tradeoff between the secondary
transmit power and throughput. Based on the power-throughput tradeoff, we
derive the analytical local optimal transmit power for SUs to achieve both high
throughput and satisfying sensing accuracy. Numerical results are given to
verify the proposed protocol and the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07591</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07591</id><created>2016-02-24</created><authors><author><keyname>Perez</keyname><forenames>Jose Alvaro</forenames></author><author><keyname>Loquen</keyname><forenames>Thomas</forenames></author><author><keyname>Alazard</keyname><forenames>Daniel</forenames></author><author><keyname>Pittet</keyname><forenames>Christelle</forenames></author></authors><title>Linear Modeling of a Flexible Substructure Actuated through
  Piezoelectric Components for Use in Integrated Control/Structure Design</title><categories>cs.SY</categories><comments>submitted to IFAC ACA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents a generic TITOP (Two-Input Two-Output Port) model of a
substructure actuated with embedded piezoelectric materials as actuators
(PEAs), previously modeled with the FE technique. This allows intuitive
assembly of actuated flexible substructures in large flexible multi-body
systems. The modeling technique is applied to an illustrative example of a
flexible beam with bonded piezoelectric strip and vibration attenuation of a
chain of flexible beams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07593</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07593</id><created>2016-02-24</created><authors><author><keyname>D&#xfc;tting</keyname><forenames>Paul</forenames></author><author><keyname>Fischer</keyname><forenames>Felix</forenames></author><author><keyname>Parkes</keyname><forenames>David C.</forenames></author></authors><title>Truthful Outcomes from Non-Truthful Position Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adding to a long list of properties that explain why the VCG mechanism is
rarely used in practice, we exhibit a relative lack of robustness to
inaccuracies in the choice of its parameters. For a standard position auction
environment in which the auctioneer may not know the precise relative values of
the positions, we show that under both complete and incomplete information a
non-truthful mechanism supports the truthful outcome of the VCG mechanism for a
wider range of these values than the VCG mechanism itself. The result for
complete information concerns the generalized second-price mechanism and lends
additional support to Google's use of this mechanism for ad placement.
Particularly interesting from a technical perspective is the case of incomplete
information, where a surprising combinatorial equivalence helps us to avoid
confrontation with an unwieldy differential equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07613</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07613</id><created>2016-02-23</created><authors><author><keyname>Aghasi</keyname><forenames>Alireza</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Object Learning and Convex Cardinal Shape Composition</title><categories>cs.CV math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work mainly focuses on a novel segmentation and partitioning scheme,
based on learning the principal elements of the optimal partitioner in the
image. The problem of interest is characterizing the objects present in an
image as a composition of matching elements from a dictionary of prototype
shapes. The composition model allows set union and difference among the
selected elements, while regularizing the problem by restricting their count to
a fixed level. This is a combinatorial problem addressing which is not in
general computationally tractable. Convex cardinal shape composition (CSC) is a
recent relaxation scheme presented as a proxy to the original problem. From a
theoretical standpoint, this paper improves the results presented in the
original work, by deriving the general sufficient conditions under which CSC
identifies a target composition. We also provide qualitative results on who
well the CSC outcome approximates the combinatorial solution. From a
computational standpoint, two convex solvers, one supporting distributed
processing for large-scale problems, and one cast as a linear program are
presented. Applications such as multi-resolution segmentation and recovery of
the principal shape components are presented as the experiments supporting the
proposed ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07614</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07614</id><created>2016-02-15</created><authors><author><keyname>Ramazzotti</keyname><forenames>Daniele</forenames></author></authors><title>A Model of Selective Advantage for the Efficient Inference of Cancer
  Clonal Evolution</title><categories>cs.LG</categories><comments>Doctoral thesis, University of Milan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a resurgence of interest in rigorous algorithms for
the inference of cancer progression from genomic data. The motivations are
manifold: (i) growing NGS and single cell data from cancer patients, (ii) need
for novel Data Science and Machine Learning algorithms to infer models of
cancer progression, and (iii) a desire to understand the temporal and
heterogeneous structure of tumor to tame its progression by efficacious
therapeutic intervention. This thesis presents a multi-disciplinary effort to
model tumor progression involving successive accumulation of genetic
alterations, each resulting populations manifesting themselves in a cancer
phenotype. The framework presented in this work along with algorithms derived
from it, represents a novel approach for inferring cancer progression, whose
accuracy and convergence rates surpass the existing techniques. The approach
derives its power from several fields including algorithms in machine learning,
theory of causality and cancer biology. Furthermore, a modular pipeline to
extract ensemble-level progression models from sequenced cancer genomes is
proposed. The pipeline combines state-of-the-art techniques for sample
stratification, driver selection, identification of fitness-equivalent
exclusive alterations and progression model inference. Furthermore, the results
are validated by synthetic data with realistic generative models, and
empirically interpreted in the context of real cancer datasets; in the later
case, biologically significant conclusions are also highlighted. Specifically,
it demonstrates the pipeline's ability to reproduce much of the knowledge on
colorectal cancer, as well as to suggest novel hypotheses. Lastly, it also
proves that the proposed framework can be applied to reconstruct the
evolutionary history of cancer clones in single patients, as illustrated by an
example from clear cell renal carcinomas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07615</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07615</id><created>2016-02-24</created><authors><author><keyname>Maill&#xe9;</keyname><forenames>Patrick</forenames></author><author><keyname>Schwartz</keyname><forenames>Galina</forenames></author></authors><title>Content Providers Volunteering to Pay Network Providers: Better than
  Neutrality?</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the effects on user welfare of imposing network
neutrality, using a game-theoretic model of provider interactions based on a
two-sided market framework: we assume that the platform--the last-mile access
providers (ISPs)--are monopolists, and consider content providers (CPs) entry
decisions. All decisions affect the choices made by users, who are sensitive
both to CP and ISP investments (in content creation and quality-of-service,
respectively). In a non-neutral regime, CPs and ISPs can charge each other,
while such charges are prohibited in the neutral regime. We assume those
charges (if any) are chosen by CPs, a direction rarely considered in the
literature, where they are assumed fixed by ISPs.
  Our analysis suggests that, unexpectedly, more CPs enter the market in a
non-neutral regime where they pay ISPs, than without such payments.
Additionally, in this case ISPs tend to invest more than in the neutral regime.
From our results, the best regime in terms of user welfare is parameter
dependent, calling for caution in designing neutrality regulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07616</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07616</id><created>2016-02-24</created><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Saks</keyname><forenames>Michael</forenames></author><author><keyname>Tang</keyname><forenames>Sijian</forenames></author></authors><title>Noisy population recovery in polynomial time</title><categories>cs.CC cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the noisy population recovery problem of Dvir et al., the goal is to learn
an unknown distribution $f$ on binary strings of length $n$ from noisy samples.
For some parameter $\mu \in [0,1]$, a noisy sample is generated by flipping
each coordinate of a sample from $f$ independently with probability
$(1-\mu)/2$. We assume an upper bound $k$ on the size of the support of the
distribution, and the goal is to estimate the probability of any string to
within some given error $\varepsilon$. It is known that the algorithmic
complexity and sample complexity of this problem are polynomially related to
each other.
  We show that for $\mu &gt; 0$, the sample complexity (and hence the algorithmic
complexity) is bounded by a polynomial in $k$, $n$ and $1/\varepsilon$
improving upon the previous best result of $\mathsf{poly}(k^{\log\log
k},n,1/\varepsilon)$ due to Lovett and Zhang.
  Our proof combines ideas from Lovett and Zhang with a \emph{noise attenuated}
version of M\&quot;{o}bius inversion. In turn, the latter crucially uses the
construction of \emph{robust local inverse} due to Moitra and Saks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07618</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07618</id><created>2016-02-22</created><authors><author><keyname>Coecke</keyname><forenames>Bob</forenames></author></authors><title>From quantum foundations via natural language meaning to a theory of
  everything</title><categories>cs.CL quant-ph</categories><comments>Invited contribution to: `The Incomputable'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we argue for a paradigmatic shift from `reductionism' to
`togetherness'. In particular, we show how interaction between systems in
quantum theory naturally carries over to modelling how word meanings interact
in natural language. Since meaning in natural language, depending on the
subject domain, encompasses discussions within any scientific discipline, we
obtain a template for theories such as social interaction, animal behaviour,
and many others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07620</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07620</id><created>2015-09-15</created><authors><author><keyname>Mishra</keyname><forenames>Ashutosh</forenames></author><author><keyname>Mahapatra</keyname><forenames>Sudipta</forenames></author><author><keyname>Banerjee</keyname><forenames>Swapna</forenames></author></authors><title>A Low Complexity VLSI Architecture for Multi-Focus Image Fusion in DCT
  Domain</title><categories>cs.CV</categories><comments>Submitting to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the confined focal length of optical sensors, focusing all objects in
a scene with a single sensor is a difficult task. To handle such a situation,
image fusion methods are used in multi-focus environment. Discrete Cosine
Transform (DCT) is a widely used image compression transform, image fusion in
DCT domain is an efficient method. This paper presents a low complexity
approach for multi-focus image fusion and its VLSI implementation using DCT.
The proposed method is evaluated using reference/non-reference fusion measure
criteria and the obtained results asserts it's effectiveness. The maximum
synthesized frequency on FPGA is found to be 221 MHz and consumes 42% of FPGA
resources. The proposed method consumes very less power and can process 4K
resolution images at the rate of 60 frames per second which makes the hardware
suitable for handheld portable devices such as camera module and wireless image
sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07623</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07623</id><created>2016-02-24</created><authors><author><keyname>Giovanidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Avranas</keyname><forenames>Apostolos</forenames></author></authors><title>Spatial multi-LRU Caching for Wireless Networks with Coverage Overlaps</title><categories>cs.PF cs.NI</categories><comments>14 pages, 2-column, 10 Figures (13 sub-figures in all), part of the
  results in ACM SIGMETRICS/IFIP Performance 2016, Antibes, France</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a novel family of decentralised caching policies,
applicable to wireless networks with finite storage at the edge-nodes
(stations). These policies are based on the Least-Recently-Used replacement
principle, and are, here, referred to as spatial multi-LRU. Based on these,
cache inventories are updated in a way that provides content diversity to users
who are covered by, and thus have access to, more than one station. Two
variations are proposed, namely the multi-LRU-One and -All, which differ in the
number of replicas inserted in the involved caches. By introducing spatial
approximations, we propose a Che-like method to predict the hit probability,
which gives very accurate results under the Independent Reference Model (IRM).
It is shown that the performance of multi-LRU increases the more the
multi-coverage areas increase, and it approaches the performance of other
proposed centralised policies, when multi-coverage is sufficient. For IRM
traffic multi-LRU-One outperforms multi-LRU-All, whereas when the traffic
exhibits temporal locality the -All variation can perform better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07630</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07630</id><created>2016-02-24</created><authors><author><keyname>Ying</keyname><forenames>Bicheng</forenames></author><author><keyname>Yuan</keyname><forenames>Kun</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Online Dual Coordinate Ascent Learning</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic dual coordinate-ascent (S-DCA) technique is a useful
alternative to the traditional stochastic gradient-descent algorithm for
solving large-scale optimization problems due to its scalability to large data
sets and strong theoretical guarantees. However, the available S-DCA
formulation is limited to finite sample sizes and relies on performing multiple
passes over the same data. This formulation is not well-suited for online
implementations where data keep streaming in. In this work, we develop an {\em
online} dual coordinate-ascent (O-DCA) algorithm that is able to respond to
streaming data and does not need to revisit the past data. This feature embeds
the resulting construction with continuous adaptation, learning, and tracking
abilities, which are particularly attractive for online learning scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07633</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07633</id><created>2016-02-24</created><authors><author><keyname>Borg</keyname><forenames>Markus</forenames></author></authors><title>Advancing Trace Recovery Evaluation - Applied Information Retrieval in a
  Software Engineering Context</title><categories>cs.SE</categories><comments>Introduction and synthesis of a cumulative thesis. The four papers
  included in the thesis are not included in this file</comments><report-no>Licentiate Thesis 13, 2012 ISSN 1652-4691</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful development of software systems involves efficient navigation
among software artifacts. One state-of-practice approach to structure
information is to establish trace links between artifacts, a practice that is
also enforced by several development standards. Unfortunately, manually
maintaining trace links in an evolving system is a tedious task. To tackle this
issue, several researchers have proposed treating the capture and recovery of
trace links as an Information Retrieval (IR) problem. The work contains a
Systematic Literature Review (SLR) of previous evaluations of IR-based trace
recovery. We show that a majority of previous evaluations have been
technology-oriented, conducted in &quot;the cave of IR evaluation&quot;, using small
datasets as experimental input. Also, software artifacts originating from
student projects have frequently been used in evaluations. We conducted a
survey among traceability researchers, and found that a majority consider
student artifacts to be only partly representative to industrial counterparts.
Our findings call for additional case studies to evaluate IR-based trace
recovery within the full complexity of an industrial setting. Also, this thesis
contributes to the body of empirical evidence of IR-based trace recovery in two
experiments with industrial software artifacts. The technology-oriented
experiment highlights the clear dependence between datasets and the accuracy of
IR-based trace recovery, in line with findings from the SLR. The human-oriented
experiment investigates how different quality levels of tool output affect the
tracing accuracy of engineers. Finally, we present how tools and methods are
evaluated in the general field of IR research, and propose a taxonomy of
evaluation contexts tailored for IR-based trace recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07636</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07636</id><created>2016-02-24</created><updated>2016-02-26</updated><authors><author><keyname>Clazzer</keyname><forenames>Federico</forenames></author><author><keyname>Kissling</keyname><forenames>Christian</forenames></author><author><keyname>Marchese</keyname><forenames>Mario</forenames></author></authors><title>Exploiting Combination Techniques in Random Access MAC Protocols:
  Enhanced Contention Resolution ALOHA</title><categories>cs.IT math.IT</categories><comments>28 pages, 7 figures, first draft, corrected minor typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, random access (RA) protocols have acquired new interest from the
scientific community not only in satellite communication scenarios but also due
to the opening of new fields as smart grid and machine-to-machine (M2M)
applications. Unslotted ALOHA-like RA protocols are very attractive for such
applications thanks to the low complexity transmitters and to their complete
avoidance of synchronization requirements. Evolutions of the ALOHA protocol
employ time diversity through proactive replication of the packets but this
diversity is not fully exploited at the receiver. The novel RA protocol named
enhanced contention resolution ALOHA (ECRA) is presented here, which goes
beyond the limitation introducing combination techniques in the receiver
algorithm. The behaviour of ECRA is investigated through mathematical analysis
and simulations for two combination techniques, selection combining (SC) and
maximal-ratio combining (MRC). Approximations of the packet error rate (PER)
for unslotted RA protocols, including ECRA, well suited for the low channel
load regime is also presented. Furthermore, ECRA and recently presented RA
protocols, are analyzed in terms of different metrics, including spectral
efficiency, showing that ECRA is able to largely outperform both slotted and
unslotted schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07637</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07637</id><created>2016-02-24</created><updated>2016-03-08</updated><authors><author><keyname>Portugal</keyname><forenames>Ivens</forenames></author><author><keyname>Alencar</keyname><forenames>Paulo</forenames></author><author><keyname>Cowan</keyname><forenames>Donald</forenames></author></authors><title>A Survey on Domain-Specific Languages for Machine Learning in Big Data</title><categories>cs.SE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of data generated in the modern society is increasing rapidly. New
problems and novel approaches of data capture, storage, analysis and
visualization are responsible for the emergence of the Big Data research field.
Machine Learning algorithms can be used in Big Data to make better and more
accurate inferences. However, because of the challenges Big Data imposes, these
algorithms need to be adapted and optimized to specific applications. One
important decision made by software engineers is the choice of the language
that is used in the implementation of these algorithms. Therefore, this
literature survey identifies and describes domain-specific languages and
frameworks used for Machine Learning in Big Data. By doing this, software
engineers can then make more informed choices and beginners have an overview of
the main languages used in this domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07641</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07641</id><created>2016-02-24</created><authors><author><keyname>DePalma</keyname><forenames>Nick</forenames></author><author><keyname>Breazeal</keyname><forenames>Cynthia</forenames></author></authors><title>NIMBUS: A Hybrid Cloud-Crowd Realtime Architecture for Visual Learning
  in Interactive Domains</title><categories>cs.RO</categories><comments>Presented at &quot;2nd Workshop on Cognitive Architectures for Social
  Human-Robot Interaction 2016 (arXiv:1602.01868)&quot;</comments><report-no>CogArch4sHRI/2016/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotic architectures that incorporate cloud-based resources are just now
gaining popularity. However, researchers have very few investigations into
their capabilities to support claims of their feasibility. We propose a novel
method to exchange quality for speed of response. Further, we back this
assertion with empirical findings from experiments performed with Amazon
Mechanical Turk and find that our method improves quality in exchange for
response time in our cognitive architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07646</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07646</id><created>2016-02-19</created><updated>2016-02-25</updated><authors><author><keyname>Prok</keyname><forenames>Gary R.</forenames></author></authors><title>Life, The Mind, and Everything</title><categories>cs.GL</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incompleteness theorems of Godel, Turing, Chaitin, and Algorithmic
Information Theory have profound epistemological implications. Incompleteness
limits our ability to ever understand every observable phenomenon in the
universe. Incompleteness limits the ability of evolutionary processes from
finding optimal solutions. Incompleteness limits the detectability of machine
consciousness. This is an effort to convey these thoughts and results in a
somewhat entertaining manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07662</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07662</id><created>2015-07-01</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Features of formation of a distributive infrastructure of e-commerce in
  Russia</title><categories>cs.CY</categories><comments>18 pages, in Russian. ISSN 2075-1826</comments><journal-ref>Management and Business Administration. 2014. N 2. pp. 38-55</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article about objective laws of formation of a distributive infrastructure of
e-commerce. The distributive infrastructure of e-commerce, according to the
author, plays an important role in formation of network economy. The author
opens strategic value of institutional regulation of distributive logistics for
the decision problems of modernization of Russian economy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07667</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07667</id><created>2016-02-24</created><authors><author><keyname>Goranko</keyname><forenames>Valentin</forenames></author><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author><author><keyname>R&#xf6;nnholm</keyname><forenames>Raine</forenames></author></authors><title>Game-Theoretic Semantics for Alternating-Time Temporal Logic</title><categories>math.LO cs.GT cs.LO</categories><comments>The long version of a paper in AAMAS 2016</comments><acm-class>F.4.1; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce versions of game-theoretic semantics (GTS) for Alternating-Time
Temporal Logic (ATL). In GTS, truth is defined in terms of existence of a
winning strategy in a semantic evaluation game, and thus the game-theoretic
perspective appears in the framework of ATL on two semantic levels: on the
object level, in the standard semantics of the strategic operators, and on the
meta-level, where game-theoretic logical semantics can be applied to ATL. We
unify these two perspectives into semantic evaluation games specially designed
for ATL. The novel game-theoretic perspective enables us to identify new
variants of the semantics of ATL, based on limiting the time resources
available to the verifier and falsifier in the semantic evaluation game; we
introduce and analyse an unbounded and bounded GTS and prove these to be
equivalent to the standard (Tarski-style) compositional semantics. We also
introduce a non-equivalent finitely bounded semantics and argue that it is
natural from both logical and game-theoretic perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07675</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07675</id><created>2016-02-24</created><updated>2016-02-27</updated><authors><author><keyname>Severin</keyname><forenames>Daniel</forenames></author></authors><title>On the additive chromatic number of several families of graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $f:V \rightarrow \{1,\ldots,k\}$ be a labeling of the vertices of a graph
$G = (V,E)$ and denote with $f(N(v))$ the sum of the labels of all vertices
adjacent to $v$. The least value $k$ for which a graph $G$ admits a labeling
satisfying $f(N(u)) \neq f(N(v))$ for all $(u,v) \in E$ is called
\emph{additive chromatic number} of $G$ and denoted $\eta(G)$. It was first
presented by Czerwi\'nski, Grytczuk and Zelazny who also proposed a conjecture
that for every graph $G$, $\eta(G) \leq \chi(G)$, where $\chi(G)$ is the
chromatic number of $G$. Bounds of $\eta(G)$ are known for very few families of
graphs. In this work, we show that the conjecture holds for split graphs by
giving an upper bound of the additive chromatic number and we present exact
formulas for computing $\eta(G)$ when $G$ is a fan, windmill, circuit, wheel,
complete split, headless spider, cycle/wheel/complete sun, regular bipartite or
complete multipartite observing that the conjecture is satisfied in all cases.
In addition, we propose an integer programming formulation which is used for
checking the conjecture over all connected graphs up to 10 vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07679</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07679</id><created>2015-09-04</created><authors><author><keyname>Hewer</keyname><forenames>Alexander</forenames><affiliation>DFKI, MMCI</affiliation></author><author><keyname>Steiner</keyname><forenames>Ingmar</forenames><affiliation>DFKI, MMCI</affiliation></author><author><keyname>Bolkart</keyname><forenames>Timo</forenames><affiliation>MMCI</affiliation></author><author><keyname>Wuhrer</keyname><forenames>Stefanie</forenames><affiliation>MORPHEO</affiliation></author><author><keyname>Richmond</keyname><forenames>Korin</forenames><affiliation>CSTR</affiliation></author></authors><title>A statistical shape space model of the palate surface trained on 3D MRI
  scans of the vocal tract</title><categories>cs.CV</categories><comments>Proceedings of the 18th International Congress of Phonetic Sciences,
  Aug 2015, Glasgow, United Kingdom. 2015, http://www.icphs2015.info/</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a minimally-supervised method for computing a statistical shape
space model of the palate surface. The model is created from a corpus of
volumetric magnetic resonance imaging (MRI) scans collected from 12 speakers.
We extract a 3D mesh of the palate from each speaker, then train the model
using principal component analysis (PCA). The palate model is then tested using
3D MRI from another corpus and evaluated using a high-resolution optical scan.
We find that the error is low even when only a handful of measured coordinates
are available. In both cases, our approach yields promising results. It can be
applied to extract the palate shape from MRI data, and could be useful to other
analysis modalities, such as electromagnetic articulography (EMA) and
ultrasound tongue imaging (UTI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07714</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07714</id><created>2016-02-24</created><authors><author><keyname>van Hasselt</keyname><forenames>Hado</forenames></author><author><keyname>Guez</keyname><forenames>Arthur</forenames></author><author><keyname>Hessel</keyname><forenames>Matteo</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author></authors><title>Learning functions across many orders of magnitudes</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning non-linear functions can be hard when the magnitude of the target
function is unknown beforehand, as most learning algorithms are not scale
invariant. We propose an algorithm to adaptively normalize these targets. This
is complementary to recent advances in input normalization. Importantly, the
proposed method preserves the unnormalized outputs whenever the normalization
is updated to avoid instability caused by non-stationarity. It can be combined
with any learning algorithm and any non-linear function approximation,
including the important special case of deep learning. We empirically validate
the method in supervised learning and reinforcement learning and apply it to
learning how to play Atari 2600 games. Previous work on applying deep learning
to this domain relied on clipping the rewards to make learning in different
games more homogeneous, but this uses the domain-specific knowledge that in
these games counting rewards is often almost as informative as summing these.
Using our adaptive normalization we can remove this heuristic without
diminishing overall performance, and even improve performance on some games,
such as Ms. Pac-Man and Centipede, on which previous methods did not perform
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07715</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07715</id><created>2016-02-19</created><authors><author><keyname>Parker</keyname><forenames>Austin J.</forenames></author><author><keyname>Yancey</keyname><forenames>Kelly B.</forenames></author><author><keyname>Yancey</keyname><forenames>Matthew P.</forenames></author></authors><title>Regular Language Distance and Entropy</title><categories>cs.FL math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of determining the distance between two
regular languages. It will show how to expand Jaccard distance, which works on
finite sets, to potentially-infinite regular languages. The entropy of a
regular language plays a large role in the extension. Much of the paper is
spent investigating the entropy of a regular language. This includes addressing
issues that have required previous authors to rely on the upper limit of
Shannon's traditional formulation of entropy, because its limit does not always
exist. The paper also includes proposing a new limit based formulation for the
entropy of a regular language and proves that formulation to both exist and be
equivalent to Shannon's original formulation (when it exists). Additionally,
the proposed formulation is shown to equal an analogous but formally quite
different notion of topological entropy from Symbolic Dynamics -- consequently
also showing Shannon's original formulation to be equivalent to topological
entropy. Surprisingly, the natural Jaccard-like entropy distance is trivial in
most cases. Instead, the {\it entropy sum} distance metric is suggested, and
shown to be granular in certain situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07716</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07716</id><created>2016-02-24</created><authors><author><keyname>Grillo</keyname><forenames>S. A.</forenames></author><author><keyname>Marquezino</keyname><forenames>F. L.</forenames></author></authors><title>Quantum query without unitary operators</title><categories>quant-ph cs.CC</categories><comments>25 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quantum Query Model is a framework that allow us to express most known
quantum algorithms. Algorithms represented by this model consist on a set of
unitary operators acting over a finite Hilbert space, and a final measurement
step consisting on a set of projectors. In this work, we prove that the
application of these unitary operators before the measurement step is
equivalent to decomposing a unit vector into a sum of vectors and then
inverting some of their relative phases. We also prove that the vectors of that
sum must fulfill a list of properties and we call such vectors a Block Set. If
we take the same measurement step on the Quantum Query Model and on the Block
Set Formulation, then we prove that both formulations give the same Gram matrix
of output states, although the Block Set Formulation allows a much more
explicit form. Therefore, the Block Set reformulation of the Quantum Query
Model has advantages for the design of exact quantum algorithms. Finally, we
test these advantages of Block Sets by applying our approach to the
construction of quantum exact algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07720</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07720</id><created>2016-02-24</created><authors><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author><author><keyname>Pal</keyname><forenames>Martin</forenames></author><author><keyname>Vassilvitskii</keyname><forenames>Sergei</forenames></author></authors><title>A Field Guide to Personalized Reserve Prices</title><categories>cs.GT</categories><comments>Accepted to WWW'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the question of setting and testing reserve prices in single item
auctions when the bidders are not identical. At a high level, there are two
generalizations of the standard second price auction: in the lazy version we
first determine the winner, and then apply reserve prices; in the eager version
we first discard the bidders not meeting their reserves, and then determine the
winner among the rest. We show that the two versions have dramatically
different properties: lazy reserves are easy to optimize, and A/B test in
production, whereas eager reserves always lead to higher welfare, but their
optimization is NP-complete, and naive A/B testing will lead to incorrect
conclusions. Despite their different characteristics, we show that the overall
revenue for the two scenarios is always within a factor of 2 of each other,
even in the presence of correlated bids. Moreover, we prove that the eager
auction dominates the lazy auction on revenue whenever the bidders are
independent or symmetric. We complement our theoretical results with
simulations on real world data that show that even suboptimally set eager
reserve prices are preferred from a revenue standpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07721</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07721</id><created>2016-02-22</created><authors><author><keyname>Guzdial</keyname><forenames>Matthew</forenames></author><author><keyname>Riedl</keyname><forenames>Mark</forenames></author></authors><title>Toward Game Level Generation from Gameplay Videos</title><categories>cs.AI</categories><comments>8 pages, 10 figures, Procedural Content Generation Workshop 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms that generate computer game content require game design knowledge.
We present an approach to automatically learn game design knowledge for level
design from gameplay videos. We further demonstrate how the acquired design
knowledge can be used to generate sections of game levels. Our approach
involves parsing video of people playing a game to detect the appearance of
patterns of sprites and utilizing machine learning to build a probabilistic
model of sprite placement. We show how rich game design information can be
automatically parsed from gameplay videos and represented as a set of
generative probabilistic models. We use Super Mario Bros. as a proof of
concept. We evaluate our approach on a measure of playability and stylistic
similarity to the original levels as represented in the gameplay videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07726</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07726</id><created>2016-02-24</created><authors><author><keyname>Cummings</keyname><forenames>Rachel</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Adaptive Learning with Robust Generalization Guarantees</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional notion of generalization --- i.e., learning a hypothesis
whose empirical error is close to its true error --- is surprisingly brittle.
As has recently been noted [DFH+15b], even if several algorithms have this
guarantee in isolation, the guarantee need not hold if the algorithms are
composed adaptively. In this paper, we study three notions of generalization
---increasing in strength--- that are robust to post-processing and amenable to
adaptive composition, and examine the relationships between them.
  We call the weakest such notion Robust Generalization. A second,
intermediate, notion is the stability guarantee known as differential privacy.
The strongest guarantee we consider we call Perfect Generalization. We prove
that every hypothesis class that is PAC learnable is also PAC learnable in a
robustly generalizing fashion, albeit with an exponential blowup in sample
complexity. We conjecture that a stronger version of this theorem also holds
that avoids any blowup in sample complexity (and, in fact, it would, subject to
a longstanding conjecture [LW86, War03]). It was previously known that
differentially private algorithms satisfy robust generalization. In this paper,
we show that robust generalization is a strictly weaker concept, and that there
is a learning task that can be carried out subject to robust generalization
guarantees, yet cannot be carried out subject to differential privacy,
answering an open question of [DFH+15a]. We also show that perfect
generalization is a strictly stronger guarantee than differential privacy, but
that, nevertheless, many learning tasks can be carried out subject to the
guarantees of perfect generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07729</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07729</id><created>2016-02-24</created><authors><author><keyname>Shiraga</keyname><forenames>Takeharu</forenames></author></authors><title>The Cover Time of Deterministic Random Walks for General Transition
  Probabilities</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deterministic random walk is a deterministic process analogous to a
random walk. While there are some results on the cover time of the rotor-router
model, which is a deterministic random walk corresponding to a simple random
walk, nothing is known about the cover time of deterministic random walks
emulating general transition probabilities. This paper is concerned with the
SRT-router model with multiple tokens, which is a deterministic process coping
with general transition probabilities possibly containing irrational numbers.
For the model, we give an upper bound of the cover time, which is the first
result on the cover time of deterministic random walks for general transition
probabilities. Our upper bound also improves the existing bounds for the
rotor-router model in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07731</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07731</id><created>2016-02-24</created><authors><author><keyname>Giordani</keyname><forenames>Marco</forenames></author><author><keyname>Mezzavilla</keyname><forenames>Marco</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Initial Access in 5G mm-Wave Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 3 figures, 3 tables, 15 references, submitted to IEEE COMMAG
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The massive amounts of bandwidth available at millimeter-wave frequencies
(roughly above 10 GHz) have the potential to greatly increase the capacity of
fifth generation cellular wireless systems. However, to overcome the high
isotropic pathloss experienced at these frequencies, high directionality will
be required at both the base station and the mobile user equipment to establish
sufficient link budget in wide area net- works. This reliance on directionality
has important implications for control layer procedures. Initial access in
particular can be significantly delayed due to the need for the base station
and the user to find the proper alignment for directional transmission and
reception. This paper provides a survey of several recently proposed techniques
for this purpose. A coverage and delay analysis is performed to compare various
techniques including exhaustive and iterative search, and Context-Information
based algorithms. We show that the best strategy depends on the target SNR
regime, and provide guidelines to characterize the optimal choice as a function
of the system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07732</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07732</id><created>2016-02-24</created><authors><author><keyname>Rebato</keyname><forenames>Mattia</forenames></author><author><keyname>Mezzavilla</keyname><forenames>Marco</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>The Potential of Resource Sharing in 5G Millimeter-Wave Bands</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the severe spectrum shortage in conventional cellular bands, the
millimeter (mmWave) frequencies, roughly above 10~GHz, have been attracting
growing attention for next-generation micro- and pico- cellular wireless
networks. A fundamental and open question is how these bands should be used by
cellular operators. Cellular spectrum has been traditionally allocated
following an exclusive ownership model. However, in this paper we argue that
the distinct nature of mmWave communication -- the massive bandwidth degrees of
freedom, directional isolation and high susceptibility to blockage -- suggest
that spectrum and infrastructure sharing between multiple operators may be
necessary to exploit the full potential of these bands. High-level capacity
analyses are presented that reveal significant possible gains under spectrum
and infrastructure sharing, even under minimal coordination between operators.
Moreover, we discuss how network technologies including software defined
networks (SDNs) and network function virtualization (NFV) can easily enable
resource sharing by having a programmable core entity provide transparent
inter-operator access to the end user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07733</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07733</id><created>2016-02-24</created><authors><author><keyname>Teague</keyname><forenames>Harris</forenames></author></authors><title>Comparison of Attitude Estimation Techniques for Low-cost Unmanned
  Aerial Vehicles</title><categories>math.OC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attitude estimation for small, low-cost unmanned aerial vehicles is often
achieved using a relatively simple complementary filter that combines onboard
accelerometers, gyroscopes, and magnetometer sensing. This paper explores the
limits of performance of such attitude estimation, with a focus on performance
in highly dynamic maneuvers. The complementary filter is derived along with the
extended Kalman filter and unscented Kalman filter to evaluate the potential
performance gains when using a more sophisticated estimator. Simulations are
presented that compare performance across a range of test cases, many where
ground truth was generated from manually controlled flights in a flight
simulator. Estimator scenarios that are generic across the different estimator
types (such as the way sensor information is processed, and the use of
dynamically changing gains) are compared across the test cases. An appendix is
included as a quick reference for the common attitude representations and their
kinematic expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07736</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07736</id><created>2016-02-24</created><authors><author><keyname>Wan</keyname><forenames>Yiming</forenames></author><author><keyname>Keviczky</keyname><forenames>Tamas</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author></authors><title>Robust Air Data Sensor Fault Diagnosis With Enhanced Fault Sensitivity
  Using Moving Horizon Estimation</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates robust fault diagnosis of multiple air data sensor
faults in the presence of winds. The trade-off between robustness to winds and
sensitivity to faults is challenging due to simultaneous influence of winds and
latent faults on monitored sensors. Different from conventional residual
generators that do not consider any constraints, we propose a constrained
residual generator using moving horizon estimation. The main contribution is
improved fault sensitivity by exploiting known bounds on winds in residual
generation. By analyzing the Karush-Kuhn-Tucker conditions of the formulated
moving horizon estimation problem, it is shown that this improvement is
attributed to active inequality constraints caused by faults. When the
weighting matrices in the moving horizon estimation problem are tuned to
increase robustness to winds, its fault sensitivity does not simply decrease as
one would expect in conventional unconstrained residual generators. Instead,
its fault sensitivity increases when the fault is large enough to activate some
inequality constraints. This fault sensitivity improvement is not restricted to
this particular application, but can be achieved by any general moving horizon
estimation based residual generator. A high-fidelity Airbus simulator is used
to illustrate the advantage of our proposed approach in terms of fault
sensitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07743</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07743</id><created>2016-02-24</created><authors><author><keyname>Taranalli</keyname><forenames>Veeresh</forenames></author><author><keyname>Uchikawa</keyname><forenames>Hironori</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author></authors><title>Channel Models for Multi-Level Cell Flash Memories Based on Empirical
  Error Analysis</title><categories>cs.IT math.IT</categories><comments>12 pages, submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose binary discrete parametric channel models for multi-level cell
(MLC) flash memories that provide accurate ECC performance estimation by
modeling the empirically observed error characteristics under program/erase
(P/E) cycling stress. Through a detailed empirical error characterization of
1X-nm and 2Y-nm MLC flash memory chips from two different vendors, we observe
and characterize the overdispersion phenomenon in the number of bit errors per
ECC frame. A well studied channel model such as the binary asymmetric channel
(BAC) model is unable to provide accurate ECC performance estimation. Hence we
propose a channel model based on the beta-binomial probability distribution
(2-BBM channel model) which is a good fit for the overdispersed empirical error
characteristics and show through statistical tests and simulation results for
BCH, LDPC and polar codes, that the 2-BBM channel model provides accurate ECC
performance estimation in MLC flash memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07745</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07745</id><created>2016-02-24</created><authors><author><keyname>Chen</keyname><forenames>Juntao</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>Interdependent Network Formation Games</title><categories>cs.SI cs.GT</categories><comments>2 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing optimal interdependent networks is important for the robustness and
efficiency of national critical infrastructures. Here, we establish a
two-person game-theoretic model in which two network designers choose to
maximize the global connectivity independently. This framework enables
decentralized network design by using iterative algorithms. After a finite
number of steps, the algorithm will converge to a Nash equilibrium, and yields
the equilibrium topology of the network. We corroborate our results by using
numerical experiments, and compare the Nash equilibrium solutions with their
team solution counterparts. The experimental results of the game method and the
team method provide design guidelines to increase the efficiency of the
interdependent network formation games. Moreover, the proposed game framework
can be generally applied to a diverse number of applications, including power
system networks and international air transportation networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07749</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07749</id><created>2016-02-24</created><authors><author><keyname>Nguyen</keyname><forenames>Thien Huu</forenames></author><author><keyname>Sil</keyname><forenames>Avirup</forenames></author><author><keyname>Dinu</keyname><forenames>Georgiana</forenames></author><author><keyname>Florian</keyname><forenames>Radu</forenames></author></authors><title>Toward Mention Detection Robustness with Recurrent Neural Networks</title><categories>cs.CL</categories><comments>13 pages, 11 tables, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key challenges in natural language processing (NLP) is to yield
good performance across application domains and languages. In this work, we
investigate the robustness of the mention detection systems, one of the
fundamental tasks in information extraction, via recurrent neural networks
(RNNs). The advantage of RNNs over the traditional approaches is their capacity
to capture long ranges of context and implicitly adapt the word embeddings,
trained on a large corpus, into a task-specific word representation, but still
preserve the original semantic generalization to be helpful across domains. Our
systematic evaluation for RNN architectures demonstrates that RNNs not only
outperform the best reported systems (up to 9\% relative error reduction) in
the general setting but also achieve the state-of-the-art performance in the
cross-domain setting for English. Regarding other languages, RNNs are
significantly better than the traditional methods on the similar task of named
entity recognition for Dutch (up to 22\% relative error reduction).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07750</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07750</id><created>2016-02-24</created><authors><author><keyname>Chen</keyname><forenames>Jian-Jia</forenames></author><author><keyname>Huang</keyname><forenames>Wen-Hung</forenames></author><author><keyname>Nelissen</keyname><forenames>Geoffrey</forenames></author></authors><title>A Note on Modeling Self-Suspending Time as Blocking Time in Real-Time
  Systems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents a proof to support the correctness of the schedulability
test for self-suspending real-time task systems proposed by Jane W. S. Liu in
her book titled &quot;Real-Time Systems&quot; (Pages 164-165). The same concept was also
implicitly used by Rajkumar, Sha, and Lehoczky in RTSS 1988 (Page 267) for
analyzing self-suspending behaviour due to synchronization protocols in
multiprocessor systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07757</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07757</id><created>2016-02-24</created><authors><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Murin</keyname><forenames>Yonathan</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Capacity Limits of Diffusion-Based Molecular Timing Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces capacity limits for molecular timing (MT) channels,
where information is modulated on the release timing of small information
particles, and decoded from the time of arrival at the receiver. It is shown
that the random time of arrival can be represented as an additive noise
channel, and for the diffusion-based MT (DBMT) channel this noise is
distributed according to the Levy distribution. Lower and upper bounds on
capacity of the DBMT channel are derived for the case where the delay
associated with the propagation of information particles in the channel is
finite. These bounds are also shown to be tight. Moreover, it is shown that by
simultaneously releasing multiple particles, the capacity linearly increases
with the number of particles. This is analogous to receive diversity as each
particle takes a random independent path. This diversity of paths can be used
to increase data rate significantly since, in molecular communication systems,
it is possible to release many particles simultaneously and hence obtain the
associated linear capacity increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07762</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07762</id><created>2016-02-24</created><authors><author><keyname>Zhang</keyname><forenames>Chuanzong</forenames></author><author><keyname>Yuan</keyname><forenames>Zhengdao</forenames></author><author><keyname>Wang</keyname><forenames>Zhongyong</forenames></author><author><keyname>Guo</keyname><forenames>Qinghua</forenames></author></authors><title>Low Complexity Sparse Bayesian Learning Using Combined BP and MF with a
  Stretched Factor Graph</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, prepared for Signal processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns message passing based approaches to sparse Bayesian
learning (SBL) with a linear model corrupted by additive white Gaussian noise
with unknown variance. With the conventional factor graph, mean field (MF)
message passing based algorithms have been proposed in the literature. In this
work, instead of using the conventional factor graph, we modify the factor
graph by adding some extra hard constraints (the graph looks like being
`stretched'), which enables the use of combined belief propagation (BP) and MF
message passing. We then propose a low complexity BP-MF SBL algorithm based on
which an approximate BP-MF SBL algorithm is also developed to further reduce
the complexity. Thanks to the use of BP, the BP-MF SBL algorithms show their
merits compared with state-of-the-art MF SBL algorithms: they deliver even
better performance with much lower complexity compared with the vector-form MF
SBL algorithm and they significantly outperform the scalar-form MF SBL
algorithm with similar complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07763</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07763</id><created>2016-02-24</created><authors><author><keyname>Zhang</keyname><forenames>Yingrui</forenames></author><author><keyname>Yagan</keyname><forenames>Osman</forenames></author></authors><title>Optimizing the robustness of electrical power systems against cascading
  failures</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>18 pages including 2 pages of supplementary file, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electrical power systems are one of the most important infrastructures that
support our society. However, their vulnerabilities have raised great concern
recently due to several large-scale blackouts around the world. In this paper,
we investigate the robustness of power systems against cascading failures
initiated by a random attack. This is done under a simple yet useful model
based on global and equal redistribution of load upon failures. We provide a
complete understanding of system robustness by i) deriving an expression for
the final system size as a function of the size of initial attacks; ii)
deriving the critical attack size after which system breaks down completely;
iii) showing that complete system breakdown takes place through a first-order
(i.e., discontinuous) transition in terms of the attack size; and iv)
establishing the optimal load-capacity distribution that maximizes robustness.
In particular, we show that robustness is maximized when the difference between
the capacity and initial load is the same for all lines; i.e., when all lines
have the same redundant space regardless of their initial load. This is in
contrast with the intuitive and commonly used setting where capacity of a line
is a fixed factor of its initial load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07764</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07764</id><created>2016-02-24</created><authors><author><keyname>Azizzadenesheli</keyname><forenames>Kamyar</forenames></author><author><keyname>Lazaric</keyname><forenames>Alessandro</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author></authors><title>Reinforcement Learning of POMDP's using Spectral Methods</title><categories>cs.AI cs.LG cs.NA math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new reinforcement learning algorithm for partially observable
Markov decision processes (POMDP) based on spectral decomposition methods.
While spectral methods have been previously employed for consistent learning of
(passive) latent variable models such as hidden Markov models, POMDPs are more
challenging since the learner interacts with the environment and possibly
changes the future observations in the process. We devise a learning algorithm
running through episodes, in each episode we employ spectral techniques to
learn the POMDP parameters from a trajectory generated by a fixed policy. At
the end of the episode, an optimization oracle returns the optimal memoryless
planning policy which maximizes the expected reward based on the estimated
POMDP model. We prove an order-optimal regret bound w.r.t. the optimal
memoryless policy and efficient scaling with respect to the dimensionality of
observation and action spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07767</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07767</id><created>2016-02-24</created><authors><author><keyname>Hamke</keyname><forenames>Eric E.</forenames></author><author><keyname>Jordan</keyname><forenames>Ramiro</forenames></author><author><keyname>Ramon-Martinez</keyname><forenames>Manel</forenames></author></authors><title>Breath Activity Detection Algorithm</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes the use of a support vector machines with a novel
kernel, to determine the breathing rate and inhalation duration of a fire
fighter wearing a Self-Contained Breathing Apparatus. With this information, an
incident commander can monitor the firemen in his command for exhaustion and
ensure timely rotation of personnel to ensure overall fire fighter safety
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07776</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07776</id><created>2016-02-24</created><authors><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Kuncoro</keyname><forenames>Adhiguna</forenames></author><author><keyname>Ballesteros</keyname><forenames>Miguel</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Recurrent Neural Network Grammars</title><categories>cs.CL cs.NE</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce recurrent neural network grammars, probabilistic models of
sentences with explicit phrase structure. We explain efficient inference
procedures that allow application to both parsing and language modeling.
Experiments show that they provide better parsing in English than any single
previously published supervised generative model and better language modeling
than state-of-the-art sequential RNNs in English and Chinese.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07781</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07781</id><created>2016-02-24</created><authors><author><keyname>Stokes</keyname><forenames>Jonathan</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>A Markov chain model for the search time for max degree nodes in a graph
  using a biased random walk</title><categories>cs.SI cs.DM physics.soc-ph</categories><comments>7 pages, 7 figures, submitted to CISS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the expected time to find a maximum
degree node on a graph using a (parameterized) biased random walk. For
assortative graphs the positive degree correlation serves as a local gradient
for which a bias towards selecting higher degree neighbors will on average
reduce the search time. Unfortunately, although the expected absorption time on
the graph can be written down using the theory of absorbing Markov chains,
computing this time is infeasible for large graphs. With this motivation, we
construct an absorbing Markov chain with a state for each degree of the graph,
and observe computing the expected absorption time is now computationally
feasible. Our paper finds preliminary results along the following lines: i)
there are graphs for which the proposed Markov model does and graphs for which
the model does not capture the absorbtion time, ii) there are graphs where
random sampling outperforms biased random walks, and graphs where biased random
walks are superior, and iii) the optimal bias parameter for the random walk is
graph dependent, and we study the dependence on the graph assortativity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07783</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07783</id><created>2016-02-24</created><updated>2016-02-26</updated><authors><author><keyname>Kang</keyname><forenames>Zhao</forenames></author><author><keyname>Cheng</keyname><forenames>Qiang</forenames></author></authors><title>Top-N Recommendation with Novel Rank Approximation</title><categories>cs.IR cs.AI stat.ML</categories><comments>SDM 2016. arXiv admin note: text overlap with arXiv:1601.04800</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of accurate recommender systems has been widely recognized by
academia and industry. However, the recommendation quality is still rather low.
Recently, a linear sparse and low-rank representation of the user-item matrix
has been applied to produce Top-N recommendations. This approach uses the
nuclear norm as a convex relaxation for the rank function and has achieved
better recommendation accuracy than the state-of-the-art methods. In the past
several years, solving rank minimization problems by leveraging nonconvex
relaxations has received increasing attention. Some empirical results
demonstrate that it can provide a better approximation to original problems
than convex relaxation. In this paper, we propose a novel rank approximation to
enhance the performance of Top-N recommendation systems, where the
approximation error is controllable. Experimental results on real data show
that the proposed rank approximation improves the Top-$N$ recommendation
accuracy substantially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07787</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07787</id><created>2016-02-24</created><authors><author><keyname>Winter</keyname><forenames>Philipp</forenames></author><author><keyname>Ensafi</keyname><forenames>Roya</forenames></author><author><keyname>Loesing</keyname><forenames>Karsten</forenames></author><author><keyname>Feamster</keyname><forenames>Nick</forenames></author></authors><title>Identifying and characterizing Sybils in the Tor network</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being a volunteer-run, distributed anonymity network, Tor is vulnerable to
Sybil attacks. Little is known about real-world Sybils in the Tor network, and
we lack practical tools and methods to expose Sybil attacks. In this work, we
develop sybilhunter, the first system for detecting Sybil relays based on their
appearance, such as configuration; and behavior, such as uptime sequences. We
used sybilhunter's diverse analysis techniques to analyze nine years of
archived Tor network data, providing us with new insights into the operation of
real-world attackers. Our findings include diverse Sybils, ranging from
botnets, to academic research, and relays that hijack Bitcoin transactions. Our
work shows that existing Sybil defenses do not apply to Tor, it delivers
insights into real-world attacks, and provides practical tools to uncover and
characterize Sybils, making the network safer for its users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07795</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07795</id><created>2016-02-25</created><authors><author><keyname>Fletcher</keyname><forenames>Alyson</forenames></author><author><keyname>Sahraee-Ardakan</keyname><forenames>Mojtaba</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>Expectation Consistent Approximate Inference: Generalizations and
  Convergence</title><categories>cs.IT math.IT stat.ML</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximations of loopy belief propagation, including expectation propagation
and approximate message passing, have attracted considerable attention for
probabilistic inference problems. This paper proposes and analyzes a
generalization of Opper and Winther's expectation consistent (EC) approximate
inference method. The proposed method, called Generalized Expectation
Consistency (GEC), can be applied to both maximum a posteriori (MAP) and
minimum mean squared error (MMSE) estimation. Here we characterize its fixed
points, convergence, and performance relative to the replica prediction of
optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07796</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07796</id><created>2015-09-15</created><updated>2016-02-27</updated><authors><author><keyname>Xu</keyname><forenames>Jin</forenames></author></authors><title>Probe Machine</title><categories>cs.CC</categories><comments>12 pages, 13 figures</comments><msc-class>68Qxx</msc-class><acm-class>F.1.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A novel computing model, called \emph{Probe Machine}, is proposed in this
paper. Different from Turing Machine, Probe Machine is a fully-parallel
computing model in the sense that it can simultaneously process multiple pairs
of data, rather than sequentially process every pair of linearly-adjacent data.
In this paper, we establish the mathematical model of Probe Machine as a
9-tuple consisting of data library, probe library, data controller, probe
controller, probe operation, computing platform, detector, true solution
storage, and residue collector. We analyze the computation capability of the
Probe Machine model, and in particular we show that Turing Machine is a special
case of Probe Machine. We revisit two NP-complete problems---i.e., the Graph
Coloring and Hamilton Cycle problems, and devise two algorithms on basis of the
established Probe Machine model, which can enumerate all solutions to each of
these problems by only one probe operation. Furthermore, we show that Probe
Machine can be implemented by leveraging the nano-DNA probe technologies. The
computational power of an electronic computer based on Turing Machine is known
far more than that of the human brain. A question naturally arises: will a
future computer based on Probe Machine outperform the human brain in more ways
beyond the computational power?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07799</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07799</id><created>2016-02-25</created><authors><author><keyname>Kanagavalli</keyname><forenames>V. R.</forenames></author><author><keyname>Maheeja</keyname><forenames>G.</forenames></author></authors><title>A Study on the usage of Data Structures in Information Retrieval</title><categories>cs.IR</categories><comments>National Conference on Innovations in Communication and Computing
  Technologies Feb 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tries to throw light in the usage of data structures in the field
of information retrieval. Information retrieval is an area of study which is
gaining momentum as the need and urge for sharing and exploring information is
growing day by day. Data structures have been the area of research for a long
period in the arena of computer science. The need to have efficient data
structures has become even more important as the data grows in an exponential
nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07803</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07803</id><created>2016-02-25</created><authors><author><keyname>Haque</keyname><forenames>Md. Masudul</forenames></author><author><keyname>Habib</keyname><forenames>Md. Tarek</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Mokhlesur</forenames></author></authors><title>Automated Word Prediction in Bangla Language Using Stochastic Language
  Models</title><categories>cs.CL</categories><comments>in International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST) Vol.5, No.6, November 2015</comments><doi>10.5121/ijfcst.2015.5607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word completion and word prediction are two important phenomena in typing
that benefit users who type using keyboard or other similar devices. They can
have profound impact on the typing of disable people. Our work is based on word
prediction on Bangla sentence by using stochastic, i.e. N-gram language model
such as unigram, bigram, trigram, deleted Interpolation and backoff models for
auto completing a sentence by predicting a correct word in a sentence which
saves time and keystrokes of typing and also reduces misspelling. We use large
data corpus of Bangla language of different word types to predict correct word
with the accuracy as much as possible. We have found promising results. We hope
that our work will impact on the baseline for automated Bangla typing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07807</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07807</id><created>2016-02-25</created><authors><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Strauss</keyname><forenames>Benjamin</forenames></author></authors><title>Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly
  Detection</title><categories>cs.DB cs.CL stat.ML</categories><comments>8 pages, 4 figures, 5 tables; to appear in Proceedings of the Tenth
  IEEE International Conference on Semantic Computing (ICSC 2016), Laguna
  Hills, California, February 2016</comments><acm-class>I.5.1; I.5.4; G.3; I.2.7; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many important forms of data are stored digitally in XML format. Errors can
occur in the textual content of the data in the fields of the XML. Fixing these
errors manually is time-consuming and expensive, especially for large amounts
of data. There is increasing interest in the research, development, and use of
automated techniques for assisting with data cleaning. Electronic dictionaries
are an important form of data frequently stored in XML format that frequently
have errors introduced through a mixture of manual typographical entry errors
and optical character recognition errors. In this paper we describe methods for
flagging statistical anomalies as likely errors in electronic dictionaries
stored in XML format. We describe six systems based on different sources of
information. The systems detect errors using various signals in the data
including uncommon characters, text length, character-based language models,
word-based language models, tied-field length ratios, and tied-field
transliteration models. Four of the systems detect errors based on expectations
automatically inferred from content within elements of a single field type. We
call these single-field systems. Two of the systems detect errors based on
correspondence expectations automatically inferred from content within elements
of multiple related field types. We call these tied-field systems. For each
system, we provide an intuitive analysis of the type of error that it is
successful at detecting. Finally, we describe two larger-scale evaluations
using crowdsourcing with Amazon's Mechanical Turk platform and using the
annotations of a domain expert. The evaluations consistently show that the
systems are useful for improving the efficiency with which errors in XML
electronic dictionaries can be detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07808</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07808</id><created>2016-02-25</created><authors><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Sathiaseelan</keyname><forenames>Arjuna</forenames></author><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author></authors><title>&quot;Resource Pooling&quot; for Wireless Networks: Solutions for the Developing
  World</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We live in a world in which there is a great disparity between the lives of
the rich and the poor. Technology offers great promise in bridging this gap. In
particular, wireless technology unfetters developing communities from the
constraints of infrastructure providing a great opportunity to leapfrog years
of neglect and technological waywardness. In this paper, we highlight the role
of resource pooling for wireless networks in the developing world. Resource
pooling involves: (i) abstracting a collection of networked resources to behave
like a single unified resource pool and (ii) developing mechanisms for shifting
load between the various parts of the unified resource pool. The popularity of
resource pooling stems from its ability to provide resilience, high
utilization, and flexibility at an acceptable cost. We show that &quot;resource
pooling&quot;, which is very popular in its various manifestations, is the key
unifying principle underlying a diverse number of successful wireless
technologies (such as white space networking, community networks, etc.). We
discuss various applications of resource pooled wireless technologies and
provide a discussion on open issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07810</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07810</id><created>2016-02-25</created><authors><author><keyname>Ali</keyname><forenames>Anwaar</forenames></author><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Rasool</keyname><forenames>Raihan ur</forenames></author><author><keyname>Sathiaseelan</keyname><forenames>Arjuna</forenames></author><author><keyname>Zwitter</keyname><forenames>Andrej</forenames></author></authors><title>Big Data For Development: Applications and Techniques</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosion of social media sites and proliferation of digital
computing devices and Internet access, massive amounts of public data is being
generated on a daily basis. Efficient techniques/ algorithms to analyze this
massive amount of data can provide near real-time information about emerging
trends and provide early warning in case of an imminent emergency (such as the
outbreak of a viral disease). In addition, careful mining of these data can
reveal many useful indicators of socioeconomic and political events, which can
help in establishing effective public policies. The focus of this study is to
review the application of big data analytics for the purpose of human
development. The emerging ability to use big data techniques for development
(BD4D) promises to revolutionalize healthcare, education, and agriculture;
facilitate the alleviation of poverty; and help to deal with humanitarian
crises and violent conflicts. Besides all the benefits, the large-scale
deployment of BD4D is beset with several challenges due to the massive size,
fast-changing and diverse nature of big data. The most pressing concerns relate
to efficient data acquisition and sharing, establishing of context (e.g.,
geolocation and time) and veracity of a dataset, and ensuring appropriate
privacy. In this study, we provide a review of existing BD4D work to study the
impact of big data on the development of society. In addition to reviewing the
important works, we also highlight important challenges and open issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07811</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07811</id><created>2016-02-25</created><updated>2016-02-26</updated><authors><author><keyname>Bost</keyname><forenames>Xavier</forenames><affiliation>LIA</affiliation></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIA</affiliation></author><author><keyname>Linar&#xe8;s</keyname><forenames>Georges</forenames><affiliation>LIA</affiliation></author></authors><title>Narrative smoothing: dynamic conversational network for the analysis of
  TV Series plots</title><categories>cs.SI cs.MM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern popular tv series often develop complex storylines spanning several
seasons, but are usually watched in quite a discontinuous way. As a result, the
viewer generally needs a comprehensive summary of the previous season plot
before the new one starts. The automation of such a task requires to identify
and characterize the dynamics of the series sub-plots. One way of doing so is
to study the underlying social network of interactions between the characters
involved in the narrative. The standard tools used in the Social Networks
Analysis field to extract such a network rely on an integration of time, either
over the whole considered period, or as a sequence of several time-slices.
However, they turn out to be inappropriate in the case of tv series, due to the
fact the scenes showed onscreen alternatively focus on parallel sto-rylines,
and do not necessarily respect a traditional chronology. This makes existing
extraction methods inefficient to describe the dynamics of relationships
between characters, or to get a relevant instantaneous view of the current
social state in the plot. This is especially true for characters shown as
interacting with each other at some previous point in the plot but temporarily
neglected by the narrative. In this article, we introduce narrative smoothing,
a novel, still exploratory, network extraction method. It smooths the
relationship dynamics based on the plot properties, aiming at solving some of
the limitations present in the standard approaches. In order to assess our
method, we apply it to a new corpus of 3 popular tv series, and compare it to
both standard approaches. Our results are promising, showing narrative
smoothing leads to more relevant observations when it comes to the
characterization of the protagonists and their relationships. It could be used
as a basis for further model-ing the interleaved storylines constituting tv
series plots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07813</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07813</id><created>2016-02-25</created><authors><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Ali</keyname><forenames>Anwaar</forenames></author><author><keyname>Rasool</keyname><forenames>Raihan ur</forenames></author><author><keyname>Zwitter</keyname><forenames>Andrej</forenames></author><author><keyname>Sathiaseelan</keyname><forenames>Arjuna</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author></authors><title>Crisis Analytics: Big Data Driven Crisis Response</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disasters have long been a scourge for humanity. With the advances in
technology (in terms of computing, communications, and the ability to process
and analyze big data), our ability to respond to disasters is at an inflection
point. There is great optimism that big data tools can be leveraged to process
the large amounts of crisis-related data (in the form of user generated data in
addition to the traditional humanitarian data) to provide an insight into the
fast-changing situation and help drive an effective disaster response. This
article introduces the history and the future of big crisis data analytics,
along with a discussion on its promise, challenges, and pitfalls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07816</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07816</id><created>2016-02-25</created><authors><author><keyname>Durocher</keyname><forenames>Stephane</forenames></author><author><keyname>Mondal</keyname><forenames>Debajyoti</forenames></author></authors><title>Relating Graph Thickness to Planar Layers and Bend Complexity</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The thickness of a graph $G=(V,E)$ with $n$ vertices is the minimum number of
planar subgraphs of $G$ whose union is $G$. A polyline drawing of $G$ in
$\mathbb{R}^2$ is a drawing $\Gamma$ of $G$, where each vertex is mapped to a
point and each edge is mapped to a polygonal chain. Bend and layer complexities
are two important aesthetics of such a drawing. The bend complexity of $\Gamma$
is the maximum number of bends per edge in $\Gamma$, and the layer complexity
of $\Gamma$ is the minimum integer $r$ such that the set of polygonal chains in
$\Gamma$ can be partitioned into $r$ disjoint sets, where each set corresponds
to a planar polyline drawing. Let $G$ be a graph of thickness $t$. By
F\'{a}ry's theorem, if $t=1$, then $G$ can be drawn on a single layer with bend
complexity $0$. A few extensions to higher thickness are known, e.g., if $t=2$
(resp., $t&gt;2$), then $G$ can be drawn on $t$ layers with bend complexity 2
(resp., $3n+O(1)$). However, allowing a higher number of layers may reduce the
bend complexity, e.g., complete graphs require $\Theta(n)$ layers to be drawn
using 0 bends per edge.
  In this paper we present an elegant extension of F\'{a}ry's theorem to draw
graphs of thickness $t&gt;2$. We first prove that thickness-$t$ graphs can be
drawn on $t$ layers with $2.25n+O(1)$ bends per edge. We then develop another
technique to draw thickness-$t$ graphs on $t$ layers with bend complexity,
i.e., $O(\sqrt{2}^{t} \cdot n^{1-(1/\beta)})$, where $\beta = 2^{\lceil (t-2)/2
\rceil }$. Previously, the bend complexity was not known to be sublinear for
$t&gt;2$. Finally, we show that graphs with linear arboricity $k$ can be drawn on
$k$ layers with bend complexity $\frac{3^{k-1}n}{4\cdot 3^{k-2} + 1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07827</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07827</id><created>2016-02-25</created><authors><author><keyname>Hell</keyname><forenames>Pavol</forenames></author><author><keyname>Nevisi</keyname><forenames>Mayssam Mohammadi</forenames></author></authors><title>Minimum Cost Homomorphisms with Constrained Costs</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimum cost homomorphism problems can be viewed as a generalization of list
homomorphism problems. They also extend two well-known graph colouring
problems: the minimum colour sum problem and the optimum cost chromatic
partition problem. In both of these problems, the cost function meets an
additional constraint: the cost of using a specific colour is the same for
every vertex of the input graph. We study minimum cost homomorphism problems
with cost functions constrained to have this property. Clearly, when the
standard minimum cost homomorphism problem is polynomial, then the problem with
constrained costs is also polynomial. We expect that the same may hold for the
cases when the standard minimum cost homomorphism problem is NP-complete. We
prove that this is the case for trees $H$: we obtain a dichotomy of minimum
constrained cost homomorphism problems which coincides with the dichotomy of
standard minimum cost homomorphism problems. For general graphs $H$, we prove a
partial dichotomy: the problem is polynomial if $H$ is a proper interval graph
and NP-complete when $H$ is not chordal bipartite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07838</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07838</id><created>2016-02-25</created><authors><author><keyname>Alfeilat</keyname><forenames>Haneen Abu</forenames></author></authors><title>Visualizing Class Information</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class is used in object oriented programming to describe each object in the
system. It is as a template contains the methods and attributes for each
object. The volume of information within the class has a role in the time
required for its implementation, testing and understanding the class.
Developers are dealing with large projects that contain a large number of Lines
Of Code (LOC). So that, extracting information about the classes requires time
and big effort from the developers. To solve this problem, we present a
visualization approach to display class information for developers. Our method
assumed each class is a cone chart and each cone represents one of the class
metrics that include number of methods, attributes and the number of Lines Of
Code (LOC). The height of each cone indicates the total number of methods,
attributes and number of Lines Of Code (LOC) which found in a class. Also, each
cone has a different color from the other to facilitate the distinction among
them
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07844</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07844</id><created>2016-02-25</created><authors><author><keyname>Zheng</keyname><forenames>Shuai</forenames></author><author><keyname>Zhang</keyname><forenames>Ruiliang</forenames></author><author><keyname>Kwok</keyname><forenames>James T.</forenames></author></authors><title>Fast Nonsmooth Regularized Risk Minimization with Continuation</title><categories>cs.LG</categories><comments>AAAI-2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In regularized risk minimization, the associated optimization problem becomes
particularly difficult when both the loss and regularizer are nonsmooth.
Existing approaches either have slow or unclear convergence properties, are
restricted to limited problem subclasses, or require careful setting of a
smoothing parameter. In this paper, we propose a continuation algorithm that is
applicable to a large class of nonsmooth regularized risk minimization
problems, can be flexibly used with a number of existing solvers for the
underlying smoothed subproblem, and with convergence results on the whole
algorithm rather than just one of its subproblems. In particular, when
accelerated solvers are used, the proposed algorithm achieves the fastest known
rates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex
problems. Experiments on nonsmooth classification and regression tasks
demonstrate that the proposed algorithm outperforms the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07851</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07851</id><created>2016-02-25</created><authors><author><keyname>Lemaitre</keyname><forenames>Sophie</forenames></author><author><keyname>Salnikov</keyname><forenames>Vladimir</forenames></author><author><keyname>Choi</keyname><forenames>Daniel</forenames></author><author><keyname>Karamian-Surville</keyname><forenames>Philippe</forenames></author></authors><title>Influence of morphological parameters in 3D composite materials on their
  effective thermal properties and comparison with effective mechanical
  properties</title><categories>cs.CE</categories><comments>27 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the effective thermal behaviour of 3D representative
volume elements (RVEs) of two-phased composite materials constituted by a
matrix with cylindrical and spherical inclusions distributed randomly, with
periodic boundaries. Variations around the shape of inclusions have been taken
into account, by corrugating shapes, excavating and/or by removing pieces of
inclusions. The effective behaviour is computed with the help of homogenization
process based on an accelerated FFT-scheme giving the thermal conductivity
tensor. Several morphological parameters are also taken into account for
instance the number and the volume fraction of each type of inclusions,... in
order to analyse the behaviour of the composite for a large number of
geometries. We compare the results obtained for RVEs with and without
variations, and then with the mechanical results of such composite studied in
our previous paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07857</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07857</id><created>2016-02-25</created><authors><author><keyname>Ramazzotti</keyname><forenames>Daniele</forenames></author><author><keyname>Graudenzi</keyname><forenames>Alex</forenames></author><author><keyname>Antoniotti</keyname><forenames>Marco</forenames></author></authors><title>Modeling cumulative biological phenomena with Suppes-Bayes causal
  networks</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several statistical techniques have been recently developed for the inference
of cancer progression models from the increasingly available NGS cross
sectional mutational profiles. A particular algorithm, CAPRI, was proven to be
the most efficient with respect to sample size and level of noise in the data.
The algorithm combines structural constraints based on Suppes' theory of
probabilistic causation and maximum likelihood fit with regularization,and
defines constrained Bayesian networks, named Suppes-Bayes Causal
Networks(SBCNs), which account for the selective advantage relations among
genomic events. In general, SBCNs are effective in modeling any phenomenon
driven by cumulative dynamics, as long as the modeled events are persistent. We
here discuss on the effectiveness of the SBCN theoretical framework and we
investigate the inference of: (i) the priors based on Suppes' theory and (ii)
different maximum likelihood regularization parameters on the inference
performance estimated on large synthetically generated datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07859</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07859</id><created>2016-02-25</created><authors><author><keyname>Brandt</keyname><forenames>Rasmus</forenames></author><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Distributed Long-Term Base Station Clustering in Cellular Networks using
  Coalition Formation</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal and Information Processing
  over Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) is a promising technique for interference
mitigation in multicell networks due to its ability to completely cancel the
intercell interference through linear precoding and receive filtering. In small
networks, the amount of required channel state information (CSI) is modest and
IA is therefore typically applied jointly over all base stations. In large
networks, where the channel coherence time is short in comparison to the time
needed to obtain the required CSI, base station clustering must be applied
however. We model such clustered multicell networks as a set of coalitions,
where CSI acquisition and IA precoding is performed independently within each
coalition. We develop a long-term throughput model which includes both CSI
acquisition overhead and the level of interference mitigation ability as a
function of the coalition structure. Given the throughput model, we formulate a
coalitional game where the involved base stations are the rational players.
Allowing for individual deviations by the players, we formulate a distributed
coalition formation algorithm with low complexity and low communication
overhead that leads to an individually stable coalition structure. The dynamic
clustering is performed using only long-term CSI, but we also provide a robust
short-term precoding algorithm which accounts for the intercoalition
interference when spectrum sharing is applied between coalitions. Numerical
simulations show that the distributed coalition formation is generally able to
reach long-term sum throughputs within 10 % of the global optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07860</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07860</id><created>2016-02-25</created><authors><author><keyname>Satsangi</keyname><forenames>Yash</forenames></author><author><keyname>Whiteson</keyname><forenames>Shimon</forenames></author><author><keyname>Oliehoek</keyname><forenames>Frans A.</forenames></author></authors><title>Probably Approximately Correct Greedy Maximization</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular function maximization finds application in a variety of real-world
decision-making problems. However, most existing methods, based on greedy
maximization, assume it is computationally feasible to evaluate F, the function
being maximized. Unfortunately, in many realistic settings F is too expensive
to evaluate exactly even once. We present probably approximately correct greedy
maximization, which requires access only to cheap anytime confidence bounds on
F and uses them to prune elements. We show that, with high probability, our
method returns an approximately optimal set. We propose novel, cheap confidence
bounds for conditional entropy, which appears in many common choices of F and
for which it is difficult to find unbiased or bounded estimates. Finally,
results on a real-world dataset from a multi-camera tracking system in a
shopping mall demonstrate that our approach performs comparably to existing
methods, but at a fraction of the computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07863</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07863</id><created>2016-02-25</created><authors><author><keyname>Lepp&#xe4;-aho</keyname><forenames>Janne</forenames></author><author><keyname>Pensar</keyname><forenames>Johan</forenames></author><author><keyname>Roos</keyname><forenames>Teemu</forenames></author><author><keyname>Corander</keyname><forenames>Jukka</forenames></author></authors><title>Learning Gaussian Graphical Models With Fractional Marginal
  Pseudo-likelihood</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian approximate inference method for learning the
dependence structure of a Gaussian graphical model. Using pseudo-likelihood, we
derive an analytical expression to approximate the marginal likelihood for an
arbitrary graph structure without invoking any assumptions about
decomposability. The majority of the existing methods for learning Gaussian
graphical models are either restricted to decomposable graphs or require
specification of a tuning parameter that may have a substantial impact on
learned structures. By combining a simple sparsity inducing prior for the graph
structures with a default reference prior for the model parameters, we obtain a
fast and easily applicable scoring function that works well for even
high-dimensional data. We demonstrate the favourable performance of our
approach by large-scale comparisons against the leading methods for learning
non-decomposable Gaussian graphical models. A theoretical justification for our
method is provided by showing that it yields a consistent estimator of the
graph structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07865</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07865</id><created>2016-02-25</created><authors><author><keyname>Krijthe</keyname><forenames>Jesse H.</forenames></author><author><keyname>Loog</keyname><forenames>Marco</forenames></author></authors><title>Projected Estimators for Robust Semi-supervised Classification</title><categories>stat.ML cs.LG</categories><comments>13 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For semi-supervised techniques to be applied safely in practice we at least
want methods to outperform their supervised counterparts. We study this
question for classification using the well-known quadratic surrogate loss
function. Using a projection of the supervised estimate onto a set of
constraints imposed by the unlabeled data, we find we can safely improve over
the supervised solution in terms of this quadratic loss. Unlike other
approaches to semi-supervised learning, the procedure does not rely on
assumptions that are not intrinsic to the classifier at hand. It is
theoretically demonstrated that, measured on the labeled and unlabeled training
data, this semi-supervised procedure never gives a lower quadratic loss than
the supervised alternative. To our knowledge this is the first approach that
offers such strong, albeit conservative, guarantees for improvement over the
supervised solution. The characteristics of our approach are explicated using
benchmark datasets to further understand the similarities and differences
between the quadratic loss criterion used in the theoretical results and the
classification accuracy often considered in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07868</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07868</id><created>2016-02-25</created><updated>2016-02-26</updated><authors><author><keyname>Salimans</keyname><forenames>Tim</forenames></author><author><keyname>Kingma</keyname><forenames>Diederik P.</forenames></author></authors><title>Weight Normalization: A Simple Reparameterization to Accelerate Training
  of Deep Neural Networks</title><categories>cs.LG cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present weight normalization: a reparameterization of the weight vectors
in a neural network that decouples the length of those weight vectors from
their direction. By reparameterizing the weights in this way we improve the
conditioning of the optimization problem and we speed up convergence of
stochastic gradient descent. Our reparameterization is inspired by batch
normalization but does not introduce any dependencies between the examples in a
minibatch. This means that our method can also be applied successfully to
recurrent models such as LSTMs and to noise-sensitive applications such as deep
reinforcement learning or generative models, for which batch normalization is
less well suited. Although our method is much simpler, it still provides much
of the speed-up of full batch normalization. In addition, the computational
overhead of our method is lower, permitting more optimization steps to be taken
in the same amount of time. We demonstrate the usefulness of our method on
applications in supervised image recognition, generative modelling, and deep
reinforcement learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07873</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07873</id><created>2016-02-25</created><authors><author><keyname>Svoboda</keyname><forenames>Pavel</forenames></author><author><keyname>Hradis</keyname><forenames>Michal</forenames></author><author><keyname>Marsik</keyname><forenames>Lukas</forenames></author><author><keyname>Zemcik</keyname><forenames>Pavel</forenames></author></authors><title>CNN for License Plate Motion Deblurring</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we explore the previously proposed approach of direct blind
deconvolution and denoising with convolutional neural networks in a situation
where the blur kernels are partially constrained. We focus on blurred images
from a real-life traffic surveillance system, on which we, for the first time,
demonstrate that neural networks trained on artificial data provide superior
reconstruction quality on real images compared to traditional blind
deconvolution methods. The training data is easy to obtain by blurring sharp
photos from a target system with a very rough approximation of the expected
blur kernels, thereby allowing custom CNNs to be trained for a specific
application (image content and blur range). Additionally, we evaluate the
behavior and limits of the CNNs with respect to blur direction range and
length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07876</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07876</id><created>2016-02-25</created><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Saether</keyname><forenames>Sigve Hortemo</forenames></author><author><keyname>Telle</keyname><forenames>Jan Arne</forenames></author></authors><title>On Satisfiability Problems with a Linear Structure</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently shown \cite{STV} that satisfiability is polynomially solvable
when the incidence graph is an interval bipartite graph (an interval graph
turned into a bipartite graph by omitting all edges within each partite set).
Here we relax this condition in several directions: First, we show that it
holds for $k$-interval bigraphs, bipartite graphs which can be converted to
interval bipartite graphs by adding to each node of one side at most $k$ edges;
the same result holds for the counting and the weighted maximization version of
satisfiability. Second, given two linear orders, one for the variables and one
for the clauses, we show how to find, in polynomial time, the smallest $k$ such
that there is a $k$-interval bigraph compatible with these two orders. On the
negative side we prove that, barring complexity collapses, no such extensions
are possible for CSPs more general than satisfiability. We also show
NP-hardness of recognizing 1-interval bigraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07884</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07884</id><created>2016-02-25</created><authors><author><keyname>Tilahun</keyname><forenames>Surafel Luleseged</forenames></author><author><keyname>Ngnotchouye</keyname><forenames>Jean Medard T</forenames></author></authors><title>Firefly Algorithm for optimization problems with non-continuous
  variables: A Review and Analysis</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Firefly algorithm is a swarm based metaheuristic algorithm inspired by the
flashing behavior of fireflies. It is an effective and an easy to implement
algorithm. It has been tested on different problems from different disciplines
and found to be effective. Even though the algorithm is proposed for
optimization problems with continuous variables, it has been modified and used
for problems with non-continuous variables, including binary and integer valued
problems. In this paper a detailed review of this modifications of firefly
algorithm for problems with non-continuous variables will be discussed. The
strength and weakness of the modifications along with possible future works
will be presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07891</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07891</id><created>2016-02-25</created><updated>2016-03-07</updated><authors><author><keyname>Xie</keyname><forenames>Zhibang</forenames></author><author><keyname>Deng</keyname><forenames>Qingjin</forenames></author></authors><title>Loongson IoT Gateway: A Technical Review</title><categories>cs.OH</categories><comments>4 pages, 4 figures</comments><msc-class>97P60</msc-class><acm-class>C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prototype of Loongson IoT ZigBee gateway is already designed and
implemented. However, it is not perfect. And a lot of things should be done to
improve it, such as adding IEEE 802.11 function, using fully open source ZigBee
protocol stack or using fully open source embedded operating system, and
implementing multiple interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07905</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07905</id><created>2016-02-25</created><authors><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Lattimore</keyname><forenames>Tor</forenames></author><author><keyname>Orseau</keyname><forenames>Laurent</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Thompson Sampling is Asymptotically Optimal in General Environments</title><categories>cs.LG cs.AI</categories><comments>12 pages, double column</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a variant of Thompson sampling for nonparametric reinforcement
learning in a countable classes of general stochastic environments. These
environments can be non-Markov, non-ergodic, and partially observable. We show
that Thompson sampling learns the environment class in the sense that (1)
asymptotically its value converges to the optimal value in mean and (2) given a
recoverability assumption regret is sublinear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07907</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07907</id><created>2016-02-25</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>de Mesmay</keyname><forenames>Arnaud</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>Finding non-orientable surfaces in 3-manifolds</title><categories>math.GT cs.CG</categories><msc-class>68U05, 57M50, 68Q25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of finding an embedded non-orientable surface
of Euler genus $g$ in a triangulated $3$-manifold. This problem occurs both as
a natural question in low-dimensional topology, and as a first non-trivial
instance of embeddability of complexes into $3$-manifolds.
  We prove that the problem is NP-hard, thus adding to the relatively few
hardness results that are currently known in 3-manifold topology. In addition,
we show that the problem lies in NP when the Euler genus g is odd, and we give
an explicit algorithm in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07911</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07911</id><created>2016-02-25</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>A Phase-space Formulation of the Belavkin-Kushner-Stratonovich Filtering
  Equation for Nonlinear Quantum Stochastic Systems</title><categories>quant-ph cs.SY math-ph math.MP math.OC</categories><comments>12 pages, a brief version of this paper to be submitted to the IEEE
  2016 Conference on Norbert Wiener in the 21st Century, 13-15 July, Melbourne,
  Australia</comments><msc-class>81S22, 81S25, 81S30, 81P16, 81S05, 35Q40, 93E11, 93E24, 35R60, 60H15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a filtering problem for a class of nonlinear
quantum stochastic systems with multichannel nondemolition measurements. The
system-observation dynamics are governed by a Markovian Hudson-Parthasarathy
quantum stochastic differential equation driven by quantum Wiener processes of
bosonic fields in vacuum state. The Hamiltonian and system-field coupling
operators, as functions of the system variables, are represented in a Weyl
quantization form. Using the Wigner-Moyal phase-space framework, we obtain a
stochastic integro-differential equation for the posterior quasi-characteristic
function (QCF) of the system conditioned on the measurements. This equation is
a spatial Fourier domain representation of the Belavkin-Kushner-Stratonovich
stochastic master equation driven by the innovation process associated with the
measurements. We also discuss a more specific form of the posterior QCF
dynamics in the case of linear system-field coupling and outline a Gaussian
approximation of the posterior quantum state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07919</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07919</id><created>2016-02-25</created><updated>2016-03-01</updated><authors><author><keyname>Pace</keyname><forenames>Francesco</forenames></author><author><keyname>Milanesio</keyname><forenames>Marco</forenames></author><author><keyname>Venzano</keyname><forenames>Daniele</forenames></author><author><keyname>Carra</keyname><forenames>Damiano</forenames></author><author><keyname>Michiardi</keyname><forenames>Pietro</forenames></author></authors><title>Experimental Performance Evaluation of Cloud-Based
  Analytics-as-a-Service</title><categories>cs.DC</categories><comments>Longer version of the paper in Submission at IEEE CLOUD'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of Analytics-as-a-Service solutions has recently seen
the light, in the landscape of cloud-based services. These services allow
flexible composition of compute and storage components, that create powerful
data ingestion and processing pipelines. This work is a first attempt at an
experimental evaluation of analytic application performance executed using a
wide range of storage service configurations. We present an intuitive notion of
data locality, that we use as a proxy to rank different service compositions in
terms of expected performance. Through an empirical analysis, we dissect the
performance achieved by analytic workloads and unveil problems due to the
impedance mismatch that arise in some configurations. Our work paves the way to
a better understanding of modern cloud-based analytic services and their
performance, both for its end-users and their providers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07928</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07928</id><created>2016-02-25</created><authors><author><keyname>Bruck</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>R&#xe9;thy</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Szente</keyname><forenames>Judit</forenames></author><author><keyname>Tobochnik</keyname><forenames>Jan</forenames></author><author><keyname>&#xc9;rdi</keyname><forenames>P&#xe9;ter</forenames></author></authors><title>Recognition of Emerging Technology Trends. Class-selective study of
  citations in the U.S. Patent Citation Network</title><categories>cs.DL physics.soc-ph</categories><comments>8 pages, 2 tables, 1 figure , (accepted). in Scientometrics 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By adopting a citation-based recursive ranking method for patents the
evolution of new fields of technology can be traced. Specifically, it is
demonstrated that the laser / inkjet printer technology emerged from the
recombination of two existing technologies: sequential printing and static
image production. The dynamics of the citations coming from the different
&quot;precursor&quot; classes illuminates the mechanism of the emergence of new fields
and give the possibility to make predictions about future technological
development. For the patent network the optimal value of the PageRank damping
factor is close to 0.5; the application of d=0.85 leads to unacceptable ranking
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07936</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07936</id><created>2016-02-25</created><authors><author><keyname>Cunha</keyname><forenames>Tiago O.</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Pappa</keyname><forenames>Gisele L.</forenames></author></authors><title>The Effect of Social Feedback in a Reddit Weight Loss Community</title><categories>cs.SI</categories><comments>This is a preprint of an article appearing at ACM Digital Health 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is generally accepted as common wisdom that receiving social feedback is
helpful to (i) keep an individual engaged with a community and to (ii)
facilitate an individual's positive behavior change. However, quantitative data
on the effect of social feedback on continued engagement in an online health
community is scarce. In this work we apply Mahalanobis Distance Matching (MDM)
to demonstrate the importance of receiving feedback in the &quot;loseit&quot; weight loss
community on Reddit. Concretely we show that (i) even when correcting for
differences in word choice, users receiving more positive feedback on their
initial post are more likely to return in the future, and that (ii) there are
diminishing returns and social feedback on later posts is less important than
for the first post. We also give a description of the type of initial posts
that are more likely to attract this valuable social feedback. Though we cannot
yet argue about ultimate weight loss success or failure, we believe that
understanding the social dynamics underlying online health communities is an
important step to devise more effective interventions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07940</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07940</id><created>2016-02-25</created><authors><author><keyname>Esteban</keyname><forenames>Juan Luis</forenames></author><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author><author><keyname>G&#xf3;mez-Rodr&#xed;guez</keyname><forenames>Carlos</forenames></author></authors><title>The scaling of the minimum sum of edge lengths in uniformly random trees</title><categories>physics.data-an cs.DM physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum linear arrangement problem on a network consists of finding the
minimum sum of edge lengths that can be achieved when the vertices are arranged
linearly. Although there are algorithms to solve this problem on trees in
polynomial time, they have remained theoretical and have not been implemented
in practical contexts to our knowledge. Here we use one of those algorithms to
investigate the growth of this sum as a function of the size of the tree in
uniformly random trees. We show that this sum is bounded above by its value in
a star tree. We also show that the mean edge length grows logarithmically in
optimal linear arrangements, in stark contrast to the linear growth that is
expected on optimal arrangements of star trees or on random linear
arrangements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07943</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07943</id><created>2016-01-20</created><authors><author><keyname>Liu</keyname><forenames>Wei-Cheng</forenames></author><author><keyname>Liu</keyname><forenames>Yu-Chen</forenames></author></authors><title>Performance Analysis of Relay Selection With Enhanced Dynamic
  Decode-and-Forward and Network Coding in Two-Way Relay Channels</title><categories>cs.IT math.IT</categories><comments>20 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we adopt the relay selection (RS) protocol proposed by
Bletsas, Khisti, Reed and Lippman (2006) with Enhanced Dynamic
Decode-and-Forward (EDDF) and network coding (NC) system in a two-hop two-way
multi-relay network. All nodes are single-input single-output (SISO) and
half-duplex, i.e., they cannot transmit and receive data simultaneously. The
outage probability is analyzed and we show comparisons of outage probability
with various scenarios under Rayleigh fading channel. Our results show that the
relay selection with EDDF and network coding (RS-EDDF&amp;NC) scheme has the best
performance in the sense of outage probability upon the considered
decode-and-forward (DF) relaying if there exist sufficiently relays. In
addition, the performance loss is large if we select a relay at random. This
shows the importance of relay selection strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07944</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07944</id><created>2016-01-23</created><authors><author><keyname>Wang</keyname><forenames>Lidong</forenames></author><author><keyname>Chang</keyname><forenames>Yanxun</forenames></author></authors><title>Determination of sizes of optimal three-dimensional optical orthogonal
  codes of weight three with the AM-OPP restriction</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we further investigate the constructions on three-dimensional
$(u\times v\times w,k,1)$ optical orthogonal codes with the at most one optical
pulse per wavelength/time plane restriction (briefly AM-OPP $3$-D $(u\times
v\times w,k,1)$-OOCs) by way of the corresponding designs. Several new
auxiliary designs such as incomplete holey group divisible designs and
incomplete group divisible packings are introduced and therefore new
constructions are presented. As a consequence, the exact number of codewords of
an optimal AM-OPP $3$-D $(u\times v\times w,3,1)$-OOC is finally determined for
any positive integers $v,w$ and $u\geq3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07955</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07955</id><created>2016-02-25</created><authors><author><keyname>Zhou</keyname><forenames>Zhou</forenames></author><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Yang</keyname><forenames>Linxiao</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>Channel Estimation for Millimeter Wave Multiuser MIMO Systems via
  PARAFAC Decomposition</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of uplink channel estimation for millimeter wave
(mmWave) systems, where the base station (BS) and mobile stations (MSs) are
equipped with large antenna arrays to provide sufficient beamforming gain for
outdoor wireless communications. Hybrid analog and digital beamforming
structures are employed by both the BS and the MS due to hardware constraints.
We propose a layered pilot transmission scheme and a CANDECOMP/PARAFAC (CP)
decomposition-based method for joint estimation of the channels from multiple
users (i.e. MSs) to the BS. The proposed method exploits the sparse scattering
nature of the mmWave channel and the intrinsic multi-dimensional structure of
the multiway data collected from multiple modes. The uniqueness of the CP
decomposition is studied and sufficient conditions for essential uniqueness are
obtained. The conditions shed light on the design of the beamforming matrix,
the combining matrix and the pilot sequences, and meanwhile provide general
guidelines for choosing system parameters. Our analysis reveals that our
proposed method can achieve a substantial training overhead reduction by
employing the layered pilot transmission scheme. Simulation results show that
the proposed method presents a clear advantage over a compressed sensing-based
method in terms of both estimation accuracy and computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07967</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07967</id><created>2016-02-25</created><authors><author><keyname>Belovs</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Montoya</keyname><forenames>Juan Andres</forenames></author><author><keyname>Yakary&#x131;lmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Can one quantum bit separate any pair of words with zero-error?</title><categories>cs.FL cs.CC quant-ph</categories><comments>17 pages!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining the minimum number of states required by a finite automaton to
separate a given pair of different words is an important problem. In this
paper, we consider this problem for quantum automata (QFAs). We show that
2-state QFAs can separate any pair of words in nondeterministic acceptance mode
and conjecture that they can separate any pair also with zero-error. Then, we
focus on (a more general problem) separating a pair of two disjoint finite set
of words. We show that QFAs can separate them efficiently in nondeterministic
acceptance mode, i.e. the number of states is two to the power of the size of
the small set. Additionally, we examine affine finite automata (AfAs) and show
that two states are enough to separate any pair with zero-error. Moreover, AfAs
can separate any pair of disjoint finite sets of words with one-sided bounded
error efficiently like QFAs in nondeterministic mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07970</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07970</id><created>2016-02-25</created><authors><author><keyname>Hyttinen</keyname><forenames>Antti</forenames></author><author><keyname>Plis</keyname><forenames>Sergey</forenames></author><author><keyname>J&#xe4;rvisalo</keyname><forenames>Matti</forenames></author><author><keyname>Eberhardt</keyname><forenames>Frederick</forenames></author><author><keyname>Danks</keyname><forenames>David</forenames></author></authors><title>Causal Discovery from Subsampled Time Series Data by Constraint
  Optimization</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on causal structure estimation from time series data in
which measurements are obtained at a coarser timescale than the causal
timescale of the underlying system. Previous work has shown that such
sub-sampling can lead to significant errors about the system's causal structure
if not properly taken into account. In this paper, we first consider the search
for the system timescale causal structures that correspond to a given
measurement timescale structure. We provide a constraint satisfaction procedure
whose computational performance is several orders of magnitude better than
previous approaches, and that has considerably broader applicability. We then
consider finite sample data as input, and propose the first constraint
optimization approach for recovering the system timescale causal structure.
This algorithm optimally recovers from possible conflicts due to statistical
errors. More generally, these advances allow for a robust and non-parametric
estimation of system timescale causal structures from subsampled time series
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07978</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07978</id><created>2016-02-25</created><authors><author><keyname>Poloczek</keyname><forenames>Felix</forenames></author><author><keyname>Ciucu</keyname><forenames>Florin</forenames></author></authors><title>Contrasting Effects of Replication in Parallel Systems: From Overload to
  Underload and Back</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task replication has recently been advocated as a practical solution to
reduce latencies in parallel systems. In addition to several convincing
empirical studies, some others provide analytical results, yet under some
strong assumptions such as Poisson arrivals, exponential service times, or
independent service times of the replicas themselves, which may lend themselves
to some contrasting and perhaps contriving behavior. For instance, under the
second assumption, an overloaded system can be stabilized by a replication
factor, but can be sent back in overload through further replication. In turn,
under the third assumption, strictly larger stability regions of replication
systems do not necessarily imply smaller delays.
  Motivated by the need to dispense with such common and restricting
assumptions, which may additionally cause unexpected behavior, we develop a
unified and general theoretical framework to compute tight bounds on the
distribution of response times in general replication systems. These results
immediately lend themselves to the optimal number of replicas minimizing
response time quantiles, depending on the parameters of the system (e.g., the
degree of correlation amongst replicas). As a concrete application of our
framework, we design a novel replication policy which can improve the stability
region of classical fork-join queueing systems by $\mathcal{O}(\ln K)$, in the
number of servers $K$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.07985</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.07985</id><created>2016-02-25</created><authors><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Krimpas</keyname><forenames>George A.</forenames></author><author><keyname>Voudouris</keyname><forenames>Alexandros A.</forenames></author></authors><title>How effective can simple ordinal peer grading be?</title><categories>cs.AI cs.DS cs.LG</categories><comments>23 pages, 5 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ordinal peer grading has been proposed as a simple and scalable solution for
computing reliable information about student performance in massive open online
courses. The idea is to outsource the grading task to the students themselves
as follows. After the end of an exam, each student is asked to rank ---in terms
of quality--- a bundle of exam papers by fellow students. An aggregation rule
will then combine the individual rankings into a global one that contains all
students. We define a broad class of simple aggregation rules and present a
theoretical framework for assessing their effectiveness. When statistical
information about the grading behaviour of students is available, the framework
can be used to compute the optimal rule from this class with respect to a
series of performance objectives. For example, a natural rule known as Borda is
proved to be optimal when students grade correctly. In addition, we present
extensive simulations and a field experiment that validate our theory and prove
it to be extremely accurate in predicting the performance of aggregation rules
even when only rough information about grading behaviour is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08004</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08004</id><created>2016-02-25</created><authors><author><keyname>Neumann</keyname><forenames>Eike</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>A topological view on algebraic computation models</title><categories>cs.LO</categories><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the topological aspects of some algebraic computation models,
in particular the BSS-model. Our results can be seen as bounds on how different
BSS-computability and computability in the sense of computable analysis can be.
The framework for this is Weihrauch reducibility. As a consequence of our
characterizations, we establish that the solvability complexity index is
(mostly) independent of the computational model, and that there thus is common
ground in the study of non-computability between the BSS and TTE setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08007</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08007</id><created>2016-02-25</created><authors><author><keyname>Marceau-Caron</keyname><forenames>Ga&#xe9;tan</forenames></author><author><keyname>Ollivier</keyname><forenames>Yann</forenames></author></authors><title>Practical Riemannian Neural Networks</title><categories>cs.NE cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first experimental results on non-synthetic datasets for the
quasi-diagonal Riemannian gradient descents for neural networks introduced in
[Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets as well as a
previously unpublished electroencephalogram dataset. The quasi-diagonal
Riemannian algorithms consistently beat simple stochastic gradient gradient
descents by a varying margin. The computational overhead with respect to simple
backpropagation is around a factor $2$. Perhaps more interestingly, these
methods also reach their final performance quickly, thus requiring fewer
training epochs and a smaller total computation time.
  We also present an implementation guide to these Riemannian gradient descents
for neural networks, showing how the quasi-diagonal versions can be implemented
with minimal effort on top of existing routines which compute gradients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08010</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08010</id><created>2015-12-30</created><authors><author><keyname>Ewaisha</keyname><forenames>Ahmed</forenames></author><author><keyname>Tepedelenlio&#x11f;lu</keyname><forenames>Cihan</forenames></author></authors><title>Joint Scheduling and Power-Control for Delay Guarantees in Heterogeneous
  Cognitive Radios</title><categories>cs.IT math.IT</categories><comments>Keywords: Cognitive Radios, Delay Constraints, Resource allocation,
  Stochastic Optimization, Online Algorithm, Lyapunov Optimization, Average
  Interference Constraint, Priority Queues. arXiv admin note: substantial text
  overlap with arXiv:1601.00608, arXiv:1512.02989</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An uplink multi secondary user (SU) cognitive radio system having average
delay constraints as well as an interference constraint to the primary user
(PU) is considered. If the interference channels from the SUs to the PU have
independent but not identically distributed fading coefficients, then the SUs
will experience heterogeneous delay performances. This is because SUs causing
low interference to the PU will be scheduled more frequently, or allocated more
transmission power than those causing high interference. This means that users
will experience different performances if they have different physical
locations. A dynamic scheduling-and-power-allocation algorithm that can provide
the required average delay guarantees to all SUs irrespective of their location
is proposed. The algorithm, derived using the Lyapunov optimization technique,
is shown to be asymptotically delay optimal while satisfying the delay and
interference constraints. Our findings are supported by extensive system
simulations and the robustness of the proposed algorithm to channel estimation
errors is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08017</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08017</id><created>2016-02-25</created><authors><author><keyname>Makmal</keyname><forenames>Adi</forenames></author><author><keyname>Melnikov</keyname><forenames>Alexey A.</forenames></author><author><keyname>Dunjko</keyname><forenames>Vedran</forenames></author><author><keyname>Briegel</keyname><forenames>Hans J.</forenames></author></authors><title>Meta-learning within Projective Simulation</title><categories>cs.AI cs.LG stat.ML</categories><comments>14 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning models of artificial intelligence can nowadays perform very well on
a large variety of tasks. However, in practice different task environments are
best handled by different learning models, rather than a single, universal,
approach. Most non-trivial models thus require the adjustment of several to
many learning parameters, which is often done on a case-by-case basis by an
external party. Meta-learning refers to the ability of an agent to autonomously
and dynamically adjust its own learning parameters, or meta-parameters. In this
work we show how projective simulation, a recently developed model of
artificial intelligence, can naturally be extended to account for meta-learning
in reinforcement learning settings. The projective simulation approach is based
on a random walk process over a network of clips. The suggested meta-learning
scheme builds upon the same design and employs clip networks to monitor the
agent's performance and to adjust its meta-parameters &quot;on the fly&quot;. We
distinguish between &quot;reflexive adaptation&quot; and &quot;adaptation through learning&quot;,
and show the utility of both approaches. In addition, a trade-off between
flexibility and learning-time is addressed. The extended model is examined on
three different kinds of reinforcement learning tasks, in which the agent has
different optimal values of the meta-parameters, and is shown to perform well,
reaching near-optimal to optimal success rates in all of them, without ever
needing to manually adjust any meta-parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08022</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08022</id><created>2016-02-25</created><authors><author><keyname>Brandenburg</keyname><forenames>Franz J.</forenames></author></authors><title>Recognizing Optimal 1-Planar Graphs in Linear Time</title><categories>cs.DM</categories><comments>32 pages, 14 figures. arXiv admin note: substantial text overlap with
  arXiv:1602.06407</comments><msc-class>05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph with n vertices is 1-planar if it can be drawn in the plane such that
each edge is crossed at most once, and is optimal if it has the maximum of 4n-8
edges.
  We show that optimal 1-planar graphs can be recognized in linear time. Our
algorithm implements a graph reduction system with two rules, which can be used
to reduce every optimal 1-planar graph to an irreducible extended wheel graph.
The graph reduction system is non-deterministic, constraint, and non-confluent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08023</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08023</id><created>2016-02-25</created><authors><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Filos-Ratsikas</keyname><forenames>Aris</forenames></author><author><keyname>Frederiksen</keyname><forenames>Soren Kristoffer Stiil</forenames></author><author><keyname>Hansen</keyname><forenames>Kristoffer Arnsfelt</forenames></author><author><keyname>Tan</keyname><forenames>Zihan</forenames></author></authors><title>Truthful Facility Assignment with Resource Augmentation: An Exact
  Analysis of Serial Dictatorship</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the truthful facility assignment problem, where a set of agents with
private most-preferred points on a metric space are assigned to facilities that
lie on the metric space, under capacity constraints on the facilities. The goal
is to produce such an assignment that minimizes the social cost, i.e., the
total distance between the most-preferred points of the agents and their
corresponding facilities in the assignment, under the constraint of
truthfulness, which ensures that agents do not misreport their most-preferred
points.
  We propose a resource augmentation framework, where a truthful mechanism is
evaluated by its worst-case performance on an instance with enhanced facility
capacities against the optimal mechanism on the same instance with the original
capacities. We study a very well-known mechanism, Serial Dictatorship, and
provide an exact analysis of its performance. Although Serial Dictatorship is a
purely combinatorial mechanism, our analysis uses linear programming; a linear
program expresses its greedy nature as well as the structure of the input, and
finds the input instance that enforces the mechanism have its worst-case
performance. Bounding the objective of the linear program using duality
arguments allows us to compute tight bounds on the approximation ratio. Among
other results, we prove that Serial Dictatorship has approximation ratio
$g/(g-2)$ when the capacities are multiplied by any integer $g \geq 3$. Our
results suggest that even a limited augmentation of the resources can have
wondrous effects on the performance of the mechanism and in particular, the
approximation ratio goes to 1 as the augmentation factor becomes large. We
complement our results with bounds on the approximation ratio of Random Serial
Dictatorship, the randomized version of Serial Dictatorship, when there is no
resource augmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08032</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08032</id><created>2016-02-25</created><authors><author><keyname>Alistarh</keyname><forenames>Dan</forenames></author><author><keyname>Aspnes</keyname><forenames>James</forenames></author><author><keyname>Eisenstat</keyname><forenames>David</forenames></author><author><keyname>Gelashvili</keyname><forenames>Rati</forenames></author><author><keyname>Rivest</keyname><forenames>Ronald L.</forenames></author></authors><title>Time-Space Trade-offs in Population Protocols</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Population protocols are a popular model of distributed computing, in which
randomly-interacting agents with little computational power cooperate to
jointly perform computational tasks. Recent work has focused on the complexity
of fundamental tasks in the population model, such as leader election (which
requires convergence to a single agent in a special &quot;leader&quot; state), and
majority (in which agents must converge to a decision as to which of two
possible initial states had higher initial count). Known upper and lower bounds
point towards an inherent trade-off between the time complexity of these
protocols, and the space complexity, i.e. size of the memory available to each
agent.
  In this paper, we explore this trade-off and provide new upper and lower
bounds for these two fundamental tasks. First, we prove a new unified lower
bound, which relates the space available per node with the time complexity
achievable by the protocol: for instance, our result implies that any protocol
solving either of these tasks for $n$ agents using $O( \log \log n )$ states
must take $\Omega( n / \operatorname{polylog} n )$ expected time. This is the
first result to characterize time complexity for protocols which employ
super-constant number of states per node, and proves that fast,
poly-logarithmic running times require protocols to have relatively large space
costs.
  On the positive side, we show that $O(\operatorname{polylog} n)$ convergence
time can be achieved using $O( \log^2 n )$ space per node, in the case of both
tasks. Overall, our results highlight a time complexity separation between
$O(\log \log n)$ and $\Theta( \log^2 n )$ state space size for both majority
and leader election in population protocols. At the same time, we introduce
several new tools and techniques, which should be applicable to other tasks and
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08033</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08033</id><created>2016-02-25</created><authors><author><keyname>Ning</keyname><forenames>Yue</forenames></author><author><keyname>Muthiah</keyname><forenames>Sathappan</forenames></author><author><keyname>Rangwala</keyname><forenames>Huzefa</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Modeling Precursors for Event Forecasting via Nested Multi-Instance
  Learning</title><categories>cs.SI</categories><comments>The conference version of the paper is submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forecasting events like civil unrest movements, disease outbreaks, financial
market movements and government elections from open source indicators such as
news feeds and social media streams is an important and challenging problem.
From the perspective of human analysts and policy makers, forecasting
algorithms need to provide supporting evidence and identify the causes related
to the event of interest. We develop a novel multiple instance learning based
approach that jointly tackles the problem of identifying evidence-based
precursors and forecasts events into the future. Specifically, given a
collection of streaming news articles from multiple sources we develop a nested
multiple instance learning approach to forecast significant societal events
across three countries in Latin America. Our algorithm is able to identify news
articles considered as precursors for a protest. Our empirical evaluation shows
the strengths of our proposed approaches in filtering candidate precursors,
forecasting the occurrence of events with a lead time and predicting the
characteristics of different events in comparison to several other
formulations. We demonstrate through case studies the effectiveness of our
proposed model in filtering the candidate precursors for inspection by a human
analyst.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08034</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08034</id><created>2016-02-25</created><authors><author><keyname>Morizumi</keyname><forenames>Hiroki</forenames></author></authors><title>Zero-Suppressed Computation: A New Computation Inspired by ZDDs</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zero-suppressed binary decision diagrams (ZDDs) are a data structure
representing Boolean functions, and one of the most successful variants of
binary decision diagrams (BDDs). On the other hand, BDDs are also called
branching programs in computational complexity theory, and have been studied as
a computation model. In this paper, we consider ZDDs from the viewpoint of
computational complexity theory. Our main proposal of this paper is that we
regard the basic idea of ZDDs as a new computation, which we call
zero-suppressed computation. We consider the zero-suppressed version of two
classical computation models, decision trees and branching programs, and show
some results. Although this paper is mainly written from the viewpoint of
computational complexity theory, the concept of zero-suppressed computation can
be widely applied to various areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08044</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08044</id><created>2016-02-25</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author></authors><title>On Adjusting the Learning Rate in Frequency Domain Echo Cancellation
  With Double-Talk</title><categories>cs.SD cs.SY</categories><comments>5 pages</comments><journal-ref>IEEE Transactions on Audio, Speech and Language Processing, Vol.
  15, No. 3, pp. 1030-1034, 2007</journal-ref><doi>10.1109/TASL.2006.885935</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main difficulties in echo cancellation is the fact that the
learning rate needs to vary according to conditions such as double-talk and
echo path change. In this paper we propose a new method of varying the learning
rate of a frequency-domain echo canceller. This method is based on the
derivation of the optimal learning rate of the NLMS algorithm in the presence
of noise. The method is evaluated in conjunction with the multidelay block
frequency domain (MDF) adaptive filter. We demonstrate that it performs better
than current double-talk detection techniques and is simple to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08045</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08045</id><created>2016-02-25</created><authors><author><keyname>Ge</keyname><forenames>Zhenhao</forenames></author><author><keyname>Sharma</keyname><forenames>Sudhendu R.</forenames></author><author><keyname>Smith</keyname><forenames>Mark J. T.</forenames></author></authors><title>PCA/LDA Approach for Text-Independent Speaker Recognition</title><categories>cs.SD cs.LG</categories><comments>Society of Photo-Optical Instrumentation Engineers (SPIE) Conference
  Series</comments><doi>10.1117/12.919235</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various algorithms for text-independent speaker recognition have been
developed through the decades, aiming to improve both accuracy and e?ciency.
This paper presents a novel PCA/LDA-based approach that is faster than
traditional statistical model-based methods and achieves competitive results.
First, the performance based on only PCA and only LDA is measured; then a mixed
model, taking advantages of both methods, is introduced. A subset of the TIMIT
corpus composed of 200 male speakers, is used for enrollment, validation and
testing. The best results achieve 100%; 96% and 95% classi?cation rate at
population level 50; 100 and 200, using 39-dimensional MFCC features with delta
and double delta. These results are based on 12-second text-independent speech
for training and 4-second data for test. These are comparable to the
conventional MFCC-GMM methods, but require signi?cantly less time to train and
operate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08063</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08063</id><created>2016-02-25</created><authors><author><keyname>Brandt</keyname><forenames>Felix</forenames></author><author><keyname>Geist</keyname><forenames>Christian</forenames></author><author><keyname>Peters</keyname><forenames>Dominik</forenames></author></authors><title>Optimal Bounds for the No-Show Paradox via SAT Solving</title><categories>cs.GT</categories><comments>9 pages, appears at AAMAS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voting rules allow multiple agents to aggregate their preferences in order to
reach joint decisions. Perhaps one of the most important desirable properties
in this context is Condorcet-consistency, which requires that a voting rule
should return an alternative that is preferred to any other alternative by some
majority of voters. Another desirable property is participation, which requires
that no voter should be worse off by joining an electorate. A seminal result in
social choice theory by Moulin (1998) has shown that Condorcet-consistency and
participation are incompatible whenever there are at least 4 alternatives and
25 voters. We leverage SAT solving to obtain an elegant human-readable proof of
Moulin's result that requires only 12 voters. Moreover, the SAT solver is able
to construct a Condorcet-consistent voting rule that satisfies participation as
well as a number of other desirable properties for up to 11 voters, proving the
optimality of the above bound. We also obtain tight results for set-valued and
probabilistic voting rules, which complement and significantly improve existing
theorems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08067</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08067</id><created>2016-01-07</created><authors><author><keyname>Begovic</keyname><forenames>Natasa</forenames></author><author><keyname>Neskovic</keyname><forenames>Aleksandar</forenames></author></authors><title>Improve Positioning Accuracy in WCDMA/FDD Networks Utilizing Adaptive
  Threshold for Direct Component Detection</title><categories>cs.NI cs.IT math.IT</categories><comments>International Journal of Wireless &amp; Mobile Networks, December 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In NLOS propagation conditions power of direct component can be attenuated
significantly. Therefore detection of direct component is aggravated which can
degrades accuracy of Time of Arrival mobile positioning. The goal of this paper
is to determine possibilities to improve estimation of direct component time
delay by reducing detection threshold. Three different methods for calculating
threshold has been tested and compared in terms of positioning error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08068</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08068</id><created>2016-02-19</created><authors><author><keyname>Briata</keyname><forenames>Federica</forenames></author><author><keyname>Dall'Aglio</keyname><forenames>Andrea</forenames></author><author><keyname>Dall'Aglio</keyname><forenames>Marco</forenames></author><author><keyname>Fragnelli</keyname><forenames>Vito</forenames></author></authors><title>The Shapley Value in the Knaster Gain Game</title><categories>math.OC cs.GT</categories><comments>14 pages, 1 table</comments><msc-class>91A12, 91B14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Briata, Dall'Aglio and Fragnelli (2012), the authors introduce a
cooperative game with transferable utility for allocating the gain of a
collusion among completely risk-averse agents involved in a fair division
procedure introduced by Knaster (1946). In this paper we analyze the features
of the Shapley value (Shapley, 1953) of the game, and propose a simple
algorithm for computing it efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08073</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08073</id><created>2016-02-25</created><updated>2016-02-28</updated><authors><author><keyname>Holroyd</keyname><forenames>Alexander E.</forenames></author></authors><title>Perfect snake-in-the-box codes for rank modulation</title><categories>math.CO cs.IT math.GR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For odd n, the alternating group on n elements is generated by the
permutations that jump an element from any odd position to position 1. We prove
Hamiltonicity of the associated directed Cayley graph for all odd n not equal
to 5. (A result of Rankin implies that the graph is not Hamiltonian for n=5.)
This solves a problem arising in rank modulation schemes for flash memory. Our
result disproves a conjecture of Horovitz and Etzion, and proves another
conjecture of Yehezkeally and Schwartz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08109</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08109</id><created>2016-02-25</created><authors><author><keyname>Peters</keyname><forenames>Dominik</forenames></author></authors><title>Recognising Multidimensional Euclidean Preferences</title><categories>cs.GT</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Euclidean preferences are a widely studied preference model, in which
decision makers and alternatives are embedded in d-dimensional Euclidean space.
Decision makers prefer those alternatives closer to them. This model, also
known as multidimensional unfolding, has applications in economics,
psychometrics, marketing, and many other fields. We study the problem of
deciding whether a given preference profile is d-Euclidean. For the
one-dimensional case, polynomial-time algorithms are known. We show that, in
contrast, for every other fixed dimension d &gt; 1, the recognition problem is
equivalent to the existential theory of the reals (ETR), and so in particular
NP-hard. We further show that some Euclidean preference profiles require
exponentially many bits in order to specify any Euclidean embedding, and prove
that the domain of d-Euclidean preferences does not admit a finite forbidden
minor characterisation for any d &gt; 1. We also study dichotomous preferencesand
the behaviour of other metrics, and survey a variety of related work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08114</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08114</id><created>2016-02-25</created><authors><author><keyname>Shaghaghian</keyname><forenames>Shohreh</forenames></author><author><keyname>Coates</keyname><forenames>Mark</forenames></author></authors><title>Bayesian Inference of Diffusion Networks with Unknown Infection Times</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of diffusion processes in real-world propagation scenarios often
involves estimating variables that are not directly observed. These hidden
variables include parental relationships, the strengths of connections between
nodes, and the moments of time that infection occurs. In this paper, we propose
a framework in which all three sets of parameters are assumed to be hidden and
we develop a Bayesian approach to infer them. After justifying the model
assumptions, we evaluate the performance efficiency of our proposed approach
through numerical simulations on synthetic datasets and real-world diffusion
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08116</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08116</id><created>2016-02-25</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Collings</keyname><forenames>Iain B.</forenames></author></authors><title>Interference-Normalised Least Mean Square Algorithm</title><categories>cs.SY cs.SD</categories><comments>4 pages</comments><journal-ref>IEEE Signal Processing Letters, Vol. 14, No 12, pp. 988-991, 2007</journal-ref><doi>10.1109/LSP.2007.908017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interference-normalised least mean square (INLMS) algorithm for robust
adaptive filtering is proposed. The INLMS algorithm extends the
gradient-adaptive learning rate approach to the case where the signals are
non-stationary. In particular, we show that the INLMS algorithm can work even
for highly non-stationary interference signals, where previous
gradient-adaptive learning rate algorithms fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08118</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08118</id><created>2016-02-25</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep
  Neural Network</title><categories>cs.LG</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks (RNN) are capable of learning to encode and exploit
activation history over an arbitrary timescale. However, in practice, state of
the art gradient descent based training methods are known to suffer from
difficulties in learning long term dependencies. Here, we describe a novel
training method that involves concurrent parallel cloned networks, each sharing
the same weights, each trained at different stimulus phase and each maintaining
independent activation histories. Training proceeds by recursively performing
batch-updates over the parallel clones as activation history is progressively
increased. This allows conflicts to propagate hierarchically from short-term
contexts towards longer-term contexts until they are resolved. We illustrate
the parallel clones method and hierarchical conflict propagation with a
character-level deep RNN tasked with memorizing a paragraph of Moby Dick (by
Herman Melville).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08124</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08124</id><created>2016-02-25</created><updated>2016-02-29</updated><authors><author><keyname>Rhu</keyname><forenames>Minsoo</forenames></author><author><keyname>Gimelshein</keyname><forenames>Natalia</forenames></author><author><keyname>Clemons</keyname><forenames>Jason</forenames></author><author><keyname>Zulfiqar</keyname><forenames>Arslan</forenames></author><author><keyname>Keckler</keyname><forenames>Stephen W.</forenames></author></authors><title>Virtualizing Deep Neural Networks for Memory-Efficient Neural Network
  Design</title><categories>cs.DC cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most widely used machine learning frameworks require users to carefully
tune their memory usage so that the deep neural network (DNN) fits into the
DRAM capacity of a GPU. This restriction hampers a researcher's flexibility to
study different machine learning algorithms, forcing them to either use a less
desirable network architecture or parallelize the processing across multiple
GPUs. We propose a runtime memory manager that virtualizes the memory usage of
DNNs such that both GPU and CPU memory can simultaneously be utilized for
training larger DNNs. Our virtualized DNN (vDNN) reduces the average memory
usage of AlexNet by 61% and OverFeat by 83%, a significant reduction in memory
requirements of DNNs. Similar experiments on VGG-16, one of the deepest and
memory hungry DNNs to date, demonstrate the memory-efficiency of our proposal.
vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to be
trained on a single NVIDIA K40 GPU card containing 12 GB of memory, with 22%
performance loss compared to a hypothetical GPU with enough memory to hold the
entire DNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08127</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08127</id><created>2016-02-25</created><updated>2016-03-01</updated><authors><author><keyname>Fu</keyname><forenames>Xiping</forenames></author><author><keyname>McCane</keyname><forenames>Brendan</forenames></author><author><keyname>Mills</keyname><forenames>Steven</forenames></author><author><keyname>Albert</keyname><forenames>Michael</forenames></author><author><keyname>Szymanski</keyname><forenames>Lech</forenames></author></authors><title>Auto-JacoBin: Auto-encoder Jacobian Binary Hashing</title><categories>cs.CV cs.LG</categories><comments>Submitting to journal (TPAMI). 17 pages, 11 figures. The Matlab codes
  for AutoJacoBin and NOKMeans are available:
  https://bitbucket.org/fxpfxp/autojacobin
  https://bitbucket.org/fxpfxp/nokmeans The SIFT10M dataset is available at:
  http://archive.ics.uci.edu/ml/datasets/SIFT10M</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Binary codes can be used to speed up nearest neighbor search tasks in large
scale data sets as they are efficient for both storage and retrieval. In this
paper, we propose a robust auto-encoder model that preserves the geometric
relationships of high-dimensional data sets in Hamming space. This is done by
considering a noise-removing function in a region surrounding the manifold
where the training data points lie. This function is defined with the property
that it projects the data points near the manifold into the manifold wisely,
and we approximate this function by its first order approximation. Experimental
results show that the proposed method achieves better than state-of-the-art
results on three large scale high dimensional data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08128</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08128</id><created>2016-02-25</created><authors><author><keyname>Ge</keyname><forenames>Zhenhao</forenames></author><author><keyname>Sharma</keyname><forenames>Sudhendu R.</forenames></author><author><keyname>Smith</keyname><forenames>Mark J. T.</forenames></author></authors><title>PCA Method for Automated Detection of Mispronounced Words</title><categories>cs.SD cs.CL cs.LG</categories><comments>SPIE Defense, Security, and Sensing</comments><doi>10.1117/12.884155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for detecting mispronunciations with the aim of
improving Computer Assisted Language Learning (CALL) tools used by foreign
language learners. The algorithm is based on Principle Component Analysis
(PCA). It is hierarchical with each successive step refining the estimate to
classify the test word as being either mispronounced or correct. Preprocessing
before detection, like normalization and time-scale modification, is
implemented to guarantee uniformity of the feature vectors input to the
detection system. The performance using various features including spectrograms
and Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.
Best results were obtained using MFCCs, achieving up to 99% accuracy in word
verification and 93% in native/non-native classification. Compared with Hidden
Markov Models (HMMs) which are used pervasively in recognition application,
this particular approach is computational efficient and effective when training
data is limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08132</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08132</id><created>2016-02-25</created><authors><author><keyname>Ge</keyname><forenames>Zhenhao</forenames></author><author><keyname>Sharma</keyname><forenames>Sudhendu R.</forenames></author><author><keyname>Smith</keyname><forenames>Mark J. T.</forenames></author></authors><title>Adaptive Frequency Cepstral Coefficients for Word Mispronunciation
  Detection</title><categories>cs.SD cs.CV</categories><comments>4th International Congress on Image and Signal Processing (CISP) 2011</comments><doi>10.1109/CISP.2011.6100685</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems based on automatic speech recognition (ASR) technology can provide
important functionality in computer assisted language learning applications.
This is a young but growing area of research motivated by the large number of
students studying foreign languages. Here we propose a Hidden Markov Model
(HMM)-based method to detect mispronunciations. Exploiting the specific dialog
scripting employed in language learning software, HMMs are trained for
different pronunciations. New adaptive features have been developed and
obtained through an adaptive warping of the frequency scale prior to computing
the cepstral coefficients. The optimization criterion used for the warping
function is to maximize separation of two major groups of pronunciations
(native and non-native) in terms of classification rate. Experimental results
show that the adaptive frequency scale yields a better coefficient
representation leading to higher classification rates in comparison with
conventional HMMs using Mel-frequency cepstral coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08135</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08135</id><created>2016-02-25</created><authors><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Sathiaseelan</keyname><forenames>Arjuna</forenames></author><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author></authors><title>Taming Limits with Approximate Networking</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet is the linchpin of modern society, which the various threads of
modern life weave around. But being a part of the bigger energy-guzzling
industrial economy, it is vulnerable to disruption. It is widely believed that
our society is exhausting its vital resources to meet our energy requirements,
and the cheap fossil fuel fiesta will soon abate as we cross the tipping point
of global oil production. We will then enter the long arc of scarcity,
constraints, and limits---a post-peak &quot;long emergency&quot; that may subsist for a
long time. To avoid the collapse of the networking ecosystem in this long
emergency, it is imperative that we start thinking about how networking should
adapt to these adverse &quot;undeveloping&quot; societal conditions. We propose using the
idea of &quot;\textit{approximate networking}&quot;---which will provide
\textit{good-enough} networking services by employing
\textit{contextually-appropriate} tradeoffs---to survive, or even thrive, in
the conditions of scarcity and limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08139</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08139</id><created>2016-02-25</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Michaud</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Rouat</keyname><forenames>Jean</forenames></author></authors><title>Robust Localization and Tracking of Simultaneous Moving Sound Sources
  Using Beamforming and Particle Filtering</title><categories>cs.RO cs.SD</categories><comments>26 pages</comments><journal-ref>Robotics and Autonomous Systems Journal (Elsevier), Vol. 55, No.
  3, pp. 216-228, 2007</journal-ref><doi>10.1016/j.robot.2006.08.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile robots in real-life settings would benefit from being able to localize
and track sound sources. Such a capability can help localizing a person or an
interesting event in the environment, and also provides enhanced processing for
other capabilities such as speech recognition. To give this capability to a
robot, the challenge is not only to localize simultaneous sound sources, but to
track them over time. In this paper we propose a robust sound source
localization and tracking method using an array of eight microphones. The
method is based on a frequency-domain implementation of a steered beamformer
along with a particle filter-based tracking algorithm. Results show that a
mobile robot can localize and track in real-time multiple moving sources of
different types over a range of 7 meters. These new capabilities allow a mobile
robot to interact using more natural means with people in real life settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08141</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08141</id><created>2016-02-25</created><authors><author><keyname>Castelli</keyname><forenames>Thomas</forenames></author><author><keyname>Sharghi</keyname><forenames>Aidean</forenames></author><author><keyname>Harper</keyname><forenames>Don</forenames></author><author><keyname>Tremeau</keyname><forenames>Alain</forenames></author><author><keyname>Shah</keyname><forenames>Mubarak</forenames></author></authors><title>Autonomous navigation for low-altitude UAVs in urban areas</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, consumer Unmanned Aerial Vehicles have become very popular,
everyone can buy and fly a drone without previous experience, which raises
concern in regards to regulations and public safety. In this paper, we present
a novel approach towards enabling safe operation of such vehicles in urban
areas. Our method uses geodetically accurate dataset images with Geographical
Information System (GIS) data of road networks and buildings provided by Google
Maps, to compute a weighted A* shortest path from start to end locations of a
mission. Weights represent the potential risk of injuries for individuals in
all categories of land-use, i.e. flying over buildings is considered safer than
above roads. We enable safe UAV operation in regards to 1- land-use by
computing a static global path dependent on environmental structures, and 2-
avoiding flying over moving objects such as cars and pedestrians by dynamically
optimizing the path locally during the flight. As all input sources are first
geo-registered, pixels and GPS coordinates are equivalent, it therefore allows
us to generate an automated and user-friendly mission with GPS waypoints
readable by consumer drones' autopilots. We simulated 54 missions and show
significant improvement in maximizing UAV's standoff distance to moving objects
with a quantified safety parameter over 40 times better than the naive straight
line navigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08149</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08149</id><created>2016-02-25</created><authors><author><keyname>Santra</keyname><forenames>Siddhartha</forenames></author><author><keyname>Shehab</keyname><forenames>Omar</forenames></author><author><keyname>Balu</keyname><forenames>Radhakrishnan</forenames></author></authors><title>Exponential capacity of associative memories under quantum annealing
  recall</title><categories>quant-ph cs.OH</categories><comments>9 pages, 4 figures. Comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Associative memory models, in theoretical neuro- and computer sciences, can
generally store a sublinear number of memories. We show that using quantum
annealing for recall tasks endows associative memory models with exponential
storage capacities. Theoretically, we obtain the radius of attractor basins,
$R(N)$, and the capacity, $C(N)$, of such a scheme and their tradeoffs. Our
calculations establish that for randomly chosen memories the capacity of a
model using the Hebbian learning rule with recall via quantum annealing is
exponential in the size of the problem, $C(N)=\mathcal{O}(e^{C_1N}),~C_1\geq0$,
and succeeds on randomly chosen memory sets with a probability of
$(1-e^{-C_2N}),~C_2\geq0$ with $C_1+C_2=(.5-f)^2/(1-f)$, where,
$f=R(N)/N,~0\leq f\leq .5$ is the radius of attraction in terms of Hamming
distance of an input probe from a stored memory as a fraction of the problem
size. We demonstrate the application of this scheme on a programmable quantum
annealing device - the Dwave processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08150</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08150</id><created>2016-02-25</created><authors><author><keyname>Chen</keyname><forenames>Mo</forenames></author><author><keyname>Hu</keyname><forenames>Qie</forenames></author><author><keyname>Fisac</keyname><forenames>Jaime</forenames></author><author><keyname>Akametalu</keyname><forenames>Kene</forenames></author><author><keyname>Mackin</keyname><forenames>Casey</forenames></author><author><keyname>Tomlin</keyname><forenames>Claire</forenames></author></authors><title>Guaranteeing Safety and Liveness of Unmanned Aerial Vehicle Platoons on
  Air Highways</title><categories>cs.MA cs.SY</categories><comments>To be submitted to The American Institute of Aeronautics and
  Astronautics Journal of Guidance, Control, and Dynamics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been immense interest in using unmanned aerial vehicles
(UAVs) for civilian operations such as package delivery, aerial surveillance,
and disaster response. As a result, UAV traffic management systems are needed
to support potentially thousands of UAVs flying simultaneously in the air
space, in order to ensure their liveness and safety requirements are met.
Currently, the analysis of large multi-agent systems cannot tractably provide
these guarantees if the agents' set of maneuvers are unrestricted. In this
paper, we propose to have platoons of UAVs flying on air highways in order to
impose the air space structure that allows for tractable analysis and intuitive
monitoring. For the air highway placement problem, we use the flexible and
efficient fast marching method to solve the Eikonal equation, which produces a
sequence of air highways that minimizes the cost of flying from an origin to
any destination. Within the platoons that travel on the air highways, we model
each vehicle as a hybrid system with modes corresponding to its role in the
platoon. Using Hamilton-Jacobi reachability, we propose several liveness
controllers and a safety controller that guarantee the success and safety of
all mode transitions. For a single altitude range, our approach guarantees
safety for one safety breach per vehicle; in the unlikely event of multiple
safety breaches, safety can be guaranteed over multiple altitude ranges. We
demonstrate the satisfaction of liveness and safety requirements through
simulations of three common scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08151</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08151</id><created>2016-02-25</created><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author></authors><title>Learning to Abstain from Binary Prediction</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address how to learn a binary classifier capable of abstaining from making
a label prediction. Such a classifier hopes to abstain where it would be most
inaccurate if forced to predict, so it has two goals in tension with each
other: minimizing errors, and avoiding abstaining unnecessarily often.
  In this work, we exactly characterize the best achievable tradeoff between
these two goals in a general semi-supervised setting, given an ensemble of
classifiers of varying competence as well as unlabeled data on which we wish to
predict or abstain. We give an algorithm for learning a classifier which trades
off its errors with abstentions in a minimax optimal manner. This algorithm is
as efficient as linear learning and prediction, and comes with strong and
robust theoretical guarantees. Our analysis extends to a large class of loss
functions and other scenarios, including ensembles comprised of &quot;specialist&quot;
classifiers that can themselves abstain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08155</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08155</id><created>2016-02-25</created><authors><author><keyname>Murukan</keyname><forenames>Prajeesh</forenames></author><author><keyname>Jamaluddine</keyname><forenames>Dana</forenames></author><author><keyname>Kolhapure</keyname><forenames>Shalaka</forenames></author><author><keyname>Mikhael</keyname><forenames>Fady</forenames></author><author><keyname>Nouzari</keyname><forenames>Shiva</forenames></author></authors><title>A Cost-based Placement Algorithm for Multiple Virtual Security
  Appliances in Cloud using SDN: MO-UFLP(Multi-Ordered Uncapacitated Facility
  Location Problem)</title><categories>cs.CR</categories><comments>A Cost-based Placement Algorithm for Multiple Virtual Security
  Appliances in Cloud using SDN Concordia University, Montreal, QC H4B 1R6,
  Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Defined Networking (SDN), has introduced many advanced platforms for
managing networks and adopting different security tools with them, but the cost
of these platforms should be considered as well. In this paper, we present an
extension of the existing approach to the optimal placement of virtual security
appliances in a pre-defined network setting. The approach proposed by Bouet [1]
only considered one security appliance, we extended his approach to several
virtual security appliances. We conducted several simulation tests showing good
performances of our approach. To show the feasibility, we implemented our
approach using SDN and virtual security appliances and integrated it into
OpenStack. This extension adapts UFLP algorithm to real world situations where
several middle boxes need to be deployed to satisfy security needs for the
applications deployed in the cloud. We realized this approach by implementing
&quot;OpenStack on top of OpenStack&quot; , a nested OpenStack implementation with
OpenDayLight as the SDN controller .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08156</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08156</id><created>2016-02-25</created><authors><author><keyname>Ni</keyname><forenames>Chien-Chun</forenames></author><author><keyname>Su</keyname><forenames>Zhengyu</forenames></author><author><keyname>Gao</keyname><forenames>Jie</forenames></author><author><keyname>Gu</keyname><forenames>Xianfeng David</forenames></author></authors><title>Capacitated Kinetic Clustering in Mobile Networks by Optimal
  Transportation Theory</title><categories>cs.NI cs.CG cs.DS cs.SI</categories><comments>9 pages, 10 figures. To be appear in INFOCOM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of capacitated kinetic clustering in which $n$ mobile
terminals and $k$ base stations with respective operating capacities are given.
The task is to assign the mobile terminals to the base stations such that the
total squared distance from each terminal to its assigned base station is
minimized and the capacity constraints are satisfied. This paper focuses on the
development of \emph{distributed} and computationally efficient algorithms that
adapt to the motion of both terminals and base stations. Suggested by the
optimal transportation theory, we exploit the structural property of the
optimal solution, which can be represented by a power diagram on the base
stations such that the total usage of nodes within each power cell equals the
capacity of the corresponding base station. We show by using the kinetic data
structure framework the first analytical upper bound on the number of changes
in the optimal solution, i.e., its stability. On the algorithm side, using the
power diagram formulation we show that the solution can be represented in size
proportional to the number of base stations and can be solved by an iterative,
local algorithm. In particular, this algorithm can naturally exploit the
continuity of motion and has orders of magnitude faster than existing solutions
using min-cost matching and linear programming, and thus is able to handle
large scale data under mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08158</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08158</id><created>2016-02-25</created><authors><author><keyname>Ferrer</keyname><forenames>Gabriel J.</forenames></author></authors><title>Associative Memories and Human-Robot Social Interaction</title><categories>cs.RO</categories><comments>Presented at &quot;2nd Workshop on Cognitive Architectures for Social
  Human-Robot Interaction 2016 (arXiv:1602.01868)</comments><report-no>CogArch4sHRI/2016/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper, we discuss how the use of a cognitive architecture
based on unsupervised clustering (the Kohonen Self-Organizing Map) enables us
to meet our goals of efficient action selection in a mobile robot. This
architecture provides several opportunities for human-robot interaction, and we
discuss how its features facilitate these interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08159</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08159</id><created>2016-02-25</created><authors><author><keyname>Fujii</keyname><forenames>Keisuke</forenames></author><author><keyname>Nakajima</keyname><forenames>Kohei</forenames></author></authors><title>Harnessing disordered quantum dynamics for machine learning</title><categories>quant-ph cs.AI cs.LG cs.NE nlin.CD</categories><comments>19 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computer has an amazing potential of fast information processing.
However, realisation of a digital quantum computer is still a challenging
problem requiring highly accurate controls and key application strategies. Here
we propose a novel platform, quantum reservoir computing, to solve these issues
successfully by exploiting natural quantum dynamics, which is ubiquitous in
laboratories nowadays, for machine learning. In this framework, nonlinear
dynamics including classical chaos can be universally emulated in quantum
systems. A number of numerical experiments show that quantum systems consisting
of at most seven qubits possess computational capabilities comparable to
conventional recurrent neural networks of 500 nodes. This discovery opens up a
new paradigm for information processing with artificial intelligence powered by
quantum physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08161</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08161</id><created>2016-02-25</created><authors><author><keyname>Zhou</keyname><forenames>Fuhui</forenames></author><author><keyname>Li</keyname><forenames>Zan</forenames></author><author><keyname>Cheng</keyname><forenames>Julian</forenames></author><author><keyname>Li</keyname><forenames>Qunwei</forenames></author><author><keyname>Si</keyname><forenames>Jiangbo</forenames></author></authors><title>Robust Max-Min Fairness Energy Harvesting in Secure MISO Cognitive Radio
  With SWIPT</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures, submitted to GlobeCom 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiple-input single-output cognitive radio downlink network is studied
with simultaneous wireless information and power transfer. In this network, a
secondary user coexists with multiple primary users and multiple energy
harvesting receivers. In order to guarantee secure communication and energy
harvesting, the problem of robust secure artificial noise-aided beamforming and
power splitting design is investigated under imperfect channel state
information. Specifically, the max-min fairness energy harvesting problem is
formulated under the bounded channel state information error model. A
one-dimensional search algorithm based on ${\cal S}\text{-Procedure} $ is
proposed to solve the problem. It is shown that the optimal robust secure
beamforming can be achieved. A tradeoff is elucidated between the secrecy rate
of the secondary user receiver and the energy harvested by the energy
harvesting receivers under a max-min fairness criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08162</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08162</id><created>2016-02-25</created><authors><author><keyname>Abdullah</keyname><forenames>Amirali</forenames></author><author><keyname>Daruki</keyname><forenames>Samira</forenames></author><author><keyname>Roy</keyname><forenames>Chitradeep Dutta</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Streaming Verification of Graph Properties</title><categories>cs.DS</categories><comments>26 pages, 2 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming interactive proofs (SIPs) are a framework for outsourced
computation, so that a computationally limited streaming client (the verifier)
hands over a large data set to an untrusted server (the prover) in the cloud
and the two parties run a protocol to confirm the correctness of result with
high probability.
  SIPs are particularly interesting for problems that are hard to solve (or
even approximate) well in a streaming setting. The most notable of these
problems is finding maximum matchings, which has received intense interest in
recent years but has strong lower bounds even for constant factor
approximations. In this paper, we present efficient streaming interactive
proofs that can verify maximum matchings exactly. Our results cover all flavors
of matchings (bipartite/non-bipartite and weighted). In addition, we also
present streaming verifiers for approximate metric TSP. In particular, these
are the first efficient results for weighted matchings and for metric TSP in
any streaming verification model.
  Our streaming verifiers use only polylogarithmic space while exchanging only
polylogarithmic communication with the prover in addition to the output size of
the relevant solution and its certificate size. Our protocols use $\log n$
rounds of communication, but can be modified to work in constant rounds with a
slight increase in total communication cost in some cases.
  Our protocols work by constructing a linear map from the edge updates to
updates of a specially constructed higher dimensional tensor. We then formulate
the graph-theoretic construct as a series of inverse frequency distribution
questions on this tensor, and verify the answers using fingerprinting and
low-degree polynomial extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08166</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08166</id><created>2016-02-25</created><authors><author><keyname>Chang</keyname><forenames>Yi-Jun</forenames></author><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author></authors><title>An Exponential Separation Between Randomized and Deterministic
  Complexity in the LOCAL Model</title><categories>cs.CC cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past 30 years numerous algorithms have been designed for symmetry
breaking problems in the LOCAL model, such as maximal matching, MIS, vertex
coloring, and edge-coloring. For most problems the best randomized algorithm is
at least exponentially faster than the best deterministic algorithm. In this
paper we prove that these exponential gaps are necessary and establish
connections between the deterministic and randomized complexities in the LOCAL
model. Each result has a very compelling take-away message:
  1. Fast $\Delta$-coloring of trees requires random bits: Building on the
recent lower bounds of Brandt et al. [11], we prove that the randomized
complexity of $\Delta$-coloring a tree with maximum degree $\Delta \ge 55$ is
$\Theta(\log_\Delta \log n)$, whereas its deterministic complexity is
$\Theta(\log_\Delta n)$ for any $\Delta\ge 3$. This also establishes a large
separation between the deterministic complexity of $\Delta$-coloring and
$(\Delta+1)$-coloring trees.
  2. Randomized lower bounds imply deterministic lower bounds: We prove that
any deterministic algorithm for a natural class of problems that runs in $O(1)
+ o(\log_\Delta n)$ rounds can be transformed to run in $O(\log^* n -
\log^*\Delta + 1)$ rounds. If the transformed algorithm violates a lower bound
(even allowing randomization), then one can conclude that the problem requires
$\Omega(\log_\Delta n)$ time deterministically.
  3. Deterministic lower bounds imply randomized lower bounds: We prove that
the randomized complexity of any natural problem on instances of size $n$ is at
least its deterministic complexity on instances of size $\sqrt{\log n}$. This
shows that a deterministic $\Omega(\log_\Delta n)$ lower bound for any problem
implies a randomized $\Omega(\log_\Delta \log n)$ lower bound. It also
illustrates that the graph shattering technique is absolutely essential to the
LOCAL model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08185</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08185</id><created>2016-02-25</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author></authors><title>Extension spectrale d'un signal de parole de la bande t\'el\'ephonique
  \`a la bande AM</title><categories>cs.SD cs.MM</categories><comments>61 pages, in French, Master's thesis, University of Sherbrooke, 2001</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This document proposes a bandwidth extension system producing a wideband
signal from a narrowband speech signal. The extension is performed
independently for high and low frequencies. High-frequency extension uses the
excitation-filter model. Extension of the excitation is performed in the time
domain using a non-linear function, while the spectral envelope is extended in
the cepstral domain using a multi-layer perceptron. Low-band extension is based
on the sinusoidal model. The amplitude of sinusoids is also estimated using a
multi-layer perceptron.
  The results show that the sound quality after extension is higher than that
of narrowband speech, with a significant variation across listeners. Some of
the techniques, including excitation extension, are of interest in the field of
speech coding.
  -----
  Le pr\'esent m\'emoire propose un syst\`eme d'extension de la bande
permettant de produire un signal en bande AM \`a partir d'un signal de parole
en bande t\'el\'ephonique. L'extension est effectu\'ee de fa\c{c}on
ind\'ependante pour les hautes fr\'equences et les basses fr\'equences.
L'extension des hautes fr\'equences utilise le mod\`ele filtre-excitation.
L'extension de l'excitation est r\'ealis\'ee dans le domaine temporel par une
fonction non lin\'eaire, alors que l'extension de l'enveloppe spectrale
s'effectue dans le domaine cepstral par un perceptron multi-couches.
L'extension de la bande basse utilise le mod\`ele sinuso\&quot;idal. L'amplitude des
sinuso\&quot;ides est aussi estim\'ee par un perceptron multi-couches.
  Les r\'esultats obtenus montrent que la qualit\'e sonore apr\`es extension
est sup\'erieure \`a celle de la bande t\'el\'ephonique, avec une importante
diff\'erence entre les auditeurs. Certaines techniques d\'evelopp\'ees, dont
l'extension de l'excitation, pr\'esentent un certain int\'er\^et pour le
domaine du codage de la parole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08186</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08186</id><created>2016-02-25</created><authors><author><keyname>Ha-Thuc</keyname><forenames>Viet</forenames></author><author><keyname>Xu</keyname><forenames>Ye</forenames></author><author><keyname>Kanduri</keyname><forenames>Satya Pradeep</forenames></author><author><keyname>Wu</keyname><forenames>Xianren</forenames></author><author><keyname>Dialani</keyname><forenames>Vijay</forenames></author><author><keyname>Yan</keyname><forenames>Yan</forenames></author><author><keyname>Gupta</keyname><forenames>Abhishek</forenames></author><author><keyname>Sinha</keyname><forenames>Shakti</forenames></author></authors><title>Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn</title><categories>cs.IR cs.LG</categories><comments>Demonstrated at WWW 2015, The 25th International World Wide Web
  Conference (WWW 2015)</comments><doi>10.1145/2872518.2890549</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One key challenge in talent search is how to translate complex criteria of a
hiring position into a search query. This typically requires deep knowledge on
which skills are typically needed for the position, what are their
alternatives, which companies are likely to have such candidates, etc. However,
listing examples of suitable candidates for a given position is a relatively
easy job. Therefore, in order to help searchers overcome this challenge, we
design a next generation of talent search paradigm at LinkedIn: Search by Ideal
Candidates. This new system only needs the searcher to input one or several
examples of suitable candidates for the position. The system will generate a
query based on the input candidates and then retrieve and rank results based on
the query as well as the input candidates. The query is also shown to the
searcher to make the system transparent and to allow the searcher to interact
with it. As the searcher modifies the initial query and makes it deviate from
the ideal candidates, the search ranking function dynamically adjusts an
refreshes the ranking results balancing between the roles of query and ideal
candidates. As of writing this paper, the new system is being launched to our
customers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08191</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08191</id><created>2016-02-25</created><updated>2016-03-08</updated><authors><author><keyname>Kim</keyname><forenames>Hanjoo</forenames></author><author><keyname>Park</keyname><forenames>Jaehong</forenames></author><author><keyname>Jang</keyname><forenames>Jaehee</forenames></author><author><keyname>Yoon</keyname><forenames>Sungroh</forenames></author></authors><title>DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and
  Caffe Compatibility</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing complexity of deep neural networks (DNNs) has made it
challenging to exploit existing large-scale data processing pipelines for
handling massive data and parameters involved in DNN training. Distributed
computing platforms and GPGPU-based acceleration provide a mainstream solution
to this computational challenge. In this paper, we propose DeepSpark, a
distributed and parallel deep learning framework that simultaneously exploits
Apache Spark for large-scale distributed data management and Caffe for
GPU-based acceleration. DeepSpark directly accepts Caffe input specifications,
providing seamless compatibility with existing designs and network structures.
To support parallel operations, DeepSpark automatically distributes workloads
and parameters to Caffe-running nodes using Spark and iteratively aggregates
training results by a novel lock-free asynchronous variant of the popular
elastic averaging stochastic gradient descent (SGD) update scheme, effectively
complementing the synchronized processing capabilities of Spark. DeepSpark is
an on-going project, and the current release is available at
http://deepspark.snu.ac.kr.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08194</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08194</id><created>2016-02-26</created><authors><author><keyname>Spring</keyname><forenames>Ryan</forenames></author><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author></authors><title>Scalable and Sustainable Deep Learning via Randomized Hashing</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current deep learning architectures are growing larger in order to learn from
enormous datasets.These architectures require giant matrix multiplication
operations to train millions or billions of parameters during forward and back
propagation steps. These operations are very expensive from a computational and
energy standpoint. We present a novel technique to reduce the amount of
computation needed to train and test deep net-works drastically. Our approach
combines recent ideas from adaptive dropouts and randomized hashing for maximum
inner product search to select only the nodes with the highest activation
efficiently. Our new algorithm for training deep networks reduces the overall
computational cost,of both feed-forward pass and backpropagation,by operating
on significantly fewer nodes. As a consequence, our algorithm only requires 5%
of computations (multiplications) compared to traditional algorithms, without
any loss in the accuracy. Furthermore, due to very sparse gradient updates, our
algorithm is ideally suited for asynchronous training leading to near linear
speedup with increasing parallelism. We demonstrate the scalability and
sustainability (energy efficiency) of our proposed algorithm via rigorous
experimental evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08199</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08199</id><created>2016-02-26</created><authors><author><keyname>Kim</keyname><forenames>Makoto Naruse Song-Ju</forenames></author><author><keyname>Aono</keyname><forenames>Masashi</forenames></author><author><keyname>Berthel</keyname><forenames>Martin</forenames></author><author><keyname>Drezet</keyname><forenames>Aur&#xe9;lien</forenames></author><author><keyname>Huant</keyname><forenames>Serge</forenames></author><author><keyname>Hori</keyname><forenames>Hirokazu</forenames></author></authors><title>Category theoretic analysis of single-photon decision maker</title><categories>physics.optics cs.AI quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision making is a vital function in the era of artificial intelligence;
however, its physical realizations and their theoretical fundamentals are not
yet known. In our former study [Sci. Rep. 5, 513253 (2015)], we demonstrated
that single photons can be used to make decisions in uncertain, dynamically
changing environments. The multi-armed bandit problem was successfully solved
using the dual probabilistic and particle attributes of single photons. Herein,
we present the category theoretic foundation of the single-photon-based
decision making, including quantitative analysis that agrees well with the
experimental results. The category theoretic model unveils complex
interdependencies of the entities of the subject matter in the most simplified
manner, including a dynamically changing environment. In particular, the
octahedral structure in triangulated categories provides a clear understanding
of the underlying mechanisms of the single-photon decision maker. This is the
first demonstration of a category theoretic interpretation of decision making,
and provides a solid understanding and a design fundamental for intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08204</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08204</id><created>2016-02-26</created><authors><author><keyname>Kuzuoka</keyname><forenames>Shigeaki</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>On Distributed Computing for Functions with Certain Structures</title><categories>cs.IT math.IT</categories><comments>7 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of distributed function computation for the class of smooth
sources is studied, where functions to be computed are compositions of
symbol-wise functions and some outer functions that are not symbol-wise. The
optimal rate for computing those functions is characterized in terms of the
Slepian-Wolf rate and an equivalence class of sources induced by functions. To
prove the result, a new method to derive a converse bound for distributed
computing is proposed; the bound is derived by identifying a source that is
inevitably conveyed to the decoder and by explicitly constructing a code for
reproducing that source. As a byproduct, it provides a conceptually simple
proof of the known fact that computing a Boolean function may require as large
rate as reproducing the entire source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08207</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08207</id><created>2016-02-26</created><authors><author><keyname>Fletcher</keyname><forenames>Alyson K.</forenames></author></authors><title>Learning and Free Energy in Expectation Consistent Approximate Inference</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximations of loopy belief propagation are commonly combined with
expectation-maximization (EM) for probabilistic inference problems when the
densities have unknown parameters. This work considers an approximate EM
learning method combined with Opper and Winther's Expectation Consistent
Approximate Inference method. The combined algorithm is called EM-EC and is
shown to have a simple variational free energy interpretation. In addition, the
algorithm can provide a computationally efficient and general approach to a
number of learning problems with hidden states including empirical Bayesian
forms of regression, classification, compressed sensing, and sparse Bayesian
learning. Systems with linear dynamics interconnected with non-Gaussian or
nonlinear components can also be easily considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08210</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08210</id><created>2016-02-26</created><updated>2016-02-29</updated><authors><author><keyname>Zhang</keyname><forenames>Saizheng</forenames></author><author><keyname>Wu</keyname><forenames>Yuhuai</forenames></author><author><keyname>Che</keyname><forenames>Tong</forenames></author><author><keyname>Lin</keyname><forenames>Zhouhan</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Architectural Complexity Measures of Recurrent Neural Networks</title><categories>cs.LG cs.NE</categories><comments>19 pages, 15 figures; comments fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we systematically analyse the connecting architectures of
recurrent neural networks (RNNs). Our main contribution is twofold: first, we
present a rigorous graph-theoretic framework describing the connecting
architectures of RNNs in general. Second, we propose three architecture
complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's
over-time nonlinear complexity, (b) the feedforward depth, which captures the
local input-output nonlinearity (similar to the &quot;depth&quot; in feedforward neural
networks (FNNs)), and (c) the recurrent skip coefficient which captures how
rapidly the information propagates over time. Our experimental results show
that RNNs might benefit from larger recurrent depth and feedforward depth. We
further demonstrate that increasing recurrent skip coefficient offers
performance boosts on long term dependency problems, as we improve the
state-of-the-art for sequential MNIST dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08213</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08213</id><created>2016-02-26</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Michaud</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Rouat</keyname><forenames>Jean</forenames></author><author><keyname>L&#xe9;tourneau</keyname><forenames>Dominic</forenames></author></authors><title>Robust Sound Source Localization Using a Microphone Array on a Mobile
  Robot</title><categories>cs.RO cs.SD</categories><comments>6 pages</comments><journal-ref>Proc. IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), pp. 1228-1233, 2003</journal-ref><doi>10.1109/IROS.2003.1248813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hearing sense on a mobile robot is important because it is
omnidirectional and it does not require direct line-of-sight with the sound
source. Such capabilities can nicely complement vision to help localize a
person or an interesting event in the environment. To do so the robot auditory
system must be able to work in noisy, unknown and diverse environmental
conditions. In this paper we present a robust sound source localization method
in three-dimensional space using an array of 8 microphones. The method is based
on time delay of arrival estimation. Results show that a mobile robot can
localize in real time different types of sound sources over a range of 3 meters
and with a precision of 3 degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08215</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08215</id><created>2016-02-26</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Lefebvre</keyname><forenames>Roch</forenames></author></authors><title>Bandwidth Extension of Narrowband Speech for Low Bit-Rate Wideband
  Coding</title><categories>cs.SD</categories><comments>3 pages</comments><journal-ref>Proc. IEEE Speech Coding Workshop (SCW), 2000, pp. 130-132</journal-ref><doi>10.1109/SCFT.2000.878425</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless telephone speech is usually limited to the 300-3400 Hz band, which
reduces its quality. There is thus a growing demand for wideband speech systems
that transmit from 50 Hz to 8000 Hz. This paper presents an algorithm to
generate wideband speech from narrowband speech using as low as 500 bits/s of
side information. The 50-300 Hz band is predicted from the narrowband signal. A
source-excitation model is used for the 3400-8000 Hz band, where the excitation
is extrapolated at the receiver, and the spectral envelope is transmitted.
Though some artifacts are present, the resulting wideband speech has enhanced
quality compared to narrowband speech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08225</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08225</id><created>2016-02-26</created><authors><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Zheng</keyname><forenames>Wei-Long</forenames></author><author><keyname>Lu</keyname><forenames>Bao-Liang</forenames></author></authors><title>Multimodal Emotion Recognition Using Multimodal Deep Learning</title><categories>cs.HC cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To enhance the performance of affective models and reduce the cost of
acquiring physiological signals for real-world applications, we adopt
multimodal deep learning approach to construct affective models from multiple
physiological signals. For unimodal enhancement task, we indicate that the best
recognition accuracy of 82.11% on SEED dataset is achieved with shared
representations generated by Deep AutoEncoder (DAE) model. For multimodal
facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE)
achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets,
respectively, which are much superior to the state-of-the-art approaches. For
cross-modal learning task, our experimental results demonstrate that the mean
accuracy of 66.34% is achieved on SEED dataset through shared representations
generated by EEG-based DAE as training samples and shared representations
generated by eye-based DAE as testing sample, and vice versa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08228</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08228</id><created>2016-02-26</created><authors><author><keyname>Farouk</keyname><forenames>Ahmed</forenames></author><author><keyname>Zakaria</keyname><forenames>Magdy</forenames></author><author><keyname>Megahed</keyname><forenames>Adel</forenames></author><author><keyname>Omara</keyname><forenames>Fatma A.</forenames></author></authors><title>A generalized architecture of quantum secure direct communication for N
  disjointed users with authentication</title><categories>cs.CR quant-ph</categories><journal-ref>Nature Scientific Reports, Sci. Rep. 5, 160802015)</journal-ref><doi>10.1038/srep16080</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we generalize a secured direct communication process between N
users with partial and full cooperation of quantum server. The security
analysis of authentication and communication processes against many types of
attacks proved that the attacker cannot gain any information during
intercepting either authentication or communication processes. Hence, the
security of transmitted message among N users is ensured as the attacker
introduces an error probability irrespective of the sequence of measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08230</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08230</id><created>2016-02-26</created><authors><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>Schlotter</keyname><forenames>Ildik&#xf3;</forenames></author></authors><title>Stable Marriage with Covering Constraints: A Complete Computational
  Trichotomy</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stable marriage problems equipped with covering constraints: here
the input distinguishes a subset of women as well as a subset of men, and we
seek a matching with fewest number of blocking pairs that matches all of the
distinguished people. This concept strictly generalizes the notion of arranged
marriages introduced by Knuth in 1976, in which the partner of each
distinguished person is fixed a priori.
  Our main result is a complete computational complexity trichotomy of the
stable marriage problem with covering constraints, into polynomial-time
solvable cases, fixed-parameter tractable cases, and cases that are W[1]-hard,
for every choice among a set of natural parameters, namely the maximum length
of preference lists for men and women, the number of distinguished men and
women, and the number of blocking pairs allowed. Thereby, we fully answer an
open problem of Hamada et al. (ESA 2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08232</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08232</id><created>2016-02-26</created><authors><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Ashikhmin</keyname><forenames>Alexei</forenames></author><author><keyname>Yang</keyname><forenames>Hong</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Marzetta</keyname><forenames>Thomas L.</forenames></author></authors><title>Cell-Free Massive MIMO versus Small Cells</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Cell-Free Massive MIMO (multiple-input multiple-output) system comprises a
very large number of distributed access points (APs)which simultaneously serve
a much smaller number of users over the same time/frequency resources based on
directly measured channel characteristics. The APs and users have only one
antenna each. The APs acquire channel state information through time-division
duplex operation and the reception of uplink pilot signals transmitted by the
users. The APs perform multiplexing/de-multiplexing through conjugate
beamforming on the downlink and matched filtering on the uplink. Closed-form
expressions for individual user uplink and downlink throughputs lead to max-min
power control algorithms. Max-min power control ensures uniformly good service
throughout the area of coverage. A pilot assignment algorithm helps to mitigate
the effects of pilot contamination, but power control is far more important in
that regard.
  Cell-Free Massive MIMO has considerably improved performance with respect to
a conventional small-cell scheme, whereby each user is served by a dedicated
AP, in terms of both 95%-likely per-user throughput and immunity to shadow
fading spatial correlation. Under uncorrelated shadow fading conditions, the
cell-free scheme provides nearly 5-fold improvement in 95%-likely per-user
throughput over the small-cell scheme, and 10-fold improvement when shadow
fading is correlated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08237</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08237</id><created>2016-02-26</created><authors><author><keyname>Engen</keyname><forenames>Vegard</forenames></author><author><keyname>Pickering</keyname><forenames>J. Brian</forenames></author><author><keyname>Walland</keyname><forenames>Paul</forenames></author></authors><title>Machine Agency in Human-Machine Networks; Impacts and Trust Implications</title><categories>cs.HC cs.CY cs.SI</categories><comments>Pre-print; To be presented at the 18th International Conference on
  Human-Computer Interaction International, Toronto, Canada, 17 - 22 July 2016</comments><acm-class>H.1.2; K.4.0; K.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We live in an emerging hyper-connected era in which people are in contact and
interacting with an increasing number of other people and devices.
Increasingly, modern IT systems form networks of humans and machines that
interact with one another. As machines take a more active role in such
networks, they exert an in-creasing level of influence on other participants.
We review the existing literature on agency and propose a definition of agency
that is practical for describing the capabilities and impact human and machine
actors may have in a human-machine network. On this basis, we discuss and
demonstrate the impact and trust implica-tions for machine actors in
human-machine networks for emergency decision support, healthcare and future
smart homes. We maintain that machine agency not only facilitates human to
machine trust, but also interpersonal trust; and that trust must develop to be
able to seize the full potential of future technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08238</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08238</id><created>2016-02-26</created><authors><author><keyname>Iwasaki</keyname><forenames>Atsushi</forenames></author><author><keyname>Umeno</keyname><forenames>Ken</forenames></author></authors><title>Periodicity of odd degree Chebyshev polynomials over a ring of modulo
  $2^w$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Odd degree Chebyshev polynomials over a ring of modulo $2^w$ have two kinds
of period. One is an &quot;orbital period&quot;. Odd degree Chebyshev polynomials are
bijection over the ring. Therefore, when an odd degree Chebyshev polynomial
iterate affecting a factor of the ring, we can observe an orbit over the ring.
The &quot;orbital period&quot; is a period of the orbit. The other is a &quot;degree period&quot;.
It is observed when changing the degree of Chebyshev polynomials with a fixed
argument of polynomials. Both kinds of period have not been completely studied.
In this paper, we clarify completely both of them. The knowledge about the
periodicity are expected to be useful for analysis of cryptography and
designing a new pseudo random number generator. In addition, we show that
degree decision problem of Chebyshev polynomial over the ring can effectively
be solved by using the periodicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08254</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08254</id><created>2016-02-26</created><authors><author><keyname>Bl&#xf6;mer</keyname><forenames>Johannes</forenames></author><author><keyname>Lammersen</keyname><forenames>Christiane</forenames></author><author><keyname>Schmidt</keyname><forenames>Melanie</forenames></author><author><keyname>Sohler</keyname><forenames>Christian</forenames></author></authors><title>Theoretical Analysis of the $k$-Means Algorithm - A Survey</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-means algorithm is one of the most widely used clustering heuristics.
Despite its simplicity, analyzing its running time and quality of approximation
is surprisingly difficult and can lead to deep insights that can be used to
improve the algorithm. In this paper we survey the recent results in this
direction as well as several extension of the basic $k$-means method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08255</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08255</id><created>2016-02-26</created><updated>2016-02-29</updated><authors><author><keyname>Liu</keyname><forenames>Dong</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author></authors><title>Cache-enabled Heterogeneous Cellular Networks: Comparison and Tradeoffs</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE International Conference on Communications (ICC)
  2016. This version includes detailed proofs of the propositions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching popular contents at base stations (BSs) is a promising way to unleash
the potential of cellular heterogeneous networks (HetNets), where backhaul has
become a bottleneck. In this paper, we compare a cache-enabled HetNet where a
tier of multi-antenna macro BSs is overlaid by a tier of helper nodes having
caches but no backhaul with a conventional HetNet where the macro BSs tier is
overlaid by a tier of pico BSs with limited-capacity backhaul. We resort
stochastic geometry theory to derive the area spectral efficiencies (ASEs) of
these two kinds of HetNets and obtain the closed-form expressions under a
special case. We use numerical results to show that the helper density is only
1/4 of the pico BS density to achieve the same target ASE, and the helper
density can be further reduced by increasing cache capacity. With given total
cache capacity within an area, there exists an optimal helper node density that
maximizes the ASE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08260</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08260</id><created>2016-02-26</created><authors><author><keyname>Tesarova</keyname><forenames>Eva</forenames></author><author><keyname>Svorenova</keyname><forenames>Maria</forenames></author><author><keyname>Barnat</keyname><forenames>Jiri</forenames></author><author><keyname>Cerna</keyname><forenames>Ivana</forenames></author></authors><title>Optimal Observation Mode Scheduling for Systems under Temporal
  Constraints</title><categories>cs.LO cs.GT cs.SY</categories><comments>Technical report accompanying ACC'16 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous control systems use various sensors to decrease the amount of
uncertainty under which they operate. While providing partial observation of
the current state of the system, sensors require resources such as energy, time
and communication. We consider discrete systems with non-deterministic
transitions and multiple observation modes. The observation modes provide
different information about the states of the system and are associated with
non-negative costs. We consider two control problems. First, we aim to
construct a control and observation mode switching strategy that guarantees
satisfaction of a finite-time temporal property given as a formula of
syntactically co-safe fragment of LTL (scLTL) and at the same time, minimizes
the worst-case cost accumulated until the point of satisfaction. Second, the
bounded version of the problem is considered, where the temporal property must
be satisfied within given finite time bound. We present correct and optimal
solutions to both problems and demonstrate their usability on a case study
motivated by robotic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08268</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08268</id><created>2016-02-26</created><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Wieseke</keyname><forenames>Nicolas</forenames></author></authors><title>Construction of Gene and Species Trees from Sequence Data incl.
  Orthologs, Paralogs, and Xenologs</title><categories>q-bio.PE cs.DS q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phylogenetic reconstruction aims at finding plausible hypotheses of the
evolutionary history of genes or species based on genomic sequence information.
The distinction of orthologous genes (genes that having a common ancestry and
diverged after a speciation) is crucial and lies at the heart of many genomic
studies. However, existing methods that rely only on 1:1 orthologs to infer
species trees are strongly restricted to a small set of allowed genes that
provide information about the species tree. The use of larger gene sets that
consist in addition of non-orthologous genes (e.g. so-called paralogous or
xenologous genes) considerably increases the information about the evolutionary
history of the respective species. In this work, we introduce a novel method to
compute species phylogenies based on sequence data including orthologs,
paralogs or even xenologs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08273</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08273</id><created>2016-02-26</created><authors><author><keyname>Brandt</keyname><forenames>Rasmus</forenames></author><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Globally Optimal Base Station Clustering in Interference Alignment-Based
  Multicell Networks</title><categories>cs.IT math.IT</categories><comments>Accepted in IEEE Signal Processing Letters. (c) 2016 IEEE. Personal
  use of this material is permitted. However, permission to reprint/republish
  this material for advertising or promotional purposes or for creating new
  collective works for resale or redistribution to servers or lists, or to
  reuse any copyrighted component of this work in other works must be obtained
  from the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinated precoding based on interference alignment is a promising
technique for improving the throughputs in future wireless multicell networks.
In small networks, all base stations can typically jointly coordinate their
precoding. In large networks however, base station clustering is necessary due
to the otherwise overwhelmingly high channel state information (CSI)
acquisition overhead. In this work, we provide a branch and bound algorithm for
finding the globally optimal base station clustering. The algorithm is mainly
intended for benchmarking existing suboptimal clustering schemes. We propose a
general model for the user throughputs, which only depends on the long-term CSI
statistics. The model assumes intracluster interference alignment and is able
to account for the CSI acquisition overhead. By enumerating a search tree using
a best-first search and pruning sub-trees in which the optimal solution
provably cannot be, the proposed method converges to the optimal solution. The
pruning is done using specifically derived bounds, which exploit some assumed
structure in the throughput model. It is empirically shown that the proposed
method has an average complexity which is orders of magnitude lower than that
of exhaustive search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08290</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08290</id><created>2016-02-26</created><authors><author><keyname>Van Houdt</keyname><forenames>Benny</forenames></author></authors><title>Explicit back-off rates for achieving target throughputs in CSMA/CA
  networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CSMA/CA networks have often been analyzed using a stylized model that is
fully characterized by a vector of back-off rates and a conflict graph.
Further, for any achievable throughput vector $\vec \theta$ the existence of a
unique vector $\vec \nu(\vec \theta)$ of back-off rates that achieves this
throughput vector was proven. Although this unique vector can in principle be
computed iteratively, the required time complexity grows exponentially in the
network size, making this only feasible for small networks.
  In this paper, we present an explicit formula for the unique vector of
back-off rates $\vec \nu(\vec \theta)$ needed to achieve any achievable
throughput vector $\vec \theta$ provided that the network has a chordal
conflict graph. This class of networks contains a number of special cases of
interest such as (inhomogeneous) line networks and networks with an acyclic
conflict graph. Moreover, these back-off rates are such that the back-off rate
of a node only depends on its own target throughput and the target throughput
of its neighbors and can be determined in a distributed manner.
  We further indicate that back-off rates of this form cannot be obtained in
general for networks with non-chordal conflict graphs. For general conflict
graphs we nevertheless show how to adapt the back-off rates when a node is
added to the network when its interfering nodes form a clique in the conflict
graph. Finally, we introduce a distributed chordal approximation algorithm for
general conflict graphs which is shown (using numerical examples) to be more
accurate than the Bethe approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08298</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08298</id><created>2016-02-26</created><authors><author><keyname>Augustine</keyname><forenames>John</forenames></author><author><keyname>Moses</keyname><forenames>William K.</forenames><suffix>Jr.</suffix></author><author><keyname>Redlich</keyname><forenames>Amanda</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>Balanced Allocation: Patience is not a Virtue</title><categories>cs.DS cs.DM</categories><comments>25 pages, preliminary version accepted at SODA 2016</comments><acm-class>F.2.2; G.2.0; G.3</acm-class><journal-ref>In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on
  Discrete Algorithms (SODA 2016), 655-671</journal-ref><doi>10.1137/1.9781611974331.ch48</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Load balancing is a well-studied problem, with balls-in-bins being the
primary framework. The greedy algorithm $\mathsf{Greedy}[d]$ of Azar et al.
places each ball by probing $d &gt; 1$ random bins and placing the ball in the
least loaded of them. It ensures a maximum load that is exponentially better
than the strategy of placing each ball uniformly at random. V\&quot;ocking showed
that a slightly asymmetric variant, $\mathsf{Left}[d]$, provides a further
significant improvement. However, this improvement comes at an additional
computational cost of imposing structure on the bins.
  Here, we present a fully decentralized and easy-to-implement algorithm called
$\mathsf{FirstDiff}[d]$ that combines the simplicity of $\mathsf{Greedy}[d]$
and the improved balance of $\mathsf{Left}[d]$. The key idea in
$\mathsf{FirstDiff}[d]$ is to probe until a different bin size from the first
observation is located, then place the ball. Although the number of probes
could be quite large for some of the balls, we show that
$\mathsf{FirstDiff}[d]$ requires only $d$ probes on average per ball (in both
the standard and the heavily-loaded settings). Thus the number of probes is no
greater than either that of $\mathsf{Greedy}[d]$ or $\mathsf{Left}[d]$. More
importantly, we show that $\mathsf{FirstDiff}[d]$ closely matches the improved
maximum load ensured by $\mathsf{Left}[d]$ in both the standard and
heavily-loaded settings. We further provide a tight lower bound on the maximum
load up to $O(\log \log \log n)$ terms. We additionally give experimental data
that $\mathsf{FirstDiff}[d]$ is indeed as good as $\mathsf{Left}[d]$, if not
better, in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08313</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08313</id><created>2016-02-26</created><authors><author><keyname>Hassanat</keyname><forenames>Ahmad B. A.</forenames></author><author><keyname>Alkafaween</keyname><forenames>Esra'a</forenames></author><author><keyname>Al-Nawaiseh</keyname><forenames>Nedal A.</forenames></author><author><keyname>Abbadi</keyname><forenames>Mohammad A.</forenames></author><author><keyname>Alkasassbeh</keyname><forenames>Mouhammd</forenames></author><author><keyname>Alhasanat</keyname><forenames>Mahmoud B.</forenames></author></authors><title>Enhancing Genetic Algorithms using Multi Mutations</title><categories>cs.AI cs.NE</categories><comments>17 pages, 11 figures, 1 table, 41 references</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mutation is one of the most important stages of the genetic algorithm because
of its impact on the exploration of global optima, and to overcome premature
convergence. There are many types of mutation, and the problem lies in
selection of the appropriate type, where the decision becomes more difficult
and needs more trial and error. This paper investigates the use of more than
one mutation operator to enhance the performance of genetic algorithms. Novel
mutation operators are proposed, in addition to two selection strategies for
the mutation operators, one of which is based on selecting the best mutation
operator and the other randomly selects any operator. Several experiments on
some Travelling Salesman Problems (TSP) were conducted to evaluate the proposed
methods, and these were compared to the well-known exchange mutation and
rearrangement mutation. The results show the importance of some of the proposed
methods, in addition to the significant enhancement of the genetic algorithm's
performance, particularly when using more than one mutation operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08321</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08321</id><created>2016-02-26</created><authors><author><keyname>Narayanaswamy</keyname><forenames>Ganesh</forenames></author><author><keyname>Joshi</keyname><forenames>Saurabh</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author></authors><title>The Virtues of Conflict: Analyzing Modern Concurrency</title><categories>cs.SE</categories><comments>13 pages, 5 figures, Accepted at 21st 21st ACM SIGPLAN Symposium on
  Principles and Practice of Parallel Programming</comments><doi>10.1145/2851141.2851165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern shared memory multiprocessors permit reordering of memory operations
for performance reasons. These reorderings are often a source of subtle bugs in
programs written for such architectures. Traditional approaches to verify weak
memory programs often rely on interleaving semantics, which is prone to state
space explosion, and thus severely limits the scalability of the analysis. In
recent times, there has been a renewed interest in modelling dynamic executions
of weak memory programs using partial orders. However, such an approach
typically requires ad-hoc mechanisms to correctly capture the data and
control-flow choices/conflicts present in real-world programs. In this work, we
propose a novel, conflict-aware, composable, truly concurrent semantics for
programs written using C/C++ for modern weak memory architectures. We exploit
our symbolic semantics based on general event structures to build an efficient
decision procedure that detects assertion violations in bounded multi-threaded
programs. Using a large, representative set of benchmarks, we show that our
conflict-aware semantics outperforms the state-of-the-art partial-order based
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08323</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08323</id><created>2016-02-26</created><authors><author><keyname>O'Connor</keyname><forenames>Peter</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Deep Spiking Networks</title><categories>cs.NE</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Spiking Multi-Layer Perceptron (SMLP). The SMLP is a spiking
version of a conventional Multi-Layer Perceptron with rectified-linear units.
Our architecture is event-based, meaning that neurons in the network
communicate by sending &quot;events&quot; to downstream neurons, and that the state of
each neuron is only updated when it receives an event. We show that the SMLP
behaves identically, during both prediction and training, to a conventional
deep network of rectified-linear units in the limiting case where we run the
spiking network for a long time. We apply this architecture to a conventional
classification problem (MNIST) and achieve performance very close to that of a
conventional MLP with the same architecture. Our network is a natural
architecture for learning based on streaming event-based data, and has
potential applications in robotic systems systems, which require low power and
low response latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08325</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08325</id><created>2016-02-26</created><authors><author><keyname>Hassanat</keyname><forenames>Ahmad B. A.</forenames></author><author><keyname>Alhasanat</keyname><forenames>Mahmoud B.</forenames></author><author><keyname>Abbadi</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Btoush</keyname><forenames>Eman</forenames></author><author><keyname>Al-Awadi</keyname><forenames>Mouhammd</forenames></author><author><keyname>Tarawneh</keyname><forenames>Ahmad S.</forenames></author></authors><title>Victory Sign Biometric for Terrorists Identification</title><categories>cs.CV</categories><comments>7 pages, 5 figures, 4 tables, 26 references</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Covering the face and all body parts, sometimes the only evidence to identify
a person is their hand geometry, and not the whole hand- only two fingers (the
index and the middle fingers) while showing the victory sign, as seen in many
terrorists videos. This paper investigates for the first time a new way to
identify persons, particularly (terrorists) from their victory sign. We have
created a new database in this regard using a mobile phone camera, imaging the
victory signs of 50 different persons over two sessions. Simple measurements
for the fingers, in addition to the Hu Moments for the areas of the fingers
were used to extract the geometric features of the shown part of the hand shown
after segmentation. The experimental results using the KNN classifier were
encouraging for most of the recorded persons; with about 40% to 93% total
identification accuracy, depending on the features, distance metric and K used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08327</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08327</id><created>2016-02-26</created><authors><author><keyname>Zheng</keyname><forenames>Kan</forenames><affiliation>Sherman</affiliation></author><author><keyname>Li</keyname><forenames>Hang</forenames><affiliation>Sherman</affiliation></author><author><keyname>Lei</keyname><forenames>Lei</forenames><affiliation>Sherman</affiliation></author><author><keyname>Xiang</keyname><forenames>Wei</forenames><affiliation>Sherman</affiliation></author><author><keyname>Qiao</keyname><forenames>Jian</forenames><affiliation>Sherman</affiliation></author><author><keyname>Xuemin</keyname><affiliation>Sherman</affiliation></author><author><keyname>Shen</keyname></author></authors><title>Energy-Efficient Localization and Tracking of Mobile Devices in Wireless
  Sensor Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE Transactions on Vehicular Technology (submitted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSNs) are effective for locating and tracking
people and objects in various industrial environments. Since energy consumption
is critical to prolonging the lifespan of WSNs, we propose an energy-efficient
LOcalization and Tracking} (eLOT) system, using low-cost and portable hardware
to enable highly accurate tracking of targets. Various fingerprint-based
approaches for localization and tracking are implemented in eLOT. In order to
achieve high energy efficiency, a network-level scheme coordinating collision
and interference is proposed. On the other hand, based on the location
information, mobile devices in eLOT can quickly associate with the specific
channel in a given area, while saving energy through avoiding unnecessary
transmission. Finally, a platform based on TI CC2530 and the Linux operating
system is built to demonstrate the effectiveness of our proposed scheme in
terms of localization accuracy and energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08332</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08332</id><created>2016-02-26</created><authors><author><keyname>Leibfried</keyname><forenames>Felix</forenames></author><author><keyname>Braun</keyname><forenames>Daniel Alexander</forenames></author></authors><title>Bounded Rational Decision-Making in Feedforward Neural Networks</title><categories>cs.AI cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded rational decision-makers transform sensory input into motor output
under limited computational resources. Mathematically, such decision-makers can
be modeled as information-theoretic channels with limited transmission rate.
Here, we apply this formalism for the first time to multilayer feedforward
neural networks. We derive synaptic weight update rules for two scenarios,
where either each neuron is considered as a bounded rational decision-maker or
the network as a whole. In the update rules, bounded rationality translates
into information-theoretically motivated types of regularization in weight
space. In experiments on the MNIST benchmark classification task for
handwritten digits, we show that such information-theoretic regularization
successfully prevents overfitting across different architectures and attains
state-of-the-art results for both ordinary and convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08349</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08349</id><created>2016-02-26</created><authors><author><keyname>Santocanale</keyname><forenames>Luigi</forenames><affiliation>LIF</affiliation></author></authors><title>Relational lattices via duality</title><categories>cs.LO cs.DB math.LO</categories><comments>Coalgebraic Methods in Computer Science 2016, Apr 2016, Eindhoven,
  Netherlands</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The natural join and the inner union combine in different ways tables of a
relational database. Tropashko [18] observed that these two operations are the
meet and join in a class of lattices-called the relational lattices- and
proposed lattice theory as an alternative algebraic approach to databases.
Aiming at query optimization, Litak et al. [12] initiated the study of the
equational theory of these lattices. We carry on with this project, making use
of the duality theory developed in [16]. The contributions of this paper are as
follows. Let A be a set of column's names and D be a set of cell values; we
characterize the dual space of the relational lattice R(D, A) by means of a
generalized ultrametric space, whose elements are the functions from A to D,
with the P (A)-valued distance being the Hamming one but lifted to subsets of
A. We use the dual space to present an equational axiomatization of these
lattices that reflects the combinatorial properties of these generalized
ultrametric spaces: symmetry and pairwise completeness. Finally, we argue that
these equations correspond to combinatorial properties of the dual spaces of
lattices, in a technical sense analogous of correspondence theory in modal
logic. In particular, this leads to an exact characterization of the finite
lattices satisfying these equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08350</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08350</id><created>2016-02-26</created><authors><author><keyname>Glauner</keyname><forenames>Patrick O.</forenames></author><author><keyname>Boechat</keyname><forenames>Andre</forenames></author><author><keyname>Dolberg</keyname><forenames>Lautaro</forenames></author><author><keyname>State</keyname><forenames>Radu</forenames></author><author><keyname>Bettinger</keyname><forenames>Franck</forenames></author><author><keyname>Rangoni</keyname><forenames>Yves</forenames></author><author><keyname>Duarte</keyname><forenames>Diogo</forenames></author></authors><title>Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-technical losses (NTL) such as electricity theft cause significant harm
to our economies, as in some countries they may range up to 40% of the total
electricity distributed. Detecting NTLs requires costly on-site inspections.
Accurate prediction of NTLs for customers using machine learning is therefore
crucial. To date, related research largely ignore that the two classes of
regular and non-regular customers are highly imbalanced, that NTL proportions
may change and mostly consider small data sets, often not allowing to deploy
the results in production. In this paper, we present a comprehensive approach
to assess three NTL detection models for different NTL proportions in large
real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and
Support Vector Machine. This work has resulted in appreciable results that are
about to be deployed in a leading industry solution. We believe that the
considerations and observations made in this contribution are necessary for
future smart meter research in order to report their effectiveness on
imbalanced and large real world data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08355</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08355</id><created>2016-02-25</created><authors><author><keyname>Aboua&#xef;ssa</keyname><forenames>Hassane</forenames></author><author><keyname>Fliess</keyname><forenames>Michel</forenames></author><author><keyname>Join</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>On short-term traffic flow forecasting and its reliability</title><categories>stat.AP cs.OH</categories><comments>8th IFAC Conference on Manufacturing Modeling, Management &amp; Control
  (Troyes, France, June 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in time series, where deterministic and stochastic modelings
as well as the storage and analysis of big data are useless, permit a new
approach to short-term traffic flow forecasting and to its reliability, i.e.,
to the traffic volatility. Several convincing computer simulations, which
utilize concrete data, are presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08357</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08357</id><created>2016-02-26</created><authors><author><keyname>Papadimitrou</keyname><forenames>Christos</forenames></author><author><keyname>Petti</keyname><forenames>Samantha</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Cortical Computation via Iterative Constructions</title><categories>cs.NE cs.DS</categories><comments>30 pages, 3 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Boolean functions of an arbitrary number of input variables that can
be realized by simple iterative constructions based on constant-size
primitives. This restricted type of construction needs little global
coordination or control and thus is a candidate for neurally feasible
computation. Valiant's construction of a majority function can be realized in
this manner and, as we show, can be generalized to any uniform threshold
function. We study the rate of convergence, finding that while linear
convergence to the correct function can be achieved for any threshold using a
fixed set of primitives, for quadratic convergence, the size of the primitives
must grow as the threshold approaches 0 or 1. We also study finite realizations
of this process and the learnability of the functions realized. We show that
the constructions realized are accurate outside a small interval near the
target threshold, where the size of the construction grows as the inverse
square of the interval width. This phenomenon, that errors are higher closer to
thresholds (and thresholds closer to the boundary are harder to represent), is
a well-known cognitive finding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08358</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08358</id><created>2016-02-26</created><authors><author><keyname>Frey</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>Potioc, LaBRI, UB</affiliation></author></authors><title>Remote Heart Rate Sensing and Projection to Renew Traditional Board
  Games and Foster Social Interactions</title><categories>cs.HC</categories><proxy>ccsd</proxy><journal-ref>CHI '16 Extended Abstracts, May 2016, San Jose, United States.
  2016</journal-ref><doi>10.1145/2851581.2892391</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While physiological sensors enter the mass market and reach the general
public, they are still mainly employed to monitor health -- whether it is for
medical purpose or sports. We describe an application that uses heart rate
feedback as an incentive for social interactions. A traditional board game has
been &quot;augmented&quot; through remote physiological sensing, using webcams.
Projection helped to conceal the technological aspects from users. We detail
how players reacted -- stressful situations could emerge when users are
deprived from their own signals -- and we give directions for game designers to
integrate physiological sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08361</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08361</id><created>2016-02-26</created><authors><author><keyname>Courtieu</keyname><forenames>Pierre</forenames><affiliation>CEDRIC</affiliation></author><author><keyname>Rieg</keyname><forenames>Lionel</forenames><affiliation>LINCS, NPA, IUF</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LINCS, NPA, IUF</affiliation></author><author><keyname>Urbain</keyname><forenames>Xavier</forenames><affiliation>ENSIIE, LRI</affiliation></author></authors><title>Certified Universal Gathering in $R^2$ for Oblivious Mobile Robots</title><categories>cs.DC cs.DS cs.LO cs.RO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1506.01603</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a unified formal framework for expressing mobile robots models,
protocols, and proofs, and devise a protocol design/proof methodology dedicated
to mobile robots that takes advantage of this formal framework. As a case
study, we present the first formally certified protocol for oblivious mobile
robots evolving in a two-dimensional Euclidean space. In more details, we
provide a new algorithm for the problem of universal gathering mobile oblivious
robots (that is, starting from any initial configuration that is not bivalent,
using any number of robots, the robots reach in a finite number of steps the
same position, not known beforehand) without relying on a common orientation
nor chirality. We give very strong guaranties on the correctness of our
algorithm by proving formally that it is correct, using the COQ proof
assistant. This result demonstrates both the effectiveness of the approach to
obtain new algorithms that use as few assumptions as necessary, and its
manageability since the amount of developed code remains human readable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08369</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08369</id><created>2016-02-26</created><authors><author><keyname>Gast</keyname><forenames>Mikael</forenames></author><author><keyname>Hauptmann</keyname><forenames>Mathias</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author></authors><title>Approximation Complexity of Max-Cut on Power Law Graphs</title><categories>cs.DS cs.CC cs.DM math.CO math.OC</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the MAX-CUT problem on power law graphs (PLGs) with
power law exponent $\beta$. We prove some new approximability results on that
problem. In particular we show that there exist polynomial time approximation
schemes (PTAS) for MAX-CUT on PLGs for the power law exponent $\beta$ in the
interval $(0,2)$. For $\beta&gt;2$ we show that for some $\epsilon&gt;0$, MAX-CUT is
NP-hard to approximate within approximation ratio $1+\epsilon$, ruling out the
existence of a PTAS in this case. Moreover we give an approximation algorithm
with improved constant approximation ratio for the case of $\beta&gt;2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08371</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08371</id><created>2016-02-26</created><authors><author><keyname>Neuen</keyname><forenames>Daniel</forenames></author></authors><title>Graph Isomorphism for unit square graphs</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past decades for more and more graph classes the Graph Isomorphism
Problem was shown to be solvable in polynomial time. An interesting family of
graph classes arises from intersection graphs of geometric objects. In this
work we show that the Graph Isomorphism Problem for unit square graphs,
intersection graphs of axis-parallel unit squares in the plane, can be solved
in polynomial time. The algorithm is based on a combination of structural
insight and understanding the automorphism group. For the latter we introduce a
generalization of bounded degree graphs, which is used to capture the main
structure of unit square graphs. Using group theoretic algorithms we obtain
sufficient information to solve the isomorphism problem for unit square graphs.
Additionally, this gives us insight on the structure of the automorphism group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08388</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08388</id><created>2015-10-26</created><authors><author><keyname>Veitas</keyname><forenames>Viktoras</forenames><affiliation>Weaver</affiliation></author><author><keyname>Weinbaum</keyname><forenames>David</forenames><affiliation>Weaver</affiliation></author></authors><title>Living Cognitive Society: a `digital' World of Views</title><categories>cs.CY nlin.AO</categories><comments>Preprint: 29 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current social reality is characterized by all-encompassing change, which
disrupts existing social structures at all levels. Yet the prevailing view of
society is based on the ontological primacy of stable hierarchical structures,
which is no longer adequate.
  We propose a conceptual framework for thinking about a dynamically changing
social system: the Living Cognitive Society. Importantly, we show how it
follows from a much broader philosophical framework, guided by the theory of
individuation, which emphasizes the importance of relationships and interactive
processes in the evolution of a system.
  The framework addresses society as a living cognitive system -- an ecology of
interacting social subsystems -- each of which is also a living cognitive
system. We argue that this approach can help us to conceive sustainable social
systems that will thrive in the circumstances of accelerating change. The
Living Cognitive Society is explained in terms of its fluid structure, dynamics
and the mechanisms at work. We then discuss the disruptive effects of
Information and Communication Technologies on the mechanisms at work.
  We conclude by delineating a major topic for future research -- distributed
social governance -- which focuses on processes of coordination rather than on
stable structures within global society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08391</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08391</id><created>2015-11-20</created><authors><author><keyname>Shcherbakov</keyname><forenames>V. I.</forenames></author></authors><title>Ultrafast a Distributed Arithmetic in multi-row codes</title><categories>cs.NA</categories><comments>22pages (russian-original) + 19 pages(english-robotic translationn)</comments><acm-class>B.2; B.2.1; B.2.4; B.6.1; B.7.1; C.0; C.1; C.1.2; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the matrix structure of arithmetic processors based
on distributed arithmetic in multi-row codes. Scope - development of
supercomputers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08393</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08393</id><created>2016-02-26</created><authors><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author></authors><title>Exact Weighted Minwise Hashing in Constant Time</title><categories>cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted minwise hashing (WMH) is one of the fundamental subroutine, required
by many celebrated approximation algorithms, commonly adopted in industrial
practice for large scale-search and learning. The resource bottleneck of the
algorithms is the computation of multiple (typically a few hundreds to
thousands) independent hashes of the data. The fastest hashing algorithm is by
Ioffe \cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data
vector, $O(d)$ ($d$ is the number of non-zeros), for computing one hash.
However, the requirement of multiple hashes demands hundreds or thousands
passes over the data. This is very costly for modern massive dataset.
  In this work, we break this expensive barrier and show an expected constant
amortized time algorithm which computes $k$ independent and unbiased WMH in
time $O(k)$ instead of $O(dk)$ required by Ioffe's method. Moreover, our
proposal only needs a few bits (5 - 9 bits) of storage per hash value compared
to around $64$ bits required by the state-of-art-methodologies. Experimental
evaluations, on real datasets, show that for computing 500 WMH, our proposal
can be 60000x faster than the Ioffe's method without losing any accuracy. Our
method is also around 100x faster than approximate heuristics capitalizing on
the efficient &quot;densified&quot; one permutation hashing schemes
\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its
significant advantages, we hope that it will replace existing implementations
in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08394</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08394</id><created>2016-02-26</created><authors><author><keyname>Sohrabi</keyname><forenames>Foad</forenames></author><author><keyname>Davidson</keyname><forenames>Timothy N.</forenames></author></authors><title>Coordinate Update Algorithms for Robust Power Loading for the MU-MISO
  Downlink with Outage Constraints</title><categories>cs.IT math.IT</categories><comments>14 pages, 6 figures, to appear in IEEE Transactions on Signal
  Processing, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of power allocation for the single-cell multi-user
(MU) multiple-input single-output (MISO) downlink with quality-of-service (QoS)
constraints. The base station acquires an estimate of the channels and, for a
given beamforming structure, designs the power allocation so as to minimize the
total transmission power required to ensure that target
signal-to-interference-and-noise ratios at the receivers are met, subject to a
specified outage probability. We consider scenarios in which the errors in the
base station's channel estimates can be modelled as being zero-mean and
Gaussian. Such a model is particularly suitable for time division duplex (TDD)
systems with quasi-static channels, in which the base station estimates the
channel during the uplink phase. Under that model, we employ a precise
deterministic characterization of the outage probability to transform the
chance-constrained formulation to a deterministic one. Although that
deterministic formulation is not convex, we develop a coordinate descent
algorithm that can be shown to converge to a globally optimal solution when the
starting point is feasible. Insight into the structure of the deterministic
formulation yields approximations that result in coordinate update algorithms
with good performance and significantly lower computational cost. The proposed
algorithms provide better performance than existing robust power loading
algorithms that are based on tractable conservative approximations, and can
even provide better performance than robust precoding algorithms based on such
approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08399</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08399</id><created>2015-07-31</created><authors><author><keyname>Alimam</keyname><forenames>Mayla</forenames><affiliation>UPMC, RS2M</affiliation></author><author><keyname>Bertin</keyname><forenames>Emmanuel</forenames><affiliation>RS2M, SAMOVAR</affiliation></author><author><keyname>Crespi</keyname><forenames>Noel</forenames><affiliation>RS2M, SAMOVAR</affiliation></author></authors><title>Social and Collaborative Services for Organizations: Back to
  Requirements</title><categories>cs.CY</categories><comments>The Spring Servitization Conference, May 2015, Birmingham, United
  Kingdom</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social and collaborative services have widely spread within the enterprises
as they play a part in improving productivity and business outcomes. However,
the deployment of these services fluctuates between success and failure. This
paper intends to assess their deployment and how they can contribute to value
creation in different industries. We investigate the relationship between the
services' functionalities and the organizational requirement of these services
represented by the coordination. We also consider the organizational
transformation driven by servitization and emphasize its impact on the act of
coordination. We highlight the tight correlation between the functionalities
and the requirement in organic forms which suggests a successful deployment in
such enterprises. We nonetheless find that, when the servitization is a
strategic intent in organizations with mechanistic characteristics, deploying
social and collaborative services can contribute to achieving this aim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08405</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08405</id><created>2016-02-26</created><authors><author><keyname>Papadopoulos</keyname><forenames>Dim P.</forenames></author><author><keyname>Uijlings</keyname><forenames>Jasper R. R.</forenames></author><author><keyname>Keller</keyname><forenames>Frank</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>We don't need no bounding-boxes: Training object class detectors using
  only human verification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training object class detectors typically requires a large set of images in
which objects are annotated by bounding-boxes. However, manually drawing
bounding-boxes is very time consuming. We propose a new scheme for training
object detectors which only requires annotators to verify bounding-boxes
produced automatically by the learning algorithm. Our scheme iterates between
re-training the detector, re-localizing objects in the training images, and
human verification. We use the verification signal both to improve re-training
and to reduce the search space for re-localisation, which makes these steps
different to what is normally done in a weakly supervised setting. Extensive
experiments on PASCAL VOC 2007 show that (1) using human verification to update
detectors and reduce the search space leads to the rapid production of
high-quality bounding-box annotations; (2) our scheme delivers detectors
performing almost as good as those trained in a fully supervised setting,
without ever drawing any bounding-box; (3) as the verification task is very
quick, our scheme substantially reduces total annotation time by a factor
6x-9x.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08406</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08406</id><created>2016-02-26</created><authors><author><keyname>Jaber</keyname><forenames>Guilhem</forenames></author><author><keyname>Tzevelekos</keyname><forenames>Nikos</forenames></author></authors><title>Trace semantics for polymorphic references</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a trace semantics for a call-by-value language with full
polymorphism and higher-order references. This is an operational game semantics
model based on a nominal interpretation of parametricity whereby polymorphic
values are abstracted with special kinds of names. The use of polymorphic
references leads to violations of parametricity which we counter by closely
recoding the disclosure of typing information in the semantics. We prove the
model sound for the full language and strengthen our result to full abstraction
for a large fragment where polymorphic references obey specific inhabitation
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08409</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08409</id><created>2016-02-26</created><updated>2016-02-29</updated><authors><author><keyname>Guevara</keyname><forenames>Miguel R.</forenames></author><author><keyname>Hartmann</keyname><forenames>Dominik</forenames></author><author><keyname>Aristar&#xe1;n</keyname><forenames>Manuel</forenames></author><author><keyname>Mendoza</keyname><forenames>Marcelo</forenames></author><author><keyname>Hidalgo</keyname><forenames>C&#xe9;sar A.</forenames></author></authors><title>The Research Space: using the career paths of scholars to predict the
  evolution of the research output of individuals, institutions, and nations</title><categories>cs.DL cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years scholars have built maps of science by connecting the
academic fields that cite each other, are cited together, or that cite a
similar literature. But since scholars cannot always publish in the fields they
cite, or that cite them, these science maps are only rough proxies for the
potential of a scholar, organization, or country, to enter a new academic
field. Here we use a large dataset of scholarly publications disambiguated at
the individual level to create a map of science-or research space-where links
connect pairs of fields based on the probability that an individual has
published in both of them. We find that the research space is a significantly
more accurate predictor of the fields that individuals and organizations will
enter in the future than citation based science maps. At the country level,
however, the research space and citations based science maps are equally
accurate. These findings show that data on career trajectories-the set of
fields that individuals have previously published in-provide more accurate
predictors of future research output for more focalized units-such as
individuals or organizations-than citation based science maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08410</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08410</id><created>2016-02-26</created><authors><author><keyname>Rastogi</keyname><forenames>Vaibhav</forenames></author><author><keyname>Davidson</keyname><forenames>Drew</forenames></author><author><keyname>De Carli</keyname><forenames>Lorenzo</forenames></author><author><keyname>Jha</keyname><forenames>Somesh</forenames></author><author><keyname>McDaniel</keyname><forenames>Patrick</forenames></author></authors><title>Towards Least Privilege Containers with Cimplifier</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Application containers, such as Docker containers, have recently gained
popularity as a solution for agile and seamless deployment of applications.
These light-weight virtualization environments run applications that are packed
together with their resources and configuration information, and thus can be
deployed across various software platforms. However, these software ecosystems
are not conducive to the true and tried security principles of privilege
separation (PS) and principle of least privilege (PLP). We propose algorithms
and a tool Cimplifier, which address these concerns in the context of
containers. Specifically, given a container our tool partitions them into
simpler containers, which are only provided enough resources to perform their
functionality. As part our solution, we develop techniques for analyzing
resource usage, for performing partitioning, and gluing the containers together
to preserve functionality. Our evaluation on real-world containers demonstrates
that Cimplifier can preserve the original functionality, leads to reduction in
image size of 58-95%, and processes even large containers in under thirty
seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08412</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08412</id><created>2016-02-01</created><authors><author><keyname>Fernandez-de-Cossio-Diaz</keyname><forenames>Jorge</forenames></author><author><keyname>Mulet</keyname><forenames>Roberto</forenames></author></authors><title>Fast inference of ill-posed problems within a convex space</title><categories>cs.OH cond-mat.dis-nn cond-mat.stat-mech q-bio.MN</categories><comments>25 pages, 11 figures</comments><msc-class>62F30, 52A20, 91D30</msc-class><acm-class>G.3; F.2.1; C.2.1; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multiple scientific and technological applications we face the problem of
having low dimensional data to be justified by a linear model defined in a high
dimensional parameter space. The difference in dimensionality makes the problem
ill-defined: the model is consistent with the data for many values of its
parameters. The objective is to find the probability distribution of parameter
values consistent with the data, a problem that can be cast as the exploration
of a high dimensional convex polytope. In this work we introduce a novel
algorithm to solve this problem efficiently. It provides results that are
statistically indistinguishable from currently used numerical techniques while
its running time scales linearly with the system size. We show that the
algorithm performs robustly in many abstract and practical applications. As
working examples we simulate the effects of restricting reaction fluxes on the
space of feasible phenotypes of a {\em genome} scale E. Coli metabolic network
and infer the traffic flow between origin and destination nodes in a real
communication network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08423</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08423</id><created>2016-02-26</created><authors><author><keyname>Imran</keyname><forenames>Muhammad</forenames></author><author><keyname>Meier</keyname><forenames>Patrick</forenames></author><author><keyname>Castillo</keyname><forenames>Carlos</forenames></author><author><keyname>Lesa</keyname><forenames>Andre</forenames></author><author><keyname>Herranz</keyname><forenames>Manuel Garcia</forenames></author></authors><title>Enabling Digital Health by Automatic Classification of Short Messages</title><categories>cs.CY cs.HC cs.SI</categories><comments>Accepted at the ACM Digital Health Conference, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In response to the growing HIV/AIDS and other health-related issues, UNICEF
through their U-Report platform receives thousands of messages (SMS) every day
to provide prevention strategies, health case advice, and counsel- ing support
to vulnerable population. Due to a rapid increase in U-Report usage (up to 300%
in last 3 years), plus approximately 1,000 new registrations each day, the
volume of messages has thus continued to increase, which made it impossible for
the team at UNICEF to process them in a timely manner. In this paper, we
present a platform designed to perform automatic classification of short
messages (SMS) in real-time to help UNICEF categorize and prioritize
health-related messages as they arrive. We employ a hybrid approach, which
combines human and machine intelligence that seeks to resolve the information
overload issue by introducing processing of large-scale data at high-speed
while maintaining a high classification accuracy. The system has recently been
tested in conjunction with UNICEF in Zambia to classify short messages received
via the U-Report platform on various health related issues. The system is
designed to enable UNICEF make sense of a large volume of short messages in a
timely manner. In terms of evaluation, we report design choices, challenges,
and performance of the system observed during the deployment to validate its
effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08425</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08425</id><created>2016-02-26</created><authors><author><keyname>Bernard</keyname><forenames>Florian</forenames></author><author><keyname>Salamanca</keyname><forenames>Luis</forenames></author><author><keyname>Thunberg</keyname><forenames>Johan</forenames></author><author><keyname>Tack</keyname><forenames>Alexander</forenames></author><author><keyname>Jentsch</keyname><forenames>Dennis</forenames></author><author><keyname>Lamecker</keyname><forenames>Hans</forenames></author><author><keyname>Zachow</keyname><forenames>Stefan</forenames></author><author><keyname>Hertel</keyname><forenames>Frank</forenames></author><author><keyname>Goncalves</keyname><forenames>Jorge</forenames></author><author><keyname>Gemmar</keyname><forenames>Peter</forenames></author></authors><title>Shape-aware Surface Reconstruction from Sparse Data</title><categories>cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reconstruction of an object's shape or surface from a set of 3D points is
a common topic in materials and life sciences, computationally handled in
computer graphics. Such points usually stem from optical or tactile 3D
coordinate measuring equipment. Surface reconstruction also appears in medical
image analysis, e.g. in anatomy reconstruction from tomographic measurements or
the alignment of intra-operative navigation and preoperative planning data. In
contrast to mere 3D point clouds, medical imaging yields contextual information
on the 3D point data that can be used to adopt prior information on the shape
that is to be reconstructed from the measurements. In this work we propose to
use a statistical shape model (SSM) as a prior for surface reconstruction. The
prior knowledge is represented by a point distribution model (PDM) that is
associated with a surface mesh. Using the shape distribution that is modelled
by the PDM, we reformulate the problem of surface reconstruction from a
probabilistic perspective based on a Gaussian Mixture Model (GMM). In order to
do so, the given measurements are interpreted as samples of the GMM. By using
mixture components with anisotropic covariances that are oriented according to
the surface normals at the PDM points, a surface-based fitting is accomplished.
By estimating the parameters of the GMM in a maximum a posteriori manner, the
reconstruction of the surface from the given measurements is achieved.
Extensive experiments suggest that our proposed approach leads to superior
surface reconstructions compared to Iterative Closest Point (ICP) methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08444</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08444</id><created>2016-02-26</created><authors><author><keyname>You</keyname><forenames>Lei</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author></authors><title>Optimizing Power and User Association for Energy Saving in Load-Coupled
  Cooperative LTE</title><categories>cs.IT math.IT</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider an energy minimization problem for cooperative LTE networks. To
reduce energy consumption, we investigate how to jointly optimize the transmit
power and the association between cells and user equipments (UEs), by taking
into consideration joint transmission (JT), one of the coordinated multipoint
(CoMP) techniques. We formulate the optimization problem mathematically. For
solving the problem, a dynamic power allocation algorithm that adjusts the
transmit power of all cells, and an algorithm for optimizing the cell-UE
association, are proposed. The two algorithms are iteratively used in an
algorithmic framework to enhance the energy performance. Numerically, the
proposed algorithms can lead to lower energy consumption than the optimal
energy setting in the non-JT case. In comparison to fixed power allocation in
JT, the proposed dynamic power allocation algorithm is able to significantly
reduce the energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08446</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08446</id><created>2016-02-26</created><authors><author><keyname>You</keyname><forenames>Lei</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author></authors><title>Load Balancing via Joint Transmission in Heterogeneous LTE: Modeling and
  Computation</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  As one of the Coordinated Multipoint (CoMP) techniques, Joint Transmission
(JT) can improve the overall system performance. In this paper, from the load
balancing perspective, we study how the maximum load can be reduced by
optimizing JT pattern that characterizes the association between cells and User
Equipments (UEs). To give a model of the interference caused by cells with
different time-frequency resource usage, we extend a load coupling model, by
taking into account JT. In this model, the mutual interference depends on the
load of cells coupled in a non-linear system with each other. Under this model,
we study a two-cell case and proved that the optimality is achieved in linear
time in the number of UEs. After showing the complexity of load balancing in
the general network scenario, an iterative algorithm for minimizing the maximum
load, named JT-MinMax, is proposed. We evaluate JT-MinMax in a Heterogeneous
Network (HetNet), though it is not limited to this type of scenarios. Numerical
results demonstrate the significant performance improvement of JT-MinMax on
min-max cell load, compared to the conventional non-JT solution where each UE
is served by the cell with best received transmit signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08447</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08447</id><created>2016-02-24</created><authors><author><keyname>Ali</keyname><forenames>Mumtaz</forenames></author><author><keyname>Van Minh</keyname><forenames>Nguyen</forenames></author><author><keyname>Son</keyname><forenames>Le Hoang</forenames></author></authors><title>A Neutrosophic Recommender System for Medical Diagnosis Based on
  Algebraic Neutrosophic Measures</title><categories>cs.AI</categories><comments>Keywords: Medical diagnosis, neutrosophic set, neutrosophic
  recommender system, non-linear regression model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neutrosophic set has the ability to handle uncertain, incomplete,
inconsistent, indeterminate information in a more accurate way. In this paper,
we proposed a neutrosophic recommender system to predict the diseases based on
neutrosophic set which includes single-criterion neutrosophic recommender
system (SC-NRS) and multi-criterion neutrosophic recommender system (MC-NRS).
Further, we investigated some algebraic operations of neutrosophic recommender
system such as union, complement, intersection, probabilistic sum, bold sum,
bold intersection, bounded difference, symmetric difference, convex linear sum
of min and max operators, Cartesian product, associativity, commutativity and
distributive. Based on these operations, we studied the algebraic structures
such as lattices, Kleen algebra, de Morgan algebra, Brouwerian algebra, BCK
algebra, Stone algebra and MV algebra. In addition, we introduced several types
of similarity measures based on these algebraic operations and studied some of
their theoretic properties. Moreover, we accomplished a prediction formula
using the proposed algebraic similarity measure. We also proposed a new
algorithm for medical diagnosis based on neutrosophic recommender system.
Finally to check the validity of the proposed methodology, we made experiments
on the datasets Heart, RHC, Breast cancer, Diabetes and DMD. At the end, we
presented the MSE and computational time by comparing the proposed algorithm
with the relevant ones such as ICSM, DSM, CARE, CFMD, as well as other variants
namely Variant 67, Variant 69, and Varian 71 both in tabular and graphical form
to analyze the efficiency and accuracy. Finally we analyzed the strength of all
8 algorithms by ANOVA statistical tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08448</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08448</id><created>2016-02-26</created><updated>2016-02-29</updated><authors><author><keyname>Russo</keyname><forenames>Daniel</forenames></author></authors><title>Simple Bayesian Algorithms for Best Arm Identification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the optimal adaptive allocation of measurement effort
for identifying the best among a finite set of options or designs. An
experimenter sequentially chooses designs to measure and observes noisy signals
of their quality with the goal of confidently identifying the best design after
a small number of measurements. I propose three simple Bayesian algorithms for
adaptively allocating measurement effort. One is Top-Two Probability sampling,
which computes the two designs with the highest posterior probability of being
optimal, and then randomizes to select among these two. One is a variant a
top-two sampling which considers not only the probability a design is optimal,
but the expected amount by which it exceeds other designs. The final algorithm
is a modified version of Thompson sampling that is tailored for identifying the
best design. I prove that these simple algorithms satisfy a strong optimality
property. In a frequestist setting where the true quality of the designs is
fixed, the posterior is said to be consistent if it correctly identifies the
optimal design, in the sense that that the posterior probability assigned to
the event that some other design is optimal converges to zero as measurements
are collected. I show that under the proposed algorithms this convergence
occurs at an exponential rate, and the corresponding exponent is the best
possible among all allocation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08451</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08451</id><created>2016-02-06</created><authors><author><keyname>Palchykov</keyname><forenames>Vasyl</forenames></author><author><keyname>Gemmetto</keyname><forenames>Valerio</forenames></author><author><keyname>Boyarsky</keyname><forenames>Alexey</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author></authors><title>Ground truth? Concept-based communities versus the external
  classification of physics manuscripts</title><categories>cs.DL physics.soc-ph</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection techniques are widely used to infer hidden structures
within interconnected systems. Despite demonstrating high accuracy on
benchmarks, they reproduce the external classification for many real-world
systems with a significant level of discrepancy. A widely accepted reason
behind such outcome is the unavoidable loss of non-topological information
(such as node attributes) encountered when the original complex system is
represented as a network. In this article we emphasize that the observed
discrepancies may also be caused by a different reason: the external
classification itself. For this end we use scientific publication data which i)
exhibit a well defined modular structure and ii) hold an expert-made
classification of research articles. Having represented the articles and the
extracted scientific concepts both as a bipartite network and as its unipartite
projection, we applied modularity optimization to uncover the inner thematic
structure. The resulting clusters are shown to partly reflect the author-made
classification, although some significant discrepancies are observed. A
detailed analysis of these discrepancies shows that they carry essential
information about the system, mainly related to the use of similar techniques
and methods across different (sub)disciplines, that is otherwise omitted when
only the external classification is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08455</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08455</id><created>2016-02-26</created><authors><author><keyname>You</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Jianbo</forenames></author><author><keyname>Wei</keyname><forenames>Changjiang</forenames></author><author><keyname>Hu</keyname><forenames>Lejuan</forenames></author></authors><title>MPAR: A Movement Pattern-Aware Optimal Routing for Social Delay Tolerant
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>18 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Social Delay Tolerant Networks (SDTNs) are a special kind of Delay Tolerant
Network (DTN) that consists of a number of mobile devices with social
characteristics. The current research achievements on routing algorithms tend
to separately evaluate the available profit for each prospective relay node and
cannot achieve the global optimal performance in an overall perspective. In
this paper, we propose a Movement Pattern-Aware optimal Routing (MPAR) for
SDTNs, by choosing the optimal relay node(s) set for each message, which
eventually based on running a search algorithm on a hyper-cube solution space.
Concretely, the movement pattern of a group of node(s) can be extracted from
the movement records of nodes. Then the set of commonly visited locations for
the relay node(s) set and the destination node is obtained, by which we can
further evaluate the co-delivery probability of the relay node(s) set. Both
local search scheme and tabu-search scheme are utilized in finding the optimal
set, and the tabu-search based routing Tabu-MPAR is proved able to guide the
relay node(s) set in evolving to the optimal one. We demonstrate how the MPAR
algorithm significantly outperforms the previous ones through extensive
simulations, based on the synthetic SDTN mobility model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08456</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08456</id><created>2016-02-26</created><authors><author><keyname>Ogura</keyname><forenames>Masaki</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Epidemic Processes over Adaptive State-Dependent Networks</title><categories>cs.SI math.PR physics.soc-ph q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the dynamics of epidemic processes taking place in
adaptive networks of arbitrary topology. We focus our study on the adaptive
susceptible-infected-susceptible (ASIS) model, where healthy individuals are
allowed to temporarily cut edges connecting them to infected nodes in order to
prevent the spread of the infection. In this paper, we derive a closed-form
expression for the epidemic threshold of the ASIS model in arbitrary networks
with heterogeneous node and edge dynamics. For networks with homogeneous node
and edge dynamics, we show that the resulting epidemic threshold is
proportional to the threshold of the standard SIS model over static networks.
Furthermore, based on our results, we propose an efficient algorithm to
optimally tune the adaptation rates in order to eradicate epidemic outbreaks in
arbitrary networks. We confirm the accuracy of our theoretical results with
several numerical simulations and compare our optimal adaptation rates with
popular centrality measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08459</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08459</id><created>2016-02-26</created><authors><author><keyname>Wang</keyname><forenames>Zheng</forenames></author></authors><title>Take up DNSSEC When Needed</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The threats of caching poisoning attacks largely stimulate the deployment of
DNSSEC. Being a strong but demanding cryptographical defense, DNSSEC has its
universal adoption predicted to go through a lengthy transition. Thus the
DNSSEC practitioners call for a secure yet lightweight solution to speed up
DNSSEC deployment while offering an acceptable DNSSEC-like defense. This paper
proposes a new defense against cache poisoning attacks, still using but lightly
using DNSSEC. In the solution, DNS operates in the DNSSEC-oblivious mode unless
a potential attack is detected and triggers a switch to the DNSSEC-aware mode.
The performance of the defense is analyzed and validated. The modeling checking
results demonstrate that only a small DNSSEC query load is needed to ensure a
small enough cache poisoning success rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08461</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08461</id><created>2016-02-26</created><authors><author><keyname>You</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Jianbo</forenames></author><author><keyname>We</keyname><forenames>Changjiang</forenames></author><author><keyname>Dai</keyname><forenames>Chenqu</forenames></author></authors><title>A One-Hop Information Based Geographic Routing Protocol for Delay
  Tolerant MANETs</title><categories>cs.NI cs.IT math.IT</categories><comments>14 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Delay and Disruption Tolerant Networks (DTNs) may lack continuous network
connectivity. Routing in DTNs is thus a challenge since it must handle network
partitioning, long delays, and dynamic topology. Meanwhile, routing protocols
of the traditional Mobile Ad hoc NETworks (MANETs) cannot work well due to the
failure of its assumption that most network connections are available. In this
article, a geographic routing protocol is proposed for MANETs in delay tolerant
situations, by using no more than one-hop information. A utility function is
designed for implementing the under-controlled replication strategy. To reduce
the overheads caused by message flooding, we employ a criterion so as to
evaluate the degree of message redundancy. Consequently a message redundancy
coping mechanism is added to our routing protocol. Extensive simulations have
been conducted and the results show that when node moving speed is relatively
low, our routing protocol outperforms the other schemes such as Epidemic, Spray
and Wait, FirstContact in delivery ratio and average hop count, while
introducing an acceptable overhead ratio into the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08463</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08463</id><created>2016-02-26</created><authors><author><keyname>Ferguson</keyname><forenames>Holly T.</forenames></author><author><keyname>Buccellato</keyname><forenames>Aimee. P. C.</forenames></author><author><keyname>Paolucci</keyname><forenames>Samuel</forenames></author><author><keyname>Yu</keyname><forenames>Na</forenames></author><author><keyname>Vardeman</keyname><forenames>Charles F.</forenames><suffix>II</suffix></author></authors><title>Green Scale Research Tool for Multi-Criteria and Multi-Metric Energy
  Analysis Performed During the Architectural Design Process</title><categories>cs.CE</categories><comments>38 pages double spaced, including appendices; single column format;
  multi-disciplinary work</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Prevailing computational tools available to and used by architecture and
engineering professionals purport to gather and present thorough and accurate
perspectives of the environmental impacts associated with their contributions
to the built environment. The presented research of building modeling and
analysis software used by the Architecture, Engineering, Construction, and
Operations (AECO) industry reveals that many of the most heavily relied-upon
industry tools are isolated in functionality, utilize incomplete models and
data, and are disruptive to normative design and building optimization
workflows. This paper describes the current models and tools, their primary
functions and limitations, and presents our concurrent research to develop more
advanced models to assess lifetime building energy consumption alongside
operating energy use. A series of case studies describes the current
state-of-the-art in tools and building energy analysis followed by the research
models and novel design and analysis Tool that the Green Scale Research Group
has developed in response. A fundamental goal of this effort is to increase the
use and efficacy of building impact studies conducted by architects, engineers,
and building owners and operators during the building design process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08465</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08465</id><created>2016-02-26</created><updated>2016-02-29</updated><authors><author><keyname>Han</keyname><forenames>Wei</forenames></author><author><keyname>Khorrami</keyname><forenames>Pooya</forenames></author><author><keyname>Paine</keyname><forenames>Tom Le</forenames></author><author><keyname>Ramachandran</keyname><forenames>Prajit</forenames></author><author><keyname>Babaeizadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Shi</keyname><forenames>Honghui</forenames></author><author><keyname>Li</keyname><forenames>Jianan</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Seq-NMS for Video Object Detection</title><categories>cs.CV</categories><comments>Technical Report for Imagenet VID Competition 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video object detection is challenging because objects that are easily
detected in one frame may be difficult to detect in another frame within the
same clip. Recently, there have been major advances for doing object detection
in a single image. These methods typically contain three phases: (i) object
proposal generation (ii) object classification and (iii) post-processing. We
propose a modification of the post-processing phase that uses high-scoring
object detections from nearby frames to boost scores of weaker detections
within the same clip. We show that our method obtains superior results to
state-of-the-art single image object detection techniques. Our method placed
3rd in the video object detection (VID) task of the ImageNet Large Scale Visual
Recognition Challenge 2015 (ILSVRC2015).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08466</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08466</id><created>2016-02-26</created><authors><author><keyname>You</keyname><forenames>Lei</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author></authors><title>Range Assignment for Power Optimization in Load-Coupled Heterogeneous
  Networks</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider the problem of transmission energy op- timization via range
assignment for Low Power Nodes (LPNs) in Long Term Evolution (LTE) Heterogenous
Networks (HetNets). The optimization is subject to the load coupling model,
where the cells interfere with one another. Each cell provides data service for
its users so as to maintain a target Quality-of-Service (QoS). We prove that,
irrespective the presence of maximum power limit or its value, operating at
full load is optimal. We perform energy minimization by optimizing the
association between User Equipments (UEs) and cells via selecting cell-
specific offsets on LPNs. Moreover, the optimization problem is proved to be
NP-hard. We propose a tabu search algorithm for offset optimization (TSO). For
each offset, TSO computes the optimal power solution such that all cells
operate at full load. Numerical results demonstrate the significant performance
improvement of TSO on optimizing the sum transmission energy, compared to the
conventional solution where uniform offset is used for all LPNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08469</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08469</id><created>2016-02-26</created><authors><author><keyname>You</keyname><forenames>Lei</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author></authors><title>A Performance Study of Energy Minimization for Interleaved and Localized
  FDMA</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Optimal channel allocation is a key performance engineering aspect in
single-carrier frequency-division multiple access (SC-FDMA). It is of
significance to consider minimum sum power (Min-Power), subject to meeting
specified user's demand, since mobile users typically employ battery-powered
handsets. In this paper, we prove that Min-Power is polynomial-time solvable
for interleaved SC-FDMA (IFDMA). Then we propose a channel allocation algorithm
for IFDMA, which is guaranteed to achieve global optimum in polynomial time. We
numerically compare the proposed algorithm with optimal localized SC-FDMA
(LFDMA) for Min-Power. The results show that LFDMA outperforms IFDMA in the
maximal supported user demand. When the user demand can be satisfied in both
LFDMA and IFDMA, LFDMA performs slightly better than IFDMA. However Min- Power
is polynomial-time solvable for IFDMA whereas it is not for LFDMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08472</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08472</id><created>2016-02-26</created><authors><author><keyname>Zhou</keyname><forenames>Kai</forenames></author><author><keyname>Afifi</keyname><forenames>M. H.</forenames></author><author><keyname>Ren</keyname><forenames>Jian</forenames></author></authors><title>ExpSOS: Secure and Verifiable Outsourcing of Exponentiation Operations
  for Mobile Cloud Computing</title><categories>cs.CR</categories><comments>28 pages, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete exponential operation, such as modular exponentiation and scalar
multiplication on elliptic curves, is a basic operation of many public-key
cryptosystems. However, the exponential operations are considered prohibitively
expensive for resource-constrained mobile devices. In this paper, we address
the problem of secure outsourcing of exponentiation operations to one single
untrusted server. Our proposed scheme (ExpSOS) only requires very limited
number of modular multiplications at local mobile environment thus it can
achieve impressive computational gain. ExpSOS also provides a secure
verification scheme with probability approximately 1 to ensure that the mobile
end-users can always receive valid results. The comprehensive analysis as well
as the simulation results in real mobile device demonstrates that our proposed
ExpSOS can significantly improve the existing schemes in efficiency, security
and result verifiability. We apply ExpSOS to securely outsource several
cryptographic protocols to show that ExpSOS is widely applicable to many
cryptographic computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08477</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08477</id><created>2016-02-26</created><authors><author><keyname>Zenker</keyname><forenames>Erik</forenames></author><author><keyname>Worpitz</keyname><forenames>Benjamin</forenames></author><author><keyname>Widera</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Huebl</keyname><forenames>Axel</forenames></author><author><keyname>Juckeland</keyname><forenames>Guido</forenames></author><author><keyname>Kn&#xfc;pfer</keyname><forenames>Andreas</forenames></author><author><keyname>Nagel</keyname><forenames>Wolfgang E.</forenames></author><author><keyname>Bussmann</keyname><forenames>Michael</forenames></author></authors><title>Alpaka - An Abstraction Library for Parallel Kernel Acceleration</title><categories>cs.DC cs.MS</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Porting applications to new hardware or programming models is a tedious and
error prone process. Every help that eases these burdens is saving developer
time that can then be invested into the advancement of the application itself
instead of preserving the status-quo on a new platform.
  The Alpaka library defines and implements an abstract hierarchical redundant
parallelism model. The model exploits parallelism and memory hierarchies on a
node at all levels available in current hardware. By doing so, it allows to
achieve platform and performance portability across various types of
accelerators by ignoring specific unsupported levels and utilizing only the
ones supported on a specific accelerator. All hardware types (multi- and
many-core CPUs, GPUs and other accelerators) are supported for and can be
programmed in the same way. The Alpaka C++ template interface allows for
straightforward extension of the library to support other accelerators and
specialization of its internals for optimization.
  Running Alpaka applications on a new (and supported) platform requires the
change of only one source code line instead of a lot of \#ifdefs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08481</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08481</id><created>2016-02-26</created><updated>2016-03-03</updated><authors><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author><author><keyname>Scquizzato</keyname><forenames>Michele</forenames></author></authors><title>Tight Bounds for Distributed Graph Computations</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the need to understand the algorithmic foundations of
distributed large-scale graph computations, we study some fundamental graph
problems in a message-passing model for distributed computing where $k \geq 2$
machines jointly perform computations on graphs with $n$ nodes (typically, $n
\gg k$). The input graph is assumed to be initially randomly partitioned among
the $k$ machines. Communication is point-to-point, and the goal is to minimize
the number of communication rounds of the computation.
  We present (almost) tight bounds for the round complexity of two fundamental
graph problems, namely PageRank computation and triangle enumeration. Our tight
lower bounds, a main contribution of the paper, are established through an
information-theoretic approach that relates the round complexity to the minimal
amount of information required by machines for correctly solving a problem. Our
approach is generic and might be useful in showing lower bounds in the context
of similar problems and similar models.
  We show a lower bound of $\tilde{\Omega}(n/k^2)$ rounds for computing the
PageRank. (Notation $\tilde \Omega$ hides a ${1}/{\text{polylog}(n)}$ factor.)
We also present a simple distributed algorithm that computes the PageRank of
all the nodes of a graph in $\tilde{O}(n/k^2)$ rounds (notation $\tilde O$
hides a $\text{polylog}(n)$ factor and an additive $\text{polylog}(n)$ term).
  For triangle enumeration, we show a lower bound of
$\tilde{\Omega}(m/k^{5/3})$ rounds, where $m$ is the number of edges of the
graph. Our result implies a lower bound of $\tilde\Omega(n^{1/3})$ for the
congested clique, which is tight up to logarithmic factors. We also present a
distributed algorithm that enumerates all the triangles of a graph in
$\tilde{O}(m/k^{5/3})$ rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08486</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08486</id><created>2016-02-26</created><authors><author><keyname>Shan</keyname><forenames>Honghao</forenames></author><author><keyname>Tong</keyname><forenames>Matthew H.</forenames></author><author><keyname>Cottrell</keyname><forenames>Garrison W.</forenames></author></authors><title>A Single Model Explains both Visual and Auditory Precortical Coding</title><categories>q-bio.NC cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precortical neural systems encode information collected by the senses, but
the driving principles of the encoding used have remained a subject of debate.
We present a model of retinal coding that is based on three constraints:
information preservation, minimization of the neural wiring, and response
equalization. The resulting novel version of sparse principal components
analysis successfully captures a number of known characteristics of the retinal
coding system, such as center-surround receptive fields, color opponency
channels, and spatiotemporal responses that correspond to magnocellular and
parvocellular pathways. Furthermore, when trained on auditory data, the same
model learns receptive fields well fit by gammatone filters, commonly used to
model precortical auditory coding. This suggests that efficient coding may be a
unifying principle of precortical encoding across modalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08504</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08504</id><created>2016-02-26</created><authors><author><keyname>Isaev</keyname><forenames>Valery</forenames></author></authors><title>Algebraic Presentations of Dependent Type Theories</title><categories>math.LO cs.LO math.CT</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an abstract definition of dependent type theories
as essentially algebraic theories. One of the main advantages of this
definition is its composability: simple theories can be combined into more
complex ones, and different properties of the resulting theory may be deduced
from properties of the basic ones. We define a category of algebraic dependent
type theories which allows us not only to combine theories but also to consider
equivalences between them. We also study models of such theories and show that
one can think of them as contextual categories with additional structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08507</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08507</id><created>2016-02-23</created><authors><author><keyname>Huang</keyname><forenames>Qian</forenames></author><author><keyname>Ge</keyname><forenames>Zhenhao</forenames></author><author><keyname>Lu</keyname><forenames>Chao</forenames></author></authors><title>Occupancy Estimation in Smart Buildings using Audio-Processing
  Techniques</title><categories>cs.SD</categories><comments>International Conference on Computing in Civil and Building
  Engineering (ICCCBE) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years, several case studies have illustrated that the use of
occupancy information in buildings leads to energy-efficient and low-cost HVAC
operation. The widely presented techniques for occupancy estimation include
temperature, humidity, CO2 concentration, image camera, motion sensor and
passive infrared (PIR) sensor. So far little studies have been reported in
literature to utilize audio and speech processing as indoor occupancy
prediction technique. With rapid advances of audio and speech processing
technologies, nowadays it is more feasible and attractive to integrate
audio-based signal processing component into smart buildings. In this work, we
propose to utilize audio processing techniques (i.e., speaker recognition and
background audio energy estimation) to estimate room occupancy (i.e., the
number of people inside a room). Theoretical analysis and simulation results
demonstrate the accuracy and effectiveness of this proposed occupancy
estimation technique. Based on the occupancy estimation, smart buildings will
adjust the thermostat setups and HVAC operations, thus, achieving greater
quality of service and drastic cost savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08509</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08509</id><created>2016-02-26</created><updated>2016-03-01</updated><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author></authors><title>Estimating Distribution Grid Topologies: A Graphical Learning based
  Approach</title><categories>math.OC cs.SY</categories><comments>7 pages, 4 figures, A version of this paper will appear in PSCC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distribution grids represent the final tier in electric networks consisting
of medium and low voltage lines that connect the distribution substations to
the end-users. Traditionally, distribution networks have been operated in a
radial topology that may be changed from time to time. Due to absence of a
significant number of real-time line monitoring devices in the distribution
grid, estimation of the topology is a problem critical for its observability
and control. This paper develops a novel graphical learning based approach to
estimate the radial operational grid structure using voltage measurements
collected from the grid loads. The learning algorithm is based on conditional
independence tests for continuous variables over chordal graphs and has wide
applicability. It is proven that the scheme can be used for several power flow
laws (DC or AC approximations) and more importantly is independent of the
specific probability distribution controlling individual bus power usage. The
complexity of the algorithm is discussed and its performance is demonstrated by
simulations on distribution test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08510</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08510</id><created>2016-02-26</created><authors><author><keyname>Vaksman</keyname><forenames>Gregory</forenames></author><author><keyname>Zibulevsky</keyname><forenames>Michael</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Patch-Ordering as a Regularization for Inverse Problems in Image
  Processing</title><categories>cs.CV</categories><msc-class>62H35, 68U10, 94A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work in image processing suggests that operating on (overlapping)
patches in an image may lead to state-of-the-art results. This has been
demonstrated for a variety of problems including denoising, inpainting,
deblurring, and super-resolution. The work reported in [1,2] takes an extra
step forward by showing that ordering these patches to form an approximate
shortest path can be leveraged for better processing. The core idea is to apply
a simple filter on the resulting 1D smoothed signal obtained after the
patch-permutation. This idea has been also explored in combination with a
wavelet pyramid, leading eventually to a sophisticated and highly effective
regularizer for inverse problems in imaging. In this work we further study the
patch-permutation concept, and harness it to propose a new simple yet effective
regularization for image restoration problems. Our approach builds on the
classic Maximum A'posteriori probability (MAP), with a penalty function
consisting of a regular log-likelihood term and a novel permutation-based
regularization term. Using a plain 1D Laplacian, the proposed regularization
forces robust smoothness (L1) on the permuted pixels. Since the permutation
originates from patch-ordering, we propose to accumulate the smoothness terms
over all the patches' pixels. Furthermore, we take into account the found
distances between adjacent patches in the ordering, by weighting the Laplacian
outcome. We demonstrate the proposed scheme on a diverse set of problems: (i)
severe Poisson image denoising, (ii) Gaussian image denoising, (iii) image
deblurring, and (iv) single image super-resolution. In all these cases, we use
recent methods that handle these problems as initialization to our scheme. This
is followed by an L-BFGS optimization of the above-described penalty function,
leading to state-of-the-art results, and especially so for highly ill-posed
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08515</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08515</id><created>2016-02-26</created><authors><author><keyname>Ahmed</keyname><forenames>Shabbir</forenames></author><author><keyname>He</keyname><forenames>Qie</forenames></author><author><keyname>Li</keyname><forenames>Shi</forenames></author><author><keyname>Nemhauser</keyname><forenames>George</forenames></author></authors><title>On the computational complexity of minimum-concave-cost flow in a
  two-dimensional grid</title><categories>cs.DS math.OC</categories><msc-class>90C27, 90C39, 90B10, 90B30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the minimum-concave-cost flow problem on a two-dimensional grid. We
characterize the computational complexity of this problem based on the number
of rows and columns of the grid, the number of different capacities over all
arcs, and the location of sources and sinks. The concave cost over each arc is
assumed to be evaluated through an oracle machine, i.e., the oracle machine
returns the cost over an arc in a single computational step, given the flow
value and the arc index. We propose an algorithm whose running time is
polynomial in the number of columns of the grid, for the following cases: (1)
the grid has a constant number of rows, a constant number of different
capacities over all arcs, and sources and sinks in at most two rows; (2) the
grid has two rows and a constant number of different capacities over all arcs
connecting rows; (3) the grid has a constant number of rows and all sources in
one row, with infinite capacity over each arc. These are presumably the most
general polynomially solvable cases, since we show the problem becomes NP-hard
when any condition in these cases is removed. Our results apply to abundant
variants and generalizations of the dynamic lot sizing model, and answer
several questions raised in serial supply chain optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08519</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08519</id><created>2016-02-22</created><authors><author><keyname>Hetterich</keyname><forenames>Samuel</forenames></author></authors><title>Analysing Survey Propagation Guided Decimation on Random Formulas</title><categories>cs.DS math.CO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1007.1328 by
  other authors</comments><msc-class>68R01</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\varPhi$ be a uniformly distributed random $k$-SAT formula with $n$
variables and $m$ clauses. For clauses/variables ratio $m/n \leq
r_{k\text{-SAT}} \sim 2^k\ln2$ the formula $\varPhi$ is satisfiable with high
probability. However, no efficient algorithm is known to provably find a
satisfying assignment beyond $m/n \sim 2k \ln(k)/k$ with a non-vanishing
probability. Non-rigorous statistical mechanics work on $k$-CNF led to the
development of a new efficient &quot;message passing algorithm&quot; called \emph{Survey
Propagation Guided Decimation} [M\'ezard et al., Science 2002]. Experiments
conducted for $k=3,4,5$ suggest that the algorithm finds satisfying assignments
close to $r_{k\text{-SAT}}$. However, in the present paper we prove that the
basic version of Survey Propagation Guided Decimation fails to solve random
$k$-SAT formulas efficiently already for $m/n=2^k(1+\varepsilon_k)\ln(k)/k$
with $\lim_{k\to\infty}\varepsilon_k= 0$ almost a factor $k$ below
$r_{k\text{-SAT}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08534</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08534</id><created>2016-02-26</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Wang</keyname><forenames>Ning</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay K.</forenames></author></authors><title>Analysis and Design of Secure Massive MIMO Systems in the Presence of
  Hardware Impairments</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To keep the hardware costs of future communications systems manageable, the
use of low-cost hardware components is desirable. This is particularly true for
the emerging massive multiple-input multiple-output (MIMO) systems which equip
base stations (BSs) with a large number of antenna elements. However, low-cost
transceiver designs will further accentuate the hardware impairments which are
present in any practical communication system. In this paper, we investigate
the impact of hardware impairments on the secrecy performance of downlink
massive MIMO systems in the presence of a passive multiple-antenna
eavesdropper. Thereby, for the BS and the legitimate users, the joint effects
of multiplicative phase noise, additive distortion noise, and amplified
receiver noise are taken into account, whereas the eavesdropper is assumed to
employ ideal hardware. We derive a lower bound for the ergodic secrecy rate of
a given user when matched filter (MF) data precoding and artificial noise (AN)
transmission are employed at the BS. Based on the derived analytical
expression, we investigate the impact of the various system parameters on the
secrecy rate and optimize both the pilot sets used for uplink training and the
AN precoding. Our analytical and simulation results reveal that 1) the additive
distortion noise at the BS may be beneficial for the secrecy performance,
especially if the power assigned for AN emission is not sufficient; 2) all
other hardware impairments have a negative impact on the secrecy performance;
3) so-called spatially orthogonal pilot sequences are preferable unless the
phase noise is very strong; 4) the proposed generalized null-space (NS) AN
precoding method can efficiently mitigate the negative effects of phase noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08539</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08539</id><created>2016-02-26</created><authors><author><keyname>Delfanti</keyname><forenames>Alessandro</forenames></author></authors><title>Beams of Particles and Papers. The Role of Preprint Archives in High
  Energy Physics</title><categories>physics.soc-ph cs.DL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In high energy physics scholarly papers circulate primarily through online
preprint archives based on a centralized repository, arXiv.org, that physicists
simply refer to as 'the archive.' This is not a tool for preservation and
memory, but rather a space of flows where written objects are detected and then
disappear, and their authors made available for scrutiny. In this work I
analyse the reading and publishing practices of two subsets of particle
physicists, theorists and experimentalists. In order to be recognized as
legitimate and productive members of their community, physicists need to abide
by the temporalities and authorial practices structured by the archive.
Theorists live in a state of accelerated time that shapes their reading and
publishing practices around a 24 hour cycle. Experimentalists resolve to
tactics that allow them to circumvent the slowed-down time and invisibility
they experience as members of large collaborations. As digital archives for the
exchange of preprint articles emerge in other scientific fields, physics could
help shed light on general transformations of contemporary scholarly
communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08540</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08540</id><created>2016-02-26</created><authors><author><keyname>Wu</keyname><forenames>Xiugang</forenames></author><author><keyname>Ozgur</keyname><forenames>Ayfer</forenames></author><author><keyname>Xie</keyname><forenames>Liang-Liang</forenames></author></authors><title>Improving on the Cut-Set Bound via Geometric Analysis of Typical Sets</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the discrete memoryless symmetric primitive relay channel, where,
a source $X$ wants to send information to a destination $Y$ with the help of a
relay $Z$ and the relay can communicate to the destination via an error-free
digital link of rate $R_0$, while $Y$ and $Z$ are conditionally independent and
identically distributed given $X$. We develop two new upper bounds on the
capacity of this channel that are tighter than existing bounds, including the
celebrated cut-set bound. Our approach significantly deviates from the standard
information-theoretic approach for proving upper bounds on the capacity of
multi-user channels. We build on the blowing-up lemma to analyze the
probabilistic geometric relations between the typical sets of the $n$-letter
random variables associated with a reliable code for communicating over this
channel. These relations translate to new entropy inequalities between the
$n$-letter random variables involved.
  As an application of our bounds, we study an open question posed by (Cover,
1987), namely, what is the minimum needed $Z$-$Y$ link rate $R_0^*$ in order
for the capacity of the relay channel to be equal to that of the broadcast cut.
We consider the special case when the $X$-$Y$ and $X$-$Z$ links are both binary
symmetric channels. Our tighter bounds on the capacity of the relay channel
immediately translate to tighter lower bounds for $R_0^*$. More interestingly,
we show that when $p\to 1/2$, $R_0^*\geq 0.1803$; even though the broadcast
channel becomes completely noisy as $p\to 1/2$ and its capacity, and therefore
the capacity of the relay channel, goes to zero, a strictly positive rate $R_0$
is required for the relay channel capacity to be equal to the broadcast bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08549</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08549</id><created>2016-02-26</created><authors><author><keyname>Otmani</keyname><forenames>Ayoub</forenames></author><author><keyname>Kalachi</keyname><forenames>Herv&#xe9; Tal&#xe9;</forenames></author><author><keyname>Ndjeya</keyname><forenames>S&#xe9;lestin</forenames></author></authors><title>Improved Cryptanalysis of Rank Metric Schemes Based on Gabidulin Codes</title><categories>cs.CR cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that any variant of the GPT cryptosystem which uses a right column
scrambler over the extension field as advocated by the works of Gabidulin et
al. with the goal to resist to Overbeck's structural attack are actually still
vulnerable to that attack. We show that by applying the Frobenius operator
appropriately on the public key, it is possible to build a Gabidulin code
having the same dimension as the original secret Gabidulin code but with a
lower length. In particular, the code obtained by this way correct less errors
than the secret one but its error correction capabilities are beyond the number
of errors added by a sender, and consequently an attacker is able to decrypt
any ciphertext with this degraded Gabidulin code. We also considered the case
where an isometric transformation is applied in conjunction with a right column
scrambler which has its entries in the extension field. We proved that this
protection is useless both in terms of performance and security. Consequently,
our results show that all the existing techniques aiming to hide the inherent
algebraic structure of Gabidulin codes have failed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08556</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08556</id><created>2016-02-27</created><authors><author><keyname>Srinivasan</keyname><forenames>Gopalakrishnan</forenames></author><author><keyname>Wijesinghe</keyname><forenames>Parami</forenames></author><author><keyname>Sarwar</keyname><forenames>Syed Shakib</forenames></author><author><keyname>Jaiswal</keyname><forenames>Akhilesh</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Significance Driven Hybrid 8T-6T SRAM for Energy-Efficient Synaptic
  Storage in Artificial Neural Networks</title><categories>cs.NE</categories><comments>Accepted in Design, Automation and Test in Europe 2016 conference
  (DATE-2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilayered artificial neural networks (ANN) have found widespread utility
in classification and recognition applications. The scale and complexity of
such networks together with the inadequacies of general purpose computing
platforms have led to a significant interest in the development of efficient
hardware implementations. In this work, we focus on designing energy efficient
on-chip storage for the synaptic weights. In order to minimize the power
consumption of typical digital CMOS implementations of such large-scale
networks, the digital neurons could be operated reliably at scaled voltages by
reducing the clock frequency. On the contrary, the on-chip synaptic storage
designed using a conventional 6T SRAM is susceptible to bitcell failures at
reduced voltages. However, the intrinsic error resiliency of NNs to small
synaptic weight perturbations enables us to scale the operating voltage of the
6TSRAM. Our analysis on a widely used digit recognition dataset indicates that
the voltage can be scaled by 200mV from the nominal operating voltage (950mV)
for practically no loss (less than 0.5%) in accuracy (22nm predictive
technology). Scaling beyond that causes substantial performance degradation
owing to increased probability of failures in the MSBs of the synaptic weights.
We, therefore propose a significance driven hybrid 8T-6T SRAM, wherein the
sensitive MSBs are stored in 8T bitcells that are robust at scaled voltages due
to decoupled read and write paths. In an effort to further minimize the area
penalty, we present a synaptic-sensitivity driven hybrid memory architecture
consisting of multiple 8T-6T SRAM banks. Our circuit to system-level simulation
framework shows that the proposed synaptic-sensitivity driven architecture
provides a 30.91% reduction in the memory access power with a 10.41% area
overhead, for less than 1% loss in the classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08557</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08557</id><created>2016-02-27</created><authors><author><keyname>Sarwar</keyname><forenames>Syed Shakib</forenames></author><author><keyname>Venkataramani</keyname><forenames>Swagath</forenames></author><author><keyname>Raghunathan</keyname><forenames>Anand</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Multiplier-less Artificial Neurons Exploiting Error Resiliency for
  Energy-Efficient Neural Computing</title><categories>cs.NE</categories><comments>Accepted in Design, Automation and Test in Europe 2016 conference
  (DATE-2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale artificial neural networks have shown significant promise in
addressing a wide range of classification and recognition applications.
However, their large computational requirements stretch the capabilities of
computing platforms. The fundamental components of these neural networks are
the neurons and its synapses. The core of a digital hardware neuron consists of
multiplier, accumulator and activation function. Multipliers consume most of
the processing energy in the digital neurons, and thereby in the hardware
implementations of artificial neural networks. We propose an approximate
multiplier that utilizes the notion of computation sharing and exploits error
resilience of neural network applications to achieve improved energy
consumption. We also propose Multiplier-less Artificial Neuron (MAN) for even
larger improvement in energy consumption and adapt the training process to
ensure minimal degradation in accuracy. We evaluated the proposed design on 5
recognition applications. The results show, 35% and 60% reduction in energy
consumption, for neuron sizes of 8 bits and 12 bits, respectively, with a
maximum of ~2.83% loss in network accuracy, compared to a conventional neuron
implementation. We also achieve 37% and 62% reduction in area for a neuron size
of 8 bits and 12 bits, respectively, under iso-speed conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08563</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08563</id><created>2016-02-27</created><authors><author><keyname>Bienkowski</keyname><forenames>Marcin</forenames></author><author><keyname>Marcinkowski</keyname><forenames>Jan</forenames></author><author><keyname>Pacut</keyname><forenames>Maciej</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author><author><keyname>Spyra</keyname><forenames>Aleksandra</forenames></author></authors><title>Online Tree Caching</title><categories>cs.DS</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a natural and practically relevant new variant of online caching
with bypassing where the to-be-cached items can have dependencies. We assume
that the universe is a tree T and items are tree nodes; we require that if a
node v is cached then T(v) is cached as well. This theoretical problem finds an
immediate application in the context of forwarding table optimization in IP
routing and software-defined networks. We present an O(height(T)
kONL/(kONL-kOPT+ 1))-competitive deterministic algorithm TC, where kONL and
kOPT denote the cache sizes of an online and the optimal offline algorithm,
respectively. The result is optimal up to a factor of O(height(T)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08565</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08565</id><created>2016-02-27</created><authors><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Jecker</keyname><forenames>Isma&#xeb;l</forenames></author><author><keyname>L&#xf6;ding</keyname><forenames>Christof</forenames></author><author><keyname>Winter</keyname><forenames>Sarah</forenames></author></authors><title>On Equivalence and Uniformisation Problems for Finite Transducers</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transductions are binary relations of finite words. For rational
transductions, i.e., transductions defined by finite transducers, the
inclusion, equivalence and sequential uniformisation problems are known to be
undecidable. In this paper, we investigate stronger variants of inclusion,
equivalence and sequential uniformisation, based on a general notion of
transducer resynchronisation, and show their decidability. We also investigate
the classes of finite-valued rational transductions and deterministic rational
transductions, which are known to have a decidable equivalence problem. We show
that sequential uniformisation is also decidable for them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08567</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08567</id><created>2016-02-27</created><authors><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Hui-Ming</forenames></author></authors><title>Opportunistic Jamming for Enhancing Security: Stochastic Geometry
  Modeling and Analysis</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Vehicular Technology, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This correspondence studies the secrecy communication of the single-input
single-output multi-eavesdropper (SISOME) channel with multiple single-antenna
jammers, where the jammers and eavesdroppers are distributed according to the
independent two-dimensional homogeneous Poisson point process (PPP). For
enhancing the physical layer security, we propose an opportunistic multiple
jammer selection scheme, where the jammers whose channel gains to the
legitimate receiver less than a threshold, are selected to transmit independent
and identically distributed (\emph{i.i.d.}) Gaussian jamming signals to
confound the eavesdroppers. We characterize the secrecy throughput achieved by
our proposed jammer selection scheme, and show that the secrecy throughput is a
quasi-concave function of the selection threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08571</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08571</id><created>2016-02-27</created><authors><author><keyname>Zhang</keyname><forenames>Haoxi</forenames></author><author><keyname>Sanin</keyname><forenames>Cesar</forenames></author><author><keyname>Szczerbicki</keyname><forenames>Edward</forenames></author></authors><title>Towards Neural Knowledge DNA</title><categories>cs.AI cs.NE</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the Neural Knowledge DNA, a framework that tailors
the ideas underlying the success of neural networks to the scope of knowledge
representation. Knowledge representation is a fundamental field that dedicate
to representing information about the world in a form that computer systems can
utilize to solve complex tasks. The proposed Neural Knowledge DNA is designed
to support discovering, storing, reusing, improving, and sharing knowledge
among machines and organisation. It is constructed in a similar fashion of how
DNA formed: built up by four essential elements. As the DNA produces
phenotypes, the Neural Knowledge DNA carries information and knowledge via its
four essential elements, namely, Networks, Experiences, States, and Actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08574</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08574</id><created>2016-02-27</created><authors><author><keyname>Calatroni</keyname><forenames>Luca</forenames></author><author><keyname>van Gennip</keyname><forenames>Yves</forenames></author><author><keyname>Sch&#xf6;nlieb</keyname><forenames>Carola-Bibiane</forenames></author><author><keyname>Rowland</keyname><forenames>Hannah</forenames></author><author><keyname>Flenner</keyname><forenames>Arjuna</forenames></author></authors><title>Graph clustering, variational image segmentation methods and Hough
  transform scale detection for object measurement in images</title><categories>math.AP cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of scale detection in images where a region of
interest is present together with a measurement tool (e.g. a ruler). For the
segmentation part, we focus on the graph based method by Flenner and Bertozzi
which reinterprets classical continuous Ginzburg-Landau minimisation models in
a totally discrete framework. To overcome the numerical difficulties due to the
large size of the images considered we use matrix completion and splitting
techniques. The scale on the measurement tool is detected via a Hough transform
based algorithm. The method is then applied to some measurement tasks arising
in real-world applications such as zoology, medicine and archaeology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08575</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08575</id><created>2016-02-27</created><authors><author><keyname>Czaja</keyname><forenames>Wojciech</forenames></author><author><keyname>Murphy</keyname><forenames>James M.</forenames></author><author><keyname>Weinberg</keyname><forenames>Daniel</forenames></author></authors><title>Single-Image Superresolution Through Directional Representations</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a mathematically-motivated algorithm for image superresolution,
based on the discrete shearlet transform. The shearlet transform is strongly
directional, and is known to provide near-optimally sparse representations for
a broad class of images. This often leads to superior performance in edge
detection and image representation, when compared to other isotropic frames. We
justify the use of shearlet frames for superresolution mathematically before
presenting a superresolution algorithm that combines the shearlet transform
with the sparse mixing estimators (SME) approach pioneered by Mallat and Yu.
Our algorithm is compared with an isotropic superresolution method, a previous
prototype of a shearlet superresolution algorithm, and SME superresolution with
a discrete wavelet frame. Our numerical results on a variety of image types
show strong performance in terms of PSNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08579</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08579</id><created>2016-02-27</created><authors><author><keyname>Bosma</keyname><forenames>Wieb</forenames></author><author><keyname>Fokkink</keyname><forenames>Robbert</forenames></author><author><keyname>Krebs</keyname><forenames>Thijmen</forenames></author></authors><title>On automatic subsets of the Gaussian integers</title><categories>cs.FL</categories><msc-class>11B85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that $a$ and $b$ are multiplicatively independent Gaussian integers,
that are both of modulus~$\geq \sqrt 5$. We prove that there exist a $X\subset
\mathbb Z[i]$ which is $a$-automatic but not $b$-automatic. This settles a
problem of Allouche, Cateland, Gilbert, Peitgen, Shallit, and Skordev.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08581</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08581</id><created>2016-02-27</created><authors><author><keyname>Iyer</keyname><forenames>Rahul Radhakrishnan</forenames></author><author><keyname>Parekh</keyname><forenames>Sanjeel</forenames></author><author><keyname>Mohandoss</keyname><forenames>Vikas</forenames></author><author><keyname>Ramsurat</keyname><forenames>Anush</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author><author><keyname>Singh</keyname><forenames>Rita</forenames></author></authors><title>Content-based Video Indexing and Retrieval Using Corr-LDA</title><categories>cs.IR cs.CV</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing video indexing and retrieval methods on popular web-based multimedia
sharing websites are based on user-provided sparse tagging. This paper proposes
a very specific way of searching for video clips, based on the content of the
video. We present our work on Content-based Video Indexing and Retrieval using
the Correspondence-Latent Dirichlet Allocation (corr-LDA) probabilistic
framework. This is a model that provides for auto-annotation of videos in a
database with textual descriptors, and brings the added benefit of utilizing
the semantic relations between the content of the video and text. We use the
concept-level matching provided by corr-LDA to build correspondences between
text and multimedia, with the objective of retrieving content with increased
accuracy. In our experiments, we employ only the audio components of the
individual recordings and compare our results with an SVM-based approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08591</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08591</id><created>2016-02-27</created><authors><author><keyname>Hahm</keyname><forenames>Oliver</forenames></author><author><keyname>Adjih</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Baccelli</keyname><forenames>Emmanuel</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author></authors><title>A Case for Time Slotted Channel Hopping for ICN in the IoT</title><categories>cs.NI</categories><proxy>Matthias W&#xc3;&#x83;&#xc2;&#xa4;hlisch</proxy><acm-class>C.2.2; C.4; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent proposals to simplify the operation of the IoT include the use of
Information Centric Networking (ICN) par\-adigms. While this is promising,
several challenges remain. In this paper, our core contributions (a) leverage
ICN communication patterns to dynamically optimize the use of TSCH (Time
Slotted Channel Hopping), a wireless link layer technology increasingly popular
in the IoT, and (b) make IoT-style routing adaptive to names, resources, and
traffic patterns throughout the network---both without cross-layering. Through
a series of experiments on the FIT IoT-LAB interconnecting typical IoT
hardware, we find that our approach is fully robust against wireless
interference, and almost halves the energy consumed for transmission when
compared to CSMA. Most importantly, our adaptive scheduling prevents the
time-slotted MAC layer from sacrificing throughput and delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08609</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08609</id><created>2016-02-27</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Collings</keyname><forenames>Iain B.</forenames></author></authors><title>A New Robust Frequency Domain Echo Canceller With Closed-Loop Learning
  Rate Adaptation</title><categories>cs.SD cs.SY</categories><comments>4 pages, Proc. International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP), 2007</comments><doi>10.1109/ICASSP.2007.366624</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main difficulties in echo cancellation is the fact that the
learning rate needs to vary according to conditions such as double-talk and
echo path change. Several methods have been proposed to vary the learning. In
this paper we propose a new closed-loop method where the learning rate is
proportional to a misalignment parameter, which is in turn estimated based on a
gradient adaptive approach. The method is presented in the context of a
multidelay block frequency domain (MDF) echo canceller. We demonstrate that the
proposed algorithm outperforms current popular double-talk detection techniques
by up to 6 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08610</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08610</id><created>2016-02-27</created><authors><author><keyname>Yang</keyname><forenames>Hongyu</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author><author><keyname>Seltzer</keyname><forenames>Margo</forenames></author></authors><title>Scalable Bayesian Rule Lists</title><categories>cs.AI</categories><comments>31 pages, 18 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present an algorithm for building rule lists that is two orders of
magnitude faster than previous work. Rule list algorithms are competitors for
decision tree algorithms. They are associative classifiers, in that they are
built from pre-mined association rules. They have a logical structure that is a
sequence of IF-THEN rules, identical to a decision list or one-sided decision
tree. Instead of using greedy splitting and pruning like decision tree
algorithms, we fully optimize over rule lists, striking a practical balance
between accuracy, interpretability, and computational speed. The algorithm
presented here uses a mixture of theoretical bounds (tight enough to have
practical implications as a screening or bounding procedure), computational
reuse, and highly tuned language libraries to achieve computational efficiency.
Currently, for many practical problems, this method achieves better accuracy
and sparsity than decision trees; further, in many cases, the computational
time is practical and often less than that of decision trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08614</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08614</id><created>2016-02-27</created><authors><author><keyname>Li</keyname><forenames>Qiuwei</forenames></author><author><keyname>Prater</keyname><forenames>Ashley</forenames></author><author><keyname>Shen</keyname><forenames>Lixin</forenames></author><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author></authors><title>Overcomplete Tensor Decomposition via Convex Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensors provide natural representations for massive multi-mode datasets and
tensor methods also form the backbone of many machine learning, signal
processing, and statistical algorithms. The utility of tensors is mainly due to
the ability to identify overcomplete, non-orthogonal factors from tensor data,
which is known as tensor decomposition. This work develops theories and
computational methods for guaranteed overcomplete, non-orthogonal tensor
decomposition using convex optimization. We consider tensor decomposition as a
problem of measure estimation from moments. We develop the theory for
guaranteed decomposition under three assumptions: (i) Incoherence; (ii) Bounded
spectral norm; and (iii) Gram isometry. We show that under these three
assumptions, one can retrieve tensor decomposition by solving a convex,
infinite-dimensional analog of $\ell_1$ minimization on the space of measures.
The optimal value of this optimization defines the tensor nuclear norm that can
be used to regularize tensor inverse problems, including tensor completion,
denoising, and robust tensor principal component analysis. Remarkably, all the
three assumptions are satisfied with high probability if the rank-one tensor
factors are uniformly distributed on the unit spheres, implying exact
decomposition for tensors with random factors. We also present and numerically
test two computational methods based respectively on Burer-Monteiro low-rank
factorization reformulation and the Sum-of-Squares relaxations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08615</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08615</id><created>2016-02-27</created><authors><author><keyname>Yajnanarayana</keyname><forenames>Vijaya</forenames></author><author><keyname>Handel</keyname><forenames>Peter</forenames></author></authors><title>Compressive Sampling Based UWB TOA Estimator</title><categories>cs.IT math.IT</categories><comments>15 Pages, EURASIP WCN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes two compressive sampling based time of arrival (TOA)
estimation algorithms using a sub-Nyquist rate receiver. We also describe a
novel compressive sampling dictionary design for the compact representation of
the received UWB signal. One of the proposed algorithm exploits the a-priori
information with regard to the channel and range of the target. The performance
of the algorithms are compared against the matched filter based high sampling
rate receiver using IEEE 802.15.4a CM1 multipath channel model. The proposed
algorithm yields the same performance as the TOA estimation algorithm employing
maximal energy selection (MES) criteria for the matched filter output with only
1/4-th the sampling rate at 24 dB SNR. We also analyze the performance of the
algorithm with respect to practical constraints like size of the holographic
dictionary and sampling rates. We also demonstrate that when the a-priori
information regarding the UWB channel and the geographical constraints on the
target are available at the receiver, a substantial boost in the performance at
low SNRs can be accomplished using the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08620</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08620</id><created>2016-02-27</created><authors><author><keyname>Mull</keyname><forenames>Nathan</forenames></author><author><keyname>Fremont</keyname><forenames>Daniel J.</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>On the Hardness of SAT with Community Structure</title><categories>cs.LO cs.CC</categories><comments>22 pages</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent attempts to explain the effectiveness of Boolean satisfiability (SAT)
solvers based on conflict-driven clause learning (CDCL) on large industrial
benchmarks have focused on the concept of community structure. Specifically,
industrial benchmarks have been empirically found to have good community
structure, and experiments seem to show a correlation between such structure
and the efficiency of CDCL. However, in this paper we establish hardness
results suggesting that community structure is not sufficient to explain the
success of CDCL in practice. First, we formally characterize a property shared
by a wide class of metrics capturing community structure, including
&quot;modularity&quot;. Next, we show that the SAT instances with good community
structure according to any metric with this property are still NP-hard.
Finally, we also prove that with high probability, random unsatisfiable modular
instances generated from the &quot;pseudo-industrial&quot; community attachment model of
Gir\'aldez-Cru and Levy have exponentially long resolution proofs. Such
instances are therefore hard for CDCL on average, indicating that actual
industrial instances easily solved by CDCL may have some other relevant
structure not captured by this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08629</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08629</id><created>2016-02-27</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Michaud</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Hadjou</keyname><forenames>Brahim</forenames></author><author><keyname>Rouat</keyname><forenames>Jean</forenames></author></authors><title>Localization of Simultaneous Moving Sound Sources for Mobile Robot Using
  a Frequency-Domain Steered Beamformer Approach</title><categories>cs.RO cs.SD</categories><comments>6 pages. arXiv admin note: substantial text overlap with
  arXiv:1602.08139</comments><journal-ref>Proceedings of IEEE International Conference on Robotics and
  Automation (ICRA), pp. 1033-1038, 2004</journal-ref><doi>10.1109/ROBOT.2004.1307286</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile robots in real-life settings would benefit from being able to localize
sound sources. Such a capability can nicely complement vision to help localize
a person or an interesting event in the environment, and also to provide
enhanced processing for other capabilities such as speech recognition. In this
paper we present a robust sound source localization method in three-dimensional
space using an array of 8 microphones. The method is based on a
frequency-domain implementation of a steered beamformer along with a
probabilistic post-processor. Results show that a mobile robot can localize in
real time multiple moving sources of different types over a range of 5 meters
with a response time of 200 ms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08633</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08633</id><created>2016-02-27</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author></authors><title>Perceptually-Motivated Nonlinear Channel Decorrelation For Stereo
  Acoustic Echo Cancellation</title><categories>cs.SD</categories><comments>4 pages</comments><journal-ref>Proceedings of Joint Workshop on Hands-free Speech Communication
  and Microphone Arrays (HSCMA), pp. 188-191, 2008</journal-ref><doi>10.1109/HSCMA.2008.4538718</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acoustic echo cancellation with stereo signals is generally an
under-determined problem because of the high coherence between the left and
right channels. In this paper, we present a novel method of significantly
reducing inter-channel coherence without affecting the audio quality. Our work
takes into account psychoacoustic masking and binaural auditory cues. The
proposed non-linear processing combines a shaped comb-allpass (SCAL) filter
with the injection of psychoacoustically masked noise. We show that the
proposed method performs significantly better than other known methods for
reducing inter-channel coherence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08646</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08646</id><created>2016-02-27</created><authors><author><keyname>Aghamohammadi</keyname><forenames>Cina</forenames></author><author><keyname>Mahoney</keyname><forenames>John R.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>The Ambiguity of Simplicity</title><categories>quant-ph cond-mat.stat-mech cs.IT math.IT</categories><comments>7 pages, 6 figures, http://csc.ucdavis.edu/~cmg/compmech/pubs/aos.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system's apparent simplicity depends on whether it is represented
classically or quantally. This is not so surprising, as classical and quantum
physics are descriptive frameworks built on different assumptions that capture,
emphasize, and express different properties and mechanisms. What is surprising
is that, as we demonstrate, simplicity is ambiguous: the relative simplicity
between two systems can change sign when moving between classical and quantum
descriptions. Thus, notions of absolute physical simplicity---minimal structure
or memory---at best form a partial, not a total, order. This suggests that
appeals to principles of physical simplicity, via Ockham's Razor or to the
&quot;elegance&quot; of competing theories, may be fundamentally subjective, perhaps even
beyond the purview of physics itself. It also raises challenging questions in
model selection between classical and quantum descriptions. Fortunately,
experiments are now beginning to probe measures of simplicity, creating the
potential to directly test for ambiguity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08648</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08648</id><created>2016-02-27</created><authors><author><keyname>Yu</keyname><forenames>Y. William</forenames></author></authors><title>Approximation hardness of Shortest Common Superstring variants</title><categories>cs.CC q-bio.GN</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The shortest common superstring (SCS) problem has been studied at great
length because of its connections to the de novo assembly problem in
computational genomics. The base problem is APX-complete, but several
generalizations of the problem have also been studied. In particular, previous
results include that SCS with Negative strings (SCSN) is in Log-APX (though
there is no known hardness result) and SCS with Wildcards (SCSW) is
Poly-APX-hard. Here, we prove two new hardness results: (1) SCSN is
Log-APX-hard (and therefore Log-APX-complete) by a reduction from Minimum Set
Cover and (2) SCS with Negative strings and Wildcards (SCSNW) is NPOPB-hard by
a reduction from Minimum Ones 3SAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08653</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08653</id><created>2016-02-27</created><authors><author><keyname>Bento</keyname><forenames>Lucila M. S.</forenames></author><author><keyname>Boccardo</keyname><forenames>Davidson R.</forenames></author><author><keyname>Machado</keyname><forenames>Raphael C. S.</forenames></author><author><keyname>de S&#xe1;</keyname><forenames>Vin&#xed;cius G. Pereira</forenames></author><author><keyname>Szwarcfiter</keyname><forenames>Jayme L.</forenames></author></authors><title>Dijkstra Graphs</title><categories>cs.DM</categories><comments>16 pages, 7 figures</comments><msc-class>D.2.3, D.2.4, D.2.8, G.2.2</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit a concept that has been central in some early stages of computer
science, that of structured programming: a set of rules that an algorithm must
follow in order to acquire a structure that is desirable in many aspects. While
much has been written about structured programming, an important issue has been
left unanswered: given an arbitrary, compiled program, decide whether it is
structured, that is, whether it conforms to the stated principles of structured
programming. By employing graph-theoretic tools, we formulate an efficient
algorithm for answering this question. To do so, we first introduce the class
of graphs which correspond to structured programs, which we call Dijkstra
Graphs. Our problem then becomes the recognition of such graphs, for which we
present an $O(n^2)$-time algorithm. Furthermore, we describe an isomorphism
algorithm for Dijkstra graphs presenting the same quadratic complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08656</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08656</id><created>2016-02-27</created><authors><author><keyname>Morimae</keyname><forenames>Tomoyuki</forenames></author></authors><title>Quantum Arthur-Merlin with single-qubit measurements</title><categories>quant-ph cs.CC</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the class QAM does not change even if the verifier's ability is
restricted to only single-qubit measurements. To show the result, we use the
idea of the measurement-based quantum computing: the verifier, who can do only
single-qubit measurements, can test the graph state sent from the prover and
use it for his measurement-based quantum computing. We also introduce a new
QMA-complete problem related to the stabilizer test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08657</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08657</id><created>2016-02-27</created><authors><author><keyname>Herren</keyname><forenames>Luc</forenames></author></authors><title>QuotationFinder - Searching for Quotations and Allusions in Greek and
  Latin Texts and Establishing the Degree to Which a Quotation or Allusion
  Matches Its Source</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The software programs generally used with the TLG (Thesaurus Linguae Graecae)
and the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not well
suited for finding quotations and allusions. QuotationFinder uses more
sophisticated criteria as it ranks search results based on how closely they
match the source text, listing search results with literal quotations first and
loose verbal parallels last.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08658</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08658</id><created>2016-02-27</created><authors><author><keyname>Ali</keyname><forenames>Mohamad Jaafar</forenames></author><author><keyname>Moungla</keyname><forenames>Hassine</forenames></author><author><keyname>Mehaoua</keyname><forenames>Ahmed</forenames></author></authors><title>Interference Avoidance Algorithm (IAA) for Multi-hop Wireless Body Area
  Network Communication</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a distributed multi-hop interference avoidance
algorithm, namely, IAA to avoid co-channel interference inside a wireless body
area network (WBAN). Our proposal adopts carrier sense multiple access with
collision avoidance (CSMA/CA) between sources and relays and a flexible time
division multiple access (FTDMA) between relays and coordinator. The proposed
scheme enables low interfering nodes to transmit their messages using base
channel. Depending on suitable situations, high interfering nodes double their
contention windows (CW) and probably use switched orthogonal channel.
Simulation results show that proposed scheme has far better minimum SINR (12dB
improvement) and longer energy lifetime than other schemes (power control and
opportunistic relaying). Additionally, we validate our proposal in a
theoretical analysis and also propose a probabilistic approach to prove the
outage probability can be effectively reduced to the minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08662</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08662</id><created>2016-02-27</created><authors><author><keyname>Wang</keyname><forenames>Feng</forenames></author><author><keyname>Xu</keyname><forenames>Chongbin</forenames></author><author><keyname>Huang</keyname><forenames>Yongwei</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author></authors><title>LiST-BF Design: Achieving the SDP Bound for Downlink Beamforming with
  Arbitrary Shaping Constraints</title><categories>cs.IT math.IT</categories><comments>13 pages, 7 figures, Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the beamforming design for a multiuser multiple-input
single-output (MU-MISO) downlink with an arbitrary number of (context-specific)
shaping constraints. In this setup, the state-of-the-art beamforming schemes
cannot attain the well-known performance bound promised by the semidefinite
program (SDP) relaxation technique. To close the gap, we propose a linear
space-time beamforming (LiST-BF) scheme, consisting of a circulant space-time
symbol mapper followed by the beamforming design with orthogonality
constraints. It is shown that the proposed LiST-BF scheme can perform general
rank-$K$ beamforming for user symbols in a low-complexity and structured
manner. In addition, sufficient conditions are derived to guarantee that the
LiST-BF scheme always achieves the SDP bound for linear beamforming schemes.
Based on such conditions, an efficient algorithm is then developed to obtain
the optimal LiST-BF solution in polynomial time. Numerical results demonstrate
that the proposed scheme enjoys substantial performance gains over the existing
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08668</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08668</id><created>2016-02-27</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author></authors><title>Speex: A Free Codec For Free Speech</title><categories>cs.SD</categories><comments>Presented at linux.conf.au 2006, Dunedin. 8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Speex project has been started in 2002 to address the need for a free,
open-source speech codec. Speex is based on the Code Excited Linear Prediction
(CELP) algorithm and, unlike the previously existing Vorbis codec, is optimised
for transmitting speech for low latency communication over an unreliable packet
network. This paper presents an overview of Speex, the technology involved in
it and how it can be used in applications. The most recent developments in
Speex, such as the fixed-point port, acoustic echo cancellation and noise
suppression are also addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08671</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08671</id><created>2016-02-27</created><authors><author><keyname>Yang</keyname><forenames>Greg</forenames></author></authors><title>Lie Access Neural Turing Machine</title><categories>cs.NE cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Neural Turing Machine and Memory Networks have shown that adding an
external memory can greatly ameliorate a traditional recurrent neural network's
tendency to forget after a long period of time. Here we present a new design of
an external memory, wherein memories are stored in an Euclidean key space
$\mathbb R^n$. An LSTM controller performs read and write via specialized
structures called read and write heads, following the design of Neural Turing
Machine. It can move a head by either providing a new address in the key space
(aka random access) or moving from its previous position via a Lie group action
(aka Lie access). In this way, the &quot;L&quot; and &quot;R&quot; instructions of a traditional
Turing Machine is generalized to arbitrary elements of a fixed Lie group
action. For this reason, we name this new model the Lie Access Neural Turing
Machine, or LANTM.
  We tested two different configurations of LANTM against an LSTM baseline in
several basic experiments. As LANTM is differentiable end-to-end, training was
done with RMSProp. We found the right configuration of LANTM to be capable of
learning different permutation and arithmetic tasks and extrapolating to at
least twice the input size, all with the number of parameters 2 orders of
magnitude below that for the LSTM baseline. In particular, we trained LANTM on
addition of $k$-digit numbers for $2 \le k \le 16$, but it was able to
generalize almost perfectly to $17 \le k \le 32$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08675</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08675</id><created>2016-02-28</created><authors><author><keyname>Wang</keyname><forenames>Yafei</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Mitra</keyname><forenames>Prasenjit</forenames></author></authors><title>Quantified Self Meets Social Media: Sharing of Weight Updates on Twitter</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of people use wearables and other smart devices to
quantify various health conditions, ranging from sleep patterns, to body
weight, to heart rates. Of these Quantified Selfs many choose to openly share
their data via online social networks such as Twitter and Facebook. In this
study, we use data for users who have chosen to connect their smart scales to
Twitter, providing both a reliable time series of their body weight, as well as
insights into their social surroundings and general online behavior.
Concretely, we look at which social media features are predictive of physical
status, such as body weight at the individual level, and activity patterns at
the population level. We show that it is possible to predict an individual's
weight using their online social behaviors, such as their self-description and
tweets. Weekly and monthly patterns of quantified-self behaviors are also
discovered. These findings could contribute to building models to monitor
public health and to have more customized personal training interventions.
  While there are many studies using either quantified self or social media
data in isolation, this is one of the few that combines the two data sources
and, to the best of our knowledge, the only one that uses public data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08680</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08680</id><created>2016-02-28</created><authors><author><keyname>Li</keyname><forenames>Shangwen</forenames></author><author><keyname>Purushotham</keyname><forenames>Sanjay</forenames></author><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Ren</keyname><forenames>Yuzhuo</forenames></author><author><keyname>Kuo</keyname><forenames>C. -C. Jay</forenames></author></authors><title>Measuring and Predicting Tag Importance for Image Retrieval</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Textual data such as tags, sentence descriptions are combined with visual
cues to reduce the semantic gap for image retrieval applications in today's
Multimodal Image Retrieval (MIR) systems. However, all tags are treated as
equally important in these systems, which may result in misalignment between
visual and textual modalities during MIR training. This will further lead to
degenerated retrieval performance at query time. To address this issue, we
investigate the problem of tag importance prediction, where the goal is to
automatically predict the tag importance and use it in image retrieval. To
achieve this, we first propose a method to measure the relative importance of
object and scene tags from image sentence descriptions. Using this as the
ground truth, we present a tag importance prediction model by exploiting joint
visual, semantic and context cues. The Structural Support Vector Machine (SSVM)
formulation is adopted to ensure efficient training of the prediction model.
Then, the Kernel Canonical Correlation Analysis (KCCA) is employed to learn the
relation between the image visual feature and tag importance to obtain robust
image-to-image (I2I) retrieval performance. Experimental results on two
real-world datasets show a significant performance improvement of the proposed
MIR with Tag Importance Prediction (MIR/TIP) system over traditional MIR
systems that ignore tag importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08687</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08687</id><created>2016-02-28</created><authors><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author><author><keyname>Talmon</keyname><forenames>Nimrod</forenames></author></authors><title>Multiwinner Analogues of Plurality Rule: Axiomatic and Algorithmic
  Perspectives</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the class of committee scoring rules that satisfy the
fixed-majority criterion. In some sense, the committee scoring rules in this
class are multiwinner analogues of the single-winner Plurality rule, which is
uniquely characterized as the only single-winner scoring rule that satisfies
the simple majority criterion. We define top-$k$-counting committee scoring
rules and show that the fixed majority consistent rules are a subclass of the
top-$k$-counting rules. We give necessary and sufficient conditions for a
top-$k$-counting rule to satisfy the fixed-majority criterion. We find that,
for most of the rules in our new class, the complexity of winner determination
is high (that is, the problem of computing the winners is NP-hard), but we also
show examples of rules with polynomial-time winner determination procedures.
For some of the computationally hard rules, we provide either exact FPT
algorithms or approximate polynomial-time algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08710</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08710</id><created>2016-02-28</created><authors><author><keyname>Ali</keyname><forenames>Mohamad Jaafar</forenames></author><author><keyname>Moungla</keyname><forenames>Hassine</forenames></author><author><keyname>Mehaoua</keyname><forenames>Ahmed</forenames></author></authors><title>Dynamic Channel Access Scheme for Interference Mitigation in
  Relay-assisted Intra-WBANs</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses problems related to interference mitigation in a single
wireless body area network (WBAN). In this paper, We propose a distributed
\textit{C}ombined carrier sense multiple access with collision avoidance
(CSMA/CA) with \textit{F}lexible time division multiple access (\textit{T}DMA)
scheme for \textit{I}nterference \textit{M}itigation in relay-assisted
intra-WBAN, namely, CFTIM. In CFTIM scheme, non interfering sources
(transmitters) use CSMA/CA to communicate with relays. Whilst, high interfering
sources and best relays use flexible TDMA to communicate with coordinator (C)
through using stable channels. Simulation results of the proposed scheme are
compared to other schemes and consequently CFTIM scheme outperforms in all
cases. These results prove that the proposed scheme mitigates interference,
extends WBAN energy lifetime and improves the throughput. To further reduce the
interference level, we analytically show that the outage probability can be
effectively reduced to the minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08714</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08714</id><created>2016-02-28</created><authors><author><keyname>Aguerri</keyname><forenames>I&#xf1;aki Estella</forenames></author><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames></author></authors><title>Lossy Compression for Compute-and-Forward in Limited Backhaul Uplink
  Multicell Processing</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the transmission over a cloud radio access network in which multiple
base stations (BS) are connected to a central processor (CP) via
finite-capacity backhaul links. We propose two lattice-based coding schemes. In
the first scheme, the base stations decode linear combinations of the
transmitted messages, in the spirit of compute-and-forward (CoF), but differs
from it essentially in that the decoded equations are remapped to linear
combinations of the channel input symbols, sent compressed in a lossy manner to
the central processor, and are not required to be linearly independent. Also,
by opposition to the standard CoF, an appropriate multi-user decoder is
utilized to recover the sent messages. The second coding scheme generalizes the
first one by also allowing, at each relay node, a joint compression of the
decoded equation and the received signal. Both schemes apply in general, but
are more suited for situations in which there are more users than base
stations. We show that both schemes can outperform standard CoF and successive
Wyner-Ziv schemes in certain regimes, and illustrate the gains through some
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08715</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08715</id><created>2016-02-28</created><authors><author><keyname>Shmidman</keyname><forenames>Avi</forenames></author><author><keyname>Koppel</keyname><forenames>Moshe</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author></authors><title>Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus</title><categories>cs.CL</categories><comments>Submission to the Journal of Data Mining and Digital Humanities
  (Special Issue on Computer-Aided Processing of Intertextuality in Ancient
  Languages)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for efficiently finding all parallel passages in a large
corpus, even if the passages are not quite identical due to rephrasing and
orthographic variation. The key ideas are the representation of each word in
the corpus by its two most infrequent letters, finding matched pairs of strings
of four or five words that differ by at most one word and then identifying
clusters of such matched pairs. Using this method, over 4600 parallel pairs of
passages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of
over 1.8 million words, in just over 30 seconds. Empirical comparisons on
sample data indicate that the coverage obtained by our method is essentially
the same as that obtained using slow exhaustive methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08719</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08719</id><created>2016-02-28</created><authors><author><keyname>Br&#xe2;nzei</keyname><forenames>Simina</forenames></author><author><keyname>Filos-Ratsikas</keyname><forenames>Aris</forenames></author><author><keyname>Miltersen</keyname><forenames>Peter Bro</forenames></author><author><keyname>Zeng</keyname><forenames>Yulong</forenames></author></authors><title>Envy-Free Pricing in Multi-unit Markets</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the envy-free pricing problem in multi-unit markets with budgets,
where there is a seller who brings multiple units of an item, while several
buyers bring monetary endowments (budgets). Our goal is to compute an envy-free
(item) price and allocation---i.e. an outcome where all the demands of the
buyers are met given their budget constraints---which additionally achieves a
desirable objective. We analyze markets with linear valuations, where the
buyers are price takers and price makers, respectively. For the price taking
scenario, we provide a polynomial time algorithm for computing the welfare
maximizing envy-free pricing, followed by an FPTAS and exact algorithm---that
is polynomial for a constant number of types of buyers---for computing a
revenue maximizing envy-free pricing.
  In the price taking model, where the buyers can strategize, we show a general
impossibility of designing strategyproof and efficient mechanisms even with
public budgets. On the positive side, we provide an optimal strategyproof
mechanism for common budgets that simultaneously approximates the maximal
revenue and welfare within a factor of 2 on all markets except monopsonies.
  Finally, for markets with general valuations in the price taking scenario, we
provide hardness results for computing envy-free pricing schemes that maximize
revenue and welfare, as well as fully polynomial time approximation schemes for
both problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08721</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08721</id><created>2016-02-28</created><authors><author><keyname>Kalinsky</keyname><forenames>Oren</forenames></author><author><keyname>Etsion</keyname><forenames>Yoav</forenames></author><author><keyname>Kimelfeld</keyname><forenames>Benny</forenames></author></authors><title>Flexible Caching in Trie Joins</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional algorithms for multiway join computation are based on rewriting
the order of joins and combining results of intermediate subqueries. Recently,
several approaches have been proposed for algorithms that are &quot;worst-case
optimal&quot; wherein all relations are scanned simultaneously. An example is
Veldhuizen's Leapfrog Trie Join (LFTJ). An important advantage of LFTJ is its
small memory footprint, due to the fact that intermediate results are full
tuples that can be dumped immediately. However, since the algorithm does not
store intermediate results, recurring joins must be reconstructed from the
source relations, resulting in excessive memory traffic. In this paper, we
address this problem by incorporating caches into LFTJ. We do so by adopting
recent developments on join optimization, tying variable ordering to tree
decomposition. While the traditional usage of tree decomposition computes the
result for each bag in advance, our proposed approach incorporates caching
directly into LFTJ and can dynamically adjust the size of the cache.
Consequently, our solution balances memory usage and repeated computation, as
confirmed by our experiments over SNAP datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08728</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08728</id><created>2016-02-28</created><authors><author><keyname>Peng</keyname><forenames>Xi</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Song</keyname><forenames>S. H.</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Cache Size Allocation in Backhaul Limited Wireless Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, accepted to IEEE Int. Conf. Commun. (ICC), Kuala
  Lumpur, Malaysia, May 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching popular content at base stations is a powerful supplement to existing
limited backhaul links for accommodating the exponentially increasing mobile
data traffic. Given the limited cache budget, we investigate the cache size
allocation problem in cellular networks to maximize the user success
probability (USP), taking wireless channel statistics, backhaul capacities and
file popularity distributions into consideration. The USP is defined as the
probability that one user can successfully download its requested file either
from the local cache or via the backhaul link. We first consider a single-cell
scenario and derive a closed-form expression for the USP, which helps reveal
the impacts of various parameters, such as the file popularity distribution.
More specifically, for a highly concentrated file popularity distribution, the
required cache size is independent of the total number of files, while for a
less concentrated file popularity distribution, the required cache size is in
linear relation to the total number of files. Furthermore, we study the
multi-cell scenario, and provide a bisection search algorithm to find the
optimal cache size allocation. The optimal cache size allocation is verified by
simulations, and it is shown to play a more significant role when the file
popularity distribution is less concentrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08730</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08730</id><created>2016-02-28</created><authors><author><keyname>Harris</keyname><forenames>David G.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author></authors><title>Improved bounds and algorithms for graph cuts and network reliability</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Karger (SIAM Journal on Computing, 1999) developed the first fully-polynomial
approximation scheme to estimate the probability that a graph $G$ becomes
disconnected, given that its edges are removed independently with probability
$p$. This algorithm runs in $n^{5+o(1)} \epsilon^{-3}$ time to obtain an
estimate within relative error $\epsilon$.
  We improve this run-time through algorithmic and graph-theoretic advances.
First, there is a certain key sub-problem encountered by Karger, for which a
generic estimation procedure is employed, we show that this has a special
structure for which a much more efficient algorithm can be used. Second, we
show better bounds on the number of edge cuts which are likely to fail. Here,
Karger's analysis uses a variety of bounds for various graph parameters, we
show that these bounds cannot be simultaneously tight. We describe a new graph
parameter, which simultaneously influences all the bounds used by Karger, and
obtain much tighter estimates of the cut structure of $G$. These techniques
allow us to improve the runtime to $n^{3+o(1)} \epsilon^{-2}$, our results also
rigorously prove certain experimental observations of Karger \&amp; Tai (Proc.
ACM-SIAM Symposium on Discrete Algorithms, 1997). Our rigorous proofs are
motivated by certain non-rigorous differential-equation approximations which,
however, provably track the worst-case trajectories of the relevant parameters.
  A key driver of Karger's approach (and other cut-related results) is a bound
on the number of small cuts: we improve these estimates when the min-cut size
is &quot;small&quot; and odd, augmenting, in part, a result of Bixby (Bulletin of the
AMS, 1974).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08734</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08734</id><created>2016-02-28</created><authors><author><keyname>Salimans</keyname><forenames>Tim</forenames></author></authors><title>A Structured Variational Auto-encoder for Learning Deep Hierarchies of
  Sparse Features</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we present a generative model of natural images consisting of a
deep hierarchy of layers of latent random variables, each of which follows a
new type of distribution that we call rectified Gaussian. These rectified
Gaussian units allow spike-and-slab type sparsity, while retaining the
differentiability necessary for efficient stochastic gradient variational
inference. To learn the parameters of the new model, we approximate the
posterior of the latent variables with a variational auto-encoder. Rather than
making the usual mean-field assumption however, the encoder parameterizes a new
type of structured variational approximation that retains the prior
dependencies of the generative model. Using this structured posterior
approximation, we are able to perform joint training of deep models with many
layers of latent random variables, without having to resort to stacking or
other layerwise training procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08735</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08735</id><created>2016-02-28</created><updated>2016-03-01</updated><authors><author><keyname>AlEnezi</keyname><forenames>Qadha'a</forenames></author><author><keyname>AboElFotoh</keyname><forenames>Hosam</forenames></author><author><keyname>AlBdaiwi</keyname><forenames>Bader</forenames></author><author><keyname>AlMulla</keyname><forenames>Mohammad Ali</forenames></author></authors><title>Heuristics for the Variable Sized Bin Packing Problem Using a Hybrid
  P-System and CUDA Architecture</title><categories>cs.DC</categories><comments>20 pages; This paper is extracted from M.Sc. Thesis: Solution of the
  Variable Sized Bin Packing Problem using the P System and Cuda Architecture,
  by Qadhaa Al-Enezi, submitted March 2015, Computer Sc. Dept. Kuwait
  University, Supervised by Hosam AboElFotoh and Bader AlBdaiwi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Variable Sized Bin Packing Problem has a wide range of application areas
including packing, scheduling, and manufacturing. Given a list of items and
variable sized bin types, the objective is to minimize the total size of the
used bins. This problem is known to be NP-hard. In this article, we present two
new heuristics for solving the problem using a new variation of P systems with
active membranes, which we call a hybrid P system, implemented in CUDA. Our
hybrid P-system model allows using the polarity and labels of membranes to
represent object properties which results in reducing the complexity of
implementing the P-system. We examine the performance of the two heuristics,
and compare the results with those of other known algorithms. The numerical
results show that good solutions for large instances (10000 items) of this
problem could be obtained in a very short time (seconds) using our CUDA
simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08736</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08736</id><created>2016-02-28</created><authors><author><keyname>Dmitriev</keyname><forenames>Alexei</forenames></author><author><keyname>Dainiak</keyname><forenames>Alex</forenames></author></authors><title>Uniqueness of the extremal graph in the problem of maximizing the number
  of independent sets in regular graphs</title><categories>math.CO cs.DM</categories><comments>in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this paper is to prove the uniqueness of a graph
attaining the maximum of the number of independent sets over all $k$-regular
graphs on $n$ vertices for $2k|n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08739</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08739</id><created>2016-02-28</created><authors><author><keyname>Kock</keyname><forenames>Joachim</forenames></author><author><keyname>Spivak</keyname><forenames>David I.</forenames></author></authors><title>Homotopy composition of cospans</title><categories>math.CT cs.LO math.AT quant-ph</categories><comments>4 pages</comments><msc-class>16B50, 18D10, 18A30, 16S15, 57N70, 57-XX, 94Cxx, 81P45</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  It is well known that the category of finite sets and cospans, composed by
pushout, contains the universal {\em special} commutative Frobenius algebra. In
this note we observe that the same construction yields also general commutative
Frobenius algebras, if just the pushouts are changed to homotopy pushouts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08741</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08741</id><created>2016-02-28</created><authors><author><keyname>Vasiliev</keyname><forenames>Nikolay N.</forenames></author></authors><title>Gibberish Semantics: How Good is Russian Twitter in Word Semantic
  Similarity Task?</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most studied and most successful language models were developed and
evaluated mainly for English and other close European languages, such as
French, German, etc. It is important to study applicability of these models to
other languages. The use of vector space models for Russian was recently
studied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These models
were evaluated against word semantic similarity task. For our knowledge Twitter
was not considered as a corpus for this task, with this work we fill the gap.
Results for vectors trained on Twitter corpus are comparable in accuracy with
other single-corpus trained models, although the best performance is currently
achieved by combination of multiple corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08742</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08742</id><created>2016-02-28</created><authors><author><keyname>Loach</keyname><forenames>James C.</forenames></author><author><keyname>Wang</keyname><forenames>Jinzhao</forenames></author></authors><title>Optimizing the Learning Order of Chinese Characters Using a Novel
  Topological Sort Algorithm</title><categories>cs.CL physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop a novel algorithm for optimizing the order in which Chinese
characters are learned, one that incorporates the benefits of learning them in
order of usage frequency and in order of their hierarchal structural
relationships. We show that our work outperforms previously published orderings
and algorithms. Our algorithm is applicable to any scheduling task where nodes
have intrinsic differences in importance and must be visited in topological
order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08750</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08750</id><created>2016-02-28</created><authors><author><keyname>Thom&#xe9;</keyname><forenames>Carl</forenames></author></authors><title>Filtering Video Noise as Audio with Motion Detection to Form a Musical
  Instrument</title><categories>cs.HC cs.SD</categories><comments>Received the 2015 best paper award in the KTH Royal Institute of
  Technology course &quot;Musical Communication and Music Technology&quot;</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Even though they differ in the physical domain, digital video and audio share
many characteristics. Both are temporal data streams often stored in buffers
with 8-bit values. This paper investigates a method for creating harmonic
sounds with a video signal as input. A musical instrument is proposed, that
utilizes video in both a sound synthesis method, and in a controller interface
for selecting musical notes at specific velocities. The resulting instrument
was informally determined by the author to sound both pleasant and interesting,
but hard to control, and therefore suited for synth pad sounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08751</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08751</id><created>2016-02-28</created><authors><author><keyname>Iacovacci</keyname><forenames>Jacopo</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author></authors><title>Extracting Information from Multiplex Networks</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages; 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplex networks are generalized network structures that are able to
describe networks in which the same set of nodes are connected by links that
have different connotations. Multiplex networks are ubiquitous since they
describe social, financial, engineering and biological networks as well.
Extending our ability to analyze complex networks to multiplex network
structures increases greatly the level of information that is possible to
extract from Big Data. For these reasons characterizing the centrality of nodes
in multiplex networks and finding new ways to solve challenging inference
problems defined on multiplex networks are fundamental questions of network
science. In this paper we discuss the relevance of the Multiplex PageRank
algorithm for measuring the centrality of nodes in multilayer networks and we
characterize the utility of the recently introduced indicator function
$\widetilde{\Theta}^{S}$ for describing their mesoscale organization and
community structure. As working examples for studying these measures we
consider three multiplex network datasets coming for social science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08761</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08761</id><created>2016-02-28</created><authors><author><keyname>Bolukbasi</keyname><forenames>Tolga</forenames></author><author><keyname>Chang</keyname><forenames>Kai-Wei</forenames></author><author><keyname>Wang</keyname><forenames>Joseph</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Structured Prediction with Test-time Budget Constraints</title><categories>stat.ML cs.CL cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of structured prediction under test-time budget
constraints. We propose a novel approach applicable to a wide range of
structured prediction problems in computer vision and natural language
processing. Our approach seeks to adaptively generate computationally costly
features during test-time in order to reduce the computational cost of
prediction while maintaining prediction performance. We show that training the
adaptive feature generation system can be reduced to a series of structured
learning problems, resulting in efficient training using existing structured
learning algorithms. This framework provides theoretical justification for
several existing heuristic approaches found in literature. We evaluate our
proposed adaptive system on two real-world structured prediction tasks, optical
character recognition (OCR) and dependency parsing. For OCR our method cuts the
feature acquisition time by half coming within a 1% margin of top accuracy. For
dependency parsing we realize an overall runtime gain of 20% without
significant loss in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08765</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08765</id><created>2016-02-28</created><authors><author><keyname>Chizari</keyname><forenames>Ata</forenames></author><author><keyname>Jamali</keyname><forenames>Mohammad Vahid</forenames></author><author><keyname>Salehi</keyname><forenames>Jawad A.</forenames></author><author><keyname>Dargahi</keyname><forenames>Akbar</forenames></author></authors><title>Designing A Dimmable OPPM-Based VLC System Under Channel Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design a dimming compatible visible light communication
(VLC) system in a standard office room according to illumination standards
under channel constraints. We use overlapping pulse position modulation (OPPM)
to support dimming control by changing the code weights. The system parameters
such as a valid interval for dimming together with an upper bound for bit rate
according to the channel delay spread are investigated. Moreover, considering
the dispersive VLC channel and using Monte Carlo (MC) simulations, a method is
proposed to determine the minimum code length in different dimming levels in
order to achieve a valid bit error rate (BER).
  Finally, trellis coded modulation (TCM) is suggested to be applied to OPPM in
order to take advantage of consequent coding gain which could be up to $3$ dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08771</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08771</id><created>2016-02-28</created><authors><author><keyname>White</keyname><forenames>Adam</forenames></author><author><keyname>White</keyname><forenames>Martha</forenames></author></authors><title>Investigating practical, linear temporal difference learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>Autonomous Agents and Multi-agent Systems, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Off-policy reinforcement learning has many applications including: learning
from demonstration, learning multiple goal seeking policies in parallel, and
representing predictive knowledge. Recently there has been an proliferation of
new policy-evaluation algorithms that fill a longstanding algorithmic void in
reinforcement learning: combining robustness to off-policy sampling, function
approximation, linear complexity, and temporal difference (TD) updates. This
paper contains two main contributions. First, we derive two new hybrid TD
policy-evaluation algorithms, which fill a gap in this collection of
algorithms. Second, we perform an empirical comparison to elicit which of these
new linear TD methods should be preferred in different situations, and make
concrete suggestions about practical use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08777</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08777</id><created>2016-02-28</created><updated>2016-03-01</updated><authors><author><keyname>Strohmeier</keyname><forenames>Martin</forenames></author><author><keyname>Sch&#xe4;fer</keyname><forenames>Matthias</forenames></author><author><keyname>Pinheiro</keyname><forenames>Rui</forenames></author><author><keyname>Lenders</keyname><forenames>Vincent</forenames></author><author><keyname>Martinovic</keyname><forenames>Ivan</forenames></author></authors><title>On Perception and Reality in Wireless Air Traffic Communications
  Security</title><categories>cs.CR cs.CY</categories><comments>18 pages, 5 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More than a dozen wireless technologies are used by air traffic communication
systems during different flight phases. From a conceptual perspective, all of
them are insecure as security was never part of their design. Recent
contributions from academic and hacking communities have exploited this
inherent vulnerability to demonstrate attacks on some of these technologies.
However, not all of these contributions have resonated widely within aviation
circles. At the same time, the security community lacks certain aviation domain
knowledge, preventing aviation authorities from giving credence to their
findings.
  In this paper, we aim to reconcile the view of the security community and the
perspective of aviation professionals concerning the safety of air traffic
communication technologies. To achieve this, we first provide a systematization
of the applications of wireless technologies upon which civil aviation relies.
Based on these applications, we comprehensively analyze vulnerabilities,
attacks, and countermeasures. We categorize the existing research on
countermeasures into approaches that are applicable in the short term and
research of secure new technologies deployable in the long term.
  Since not all of the required aviation knowledge is codified in academic
publications, we additionally examine existing aviation standards and survey
242 international aviation experts. Besides their domain knowledge, we also
analyze the awareness of members of the aviation community concerning the
security of wireless systems and collect their expert opinions on the potential
impact of concrete attack scenarios using these technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08778</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08778</id><created>2016-02-28</created><authors><author><keyname>Drabent</keyname><forenames>W&#x142;odzimierz</forenames></author></authors><title>Proving completeness of logic programs with the cut</title><categories>cs.LO cs.PL</categories><comments>17 pages</comments><acm-class>D.1.6; F.3.1; D.2.4; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Completeness of a logic program means that the program produces all the
answers required by its specification. The cut is an important construct of
programming language Prolog. It prunes part of the search space, this may
result in a loss of completeness. This paper proposes a way of proving
completeness of programs with the cut. The semantics of the cut is formalized
by describing how SLD-trees are pruned. A sufficient condition for completeness
is presented, proved sound, and illustrated by examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08780</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08780</id><created>2016-02-28</created><authors><author><keyname>Tasche</keyname><forenames>Dirk</forenames></author></authors><title>Does quantification without adjustments work?</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>15 pages, 2 figures</comments><msc-class>62C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification is the task of predicting the class labels of objects based on
the observation of their features. In contrast, quantification has been defined
as the task of determining the prevalence of the positive class labels in a
target dataset. The simplest approach to quantification is Classify &amp; Count
where a classifier is optimised for classification on a training set and
applied to the target dataset for the prediction of positive class labels. The
number of predicted positive labels is then used as an estimate of the positive
class prevalence in the target dataset. Since the performance of Classify &amp;
Count for quantification is known to be inferior its results typically are
subject to adjustments. However, some researchers recently have suggested that
Classify &amp; Count might actually work without adjustments if it is based on a
classifier that was specifically trained for quantification. We discuss the
theoretical foundation for this claim and explore its potential and limitations
with a numerical example based on the binormal model with equal variances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08791</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08791</id><created>2016-02-28</created><authors><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Duggan</keyname><forenames>Jennie</forenames></author><author><keyname>Elmore</keyname><forenames>Aaron</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author><author><keyname>Mattson</keyname><forenames>Tim</forenames></author><author><keyname>Stonebraker</keyname><forenames>Michael</forenames></author></authors><title>The BigDAWG Architecture</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BigDAWG is a polystore system designed to work on complex problems that
naturally span across different processing or storage engines. BigDAWG provides
an architecture that supports diverse database systems working with different
data models, support for the competing notions of location transparency and
semantic completeness via islands of information and a middleware that provides
a uniform multi-island interface. In this article, we describe the current
architecture of BigDAWG, its application on the MIMIC II medical dataset, and
our plans for the mechanics of cross-system queries. During the presentation,
we will also deliver a brief demonstration of the current version of BigDAWG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08799</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08799</id><created>2016-02-28</created><authors><author><keyname>Kang</keyname><forenames>Wang</forenames></author><author><keyname>Zheng</keyname><forenames>Chentian</forenames></author><author><keyname>Huang</keyname><forenames>Yangqi</forenames></author><author><keyname>Zhang</keyname><forenames>Xichao</forenames></author><author><keyname>Zhou</keyname><forenames>Yan</forenames></author><author><keyname>Lv</keyname><forenames>Weifeng</forenames></author><author><keyname>Zhao</keyname><forenames>Weisheng</forenames></author></authors><title>Complementary Skyrmion Racetrack Memory with Voltage Manipulation</title><categories>cond-mat.mes-hall cs.ET</categories><comments>3 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic skyrmion holds promise as information carriers in the
next-generation memory and logic devices, owing to the topological stability,
small size and extremely low current needed to drive it. One of the most
potential applications of skyrmion is to design racetrack memory (RM), named
Sk-RM, instead of utilizing domain wall (DW). However, current studies face
some key design challenges, e.g., skyrmion manipulation, data representation
and synchronization etc. To address these challenges, we propose here a
complementary Sk-RM structure with voltage manipulation. Functionality and
performance of the proposed design are investigated with micromagnetic
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08800</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08800</id><created>2016-02-28</created><authors><author><keyname>Bulgakov</keyname><forenames>Vitaly</forenames></author></authors><title>Iterative Aggregation Method for Solving Principal Component Analysis
  Problems</title><categories>cs.NA cs.IR cs.LG</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Motivated by the previously developed multilevel aggregation method for
solving structural analysis problems a novel two-level aggregation approach for
efficient iterative solution of Principal Component Analysis (PCA) problems is
proposed. The course aggregation model of the original covariance matrix is
used in the iterative solution of the eigenvalue problem by a power iterations
method. The method is tested on several data sets consisting of large number of
text documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08802</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08802</id><created>2016-02-28</created><authors><author><keyname>Olson</keyname><forenames>Randal S.</forenames></author><author><keyname>Hintze</keyname><forenames>Arend</forenames></author><author><keyname>Dyer</keyname><forenames>Fred C.</forenames></author><author><keyname>Moore</keyname><forenames>Jason H.</forenames></author><author><keyname>Adami</keyname><forenames>Christoph</forenames></author></authors><title>Exploring the coevolution of predator and prey morphology and behavior</title><categories>q-bio.PE cs.NE</categories><comments>8 pages, 8 figures, submitted to Artificial Life 2016 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common idiom in biology education states, &quot;Eyes in the front, the animal
hunts. Eyes on the side, the animal hides.&quot; In this paper, we explore one
possible explanation for why predators tend to have forward-facing, high-acuity
visual systems. We do so using an agent-based computational model of evolution,
where predators and prey interact and adapt their behavior and morphology to
one another over successive generations of evolution. In this model, we observe
a coevolutionary cycle between prey swarming behavior and the predator's visual
system, where the predator and prey continually adapt their visual system and
behavior, respectively, over evolutionary time in reaction to one another due
to the well-known &quot;predator confusion effect.&quot; Furthermore, we provide evidence
that the predator visual system is what drives this coevolutionary cycle, and
suggest that the cycle could be closed if the predator evolves a hybrid visual
system capable of narrow, high-acuity vision for tracking prey as well as
broad, coarse vision for prey discovery. Thus, the conflicting demands imposed
on a predator's visual system by the predator confusion effect could have led
to the evolution of complex eyes in many predators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08805</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08805</id><created>2016-02-28</created><authors><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Chen</keyname><forenames>Xiaojing</forenames></author><author><keyname>Chen</keyname><forenames>Tianyi</forenames></author><author><keyname>Huang</keyname><forenames>Longbo</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Two-Scale Stochastic Control for Multipoint Communication Systems with
  Renewables</title><categories>cs.SY math.OC</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing threats of global warming and climate changes call for an
energy-efficient and sustainable design of future wireless communication
systems. To this end, a novel two-scale stochastic control framework is put
forth for smart-grid powered coordinated multi-point (CoMP) systems. Taking
into account renewable energy sources (RES), dynamic pricing, two-way energy
trading facilities and imperfect energy storage devices, the energy management
task is formulated as an infinite-horizon optimization problem minimizing the
time-average energy transaction cost, subject to the users' quality of service
(QoS) requirements. Leveraging the Lyapunov optimization approach as well as
the stochastic subgradient method, a two-scale online control (TS-OC) approach
is developed for the resultant smart-grid powered CoMP systems. Using only
historical data, the proposed TS-OC makes online control decisions at two
timescales, and features a provably feasible and asymptotically near-optimal
solution. Numerical tests further corroborate the theoretical analysis, and
demonstrate the merits of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08809</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08809</id><created>2016-02-28</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author><author><keyname>Foster</keyname><forenames>Keith</forenames></author></authors><title>Operators for Space and Time in BeSpaceD</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we present some spatio-temporal operators for our BeSpaceD
framework. We port operators known from functional programming languages such
as filtering, folding and normalization on abstract data structures to the
BeSpaceD specification language. We present the general ideas behind the
operators, highlight implementation details and present some simple examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08820</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08820</id><created>2016-02-28</created><authors><author><keyname>Dhulipala</keyname><forenames>Laxman</forenames></author><author><keyname>Kabiljo</keyname><forenames>Igor</forenames></author><author><keyname>Karrer</keyname><forenames>Brian</forenames></author><author><keyname>Ottaviano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author><author><keyname>Shalita</keyname><forenames>Alon</forenames></author></authors><title>Compressing Graphs and Indexes with Recursive Graph Bisection</title><categories>cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph reordering is a powerful technique to increase the locality of the
representations of graphs, which can be helpful in several applications. We
study how the technique can be used to improve compression of graphs and
inverted indexes.
  We extend the recent theoretical model of Chierichetti et al. (KDD 2009) for
graph compression, and show how it can be employed for compression-friendly
reordering of social networks and web graphs and for assigning document
identifiers in inverted indexes. We design and implement a novel theoretically
sound reordering algorithm that is based on recursive graph bisection.
  Our experiments show a significant improvement of the compression rate of
graph and indexes over existing heuristics. The new method is relatively simple
and allows efficient parallel and distributed implementations, which is
demonstrated on graphs with billions of vertices and hundreds of billions of
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08829</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08829</id><created>2016-02-29</created><authors><author><keyname>Petri</keyname><forenames>Matthias</forenames></author><author><keyname>Moffat</keyname><forenames>Alistair</forenames></author><author><keyname>Nagesh</keyname><forenames>P. C.</forenames></author><author><keyname>Wirth</keyname><forenames>Anthony</forenames></author></authors><title>Access Time Tradeoffs in Archive Compression</title><categories>cs.IT cs.DS math.IT</categories><comments>Note that the final published version of this paper prepared by
  Springer/LNCS introduced errors in the publication process in Figures 1, 2,
  and 3 that are not present in this preprint. In all other regards the
  preprint and the published version are identical in their content</comments><journal-ref>Asia Information Retrieval Societies Conference (AIRS), LNCS vol.
  9460, pages 15-28, 2015</journal-ref><doi>10.1007/978-3-319-28940-3_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web archives, query and proxy logs, and so on, can all be very large and
highly repetitive; and are accessed only sporadically and partially, rather
than continually and holistically. This type of data is ideal for
compression-based archiving, provided that random-access to small fragments of
the original data can be achieved without needing to decompress everything. The
recent RLZ (relative Lempel Ziv) compression approach uses a semi-static model
extracted from the text to be compressed, together with a greedy factorization
of the whole text encoded using static integer codes. Here we demonstrate more
precisely than before the scenarios in which RLZ excels. We contrast RLZ with
alternatives based on block-based adaptive methods, including approaches that
&quot;prime&quot; the encoding for each block, and measure a range of implementation
options using both hard-disk (HDD) and solid-state disk (SSD) drives. For HDD,
the dominant factor affecting access speed is the compression rate achieved,
even when this involves larger dictionaries and larger blocks. When the data is
on SSD the same effects are present, but not as markedly, and more complex
trade-offs apply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08834</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08834</id><created>2016-02-29</created><authors><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel</forenames></author><author><keyname>Rao</keyname><forenames>K. S. Mallikarjuna</forenames></author></authors><title>Characterization of maximum hands-off control</title><categories>cs.SY math.OC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum hands-off control aims to maximize the length of time over which zero
actuator values are applied to a system when executing specified control tasks.
To tackle such problems, recent literature has investigated optimal control
problems which penalize the size of the support of the control function and
thereby lead to desired sparsity properties. This article gives the exact set
of necessary conditions for a maximum hands-off optimal control problem using
an $L_0$-(semi)norm, and also provides sufficient conditions for the optimality
of such controls. Numerical example illustrates that adopting an $L_0$ cost
leads to a sparse control, whereas an $L_1$-relaxation in singular problems
leads to a non-sparse solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08836</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08836</id><created>2016-02-29</created><authors><author><keyname>Mohammadi</keyname><forenames>Mohammadali</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Tellambura</keyname><forenames>Chintha</forenames></author></authors><title>Full-Duplex Cloud-RAN with Uplink/Downlink Remote Radio Head Association</title><categories>cs.IT math.IT</categories><comments>Accepted for the IEEE International Conference on Communications (ICC
  2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a cloud radio access network (C-RAN) where spatially
distributed remote radio heads (RRHs) communicate with a full-duplex user. In
order to reflect a realistic scenario, the uplink (UL) and downlink (DL) RRHs
are assumed to be equipped with multiple antennas and distributed according to
a Poisson point process. We consider all participate and nearest RRH
association schemes with distributed beamforming in the form of maximum ratio
combining/maximal ratio transmission (MRC/MRT) and zero-forcing/MRT(ZF/MRT)
processing. We derive analytical expressions useful to compare the average sum
rate among association schemes as a function of the number of RRHs antennas and
density of the UL and DL RRHs. Numerical results show that significant
performance improvements can be achieved by using the full-duplex mode as
compared to the half-duplex mode, while the choice of the beamforming design as
well as the RRH association scheme plays a critical role in determining the
full-duplex gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08844</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08844</id><created>2016-02-29</created><authors><author><keyname>Chaudhuri</keyname><forenames>Pramit</forenames></author><author><keyname>Dexter</keyname><forenames>Joseph P.</forenames></author></authors><title>Bioinformatics and Classical Literary Study</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a collaborative project between classicists,
quantitative biologists, and computer scientists to apply ideas and methods
drawn from the sciences to the study of literature. A core goal of the project
is the use of computational biology, natural language processing, and machine
learning techniques to investigate intertextuality, reception, and related
phenomena of literary significance. As a case study in our approach, here we
describe the use of sequence alignment, a common technique in genomics, to
detect intertextuality in Latin literature. Sequence alignment is distinguished
by its ability to find inexact verbal parallels, which makes it ideal for
identifying phonetic resemblances in large corpora of Latin texts. Although
especially suited to Latin, sequence alignment in principle can be extended to
many other languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08845</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08845</id><created>2016-02-29</created><authors><author><keyname>Qin</keyname><forenames>Chengjie</forenames></author><author><keyname>Rusu</keyname><forenames>Florin</forenames></author></authors><title>Dot-Product Join: An Array-Relation Join Operator for Big Model
  Analytics</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big Data analytics has been approached exclusively from a data-parallel
perspective, where data are partitioned to multiple workers -- threads or
separate servers -- and model training is executed concurrently over different
partitions, under various synchronization schemes that guarantee speedup and/or
convergence. The dual -- Big Model -- problem that, surprisingly, has received
no attention in database analytics, is how to manage models with millions if
not billions of parameters that do not fit in memory. This distinction in model
representation changes fundamentally how in-database analytics tasks are
carried out. In this paper, we introduce the first secondary storage
array-relation dot-product join operator between a set of sparse arrays and a
dense relation targeted. The paramount challenge in designing such an operator
is how to optimally schedule access to the dense relation based on the sparse
non-contiguous entries in the sparse arrays. We prove that this problem is
NP-hard and propose a practical solution characterized by two important
technical contributions---dynamic batch processing and array reordering. We
execute extensive experiments over synthetic and real data that confirm the
minimal overhead the operator incurs when sufficient memory is available and
the graceful degradation it suffers as memory resources become scarce.
Moreover, dot-product join achieves an order of magnitude reduction in
execution time for Big Model analytics over alternative in-database solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08855</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08855</id><created>2016-02-29</created><authors><author><keyname>Florea</keyname><forenames>Corneliu</forenames></author><author><keyname>Condorovici</keyname><forenames>Razvan</forenames></author><author><keyname>Vertan</keyname><forenames>Constantin</forenames></author><author><keyname>Boia</keyname><forenames>Raluca</forenames></author><author><keyname>Florea</keyname><forenames>Laura</forenames></author><author><keyname>Vranceanu</keyname><forenames>Ruxandra</forenames></author></authors><title>Pandora: Description of a Painting Database for Art Movement Recognition
  with Baselines and Perspectives</title><categories>cs.CV</categories><comments>11 pages, 1 figure, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To facilitate computer analysis of visual art, in the form of paintings, we
introduce Pandora (Paintings Dataset for Recognizing the Art movement)
database, a collection of digitized paintings labelled with respect to the
artistic movement. Noting that the set of databases available as benchmarks for
evaluation is highly reduced and most existing ones are limited in variability
and number of images, we propose a novel large scale dataset of digital
paintings. The database consists of more than 7700 images from 12 art
movements. Each genre is illustrated by a number of images varying from 250 to
nearly 1000. We investigate how local and global features and classification
systems are able to recognize the art movement. Our experimental results
suggest that accurate recognition is achievable by a combination of various
categories.To facilitate computer analysis of visual art, in the form of
paintings, we introduce Pandora (Paintings Dataset for Recognizing the Art
movement) database, a collection of digitized paintings labelled with respect
to the artistic movement. Noting that the set of databases available as
benchmarks for evaluation is highly reduced and most existing ones are limited
in variability and number of images, we propose a novel large scale dataset of
digital paintings. The database consists of more than 7700 images from 12 art
movements. Each genre is illustrated by a number of images varying from 250 to
nearly 1000. We investigate how local and global features and classification
systems are able to recognize the art movement. Our experimental results
suggest that accurate recognition is achievable by a combination of various
categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08857</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08857</id><created>2016-02-29</created><authors><author><keyname>Li</keyname><forenames>Changming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Song</keyname><forenames>Shenghui</forenames></author><author><keyname>Letaief</keyname><forenames>K. B.</forenames></author></authors><title>Selective Uplink Training for Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, accepted to IEEE International Conference on
  Communications (ICC) 2016, Kuala Lumpur, Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a promising technique to meet the drastically growing demand for both high
throughput and uniform coverage in the fifth generation (5G) wireless networks,
massive multiple-input multiple-output (MIMO) systems have attracted
significant attention in recent years. However, in massive MIMO systems, as the
density of mobile users (MUs) increases, conventional uplink training methods
will incur prohibitively high training overhead, which is proportional to the
number of MUs. In this paper, we propose a selective uplink training method for
massive MIMO systems, where in each channel block only part of the MUs will
send uplink pilots for channel training, and the channel states of the
remaining MUs are predicted from the estimates in previous blocks, taking
advantage of the channels' temporal correlation. We propose an efficient
algorithm to dynamically select the MUs to be trained within each block and
determine the optimal uplink training length. Simulation results show that the
proposed training method provides significant throughput gains compared to the
existing methods, while much lower estimation complexity is achieved. It is
observed that the throughput gain becomes higher as the MU density increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08863</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08863</id><created>2016-02-29</created><authors><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames></author></authors><title>Choreographies in Practice</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Choreographic Programming is a development methodology for concurrent
software that guarantees correctness by construction. The key to this paradigm
is to disallow mismatched I/O operations in programs, called choreographies,
and then mechanically synthesise distributed implementations in terms of
standard process models via a mechanism known as EndPoint Projection (EPP).
  Despite the promise of choreographic programming, there is still a lack of
practical evaluations that illustrate the applicability of choreographies to
concrete computational problems with standard concurrent solutions. In this
work, we explore the potential of choreographies by using Procedural
Choreographies (PC), a model that we recently proposed, to write distributed
algorithms for sorting (Quicksort), solving linear equations (Gaussian
elimination), and computing Fast Fourier Transform. We discuss the lessons
learned from this experiment, giving possible directions for the usage and
future improvements of choreography languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08877</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08877</id><created>2016-02-29</created><authors><author><keyname>Wang</keyname><forenames>Zhongju</forenames></author><author><keyname>Babu</keyname><forenames>Prabhu</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>Design of PAR-Constrained Sequences for MIMO Channel Estimation via
  Majorization-Minimization</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PAR-constrained sequences are widely used in communication systems and radars
due to various practical needs; specifically, sequences are required to be
unimodular or of low peak-to-average power ratio (PAR). For unimodular sequence
design, plenty of efforts have been devoted to obtaining good correlation
properties. Regarding channel estimation, however, sequences of such properties
do not necessarily help produce optimal estimates. Tailored unimodular
sequences for the specific criterion concerned are desirable especially when
the prior knowledge of the channel is taken into account as well. In this
paper, we formulate the problem of optimal unimodular sequence design for
minimum mean square error estimation of the channel impulse response and
conditional mutual information maximization, respectively. Efficient algorithms
based on the majorization-minimization framework are proposed for both problems
with guaranteed convergence. As the unimodular constraint is a special case of
the low PAR constraint, optimal sequences of low PAR are also considered.
Numerical examples are provided to show the performance of the proposed
training sequences, with the efficiency of the derived algorithms demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08886</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08886</id><created>2016-02-29</created><authors><author><keyname>Kolla</keyname><forenames>Ravi Kumar</forenames></author><author><keyname>Jagannathan</keyname><forenames>Krishna</forenames></author><author><keyname>Gopalan</keyname><forenames>Aditya</forenames></author></authors><title>Stochastic bandits on a social network: Collaborative learning with
  local information sharing</title><categories>cs.LG stat.ML</categories><comments>16 Pages, 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a collaborative online learning paradigm, wherein a group of
agents connected through a social network are engaged in learning a Multi-Armed
Bandit problem. Each time an agent takes an action, the corresponding reward is
instantaneously observed by the agent, as well as its neighbours in the social
network. We perform a regret analysis of various policies in this collaborative
learning setting. A key finding of this paper is that appropriate network
extensions of widely-studied single agent learning policies do not perform well
in terms of regret. In particular, we identify a class of non-altruistic and
individually consistent policies, which could suffer a large regret. We also
show that the regret performance can be substantially improved by exploiting
the network structure. Specifically, we consider a star network, which is a
common motif in hierarchical social networks, and show that the hub agent can
be used as an information sink, to aid the learning rates of the entire
network. We also present numerical experiments to corroborate our analytical
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08895</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08895</id><created>2016-02-29</created><authors><author><keyname>Nowak</keyname><forenames>Rafa&#x142;</forenames></author><author><keyname>Wo&#x17a;ny</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>New properties of a certain method of summation of generalized
  hypergeometric series</title><categories>math.NA cs.NA</categories><msc-class>65B10, 33F05</msc-class><acm-class>G.1.0; G.1.2; G.1.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper [P. Wo\'zny and R. Nowak, Method of summation of some
slowly convergent series. Appl. Math. Comput., 215, 4, pp. 1622--1645, 2009]
the authors proposed a method of summation of some slowly convergent series.
The purpose of this note is to give more analysis for this transformation,
including the convergence acceleration theorem, in the case of summation of
generalized hypergeometric series. Some new theoretical results and
illustrative numerical examples are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08898</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08898</id><created>2016-02-29</created><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author><author><keyname>Berta</keyname><forenames>Mario</forenames></author></authors><title>Converse bounds for private communication over quantum channels</title><categories>quant-ph cs.IT math.IT</categories><comments>51 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes several converse bounds on the private transmission
capabilities of a quantum channel. The main conceptual development builds
firmly on the notion of a private state [Horodecki et al., PRL 94, 160502
(2005)], which is a powerful, uniquely quantum method for simplifying the
tripartite picture of privacy involving local operations and public classical
communication to a bipartite picture of quantum privacy involving local
operations and classical communication. This approach has previously led to
some of the strongest upper bounds on secret key rates, including the squashed
entanglement and the relative entropy of entanglement. Here we use this
approach along with a &quot;privacy test&quot; to establish a general meta-converse bound
for private communication, which has a number of applications. The
meta-converse allows for proving that any quantum channel's relative entropy of
entanglement is a strong converse rate for private communication. For covariant
channels, the meta-converse also leads to second-order expansions of relative
entropy of entanglement bounds for private communication rates. For such
channels, the bounds also apply to the private communication setting in which
the sender and receiver are assisted by unlimited public classical
communication, and as such, they are relevant for establishing various converse
bounds for quantum key distribution protocols conducted over these channels. We
find precise characterizations for several channels of interest and apply the
methods to establish several converse bounds on the private transmission
capabilities of all phase-insensitive bosonic channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08903</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08903</id><created>2016-02-29</created><authors><author><keyname>Osorio</keyname><forenames>Mauricio</forenames></author><author><keyname>Nieves</keyname><forenames>Juan Carlos</forenames></author></authors><title>Range-based argumentation semantics as 2-valued models</title><categories>cs.LO cs.AI</categories><comments>16 pages, to appear in Theory and Practice of Logic Programming
  (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizations of semi-stable and stage extensions in terms of 2-valued
logical models are presented. To this end, the so-called GL-supported and
GL-stage models are defined. These two classes of logical models are logic
programming counterparts of the notion of range which is an established concept
in argumentation semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08908</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08908</id><created>2016-02-29</created><authors><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Song</keyname><forenames>S. H.</forenames></author><author><keyname>Letaief</keyname><forenames>K. B.</forenames></author></authors><title>QoS-Aware Joint Mode Selection and Channel Assignment for D2D
  Communications</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, accepted to IEEE International Conference on
  Communications (ICC) 2016, Kuala Lumpur, Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Underlaying device-to-device (D2D) communications to a cellular network is
considered as a key technique to improve spectral efficiency in 5G networks.
For such D2D systems, mode selection and resource allocation have been widely
utilized for managing interference. However, previous works allowed at most one
D2D link to access the same channel, while mode selection and resource
allocation are typically separately designed. In this paper, we jointly
optimize the mode selection and channel assignment in a cellular network with
underlaying D2D communications, where multiple D2D links may share the same
channel. Meanwhile, the QoS requirements for both cellular and D2D links are
guaranteed, in terms of Signal-to-Interference-plus-Noise Ratio (SINR). We
first propose an optimal dynamic programming (DP) algorithm, which provides a
much lower computation complexity compared to exhaustive search and serves as
the performance bench mark. A bipartite graph based greedy algorithm is then
proposed to achieve a polynomial time complexity. Simulation results will
demonstrate the advantage of allowing each channel to be accessed by multiple
D2D links in dense D2D networks, as well as, the effectiveness of the proposed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08912</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08912</id><created>2016-02-29</created><authors><author><keyname>Roux</keyname><forenames>St&#xe9;phane Le</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>Extending finite memory determinacy: General techniques and an
  application to energy parity games</title><categories>cs.GT cs.LO</categories><msc-class>91A80, 03D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide several techniques to extend finite memory determinacy from some
restricted class of games played on a finite graph to a larger class. As a
particular example, we study energy parity games.
  First we show that under some general conditions the finite memory
determinacy of a class of two-player win/lose games played on finite graphs
implies the existence of a Nash equilibrium built from finite memory strategies
for the corresponding class of multi-player multi-outcome games. This
generalizes a previous result by Brihaye, De Pril and Schewe.
  Then we investigate adding additional constraints to the winning conditions,
in a way that generalizes the move from parity to bounded energy parity games.
We prove that under some conditions, this preserves finite memory determinacy.
We show that bounded energy parity games and unbounded energy parity games are
equivalent, and thus obtain a new proof of finite memory determinacy for energy
parity games. Our proof yields significantly improved bounds on the memory
required compared to the original one by Chatterjee and Doyen. We then apply
our main theorem to show that multi-player multi-outcome energy parity games
have finite memory Nash equilibria.
  Our proofs are generally constructive, that is, provide upper bounds for the
memory required, as well as algorithms to compute the relevant winning
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08925</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08925</id><created>2016-02-29</created><authors><author><keyname>Feuilloley</keyname><forenames>Laurent</forenames></author><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames></author><author><keyname>Hirvonen</keyname><forenames>Juho</forenames></author></authors><title>A hierarchy of local decision</title><categories>cs.DC</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the notion of distributed decision in the framework of distributed
network computing, inspired by recent results on so-called distributed graph
automata. We show that, by using distributed decision mechanisms based on the
interaction between a prover and a disprover, the size of the certificates
distributed to the nodes for certifying a given network property can be
drastically reduced. For instance, we prove that minimum spanning tree can be
certified with $O(\log n)$-bit certificates in $n$-node graphs, with just one
interaction between the prover and the disprover, while it is known that
certifying MST requires $\Omega(\log^2 n)$-bit certificates if only the prover
can act. The improvement can even be exponential for some simple graph
properties. For instance, it is known that certifying the existence of a
nontrivial automorphism requires $\Omega(n^2)$ bits if only the prover can act.
We show that there is a protocol with two interactions between the prover and
the disprover enabling to certify nontrivial automorphism with $O(\log n)$-bit
certificates. These results are achieved by defining and analysing a local
hierarchy of decision which generalizes the classical notions of
proof-labelling schemes and locally checkable proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08927</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08927</id><created>2016-02-29</created><authors><author><keyname>Luo</keyname><forenames>Ye</forenames></author><author><keyname>Spindler</keyname><forenames>Martin</forenames></author></authors><title>$L_2$Boosting in High-Dimensions: Rate of Convergence</title><categories>stat.ML cs.LG math.ST stat.ME stat.TH</categories><comments>19 pages, 4 tables; AMS 2000 subject classifications: Primary 62J05,
  62J07; secondary 49M15, 68Q32</comments><msc-class>62J05, 62J07, 49M15, 68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boosting is one of the most significant developments in machine learning.
This paper studies the rate of convergence of $L_2$Boosting, which is tailored
for regression, in a high-dimensional setting. Moreover, we introduce so-called
\textquotedblleft post-Boosting\textquotedblright. This is a post-selection
estimator which applies ordinary least squares to the variables selected in the
first stage by $L_2$Boosting. Another variant is orthogonal boosting where
after each step an orthogonal projection is conducted. We show that both
post-$L_2$Boosting and the orthogonal boosting achieve the same rate of
convergence as Lasso in a sparse, high-dimensional setting. The
\textquotedblleft classical\textquotedblright $L_2$Boosting achieves a slower
convergence rate for prediction, but no assumptions on the design matrix are
imposed for this result in contrast to rates e.g.~established with LASSO. We
also introduce rules for early stopping which can easily be implemented and
will be used in applied work. Moreover, our results also allow a direct
comparison between LASSO and boosting that has been missing in the literature.
Finally, we present simulation studies to illustrate the relevance of our
theoretical results and to provide insights into the practical aspects of
boosting. In the simulation studies post-$L_2$Boosting clearly outperforms
LASSO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08952</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08952</id><created>2016-02-29</created><authors><author><keyname>K&#xe1;d&#xe1;r</keyname><forenames>&#xc1;kos</forenames></author><author><keyname>Chrupa&#x142;a</keyname><forenames>Grzegorz</forenames></author><author><keyname>Alishahi</keyname><forenames>Afra</forenames></author></authors><title>Representation of linguistic form and function in recurrent neural
  networks</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present novel methods for analysing the activation patterns of RNNs and
identifying the types of linguistic structure they learn. As a case study, we
use a multi-task gated recurrent network model consisting of two parallel
pathways with shared word embeddings trained on predicting the representations
of the visual scene corresponding to an input sentence, and predicting the next
word in the same sentence. We show that the image prediction pathway is
sensitive to the information structure of the sentence, and pays selective
attention to lexical categories and grammatical functions that carry semantic
information. It also learns to treat the same input token differently depending
on its grammatical functions in the sentence. The language model is
comparatively more sensitive to words with a syntactic function. Our analysis
of the function of individual hidden units shows that each pathway contains
specialized units tuned to patterns informative for the task, some of which can
carry activations to later time steps to encode long-term dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08960</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08960</id><created>2016-02-29</created><authors><author><keyname>Palomares</keyname><forenames>Roberto P.</forenames></author><author><keyname>Haro</keyname><forenames>Gloria</forenames></author><author><keyname>Ballester</keyname><forenames>Coloma</forenames></author><author><keyname>Meinhardt-Llopis</keyname><forenames>Enric</forenames></author></authors><title>FALDOI: Large Displacement Optical Flow by Astute Initialization</title><categories>cs.CV</categories><msc-class>68U10, 49M29, 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a large displacement optical flow method that introduces a new
strategy to compute a good local minimum of any optical flow energy functional.
The method requires a given set of discrete matches, which can be extremely
sparse, and an energy functional. The matches are used to guide a structured
coordinate-descent of the energy functional around these keypoints. It results
in a two-step minimization method at the finest scale which is very robust to
the inevitable outliers of the sparse matcher, and it is better than the multi-
scale methods, especially when there are small objects with very large
displacements, that the multi-scale methods are incapable to find. Indeed, the
proposed method recovers the correct motion field of any object which has at
least one correct match, regardless of the magnitude of the displacement. We
validate our proposal using several optical flow variational models. The
results consistently outperform the coarse-to-fine approaches and achieve good
qualitative and quantitative performance on the standard optical flow
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08971</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08971</id><created>2016-02-29</created><authors><author><keyname>Reiter</keyname><forenames>Fabian</forenames></author></authors><title>Alternating Set Quantifiers in Modal Logic</title><categories>cs.LO cs.FL math.LO</categories><comments>20 pages, 2 figures; Can be seen as a follow-up paper to
  arXiv:1408.3030; Typeset using Linux Libertine as main font and AMS Euler as
  math font</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the strictness of several set quantifier alternation hierarchies
that are based on modal logic, evaluated on various classes of finite graphs.
This extends to the modal setting a celebrated result of Matz, Schweikardt and
Thomas (2002), which states that the analogous hierarchy of monadic
second-order logic is strict.
  Thereby, the present paper settles a question raised by van Benthem (1983),
revived by ten Cate (2006), and partially answered by Kuusisto (2008, 2015).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08975</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08975</id><created>2016-02-29</created><authors><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author><author><keyname>Afrasiabi-Gorgani</keyname><forenames>Saeed</forenames></author></authors><title>Overshooting and $L^1$-Norms of a Class of Nyquist Filters</title><categories>cs.IT math.IT</categories><comments>Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To tightly control the signal envelope, estimating the peak regrowth between
FFT samples is an important sub-problem in multicarrier communications. While
the problem is well-investigated for trigonometric polynomials (i.e. OFDM), the
impact of an aperiodic transmit filter is important too and typically neglected
in the peak regrowth analysis. In this paper, we provide new bounds on the
overshooting between samples for general multicarrier signals improving on
available bounds for small oversampling factors. In particular, we generalize a
result of [1, Theorem 4.10]. Our results will be extended to bound overshooting
of a class of Nyquist filters as well. Eventually, results are related to some
respective $L^1$-properties of these filters with application to filter design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08977</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08977</id><created>2016-02-29</created><authors><author><keyname>Mackenzie</keyname><forenames>Crist&#xf3;bal</forenames></author><author><keyname>Pichara</keyname><forenames>Karim</forenames></author><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author></authors><title>Clustering Based Feature Learning on Variable Stars</title><categories>astro-ph.SR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of automatic classification of variable stars strongly depends on
the lightcurve representation. Usually, lightcurves are represented as a vector
of many statistical descriptors designed by astronomers called features. These
descriptors commonly demand significant computational power to calculate,
require substantial research effort to develop and do not guarantee good
performance on the final classification task. Today, lightcurve representation
is not entirely automatic; algorithms that extract lightcurve features are
designed by humans and must be manually tuned up for every survey. The vast
amounts of data that will be generated in future surveys like LSST mean
astronomers must develop analysis pipelines that are both scalable and
automated. Recently, substantial efforts have been made in the machine learning
community to develop methods that prescind from expert-designed and manually
tuned features for features that are automatically learned from data. In this
work we present what is, to our knowledge, the first unsupervised feature
learning algorithm designed for variable stars. Our method first extracts a
large number of lightcurve subsequences from a given set of photometric data,
which are then clustered to find common local patterns in the time series.
Representatives of these patterns, called exemplars, are then used to transform
lightcurves of a labeled set into a new representation that can then be used to
train an automatic classifier. The proposed algorithm learns the features from
both labeled and unlabeled lightcurves, overcoming the bias generated when the
learning process is done only with labeled data. We test our method on MACHO
and OGLE datasets; the results show that the classification performance we
achieve is as good and in some cases better than the performance achieved using
traditional features, while the computational cost is significantly lower.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08978</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08978</id><created>2015-11-17</created><authors><author><keyname>Maeno</keyname><forenames>Yoshiharu</forenames></author></authors><title>Epidemiological geographic profiling for a meta-population network</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemiological geographic profiling is a statistical method for making
inferences about likely areas of a source from the geographical distribution of
patients. Epidemiological geographic profiling algorithms are developed to
locate a source from the dataset on the number of new cases for a
meta-population network model. It is found from the WHO dataset on the SARS
outbreak that Hong Kong remains the most likely source throughout the period of
observation. This reasoning is pertinent under the restricted circumstance that
the number of reported probable cases in China was missing, unreliable, and
incomprehensive. It may also imply that globally connected Hong Kong was more
influential as a spreader than China. Singapore, Taiwan, Canada, and the United
States follow Hong Kong in the likeliness ranking list.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08979</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08979</id><created>2015-11-09</created><authors><author><keyname>Bisht</keyname><forenames>Raj Kishor</forenames></author></authors><title>A fuzzy based conceptual framework for career counselling</title><categories>cs.CY</categories><journal-ref>Advance Computational Intelligence: An international
  Journal,(2015), Vol 2, No. 4, 27-35</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Career guidance for students, particularly in rural areas is a challenging
issue in India. In the present era of digitalization, there is a need of an
automated system that can analyze a student for his/her capabilities, suggest a
career and provide related information. Keeping in mind the requirement, the
present paper is an effort in this direction. In this paper, a fuzzy based
conceptual framework has been suggested. It has two parts; in the first part a
students will be analyzed for his/her capabilities and in the second part the
available courses, job aspects related to their capabilities will be suggested.
To analyze a student, marks in various subject in 10+2 standards and vocational
interest in different fields have been considered and fuzzy sets have been
formed. On example basis, fuzzy inference rules have been framed for analyzing
the abilities in engineering, medical and hospitality fields only. In second
part, concept of composition of relations has been used to suggest the related
courses and jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08981</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08981</id><created>2016-02-29</created><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Walter</keyname><forenames>Tobias</forenames></author></authors><title>Characterizing classes of regular languages using prefix codes of
  bounded synchronization delay</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we continue a classical work of Sch\&quot;utzenberger on codes with
bounded synchronization delay. He was interested to characterize those regular
languages where the groups in the syntactic monoid belong to a variety $H$. He
allowed operations on the language side which are union, intersection,
concatenation and modified Kleene-star involving a mapping of a prefix code of
bounded synchronization delay to a group $G\in H$, but no complementation. In
our notation this leads to the language classes $SD_G(A^\infty)$ and
$SD_H(A^\infty$). Our main result shows that $SD_H(A^\infty)$ always
corresponds to the languages having syntactic monoids where all subgroups are
in $H$. Sch\&quot;utzenberger showed this for a variety $H$ if $H$ contains Abelian
groups, only. Our method shows the general result for all $H$ directly on
finite and infinite words. Furthermore, we introduce the notion of local Rees
products which refers to a simple type of classical Rees extensions. We give a
decomposition of a monoid in terms of its groups and local Rees products. This
gives a somewhat similar, but simpler decomposition than in Rhodes' synthesis
theorem. Moreover, we need a singly exponential number of operations, only.
Finally, our decomposition yields an answer to a question in a recent paper of
Almeida and Kl\'ima about varieties that are closed under Rees products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08986</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08986</id><created>2016-02-29</created><authors><author><keyname>Falher</keyname><forenames>G&#xe9;raud Le</forenames></author><author><keyname>Vitale</keyname><forenames>Fabio</forenames></author></authors><title>Even Trolls Are Useful: Efficient Link Classification in Signed Networks</title><categories>cs.LG cs.SI physics.soc-ph</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of classifying the links of signed social networks
given their full structural topology. Motivated by a binary user behaviour
assumption, which is supported by decades of research in psychology, we develop
an efficient and surprisingly simple approach to solve this classification
problem. Our methods operate both within the active and batch settings. We
demonstrate that the algorithms we developed are extremely fast in both
theoretical and practical terms. Within the active setting, we provide a new
complexity measure and a rigorous analysis of our methods that hold for
arbitrary signed networks. We validate our theoretical claims carrying out a
set of experiments on three well known real-world datasets, showing that our
methods outperform the competitors while being much faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08987</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08987</id><created>2016-02-29</created><updated>2016-03-01</updated><authors><author><keyname>Nguyen</keyname><forenames>Tam</forenames></author><author><keyname>Garone</keyname><forenames>Emanuele</forenames></author></authors><title>Proof of Control of a UAV and a UGV Cooperating to Manipulate an Object</title><categories>cs.SY</categories><comments>8 pages, 5 figures, American Control Conference (ACC) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the control of a system composed of an Unmanned Aerial
Vehicle (UAV) and an Unmanned Ground Vehicle (UGV) which cooperate to
manipulate an object. The two units are subject to actuator saturations and
cooperate to move the object to a desired pose, characterized by its position
and inclination. The paper proposes a control strategy where the ground vehicle
is tasked to deploy the object to a certain position, whereas the aerial
vehicle adjusts its inclination. The ground vehicle is governed by a saturated
proportional-derivative control law. The aerial vehicle is regulated by means
of a cascade control specifically designed for this problem that is able to
exploit the mechanical interconnection. The stability of the overall system is
proved through Input-to-State Stability and Small Gain theorem arguments. To
solve the problem of constraints satisfaction, a nonlinear Reference Governor
scheme is implemented. Numerical simulations are provided to demonstrate the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.08991</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.08991</id><created>2016-02-25</created><authors><author><keyname>Leibner</keyname><forenames>Tobias</forenames></author><author><keyname>Milk</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Schindler</keyname><forenames>Felix</forenames></author></authors><title>Extending DUNE: The dune-xt modules</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present our effort to extend and complement the core modules of the
Distributed and Unified Numerics Environment DUNE (http://dune-project.org) by
a well tested and structured collection of utilities and concepts. We describe
key elements of our four modules dune-xt-common, dune-xt-grid, dune-xt-la and
dune-xt-functions, which aim at further enabling the programming of generic
algorithms within DUNE as well as adding an extra layer of usability and
convenience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09000</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09000</id><created>2016-02-29</created><authors><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author><author><keyname>Saez-Trumper</keyname><forenames>Diego</forenames></author></authors><title>A Day of Your Days: Estimating Individual Daily Journeys Using Mobile
  Data to Understand Urban Flow</title><categories>cs.SI physics.soc-ph</categories><comments>Submitted for review - please contact authors before citing. 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, travel surveys provide rich information about urban mobility and
commuting patterns. But, at the same time, they have drawbacks: they are static
pictures of a dynamic phenomena, are expensive to make, and take prolonged
periods of time to finish. However, the availability of mobile usage data (Call
Detail Records) makes the study of urban mobility possible at levels not known
before. This has been done in the past with good results--mobile data makes
possible to find and understand aggregated mobility patterns. In this paper, we
propose to analyze mobile data at individual level by estimating daily
journeys, and use those journeys to build Origin-Destiny matrices to understand
urban flow. We evaluate this approach with large anonymized CDRs from Santiago,
Chile, and find that our method has a high correlation ($\rho = 0.89$) with the
current travel survey, and that it captures external anomalies in daily travel
patterns, making our method suitable for inclusion into urban computing
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09001</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09001</id><created>2016-02-29</created><authors><author><keyname>Vellambi</keyname><forenames>Badri N</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu</forenames></author></authors><title>Strong Coordination over Multi-hop Line Networks</title><categories>cs.IT math.IT</categories><comments>35 pages, 9 Figures, 4 Tables. A part of this work were published in
  the 2015 IEEE Information Theory Workshop, and a part was accepted for
  publication in the 50th Annual Conference on Information Sciences and Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the problem of strong coordination over a multi-hop line network
where the node initiating the coordination is a terminal network node. We
assume that each node has access to a certain quantum of randomness that is
local to the node, and together, the nodes also share some common randomness,
which are used together with explicit hop-by-hop communication to achieve
strong coordination. We derive the trade-offs among the required rates of
communication on the network links, the rates of local randomness available to
network nodes, and the rate of common randomness to realize strong
coordination. We present an achievable coding scheme built using multiple
layers of channel resolvability codes, and establish several settings where
this scheme is proven to offer the best possible trade-offs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09013</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09013</id><created>2016-02-29</created><authors><author><keyname>Podosinnikova</keyname><forenames>Anastasia</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Lacoste-Julien</keyname><forenames>Simon</forenames></author></authors><title>Beyond CCA: Moment Matching for Multi-View Models</title><categories>stat.ML cs.LG</categories><comments>26 pages, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce three novel semi-parametric extensions of probabilistic
canonical correlation analysis with identifiability guarantees. We consider
moment matching techniques for estimation in these models. For that, by drawing
explicit links between the new models and a discrete version of independent
component analysis (DICA), we first extend the DICA cumulant tensors to the new
discrete version of CCA. By further using a close connection with independent
component analysis, we introduce generalized covariance matrices, which can
replace the cumulant tensors in the moment matching framework, and, therefore,
improve sample complexity and simplify derivations and algorithms
significantly. As the tensor power method or orthogonal joint diagonalization
are not applicable in the new setting, we use non-orthogonal joint
diagonalization techniques for matching the cumulants. We demonstrate
performance of the proposed models and estimation techniques on experiments
with both synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09019</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09019</id><created>2016-02-29</created><authors><author><keyname>Cano</keyname><forenames>Cristina</forenames></author><author><keyname>Pittolo</keyname><forenames>Alberto</forenames></author><author><keyname>Malone</keyname><forenames>David</forenames></author><author><keyname>Lampe</keyname><forenames>Lutz</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author><author><keyname>Dabak</keyname><forenames>Anand</forenames></author></authors><title>State-of-the-art in Power Line Communications: from the Applications to
  the Medium</title><categories>cs.NI</categories><comments>19 pages, 12 figures. Accepted for publication, IEEE Journal on
  Selected Areas in Communications. Special Issue on Power Line Communications
  and its Integration with the Networking Ecosystem. 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent decades, power line communication has attracted considerable
attention from the research community and industry, as well as from regulatory
and standardization bodies. In this article we provide an overview of both
narrowband and broadband systems, covering potential applications, regulatory
and standardization efforts and recent research advancements in channel
characterization, physical layer performance, medium access and higher layer
specifications and evaluations. We also identify areas of current and further
study that will enable the continued success of power line communication
technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09022</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09022</id><created>2016-02-29</created><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Moritz</forenames></author></authors><title>The parameterized space complexity of embedding along a path</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The embedding problem is to decide, given an ordered pair of structures,
whether or not there is an injective homomorphism from the first structure to
the second. We study this problem using an established perspective in
parameterized complexity theory: the universe size of the first structure is
taken to be the parameter, and we define the embedding problem relative to a
class ${\cl A}$ of structures to be the restricted version of the general
problem where the first structure must come from ${\cl A}$. We initiate a
systematic complexity study of this problem family, by considering classes
whose structures are what we call rooted path structures; these structures have
paths as Gaifman graphs. Our main theorem is a dichotomy theorem on classes of
rooted path structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09028</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09028</id><created>2016-02-29</created><authors><author><keyname>Joudeh</keyname><forenames>Hamdi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Sum-Rate Maximization for Linearly Precoded Downlink Multiuser MISO
  Systems with Partial CSIT: A Rate-Splitting Approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the Sum-Rate (SR) maximization problem in downlink
MU-MISO systems under imperfect Channel State Information (CSI) at the BS.
Contrary to existing works, we consider a rather unorthodox transmission
scheme. In particular, the message intended to one of the users is split into
two parts: a common part which can be recovered by all users, and a private
part recovered by the corresponding user. On the other hand, the rest of users
receive their information through private messages. This Rate-Splitting (RS)
approach was shown to boost the achievable Degrees of Freedom (DoF) when CSI
errors decay with increased SNR. In this work, the RS strategy is married with
linear precoder design and optimization techniques to achieve a maximized
Ergodic SR (ESR) performance over the entire range of SNRs. Precoders are
designed based on partial CSIT knowledge by solving a stochastic rate
optimization problem using means of Sample Average Approximation (SAA), coupled
with an efficient Weighted Mean Square Error (WMSE) Alternating Optimization
(AO) algorithm. Numerical results show that in addition to the ESR gains, the
benefits of RS also include relaxed CSI quality requirements and enhanced
achievable rate regions compared to conventional transmission with NoRS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09037</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09037</id><created>2016-02-29</created><authors><author><keyname>Kirchner</keyname><forenames>Paul</forenames></author></authors><title>Algorithms on Ideal over Complex Multiplication order</title><categories>cs.DS cs.CR cs.DM math.NT</categories><comments>Full version of a paper submitted to ANTS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show in this paper that the Gentry-Szydlo algorithm for cyclotomic orders,
previously revisited by Lenstra-Silverberg, can be extended to
complex-multiplication (CM) orders, and even to a more general structure. This
algorithm allows to test equality over the polarized ideal class group, and
finds a generator of the polarized ideal in polynomial time. Also, the
algorithm allows to solve the norm equation over CM orders and the recent
reduction of principal ideals to the real suborder can also be performed in
polynomial time. Furthermore, we can also compute in polynomial time a unit of
an order of any number field given a (not very precise) approximation of it.
Our description of the Gentry-Szydlo algorithm is different from the original
and Lenstra- Silverberg's variant and we hope the simplifications made will
allow a deeper understanding. Finally, we show that the well-known speed-up for
enumeration and sieve algorithms for ideal lattices over power of two
cyclotomics can be generalized to any number field with many roots of unity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09039</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09039</id><created>2016-02-29</created><authors><author><keyname>Ali</keyname><forenames>Mohamad Jaafar</forenames></author><author><keyname>Moungla</keyname><forenames>Hassine</forenames></author><author><keyname>Mehaoua</keyname><forenames>Ahmed</forenames></author></authors><title>Dynamic Channel Allocation for Interference Mitigation in Relay-assisted
  Wireless Body Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on interference mitigation and energy conservation within a single
wireless body area network (WBAN). We adopt two-hop communication scheme
supported by the the IEEE 802.15.6 standard (2012). In this paper, we propose a
dynamic channel allocation scheme, namely DCAIM to mitigate node-level
interference amongst the coexisting regions of a WBAN. At the time, the sensors
are in the radius communication of a relay, they form a relay region (RG)
coordinated by that relay using time division multiple access (TDMA). In the
proposed scheme, each RG creates a table consisting of interfering sensors
which it broadcasts to its neighboring sensors. This broadcast allows each pair
of RGs to create an interference set (IS). Thus, the members of IS are assigned
orthogonal sub-channels whereas other sonsors that do not belong to IS can
transmit using the same time slots. Experimental results show that our proposal
mitigates node-level interference and improves node and WBAN energy savings.
These results are then compared to the results of other schemes. As a result,
our scheme outperforms in all cases. Node-level signal to interference and
noise ratio (SINR) improved by 11dB whilst, the energy consumption decreased
significantly. We further present a probabilistic method and analytically show
the outage probability can be effectively reduced to the minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09046</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09046</id><created>2016-02-29</created><authors><author><keyname>Guberman</keyname><forenames>Nitzan</forenames></author></authors><title>On Complex Valued Convolutional Neural Networks</title><categories>cs.NE</categories><comments>M.Sc. thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) are the cutting edge model for
supervised machine learning in computer vision. In recent years CNNs have
outperformed traditional approaches in many computer vision tasks such as
object detection, image classification and face recognition. CNNs are
vulnerable to overfitting, and a lot of research focuses on finding
regularization methods to overcome it. One approach is designing task specific
models based on prior knowledge.
  Several works have shown that properties of natural images can be easily
captured using complex numbers. Motivated by these works, we present a
variation of the CNN model with complex valued input and weights. We construct
the complex model as a generalization of the real model. Lack of order over the
complex field raises several difficulties both in the definition and in the
training of the network. We address these issues and suggest possible
solutions.
  The resulting model is shown to be a restricted form of a real valued CNN
with twice the parameters. It is sensitive to phase structure, and we suggest
it serves as a regularized model for problems where such structure is
important. This suggestion is verified empirically by comparing the performance
of a complex and a real network in the problem of cell detection. The two
networks achieve comparable results, and although the complex model is hard to
train, it is significantly less vulnerable to overfitting. We also demonstrate
that the complex network detects meaningful phase structure in the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09052</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09052</id><created>2016-02-29</created><authors><author><keyname>Heuvel</keyname><forenames>Jan van den</forenames></author><author><keyname>de Mendez</keyname><forenames>Patrice Ossona</forenames></author><author><keyname>Quiroz</keyname><forenames>Daniel</forenames></author><author><keyname>Rabinovich</keyname><forenames>Roman</forenames></author><author><keyname>Siebertz</keyname><forenames>Sebastian</forenames></author></authors><title>On the Generalised Colouring Numbers of Graphs that Exclude a Fixed
  Minor</title><categories>math.CO cs.DM</categories><comments>20 pages, 2 figures</comments><msc-class>05C15 (Primary), 05C83, 05C12 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The generalised colouring numbers $\mathrm{col}_r(G)$ and
$\mathrm{wcol}_r(G)$ were introduced by Kierstead and Yang as a generalisation
of the usual colouring number, and have since then found important theoretical
and algorithmic applications. In this paper, we dramatically improve upon the
known upper bounds for generalised colouring numbers for graphs excluding a
fixed minor, from the exponential bounds of Grohe \emph{et al.}\ to a linear
bound for the $r$-colouring number $\mathrm{col}_r$ and a polynomial bound for
the weak $r$-colouring number $\mathrm{wcol}_r$. In particular, we show that if
$G$ excludes $K_t$ as a minor, for some fixed $t\ge4$, then
$\mathrm{col}_r(G)\le\binom{t-1}{2}\,(2r+1)$ and
$\mathrm{wcol}_r(G)\le\binom{r+t-2}{t-2}\,(t-3)(2r+1)\in\mathcal{O}(r^{\,t-1})$.
In the case of graphs $G$ of bounded genus $g$, we improve the bounds to
$\mathrm{col}_r(G)\le(2g+3)(2r+1)$ (and even $\mathrm{col}_r(G)\le5r+1$ if
$g=0$, i.e.\ if $G$ is planar) and
$\mathrm{wcol}_r(G)\le\Bigl(2g+\binom{r+2}{2}\Bigr)\,(2r+1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09061</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09061</id><created>2016-02-29</created><authors><author><keyname>Charatonik</keyname><forenames>Witold</forenames></author><author><keyname>Witkowski</keyname><forenames>Piotr</forenames></author></authors><title>Bounded Model Checking of Pointer Programs Revisited</title><categories>cs.LO cs.PL</categories><msc-class>F.3.1</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded model checking of pointer programs is a debugging technique for
programs that manipulate dynamically allocated pointer structures on the heap.
It is based on the following four observations. First, error conditions like
dereference of a dangling pointer, are expressible in a~fragment of first-order
logic with two-variables. Second, the fragment is closed under weakest
preconditions wrt. finite paths. Third, data structures like trees, lists etc.
are expressible by inductive predicates defined in a fragment of Datalog.
Finally, the combination of the two fragments of the two-variable logic and
Datalog is decidable.
  In this paper we improve this technique by extending the expressivity of the
underlying logics. In a~sequence of examples we demonstrate that the new logic
is capable of modeling more sophisticated data structures with more complex
dependencies on heaps and more complex analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09065</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09065</id><created>2016-02-29</created><authors><author><keyname>Gattupalli</keyname><forenames>Srujana</forenames></author><author><keyname>Ghaderi</keyname><forenames>Amir</forenames></author><author><keyname>Athitsos</keyname><forenames>Vassilis</forenames></author></authors><title>Evaluation of Deep Learning based Pose Estimation for Sign Language</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human body pose estimation and hand detection being the prerequisites for
sign language recognition(SLR), are both crucial and challenging tasks in
Computer Vision and Machine Learning. There are many algorithms to accomplish
these tasks for which the performance measures need to be evaluated for body
posture recognition on a sign language dataset, that would serve as a baseline
to provide important non-manual features for SLR. In this paper, we propose a
dataset for human pose estimation for SLR domain. On the other hand, deep
learning is on the edge of the computer science and obtains the
state-of-the-art results in almost every area of Computer Vision. Our main
contribution is to evaluate performance of deep learning based pose estimation
methods by performing user-independent experiments on our dataset. We also
perform transfer learning on these methods for which the results show huge
improvement and demonstrate that transfer learning can help improvement on pose
estimation performance of a method through the transferred knowledge from
another trained model. The dataset and results from these methods can create a
good baseline for future works and help gain significant amount of information
beneficial for SLR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09067</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09067</id><created>2016-02-29</created><authors><author><keyname>Madaio</keyname><forenames>Michael</forenames></author><author><keyname>Chen</keyname><forenames>Shang-Tse</forenames></author><author><keyname>Haimson</keyname><forenames>Oliver L.</forenames></author><author><keyname>Zhang</keyname><forenames>Wenwen</forenames></author><author><keyname>Cheng</keyname><forenames>Xiang</forenames></author><author><keyname>Hinds-Aldrich</keyname><forenames>Matthew</forenames></author><author><keyname>Chau</keyname><forenames>Duen Horng</forenames></author><author><keyname>Dilkina</keyname><forenames>Bistra</forenames></author></authors><title>Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in
  Atlanta</title><categories>cs.CY</categories><comments>10 pages, 4 figures, submitted to KDD 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Atlanta Fire Rescue Department (AFRD), like many municipal fire
departments, actively works to reduce fire risk by inspecting commercial
properties for potential hazards and fire code violations. However, AFRD's fire
inspection practices relied on tradition and intuition, with no existing
data-driven process for prioritizing fire inspections or identifying new
properties requiring inspection. In collaboration with AFRD, we developed the
Firebird framework to help municipal fire departments identify and prioritize
commercial property fire inspections, using machine learning, geocoding, and
information visualization. Firebird computes fire risk scores for over 5,000
buildings in the city, with true positive rates of up to 71% in predicting
fires. It has identified 6,096 new potential commercial properties to inspect,
based on AFRD's criteria for inspection. Furthermore, through an interactive
map, Firebird integrates and visualizes fire incidents, property information
and risk scores to help AFRD make informed decisions about fire inspections.
Firebird has already begun to make positive impact at both local and national
levels. It is improving AFRD's inspection processes and Atlanta residents'
safety, and was highlighted by National Fire Protection Association (NFPA) as a
best practice for using data to inform fire inspections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09069</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09069</id><created>2016-02-29</created><authors><author><keyname>Garrison</keyname><forenames>William C.</forenames><suffix>III</suffix></author><author><keyname>Shull</keyname><forenames>Adam</forenames></author><author><keyname>Lee</keyname><forenames>Adam J.</forenames></author><author><keyname>Myers</keyname><forenames>Steven</forenames></author></authors><title>Dynamic and Private Cryptographic Access Control for Untrusted Clouds:
  Costs and Constructions (Extended Version)</title><categories>cs.CR</categories><comments>21 pages; WIP extended version of the IEEE S&amp;P paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to enforce robust and dynamic access controls on cloud-hosted
data while simultaneously ensuring confidentiality with respect to the cloud
itself is a clear goal for many users and organizations. To this end, there has
been much cryptographic research proposing the use of (H)IBE, ABE, PE, and FE
and related technologies to perform robust and private access control on
untrusted cloud providers. However, the vast majority of this work studies
static models, in which the access control policies being enforced do not
change over time. This is contrary to the needs of most practical applications,
which leverage dynamic data and/or policies.
  In this paper, we explore the viability and costs associated with adapting
the above types of cryptosystems to perform dynamic and private access control
on the cloud using a threat model commonly assumed in the cryptographic
literature. Specifically, we develop lightweight IBE/IBS- and PKI-based
constructions for cryptographically enforcing $\mathsf{RBAC}_0$ access controls
over files hosted by a cloud storage provider. We prove the correctness of
these constructions and leverage real-world $\mathsf{RBAC}$ datasets and recent
techniques developed by the access control community to experimentally analyze
their associated cryptographic costs. Although IBE/IBS and PKI systems are a
natural fit for enforcing static $\mathsf{RBAC}$ policies, we show that
supporting revocation, update, and other state change functionality incurs
significant overheads in realistic scenarios. We identify a number of
bottlenecks in such systems, and fruitful areas for future work that will lead
to more natural and efficient constructions for cryptographic enforcement of
dynamic access controls. The majority of our findings also extend to similar
attempts to use HIBE, ABE, and PE schemes to enforce dynamic $\mathsf{RBAC}_1$
or $\mathsf{ABAC}$ policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09070</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09070</id><created>2016-02-29</created><authors><author><keyname>Simini</keyname><forenames>Filippo</forenames></author></authors><title>Nanoscale artificial intelligence: creating artificial neural networks
  using autocatalytic reactions</title><categories>nlin.AO cs.ET</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general methodology is proposed to engineer a system of interacting
components (particles) which is able to self-regulate their concentrations in
order to produce any prescribed output in response to a particular input. The
methodology is based on the mathematical equivalence between artificial neurons
in neural networks and species in autocatalytic reactions, and it specifies the
relationship between the artificial neural network's parameters and the rate
coefficients of the reactions between particle species. Such systems are
characterised by a high degree of robustness as they are able to reach the
desired output despite disturbances and perturbations of the concentrations of
the various species.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09076</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09076</id><created>2016-02-29</created><authors><author><keyname>Campigotto</keyname><forenames>Paolo</forenames></author><author><keyname>Rudloff</keyname><forenames>Christian</forenames></author><author><keyname>Leodolter</keyname><forenames>Maximilian</forenames></author><author><keyname>Bauer</keyname><forenames>Dietmar</forenames></author></authors><title>Personalized and situation-aware multimodal route recommendations: the
  FAVOUR algorithm</title><categories>cs.AI</categories><comments>12 pages, 6 figures, 1 table. Submitted to IEEE Transactions on
  Intelligent Transportation Systems journal for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Route choice in multimodal networks shows a considerable variation between
different individuals as well as the current situational context.
Personalization of recommendation algorithms are already common in many areas,
e.g., online retail. However, most online routing applications still provide
shortest distance or shortest travel-time routes only, neglecting individual
preferences as well as the current situation. Both aspects are of particular
importance in a multimodal setting as attractivity of some transportation modes
such as biking crucially depends on personal characteristics and exogenous
factors like the weather. This paper introduces the FAVourite rOUte
Recommendation (FAVOUR) approach to provide personalized, situation-aware route
proposals based on three steps: first, at the initialization stage, the user
provides limited information (home location, work place, mobility options,
sociodemographics) used to select one out of a small number of initial
profiles. Second, based on this information, a stated preference survey is
designed in order to sharpen the profile. In this step a mass preference prior
is used to encode the prior knowledge on preferences from the class identified
in step one. And third, subsequently the profile is continuously updated during
usage of the routing services. The last two steps use Bayesian learning
techniques in order to incorporate information from all contributing
individuals. The FAVOUR approach is presented in detail and tested on a small
number of survey participants. The experimental results on this real-world
dataset show that FAVOUR generates better-quality recommendations w.r.t.
alternative learning algorithms from the literature. In particular the
definition of the mass preference prior for initialization of step two is shown
to provide better predictions than a number of alternatives from the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09088</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09088</id><created>2016-02-29</created><authors><author><keyname>Br&#xe2;nzei</keyname><forenames>Simina</forenames></author><author><keyname>Lv</keyname><forenames>Yuezhou</forenames></author><author><keyname>Mehta</keyname><forenames>Ruta</forenames></author></authors><title>To Give or not to Give: Fair Division for Strict Preferences</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single minded agents have strict preferences, in which a bundle is acceptable
only if it meets a certain demand. Such preferences arise naturally in
scenarios such as allocating computational resources among users, where the
goal is to fairly serve as many requests as possible. In this paper we study
the fair division problem for such agents, which is harder to handle due to
discontinuity and complementarities of the preferences.
  Our solution concept---the competitive allocation from equal incomes
(CAEI)---is inspired from market equilibria and implements fair outcomes
through a pricing mechanism. We study the existence and computation of CAEI for
multiple divisible goods, cake cutting, and multiple discrete goods. For the
first two scenarios we show that existence of CAEI solutions is guaranteed,
while for the third we give a succinct characterization of instances that admit
this solution; then we give an efficient algorithm to find one in all three
cases. Maximizing social welfare turns out to be NP-hard in general, however we
obtain efficient algorithms for (i) divisible and discrete goods when the
number of different \emph{types} of players is a constant, (ii) cake cutting
with contiguous demands and (iii) cake cutting with a constant number of
players with arbitrary demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09102</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09102</id><created>2016-02-29</created><authors><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author><author><keyname>Klein</keyname><forenames>Martin</forenames></author><author><keyname>Jones</keyname><forenames>Shawn M.</forenames></author></authors><title>Persistent URIs Must Be Used To Be Persistent</title><categories>cs.DL</categories><comments>2 pages, 2 figures, accepted for publication at WWW 2016 (poster
  track)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We quantify the extent to which references to papers in scholarly literature
use persistent HTTP URIs that leverage the Digital Object Identifier
infrastructure. We find a significant number of references that do not,
speculate why authors would use brittle URIs when persistent ones are
available, and propose an approach to alleviate the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09104</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09104</id><created>2016-02-29</created><authors><author><keyname>Derakhshani</keyname><forenames>Mahsa</forenames></author><author><keyname>Parsaeefard</keyname><forenames>Saeedeh</forenames></author><author><keyname>Le-Ngoc</keyname><forenames>Tho</forenames></author><author><keyname>Leon-Garcia</keyname><forenames>Alberto</forenames></author></authors><title>Leveraging Synergy of 5G SDWN and Multi-Layer Resource Management for
  Network Optimization</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fifth-generation (5G) cellular wireless networks are envisioned to predispose
service-oriented, flexible, and spectrum/energy-efficient edge-to-core
infrastructure, aiming to offer diverse applications. Convergence of
software-defined networking (SDN), software-defined radio (SDR) compatible with
multiple radio access technologies (RATs), and virtualization on the concept of
5G software-defined wireless networking (5G-SDWN) is a promising approach to
provide such a dynamic network. The principal technique behind the 5G-SDWN
framework is the separation of the control and data planes, from the deep core
entities to edge wireless access points (APs). This separation allows the
abstraction of resources as transmission parameters of each user over the
5G-SDWN. In this user-centric and service-oriented environment, resource
management plays a critical role to achieve efficiency and reliability.
However, it is natural to wonder if 5G-SDWN can be leveraged to enable
converged multi-layer resource management over the portfolio of resources, and
reciprocally, if CML resource management can effectively provide performance
enhancement and reliability for 5G-SDWN. We believe that replying to these
questions and investigating this mutual synergy are not trivial, but
multidimensional and complex for 5G-SDWN, which consists of different
technologies and also inherits legacy generations of wireless networks. In this
paper, we propose a flexible protocol structure based on three mentioned
pillars for 5G-SDWN, which can handle all the required functionalities in a
more crosslayer manner. Based on this, we demonstrate how the general framework
of CML resource management can control the end user quality of experience. For
two scenarios of 5G-SDWN, we investigate the effects of joint user-association
and resource allocation via CML resource management to improve performance in a
virtualized network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09107</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09107</id><created>2016-02-29</created><authors><author><keyname>Hrab&#xe1;k</keyname><forenames>Pavel</forenames></author><author><keyname>Tich&#xe1;&#x10d;ek</keyname><forenames>Ond&#x159;ej</forenames></author><author><keyname>Se&#x10d;k&#xe1;rov&#xe1;</keyname><forenames>Vladim&#xed;ra</forenames></author></authors><title>Estimation of Discretized Motion of Pedestrians by the Decision-Making
  Model</title><categories>cs.MA cs.IT math.IT</categories><comments>Conference Traffic and Granullar Flow 2015, to be published by
  Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The contribution gives a micro-structural insight into the pedestrian
decision process during an egress situation. A method how to extract the
decisions of pedestrians from the trajectories recorded during the experiments
is introduced. The underlying Markov decision process is estimated using the
finite mixture approximation. Furthermore, the results of this estimation can
be used as an input to the optimization of a Markov decision process for one
`clever' agent. This agent optimizes his strategy of motion with respect to
different reward functions, minimizing the time spent in the room or minimizing
the amount of inhaled CO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09111</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09111</id><created>2016-02-29</created><authors><author><keyname>Benmedjdoub</keyname><forenames>Brahim</forenames><affiliation>L'IFORCE</affiliation></author><author><keyname>Sopena</keyname><forenames>Eric</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Bouchemakh</keyname><forenames>Isma</forenames><affiliation>L'IFORCE</affiliation></author></authors><title>2-Distance Colorings of Integer Distance Graphs</title><categories>cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 2-distance k-coloring of a graph G is a mapping from V (G) to the set of
colors {1,. .. , k} such that every two vertices at distance at most 2 receive
distinct colors. The 2-distance chromatic number $\chi$ 2 (G) of G is then the
mallest k for which G admits a 2-distance k-coloring. For any finite set of
positive integers D = {d 1 ,. .. , d k }, the integer distance graph G = G(D)
is the infinite graph defined by V (G) = Z and uv $\in$ E(G) if and only if |v
-- u| $\in$ D. We study the 2-distance chromatic number of integer distance
graphs for several types of sets D. In each case, we provide exact values or
upper bounds on this parameter and characterize those graphs G(D) with $\chi$ 2
(G(D)) = {\Delta}(G(D)) + 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09115</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09115</id><created>2016-02-29</created><authors><author><keyname>Goyal</keyname><forenames>Sanjay</forenames></author><author><keyname>Galiotto</keyname><forenames>Carlo</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra</forenames></author></authors><title>Throughput and Coverage for a Mixed Full and Half Duplex Small Cell
  Network</title><categories>cs.IT math.IT</categories><comments>7 Pages, Conference, accepted in ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in self-interference cancellation enable radios to transmit
and receive on the same frequency at the same time. Such a full duplex radio is
being considered as a potential candidate for the next generation of wireless
networks due to its ability to increase the spectral efficiency of wireless
systems. In this paper, the performance of full duplex radio in small cellular
systems is analyzed by assuming full duplex capable base stations and half
duplex user equipment. However, using only full duplex base stations increases
interference leading to outage. We therefore propose a mixed multi-cell system,
composed of full duplex and half duplex cells. A stochastic geometry based
model of the proposed mixed system is provided, which allows us to derive the
outage and area spectral efficiency of such a system. The effect of full duplex
cells on the performance of the mixed system is presented under different
network parameter settings. We show that the fraction of cells that have full
duplex base stations can be used as a design parameter by the network operator
to target an optimal tradeoff between area spectral efficiency and outage in a
mixed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09118</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09118</id><created>2016-02-29</created><authors><author><keyname>Achiam</keyname><forenames>Joshua</forenames></author></authors><title>Easy Monotonic Policy Iteration</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key problem in reinforcement learning for control with general function
approximators (such as deep neural networks and other nonlinear functions) is
that, for many algorithms employed in practice, updates to the policy or
$Q$-function may fail to improve performance---or worse, actually cause the
policy performance to degrade. Prior work has addressed this for policy
iteration by deriving tight policy improvement bounds; by optimizing the lower
bound on policy improvement, a better policy is guaranteed. However, existing
approaches suffer from bounds that are hard to optimize in practice because
they include sup norm terms which cannot be efficiently estimated or
differentiated. In this work, we derive a better policy improvement bound where
the sup norm of the policy divergence has been replaced with an average
divergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that
generates sequences of policies with guaranteed non-decreasing returns and is
easy to implement in a sample-based framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09120</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09120</id><created>2016-02-29</created><authors><author><keyname>Gerstoft</keyname><forenames>Peter</forenames></author><author><keyname>Mecklenbr&#xe4;uker</keyname><forenames>Christoph F.</forenames></author><author><keyname>Xenaki</keyname><forenames>Angeliki</forenames></author></authors><title>Multi Snapshot Sparse Bayesian Learning for DOA Estimation</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>submitted to Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The directions of arrival (DOA) of plane waves are estimated from
multi-snapshot sensor array data using Sparse Bayesian Learning (SBL). The
prior source amplitudes is assumed independent zero-mean complex Gaussian
distributed with hyperparameters the unknown variances (i.e. the source
powers). For a complex Gaussian likelihood with hyperparameter the unknown
noise variance, the corresponding Gaussian posterior distribution is derived.
For a given number of DOAs, the hyperparameters are automatically selected by
maximizing the evidence and promote sparse DOA estimates. The SBL scheme for
DOA estimation is discussed and evaluated competitively against LASSO
($\ell_1$-regularization), conventional beamforming, and MUSIC
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09123</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09123</id><created>2016-02-29</created><authors><author><keyname>Shuai</keyname><forenames>Xin</forenames></author><author><keyname>Moulinier</keyname><forenames>Isabelle</forenames></author><author><keyname>Rollins</keyname><forenames>Jason</forenames></author><author><keyname>Custis</keyname><forenames>Tonya</forenames></author><author><keyname>Schilder</keyname><forenames>Frank</forenames></author><author><keyname>Edmunds</keyname><forenames>Mathilda</forenames></author></authors><title>A Multi-dimensional Investigation of the Effects of Publication
  Retraction on Scholarly Impact</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>18 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few decades, the rate of publication retractions has increased
dramatically in academia. In this study, we investigate retractions from a
quantitative perspective, aiming to answer two fundamental questions. One, how
do retractions influence the scholarly impact of retracted papers, authors, and
institutions? Two, does this influence propagate to the wider academic
community through scholarly associations? Specifically, we analyzed a set of
retracted articles indexed in Thomson Reuters Web of Science (WoS), and ran
multiple experiments to compare changes in scholarly impact against a control
set of non-retracted articles, authors, and institutions. We further applied
the Granger Causality test to investigate whether different scientific topics
are dynamically affected by retracted papers occurring within those topics. Our
results show two key findings: first, the scholarly impact of retracted papers
and authors significantly decreases after retraction, and the most severe
impact decrease correlates to retractions based on proven purposeful scientific
misconduct; second, this retraction penalty does not seem to spread through the
broader scholarly social graph, but instead has a limited and localized effect.
Our findings may provide useful insights for scholars or science committees to
evaluate the scholarly value of papers, authors, or institutions related to
retractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09124</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09124</id><created>2016-02-29</created><authors><author><keyname>Lozin</keyname><forenames>Vadim</forenames></author><author><keyname>Mosca</keyname><forenames>Raffaele</forenames></author><author><keyname>Zamaraev</keyname><forenames>Viktor</forenames></author></authors><title>Independent Domination versus Weighted Independent Domination</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independent domination is one of the rare problems for which the complexity
of weighted and unweighted versions is known to be different in some classes of
graphs. In the present paper, we prove two NP-hardness results, one for the
weighted version and one for unweighted, which tighten the gap between them. We
also prove that both versions of the problem can be solved in polynomial time
for $(P_5,\bar{P}_5)$-free graphs generalizing some of the previously known
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09125</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09125</id><created>2016-02-18</created><authors><author><keyname>Liu</keyname><forenames>Xuanzhe</forenames></author><author><keyname>Xu</keyname><forenames>Mengwei</forenames></author><author><keyname>Huang</keyname><forenames>Gang</forenames></author><author><keyname>Teng</keyname><forenames>Teng</forenames></author><author><keyname>Zheng</keyname><forenames>Zibin</forenames></author><author><keyname>Mei</keyname><forenames>Hong</forenames></author></authors><title>MUIT: A Middleware for Adaptive Mobile Web-based User Interfaces in
  WS-BPEL</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In enterprise organizations, the Bring-Your-Own-Device (BYOD) requirement has
become prevalent as employees use their own mobile devices to process the
workflow-oriented tasks. Consequently, it calls for approaches that can quickly
develop and integrate mobile user interactions into existing business
processes, and adapt to various contexts. However, designing, developing and
deploying adaptive and mobile-oriented user interfaces for existing process
engines are non-trivial, and require significant systematic efforts. To address
this issue, we present a novel middleware-based approach, called MUIT, to
developing and deploying the Mobility, User Interactions and Tasks into WS-BPEL
engines. MUIT can be seamlessly into WS-BPEL without intrusions of existing
process instances. MUIT provides a Domain-Specific Language (DSL) that provides
some intuitive APIs to support the declarative development of adaptive,
mobile-oriented, and Web-based user interfaces in WS-BPEL. The DSL can
significantly improve the development of user interactions by preventing
arbitrarily mixed codes, and its runtime supports satisfactory user
experiences. We implement a proof- of-concept prototype by integrating MUIT
into the commodity WS-BPEL-based Apusic Platform, and evaluate the performance
and usability of MUIT platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09127</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09127</id><created>2016-02-29</created><authors><author><keyname>Deniz</keyname><forenames>Zakir</forenames></author><author><keyname>Ekim</keyname><forenames>T&#x131;naz</forenames></author></authors><title>Stable Equimatchable Graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is \emph{equimatchable} if every maximal matching of $G$ has the
same cardinality. We are interested in equimatchable graphs such that the
removal of any edge from the graph preserves the equimatchability. We call an
equimatchable graph $G$ \emph{edge-stable} if $G\setminus {e}$ is equimatchable
for any $e \in E(G)$. After noticing that edge-stable equimatchable graphs are
either 2-connected factor-critical or bipartite, we characterize edge-stable
equimatchable graphs. This characterization yields an $O(\min(n^{3.376},
n^{1.5}m))$ time recognition algorithm. We also define \emph{vertex-stable}
equimatchable graphs and show that they admit a simpler characterization.
Lastly, we introduce and shortly discuss the related notions of
\emph{edge-critical} and \emph{vertex-critical} equimatchable graphs, pointing
out the most interesting case in their characterization as an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09130</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09130</id><created>2016-02-29</created><authors><author><keyname>Singh</keyname><forenames>Abhineet</forenames></author><author><keyname>Jagersand</keyname><forenames>Martin</forenames></author></authors><title>Modular Tracking Framework: A Unified Approach to Registration based
  Tracking</title><categories>cs.CV cs.RO</categories><comments>Submitted to CRV 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a modular, extensible and highly efficient open source
framework for registration based tracking. It is implemented entirely in C++
and is designed from the ground up to easily integrate with systems that
support any of several major vision and robotics libraries including
{http://opencv.org/}{OpenCV}, {http://www.ros.org/}{ROS} and
{http://eigen.tuxfamily.org/}{Eigen}. In addition, while laying the theoretical
foundation for the design of the system, we introduce a new way to study
registration based trackers by decomposing them into three constituent sub
modules - appearance model, state space model and search method. We also extend
the unifying formulation described in \cite{Baker04lucasKanade_paper} to
account for several important advances in the field since then.
  In addition to being a practical solution for fast and high precision
tracking, this system can also serve as a useful research tool by allowing
existing and new methods for any of the aforementioned sub modules to be
studied better. When a new method for one of these sub modules is introduced in
literature, this breakdown can help to experimentally find the combination of
methods for the other sub modules that is optimum for it while also allowing
more comprehensive comparisons with existing methods to understand its
contributions better.
  By extensive use of generic programming, the system makes it easy to plug in
a new method for any of the sub modules so that it can not only be tested with
existing methods for other sub modules but also become immediately available
for deployment in any system that uses the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09134</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09134</id><created>2016-02-29</created><authors><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>The Capacity of Private Information Retrieval</title><categories>cs.IT cs.CR cs.IR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the private information retrieval (PIR) problem a user wishes to retrieve,
as efficiently as possible, one out of $K$ messages from $N$ non-communicating
databases (each holds all $K$ messages) while revealing nothing about the
identity of the desired message index to any individual database. The
information theoretic capacity of PIR is the maximum number of bits of desired
information that can be privately retrieved per bit of downloaded information.
For $K$ messages and $N$ databases, we show that the PIR capacity is
$(1+1/N+1/N^2+\cdots+1/N^{K-1})^{-1}$. A remarkable feature of the capacity
achieving scheme is that if it is projected onto any subset of messages by
eliminating the remaining messages, it also achieves the PIR capacity for that
subset of messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.09140</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.09140</id><created>2016-02-29</created><authors><author><keyname>Pacher</keyname><forenames>Christoph</forenames></author><author><keyname>Martinez-Mateo</keyname><forenames>Jesus</forenames></author><author><keyname>Duhme</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Gehring</keyname><forenames>Tobias</forenames></author><author><keyname>Furrer</keyname><forenames>Fabian</forenames></author></authors><title>Information Reconciliation for Continuous-Variable Quantum Key
  Distribution using Non-Binary Low-Density Parity-Check Codes</title><categories>quant-ph cs.IT math.IT</categories><comments>23 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An information reconciliation method for continuous-variable quantum key
distribution with Gaussian modulation that is based on non-binary low-density
parity-check (LDPC) codes is presented. Sets of regular and irregular LDPC
codes with different code rates over the Galois fields $GF(8)$, $GF(16)$,
$GF(32)$, and $GF(64)$ have been constructed. We have performed simulations to
analyze the efficiency and the frame error rate using the sum-product
algorithm. The proposed method achieves an efficiency between $0.94$ and $0.98$
if the signal-to-noise ratio is between $4$ dB and $24$ dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00001</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00001</id><created>2016-02-29</created><authors><author><keyname>Wessing</keyname><forenames>Simon</forenames></author></authors><title>Towards a Systematic Development Process of Optimization Methods</title><categories>math.OC cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ultimate goal of all optimization methods is to solve real-world
problems. For a successful project execution, knowledge about optimization and
the application has to be pooled. As it is too inefficient to highly train one
person in both fields, a team of experts is usually put together.
Unfortunately, communication errors must be expected when several people
collaborate. In this work, we deal with the avoidance and the repair of these
communication errors. The tools proposed in this regard are, among others, the
algorithm engineering cycle, checklists for structuring communication, and
knowledge databases. The discussion is enriched with examples from continuous
optimization, but most tools have a much wider applicability, even beyond
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00002</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00002</id><created>2016-02-26</created><authors><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author></authors><title>The Fourier structure of low degree polynomials</title><categories>math.CO cs.CC math.CA</categories><msc-class>12E05, 05E40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure of the Fourier coefficients of low degree multivariate
polynomials over finite fields. We consider three properties: (i) the number of
nonzero Fourier coefficients; (ii) the sum of the absolute value of the Fourier
coefficients; and (iii) the size of the linear subspace spanned by the nonzero
Fourier coefficients. For quadratic polynomials, tight relations are known
between all three quantities. In this work, we extend this relation to higher
degree polynomials. Specifically, for degree $d$ polynomials, we show that the
three quantities are equivalent up to factors exponential in $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00040</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00040</id><created>2016-02-29</created><authors><author><keyname>Block</keyname><forenames>Kai Tobias</forenames></author><author><keyname>Uecker</keyname><forenames>Martin</forenames></author><author><keyname>Frahm</keyname><forenames>Jens</forenames></author></authors><title>Model-Based Iterative Reconstruction for Radial Fast Spin-Echo MRI</title><categories>physics.med-ph cs.CE math.OC</categories><comments>25 pages, 6 figures, 2 tables. Copyright 2009 IEEE</comments><journal-ref>IEEE Transactions on Medical Imaging 28:1759-1769 (2009)</journal-ref><doi>10.1109/TMI.2009.2023119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In radial fast spin-echo MRI, a set of overlapping spokes with an
inconsistent T2 weighting is acquired, which results in an averaged image
contrast when employing conventional image reconstruction techniques. This work
demonstrates that the problem may be overcome with the use of a dedicated
reconstruction method that further allows for T2 quantification by extracting
the embedded relaxation information. Thus, the proposed reconstruction method
directly yields a spin-density and relaxivity map from only a single radial
data set. The method is based on an inverse formulation of the problem and
involves a modeling of the received MRI signal. Because the solution is found
by numerical optimization, the approach exploits all data acquired. Further, it
handles multi-coil data and optionally allows for the incorporation of
additional prior knowledge. Simulations and experimental results for a phantom
and human brain in vivo demonstrate that the method yields spin-density and
relaxivity maps that are neither affected by the typical artifacts from TE
mixing, nor by streaking artifacts from the incomplete k-space coverage at
individual echo times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00048</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00048</id><created>2016-02-07</created><authors><author><keyname>Gurzadyan</keyname><forenames>A. V.</forenames></author><author><keyname>Allahverdyan</keyname><forenames>A. E.</forenames></author></authors><title>Non-random structures in universal compression and the Fermi paradox</title><categories>astro-ph.IM cs.IT math.IT physics.data-an</categories><comments>6 pages, 1 figure</comments><journal-ref>European Physical J. Plus (2016) 131: 26</journal-ref><doi>10.1140/epjp/i2016-16026-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the hypothesis of information panspermia assigned recently among
possible solutions of the Fermi paradox (&quot;where are the aliens?&quot;). It suggests
that the expenses of alien signaling can be significantly reduced, if their
messages contain compressed information. To this end we consider universal
compression and decoding mechanisms (e.g. the Lempel-Ziv-Welch algorithm) that
can reveal non-random structures in compressed bit strings. The efficiency of
Kolmogorov stochasticity parameter for detection of non-randomness is
illustrated, along with the Zipf's law. The universality of these methods, i.e.
independence on data details, can be principal in searching for intelligent
messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00050</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00050</id><created>2016-02-21</created><authors><author><keyname>Healey</keyname><forenames>Glenn</forenames></author></authors><title>Learning, Visualizing, and Exploiting a Model for the Intrinsic Value of
  a Batted Ball</title><categories>stat.AP cs.LG</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for learning the intrinsic value of a batted ball in
baseball. This work addresses the fundamental problem of separating the value
of a batted ball at contact from factors such as the defense, weather, and
ballpark that can affect its observed outcome. The algorithm uses a Bayesian
model to construct a continuous mapping from a vector of batted ball parameters
to an intrinsic measure defined as the expected value of a linear weights
representation for run value. A kernel method is used to build nonparametric
estimates for the component probability density functions in Bayes theorem from
a set of over one hundred thousand batted ball measurements recorded by the
HITf/x system during the 2014 major league baseball (MLB) season.
Cross-validation is used to determine the optimal vector of smoothing
parameters for the density estimates. Properties of the mapping are visualized
by considering reduced-dimension subsets of the batted ball parameter space. We
use the mapping to derive statistics for intrinsic quality of contact for
batters and pitchers which have the potential to improve the accuracy of player
models and forecasting systems. We also show that the new approach leads to a
simple automated measure of contact-adjusted defense and provides insight into
the impact of environmental variables on batted balls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00059</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00059</id><created>2016-02-29</created><authors><author><keyname>Malmi</keyname><forenames>Eric</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>You Are What Apps You Use: Demographic Prediction Based on User's Apps</title><categories>cs.SI</categories><comments>This is a pre-print of an article appearing at ICWSM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the demographics of app users is crucial, for example, for app
developers, who wish to target their advertisements more effectively. Our work
addresses this need by studying the predictability of user demographics based
on the list of a user's apps which is readily available to many app developers.
We extend previous work on the problem on three frontiers: (1) We predict new
demographics (age, race, and income) and analyze the most informative apps for
four demographic attributes included in our analysis. The most predictable
attribute is gender (82.3 % accuracy), whereas the hardest to predict is income
(60.3 % accuracy). (2) We compare several dimensionality reduction methods for
high-dimensional app data, finding out that an unsupervised method yields
superior results compared to aggregating the apps at the app category level,
but the best results are obtained simply by the raw list of apps. (3) We look
into the effect of the training set size and the number of apps on the
predictability and show that both of these factors have a large impact on the
prediction accuracy. The predictability increases, or in other words, a user's
privacy decreases, the more apps the user has used, but somewhat surprisingly,
after 100 apps, the prediction accuracy starts to decrease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00060</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00060</id><created>2016-02-29</created><authors><author><keyname>Balas</keyname><forenames>Kevin</forenames></author><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>Anchored Rectangle and Square Packings</title><categories>cs.CG math.CO</categories><comments>33 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For points $p_1,\ldots , p_n$ in the unit square $[0,1]^2$, an \emph{anchored
rectangle packing} consists of interior-disjoint axis-aligned empty rectangles
$r_1,\ldots , r_n\subseteq [0,1]^2$ such that point $p_i$ is a corner of the
rectangle $r_i$ (that is, $r_i$ is \emph{anchored} at $p_i$) for $i=1,\ldots,
n$. We show that for every set of $n$ points in $[0,1]^2$, there is an anchored
rectangle packing of area at least $7/12-O(1/n)$, and for every $n\in
\mathbf{N}$, there are point sets for which the area of every anchored
rectangle packing is at most $2/3$. The maximum area of an anchored
\emph{square} packing is always at least $5/32$ and sometimes at most $7/27$.
  The above constructive lower bounds immediately yield constant-factor
approximations, of $7/12 -\varepsilon$ for rectangles and $5/32$ for squares,
for computing anchored packings of maximum area in $O(n\log n)$ time. We prove
that a simple greedy strategy achieves a $9/47$-approximation for anchored
square packings, and $1/3$ for lower-left anchored square packings. Reductions
to maximum weight independent set (MWIS) yield a QPTAS and a PTAS for anchored
rectangle and square packings in $n^{O(1/\varepsilon)}$ and $\exp({\rm
poly}(\log (n/\varepsilon)))$ time, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00074</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00074</id><created>2016-02-29</created><authors><author><keyname>Skaza</keyname><forenames>Jonathan</forenames></author><author><keyname>Blais</keyname><forenames>Brian</forenames></author></authors><title>Modeling the Infectiousness of Twitter Hashtags</title><categories>cs.SI q-bio.PE stat.AP</categories><msc-class>91D30, 92B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study applies dynamical and statistical modeling techniques to quantify
the proliferation and popularity of trending hashtags on Twitter. Using
time-series data reflecting actual tweets in New York City and San Francisco,
we present estimates for the dynamics (i.e., rates of infection and recovery)
of several hundred trending hashtags using an epidemic modeling framework
coupled with Bayesian Markov Chain Monte Carlo (MCMC) methods. This
methodological strategy is an extension of techniques traditionally used to
model the spread of infectious disease. We demonstrate that in some models,
hashtags can be grouped by infectiousness, possibly providing a method for
quantifying the trendiness of a topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00082</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00082</id><created>2016-02-29</created><updated>2016-03-02</updated><authors><author><keyname>Kompatscher</keyname><forenames>Michael</forenames></author><author><keyname>Van Pham</keyname><forenames>Trung</forenames></author></authors><title>A complexity dichotomy for poset constraint satisfaction</title><categories>cs.CC math.LO</categories><comments>30 pages</comments><msc-class>08A70, 03C05, 03C40 (primary), 06A07, 08A35 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we determine the complexity of a broad class of problems that
extends the temporal constraint satisfaction problems. To be more precise we
study the problems Poset-SAT($\Phi$), where $\Phi$ is a given set of
quantifier-free $\leq$-formulas. An instance of Poset-SAT($\Phi$) consists of
finitely many variables $x_1,\ldots,x_n$ and formulas
$\phi_i(x_{i_1},\ldots,x_{i_k})$ with $\phi_i \in \Phi$; the question is
whether this input is satisfied by any partial order on $x_1,\ldots,x_n$ or
not. We show that every such problem is NP-complete or can be solved in
polynomial time, depending on $\Phi$. All Poset-SAT problems can be formalized
as constraint satisfaction problems on reducts of the random partial order. We
use model-theoretic concepts and techniques from universal algebra to study
these reducts. In the course of this analysis we establish a dichotomy that we
believe is of independent interest in universal algebra and model theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00087</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00087</id><created>2016-02-29</created><authors><author><keyname>Santiago</keyname><forenames>Sonia</forenames></author><author><keyname>Escobar</keyname><forenames>Santiago</forenames></author><author><keyname>Meadows</keyname><forenames>Catherine</forenames></author><author><keyname>Meseguer</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>Effective Sequential Protocol Composition in Maude-NPA</title><categories>cs.CR cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protocols do not work alone, but together, one protocol relying on another to
provide needed services. Many of the problems in cryptographic protocols arise
when such composition is done incorrectly or is not well understood. In this
paper we discuss an extension to the Maude-NPA syntax and its operational
semantics to support dynamic sequential composition of protocols, so that
protocols can be specified separately and composed when desired. This allows
one to reason about many different compositions with minimal changes to the
specification, as well as improving, in terms of both performance and ease of
specification, on an earlier composition extension we presented in [18]. We
show how compositions can be defined and executed symbolically in Maude-NPA
using the compositional syntax and semantics. We also provide an experimental
analysis of the performance of Maude-NPA using the compositional syntax and
semantics, and compare it to the performance of a syntax and semantics for
composition developed in earlier research. Finally, in the conclusion we give
some lessons learned about the best ways of extending narrowing-based state
reachability tools, as well as comparison with related work and future plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00091</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00091</id><created>2016-02-29</created><authors><author><keyname>Calders</keyname><forenames>Toon</forenames></author><author><keyname>Van Assche</keyname><forenames>Dimitri</forenames></author></authors><title>PROMETHEE is Not Quadratic: An O(qn log(n)) Algorithm</title><categories>cs.DS cs.CC</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is generally believed that the preference ranking method PROMETHEE has a
quadratic time complexity. In this paper, however, we present an exact
algorithm that computes PROMETHEE's net flow scores in time O(qn log(n)), where
q represents the number of criteria and n the number of alternatives. The
method is based on first sorting the alternatives after which the unicriterion
flow scores of all alternatives can be computed in one scan over the sorted
list of alternatives while maintaining a sliding window. This method works with
the linear and level criterion preference functions. The algorithm we present
is exact and, due to the sub-quadratic time complexity, vastly extends the
applicability of the PROMETHEE method. Experiments show that with the new
algorithm, PROMETHEE can scale up to millions of tuples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00092</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00092</id><created>2016-02-29</created><authors><author><keyname>Thapa</keyname><forenames>Chandra</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>Interlinked Cycles for Index Coding: Generalizing Cycles and Cliques</title><categories>cs.IT math.IT</categories><comments>Submitted for journal publication. Part of the material in this paper
  was presented at the IEEE International Symposium on Information Theory,
  HongKong, Jun. 14-19, 2015, and at the IEEE Information Theory Workshop, Jeju
  Island, Korea, Oct. 11-15, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a graphical approach to index coding. While cycles have been
shown to provide coding gain, only disjoint cycles and cliques (a specific type
of overlapping cycles) have been exploited in existing literature. In this
paper, we define a more general form of overlapping cycles, called the
interlinked-cycle (IC) structure, that generalizes cycles and cliques. We
propose a scheme, called the interlinked-cycle-cover (ICC) scheme, that
leverages IC structures in digraphs to construct scalar linear index codes. We
characterize a class of infinitely many digraphs where our proposed scheme is
optimal over all linear and non-linear index codes. Consequently, for this
class of digraphs, we indirectly prove that scalar linear index codes are
optimal. Furthermore, we show that the ICC scheme can outperform all existing
graph-based schemes (including partial-clique-cover and
fractional-local-chromatic number schemes), and a random-coding scheme (namely,
composite coding) for certain graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00100</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00100</id><created>2016-02-29</created><authors><author><keyname>Kaminaga</keyname><forenames>Masahiro</forenames></author><author><keyname>Yoshikawa</keyname><forenames>Hideki</forenames></author><author><keyname>Shikoda</keyname><forenames>Arimitsu</forenames></author><author><keyname>Suzuki</keyname><forenames>Toshinori</forenames></author></authors><title>Crashing Modulus Attack on Modular Squaring for Rabin Cryptosystem</title><categories>cs.CR</categories><comments>18 pages, 2 figures</comments><msc-class>94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rabin cryptosystem has been proposed protect the unique ID (UID) in
radio-frequency identification tags. The Rabin cryptosystem is a type of
lightweight public key system that is theoretetically quite secure; however it
is vulnerable to several side-channel attacks. In this paper, a crashing
modulus attack is presented as a new fault attack on modular squaring during
Rabin encryption. This attack requires only one fault in the public key if its
perturbed public key can be factored. Our simulation results indicate that the
attack is more than 50\% successful with several faults in practical time. A
complicated situation arises when reconstrucing the message, including the UID,
from ciphertext, i.e., the message and the perturbed public key are not
relatively prime. We present a complete and mathematically rigorous message
reconstruction algorithm for such a case. Moreover, we propose an exact formula
to obtain a number of candidate messages. We show that the number is not
generally equal to a power of two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00104</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00104</id><created>2016-02-29</created><authors><author><keyname>Sawyer</keyname><forenames>Nicole</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author></authors><title>Dynamic Stackelberg User-in-the-Loop Repeated Game for Device-to-Device
  Communications</title><categories>cs.NI</categories><comments>11 pages, 8 figures, 4 tables, submitted to IEEE Transactions on
  Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a dynamic Stackelberg repeated game for decentralized
Device-to-Device (D2D) communications overlaying cellular communications. The
proposed game aims to jointly optimize D2D user behavior via a user-in-the-loop
(UIL) approach; and minimize transmit power, while maximizing packet delivery
ratio (PDR) for all D2D users. The Stackelberg game architecture consists of a
single leader (base station - BS) and multiple followers (D2D users), where the
leader selects its satisfaction with the social welfare of the D2D users by
considering the reaction of the followers from the charged price it assigns.
The followers on the other hand select transmit power so as to guarantee a
certain level of quality-of-experience (QoE) for all players. We express D2D
user behavior as: (i) low data requirement; (ii) medium data requirement; and
(iii) high data requirement. Therefore, each type of data requirement is
assigned different utility functions due to the performance demands of D2D
users. Both the leader and followers aim to maximize their utility function
within the Stackelberg game. We prove that there exists an overall Stackelberg
Equilibrium for our proposed game for all players, which is a sub-game perfect
equilibrium, as the equilibrium achieved at each stage is independent of the
game history. Our simulation results illustrate that once the leader rapidly
converges to an optimal outcome with respect to satisfaction, the followers
then rapidly converge to a Pareto-efficient outcome with respect to transmit
power and PDR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00106</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00106</id><created>2016-02-29</created><authors><author><keyname>Ghosh</keyname><forenames>Saurav</forenames></author><author><keyname>Chakraborty</keyname><forenames>Prithwish</forenames></author><author><keyname>Cohn</keyname><forenames>Emily</forenames></author><author><keyname>Brownstein</keyname><forenames>John S.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Designing Domain Specific Word Embeddings: Applications to Disease
  Surveillance</title><categories>cs.LG cs.CL stat.ML</categories><comments>this paper has been submitted to a conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional disease surveillance can be augmented with a wide variety of
realtime sources such as news and social media. However, these sources are in
general unstructured and construction of surveillance tools such as taxonomical
correlations and trace mapping involves considerable human supervision. In this
paper, we motivate a disease vocabulary driven word2vec model (Dis2Vec) which
we use to model diseases and constituent attributes as word embeddings from the
HealthMap news corpus. We use these word embeddings to create disease
taxonomies and evaluate our model accuracy against human annotated taxonomies.
We compare our accuracies against several state-of-the art word2vec methods.
Our results demonstrate that Dis2Vec outperforms traditional distributed vector
representations in its ability to faithfully capture disease attributes and
accurately forecast outbreaks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00110</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00110</id><created>2016-02-29</created><authors><author><keyname>Ji</keyname><forenames>Pan</forenames></author><author><keyname>Li</keyname><forenames>Hongdong</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Zhong</keyname><forenames>Yiran</forenames></author></authors><title>Robust Multi-body Feature Tracker: A Segmentation-free Approach</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature tracking is a fundamental problem in computer vision, with
applications in many computer vision tasks, such as visual SLAM and action
recognition. This paper introduces a novel multi-body feature tracker that
exploits a multi-body rigidity assumption to improve tracking robustness under
a general perspective camera model. A conventional approach to addressing this
problem would consist of alternating between solving two subtasks: motion
segmentation and feature tracking under rigidity constraints for each segment.
This approach, however, requires knowing the number of motions, as well as
assigning points to motion groups, which is typically sensitive to the motion
estimates. By contrast, here, we introduce a segmentation-free solution to
multi-body feature tracking that bypasses the motion assignment step and
reduces to solving a series of subproblems with closed-form solutions. Our
experiments demonstrate the benefits of our approach in terms of tracking
accuracy and robustness to noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00119</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00119</id><created>2016-02-29</created><updated>2016-03-02</updated><authors><author><keyname>Ahmadinejad</keyname><forenames>Mahdi</forenames></author><author><keyname>Dehghani</keyname><forenames>Sina</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author><author><keyname>Mahini</keyname><forenames>Hamid</forenames></author><author><keyname>Seddighin</keyname><forenames>Saeed</forenames></author></authors><title>From Duels to Battefields: Computing Equilibria of Blotto and Other
  Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing Nash equilibria of zero-sum games. Many
natural zero-sum games have exponentially many strategies, but highly
structured payoffs. For example, in the well-studied Colonel Blotto game
(introduced by Borel in 1921), players must divide a pool of troops among a set
of battlefields with the goal of winning (i.e., having more troops in) a
majority. The Colonel Blotto game is commonly used for analyzing a wide range
of applications from the U.S presidential election, to innovative technology
competitions, to advertisement, to sports. However, because of the size of the
strategy space, standard methods for computing equilibria of zero-sum games
fail to be computationally feasible. Indeed, despite its importance, only a few
solutions for special variants of the problem are known.
  In this paper we show how to compute equilibria of Colonel Blotto games.
  Moreover, our approach takes the form of a general reduction: to find a Nash
equilibrium of a zero-sum game, it suffices to design a separation oracle for
the strategy polytope of any bilinear game that is payoff-equivalent. We then
apply this technique to obtain the first polytime algorithms for a variety of
games. In addition to Colonel Blotto, we also show how to compute equilibria in
an infinite-strategy variant called the General Lotto game; this involves
showing how to prune the strategy space to a finite subset before applying our
reduction. We also consider the class of dueling games, first introduced by
Immorlica et al. (2011). We show that our approach provably extends the class
of dueling games for which equilibria can be computed: we introduce a new
dueling game, the matching duel, on which prior methods fail to be
computationally feasible but upon which our reduction can be applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00124</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00124</id><created>2016-02-29</created><authors><author><keyname>Cao</keyname><forenames>Jiale</forenames></author><author><keyname>Pang</keyname><forenames>Yanwei</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>Learning Multilayer Channel Features for Pedestrian Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pedestrian detection based on the combination of Convolutional Neural Network
(i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achieved
great success. Generally, HOG+LUV are used to generate the candidate proposals
and then CNN classifies these proposals. Despite its success, there is still
room for improvement. For example, CNN classifies these proposals by the
full-connected layer features while proposal scores and the features in the
inner-layers of CNN are ignored. In this paper, we propose a unifying framework
called Multilayer Channel Features (MCF) to overcome the drawback. It firstly
integrates HOG+LUV with each layer of CNN into a multi-layer image channels.
Based on the multi-layer image channels, a multi-stage cascade AdaBoost is then
learned. The weak classifiers in each stage of the multi-stage cascade is
learned from the image channels of corresponding layer. With more abundant
features, MCF achieves the state-of-the-art on Caltech pedestrian dataset
(i.e., 10.40% miss rate). Using new and accurate annotations, MCF achieves
7.98% miss rate. As many non-pedestrian detection windows can be quickly
rejected by the first few stages, it accelerates detection speed by 1.43 times.
By eliminating the highly overlapped detection windows with lower scores after
the first stage, it's 4.07 times faster with negligible performance loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00126</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00126</id><created>2016-02-29</created><authors><author><keyname>Duchi</keyname><forenames>John C.</forenames></author><author><keyname>Khosravi</keyname><forenames>Khashayar</forenames></author><author><keyname>Ruan</keyname><forenames>Feng</forenames></author></authors><title>Information Measures, Experiments, Multi-category Hypothesis Tests, and
  Surrogate Losses</title><categories>math.ST cs.IT math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a unifying view of statistical information measures, multi-class
classification problems, multi-way Bayesian hypothesis testing, and loss
functions, elaborating equivalence results between all of these objects. In
particular, we consider a particular generalization of $f$-divergences to
multiple distributions, and we show that there is a constructive equivalence
between $f$-divergences, statistical information (in the sense of uncertainty
as elaborated by DeGroot), and loss functions for multi-category
classification. We also study an extension of our results to multi-class
classification problems in which we must both infer a discriminant function
$\gamma$ and a data representation (or, in the setting of a hypothesis testing
problem, an experimental design), represented by a quantizer $\mathsf{q}$ from
a family of possible quantizers $\mathsf{Q}$. There, we give a complete
characterization of the equivalence between loss functions, meaning that
optimizing either of two losses yields the same optimal discriminant and
quantizer $\mathsf{q}$. A main consequence of our results is to describe those
convex loss functions that are Fisher consistent for jointly choosing a data
representation and minimizing the (weighted) probability of error in
multi-category classification and hypothesis testing problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00128</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00128</id><created>2016-02-29</created><authors><author><keyname>Jiang</keyname><forenames>Xiaoheng</forenames></author><author><keyname>Pang</keyname><forenames>Yanwei</forenames></author><author><keyname>Sun</keyname><forenames>Manli</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>Cascaded Subpatch Networks for Effective CNNs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional Convolutional Neural Networks (CNNs) use either a linear or
non-linear filter to extract features from an image patch (region) of spatial
size $ H\times W $ (Typically, $ H $ is small and is equal to $ W$, e.g., $ H $
is 5 or 7). Generally, the size of the filter is equal to the size $ H\times W
$ of the input patch. We argue that the representation ability of equal-size
strategy is not strong enough. To overcome the drawback, we propose to use
subpatch filter whose spatial size $ h\times w $ is smaller than $ H\times W $.
The proposed subpatch filter consists of two subsequent filters. The first one
is a linear filter of spatial size $ h\times w $ and is aimed at extracting
features from spatial domain. The second one is of spatial size $ 1\times 1 $
and is used for strengthening the connection between different input feature
channels and for reducing the number of parameters. The subpatch filter
convolves with the input patch and the resulting network is called a subpatch
network. Taking the output of one subpatch network as input, we further repeat
constructing subpatch networks until the output contains only one neuron in
spatial domain. These subpatch networks form a new network called Cascaded
Subpatch Network (CSNet). The feature layer generated by CSNet is called csconv
layer. For the whole input image, we construct a deep neural network by
stacking a sequence of csconv layers. Experimental results on four benchmark
datasets demonstrate the effectiveness and compactness of the proposed CSNet.
For example, our CSNet reaches a test error of $ 5.68\% $ on the CIFAR10
dataset without model averaging. To the best of our knowledge, this is the best
result ever obtained on the CIFAR10 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00131</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00131</id><created>2016-02-29</created><authors><author><keyname>Kamala</keyname><forenames>B.</forenames></author><author><keyname>Priya</keyname><forenames>B.</forenames></author><author><keyname>Nandhini</keyname><forenames>J. M.</forenames></author></authors><title>Platform Autonomous Custom Scalable Service using Service Oriented Cloud
  Computing Architecture</title><categories>cs.DC</categories><comments>IJERA</comments><journal-ref>International Journal of Engineering Research and Applications
  (IJERA) ISSN: 2248-9622,Vol. 2, Issue 2,Mar-Apr 2012, pp.1467-1471</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The global economic recession and the shrinking budget of IT projects have
led to the need of development of integrated information systems at a lower
cost. Today, the emerging phenomenon of cloud computing aims at transforming
the traditional way of computing by providing both software applications and
hardware resources as a service. With the rapid evolution of Information
Communication Technology (ICT) governments, organizations and businesses are
looking for solutions to improve their services and integrate their IT
infrastructures. In recent years advanced technologies such as SOA and Cloud
computing have been evolved to address integration problems. The Clouds
enormous capacity with comparable low cost makes it an ideal platform for SOA
deployment. This paper deals with the combined approach of Cloud and Service
Oriented Architecture along with a Case Study and a review.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="92000" completeListSize="102538">1122234|93001</resumptionToken>
</ListRecords>
</OAI-PMH>
