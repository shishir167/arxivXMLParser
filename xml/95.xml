<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:04:45Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|94001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607026</id><created>1996-07-25</created><authors><author><keyname>Paris</keyname><forenames>Cecile</forenames><affiliation>ITRI, Univ. of Brighton</affiliation></author><author><keyname>Linden</keyname><forenames>Keith Vander</forenames><affiliation>ITRI, Univ. of Brighton</affiliation></author></authors><title>Building Knowledge Bases for the Generation of Software Documentation</title><categories>cmp-lg cs.CL</categories><comments>6 pages, from COLING-96</comments><report-no>ITRI-96-06</report-no><abstract>  Automated text generation requires a underlying knowledge base from which to
generate, which is often difficult to produce. Software documentation is one
domain in which parts of this knowledge base may be derived automatically. In
this paper, we describe \drafter, an authoring support tool for generating
user-centred software documentation, and in particular, we describe how parts
of its required knowledge base can be obtained automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607027</id><created>1996-07-26</created><authors><author><keyname>Cicekli</keyname><forenames>Ilyas</forenames></author><author><keyname>Guvenir</keyname><forenames>H. Altay</forenames></author></authors><title>Learning Translation Rules From A Bilingual Corpus</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Latex, uses nemlap.sty</comments><journal-ref>Published in Proceedings of NEMLAP-2</journal-ref><abstract>  This paper proposes a mechanism for learning pattern correspondences between
two languages from a corpus of translated sentence pairs. The proposed
mechanism uses analogical reasoning between two translations. Given a pair of
translations, the similar parts of the sentences in the source language must
correspond the similar parts of the sentences in the target language.
Similarly, the different parts should correspond to the respective parts in the
translated sentences. The correspondences between the similarities, and also
differences are learned in the form of translation rules. The system is tested
on a small training dataset and produced promising results for further
investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607028</id><created>1996-07-26</created><authors><author><keyname>Wilks</keyname><forenames>Yorick</forenames><affiliation>University of Sheffield, UK</affiliation></author><author><keyname>Stevenson</keyname><forenames>Mark</forenames><affiliation>University of Sheffield, UK</affiliation></author></authors><title>The Grammar of Sense: Is word-sense tagging much more than
  part-of-speech tagging?</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX</comments><report-no>CS-96-05</report-no><abstract>  This squib claims that Large-scale Automatic Sense Tagging of text (LAST) can
be done at a high-level of accuracy and with far less complexity and
computational effort than has been believed until now. Moreover, it can be done
for all open class words, and not just carefully selected opposed pairs as in
some recent work. We describe two experiments: one exploring the amount of
information relevant to sense disambiguation which is contained in the
part-of-speech field of entries in Longman Dictionary of Contemporary English
(LDOCE). Another, more practical, experiment attempts sense disambiguation of
all open class words in a text assigning LDOCE homographs as sense tags using
only part-of-speech information. We report that 92% of open class words can be
successfully tagged in this way. We plan to extend this work and to implement
an improved large-scale tagger, a description of which is included here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607029</id><created>1996-07-30</created><authors><author><keyname>Hakkani</keyname><forenames>Dilek Zeynep</forenames></author></authors><title>Design and Implementation of a Tactical Generator for Turkish, a Free
  Constituent Order Language</title><categories>cmp-lg cs.CL</categories><comments>M.Sc. Thesis submitted to the Department of Computer Engineering and
  Information Science, Bilkent University, Ankara, Turkey. 146 pages (including
  title pages). Also available as:
  ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1996/BU-CEIS-9614.ps.z</comments><report-no>BU-CEIS-9614</report-no><abstract>  This thesis describes a tactical generator for Turkish, a free constituent
order language, in which the order of the constituents may change according to
the information structure of the sentences to be generated. In the absence of
any information regarding the information structure of a sentence (i.e., topic,
focus, background, etc.), the constituents of the sentence obey a default
order, but the order is almost freely changeable, depending on the constraints
of the text flow or discourse. We have used a recursively structured finite
state machine for handling the changes in constituent order, implemented as a
right-linear grammar backbone. Our implementation environment is the GenKit
system, developed at Carnegie Mellon University--Center for Machine
Translation. Morphological realization has been implemented using an external
morphological analysis/generation component which performs concrete morpheme
selection and handles morphographemic processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607030</id><created>1996-07-30</created><authors><author><keyname>Tur</keyname><forenames>Gokhan</forenames></author></authors><title>Using Multiple Sources of Information for Constraint-Based Morphological
  Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>M.Sc. Thesis submitted to the Department of Computer Engineering and
  Information Science, Bilkent University, Ankara, Turkey. Also available as:
  ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1996/BU-CEIS-9615ps.z</comments><report-no>BU-CEIS-9615</report-no><abstract>  This thesis presents a constraint-based morphological disambiguation approach
that is applicable to languages with complex morphology--specifically
agglutinative languages with productive inflectional and derivational
morphological phenomena. For morphologically complex languages like Turkish,
automatic morphological disambiguation involves selecting for each token
morphological parse(s), with the right set of inflectional and derivational
markers. Our system combines corpus independent hand-crafted constraint rules,
constraint rules that are learned via unsupervised learning from a training
corpus, and additional statistical information obtained from the corpus to be
morphologically disambiguated. The hand-crafted rules are linguistically
motivated and tuned to improve precision without sacrificing recall. In certain
respects, our approach has been motivated by Brill's recent work, but with the
observation that his transformational approach is not directly applicable to
languages like Turkish. Our approach also uses a novel approach to unknown word
processing by employing a secondary morphological processor which recovers any
relevant inflectional and derivational information from a lexical item whose
root is unknown. With this approach, well below 1% of the tokens remains as
unknown in the texts we have experimented with. Our results indicate that by
combining these hand-crafted, statistical and learned information sources, we
can attain a recall of 96 to 97% with a corresponding precision of 93 to 94%,
and ambiguity of 1.02 to 1.03 parses per token.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607031</id><created>1996-07-30</created><authors><author><keyname>Bos</keyname><forenames>Johan</forenames></author><author><keyname>Gamb&#xe4;ck</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Lieske</keyname><forenames>Christian</forenames></author><author><keyname>Mori</keyname><forenames>Yoshiki</forenames></author><author><keyname>Pinkal</keyname><forenames>Manfred</forenames></author><author><keyname>Worm</keyname><forenames>Karsten</forenames></author></authors><title>Compositional Semantics in Verbmobil</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX, uses colap.sty</comments><journal-ref>Proceedings of COLING '96</journal-ref><abstract>  The paper discusses how compositional semantics is implemented in the
Verbmobil speech-to-speech translation system using LUD, a description language
for underspecified discourse representation structures. The description
language and its formal interpretation in DRT are described as well as its
implementation together with the architecture of the system's entire
syntactic-semantic processing module. We show that a linguistically sound
theory and formalism can be properly implemented in a system with (near)
real-time requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607032</id><created>1996-07-30</created><authors><author><keyname>Heinecke</keyname><forenames>Johannes</forenames><affiliation>Humboldt-Universit&#xe4;t zu Berlin</affiliation></author><author><keyname>Worm</keyname><forenames>Karsten L.</forenames><affiliation>Universit&#xe4;t des Saarlandes</affiliation></author></authors><title>A Lexical Semantic Database for Verbmobil</title><categories>cmp-lg cs.CL</categories><comments>13 Pages, 2 Postscript figures, uses epsf.sty, a4.sty, alltt.sty
  (included), chicago.sty; .tar.gz-file includes .dvi and .ps versions as well</comments><abstract>  This paper describes the development and use of a lexical semantic database
for the Verbmobil speech-to-speech machine translation system. The motivation
is to provide a common information source for the distributed development of
the semantics, transfer and semantic evaluation modules and to store lexical
semantic information application-independently.
 The database is organized around a set of abstract semantic classes and has
been used to define the semantic contributions of the lemmata in the vocabulary
of the system, to automatically create semantic lexica and to check the
correctness of the semantic representations built up. The semantic classes are
modelled using an inheritance hierarchy. The database is implemented using the
lexicon formalism LeX4 developed during the project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607033</id><created>1996-07-30</created><authors><author><keyname>Mori</keyname><forenames>Yoshiki</forenames></author></authors><title>Multiple Discourse Relations on the Sentential Level in Japanese</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Postscript</comments><journal-ref>Proceedings of COLING '96</journal-ref><abstract>  In the German government (BMBF) funded project Verbmobil, a semantic
formalism Language for Underspecified Discourse Representation Structures (LUD)
is used which describes several DRSs and allows for underspecification. Dealing
with Japanese poses challenging problems. In this paper, a treatment of
multiple discourse relation constructions on the sentential level is shown,
which are common in Japanese but cause a problem for the formalism,. The
problem is to distinguish discourse relations which take the widest scope
compared with other scope-taking elements on the one hand and to have them
underspecified among each other on the other hand. We also state a semantic
constraint on the resolution of multiple discourse relations which seems to
prevail over the syntactic c-command constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607034</id><created>1996-07-30</created><authors><author><keyname>Ferrari</keyname><forenames>St&#xe9;phane</forenames><affiliation>Limsi-CNRS, France</affiliation></author></authors><title>Using textual clues to improve metaphor processing</title><categories>cmp-lg cs.CL</categories><comments>3 pages, single LaTeX file, uses aclap.sty</comments><journal-ref>Proceedings of the ACL'96, 34th Annual Meeting of the Association
  for Computational Linguistics, 351-353, Santa Cruz, California, 1996</journal-ref><abstract>  In this paper, we propose a textual clue approach to help metaphor detection,
in order to improve the semantic processing of this figure. The previous works
in the domain studied the semantic regularities only, overlooking an obvious
set of regularities. A corpus-based analysis shows the existence of surface
regularities related to metaphors. These clues can be characterized by
syntactic structures and lexical markers. We present an object oriented model
for representing the textual clues that were found. This representation is
designed to help the choice of a semantic processing, in terms of possible
non-literal meanings. A prototype implementing this model is currently under
development, within an incremental approach allowing step-by-step evaluations.
\footnote{This work takes part in a research project sponsored by the
AUPELF-UREF (Francophone Agency For Education and Research)}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607035</id><created>1996-07-31</created><authors><author><keyname>Huijsen</keyname><forenames>Willem-Olaf</forenames><affiliation>Research Institute for Language and Speech</affiliation></author></authors><title>Completeness of Compositional Translation for Context-Free Grammars</title><categories>cmp-lg cs.CL</categories><comments>14 pages, LaTeX 2e, to appear in Proceedings of CLIN'95. Also
  available as: http://wwwots.let.ruu.nl/Personal/Doc/clin-artikel-1995.ps</comments><abstract>  A machine translation system is said to be *complete* if all expressions that
are correct according to the source-language grammar can be translated into the
target language. This paper addresses the completeness issue for compositional
machine translation in general, and for compositional machine translation of
context-free grammars in particular. Conditions that guarantee translation
completeness of context-free grammars are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607036</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607036</id><created>1996-07-31</created><authors><author><keyname>Ingels</keyname><forenames>Peter</forenames><affiliation>Linkoping University, Linkoping, Sweden</affiliation></author></authors><title>Connected Text Recognition Using Layered HMMs and Token Passing</title><categories>cmp-lg cs.CL</categories><comments>12 pages, LaTeX format, 3 encapsulated Postscript figures, uses
  nemlap.sty</comments><journal-ref>Proceedings of NeMLaP-2</journal-ref><abstract>  We present a novel approach to lexical error recovery on textual input. An
advanced robust tokenizer has been implemented that can not only correct
spelling mistakes, but also recover from segmentation errors. Apart from the
orthographic considerations taken, the tokenizer also makes use of linguistic
expectations extracted from a training corpus. The idea is to arrange Hidden
Markov Models (HMM) in multiple layers where the HMMs in each layer are
responsible for different aspects of the processing of the input. We report on
experimental evaluations with alternative probabilistic language models to
guide the lexical error recovery process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607037</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607037</id><created>1996-07-31</created><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Automatic Construction of Clean Broad-Coverage Translation Lexicons</title><categories>cmp-lg cs.CL</categories><comments>PostScript file, 10 pages. To appear in Proceedings of AMTA-96</comments><abstract>  Word-level translational equivalences can be extracted from parallel texts by
surprisingly simple statistical techniques. However, these techniques are
easily fooled by {\em indirect associations} --- pairs of unrelated words whose
statistical properties resemble those of mutual translations. Indirect
associations pollute the resulting translation lexicons, drastically reducing
their precision. This paper presents an iterative lexicon cleaning method. On
each iteration, most of the remaining incorrect lexicon entries are filtered
out, without significant degradation in recall. This lexicon cleaning technique
can produce translation lexicons with recall and precision both exceeding 90\%,
as well as dictionary-sized translation lexicons that are over 99\% correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608001</id><created>1996-08-02</created><authors><author><keyname>Collier</keyname><forenames>Nigel</forenames><affiliation>Department of Language Engineering, UMIST, UK</affiliation></author></authors><title>Storage of Natural Language Sentences in a Hopfield Network</title><categories>cmp-lg cs.CL</categories><comments>latex, 10 pages with 2 tex figures and a .bib file, uses nemlap.sty,
  to appear in Proceedings of NeMLaP-2</comments><abstract>  This paper look at how the Hopfield neural network can be used to store and
recall patterns constructed from natural language sentences. As a pattern
recognition and storage tool, the Hopfield neural network has received much
attention. This attention however has been mainly in the field of statistical
physics due to the model's simple abstraction of spin glass systems. A
discussion is made of the differences, shown as bias and correlation, between
natural language sentence patterns and the randomly generated ones used in
previous experiments. Results are given for numerical simulations which show
the auto-associative competence of the network when trained with natural
language patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608002</id><created>1996-08-06</created><authors><author><keyname>Backofen</keyname><forenames>Rolf</forenames><affiliation>Theoretische Informatik, LMU Muenchen, Germany</affiliation></author></authors><title>Controlling Functional Uncertainty</title><categories>cmp-lg cs.CL</categories><comments>5 pages (to appear in Proceedings of ECAI '96)</comments><abstract>  There have been two different methods for checking the satisfiability of
feature descriptions that use the functional uncertainty device,
namely~\cite{Kaplan:88CO} and \cite{Backofen:94JSC}. Although only the one in
\cite{Backofen:94JSC} solves the satisfiability problem completely, both
methods have their merits. But it may happen that in one single description,
there are parts where the first method is more appropriate, and other parts
where the second should be applied. In this paper, we present a common
framework that allows one to combine both methods. This is done by presenting a
set of rules for simplifying feature descriptions. The different methods are
described as different controls on this rule set, where a control specifies in
which order the different rules must be applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608003</id><created>1996-08-08</created><authors><author><keyname>Karlgren</keyname><forenames>Jussi</forenames><affiliation>NYU</affiliation></author></authors><title>Stylistic Variation in an Information Retrieval Experiment</title><categories>cmp-lg cs.CL</categories><comments>Proceedings of NEMLAP-2</comments><abstract>  Texts exhibit considerable stylistic variation. This paper reports an
experiment where a corpus of documents (N= 75 000) is analyzed using various
simple stylistic metrics. A subset (n = 1000) of the corpus has been previously
assessed to be relevant for answering given information retrieval queries. The
experiment shows that this subset differs significantly from the rest of the
corpus in terms of the stylistic metrics studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608004</id><created>1996-08-09</created><authors><author><keyname>Freeman</keyname><forenames>Robert John</forenames></author></authors><title>Patterns of Language - A Population Model for Language Structure</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Postscript</comments><abstract>  A key problem in the description of language structure is to explain its
contradictory properties of specificity and generality, the contrasting poles
of formulaic prescription and generative productivity. I argue that this is
possible if we accept analogy and similarity as the basic mechanisms of
structural definition. As a specific example I discuss how it would be possible
to use analogy to define a generative model of syntactic structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608005</id><created>1996-08-13</created><authors><author><keyname>Milward</keyname><forenames>David</forenames><affiliation>SRI Cambridge</affiliation></author><author><keyname>Konrad</keyname><forenames>Karsten</forenames><affiliation>University of the Saarland</affiliation></author><author><keyname>Maier</keyname><forenames>Holger</forenames><affiliation>University of the Saarland</affiliation></author><author><keyname>Pinkal</keyname><forenames>Manfred</forenames><affiliation>University of the Saarland</affiliation></author></authors><title>CLEARS - An Education and Research Tool for Computational Semantics</title><categories>cmp-lg cs.CL</categories><comments>4 pages, includes 6 postscript figures, needs colap.sty and acl.bst</comments><abstract>  The CLEARS (Computational Linguistics Education and Research for Semantics)
tool provides a graphical interface allowing interactive construction of
semantic representations in a variety of different formalisms, and using
several construction methods. CLEARS was developed as part of the FraCaS
project which was designed to encourage convergence between different semantic
formalisms, such as Montague-Grammar, DRT, and Situation Semantics. The CLEARS
system is freely available on the WWW from
http://coli.uni-sb.de/~clears/clears.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608006</id><created>1996-08-14</created><authors><author><keyname>Yvon</keyname><forenames>Francois</forenames><affiliation>Ecole Nationale Superieure des Telecommunications, Paris</affiliation></author></authors><title>Grapheme-to-Phoneme Conversion using Multiple Unbounded Overlapping
  Chunks</title><categories>cmp-lg cs.CL</categories><comments>11 pages, Postscript only, Proceedings of NeMLaP II</comments><abstract>  We present in this paper an original extension of two data-driven algorithms
for the transcription of a sequence of graphemes into the corresponding
sequence of phonemes. In particular, our approach generalizes the algorithm
originally proposed by Dedina and Nusbaum (D&amp;N) (1991), which had originally
been promoted as a model of the human ability to pronounce unknown words by
analogy to familiar lexical items. We will show that DN's algorithm performs
comparatively poorly when evaluated on a realistic test set, and that our
extension allows us to improve substantially the performance of the
analogy-based model. We will also suggest that both algorithms can be
reformulated in a much more general framework, which allows us to anticipate
other useful extensions. However, considering the inability to define in these
models important notions like lexical neighborhood, we conclude that both
approaches fail to offer a proper model of the analogical processes involved in
reading aloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608007</id><created>1996-08-14</created><authors><author><keyname>Di Eugenio</keyname><forenames>Barbara</forenames><affiliation>Carnegie Mellon University / University of Pittsburgh</affiliation></author></authors><title>Centering in Italian</title><categories>cmp-lg cs.CL</categories><comments>18 pages, uses cgloss4e.sty and fullname.sty. To appear in &quot;Centering
  in Discourse&quot;, OUP, 1997</comments><abstract>  This paper explores the correlation between centering and different forms of
pronominal reference in Italian, in particular zeros and overt pronouns in
subject position. Such correlations, that I had proposed in earlier work
(COLING 90), are verified through the analysis of a corpus of naturally
occurring texts. In the process, I extend my previous analysis in several ways,
for example by taking possessives and subordinates into account. I also provide
a more detailed analysis of the &quot;continue&quot; transition: more specifically, I
show that pronouns are used in a markedly different way in a &quot;continue&quot;
preceded by another &quot;continue&quot; or by a &quot;shift&quot;, and in a &quot;continue&quot; preceded by
a &quot;retain&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608008</id><created>1996-08-14</created><authors><author><keyname>Di Eugenio</keyname><forenames>Barbara</forenames><affiliation>Carnegie Mellon University / University of Pittsburgh</affiliation></author></authors><title>The discourse functions of Italian subjects: a centering approach</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses colap.sty. Revised version of COLING96 paper (fixes
  wrong chi-square values in published version)</comments><journal-ref>Proceedings COLING96, Copenhagen, August 1996</journal-ref><abstract>  This paper examines the discourse functions that different types of subjects
perform in Italian within the centering framework. I build on my previous work
(COLING90) that accounted for the alternation of null and strong pronouns in
subject position. I extend my previous analysis in several ways: for example, I
refine the notion of {\sc continue} and discuss the centering functions of full
NPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608009</id><created>1996-08-14</created><authors><author><keyname>Di Eugenio</keyname><forenames>Barbara</forenames><affiliation>Carnegie Mellon University / University of Pittsburgh</affiliation></author></authors><title>Centering theory and the Italian pronominal system</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uses colap.sty</comments><journal-ref>Proceedings COLING90, Helsinki, August 1990</journal-ref><abstract>  In this paper, I give an account of some phenomena of pronominalization in
Italian in terms of centering theory. After a general introduction to the
Italian pronominal system, I will review centering, and then show how the
original rules have to be extended or modified. Finally, I will show that
centering does not account for two phenomena: first, the functional role of an
utterance may override the predictions of centering; second, a null subject can
be used to refer to a whole discourse segment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608010</id><created>1996-08-16</created><authors><author><keyname>Pedersen</keyname><forenames>Ted</forenames><affiliation>Southern Methodist University, Dallas, TX</affiliation></author></authors><title>Fishing for Exactness</title><categories>cmp-lg cs.CL</categories><comments>13 pages - postscript</comments><journal-ref>Proceedings of the South-Central SAS Users Group Conference
  (SCSUG-96), Austin, TX, Oct 27-29, 1996</journal-ref><abstract>  Statistical methods for automatically identifying dependent word pairs (i.e.
dependent bigrams) in a corpus of natural language text have traditionally been
performed using asymptotic tests of significance. This paper suggests that
Fisher's exact test is a more appropriate test due to the skewed and sparse
data samples typical of this problem. Both theoretical and experimental
comparisons between Fisher's exact test and a variety of asymptotic tests (the
t-test, Pearson's chi-square test, and Likelihood-ratio chi-square test) are
presented. These comparisons show that Fisher's exact test is more reliable in
identifying dependent word pairs. The usefulness of Fisher's exact test extends
to other problems in statistical natural language processing as skewed and
sparse data appears to be the rule in natural language. The experiment
presented in this paper was performed using PROC FREQ of the SAS System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608011</id><created>1996-08-16</created><authors><author><keyname>Doran</keyname><forenames>Christine</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Punctuation in Quoted Speech</title><categories>cmp-lg cs.CL</categories><comments>11 pages, 11 ps figures, Proceedings of SIGPARSE 96 - Punctuation in
  Computational Linguistics</comments><abstract>  Quoted speech is often set off by punctuation marks, in particular quotation
marks. Thus, it might seem that the quotation marks would be extremely useful
in identifying these structures in texts. Unfortunately, the situation is not
quite so clear. In this work, I will argue that quotation marks are not
adequate for either identifying or constraining the syntax of quoted speech.
More useful information comes from the presence of a quoting verb, which is
either a verb of saying or a punctual verb, and the presence of other
punctuation marks, usually commas. Using a lexicalized grammar, we can license
most quoting clauses as text adjuncts. A distinction will be made not between
direct and indirect quoted speech, but rather between adjunct and non-adjunct
quoting clauses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608012</id><created>1996-08-19</created><authors><author><keyname>Sproat</keyname><forenames>Richard</forenames></author></authors><title>Multilingual Text Analysis for Text-to-Speech Synthesis</title><categories>cmp-lg cs.CL</categories><journal-ref>ECAI Workshop on Extended Finite-State Models of Language</journal-ref><abstract>  We present a model of text analysis for text-to-speech (TTS) synthesis based
on (weighted) finite-state transducers, which serves as the text-analysis
module of the multilingual Bell Labs TTS system. The transducers are
constructed using a lexical toolkit that allows declarative descriptions of
lexicons, morphological rules, numeral-expansion rules, and phonological rules,
inter alia. To date, the model has been applied to eight languages: Spanish,
Italian, Romanian, French, German, Russian, Mandarin and Japanese.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608013</id><created>1996-08-20</created><authors><author><keyname>Oztaner</keyname><forenames>S. Murat</forenames><affiliation>Middle East Technical University</affiliation></author></authors><title>A Word Grammar of Turkish with Morphophonemic Rules</title><categories>cmp-lg cs.CL</categories><comments>128 pages, postscript, MS Thesis in Dept of Computer Engineering</comments><abstract>  In this thesis, morphological description of Turkish is encoded using the
two-level model. This description is made up of the phonological component that
contains the two-level morphophonemic rules, and the lexicon component which
lists the lexical items and encodes the morphotactic constraints. The word
grammar is expressed in tabular form. It includes the verbal and the nominal
paradigm. Vowel and consonant harmony, epenthesis, reduplication, etc. are
described in detail and coded in two-level notation. Loan-word phonology is
modelled separately.
  The implementation makes use of Lexc/Twolc from Xerox. Mechanisms to
integrate the morphological analyzer with the lexical and syntactic components
are discussed, and a simple graphical user interface is provided. Work is
underway to use this model in a classroom setting for teaching Turkish
morphology to non-native speakers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608014</id><created>1996-08-21</created><authors><author><keyname>Bond</keyname><forenames>Francis</forenames><affiliation>NTT</affiliation></author><author><keyname>Ogura</keyname><forenames>Kentaro</forenames><affiliation>NTT</affiliation></author><author><keyname>Ikehara</keyname><forenames>Satoru</forenames><affiliation>Tottori University</affiliation></author></authors><title>Classifiers in Japanese-to-English Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX, uses gb4e.sty, fullname.bst</comments><journal-ref>Proceedings of the 16th International Conference on Computational
  Linguistics (COLING'96), pp 125--130.</journal-ref><abstract>  This paper proposes an analysis of classifiers into four major types: UNIT,
METRIC, GROUP and SPECIES, based on properties of both Japanese and English.
The analysis makes possible a uniform and straightforward treatment of noun
phrases headed by classifiers in Japanese-to-English machine translation, and
has been implemented in the MT system ALT-J/E. Although the analysis is based
on the characteristics of, and differences between, Japanese and English, it is
shown to be also applicable to the unrelated language Thai.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608015</id><created>1996-08-23</created><authors><author><keyname>Sehitoglu</keyname><forenames>Onur</forenames><affiliation>Middle East Technical University</affiliation></author><author><keyname>Bozsahin</keyname><forenames>Cem</forenames><affiliation>Middle East Technical University</affiliation></author></authors><title>Morphological Productivity in the Lexicon</title><categories>cmp-lg cs.CL</categories><comments>10 pages LaTeX, {lingmacros,avm,psfig}.sty, 1 figure, 1 bibtex file</comments><journal-ref>Proc. of the ACL'96 SIGLEX Workshop, Santa Cruz, 105--114</journal-ref><abstract>  In this paper we outline a lexical organization for Turkish that makes use of
lexical rules for inflections, derivations, and lexical category changes to
control the proliferation of lexical entries. Lexical rules handle changes in
grammatical roles, enforce type constraints, and control the mapping of
subcategorization frames in valency-changing operations. A lexical inheritance
hierarchy facilitates the enforcement of type constraints. Semantic
compositions in inflections and derivations are constrained by the properties
of the terms and predicates.
  The design has been tested as part of a HPSG grammar for Turkish. In terms of
performance, run-time execution of the rules seems to be a far better
alternative than pre-compilation. The latter causes exponential growth in the
lexicon due to intensive use of inflections and derivations in Turkish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608016</id><created>1996-08-26</created><updated>1996-08-28</updated><authors><author><keyname>Sehitoglu</keyname><forenames>Onur Tolga</forenames></author></authors><title>A Sign-Based Phrase Structure Grammar for Turkish</title><categories>cmp-lg cs.CL</categories><comments>MS. Thesis, Dept. of Computer Engineering, Middle East Technical
  University, Ankara January, 1996, 97 pages. 5 eps figures, uses
  avm,psfig,lingmacros,tree-dvips,ulem,QobiTree,alltt
  ulem.sty,QobiTree.sty,alltt.sty and eps files included in tar</comments><abstract>  This study analyses Turkish syntax from an informational point of view. Sign
based linguistic representation and principles of HPSG (Head-driven Phrase
Structure Grammar) theory are adapted to Turkish. The basic informational
elements are nested and inherently sorted feature structures called signs.
  In the implementation, logic programming tool ALE (Attribute Logic Engine)
which is primarily designed for implementing HPSG grammars is used. A type and
structure hierarchy of Turkish language is designed. Syntactic phenomena such a
s subcategorization, relative clauses, constituent order variation, adjuncts,
nomina l predicates and complement-modifier relations in Turkish are analyzed.
A parser is designed and implemented in ALE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608017</id><created>1996-08-27</created><authors><author><keyname>Xu</keyname><forenames>Donghua</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Tan</keyname><forenames>Chew Lim</forenames><affiliation>National University of Singapore</affiliation></author></authors><title>Automatic Alignment of English-Chinese Bilingual Texts of CNS News</title><categories>cmp-lg cs.CL</categories><comments>9 pages, Postscript only. In the Proceedings of International
  Conference on Chinese Computing'96</comments><abstract>  In this paper we address a method to align English-Chinese bilingual news
reports from China News Service, combining both lexical and satistical
approaches. Because of the sentential structure differences between English and
Chinese, matching at the sentence level as in many other works may result in
frequent matching of several sentences en masse. In view of this, the current
work also attempts to create shorter alignment pairs by permitting finer
matching between clauses from both texts if possible. The current method is
based on statiscal correlation between sentence or clause length of both texts
and at the same time uses obvious anchors such as numbers and place names
appearing frequently in the news reports as lexcial cues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608018</id><created>1996-08-27</created><updated>1996-09-17</updated><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames><affiliation>AT&amp;T Laboratories</affiliation></author><author><keyname>Riley</keyname><forenames>Michael</forenames><affiliation>AT&amp;T Laboratories</affiliation></author><author><keyname>Sproat</keyname><forenames>Richard</forenames><affiliation>Bell Laboratories</affiliation></author></authors><title>Algorithms for Speech Recognition and Language Processing</title><categories>cmp-lg cs.CL</categories><comments>Postscript file tar-compressed and uuencoded, 189 pages</comments><abstract>  Speech processing requires very efficient methods and algorithms.
Finite-state transducers have been shown recently both to constitute a very
useful abstract model and to lead to highly efficient time and space algorithms
in this field. We present these methods and algorithms and illustrate them in
the case of speech recognition. In addition to classical techniques, we
describe many new algorithms such as minimization, global and local on-the-fly
determinization of weighted automata, and efficient composition of transducers.
These methods are currently used in large vocabulary speech recognition
systems. We then show how the same formalism and algorithms can be used in
text-to-speech applications and related areas of language processing such as
morphology, syntax, and local grammars, in a very efficient way. The tutorial
is self-contained and requires no specific computational or linguistic
knowledge other than classical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608019</id><created>1996-08-29</created><authors><author><keyname>Visser</keyname><forenames>Eric M.</forenames><affiliation>Fujitsu Laboratories Ltd.</affiliation></author><author><keyname>Fuji</keyname><forenames>Masaru</forenames><affiliation>Fujitsu Laboratories Ltd.</affiliation></author></authors><title>Using sentence connectors for evaluating MT output</title><categories>cmp-lg cs.CL</categories><comments>4 pages, LaTeX, uses colap.sty</comments><journal-ref>Proceedings of COLING-96 (Poster Sessions, pgs. 1066-1069)</journal-ref><abstract>  This paper elaborates on the design of a machine translation evaluation
method that aims to determine to what degree the meaning of an original text is
preserved in translation, without looking into the grammatical correctness of
its constituent sentences. The basic idea is to have a human evaluator take the
sentences of the translated text and, for each of these sentences, determine
the semantic relationship that exists between it and the sentence immediately
preceding it. In order to minimise evaluator dependence, relations between
sentences are expressed in terms of the conjuncts that can connect them, rather
than through explicit categories. For an n-sentence text this results in a list
of n-1 sentence-to-sentence relationships, which we call the text's
connectivity profile. This can then be compared to the connectivity profile of
the original text, and the degree of correspondence between the two would be a
measure for the quality of the translation.
  A set of &quot;essential&quot; conjuncts was extracted for English and Japanese, and a
computer interface was designed to support the task of inserting the most
fitting conjuncts between sentence pairs. With these in place, several sets of
experiments were performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608020</id><created>1996-08-29</created><authors><author><keyname>Juola</keyname><forenames>Patrick</forenames><affiliation>Oxford University, Dept. of Experimental Psychology</affiliation></author></authors><title>Phonetic Ambiguity : Approaches, Touchstones, Pitfalls and New Approaches</title><categories>cmp-lg cs.CL</categories><comments>LaTex, 6 pages, self-contained</comments><journal-ref>CSNLP-96, Sept. 2-4, Dublin, Ireland</journal-ref><abstract>  Phonetic ambiguity and confusibility are bugbears for any form of bottom-up
or data-driven approach to language processing. The question of when an input
is ``close enough'' to a target word pervades the entire problem spaces of
speech recognition, synthesis, language acquisition, speech compression, and
language representation, but the variety of representations that have been
applied are demonstrably inadequate to at least some aspects of the problem.
This paper reviews this inadequacy by examining several touchstone models in
phonetic ambiguity and relating them to the problems they were designed to
solve. An good solution would be, among other things, efficient, accurate,
precise, and universally applicable to representation of words, ideally usable
as a ``phonetic distance'' metric for direct measurement of the ``distance''
between word or utterance pairs. None of the proposed models can provide a
complete solution to the problem; in general, there is no algorithmic theory of
phonetic distance. It is unclear whether this is a weakness of our
representational technology or a more fundamental difficulty with the problem
statement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9608021</id><created>1996-08-29</created><authors><author><keyname>Juola</keyname><forenames>Patrick</forenames><affiliation>Oxford University, Dept. of Experimental Psychology</affiliation></author></authors><title>Isolated-Word Confusion Metrics and the PGPfone Alphabet</title><categories>cmp-lg cs.CL</categories><comments>12 pages, includes minipage.tex and nemlapfig.eps, uses nemlap.sty</comments><journal-ref>NeMLaP-96, Sept. 16-18, 1996, Ankara TURKEY</journal-ref><abstract>  Although the confusion of individual phonemes and features have been studied
and analyzed since (Miller and Nicely, 1955), there has been little work done
on extending this to a predictive theory of word-level confusions. The PGPfone
alphabet is a good touchstone problem for developing such word-level confusion
metrics. This paper presents some difficulties incurred, along with their
proposed solutions, in the extension of phonetic confusion results to a
theoretical whole-word phonetic distance metric. The proposed solutions have
been used, in conjunction with a set of selection filters, in a genetic
algorithm to automatically generate appropriate word lists for a radio
alphabet. This work illustrates some principles and pitfalls that should be
addressed in any numeric theory of isolated word perception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609001</id><created>1996-09-02</created><authors><author><keyname>Gardent</keyname><forenames>Claire</forenames></author><author><keyname>Kohlhase</keyname><forenames>Michael</forenames></author><author><keyname>van Neusen</keyname><forenames>Noor</forenames></author></authors><title>Corrections and Higher-Order Unification</title><categories>cmp-lg cs.CL</categories><comments>12 pages, LateX file, In Proccedings of the 3. Konferenz zur
  Verarbeitung natuerlicher Sprache (KONVENS), Bielefeld, 1996</comments><report-no>CLAUS Report Nr. 77</report-no><abstract>  We propose an analysis of corrections which models some of the requirements
corrections place on context. We then show that this analysis naturally extends
to the interaction of corrections with pronominal anaphora on the one hand, and
(in)definiteness on the other. The analysis builds on previous
unification--based approaches to NL semantics and relies on Higher--Order
Unification with Equivalences, a form of unification which takes into account
not only syntactic beta-eta-identity but also denotational equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609002</id><created>1996-09-07</created><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames></author></authors><title>Inferring Acceptance and Rejection in Dialogue by Default Rules of
  Inference</title><categories>cmp-lg cs.CL</categories><comments>37 pages, uses fullpage, lingmacros, named</comments><journal-ref>Language and Speech, 39-2, 1996</journal-ref><abstract>  This paper discusses the processes by which conversants in a dialogue can
infer whether their assertions and proposals have been accepted or rejected by
their conversational partners. It expands on previous work by showing that
logical consistency is a necessary indicator of acceptance, but that it is not
sufficient, and that logical inconsistency is sufficient as an indicator of
rejection, but it is not necessary. I show how conversants can use information
structure and prosody as well as logical reasoning in distinguishing between
acceptances and logically consistent rejections, and relate this work to
previous work on implicature and default reasoning by introducing three new
classes of rejection: {\sc implicature rejections}, {\sc epistemic rejections}
and {\sc deliberation rejections}. I show how these rejections are inferred as
a result of default inferences, which, by other analyses, would have been
blocked by the context. In order to account for these facts, I propose a model
of the common ground that allows these default inferences to go through, and
show how the model, originally proposed to account for the various forms of
acceptance, can also model all types of rejection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609003</id><created>1996-09-09</created><authors><author><keyname>Litman</keyname><forenames>Diane J.</forenames><affiliation>AT&amp;T Labs - Research</affiliation></author></authors><title>Cue Phrase Classification Using Machine Learning</title><categories>cmp-lg cs.CL</categories><comments>42 pages, uses jair.sty, theapa.bst, theapa.sty</comments><journal-ref>Journal of Artificial Intelligence Research 5 (1996) 53-94</journal-ref><abstract>  Cue phrases may be used in a discourse sense to explicitly signal discourse
structure, but also in a sentential sense to convey semantic rather than
structural information. Correctly classifying cue phrases as discourse or
sentential is critical in natural language processing systems that exploit
discourse structure, e.g., for performing tasks such as anaphora resolution and
plan recognition. This paper explores the use of machine learning for
classifying cue phrases as discourse or sentential. Two machine learning
programs (Cgrendel and C4.5) are used to induce classification models from sets
of pre-classified cue phrases and their features in text and speech. Machine
learning is shown to be an effective technique for not only automating the
generation of classification models, but also for improving upon previous
results. When compared to manually derived classification models already in the
literature, the learned models often perform with higher accuracy and contain
new linguistic insights into the data. In addition, the ability to
automatically construct classification models makes it easier to comparatively
analyze the utility of alternative feature representations of the data.
Finally, the ease of retraining makes the learning approach more scalable and
flexible than manual methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609004</id><created>1996-09-23</created><authors><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>A Principled Framework for Constructing Natural Language Interfaces To
  Temporal Databases</title><categories>cmp-lg cs.CL</categories><comments>PhD thesis; 405 pages; LaTeX2e, uses the packages/macros: amstex,
  xspace, avm, examples, dvips, varioref, makeidx, epic, eepic, ecltree;
  postscript figures included</comments><abstract>  Most existing natural language interfaces to databases (NLIDBs) were designed
to be used with ``snapshot'' database systems, that provide very limited
facilities for manipulating time-dependent data. Consequently, most NLIDBs also
provide very limited support for the notion of time. The database community is
becoming increasingly interested in _temporal_ database systems. These are
intended to store and manipulate in a principled manner information not only
about the present, but also about the past and future.
  This thesis develops a principled framework for constructing English NLIDBs
for _temporal_ databases (NLITDBs), drawing on research in tense and aspect
theories, temporal logics, and temporal databases. I first explore temporal
linguistic phenomena that are likely to appear in English questions to NLITDBs.
Drawing on existing linguistic theories of time, I formulate an account for
a large number of these phenomena that is simple enough to be embodied in
practical NLITDBs. Exploiting ideas from temporal logics, I then define a
temporal meaning representation language, TOP, and I show how the HPSG grammar
theory can be modified to incorporate the tense and aspect account of this
thesis, and to map a wide range of English questions involving time to
appropriate TOP expressions. Finally, I present and prove the correctness of a
method to translate from TOP to TSQL2, TSQL2 being a temporal extension of the
SQL-92 database language. This way, I establish a sound route from English
questions involving time to a general-purpose temporal database language, that
can act as a principled framework for building NLITDBs. To demonstrate that
this framework is workable, I employ it to develop a prototype NLITDB,
implemented using ALE and Prolog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609005</id><created>1996-09-24</created><authors><author><keyname>Walker</keyname><forenames>Marilyn</forenames></author><author><keyname>Iida</keyname><forenames>Masayo</forenames></author><author><keyname>Cote</keyname><forenames>Sharon</forenames></author></authors><title>Centering in Japanese Discourse</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses twocolumn</comments><journal-ref>COLING90: Proceedings 13th International Conference on
  Computational Linguistics, Helsinki</journal-ref><abstract>  In this paper we propose a computational treatment of the resolution of zero
pronouns in Japanese discourse, using an adaptation of the centering algorithm.
We are able to factor language-specific dependencies into one parameter of the
centering algorithm. Previous analyses have stipulated that a zero pronoun and
its cospecifier must share a grammatical function property such as {\sc
Subject} or {\sc NonSubject}. We show that this property-sharing stipulation is
unneeded. In addition we propose the notion of {\sc topic ambiguity} within the
centering framework, which predicts some ambiguities that occur in Japanese
discourse. This analysis has implications for the design of
language-independent discourse modules for Natural Language systems. The
centering algorithm has been implemented in an HPSG Natural Language system
with both English and Japanese grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609006</id><created>1996-09-24</created><authors><author><keyname>Walker</keyname><forenames>Marilyn</forenames></author><author><keyname>Iida</keyname><forenames>Masayo</forenames></author><author><keyname>Cote</keyname><forenames>Sharon</forenames></author></authors><title>Japanese Discourse and the Process of Centering</title><categories>cmp-lg cs.CL</categories><comments>38 pages, uses clstyle, lingmacros</comments><journal-ref>Computational Linguistics 20-2, 1994</journal-ref><abstract>  This paper has three aims: (1) to generalize a computational account of the
discourse process called {\sc centering}, (2) to apply this account to
discourse processing in Japanese so that it can be used in computational
systems for machine translation or language understanding, and (3) to provide
some insights on the effect of syntactic factors in Japanese on discourse
interpretation. We argue that while discourse interpretation is an inferential
process, syntactic cues constrain this process, and demonstrate this argument
with respect to the interpretation of {\sc zeros}, unexpressed arguments of the
verb, in Japanese. The syntactic cues in Japanese discourse that we investigate
are the morphological markers for grammatical {\sc topic}, the postposition
{\it wa}, as well as those for grammatical functions such as {\sc subject},
{\em ga}, {\sc object}, {\em o} and {\sc object2}, {\em ni}. In addition, we
investigate the role of speaker's {\sc empathy}, which is the viewpoint from
which an event is described. This is syntactically indicated through the use of
verbal compounding, i.e. the auxiliary use of verbs such as {\it kureta, kita}.
Our results are based on a survey of native speakers of their interpretation of
short discourses, consisting of minimal pairs, varied by one of the above
factors. We demonstrate that these syntactic cues do indeed affect the
interpretation of {\sc zeros}, but that having previously been the {\sc topic}
and being realized as a {\sc zero} also contributes to the salience of a
discourse entity. We propose a discourse rule of {\sc zero topic assignment},
and show that {\sc centering} provides constraints on when a {\sc zero} can be
interpreted as the {\sc zero topic}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609007</id><created>1996-09-24</created><authors><author><keyname>Iida</keyname><forenames>Masayo</forenames><affiliation>Fujitsu Software Corporation</affiliation></author></authors><title>Discourse Coherence and Shifting Centers in Japanese Texts</title><categories>cmp-lg cs.CL</categories><comments>20 pages, uses elsart12.sty, lingmacros.sty, named.sty</comments><journal-ref>Centering in Discourse, Oxford University Press; Eds. Walker,
  Joshi and Prince, In Press</journal-ref><abstract>  In languages such as Japanese, the use of {\it zeros}, unexpressed arguments
of the verb, in utterances that shift the topic involves a risk that the
meaning intended by the speaker may not be transparent to the hearer. However,
this potentially undesirable conversational strategy often occurs in the course
of naturally-occurring discourse. In this chapter, I report on an empirical
study of 250 utterances with {\it zeros} in 20 Japanese newspaper articles.
Each utterance is analyzed in terms of centering transitions and the form in
which centers are realized by referring expressions. I also examine lexical
subcategorization information, and tense and aspect in order to test the
hypothesis that the speaker expects the hearer to use this information in
determining global discourse structure. I explain the occurrence of {\it zeros}
in {\sc retain} and {\sc rough-shift} centering transitions, by claiming that a
{\it zero} can only be used in these cases when the shift of centers is
supported by contextual information such as lexical semantics, tense and
aspect, and agreement features. I then propose an algorithm by which centering
can incorporate these observations to integrate centering with global discourse
structure, and thus enhance its ability for non-local pronoun resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609008</id><created>1996-09-25</created><authors><author><keyname>Lauer</keyname><forenames>Mark</forenames><affiliation>Microsoft Research Institute, Sydney</affiliation></author></authors><title>Designing Statistical Language Learners: Experiments on Noun Compounds</title><categories>cmp-lg cs.CL</categories><comments>PhD thesis (Macquarie University, Sydney; December 1995), LaTeX
  source, xii+214 pages</comments><abstract>  The goal of this thesis is to advance the exploration of the statistical
language learning design space. In pursuit of that goal, the thesis makes two
main theoretical contributions: (i) it identifies a new class of designs by
specifying an architecture for natural language analysis in which probabilities
are given to semantic forms rather than to more superficial linguistic
elements; and (ii) it explores the development of a mathematical theory to
predict the expected accuracy of statistical language learning systems in terms
of the volume of data used to train them.
  The theoretical work is illustrated by applying statistical language learning
designs to the analysis of noun compounds. Both syntactic and semantic analysis
of noun compounds are attempted using the proposed architecture. Empirical
comparisons demonstrate that the proposed syntactic model is significantly
better than those previously suggested, approaching the performance of human
judges on the same task, and that the proposed semantic model, the first
statistical approach to this problem, exhibits significantly better accuracy
than the baseline strategy. These results suggest that the new class of designs
identified is a promising one. The experiments also serve to highlight the need
for a widely applicable theory of data requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609009</id><created>1996-09-28</created><updated>1996-09-30</updated><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Geometric Approach to Mapping Bitext Correspondence</title><categories>cmp-lg cs.CL</categories><comments>15 pages, minor revisions on Sept. 30, 1996</comments><report-no>IRCS 96-22</report-no><abstract>  The first step in most corpus-based multilingual NLP work is to construct a
detailed map of the correspondence between a text and its translation. Several
automatic methods for this task have been proposed in recent years. Yet even
the best of these methods can err by several typeset pages. The Smooth
Injective Map Recognizer (SIMR) is a new bitext mapping algorithm. SIMR's
errors are smaller than those of the previous front-runner by more than a
factor of 4. Its robustness has enabled new commercial-quality applications.
The greedy nature of the algorithm makes it independent of memory resources.
Unlike other bitext mapping algorithms, SIMR allows crossing correspondences to
account for word order differences. Its output can be converted quickly and
easily into a sentence alignment. SIMR's output has been used to align over 200
megabytes of the Canadian Hansards for publication by the Linguistic Data
Consortium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9609010</id><created>1996-09-28</created><updated>1996-09-30</updated><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Automatic Detection of Omissions in Translations</title><categories>cmp-lg cs.CL</categories><comments>6 pages, minor revisions on Sept. 30, 1996</comments><report-no>IRCS 96-23</report-no><abstract>  ADOMIT is an algorithm for Automatic Detection of OMIssions in Translations.
The algorithm relies solely on geometric analysis of bitext maps and uses no
linguistic information. This property allows it to deal equally well with
omissions that do not correspond to linguistic units, such as might result from
word-processing mishaps. ADOMIT has proven itself by discovering many errors in
a hand-constructed gold standard for evaluating bitext mapping algorithms.
Quantitative evaluation on simulated omissions showed that, even with today's
poor bitext mapping technology, ADOMIT is a valuable quality control tool for
translators and translation bureaus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9610001</id><created>1996-10-02</created><authors><author><keyname>Dras</keyname><forenames>Mark</forenames><affiliation>Microsoft Institute and Dept of Computing, Macquarie University</affiliation></author><author><keyname>Johnson</keyname><forenames>Mike</forenames><affiliation>Microsoft Institute and Dept of Computing, Macquarie University</affiliation></author></authors><title>Death and Lightness: Using a Demographic Model to Find Support Verbs</title><categories>cmp-lg cs.CL</categories><comments>LaTeX, 8 pages, uses aclap.sty</comments><journal-ref>CSNLP-96, Sept. 2-4, Dublin, Ireland</journal-ref><abstract>  Some verbs have a particular kind of binary ambiguity: they can carry their
normal, full meaning, or they can be merely acting as a prop for the nominal
object. It has been suggested that there is a detectable pattern in the
relationship between a verb acting as a prop (a \term{support verb}) and the
noun it supports.
  The task this paper undertakes is to develop a model which identifies the
support verb for a particular noun, and by extension, when nouns are
enumerated, a model which disambiguates a verb with respect to its support
status. The paper sets up a basic model as a standard for comparison; it then
proposes a more complex model, and gives some results to support the model's
validity, comparing it with other similar approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9610002</id><created>1996-10-21</created><authors><author><keyname>Siegel</keyname><forenames>Eric V.</forenames><affiliation>Columbia University</affiliation></author><author><keyname>McKeown</keyname><forenames>Kathleen R.</forenames><affiliation>Columbia University</affiliation></author></authors><title>Gathering Statistics to Aspectually Classify Sentences with a Genetic
  Algorithm</title><categories>cmp-lg cs.CL</categories><comments>postscript, 9 pages, Proceedings of the Second International
  Conference on New Methods in Language Processing, Oflazer and Somers ed.</comments><abstract>  This paper presents a method for large corpus analysis to semantically
classify an entire clause. In particular, we use cooccurrence statistics among
similar clauses to determine the aspectual class of an input clause. The
process examines linguistic features of clauses that are relevant to aspectual
classification. A genetic algorithm determines what combinations of linguistic
features to use for this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9610003</id><created>1996-10-23</created><authors><author><keyname>Abney</keyname><forenames>Steven</forenames><affiliation>University of Tuebingen</affiliation></author></authors><title>Stochastic Attribute-Value Grammars</title><categories>cmp-lg cs.CL</categories><comments>23 pages, 21 Postscript figures, uses rotate.sty</comments><abstract>  Probabilistic analogues of regular and context-free grammars are well-known
in computational linguistics, and currently the subject of intensive research.
To date, however, no satisfactory probabilistic analogue of attribute-value
grammars has been proposed: previous attempts have failed to define a correct
parameter-estimation algorithm.
  In the present paper, I define stochastic attribute-value grammars and give a
correct algorithm for estimating their parameters. The estimation algorithm is
adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model
parameters, it is necessary to compute the expectations of certain functions
under random fields. In the application discussed by Della Pietra, Della
Pietra, and Lafferty (representing English orthographic constraints), Gibbs
sampling can be used to estimate the needed expectations. The fact that
attribute-value grammars generate constrained languages makes Gibbs sampling
inapplicable, but I show how a variant of Gibbs sampling, the
Metropolis-Hastings algorithm, can be used instead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9610004</id><created>1996-10-25</created><authors><author><keyname>Zhang</keyname><forenames>Min</forenames><affiliation>University of Melbourne</affiliation></author></authors><title>A Faster Structured-Tag Word-Classification Method</title><categories>cmp-lg cs.CL</categories><comments>10 pages, Microsoft Word 6.0, ps</comments><journal-ref>27-Aug-96 PRICAI-96 Workshop on Future Issues for Multi-lingual
  Text Processing, Cairns, Australia. ISBN 0 86857 730 8</journal-ref><abstract>  Several methods have been proposed for processing a corpus to induce a tagset
for the sub-language represented by the corpus. This paper examines a
structured-tag word classification method introduced by McMahon (1994) and
discussed further by McMahon &amp; Smith (1995) in cmp-lg/9503011 . Two major
variations, (1) non-random initial assignment of words to classes and (2)
moving multiple words in parallel, together provide robust non-random results
with a speed increase of 200% to 450%, at the cost of slightly lower quality
than McMahon's method's average quality. Two further variations, (3) retaining
information from less- frequent words and (4) avoiding reclustering closed
classes, are proposed for further study.
  Note: The speed increases quoted above are relative to my implementation of
my understanding of McMahon's algorithm; this takes time measured in hours and
days on a home PC. A revised version of the McMahon &amp; Smith (1995) paper has
appeared (June 1996) in Computational Linguistics 22(2):217- 247; this refers
to a time of &quot;several weeks&quot; to cluster 569 words on a Sparc-IPC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9610005</id><created>1996-10-29</created><updated>1997-11-02</updated><authors><author><keyname>Ristad</keyname><forenames>Eric Sven</forenames></author><author><keyname>Yianilos</keyname><forenames>Peter N.</forenames></author></authors><title>Learning string edit distance</title><categories>cmp-lg cs.CL</categories><comments>http://www.cs.princeton.edu/~ristad/papers/pu-532-96.ps.gz</comments><report-no>CS-TR-532-96</report-no><abstract>  In many applications, it is necessary to determine the similarity of two
strings. A widely-used notion of string similarity is the edit distance: the
minimum number of insertions, deletions, and substitutions required to
transform one string into the other. In this report, we provide a stochastic
model for string edit distance. Our stochastic model allows us to learn a
string edit distance function from a corpus of examples. We illustrate the
utility of our approach by applying it to the difficult problem of learning the
pronunciation of words in conversational speech. In this application, we learn
a string edit distance with one fourth the error rate of the untrained
Levenshtein distance. Our approach is applicable to any string classification
problem that may be solved using a similarity function against a database of
labeled prototypes.
  Keywords: string edit distance, Levenshtein distance, stochastic
transduction, syntactic pattern recognition, prototype dictionary, spelling
correction, string correction, string similarity, string classification, speech
recognition, pronunciation modeling, Switchboard corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9610006</id><created>1996-10-30</created><authors><author><keyname>Lezius</keyname><forenames>Wolfgang</forenames><affiliation>University of Paderborn</affiliation></author><author><keyname>Rapp</keyname><forenames>Reinhard</forenames><affiliation>University of Paderborn</affiliation></author><author><keyname>Wettler</keyname><forenames>Manfred</forenames><affiliation>University of Paderborn</affiliation></author></authors><title>A Morphology-System and Part-of-Speech Tagger for German</title><categories>cmp-lg cs.CL</categories><comments>10 pages</comments><journal-ref>In:D.Gibbon,ed.,Natural Language Processing and Speech Technology.
  Results of the 3rd KONVENS Conference. Mouton de Gruyter, Berlin, 1996.</journal-ref><abstract>  This paper presents an integrated tool for German morphology and statistical
part-of-speech tagging which aims at making some well established methods
widely available. The software is very user friendly, runs on any PC and can be
downloaded as a complete package (including lexicon and documentation) from the
World Wide Web. Compared with the performance of other tagging systems the
tagger produces similar results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611001</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9611001</id><created>1996-11-12</created><authors><author><keyname>Walther</keyname><forenames>Markus</forenames><affiliation>University of Duesseldorf, Germany</affiliation></author></authors><title>OT SIMPLE - a construction-kit approach to Optimality Theory
  implementation</title><categories>cmp-lg cs.CL</categories><comments>52 pages, uses examples.sty, chicago.sty. IPA phonetic font macro
  package ipa.sty and necessary WSU IPA fonts included. Source code of the
  software described available from
  http://www.phil-fak.uni-duesseldorf.de/~walther/otsimple.html</comments><report-no>Arbeiten des Sonderforschungsbereichs 282, Nr. 88</report-no><abstract>  This paper details a simple approach to the implementation of Optimality
Theory (OT, Prince and Smolensky 1993) on a computer, in part reusing standard
system software. In a nutshell, OT's GENerating source is implemented as a
BinProlog program interpreting a context-free specification of a GEN structural
grammar according to a user-supplied input form. The resulting set of textually
flattened candidate tree representations is passed to the CONstraint stage.
Constraints are implemented by finite-state transducers specified as `sed'
stream editor scripts that typically map ill-formed portions of the candidate
to violation marks. EVALuation of candidates reduces to simple sorting: the
violation-mark-annotated output leaving CON is fed into `sort', which orders
candidates on the basis of the violation vector column of each line, thereby
bringing the optimal candidate to the top. This approach gave rise to OT
SIMPLE, the first freely available software tool for the OT framework to
provide generic facilities for both GEN and CONstraint definition. Its
practical applicability is demonstrated by modelling the OT analysis of
apparent subtractive pluralization in Upper Hessian presented in Golston and
Wiese (1996).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9611002</id><created>1996-11-12</created><authors><author><keyname>de Marcken</keyname><forenames>Carl</forenames><affiliation>MIT</affiliation></author></authors><title>Unsupervised Language Acquisition</title><categories>cmp-lg cs.CL</categories><comments>PhD thesis, 133 pages</comments><abstract>  This thesis presents a computational theory of unsupervised language
acquisition, precisely defining procedures for learning language from ordinary
spoken or written utterances, with no explicit help from a teacher. The theory
is based heavily on concepts borrowed from machine learning and statistical
estimation. In particular, learning takes place by fitting a stochastic,
generative model of language to the evidence. Much of the thesis is devoted to
explaining conditions that must hold for this general learning strategy to
arrive at linguistically desirable grammars. The thesis introduces a variety of
technical innovations, among them a common representation for evidence and
grammars, and a learning strategy that separates the ``content'' of linguistic
parameters from their representation. Algorithms based on it suffer from few of
the search problems that have plagued other computational approaches to
language acquisition.
  The theory has been tested on problems of learning vocabularies and grammars
from unsegmented text and continuous speech, and mappings between sound and
representations of meaning. It performs extremely well on various objective
criteria, acquiring knowledge that causes it to assign almost exactly the same
structure to utterances as humans do. This work has application to data
compression, language modeling, speech recognition, machine translation,
information retrieval, and other tasks that rely on either structural or
stochastic descriptions of language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9611003</id><created>1996-11-14</created><authors><author><keyname>Bod</keyname><forenames>Rens</forenames><affiliation>University of Amsterdam</affiliation></author><author><keyname>Scha</keyname><forenames>Remko</forenames><affiliation>University of Amsterdam</affiliation></author></authors><title>Data-Oriented Language Processing. An Overview</title><categories>cmp-lg cs.CL</categories><comments>34 pages, Postscript</comments><report-no>ILLC Technical Report LP-96-13</report-no><abstract>  During the last few years, a new approach to language processing has started
to emerge, which has become known under various labels such as &quot;data-oriented
parsing&quot;, &quot;corpus-based interpretation&quot;, and &quot;tree-bank grammar&quot; (cf. van den
Berg et al. 1994; Bod 1992-96; Bod et al. 1996a/b; Bonnema 1996; Charniak
1996a/b; Goodman 1996; Kaplan 1996; Rajman 1995a/b; Scha 1990-92; Sekine &amp;
Grishman 1995; Sima'an et al. 1994; Sima'an 1995-96; Tugwell 1995). This
approach, which we will call &quot;data-oriented processing&quot; or &quot;DOP&quot;, embodies the
assumption that human language perception and production works with
representations of concrete past language experiences, rather than with
abstract linguistic rules. The models that instantiate this approach therefore
maintain large corpora of linguistic representations of previously occurring
utterances. When processing a new input utterance, analyses of this utterance
are constructed by combining fragments from the corpus; the
occurrence-frequencies of the fragments are used to estimate which analysis is
the most probable one.
  In this paper we give an in-depth discussion of a data-oriented processing
model which employs a corpus of labelled phrase-structure trees. Then we review
some other models that instantiate the DOP approach. Many of these models also
employ labelled phrase-structure trees, but use different criteria for
extracting fragments from the corpus or employ different disambiguation
strategies (Bod 1996b; Charniak 1996a/b; Goodman 1996; Rajman 1995a/b; Sekine &amp;
Grishman 1995; Sima'an 1995-96); other models use richer formalisms for their
corpus annotations (van den Berg et al. 1994; Bod et al., 1996a/b; Bonnema
1996; Kaplan 1996; Tugwell 1995).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9611004</id><created>1996-11-16</created><authors><author><keyname>Ristad</keyname><forenames>Eric Sven</forenames></author><author><keyname>Thomas</keyname><forenames>Robert G.</forenames></author></authors><title>Nonuniform Markov models</title><categories>cmp-lg cs.CL</categories><comments>17 pages</comments><report-no>CS-TR-536-96</report-no><abstract>  A statistical language model assigns probability to strings of arbitrary
length. Unfortunately, it is not possible to gather reliable statistics on
strings of arbitrary length from a finite corpus. Therefore, a statistical
language model must decide that each symbol in a string depends on at most a
small, finite number of other symbols in the string. In this report we propose
a new way to model conditional independence in Markov models. The central
feature of our nonuniform Markov model is that it makes predictions of varying
lengths using contexts of varying lengths. Experiments on the Wall Street
Journal reveal that the nonuniform model performs slightly better than the
classic interpolated Markov model. This result is somewhat remarkable because
both models contain identical numbers of parameters whose values are estimated
in a similar manner. The only difference between the two models is how they
combine the statistics of longer and shorter strings.
  Keywords: nonuniform Markov model, interpolated Markov model, conditional
independence, statistical language model, discrete time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9611005</id><created>1996-11-18</created><authors><author><keyname>Lee</keyname><forenames>Geunbae</forenames><affiliation>Department of Computer Science and Engineering, Pohang University of Science and Technology</affiliation></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames><affiliation>Department of Computer Science and Engineering, Pohang University of Science and Technology</affiliation></author><author><keyname>Kim</keyname><forenames>Sangeok</forenames><affiliation>Department of Computer Science and Engineering, Pohang University of Science and Technology</affiliation></author></authors><title>Integrating HMM-Based Speech Recognition With Direct Manipulation In A
  Multimodal Korean Natural Language Interface</title><categories>cmp-lg cs.CL</categories><comments>6 pages, ps file, presented at icmi96 (Bejing)</comments><abstract>  This paper presents a HMM-based speech recognition engine and its integration
into direct manipulation interfaces for Korean document editor. Speech
recognition can reduce typical tedious and repetitive actions which are
inevitable in standard GUIs (graphic user interfaces). Our system consists of
general speech recognition engine called ABrain {Auditory Brain} and
speech commandable document editor called SHE {Simple Hearing Editor}.
ABrain is a phoneme-based speech recognition engine which shows up to 97% of
discrete command recognition rate. SHE is a EuroBridge widget-based document
editor that supports speech commands as well as direct manipulation interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9611006</id><created>1996-11-24</created><authors><author><keyname>Androutsopoulos</keyname><forenames>I.</forenames><affiliation>Microsoft Research Institute, Macquarie University, Sydney</affiliation></author><author><keyname>Ritchie</keyname><forenames>G. D.</forenames><affiliation>Dept. of Artificial Intelligence, University of Edinburgh</affiliation></author><author><keyname>Thanisch</keyname><forenames>P.</forenames><affiliation>Dept. of Computer Science, University of Edinburgh</affiliation></author></authors><title>A Framework for Natural Language Interfaces to Temporal Databases</title><categories>cmp-lg cs.CL</categories><comments>9 pages, uses the included acsc.sty, and the included *.eps figures.
  To appear in the 20th Australasian Computer Science Conference, Sydney,
  February 1997</comments><journal-ref>Proc. of the 20th Australasian Computer Science Conference,
  Sydney, 1997, pp. 307-315.</journal-ref><abstract>  Over the past thirty years, there has been considerable progress in the
design of natural language interfaces to databases. Most of this work has
concerned snapshot databases, in which there are only limited facilities for
manipulating time-varying information. The database community is becoming
increasingly interested in temporal databases, databases with special support
for time-dependent entries. We have developed a framework for constructing
natural language interfaces to temporal databases, drawing on research on
temporal phenomena within logic and linguistics. The central part of our
framework is a logic-like formal language, called TOP, which can capture the
semantics of a wide range of English sentences. We have implemented an
HPSG-based sentence analyser that converts a large set of English queries
involving time into TOP formulae, and have formulated a provably correct
procedure for translating TOP expressions into queries in the TSQL2 temporal
database language. In this way we have established a sound route from English
to a general-purpose temporal database language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9612001</id><created>1996-12-09</created><authors><author><keyname>Mooney</keyname><forenames>Raymond J.</forenames><affiliation>University of Texas at Austin</affiliation></author></authors><title>Comparative Experiments on Disambiguating Word Senses: An Illustration
  of the Role of Bias in Machine Learning</title><categories>cmp-lg cs.CL</categories><comments>10 pages</comments><abstract>  This paper describes an experimental comparison of seven different learning
algorithms on the problem of learning to disambiguate the meaning of a word
from context. The algorithms tested include statistical, neural-network,
decision-tree, rule-based, and case-based classification techniques. The
specific problem tested involves disambiguating six senses of the word ``line''
using the words in the current and proceeding sentence as context. The
statistical and neural-network methods perform the best on this particular
problem and we discuss a potential reason for this observed difference. We also
discuss the role of bias in machine learning and its importance in explaining
performance differences observed on specific problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9612002</id><created>1996-12-12</created><updated>1996-12-13</updated><authors><author><keyname>Popovici</keyname><forenames>Cosmin</forenames><affiliation>ICI - Bucuresti, Romania</affiliation></author><author><keyname>Baggia</keyname><forenames>Paolo</forenames><affiliation>CSELT - Turin, Italy</affiliation></author></authors><title>Specialized Language Models using Dialogue Predictions</title><categories>cmp-lg cs.CL</categories><comments>4 pages, LaTeX, 2 eps figures, uses icassp.sty, a4.sty and psfig.tex;
  to appear in Proc. of ICASSP 1997, Munich, Germany</comments><abstract>  This paper analyses language modeling in spoken dialogue systems for
accessing a database. The use of several language models obtained by exploiting
dialogue predictions gives better results than the use of a single model for
the whole dialogue interaction. For this reason several models have been
created, each one for a specific system question, such as the request or the
confirmation of a parameter.
  The use of dialogue-dependent language models increases the performance both
at the recognition and at the understanding level, especially on answers to
system requests. Moreover other methods to increase performance, like automatic
clustering of vocabulary words or the use of better acoustic models during
recognition, does not affect the improvements given by dialogue-dependent
language models.
  The system used in our experiments is Dialogos, the Italian spoken dialogue
system used for accessing railway timetable information over the telephone. The
experiments were carried out on a large corpus of dialogues collected using
Dialogos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9612003</id><created>1996-12-17</created><authors><author><keyname>Danieli</keyname><forenames>Morena</forenames><affiliation>CSELT - Torino, Italy</affiliation></author><author><keyname>Gerbino</keyname><forenames>Elisabetta</forenames><affiliation>CSELT - Torino, Italy</affiliation></author></authors><title>Metrics for Evaluating Dialogue Strategies in a Spoken Language System</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTex, uses aaai.sty; presented at the 1995 AAAI Spring
  Symposium Series (Symposium: Empirical Methods in Discourse Interpretation
  and Generation)</comments><abstract>  In this paper, we describe a set of metrics for the evaluation of different
dialogue management strategies in an implemented real-time spoken language
system. The set of metrics we propose offers useful insights in evaluating how
particular choices in the dialogue management can affect the overall quality of
the man-machine dialogue. The evaluation makes use of established metrics: the
transaction success, the contextual appropriateness of system answers, the
calculation of normal and correction turns in a dialogue. We also define a new
metric, the implicit recovery, which allows to measure the ability of a
dialogue manager to deal with errors by different levels of analysis. We report
evaluation data from several experiments, and we compare two different
approaches to dialogue repair strategies using the set of metrics we argue for.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9612004</id><created>1996-12-20</created><authors><author><keyname>Albesano</keyname><forenames>Dario</forenames><affiliation>CSELT - Turin, Italy</affiliation></author><author><keyname>Baggia</keyname><forenames>Paolo</forenames><affiliation>CSELT - Turin, Italy</affiliation></author><author><keyname>Danieli</keyname><forenames>Morena</forenames><affiliation>CSELT - Turin, Italy</affiliation></author><author><keyname>Gemello</keyname><forenames>Roberto</forenames><affiliation>CSELT - Turin, Italy</affiliation></author><author><keyname>Gerbino</keyname><forenames>Elisabetta</forenames><affiliation>CSELT - Turin, Italy</affiliation></author><author><keyname>Rullent</keyname><forenames>Claudio</forenames><affiliation>CSELT - Turin, Italy</affiliation></author></authors><title>Dialogos: a Robust System for Human-Machine Spoken Dialogue on the
  Telephone</title><categories>cmp-lg cs.CL</categories><comments>4 pages, LaTeX, 1 eps figures, uses icassp91.sty, and psfig.tex; to
  appear in Proc. of ICASSP 1997, Munich, Germany</comments><abstract>  This paper presents Dialogos, a real-time system for human-machine spoken
dialogue on the telephone in task-oriented domains. The system has been tested
in a large trial with inexperienced users and it has proved robust enough to
allow spontaneous interactions both to users which get good recognition
performance and to the ones which get lower scores. The robust behavior of the
system has been achieved by combining the use of specific language models
during the recognition phase of analysis, the tolerance toward spontaneous
speech phenomena, the activity of a robust parser, and the use of
pragmatic-based dialogue knowledge. This integration of the different modules
allows to deal with partial or total breakdowns of the different levels of
analysis. We report the field trial data of the system and the evaluation
results of the overall system and of the submodules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9612005</id><created>1996-12-31</created><authors><author><keyname>Ristad</keyname><forenames>Eric Sven</forenames></author></authors><title>Maximum Entropy Modeling Toolkit</title><categories>cmp-lg cs.CL</categories><comments>32 pages, texinfo format</comments><abstract>  The Maximum Entropy Modeling Toolkit supports parameter estimation and
prediction for statistical language models in the maximum entropy framework.
The maximum entropy framework provides a constructive method for obtaining the
unique conditional distribution p*(y|x) that satisfies a set of linear
constraints and maximizes the conditional entropy H(p|f) with respect to the
empirical distribution f(x). The maximum entropy distribution p*(y|x) also has
a unique parametric representation in the class of exponential models, as
m(y|x) = r(y|x)/Z(x) where the numerator m(y|x) = prod_i alpha_i^g_i(x,y) is a
product of exponential weights, with alpha_i = exp(lambda_i), and the
denominator Z(x) = sum_y r(y|x) is required to satisfy the axioms of
probability.
  This manual explains how to build maximum entropy models for discrete domains
with the Maximum Entropy Modeling Toolkit (MEMT). First we summarize the steps
necessary to implement a language model using the toolkit. Next we discuss the
executables provided by the toolkit and explain the file formats required by
the toolkit. Finally, we review the maximum entropy framework and apply it to
the problem of statistical language modeling.
  Keywords: statistical language models, maximum entropy, exponential models,
improved iterative scaling, Markov models, triggers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9701001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9701001</id><created>1997-01-02</created><authors><author><keyname>Zhai</keyname><forenames>Chengxiang</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Exploiting Context to Identify Lexical Atoms -- A Statistical View of
  Linguistic Context</title><categories>cmp-lg cs.CL</categories><comments>10 pages, postscript file, to appear in Proceedings of International
  and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97)</comments><journal-ref>Proceedings of the International and Interdisciplinary Conference
  on Modelling and Using Context (CONTEXT-97), Rio de Janeiro, Brzil, Feb. 4-6,
  1997. 119-129.</journal-ref><abstract>  Interpretation of natural language is inherently context-sensitive. Most
words in natural language are ambiguous and their meanings are heavily
dependent on the linguistic context in which they are used. The study of
lexical semantics can not be separated from the notion of context. This paper
takes a contextual approach to lexical semantics and studies the linguistic
context of lexical atoms, or &quot;sticky&quot; phrases such as &quot;hot dog&quot;. Since such
lexical atoms may occur frequently in unrestricted natural language text,
recognizing them is crucial for understanding naturally-occurring text. The
paper proposes several heuristic approaches to exploiting the linguistic
context to identify lexical atoms from arbitrary natural language text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9701002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9701002</id><created>1997-01-02</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge</affiliation></author></authors><title>Hybrid language processing in the Spoken Language Translator</title><categories>cmp-lg cs.CL</categories><comments>4 pages, uses icassp97.sty; to appear in ICASSP-97; see
  http://www.cam.sri.com for related material</comments><report-no>CRC-064</report-no><abstract>  The paper presents an overview of the Spoken Language Translator (SLT)
system's hybrid language-processing architecture, focussing on the way in which
rule-based and statistical methods are combined to achieve robust and efficient
performance within a linguistically motivated framework. In general, we argue
that rules are desirable in order to encode domain-independent linguistic
constraints and achieve high-quality grammatical output, while corpus-derived
statistics are needed if systems are to be efficient and robust; further, that
hybrid architectures are superior from the point of view of portability to
architectures which only make use of one type of information. We address the
topics of ``multi-engine'' strategies for robust translation; robust bottom-up
parsing using pruning and grammar specialization; rational development of
linguistic rule-sets using balanced domain corpora; and efficient supervised
training by interactive disambiguation. All work described is fully implemented
in the current version of the SLT-2 system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9701003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9701003</id><created>1997-01-06</created><authors><author><keyname>Chu-Carroll</keyname><forenames>Jennifer</forenames><affiliation>Bell Laboratories</affiliation></author><author><keyname>Carberry</keyname><forenames>Sandra</forenames><affiliation>University of Delaware</affiliation></author></authors><title>Generating Information-Sharing Subdialogues in Expert-User Consultation</title><categories>cmp-lg cs.CL</categories><comments>9 pages, 1 figure; uses epsf.sty, times.sty, and named</comments><journal-ref>IJCAI'95</journal-ref><abstract>  In expert-consultation dialogues, it is inevitable that an agent will at
times have insufficient information to determine whether to accept or reject a
proposal by the other agent. This results in the need for the agent to initiate
an information-sharing subdialogue to form a set of shared beliefs within which
the agents can effectively re-evaluate the proposal. This paper presents a
computational strategy for initiating such information-sharing subdialogues to
resolve the system's uncertainty regarding the acceptance of a user proposal.
Our model determines when information-sharing should be pursued, selects a
focus of information-sharing among multiple uncertain beliefs, chooses the most
effective information-sharing strategy, and utilizes the newly obtained
information to re-evaluate the user proposal. Furthermore, our model is capable
of handling embedded information-sharing subdialogues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9701004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9701004</id><created>1997-01-17</created><authors><author><keyname>van Noord</keyname><forenames>Gertjan</forenames><affiliation>Alfa-informatica, BCN, University of Groningen</affiliation></author></authors><title>An Efficient Implementation of the Head-Corner Parser</title><categories>cmp-lg cs.CL</categories><comments>31 pages, uses cl.sty</comments><abstract>  This paper describes an efficient and robust implementation of a
bi-directional, head-driven parser for constraint-based grammars. This parser
is developed for the OVIS system: a Dutch spoken dialogue system in which
information about public transport can be obtained by telephone.
  After a review of the motivation for head-driven parsing strategies, and
head-corner parsing in particular, a non-deterministic version of the
head-corner parser is presented. A memoization technique is applied to obtain a
fast parser. A goal-weakening technique is introduced which greatly improves
average case efficiency, both in terms of speed and space requirements.
  I argue in favor of such a memoization strategy with goal-weakening in
comparison with ordinary chart-parsers because such a strategy can be applied
selectively and therefore enormously reduces the space requirements of the
parser, while no practical loss in time-efficiency is observed. On the
contrary, experiments are described in which head-corner and left-corner
parsers implemented with selective memoization and goal weakening outperform
`standard' chart parsers. The experiments include the grammar of the OVIS
system and the Alvey NL Tools grammar.
  Head-corner parsing is a mix of bottom-up and top-down processing. Certain
approaches towards robust parsing require purely bottom-up processing.
Therefore, it seems that head-corner parsing is unsuitable for such robust
parsing techniques. However, it is shown how underspecification (which arises
very naturally in a logic programming environment) can be used in the
head-corner parser to allow such robust parsing techniques. A particular robust
parsing model is described which is implemented in OVIS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702001</id><created>1997-02-03</created><authors><author><keyname>Wermter</keyname><forenames>Stefan</forenames><affiliation>University of Hamburg</affiliation></author><author><keyname>Weber</keyname><forenames>Volker</forenames><affiliation>University of Hamburg</affiliation></author></authors><title>SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis
  Using Artificial Neural Networks</title><categories>cmp-lg cs.CL</categories><comments>51 pages, Postscript. To be published in Journal of Artificial
  Intelligence Research 6(1), 1997</comments><abstract>  In this paper, we describe a so-called screening approach for learning robust
processing of spontaneously spoken language. A screening approach is a flat
analysis which uses shallow sequences of category representations for analyzing
an utterance at various syntactic, semantic and dialog levels. Rather than
using a deeply structured symbolic analysis, we use a flat connectionist
analysis. This screening approach aims at supporting speech and language
processing by using (1) data-driven learning and (2) robustness of
connectionist networks. In order to test this approach, we have developed the
SCREEN system which is based on this new robust, learned and flat analysis.
 In this paper, we focus on a detailed description of SCREEN's architecture,
the flat syntactic and semantic analysis, the interaction with a speech
recognizer, and a detailed evaluation analysis of the robustness under the
influence of noisy or incomplete input. The main result of this paper is that
flat representations allow more robust processing of spontaneous spoken
language than deeply structured representations. In particular, we show how the
fault-tolerance and learning capability of connectionist networks can support a
flat analysis for providing more robust spoken-language processing within an
overall hybrid symbolic/connectionist framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702002</id><created>1997-02-04</created><authors><author><keyname>Briscoe</keyname><forenames>Ted</forenames><affiliation>Cambridge University</affiliation></author><author><keyname>Carroll</keyname><forenames>John</forenames><affiliation>University of Sussex</affiliation></author></authors><title>Automatic Extraction of Subcategorization from Corpora</title><categories>cmp-lg cs.CL</categories><comments>8 pages; requires aclap.sty. To appear in ANLP-97</comments><abstract>  We describe a novel technique and implemented system for constructing a
subcategorization dictionary from textual corpora. Each dictionary entry
encodes the relative frequency of occurrence of a comprehensive set of
subcategorization classes for English. An initial experiment, on a sample of 14
verbs which exhibit multiple complementation patterns, demonstrates that the
technique achieves accuracy comparable to previous approaches, which are all
limited to a highly restricted set of subcategorization classes. We also
demonstrate that a subcategorization dictionary built with the system improves
the accuracy of a parser by an appreciable amount.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702003</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702003</id><created>1997-02-05</created><authors><author><keyname>Ingels</keyname><forenames>Peter</forenames><affiliation>Linkoping University, Sweden</affiliation></author></authors><title>A Robust Text Processing Technique Applied to Lexical Error Recovery</title><categories>cmp-lg cs.CL</categories><comments>Licentiate Thesis, 101 pages</comments><abstract>  This thesis addresses automatic lexical error recovery and tokenization of
corrupt text input. We propose a technique that can automatically correct
misspellings, segmentation errors and real-word errors in a unified framework
that uses both a model of language production and a model of the typing
behavior, and which makes tokenization part of the recovery process.
  The typing process is modeled as a noisy channel where Hidden Markov Models
are used to model the channel characteristics. Weak statistical language models
are used to predict what sentences are likely to be transmitted through the
channel. These components are held together in the Token Passing framework
which provides the desired tight coupling between orthographic pattern matching
and linguistic expectation.
  The system, CTR (Connected Text Recognition), has been tested on two corpora
derived from two different applications, a natural language dialogue system and
a transcription typing scenario. Experiments show that CTR can automatically
correct a considerable portion of the errors in the test sets without
introducing too much noise. The segmentation error correction rate is virtually
faultless.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702004</id><created>1997-02-10</created><authors><author><keyname>Skut</keyname><forenames>Wojciech</forenames><affiliation>University of the Saarland, Saarbr&quot;ucken, Germany</affiliation></author><author><keyname>Krenn</keyname><forenames>Brigitte</forenames><affiliation>University of the Saarland, Saarbr&quot;ucken, Germany</affiliation></author><author><keyname>Brants</keyname><forenames>Thorsten</forenames><affiliation>University of the Saarland, Saarbr&quot;ucken, Germany</affiliation></author><author><keyname>Uszkoreit</keyname><forenames>Hans</forenames><affiliation>University of the Saarland, Saarbr&quot;ucken, Germany</affiliation></author></authors><title>An Annotation Scheme for Free Word Order Languages</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX; uses aclap.sty, epsf.sty, and gb4e.sty</comments><abstract>  We describe an annotation scheme and a tool developed for creating
linguistically annotated corpora for non-configurational languages. Since the
requirements for such a formalism differ from those posited for configurational
languages, several features have been added, influencing the architecture of
the scheme. The resulting scheme reflects a stratificational notion of
language, and makes only minimal assumptions about the interrelation of the
particular representational strata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702005</id><created>1997-02-10</created><authors><author><keyname>Cunningham</keyname><forenames>Hamish</forenames></author><author><keyname>Humphreys</keyname><forenames>Kevin</forenames></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames></author><author><keyname>Wilks</keyname><forenames>Yorick</forenames></author></authors><title>Software Infrastructure for Natural Language Processing</title><categories>cmp-lg cs.CL</categories><comments>LaTeX, uses aclap.sty, 8 pages</comments><journal-ref>5th Conference on Applied Natural Language Processing, 1997</journal-ref><abstract>  We classify and review current approaches to software infrastructure for
research, development and delivery of NLP systems. The task is motivated by a
discussion of current trends in the field of NLP and Language Engineering. We
describe a system called GATE (a General Architecture for Text Engineering)
that provides a software infrastructure on top of which heterogeneous NLP
processing modules may be evaluated and refined individually, or may be
combined into larger application systems. GATE aims to support both researchers
and developers working on component technologies (e.g. parsing, tagging,
morphological analysis) and those working on developing end-user applications
(e.g. information extraction, text summarisation, document generation, machine
translation, and second language learning). GATE promotes reuse of component
technology, permits specialisation and collaboration in large-scale projects,
and allows for the comparison and evaluation of alternative technologies. The
first release of GATE is now available - see
http://www.dcs.shef.ac.uk/research/groups/nlp/gate/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702006</id><created>1997-02-10</created><updated>1997-02-11</updated><authors><author><keyname>Cunningham</keyname><forenames>Hamish</forenames></author></authors><title>Information Extraction - A User Guide</title><categories>cmp-lg cs.CL</categories><comments>LaTeX2e with PostScript figures, 17 pages (figures replaced with
  smaller versions)</comments><report-no>CS-97-02</report-no><abstract>  This technical memo describes Information Extraction from the point-of-view
of a potential user of the technology. No knowledge of language processing is
assumed. Information Extraction is a process which takes unseen texts as input
and produces fixed-format, unambiguous data as output. This data may be used
directly for display to users, or may be stored in a database or spreadsheet
for later analysis, or may be used for indexing purposes in Information
Retrieval applications. See also http://www.dcs.shef.ac.uk/~hamish
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702007</id><created>1997-02-11</created><authors><author><keyname>Busemann</keyname><forenames>Stephan</forenames><affiliation>DFKI GmbH</affiliation></author><author><keyname>Declerck</keyname><forenames>Thierry</forenames><affiliation>DFKI GmbH</affiliation></author><author><keyname>Diagne</keyname><forenames>Abdel Kader</forenames><affiliation>DFKI GmbH</affiliation></author><author><keyname>Dini</keyname><forenames>Luca</forenames><affiliation>DFKI GmbH</affiliation></author><author><keyname>Klein</keyname><forenames>Judith</forenames><affiliation>DFKI GmbH</affiliation></author><author><keyname>Schmeier</keyname><forenames>Sven</forenames><affiliation>DFKI GmbH</affiliation></author></authors><title>Natural Language Dialogue Service for Appointment Scheduling Agents</title><categories>cmp-lg cs.CL</categories><comments>8 or 9 pages, LaTeX; uses aclap.sty, epsf.tex</comments><journal-ref>Proc. 5th Conference on Applied Natural Language Processing, 1997</journal-ref><abstract>  Appointment scheduling is a problem faced daily by many individuals and
organizations. Cooperating agent systems have been developed to partially
automate this task. In order to extend the circle of participants as far as
possible we advocate the use of natural language transmitted by e-mail. We
describe COSMA, a fully implemented German language server for existing
appointment scheduling agent systems. COSMA can cope with multiple dialogues in
parallel, and accounts for differences in dialogue behaviour between human and
machine agents. NL coverage of the sublanguage is achieved through both
corpus-based grammar development and the use of message extraction techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702008</id><created>1997-02-11</created><authors><author><keyname>Pedersen</keyname><forenames>Ted</forenames><affiliation>SMU</affiliation></author><author><keyname>Bruce</keyname><forenames>Rebecca</forenames><affiliation>SMU</affiliation></author><author><keyname>Wiebe</keyname><forenames>Janyce</forenames><affiliation>NMSU</affiliation></author></authors><title>Sequential Model Selection for Word Sense Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Latex, uses aclap.sty</comments><journal-ref>Proceedings of the Fifth Conference on Applied Natural Language
  Processing, April 1997, Washington, DC</journal-ref><abstract>  Statistical models of word-sense disambiguation are often based on a small
number of contextual features or on a model that is assumed to characterize the
interactions among a set of features. Model selection is presented as an
alternative to these approaches, where a sequential search of possible models
is conducted in order to find the model that best characterizes the
interactions among features. This paper expands existing model selection
methodology and presents the first comparative study of model selection search
strategies and evaluation criteria when applied to the problem of building
probabilistic classifiers for word-sense disambiguation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702009</id><created>1997-02-12</created><authors><author><keyname>Zhai</keyname><forenames>Chengxiang</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Fast Statistical Parsing of Noun Phrases for Document Indexing</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTex; uses aclap.sty. To appear in Proceedings of the 5th
  Conference on Applied Natural Language Processing, Washington DC, 31 March -
  3 April, 1997.</comments><abstract>  Information Retrieval (IR) is an important application area of Natural
Language Processing (NLP) where one encounters the genuine challenge of
processing large quantities of unrestricted natural language text. While much
effort has been made to apply NLP techniques to IR, very few NLP techniques
have been evaluated on a document collection larger than several megabytes.
Many NLP techniques are simply not efficient enough, and not robust enough, to
handle a large amount of text. This paper proposes a new probabilistic model
for noun phrase parsing, and reports on the application of such a parsing
technique to enhance document indexing. The effectiveness of using syntactic
phrases provided by the parser to supplement single words for indexing is
evaluated with a 250 megabytes document collection. The experiment's results
show that supplementing single words with syntactic phrases for indexing
consistently and significantly improves retrieval performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702010</id><created>1997-02-17</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Inui</keyname><forenames>Kentaro</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Tokunaga</keyname><forenames>Takenobu</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Tanaka</keyname><forenames>Hozumi</forenames><affiliation>Tokyo Institute of Technology</affiliation></author></authors><title>Selective Sampling of Effective Example Sentence Sets for Word Sense
  Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>14 pages, uses epsbox.sty</comments><journal-ref>Proceedings of the Fourth Workshop on Very Large Corpora WVLC-4,
  pp. 56-69, 1996</journal-ref><abstract>  This paper proposes an efficient example selection method for example-based
word sense disambiguation systems. To construct a practical size database, a
considerable overhead for manual sense disambiguation is required. Our method
is characterized by the reliance on the notion of the training utility: the
degree to which each example is informative for future example selection when
used for the training of the system. The system progressively collects examples
by selecting those with greatest utility. The paper reports the effectivity of
our method through experiments on about one thousand sentences. Compared to
experiments with random example selection, our method reduced the overhead
without the degeneration of the performance of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702011</id><created>1997-02-17</created><authors><author><keyname>Jones</keyname><forenames>Karen Sparck</forenames><affiliation>Computer Laboratory, University of Cambridge</affiliation></author></authors><title>How much has information technology contributed to linguistics?</title><categories>cmp-lg cs.CL</categories><comments>Prepared for a British Academy Symposium on Information Technology
  and Scholarly Disciplines</comments><abstract>  Information technology should have much to offer linguistics, not only
through the opportunities offered by large-scale data analysis and the stimulus
to develop formal computational models, but through the chance to use language
in systems for automatic natural language processing. The paper discusses these
possibilities in detail, and then examines the actual work that has been done.
It is evident that this has so far been primarily research within a new field,
computational linguistics, which is largely motivated by the demands, and
interest, of practical processing systems, and that information technology has
had rather little influence on linguistics at large. There are different
reasons for this, and not all good ones: information technology deserves more
attention from linguists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702012</id><created>1997-02-19</created><authors><author><keyname>Yorulmaz</keyname><forenames>Abdullah Kurtulus</forenames></author></authors><title>Design and Implementation of a Computational Lexicon for Turkish</title><categories>cmp-lg cs.CL</categories><comments>M.Sc. Thesis submitted to the Department of Computer Engineering and
  Information Science, Bilkent University, Ankara, Turkey. 138 pages (including
  title pages). LaTeX. 8 figures. Uses avm.sty, lingmacros.sty, buthesis.sty,
  QobiTree.tex, psfig.tex. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1997/BU-CEIS-9701.ps.z</comments><report-no>BU-CEIS-9701</report-no><abstract>  All natural language processing systems (such as parsers, generators,
taggers) need to have access to a lexicon about the words in the language. This
thesis presents a lexicon architecture for natural language processing in
Turkish. Given a query form consisting of a surface form and other features
acting as restrictions, the lexicon produces feature structures containing
morphosyntactic, syntactic, and semantic information for all possible
interpretations of the surface form satisfying those restrictions. The lexicon
is based on contemporary approaches like feature-based representation,
inheritance, and unification. It makes use of two information sources: a
morphological processor and a lexical database containing all the open and
closed-class words of Turkish. The system has been implemented in SICStus
Prolog as a standalone module for use in natural language processing
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702013</id><created>1997-02-24</created><authors><author><keyname>Reiter</keyname><forenames>Ehud</forenames><affiliation>Aberdeen</affiliation></author><author><keyname>Cawsey</keyname><forenames>Alison</forenames><affiliation>Heriot-Watt</affiliation></author><author><keyname>Osman</keyname><forenames>Liesl</forenames><affiliation>Aberdeen</affiliation></author><author><keyname>Roff</keyname><forenames>Yvonne</forenames><affiliation>Valstar Systems</affiliation></author></authors><title>Knowledge Acquisition for Content Selection</title><categories>cmp-lg cs.CL</categories><comments>To appear in the 1997 European NLG workshop. 10 pages, postscript</comments><abstract>  An important part of building a natural-language generation (NLG) system is
knowledge acquisition, that is deciding on the specific schemas, plans, grammar
rules, and so forth that should be used in the NLG system. We discuss some
experiments we have performed with KA for content-selection rules, in the
context of building an NLG system which generates health-related material.
These experiments suggest that it is useful to supplement corpus analysis with
KA techniques developed for building expert systems, such as structured group
discussions and think-aloud protocols. They also raise the point that KA issues
may influence architectural design issues, in particular the decision on
whether a planning approach is used for content selection. We suspect that in
some cases, KA may be easier if other constructive expert-system techniques
(such as production rules, or case-based reasoning) are used to determine the
content of a generated text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702014</id><created>1997-02-25</created><authors><author><keyname>Radev</keyname><forenames>Dragomir R.</forenames><affiliation>Columbia University</affiliation></author><author><keyname>McKeown</keyname><forenames>Kathleen R.</forenames><affiliation>Columbia University</affiliation></author></authors><title>Building a Generation Knowledge Source using Internet-Accessible
  Newswire</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses epsf</comments><journal-ref>To appear in Proceedings of the 5th Conference on Applied Natural
  Processing, Washington DC, 31 March - 3 April, 1997.</journal-ref><abstract>  In this paper, we describe a method for automatic creation of a knowledge
source for text generation using information extraction over the Internet. We
present a prototype system called PROFILE which uses a client-server
architecture to extract noun-phrase descriptions of entities such as people,
places, and organizations. The system serves two purposes: as an information
extraction tool, it allows users to search for textual descriptions of
entities; as a utility to generate functional descriptions (FD), it is used in
a functional-unification based generation system. We present an evaluation of
the approach and its applications to natural language generation and
summarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702015</id><created>1997-02-26</created><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames></author><author><keyname>Cahn</keyname><forenames>Janet E.</forenames></author><author><keyname>Whittaker</keyname><forenames>Stephen J.</forenames></author></authors><title>Improvising Linguistic Style: Social and Affective Bases for Agent
  Personality</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uses aaai.sty, lingmacros.sty, psfig.sty</comments><journal-ref>Proceedings of the First International Conference on Autonomous
  Agents, Marina del Rey, California, USA. 1997. pp 96-105</journal-ref><abstract>  This paper introduces Linguistic Style Improvisation, a theory and set of
algorithms for improvisation of spoken utterances by artificial agents, with
applications to interactive story and dialogue systems. We argue that
linguistic style is a key aspect of character, and show how speech act
representations common in AI can provide abstract representations from which
computer characters can improvise. We show that the mechanisms proposed
introduce the possibility of socially oriented agents, meet the requirements
that lifelike characters be believable, and satisfy particular criteria for
improvisation proposed by Hayes-Roth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9702016</id><created>1997-02-27</created><authors><author><keyname>O'Hara</keyname><forenames>Tom</forenames><affiliation>New Mexico State University</affiliation></author><author><keyname>Wiebe</keyname><forenames>Janyce</forenames><affiliation>New Mexico State University</affiliation></author><author><keyname>Payne</keyname><forenames>Karen</forenames><affiliation>New Mexico State University</affiliation></author></authors><title>Instructions for Temporal Annotation of Scheduling Dialogs</title><categories>cmp-lg cs.CL</categories><comments>14 pages</comments><report-no>MCCS-97-308</report-no><abstract>  Human annotation of natural language facilitates standardized evaluation of
natural language processing systems and supports automated feature extraction.
This document consists of instructions for annotating the temporal information
in scheduling dialogs, dialogs in which the participants schedule a meeting
with one another. Task-oriented dialogs, such as these are, would arise in many
useful applications, for instance, automated information providers and
automated phone operators. Explicit instructions support good inter-rater
reliability and serve as documentation for the classes being annotated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9703001</id><created>1997-03-04</created><authors><author><keyname>Ueberla</keyname><forenames>Joerg P.</forenames><affiliation>Forum Technology - DRA Malvern</affiliation></author></authors><title>Domain Adaptation with Clustered Language Models</title><categories>cmp-lg cs.CL</categories><comments>preprint - to appear in ICASSP 97</comments><abstract>  In this paper, a method of domain adaptation for clustered language models is
developed. It is based on a previously developed clustering algorithm, but with
a modified optimisation criterion. The results are shown to be slightly
superior to the previously published 'Fillup' method, which can be used to
adapt standard n-gram models. However, the improvement both methods give
compared to models built from scratch on the adaptation data is quite small
(less than 11% relative improvement in word error rate). This suggests that
both methods are still unsatisfactory from a practical point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9703002</id><created>1997-03-05</created><authors><author><keyname>Barriere</keyname><forenames>Caroline</forenames></author><author><keyname>Popowich</keyname><forenames>Fred</forenames></author></authors><title>Concept Clustering and Knowledge Integration from a Children's Dictionary</title><categories>cmp-lg cs.CL</categories><comments>uses colap.sty</comments><journal-ref>COLING'96</journal-ref><abstract>  Knowledge structures called Concept Clustering Knowledge Graphs (CCKGs) are
introduced along with a process for their construction from a machine readable
dictionary. CCKGs contain multiple concepts interrelated through multiple
semantic relations together forming a semantic cluster represented by a
conceptual graph. The knowledge acquisition is performed on a children's first
dictionary. A collection of conceptual clusters together can form the basis of
a lexical knowledge base, where each CCKG contains a limited number of highly
connected words giving useful information about a particular domain or
situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9703003</id><created>1997-03-12</created><authors><author><keyname>Vaillant</keyname><forenames>Pascal</forenames><affiliation>Thomson-CSF LCR</affiliation></author></authors><title>A Semantics-based Communication System for Dysphasic Subjects</title><categories>cmp-lg cs.CL</categories><comments>LaTeX 2.09, 12 pages, 2 Encapsulated Postscript figures, uses
  llncs.sty and epsf.sty. To appear in Proceedings of the 6th conference on
  Artificial Intelligence in Medicine Europe (AIME'97), march 1997, Grenoble
  (France)</comments><abstract>  Dysphasic subjects do not have complete linguistic abilities and only produce
a weakly structured, topicalized language. They are offered artificial symbolic
languages to help them communicate in a way more adapted to their linguistic
abilities. After a structural analysis of a corpus of utterances from children
with cerebral palsy, we define a semantic lexicon for such a symbolic language.
We use it as the basis of a semantic analysis process able to retrieve an
interpretation of the utterances. This semantic analyser is currently used in
an application designed to convert iconic languages into natural language; it
might find other uses in the field of language rehabilitation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9703004</id><created>1997-03-18</created><authors><author><keyname>Alexandersson</keyname><forenames>Jan</forenames><affiliation>DFKI GmbH, Saarbruecken, Germany</affiliation></author><author><keyname>Reithinger</keyname><forenames>Norbert</forenames><affiliation>DFKI GmbH, Saarbruecken, Germany</affiliation></author><author><keyname>Maier</keyname><forenames>Elisabeth</forenames><affiliation>DFKI GmbH, Saarbruecken, Germany</affiliation></author></authors><title>Insights into the Dialogue Processing of VERBMOBIL</title><categories>cmp-lg cs.CL</categories><comments>Latex, 8 pages, includes 4 postscript figures; appears in the
  proceedings of ANLP-97 Washington D.C</comments><abstract>  We present the dialogue module of the speech-to-speech translation system
VERBMOBIL. We follow the approach that the solution to dialogue processing in a
mediating scenario can not depend on a single constrained processing tool, but
on a combination of several simple, efficient, and robust components. We show
how our solution to dialogue processing works when applied to real data, and
give some examples where our module contributes to the correct translation from
German to English.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9703005</id><created>1997-03-27</created><authors><author><keyname>Resnik</keyname><forenames>Philip</forenames><affiliation>University of Maryland</affiliation></author><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Semi-Automatic Acquisition of Domain-Specific Translation Lexicons</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><journal-ref>Proceedings of the 5th ANLP Conference, 1997.</journal-ref><abstract>  We investigate the utility of an algorithm for translation lexicon
acquisition (SABLE), used previously on a very large corpus to acquire general
translation lexicons, when that algorithm is applied to a much smaller corpus
to produce candidates for domain-specific translation lexicons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704001</id><created>1997-04-07</created><authors><author><keyname>Resnik</keyname><forenames>Philip</forenames><affiliation>University of Maryland</affiliation></author></authors><title>Evaluating Multilingual Gisting of Web Pages</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses psfig and aaai styles</comments><report-no>CS-TR-3783/LAMP-TR-009/UMIACS-TR-97-39</report-no><abstract>  We describe a prototype system for multilingual gisting of Web pages, and
present an evaluation methodology based on the notion of gisting as decision
support. This evaluation paradigm is straightforward, rigorous, permits fair
comparison of alternative approaches, and should easily generalize to
evaluation in other situations where the user is faced with decision-making on
the basis of information in restricted or alternative form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704002</id><created>1997-04-09</created><authors><author><keyname>Reynar</keyname><forenames>Jeffrey C.</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Ratnaparkhi</keyname><forenames>Adwait</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Maximum Entropy Approach to Identifying Sentence Boundaries</title><categories>cmp-lg cs.CL</categories><comments>4 pages, uses aclap.sty and covingtn.sty</comments><journal-ref>Proceedings of the 5th ANLP Conference, 1997</journal-ref><abstract>  We present a trainable model for identifying sentence boundaries in raw text.
Given a corpus annotated with sentence boundaries, our model learns to classify
each occurrence of ., ?, and ! as either a valid or invalid sentence boundary.
The training procedure requires no hand-crafted rules, lexica, part-of-speech
tags, or domain-specific information. The model can therefore be trained easily
on any genre of English, and should be trainable on any other Roman-alphabet
language. Performance is comparable to or better than the performance of
similar systems, but we emphasize the simplicity of retraining for new domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704003</identifier>
 <datestamp>2009-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704003</id><created>1997-04-14</created><authors><author><keyname>Knight</keyname><forenames>Kevin</forenames><affiliation>USC/ISI</affiliation></author><author><keyname>Graehl</keyname><forenames>Jonathan</forenames><affiliation>USC</affiliation></author></authors><title>Machine Transliteration</title><categories>cmp-lg cs.CL</categories><comments>8 pages, postscript, to appear, ACL-97/EACL-97</comments><abstract>  It is challenging to translate names and technical terms across languages
with different alphabets and sound inventories. These items are commonly
transliterated, i.e., replaced with approximate phonetic equivalents. For
example, &quot;computer&quot; in English comes out as &quot;konpyuutaa&quot; in Japanese.
Translating such items from Japanese back to English is even more challenging,
and of practical interest, as transliterated items make up the bulk of text
phrases not found in bilingual dictionaries. We describe and evaluate a method
for performing backwards transliterations by machine. This method uses a
generative model, incorporating several distinct stages in the transliteration
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704004</id><created>1997-04-15</created><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames><affiliation>ATT Labs - Research</affiliation></author><author><keyname>Litman</keyname><forenames>Diane J.</forenames><affiliation>ATT Labs - Research</affiliation></author><author><keyname>Kamm</keyname><forenames>Candace A.</forenames><affiliation>ATT Labs - Research</affiliation></author><author><keyname>Abella</keyname><forenames>Alicia</forenames><affiliation>ATT Labs - Research</affiliation></author></authors><title>PARADISE: A Framework for Evaluating Spoken Dialogue Agents</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uses aclap, psfig, lingmacros, times</comments><journal-ref>Proceedings of the 35th Annual Meeting of the Association for
  Computational Linguistics</journal-ref><abstract>  This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a
general framework for evaluating spoken dialogue agents. The framework
decouples task requirements from an agent's dialogue behaviors, supports
comparisons among dialogue strategies, enables the calculation of performance
over subdialogues and whole dialogues, specifies the relative contribution of
various factors to performance, and makes it possible to compare agents
performing different tasks by normalizing for task complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704005</id><created>1997-04-17</created><authors><author><keyname>Chu-Carroll</keyname><forenames>Jennifer</forenames><affiliation>Bell Laboratories</affiliation></author><author><keyname>Brown</keyname><forenames>Michael K.</forenames><affiliation>Bell Laboratories</affiliation></author></authors><title>Tracking Initiative in Collaborative Dialogue Interactions</title><categories>cmp-lg cs.CL</categories><comments>9 pages, uses psfig, and times</comments><journal-ref>ACL-97</journal-ref><abstract>  In this paper, we argue for the need to distinguish between task and dialogue
initiatives, and present a model for tracking shifts in both types of
initiatives in dialogue interactions. Our model predicts the initiative holders
in the next dialogue turn based on the current initiative holders and the
effect that observed cues have on changing them. Our evaluation across various
corpora shows that the use of cues consistently improves the accuracy in the
system's prediction of task and dialogue initiative holders by 2-4 and 8-13
percentage points, respectively, thus illustrating the generality of our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704006</id><created>1997-04-21</created><updated>1997-06-24</updated><authors><author><keyname>Morawietz</keyname><forenames>Frank</forenames><affiliation>University of Tuebingen</affiliation></author><author><keyname>Cornell</keyname><forenames>Tom</forenames><affiliation>University of Tuebingen</affiliation></author></authors><title>Representing Constraints with Automata</title><categories>cmp-lg cs.CL</categories><comments>8 pages; uses aclap.sty, harvard.sty and dcu.bst. Corrections to
  three examples</comments><abstract>  In this paper we describe an approach to constraint-based syntactic theories
in terms of finite tree automata. The solutions to constraints expressed in
weak monadic second order (MSO) logic are represented by tree automata
recognizing the assignments which make the formulas true. We show that this
allows an efficient representation of knowledge about the content of
constraints which can be used as a practical tool for grammatical theory
verification. We achieve this by using the intertranslatability of formulas of
MSO logic and tree automata and the embedding of MSO logic into a constraint
logic programming scheme. The usefulness of the approach is discussed with
examples from the realm of Principles-and-Parameters based parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704007</id><created>1997-04-21</created><authors><author><keyname>Rigau</keyname><forenames>German</forenames></author><author><keyname>Atserias</keyname><forenames>Jordi</forenames></author><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author></authors><title>Combining Unsupervised Lexical Knowledge Methods for Word Sense
  Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses aclap.sty</comments><journal-ref>Proceedings of ACL'97</journal-ref><abstract>  This paper presents a method to combine a set of unsupervised algorithms that
can accurately disambiguate word senses in a large, completely untagged corpus.
Although most of the techniques for word sense resolution have been presented
as stand-alone, it is our belief that full-fledged lexical ambiguity resolution
should combine several information sources and techniques. The set of
techniques have been applied in a combined way to disambiguate the genus terms
of two machine-readable dictionaries (MRD), enabling us to construct complete
taxonomies for Spanish and French. Tested accuracy is above 80% overall and 95%
for two-way ambiguous genus terms, showing that taxonomy building is not
limited to structured dictionaries such as LDOCE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704008</id><created>1997-04-23</created><authors><author><keyname>Heeman</keyname><forenames>Peter A.</forenames><affiliation>University of Rochester</affiliation></author><author><keyname>Allen</keyname><forenames>James F.</forenames><affiliation>University of Rochester</affiliation></author></authors><title>Intonational Boundaries, Speech Repairs and Discourse Markers: Modeling
  Spoken Dialog</title><categories>cmp-lg cs.CL</categories><comments>8 pages, 3 postscript figures</comments><journal-ref>In proceedings of ACL/EACL'97</journal-ref><abstract>  To understand a speaker's turn of a conversation, one needs to segment it
into intonational phrases, clean up any speech repairs that might have
occurred, and identify discourse markers. In this paper, we argue that these
problems must be resolved together, and that they must be resolved early in the
processing stream. We put forward a statistical language model that resolves
these problems, does POS tagging, and can be used as the language model of a
speech recognizer. We find that by accounting for the interactions between
these tasks that the performance on each task improves, as does POS tagging and
perplexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704009</id><created>1997-04-23</created><updated>1997-04-24</updated><authors><author><keyname>Voutilainen</keyname><forenames>Atro</forenames></author><author><keyname>Padro</keyname><forenames>Lluis</forenames></author></authors><title>Developing a hybrid NP parser</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses aclap.sty, epsf.sty</comments><journal-ref>Proceedings of 5th ANLP, 1997</journal-ref><abstract>  We describe the use of energy function optimization in very shallow syntactic
parsing. The approach can use linguistic rules and corpus-based statistics, so
the strengths of both linguistic and statistical approaches to NLP can be
combined in a single framework. The rules are contextual constraints for
resolving syntactic ambiguities expressed as alternative tags, and the
statistical language model consists of corpus-based n-grams of syntactic tags.
The success of the hybrid syntactic disambiguator is evaluated against a
held-out benchmark corpus. Also the contributions of the linguistic and
statistical language models to the hybrid model are estimated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704010</id><created>1997-04-25</created><authors><author><keyname>Bateman</keyname><forenames>John A.</forenames><affiliation>Dept. of English, University of Stirling</affiliation></author></authors><title>The Theoretical Status of Ontologies in Natural Language Processing</title><categories>cmp-lg cs.CL</categories><comments>43pp, uses: twocolumn,named,a4wide,psfig, 7eps figs</comments><abstract>  This paper discusses the use of `ontologies' in Natural Language Processing.
It classifies various kinds of ontologies that have been employed in NLP and
discusses various benefits and problems with those designs. Particular focus is
then placed on experiences gained in the use of the Upper Model, a
linguistically-motivated `ontology' originally designed for use with the Penman
text generation system. Some proposals for further NLP ontology design criteria
are then made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704011</id><created>1997-04-25</created><authors><author><keyname>Oflazer</keyname><forenames>Kemal</forenames><affiliation>Bilkent University, Ankara, Turkey</affiliation></author><author><keyname>Tur</keyname><forenames>Gokhan</forenames><affiliation>Bilkent University, Ankara, Turkey</affiliation></author></authors><title>Morphological Disambiguation by Voting Constraints</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Latex source. To appear in Proceedings of ACL/EACL'97
  Compressed postscript also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/acl97.ps.z</comments><abstract>  We present a constraint-based morphological disambiguation system in which
individual constraints vote on matching morphological parses, and
disambiguation of all the tokens in a sentence is performed at the end by
selecting parses that receive the highest votes. This constraint application
paradigm makes the outcome of the disambiguation independent of the rule
sequence, and hence relieves the rule developer from worrying about potentially
conflicting rule sequencing. Our results for disambiguating Turkish indicate
that using about 500 constraint rules and some additional simple statistics, we
can attain a recall of 95-96% and a precision of 94-95% with about
1.01 parses per token. Our system is implemented in Prolog and we are
currently investigating an efficient implementation based on finite state
transducers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704012</id><created>1997-04-25</created><authors><author><keyname>Teich</keyname><forenames>Elke</forenames><affiliation>GMD, Darmstadt</affiliation></author><author><keyname>Firzlaff</keyname><forenames>Beate</forenames><affiliation>GMD, Darmstadt</affiliation></author><author><keyname>Bateman</keyname><forenames>John A.</forenames><affiliation>GMD, Darmstadt</affiliation></author></authors><title>Emphatic generation: employing the theory of semantic emphasis for text
  generation</title><categories>cmp-lg cs.CL</categories><comments>11pp; uses: 11pt,twocolumn,named,a4wide,program,psfig; 1psfig</comments><abstract>  The paper deals with the problem of text generation and planning approaches
making only limited formally specifiable contact with accounts of grammar. We
propose an enhancement of a systemically-based generation architecture for
German (the KOMET system) by aspects of Kunze's theory of semantic emphasis.
Doing this, we gain more control over both concept selection in generation and
choice of fine-grained grammatical variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704013</id><created>1997-04-29</created><authors><author><keyname>Hobbs</keyname><forenames>Jerry R.</forenames><affiliation>SRI International</affiliation></author><author><keyname>Kehler</keyname><forenames>Andrew</forenames><affiliation>SRI International</affiliation></author></authors><title>A Theory of Parallelism and the Case of VP Ellipsis</title><categories>cmp-lg cs.CL</categories><comments>LaTeX, 8 pages, requires aclap.sty</comments><journal-ref>Proceedings of ACL-97</journal-ref><abstract>  We provide a general account of parallelism in discourse, and apply it to the
special case of resolving possible readings for instances of VP ellipsis. We
show how several problematic examples are accounted for in a natural and
straightforward fashion. The generality of the approach makes it directly
applicable to a variety of other types of ellipsis and reference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9704014</id><created>1997-04-30</created><authors><author><keyname>Hahn</keyname><forenames>Udo</forenames><affiliation>Computational Linguistics Research Group, Freiburg University, Freiburg, Germany</affiliation></author><author><keyname>Strube</keyname><forenames>Michael</forenames><affiliation>Computational Linguistics Research Group, Freiburg University, Freiburg, Germany</affiliation></author></authors><title>Centering in-the-large: Computing referential discourse segments</title><categories>cmp-lg cs.CL</categories><comments>LaTeX, 8 pages</comments><journal-ref>Proceedings of ACL 97 / EACL 97</journal-ref><abstract>  We specify an algorithm that builds up a hierarchy of referential discourse
segments from local centering data. The spatial extension and nesting of these
discourse segments constrain the reachability of potential antecedents of an
anaphoric expression beyond the local level of adjacent center pairs. Thus, the
centering model is scaled up to the level of the global referential structure
of discourse. An empirical evaluation of the algorithm is supplied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705001</id><created>1997-05-01</created><authors><author><keyname>Briscoe</keyname><forenames>Ted</forenames><affiliation>Computer Laboratory, University of Cambridge</affiliation></author></authors><title>Co-evolution of Language and of the Language Acquisition Device</title><categories>cmp-lg cs.CL</categories><comments>10 pages, latex, 2 postscript figures, uses aclap.sty and
  graphics.sty, to appear ACL-EACL97</comments><abstract>  A new account of parameter setting during grammatical acquisition is
presented in terms of Generalized Categorial Grammar embedded in a default
inheritance hierarchy, providing a natural partial ordering on the setting of
parameters. Experiments show that several experimentally effective learners can
be defined in this framework. Evolutionary simulations suggest that a learner
with default initial settings for parameters will emerge, provided that
learning is memory limited and the environment of linguistic adaptation
contains an appropriate language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705002</id><created>1997-05-01</created><authors><author><keyname>Gardent</keyname><forenames>Claire</forenames><affiliation>University of the Saarland, Saarbruecken, Germany</affiliation></author></authors><title>Sloppy Identity</title><categories>cmp-lg cs.CL</categories><comments>20 pages</comments><report-no>CLAUS Nr.88, University of Saarbruecken</report-no><journal-ref>Logical Aspects of Computational Linguistics, Springer-Verlag.</journal-ref><abstract>  Although sloppy interpretation is usually accounted for by theories of
ellipsis, it often arises in non-elliptical contexts. In this paper, a theory
of sloppy interpretation is provided which captures this fact. The underlying
idea is that sloppy interpretation results from a semantic constraint on
parallel structures and the theory is shown to predict sloppy readings for
deaccented and paycheck sentences as well as relational-, event-, and
one-anaphora. It is further shown to capture the interaction of sloppy/strict
ambiguity with quantification and binding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705003</id><created>1997-05-01</created><authors><author><keyname>Nederhof</keyname><forenames>Mark-Jan</forenames><affiliation>University of Groningen, Humanities Computing</affiliation></author><author><keyname>Bouma</keyname><forenames>Gosse</forenames><affiliation>University of Groningen, Humanities Computing</affiliation></author><author><keyname>Koeling</keyname><forenames>Rob</forenames><affiliation>University of Groningen, Humanities Computing</affiliation></author><author><keyname>van Noord</keyname><forenames>Gertjan</forenames><affiliation>University of Groningen, Humanities Computing</affiliation></author></authors><title>Grammatical analysis in the OVIS spoken-dialogue system</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses aclap.sty</comments><journal-ref>ACL/EACL 1997 Workshop on Spoken Dialog Systems</journal-ref><abstract>  We argue that grammatical processing is a viable alternative to concept
spotting for processing spoken input in a practical dialogue system. We discuss
the structure of the grammar, the properties of the parser, and a method for
achieving robustness. We discuss test results suggesting that grammatical
processing allows fast and accurate processing of spoken input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705004</id><created>1997-05-01</created><authors><author><keyname>Gardent</keyname><forenames>Claire</forenames><affiliation>Universitaet des Saarlandes, Saarbruecken, Germany</affiliation></author><author><keyname>Kohlhase</keyname><forenames>Michael</forenames><affiliation>Universitaet des Saarlandes, Saarbruecken, Germany</affiliation></author></authors><title>Computing Parallelism in Discourse</title><categories>cmp-lg cs.CL</categories><comments>6 pages</comments><report-no>CLAUS Nr. 90</report-no><journal-ref>Proceedings of IJCAI'97</journal-ref><abstract>  Although much has been said about parallelism in discourse, a formal,
computational theory of parallelism structure is still outstanding. In this
paper, we present a theory which given two parallel utterances predicts which
are the parallel elements. The theory consists of a sorted, higher-order
abductive calculus and we show that it reconciles the insights of discourse
theories of parallelism with those of Higher-Order Unification approaches to
discourse semantics, thereby providing a natural framework in which to capture
the effect of parallelism on discourse semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705005</id><created>1997-05-06</created><authors><author><keyname>Li</keyname><forenames>Hang</forenames><affiliation>C&amp;C Res. Labs., NEC</affiliation></author><author><keyname>Yamanishi</keyname><forenames>Kenji</forenames><affiliation>C&amp;C Res. Labs., NEC</affiliation></author></authors><title>Document Classification Using a Finite Mixture Model</title><categories>cmp-lg cs.CL</categories><comments>latex file, uses aclap.sty and epsf.sty, 9 pages, to appear
  ACL/EACL-97</comments><abstract>  We propose a new method of classifying documents into categories. The simple
method of conducting hypothesis testing over word-based distributions in
categories suffers from the data sparseness problem. In order to address this
difficulty, Guthrie et.al. have developed a method using distributions based on
hard clustering of words, i.e., in which a word is assigned to a single cluster
and words in the same cluster are treated uniformly. This method might,
however, degrade classification results, since the distributions it employs are
not always precise enough for representing the differences between categories.
We propose here the use of soft clustering of words, i.e., in which a word can
be assigned to several different clusters and each cluster is characterized by
a specific word probability distribution. We define for each document category
a finite mixture model, which is a linear combination of the probability
distributions of the clusters. We thereby treat the problem of classifying
documents as that of conducting statistical hypothesis testing over finite
mixture models. In order to accomplish this testing, we employ the EM algorithm
which helps efficiently estimate parameters in a finite mixture model.
Experimental results indicate that our method outperforms not only the method
using distributions based on hard clustering, but also the method using
word-based distributions and the method based on cosine-similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705006</id><created>1997-05-06</created><authors><author><keyname>Riezler</keyname><forenames>Stefan</forenames><affiliation>University of Tuebingen</affiliation></author></authors><title>Quantitative Constraint Logic Programming for Weighted Grammar
  Applications</title><categories>cmp-lg cs.CL</categories><comments>20 pages, uses llncs.sty</comments><journal-ref>Logical Aspects of Computational Linguistics (LACL'96), LNCS,
  Springer.</journal-ref><abstract>  Constraint logic grammars provide a powerful formalism for expressing complex
logical descriptions of natural language phenomena in exact terms. Describing
some of these phenomena may, however, require some form of graded distinctions
which are not provided by such grammars. Recent approaches to weighted
constraint logic grammars attempt to address this issue by adding numerical
calculation schemata to the deduction scheme of the underlying CLP framework.
Currently, these extralogical extensions are not related to the model-theoretic
counterpart of the operational semantics of CLP, i.e., they do not come with a
formal semantics at all. The aim of this paper is to present a clear formal
semantics for weighted constraint logic grammars, which abstracts away from
specific interpretations of weights, but nevertheless gives insights into the
parsing problem for such weighted grammars. Building on the formalization of
constraint logic grammars in the CLP scheme of Hoehfeld and Smolka 1988, this
formal semantics will be given by a quantitative version of CLP. Such a
quantitative CLP scheme can also be valuable for CLP tasks independent of
grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705007</id><created>1997-05-07</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Bretan</keyname><forenames>Ivan</forenames><affiliation>Telia Research</affiliation></author><author><keyname>Eklund</keyname><forenames>Robert</forenames><affiliation>Telia Research</affiliation></author><author><keyname>Wiren</keyname><forenames>Mats</forenames><affiliation>Telia Research</affiliation></author><author><keyname>Hansen</keyname><forenames>Steffen Leo</forenames><affiliation>Copenhagen Business School</affiliation></author><author><keyname>Kirchmeier-Andersen</keyname><forenames>Sabine</forenames><affiliation>Copenhagen Business School</affiliation></author><author><keyname>Philp</keyname><forenames>Christina</forenames><affiliation>Copenhagen Business School</affiliation></author><author><keyname>Sorensen</keyname><forenames>Finn</forenames><affiliation>Copenhagen Business School</affiliation></author><author><keyname>Thomsen</keyname><forenames>Hanne Erdman</forenames><affiliation>Copenhagen Business School</affiliation></author></authors><title>Recycling Lingware in a Multilingual MT System</title><categories>cmp-lg cs.CL</categories><comments>6 pages, needs aclap.sty. To appear in &quot;From Research to Commercial
  Applications&quot; workshop at ACL-97, see also http://www.cam.sri.com</comments><report-no>CRC-067</report-no><abstract>  We describe two methods relevant to multi-lingual machine translation
systems, which can be used to port linguistic data (grammars, lexicons and
transfer rules) between systems used for processing related languages. The
methods are fully implemented within the Spoken Language Translator system, and
were used to create versions of the system for two new language pairs using
only a month of expert effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705008</id><created>1997-05-07</created><updated>1997-07-02</updated><authors><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge</affiliation></author></authors><title>The TreeBanker: a Tool for Supervised Training of Parsed Corpora</title><categories>cmp-lg cs.CL</categories><comments>7 pages, needs aclap.sty. Replacement just corrects Figure 3</comments><report-no>CRC-068 at http://www.cam.sri.com</report-no><journal-ref>&quot;Computational Environments ...&quot; (ENVGRAM) workshop at ACL-97</journal-ref><abstract>  I describe the TreeBanker, a graphical tool for the supervised training
involved in domain customization of the disambiguation component of a speech-
or language-understanding system. The TreeBanker presents a user, who need not
be a system expert, with a range of properties that distinguish competing
analyses for an utterance and that are relatively easy to judge. This allows
training on a corpus to be completed in far less time, and with far less
expertise, than would be needed if analyses were inspected directly: it becomes
possible for a corpus of about 20,000 sentences of the complexity of those in
the ATIS corpus to be judged in around three weeks of work by a linguistically
aware non-expert.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705009</id><created>1997-05-12</created><authors><author><keyname>Dymetman</keyname><forenames>Marc</forenames><affiliation>Rank Xerox Research Centre, Grenoble</affiliation></author></authors><title>Charts, Interaction-Free Grammars, and the Compact Representation of
  Ambiguity</title><categories>cmp-lg cs.CL</categories><comments>15 pages (Latex, Postscript), to appear in Proceedings IJCAI-97</comments><report-no>MLTT-029</report-no><abstract>  Recently researchers working in the LFG framework have proposed algorithms
for taking advantage of the implicit context-free components of a unification
grammar [Maxwell 96]. This paper clarifies the mathematical foundations of
these techniques, provides a uniform framework in which they can be formally
studied and eliminates the need for special purpose runtime data-structures
recording ambiguity. The paper posits the identity: Ambiguous Feature
Structures = Grammars, which states that (finitely) ambiguous representations
are best seen as unification grammars of a certain type, here called
``interaction-free'' grammars, which generate in a backtrack-free way each of
the feature structures subsumed by the ambiguous representation. This work
extends a line of research [Billot and Lang 89, Lang 94] which stresses the
connection between charts and grammars: a chart can be seen as a specialization
of the reference grammar for a given input string. We show how this
specialization grammar can be transformed into an interaction-free form which
has the same practicality as a listing of the individual solutions, but is
produced in less time and space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705010</id><created>1997-05-12</created><authors><author><keyname>Zavrel</keyname><forenames>Jakub</forenames></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames></author></authors><title>Memory-Based Learning: Using Similarity for Smoothing</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses aclap.sty, To appear in Proc. ACL/EACL 97</comments><report-no>ILK-9702</report-no><abstract>  This paper analyses the relation between the use of similarity in
Memory-Based Learning and the notion of backed-off smoothing in statistical
language modeling. We show that the two approaches are closely related, and we
argue that feature weighting methods in the Memory-Based paradigm can offer the
advantage of automatically specifying a suitable domain-specific hierarchy
between most specific and most general conditioning information without the
need for a large number of parameters. We report two applications of this
approach: PP-attachment and POS-tagging. Our method achieves state-of-the-art
performance in both domains, and allows the easy integration of diverse
information sources, such as rich lexical representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705011</id><created>1997-05-14</created><authors><author><keyname>Buitelaar</keyname><forenames>Paul</forenames><affiliation>Dept. of Computer Science, Brandeis University</affiliation></author></authors><title>A Lexicon for Underspecified Semantic Tagging</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses aclap.sty</comments><abstract>  The paper defends the notion that semantic tagging should be viewed as more
than disambiguation between senses. Instead, semantic tagging should be a first
step in the interpretation process by assigning each lexical item a
representation of all of its systematically related senses, from which further
semantic processing steps can derive discourse dependent interpretations. This
leads to a new type of semantic lexicon (CoreLex) that supports underspecified
semantic tagging through a design based on systematic polysemous classes and a
class-based acquisition of lexical knowledge for specific domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705012</id><created>1997-05-15</created><authors><author><keyname>Winiwarter</keyname><forenames>Werner</forenames><affiliation>Dept. of Information Science, Kyoto University</affiliation></author><author><keyname>Kambayashi</keyname><forenames>Yahiko</forenames><affiliation>Dept. of Information Science, Kyoto University</affiliation></author></authors><title>A Comparative Study of the Application of Different Learning Techniques
  to Natural Language Interfaces</title><categories>cmp-lg cs.CL</categories><comments>10 pages, to appear CoNLL97</comments><abstract>  In this paper we present first results from a comparative study. Its aim is
to test the feasibility of different inductive learning techniques to perform
the automatic acquisition of linguistic knowledge within a natural language
database interface. In our interface architecture the machine learning module
replaces an elaborate semantic analysis component. The learning module learns
the correct mapping of a user's input to the corresponding database command
based on a collection of past input data. We use an existing interface to a
production planning and control system as evaluation and compare the results
achieved by different instance-based and model-based learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705013</id><created>1997-05-20</created><authors><author><keyname>Hobbs</keyname><forenames>Jerry R.</forenames><affiliation>Artificial Intelligence Center, SRI International, Menlo Park, California</affiliation></author><author><keyname>Appelt</keyname><forenames>Douglas</forenames><affiliation>Artificial Intelligence Center, SRI International, Menlo Park, California</affiliation></author><author><keyname>Bear</keyname><forenames>John</forenames><affiliation>Artificial Intelligence Center, SRI International, Menlo Park, California</affiliation></author><author><keyname>Israel</keyname><forenames>David</forenames><affiliation>Artificial Intelligence Center, SRI International, Menlo Park, California</affiliation></author><author><keyname>Kameyama</keyname><forenames>Megumi</forenames><affiliation>Artificial Intelligence Center, SRI International, Menlo Park, California</affiliation></author><author><keyname>Stickel</keyname><forenames>Mark</forenames><affiliation>Artificial Intelligence Center, SRI International, Menlo Park, California</affiliation></author><author><keyname>Tyson</keyname><forenames>Mabry</forenames><affiliation>Artificial Intelligence Center, SRI International, Menlo Park, California</affiliation></author></authors><title>FASTUS: A Cascaded Finite-State Transducer for Extracting Information
  from Natural-Language Text</title><categories>cmp-lg cs.CL</categories><comments>22 pages. In E. Roche and Y. Schabes, eds., Finite State Devices for
  Natural Language Processing, MIT Press, Cambridge, Massachusetts, in press</comments><journal-ref>In Roche E. and Y. Schabes, eds., Finite-State Language
  Processing, The MIT Press, Cambridge, MA, 1997, pages 383-406.</journal-ref><abstract>  FASTUS is a system for extracting information from natural language text for
entry into a database and for other applications. It works essentially as a
cascaded, nondeterministic finite-state automaton. There are five stages in the
operation of FASTUS. In Stage 1, names and other fixed form expressions are
recognized. In Stage 2, basic noun groups, verb groups, and prepositions and
some other particles are recognized. In Stage 3, certain complex noun groups
and verb groups are constructed. Patterns for events of interest are identified
in Stage 4 and corresponding ``event structures'' are built. In Stage 5,
distinct event structures that describe the same event are identified and
merged, and these are used in generating database entries. This decomposition
of language processing enables the system to do exactly the right amount of
domain-independent syntax, so that domain-dependent semantic and pragmatic
processing can be applied to the right larger-scale structures. FASTUS is very
efficient and effective, and has been used successfully in a number of
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705014</id><created>1997-05-22</created><authors><author><keyname>Heeman</keyname><forenames>Peter A.</forenames><affiliation>CNET, France Telecom</affiliation></author><author><keyname>Allen</keyname><forenames>James F.</forenames><affiliation>University of Rochester</affiliation></author></authors><title>Incorporating POS Tagging into Language Modeling</title><categories>cmp-lg cs.CL</categories><comments>5 pages, 2 postscript figures</comments><journal-ref>In proceedings of Eurospeech'97</journal-ref><abstract>  Language models for speech recognition tend to concentrate solely on
recognizing the words that were spoken. In this paper, we redefine the speech
recognition problem so that its goal is to find both the best sequence of words
and their syntactic role (part-of-speech) in the utterance. This is a necessary
first step towards tightening the interaction between speech recognition and
natural language understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705015</id><created>1997-05-27</created><authors><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Becket</keyname><forenames>Ralph</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Rayner</keyname><forenames>Manny</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Eklund</keyname><forenames>; Robert</forenames><affiliation>Telia Research</affiliation></author><author><keyname>MacDermid</keyname><forenames>Catriona</forenames><affiliation>Telia Research</affiliation></author><author><keyname>Wiren</keyname><forenames>Mats</forenames><affiliation>Telia Research</affiliation></author><author><keyname>Kirchmeier-Andersen</keyname><forenames>; Sabine</forenames><affiliation>Copenhagen Business School</affiliation></author><author><keyname>Philp</keyname><forenames>Christina</forenames><affiliation>Copenhagen Business School</affiliation></author></authors><title>Translation Methodology in the Spoken Language Translator: An Evaluation</title><categories>cmp-lg cs.CL</categories><comments>10 pages, needs aclap.sty. To appear in Spoken Language Translation
  workshop at (E)ACL-97</comments><report-no>CRC-070 at http://www.cam.sri.com</report-no><abstract>  In this paper we describe how the translation methodology adopted for the
Spoken Language Translator (SLT) addresses the characteristics of the speech
translation task in a context where it is essential to achieve easy
customization to new languages and new domains. We then discuss the issues that
arise in any attempt to evaluate a speech translator, and present the results
of such an evaluation carried out on SLT for several language pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9705016</id><created>1997-05-29</created><authors><author><keyname>Wilks</keyname><forenames>Yorick</forenames></author><author><keyname>Stevenson</keyname><forenames>Mark</forenames></author></authors><title>Sense Tagging: Semantic Tagging with a Lexicon</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uses aclap LaTeX style file. Also in Proceedings of the
  SIGLEX Workshop &quot;Tagging Text with Lexical Semantics&quot;</comments><abstract>  Sense tagging, the automatic assignment of the appropriate sense from some
lexicon to each of the words in a text, is a specialised instance of the
general problem of semantic tagging by category or type. We discuss which
recent word sense disambiguation algorithms are appropriate for sense tagging.
It is our belief that sense tagging can be carried out effectively by combining
several simple, independent, methods and we include the design of such a
tagger. A prototype of this system has been implemented, correctly tagging 86%
of polysemous word tokens in a small test set, providing evidence that our
hypothesis is correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706001</id><created>1997-06-04</created><authors><author><keyname>de Lima</keyname><forenames>Erika F.</forenames></author></authors><title>Assigning Grammatical Relations with a Back-off Model</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proceedings of the Second Conference on Empirical
  Methods in Natural Language Processing, 7 pages, LaTeX</comments><abstract>  This paper presents a corpus-based method to assign grammatical
subject/object relations to ambiguous German constructs. It makes use of an
unsupervised learning procedure to collect training and test data, and the
back-off model to make assignment decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706002</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706002</id><created>1997-06-05</created><authors><author><keyname>Hermjakob</keyname><forenames>Ulf</forenames><affiliation>Dept. of Computer Sciences, University of Texas at Austin</affiliation></author><author><keyname>Mooney</keyname><forenames>Raymond J.</forenames><affiliation>Dept. of Computer Sciences, University of Texas at Austin</affiliation></author></authors><title>Learning Parse and Translation Decisions From Examples With Rich Context</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX, 3 postscript figures, uses aclap.sty</comments><journal-ref>Proceedings of ACL/EACL'97</journal-ref><abstract>  We present a knowledge and context-based system for parsing and translating
natural language and evaluate it on sentences from the Wall Street Journal.
Applying machine learning techniques, the system uses parse action examples
acquired under supervision to generate a deterministic shift-reduce parser in
the form of a decision structure. It relies heavily on context, as encoded in
features which describe the morphological, syntactic, semantic and other
aspects of a given parse state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706003</identifier>
 <datestamp>2008-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706003</id><created>1997-06-05</created><updated>1997-06-07</updated><authors><author><keyname>Eisner</keyname><forenames>Jason</forenames><affiliation>Univ. of Pennsylvania</affiliation></author></authors><title>Three New Probabilistic Models for Dependency Parsing: An Exploration</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX 2.09 packaged with 4 .eps files, also uses colap.sty
  and acl.bst</comments><journal-ref>Proceedings of the 16th International Conference on Computational
  Linguistics (COLING-96), Copenhagen, August 1996, pp. 340-345</journal-ref><abstract>  After presenting a novel O(n^3) parsing algorithm for dependency grammar, we
develop three contrasting ways to stochasticize it. We propose (a) a lexical
affinity model where words struggle to modify each other, (b) a sense tagging
model where words fluctuate randomly in their selectional preferences, and (c)
a generative model where the speaker fleshes out each word's syntactic and
conceptual structure without regard to the implications for the hearer. We also
give preliminary empirical results from evaluating the three models' parsing
performance on annotated Wall Street Journal training text (derived from the
Penn Treebank). In these results, the generative (i.e., top-down) model
performs significantly better than the others, and does about equally well at
assigning part-of-speech tags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706004</id><created>1997-06-05</created><authors><author><keyname>Eisner</keyname><forenames>Jason</forenames><affiliation>Univ. of Pennsylvania</affiliation></author></authors><title>An Empirical Comparison of Probability Models for Dependency Grammar</title><categories>cmp-lg cs.CL</categories><comments>18 pages, LaTeX packaged with 1 .eps and 1 .sty file, also uses
  fullpage.sty, eepic.sty, psfig.tex</comments><report-no>Technical report IRCS-96-11, Institute for Research in Cognitive
  Science, U. of Pennsylvania</report-no><abstract>  This technical report is an appendix to Eisner (1996): it gives superior
experimental results that were reported only in the talk version of that paper.
Eisner (1996) trained three probability models on a small set of about 4,000
conjunction-free, dependency-grammar parses derived from the Wall Street
Journal section of the Penn Treebank, and then evaluated the models on a
held-out test set, using a novel O(n^3) parsing algorithm.
  The present paper describes some details of the experiments and repeats them
with a larger training set of 25,000 sentences. As reported at the talk, the
more extensive training yields greatly improved performance. Nearly half the
sentences are parsed with no misattachments; two-thirds are parsed with at most
one misattachment.
  Of the models described in the original written paper, the best score is
still obtained with the generative (top-down) &quot;model C.&quot; However, slightly
better models are also explored, in particular, two variants on the
comprehension (bottom-up) &quot;model B.&quot; The better of these has an attachment
accuracy of 90%, and (unlike model C) tags words more accurately than the
comparable trigram tagger. Differences are statistically significant.
  If tags are roughly known in advance, search error is all but eliminated and
the new model attains an attachment accuracy of 93%. We find that the parser of
Collins (1996), when combined with a highly-trained tagger, also achieves 93%
when trained and tested on the same sentences. Similarities and differences are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706005</id><created>1997-06-07</created><authors><author><keyname>Samuelsson</keyname><forenames>Christer</forenames><affiliation>Lucent Technologies</affiliation></author><author><keyname>Voutilainen</keyname><forenames>Atro</forenames><affiliation>University of Helsinki</affiliation></author></authors><title>Comparing a Linguistic and a Stochastic Tagger</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX, 2 postscript figures. E-ACL'97</comments><abstract>  Concerning different approaches to automatic PoS tagging: EngCG-2, a
constraint-based morphological tagger, is compared in a double-blind test with
a state-of-the-art statistical tagger on a common disambiguation task using a
common tag set. The experiments show that for the same amount of remaining
ambiguity, the error rate of the statistical tagger is one order of magnitude
greater than that of the rule-based one. The two related issues of priming
effects compromising the results and disagreement between human annotators are
also addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706006</id><created>1997-06-09</created><authors><author><keyname>Dagan</keyname><forenames>Ido</forenames><affiliation>Bar Ilan University, Israel</affiliation></author><author><keyname>Karov</keyname><forenames>Yael</forenames><affiliation>Weizmann Institute, Israel</affiliation></author><author><keyname>Roth</keyname><forenames>Dan</forenames><affiliation>Weizmann Institute, Israel</affiliation></author></authors><title>Mistake-Driven Learning in Text Categorization</title><categories>cmp-lg cs.CL</categories><comments>9 pages, uses aclap.sty</comments><abstract>  Learning problems in the text processing domain often map the text to a space
whose dimensions are the measured features of the text, e.g., its words. Three
characteristic properties of this domain are (a) very high dimensionality, (b)
both the learned concepts and the instances reside very sparsely in the feature
space, and (c) a high variation in the number of active features in an
instance. In this work we study three mistake-driven learning algorithms for a
typical task of this nature -- text categorization. We argue that these
algorithms -- which categorize documents by learning a linear separator in the
feature space -- have a few properties that make them ideal for this domain. We
then show that a quantum leap in performance is achieved when we further modify
the algorithms to better address some of the specific characteristics of the
domain. In particular, we demonstrate (1) how variation in document length can
be tolerated by either normalizing feature weights or by using negative
weights, (2) the positive effect of applying a threshold range in training, (3)
alternatives in considering feature frequency, and (4) the benefits of
discarding features while training. Overall, we present an algorithm, a
variation of Littlestone's Winnow, which performs significantly better than any
other algorithm tested on this task using a similar feature set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706007</id><created>1997-06-09</created><authors><author><keyname>Saul</keyname><forenames>Lawrence</forenames><affiliation>AT&amp;T Labs -- Research</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames><affiliation>AT&amp;T Labs -- Research</affiliation></author></authors><title>Aggregate and mixed-order Markov models for statistical language
  processing</title><categories>cmp-lg cs.CL</categories><comments>9 pages, 4 PostScript figures, uses psfig.sty and aclap.sty; to
  appear in the proceedings of EMNLP-2</comments><abstract>  We consider the use of language models whose size and accuracy are
intermediate between different order n-gram models. Two types of models are
studied in particular. Aggregate Markov models are class-based bigram models in
which the mapping from words to classes is probabilistic. Mixed-order Markov
models combine bigram models whose predictions are conditioned on different
words. Both types of models are trained by Expectation-Maximization (EM)
algorithms for maximum likelihood estimation. We examine smoothing procedures
in which these models are interposed between different order n-grams. This is
found to significantly reduce the perplexity of unseen word combinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706008</id><created>1997-06-09</created><authors><author><keyname>Pedersen</keyname><forenames>Ted</forenames><affiliation>Southern Methodist University</affiliation></author><author><keyname>Bruce</keyname><forenames>Rebecca</forenames><affiliation>Southern Methodist University</affiliation></author></authors><title>Distinguishing Word Senses in Untagged Text</title><categories>cmp-lg cs.CL</categories><comments>11 pages, latex, uses aclap.sty</comments><journal-ref>Appears in the Proceedings of the Second Conference on Empirical
  Methods in NLP (EMNLP-2), August 1-2, 1997, Providence, RI</journal-ref><abstract>  This paper describes an experimental comparison of three unsupervised
learning algorithms that distinguish the sense of an ambiguous word in untagged
text. The methods described in this paper, McQuitty's similarity analysis,
Ward's minimum-variance method, and the EM algorithm, assign each instance of
an ambiguous word to a known sense definition based solely on the values of
automatically identifiable features in text. These methods and feature sets are
found to be more successful in disambiguating nouns rather than adjectives or
verbs. Overall, the most accurate of these procedures is McQuitty's similarity
analysis in combination with a high dimensional feature set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706009</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706009</id><created>1997-06-09</created><authors><author><keyname>Ristad</keyname><forenames>Eric Sven</forenames></author><author><keyname>Yianilos</keyname><forenames>Peter N.</forenames></author></authors><title>Library of Practical Abstractions, Release 1.2</title><categories>cmp-lg cs.CL</categories><comments>19 pages, texinfo format</comments><abstract>  The library of practical abstractions (LIBPA) provides efficient
implementations of conceptually simple abstractions, in the C programming
language. We believe that the best library code is conceptually simple so that
it will be easily understood by the application programmer; parameterized by
type so that it enjoys wide applicability; and at least as efficient as a
straightforward special-purpose implementation. You will find that our software
satisfies the highest standards of software design, implementation, testing,
and benchmarking.
  The current LIBPA release is a source code distribution only. It consists of
modules for portable memory management, one dimensional arrays of arbitrary
types, compact symbol tables, hash tables for arbitrary types, a trie module
for length-delimited strings over arbitrary alphabets, single precision
floating point numbers with extended exponents, and logarithmic representations
of probability values using either fixed or floating point numbers.
  We have used LIBPA to implement a wide range of statistical models for both
continuous and discrete domains. The time and space efficiency of LIBPA has
allowed us to build larger statistical models than previously reported, and to
investigate more computationally-intensive techniques than previously possible.
We have found LIBPA to be indispensible in our own research, and hope that you
will find it useful in yours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706010</id><created>1997-06-10</created><authors><author><keyname>Ng</keyname><forenames>Hwee Tou</forenames><affiliation>DSO National Laboratories</affiliation></author></authors><title>Exemplar-Based Word Sense Disambiguation: Some Recent Improvements</title><categories>cmp-lg cs.CL</categories><comments>6 pages</comments><journal-ref>In Proceedings of the Second Conference on Empirical Methods in
  Natural Language Processing (EMNLP-2), August 1997</journal-ref><abstract>  In this paper, we report recent improvements to the exemplar-based learning
approach for word sense disambiguation that have achieved higher disambiguation
accuracy. By using a larger value of $k$, the number of nearest neighbors to
use for determining the class of a test example, and through 10-fold cross
validation to automatically determine the best $k$, we have obtained improved
disambiguation accuracy on a large sense-tagged corpus first used in
\cite{ng96}. The accuracy achieved by our improved exemplar-based classifier is
comparable to the accuracy on the same data set obtained by the Naive-Bayes
algorithm, which was reported in \cite{mooney96} to have the highest
disambiguation accuracy among seven state-of-the-art machine learning
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706011</id><created>1997-06-10</created><authors><author><keyname>Passonneau</keyname><forenames>Rebecca J.</forenames></author></authors><title>Applying Reliability Metrics to Co-Reference Annotation</title><categories>cmp-lg cs.CL</categories><comments>10 pages, 2-column format; uuencoded, gzipped, tarfile</comments><report-no>CUCS-017-97</report-no><abstract>  Studies of the contextual and linguistic factors that constrain discourse
phenomena such as reference are coming to depend increasingly on annotated
language corpora. In preparing the corpora, it is important to evaluate the
reliability of the annotation, but methods for doing so have not been readily
available. In this report, I present a method for computing reliability of
coreference annotation. First I review a method for applying the information
retrieval metrics of recall and precision to coreference annotation proposed by
Marc Vilain and his collaborators. I show how this method makes it possible to
construct contingency tables for computing Cohen's Kappa, a familiar
reliability metric. By comparing recall and precision to reliability on the
same data sets, I also show that recall and precision can be misleadingly high.
Because Kappa factors out chance agreement among coders, it is a preferable
measure for developing annotated corpora where no pre-existing target
annotation exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706012</id><created>1997-06-10</created><authors><author><keyname>Kehler</keyname><forenames>Andrew</forenames><affiliation>SRI International</affiliation></author></authors><title>Probabilistic Coreference in Information Extraction</title><categories>cmp-lg cs.CL</categories><comments>LaTeX, 11 pages, requires aclap.sty</comments><journal-ref>Proceedings of the Second Conference on Empirical Methods in NLP
  (EMNLP-2), August 1-2, 1997, Providence, RI</journal-ref><abstract>  Certain applications require that the output of an information extraction
system be probabilistic, so that a downstream system can reliably fuse the
output with possibly contradictory information from other sources. In this
paper we consider the problem of assigning a probability distribution to
alternative sets of coreference relationships among entity descriptions. We
present the results of initial experiments with several approaches to
estimating such distributions in an application using SRI's FASTUS information
extraction system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706013</id><created>1997-06-10</created><authors><author><keyname>Riloff</keyname><forenames>Ellen</forenames><affiliation>University of Utah</affiliation></author><author><keyname>Shepherd</keyname><forenames>Jessica</forenames><affiliation>University of Utah</affiliation></author></authors><title>A Corpus-Based Approach for Building Semantic Lexicons</title><categories>cmp-lg cs.CL</categories><comments>8 pages - to appear in Proceedings of EMNLP-2</comments><abstract>  Semantic knowledge can be a great asset to natural language processing
systems, but it is usually hand-coded for each application. Although some
semantic information is available in general-purpose knowledge bases such as
WordNet and Cyc, many applications require domain-specific lexicons that
represent words and categories for a particular topic. In this paper, we
present a corpus-based method that can be used to build semantic lexicons for
specific categories. The input to the system is a small set of seed words for a
category and a representative text corpus. The output is a ranked list of words
that are associated with the category. A user then reviews the top-ranked words
and decides which ones should be entered in the semantic lexicon. In
experiments with five categories, users typically found about 60 words per
category in 10-15 minutes to build a core semantic lexicon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706014</id><created>1997-06-11</created><authors><author><keyname>Ratnaparkhi</keyname><forenames>Adwait</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Linear Observed Time Statistical Parser Based on Maximum Entropy Models</title><categories>cmp-lg cs.CL</categories><comments>10 pages, LaTeX, uses aclap.sty, qtree.sty, to appear in EMNLP-2</comments><abstract>  This paper presents a statistical parser for natural language that obtains a
parsing accuracy---roughly 87% precision and 86% recall---which surpasses the
best previously published results on the Wall St. Journal domain. The parser
itself requires very little human intervention, since the information it uses
to make parsing decisions is specified in a concise and simple manner, and is
combined in a fully automatic way under the maximum entropy framework. The
observed running time of the parser on a test sentence is linear with respect
to the sentence length. Furthermore, the parser returns several scored parses
for a sentence, and this paper shows that a scheme to pick the best parse from
the 20 highest scoring parses could yield a dramatically higher accuracy of 93%
precision and recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706015</id><created>1997-06-11</created><authors><author><keyname>Trujillo</keyname><forenames>Arturo</forenames><affiliation>CCL, UMIST</affiliation></author></authors><title>Determining Internal and External Indices for Chart Generation</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Latex; to appear in 7th International Conference on
  Theoretical and Methodological Issues in Machine Translation (TMI-97)</comments><abstract>  This paper presents a compilation procedure which determines internal and
external indices for signs in a unification based grammar to be used in
improving the computational efficiency of lexicalist chart generation. The
procedure takes as input a grammar and a set of feature paths indicating the
position of semantic indices in a sign, and calculates the fixed-point of a set
of equations derived from the grammar. The result is a set of independent
constraints stating which indices in a sign can be bound to other signs within
a complete sentence. Based on these constraints, two tests are formulated which
reduce the search space during generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706016</id><created>1997-06-11</created><updated>1997-06-12</updated><authors><author><keyname>Beeferman</keyname><forenames>Doug</forenames><affiliation>Carnegie Mellon</affiliation></author><author><keyname>Berger</keyname><forenames>Adam</forenames><affiliation>Carnegie Mellon</affiliation></author><author><keyname>Lafferty</keyname><forenames>John</forenames><affiliation>Carnegie Mellon</affiliation></author></authors><title>Text Segmentation Using Exponential Models</title><categories>cmp-lg cs.CL</categories><comments>12 pages, LaTeX source and postscript figures for EMNLP-2 paper</comments><abstract>  This paper introduces a new statistical approach to partitioning text
automatically into coherent segments. Our approach enlists both short-range and
long-range language models to help it sniff out likely sites of topic changes
in text. To aid its search, the system consults a set of simple lexical hints
it has learned to associate with the presence of boundaries through inspection
of a large corpus of annotated data. We also propose a new probabilistically
motivated error metric for use by the natural language processing and
information retrieval communities, intended to supersede precision and recall
for appraising segmentation algorithms. Qualitative assessment of our algorithm
as well as evaluation using this new metric demonstrate the effectiveness of
our approach in two very different domains, Wall Street Journal articles and
the TDT Corpus, a collection of newswire articles and broadcast news
transcripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706017</id><created>1997-06-12</created><updated>1997-06-19</updated><authors><author><keyname>Thompson</keyname><forenames>Paul</forenames><affiliation>West Group</affiliation></author><author><keyname>Dozier</keyname><forenames>Christopher C.</forenames><affiliation>West Group</affiliation></author></authors><title>Name Searching and Information Retrieval</title><categories>cmp-lg cs.CL</categories><abstract>  The main application of name searching has been name matching in a database
of names. This paper discusses a different application: improving information
retrieval through name recognition. It investigates name recognition accuracy,
and the effect on retrieval performance of indexing and searching personal
names differently from non-name terms in the context of ranked retrieval. The
main conclusions are: that name recognition in text can be effective; that
names occur frequently enough in a variety of domains, including those of legal
documents and news databases, to make recognition worthwhile; and that
retrieval performance can be improved using name searching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706018</id><created>1997-06-12</created><updated>1997-06-16</updated><authors><author><keyname>Beeferman</keyname><forenames>Doug</forenames><affiliation>Carnegie Mellon</affiliation></author><author><keyname>Berger</keyname><forenames>Adam</forenames><affiliation>Carnegie Mellon</affiliation></author><author><keyname>Lafferty</keyname><forenames>John</forenames><affiliation>Carnegie Mellon</affiliation></author></authors><title>A Model of Lexical Attraction and Repulsion</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX source and postscript figures for ACL/EACL'97 paper</comments><abstract>  This paper introduces new methods based on exponential families for modeling
the correlations between words in text and speech. While previous work assumed
the effects of word co-occurrence statistics to be constant over a window of
several hundred words, we show that their influence is nonstationary on a much
smaller time scale. Empirical data drawn from English and Japanese text, as
well as conversational speech, reveals that the ``attraction'' between words
decays exponentially, while stylistic and syntactic contraints create a
``repulsion'' between words that discourages close co-occurrence. We show that
these characteristics are well described by simple mixture models based on
two-stage exponential distributions which can be trained using the EM
algorithm. The resulting distance distributions can then be incorporated as
penalizing features in an exponential language model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706019</id><created>1997-06-13</created><authors><author><keyname>Walker</keyname><forenames>Marilyn</forenames><affiliation>ATT Labs Research</affiliation></author><author><keyname>Hindle</keyname><forenames>Donald</forenames><affiliation>ATT Labs Research</affiliation></author><author><keyname>Fromer</keyname><forenames>Jeanne</forenames><affiliation>ATT Labs Research</affiliation></author><author><keyname>Di Fabbrizio</keyname><forenames>Giuseppe</forenames><affiliation>ATT Labs Research</affiliation></author><author><keyname>Mestel</keyname><forenames>Craig</forenames><affiliation>ATT Labs Research</affiliation></author></authors><title>Evaluating Competing Agent Strategies for a Voice Email Agent</title><categories>cmp-lg cs.CL</categories><comments>6 pages latex, uses icassp91.sty, psfig</comments><journal-ref>Proceedings of the European Conference on Speech Communication and
  Technology, EUROSPEECH97</journal-ref><abstract>  This paper reports experimental results comparing a mixed-initiative to a
system-initiative dialog strategy in the context of a personal voice email
agent. To independently test the effects of dialog strategy and user expertise,
users interact with either the system-initiative or the mixed-initiative agent
to perform three successive tasks which are identical for both agents. We
report performance comparisons across agent strategies as well as over tasks.
This evaluation utilizes and tests the PARADISE evaluation framework, and
discusses the performance function derivable from the experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706020</id><created>1997-06-16</created><authors><author><keyname>Wiebe</keyname><forenames>Janyce</forenames><affiliation>New Mexico State University</affiliation></author><author><keyname>O'Hara</keyname><forenames>Tom</forenames><affiliation>New Mexico State University</affiliation></author><author><keyname>McKeever</keyname><forenames>Kenneth</forenames><affiliation>New Mexico State University</affiliation></author><author><keyname>Oehrstroem-Sandgren</keyname><forenames>Thorsten</forenames><affiliation>New Mexico State University</affiliation></author></authors><title>An Empirical Approach to Temporal Reference Resolution</title><categories>cmp-lg cs.CL</categories><comments>13 pages, latex using aclap.sty</comments><journal-ref>Proceedings of the Second Conference On Empirical Methods in
  Natural Language Processing (EMNLP-2), August 1-2, 1997, Providence, RI</journal-ref><abstract>  This paper presents the results of an empirical investigation of temporal
reference resolution in scheduling dialogs. The algorithm adopted is primarily
a linear-recency based approach that does not include a model of global focus.
A fully automatic system has been developed and evaluated on unseen test data
with good results. This paper presents the results of an intercoder reliability
study, a model of temporal reference resolution that supports linear recency
and has very good coverage, the results of the system evaluated on unseen test
data, and a detailed analysis of the dialogs assessing the viability of the
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706021</id><created>1997-06-17</created><authors><author><keyname>Rose'</keyname><forenames>Carolyn Penstien</forenames></author><author><keyname>Lavie</keyname><forenames>Alon</forenames></author></authors><title>An Efficient Distribution of Labor in a Two Stage Robust Interpretation
  Process</title><categories>cmp-lg cs.CL</categories><comments>9 pages, 1 Postscript figure, uses aclap.sty and psfig.tex, In
  Proceedings of EMNLP 1997</comments><abstract>  Although Minimum Distance Parsing (MDP) offers a theoretically attractive
solution to the problem of extragrammaticality, it is often computationally
infeasible in large scale practical applications. In this paper we present an
alternative approach where the labor is distributed between a more restrictive
partial parser and a repair module. Though two stage approaches have grown in
popularity in recent years because of their efficiency, they have done so at
the cost of requiring hand coded repair heuristics. In contrast, our two stage
approach does not require any hand coded knowledge sources dedicated to repair,
thus making it possible to achieve a similar run time advantage over MDP
without losing the quality of domain independence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706022</id><created>1997-06-17</created><authors><author><keyname>Collins</keyname><forenames>Michael</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Three Generative, Lexicalised Models for Statistical Parsing</title><categories>cmp-lg cs.CL</categories><comments>8 pages, to appear in Proceedings of ACL/EACL 97.</comments><abstract>  In this paper we first propose a new statistical parsing model, which is a
generative model of lexicalised context-free grammar. We then extend the model
to include a probabilistic treatment of both subcategorisation and wh-movement.
Results on Wall Street Journal text show that the parser performs at 88.1/87.5%
constituent precision/recall, an average improvement of 2.3% over (Collins 96).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706023</id><created>1997-06-18</created><authors><author><keyname>Neumann</keyname><forenames>G.</forenames><affiliation>DFKI GmbH</affiliation></author><author><keyname>Backofen</keyname><forenames>R.</forenames><affiliation>DFKI GmbH</affiliation></author><author><keyname>Baur</keyname><forenames>J.</forenames><affiliation>DFKI GmbH</affiliation></author><author><keyname>Becker</keyname><forenames>M.</forenames><affiliation>DFKI GmbH</affiliation></author><author><keyname>Braun</keyname><forenames>C.</forenames><affiliation>DFKI GmbH</affiliation></author></authors><title>An Information Extraction Core System for Real World German Text
  Processing</title><categories>cmp-lg cs.CL</categories><comments>9 pages; in Proc. of 5th ANLP, 1997</comments><abstract>  This paper describes SMES, an information extraction core system for real
world German text processing. The basic design criterion of the system is of
providing a set of basic powerful, robust, and efficient natural language
components and generic linguistic knowledge sources which can easily be
customized for processing different tasks in a flexible manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706024</id><created>1997-06-18</created><authors><author><keyname>Popowich</keyname><forenames>Fred</forenames><affiliation>Simon Fraser University, Burnaby, Canada; TCC Communications, Victoria, Canada</affiliation></author><author><keyname>Turcato</keyname><forenames>Davide</forenames><affiliation>Simon Fraser University, Burnaby, Canada; TCC Communications, Victoria, Canada</affiliation></author><author><keyname>Laurens</keyname><forenames>Olivier</forenames><affiliation>Simon Fraser University, Burnaby, Canada; TCC Communications, Victoria, Canada</affiliation></author><author><keyname>McFetridge</keyname><forenames>Paul</forenames><affiliation>Simon Fraser University, Burnaby, Canada; TCC Communications, Victoria, Canada</affiliation></author><author><keyname>Nicholson</keyname><forenames>J. Devlan</forenames><affiliation>Simon Fraser University, Burnaby, Canada; TCC Communications, Victoria, Canada</affiliation></author><author><keyname>McGivern</keyname><forenames>Patrick</forenames><affiliation>Simon Fraser University, Burnaby, Canada; TCC Communications, Victoria, Canada</affiliation></author><author><keyname>Pena</keyname><forenames>Maricela Corzo</forenames><affiliation>Simon Fraser University, Burnaby, Canada; TCC Communications, Victoria, Canada</affiliation></author><author><keyname>Pidruchney</keyname><forenames>Lisa</forenames><affiliation>Simon Fraser University, Burnaby, Canada; TCC Communications, Victoria, Canada</affiliation></author><author><keyname>MacDonald</keyname><forenames>Scott</forenames><affiliation>Simon Fraser University, Burnaby, Canada; TCC Communications, Victoria, Canada</affiliation></author></authors><title>A Lexicalist Approach to the Translation of Colloquial Text</title><categories>cmp-lg cs.CL</categories><comments>11 pages, LaTeX, uses tmi.sty</comments><journal-ref>Proceedings of the 7th International Conference on Theoretical
  Issues in Machine Translation (TMI '97), Santa Fe, NM, 23-25 July 1997.</journal-ref><abstract>  Colloquial English (CE) as found in television programs or typical
conversations is different than text found in technical manuals, newspapers and
books. Phrases tend to be shorter and less sophisticated. In this paper, we
look at some of the theoretical and implementational issues involved in
translating CE. We present a fully automatic large-scale multilingual natural
language processing system for translation of CE input text, as found in the
commercially transmitted closed-caption television signal, into simple target
sentences. Our approach is based on the Whitelock's Shake and Bake machine
translation paradigm, which relies heavily on lexical resources. The system
currently translates from English to Spanish with the translation modules for
Brazilian Portuguese under development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706025</id><created>1997-06-24</created><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Portable Algorithm for Mapping Bitext Correspondence</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><journal-ref>Proceedings of the 35th Conference of the Association for
  Computational Linguistics (ACL'97), Madrid, Spain, 1997.</journal-ref><abstract>  The first step in most empirical work in multilingual NLP is to construct
maps of the correspondence between texts and their translations ({\bf bitext
maps}). The Smooth Injective Map Recognizer (SIMR) algorithm presented here is
a generic pattern recognition algorithm that is particularly well-suited to
mapping bitext correspondence. SIMR is faster and significantly more accurate
than other algorithms in the literature. The algorithm is robust enough to use
on noisy texts, such as those resulting from OCR input, and on translations
that are not very literal. SIMR encapsulates its language-specific heuristics,
so that it can be ported to any language pair with a minimal effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706026</id><created>1997-06-24</created><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Word-to-Word Model of Translational Equivalence</title><categories>cmp-lg cs.CL</categories><comments>8 pages; this version has some typos corrected</comments><journal-ref>Proceedings of the 35th Conference of the Association for
  Computational Linguistics (ACL'97), Madrid, Spain, 1997.</journal-ref><abstract>  Many multilingual NLP applications need to translate words between different
languages, but cannot afford the computational expense of inducing or applying
a full translation model. For these applications, we have designed a fast
algorithm for estimating a partial translation model, which accounts for
translational equivalence only at the word level. The model's precision/recall
trade-off can be directly controlled via one threshold parameter. This feature
makes the model more suitable for applications that are not fully statistical.
The model's hidden parameters can be easily conditioned on information
extrinsic to the model, providing an easy way to integrate pre-existing
knowledge such as part-of-speech, dictionaries, word order, etc.. Our model can
link word tokens in parallel texts as well as other translation models in the
literature. Unlike other translation models, it can automatically produce
dictionary-sized translation lexicons, and it can do so with over 99% accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706027</id><created>1997-06-24</created><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Automatic Discovery of Non-Compositional Compounds in Parallel Data</title><categories>cmp-lg cs.CL</categories><comments>12 pages; uses natbib.sty, here.sty</comments><journal-ref>Proceedings of the 2nd Conference on Empirical Methods in Natural
  Language Processing (EMNLP'97), Providence, RI, 1997.</journal-ref><abstract>  Automatic segmentation of text into minimal content-bearing units is an
unsolved problem even for languages like English. Spaces between words offer an
easy first approximation, but this approximation is not good enough for machine
translation (MT), where many word sequences are not translated word-for-word.
This paper presents an efficient automatic method for discovering sequences of
words that are translated as a unit. The method proceeds by comparing pairs of
statistical translation models induced from parallel texts in two languages. It
can discover hundreds of non-compositional compounds on each iteration, and
constructs longer compounds out of shorter ones. Objective evaluation on a
simple machine translation task has shown the method's potential to improve the
quality of MT output. The method makes few assumptions about the data, so it
can be applied to parallel data other than parallel texts, such as word
spellings and pronunciations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706028</id><created>1997-06-26</created><authors><author><keyname>Doerre</keyname><forenames>Jochen</forenames><affiliation>Univ. Stuttgart</affiliation></author></authors><title>Efficient Construction of Underspecified Semantics under Massive
  Ambiguity</title><categories>cmp-lg cs.CL</categories><comments>9 pages, 7 postscript figures, LaTeX source (aclap, psfig styles)</comments><journal-ref>Proceedings of ACL/EACL'97</journal-ref><abstract>  We investigate the problem of determining a compact underspecified semantical
representation for sentences that may be highly ambiguous. Due to combinatorial
explosion, the naive method of building semantics for the different syntactic
readings independently is prohibitive. We present a method that takes as input
a syntactic parse forest with associated constraint-based semantic construction
rules and directly builds a packed semantic structure. The algorithm is fully
implemented and runs in $O(n^4 log(n))$ in sentence length, if the grammar
meets some reasonable `normality' restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9706029</id><created>1997-06-30</created><authors><author><keyname>Hermjakob</keyname><forenames>Ulf</forenames><affiliation>Dept. of Computer Sciences, University of Texas at Austin</affiliation></author></authors><title>Learning Parse and Translation Decisions From Examples With Rich Context</title><categories>cmp-lg cs.CL</categories><comments>dissertation, 175 pages, LaTeX, 23 postscript figures, uses
  utthesis-compact.sty</comments><report-no>TR 97-12</report-no><abstract>  We propose a system for parsing and translating natural language that learns
from examples and uses some background knowledge.
  As our parsing model we choose a deterministic shift-reduce type parser that
integrates part-of-speech tagging and syntactic and semantic processing.
Applying machine learning techniques, the system uses parse action examples
acquired under supervision to generate a parser in the form of a decision
structure, a generalization of decision trees.
  To learn good parsing and translation decisions, our system relies heavily on
context, as encoded in currently 205 features describing the morphological,
syntactical and semantical aspects of a given parse state. Compared with recent
probabilistic systems that were trained on 40,000 sentences, our system relies
on more background knowledge and a deeper analysis, but radically fewer
examples, currently 256 sentences.
  We test our parser on lexically limited sentences from the Wall Street
Journal and achieve accuracy rates of 89.8% for labeled precision, 98.4% for
part of speech tagging and 56.3% of test sentences without any crossing
brackets. Machine translations of 32 Wall Street Journal sentences to German
have been evaluated by 10 bilingual volunteers and been graded as 2.4 on a 1.0
(best) to 6.0 (worst) scale for both grammatical correctness and meaning
preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707001</id><created>1997-07-03</created><authors><author><keyname>Dras</keyname><forenames>Mark</forenames><affiliation>Microsoft Research Institute, Macquarie University</affiliation></author></authors><title>Reluctant Paraphrase: Textual Restructuring under an Optimisation Model</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX source (pacling97, examples styles)</comments><abstract>  This paper develops a computational model of paraphrase under which text
modification is carried out reluctantly; that is, there are external
constraints, such as length or readability, on an otherwise ideal text, and
modifications to the text are necessary to ensure conformance to these
constraints. This problem is analogous to a mathematical optimisation problem:
the textual constraints can be described as a set of constraint equations, and
the requirement for minimal change to the text can be expressed as a function
to be minimised; so techniques from this domain can be used to solve the
problem.
  The work is done as part of a computational paraphrase system using the XTAG
system as a base. The paper will present a theoretical computational framework
for working within the Reluctant Paraphrase paradigm: three types of textual
constraints are specified, effects of paraphrase on text are described, and a
model incorporating mathematical optimisation techniques is outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707002</id><created>1997-07-08</created><authors><author><keyname>Kessler</keyname><forenames>Brett</forenames><affiliation>Xerox PARC and Stanford University</affiliation></author><author><keyname>Nunberg</keyname><forenames>Geoffrey</forenames><affiliation>Xerox PARC and Stanford University</affiliation></author><author><keyname>Schuetze</keyname><forenames>Hinrich</forenames><affiliation>Xerox PARC and Stanford University</affiliation></author></authors><title>Automatic Detection of Text Genre</title><categories>cmp-lg cs.CL</categories><comments>7 pages</comments><journal-ref>Proceedings ACL/EACL 1997, Madrid, p. 32-38</journal-ref><abstract>  As the text databases available to users become larger and more
heterogeneous, genre becomes increasingly important for computational
linguistics as a complement to topical and structural principles of
classification. We propose a theory of genres as bundles of facets, which
correlate with various surface cues, and argue that genre detection based on
surface cues is as successful as detection based on deeper structural
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707003</id><created>1997-07-11</created><authors><author><keyname>Marquez</keyname><forenames>Lluis</forenames></author><author><keyname>Padro</keyname><forenames>Lluis</forenames></author></authors><title>A Flexible POS tagger Using an Automatically Acquired Language Model</title><categories>cmp-lg cs.CL</categories><comments>8 pages, aclap.sty, 2 eps figures. Appears in (E)ACL'97</comments><journal-ref>Proceedings of EACL/ACL 1997, Madrid, Spain</journal-ref><abstract>  We present an algorithm that automatically learns context constraints using
statistical decision trees. We then use the acquired constraints in a flexible
POS tagger. The tagger is able to use information of any degree: n-grams,
automatically learned context constraints, linguistically motivated manually
written constraints, etc. The sources and kinds of constraints are
unrestricted, and the language model can be easily extended, improving the
results. The tagger has been tested and evaluated on the WSJ corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707004</id><created>1997-07-16</created><authors><author><keyname>Jaspars</keyname><forenames>Jan</forenames><affiliation>CWI</affiliation></author><author><keyname>Kameyama</keyname><forenames>Megumi</forenames><affiliation>SRI International</affiliation></author></authors><title>Discourse Preferences in Dynamic Logic</title><categories>cmp-lg cs.CL</categories><comments>To appear in Van Glabbeek, R. et al. eds., a collection of papers
  from the Fourth CSLI Workshop in Logic, Language, and Computation, CSLI
  Publications. (This is a revised version of the paper titled &quot;Preferences in
  Dynamic Semantics&quot; in the Proceedings of the Tenth Amsterdam Colloquium,
  1995, pages 445-464.)</comments><abstract>  In order to enrich dynamic semantic theories with a `pragmatic' capacity, we
combine dynamic and nonmonotonic (preferential) logics in a modal logic
setting. We extend a fragment of Van Benthem and De Rijke's dynamic modal logic
with additional preferential operators in the underlying static logic, which
enables us to define defeasible (pragmatic) entailments over a given piece of
discourse. We will show how this setting can be used for a dynamic logical
analysis of preferential resolutions of ambiguous pronouns in discourse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707005</id><created>1997-07-16</created><authors><author><keyname>Kameyama</keyname><forenames>Megumi</forenames><affiliation>SRI International</affiliation></author></authors><title>Intrasentential Centering: A Case Study</title><categories>cmp-lg cs.CL</categories><comments>A chapter in Walker, M., A. Joshi, and E. Prince, eds., {\it
  Centering Theory in Discourse}, Oxford University Press, Oxford, in press</comments><abstract>  One of the necessary extensions to the centering model is a mechanism to
handle pronouns with intrasentential antecedents. Existing centering models
deal only with discourses consisting of simple sentences. It leaves unclear how
to delimit center-updating utterance units and how to process complex
utterances consisting of multiple clauses. In this paper, I will explore the
extent to which a straightforward extension of an existing intersentential
centering model contributes to this effect. I will motivate an approach that
breaks a complex sentence into a hierarchy of center-updating units and
proposes the preferred interpretation of a pronoun in its local context
arbitrarily deep in the given sentence structure. This approach will be
substantiated with examples from naturally occurring written discourses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707006</id><created>1997-07-17</created><authors><author><keyname>Kempe</keyname><forenames>Andre</forenames><affiliation>Rank Xerox Research Centre, Grenoble Laboratory, France</affiliation></author></authors><title>Finite State Transducers Approximating Hidden Markov Models</title><categories>cmp-lg cs.CL</categories><comments>8 pages, A4, LaTeX (+1x eps)</comments><journal-ref>ACL'97, pp.460-467, Madrid, Spain. July 10, 1997</journal-ref><abstract>  This paper describes the conversion of a Hidden Markov Model into a
sequential transducer that closely approximates the behavior of the stochastic
model. This transformation is especially advantageous for part-of-speech
tagging because the resulting transducer can be composed with other transducers
that encode correction rules for the most frequent tagging errors. The speed of
tagging is also improved. The described methods have been implemented and
successfully tested on six languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707007</id><created>1997-07-18</created><authors><author><keyname>Reiter</keyname><forenames>Ehud</forenames><affiliation>Univ of Aberdeen, CS</affiliation></author><author><keyname>Osman</keyname><forenames>Liesl</forenames><affiliation>Univ of Aberdeen, Medicine</affiliation></author></authors><title>Tailored Patient Information: Some Issues and Questions</title><categories>cmp-lg cs.CL</categories><comments>This is a paper about technology-transfer. It does not have much
  technical content</comments><journal-ref>Proceedings of the 1997 ACL Workshop on From Research to
  Commercial Applications: Making NLP Work in Practice</journal-ref><abstract>  Tailored patient information (TPI) systems are computer programs which
produce personalised heath-information material for patients. TPI systems are
of growing interest to the natural-language generation (NLG) community; many
TPI systems have also been developed in the medical community, usually with
mail-merge technology. No matter what technology is used, experience shows that
it is not easy to field a TPI system, even if it is shown to be effective in
clinical trials. In this paper we discuss some of the difficulties in fielding
TPI systems. This is based on our experiences with 2 TPI systems, one for
generating asthma-information booklets and one for generating smoking-cessation
letters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707008</id><created>1997-07-18</created><authors><author><keyname>Kameyama</keyname><forenames>Megumi</forenames></author></authors><title>Stressed and Unstressed Pronouns: Complementary Preferences</title><categories>cmp-lg cs.CL</categories><comments>Uses endnotes.sty and lingmacros.sty. To appear in Bosch, Peter and
  Rob van der Sandt, eds., Focus: Linguistic, Cognitive and Computational
  Perspectives. Cambridge University Press. (This is an extended and revised
  version of the paper with the same title in Bosch, Peter and Rob van der
  Sandt, eds., Focus and Natural Language Processing, Institute for Logic and
  Linguistics, IBM, Heidelberg, 1994, 475-484.)</comments><abstract>  I present a unified account of interpretation preferences of stressed and
unstressed pronouns in discourse. The central intuition is the Complementary
Preference Hypothesis that predicts the interpretation preference of a stressed
pronoun from that of an unstressed pronoun in the same discourse position. The
base preference must be computed in a total pragmatics module including
commonsense preferences. The focus constraint in Rooth's theory of semantic
focus is interpreted to be the salient subset of the domain in the local
attentional state in the discourse context independently motivated for other
purposes in Centering Theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707009</id><created>1997-07-18</created><authors><author><keyname>Kameyama</keyname><forenames>Megumi</forenames><affiliation>SRI International</affiliation></author></authors><title>Recognizing Referential Links: An Information Extraction Perspective</title><categories>cmp-lg cs.CL</categories><comments>8 pages. a paper in the proceedings of the ACL/EACL'97 workshop on
  anaphora resolution</comments><journal-ref>In Mitkov, R. and B. Boguraev, eds., Proceedings of ACL/EACL
  Workshop on Operational Factors in Practical, Robust Anaphora Resolution for
  Unrestricted Texts, Madrid, July 1997, pages 46-53.</journal-ref><abstract>  We present an efficient and robust reference resolution algorithm in an
end-to-end state-of-the-art information extraction system, which must work with
a considerably impoverished syntactic analysis of the input sentences.
Considering this disadvantage, the basic setup to collect, filter, then order
by salience does remarkably well with third-person pronouns, but needs more
semantic and discourse information to improve the treatments of other
expression types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707010</id><created>1997-07-21</created><authors><author><keyname>Volk</keyname><forenames>Martin</forenames><affiliation>University of Zurich, Switzerland</affiliation></author><author><keyname>Richarz</keyname><forenames>Dirk</forenames><affiliation>University of Koblenz-Landau, Germany</affiliation></author></authors><title>Experiences with the GTU grammar development environment</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses aclap.sty</comments><journal-ref>Proceedings of Workshop on Computational Environments For Grammar
  Development And Linguistic Engineering at the ACL/EACL Joint Conference 1997,
  107-113</journal-ref><abstract>  In this paper we describe our experiences with a tool for the development and
testing of natural language grammars called GTU (German:
Grammatik-Testumgebumg; grammar test environment). GTU supports four grammar
formalisms under a window-oriented user interface. Additionally, it contains a
set of German test sentences covering various syntactic phenomena as well as
three types of German lexicons that can be attached to a grammar via an
integrated lexicon interface. What follows is a description of the experiences
we gained when we used GTU as a tutoring tool for students and as an
experimental tool for CL researchers. From these we will derive the features
necessary for a future grammar workbench.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707011</id><created>1997-07-22</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>A lexical database tool for quantitative phonological research</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses ipamacs.sty</comments><journal-ref>Proceedings of the Third Meeting of the ACL Special Interest Group
  in Computational Phonology, pp. 33-39, Madrid, July 1997. ACL</journal-ref><abstract>  A lexical database tool tailored for phonological research is described.
Database fields include transcriptions, glosses and hyperlinks to speech files.
Database queries are expressed using HTML forms, and these permit regular
expression search on any combination of fields. Regular expressions are passed
directly to a Perl CGI program, enabling the full flexibility of Perl extended
regular expressions. The regular expression notation is extended to better
support phonological searches, such as search for minimal pairs. Search results
are presented in the form of HTML or LaTeX tables, where each cell is either a
number (representing frequency) or a designated subset of the fields. Tables
have up to four dimensions, with an elegant system for specifying which
fragments of which fields should be used for the row/column labels. The tool
offers several advantages over traditional methods of analysis: (i) it supports
a quantitative method of doing phonological research; (ii) it gives universal
access to the same set of informants; (iii) it enables other researchers to
hear the original speech data without having to rely on published
transcriptions; (iv) it makes the full power of regular expression search
available, and search results are full multimedia documents; and (v) it enables
the early refutation of false hypotheses, shortening the
analysis-hypothesis-test loop. A life-size application to an African tone
language (Dschang) is used for exemplification throughout the paper. The
database contains 2200 records, each with approximately 15 fields. Running on a
PC laptop with a stand-alone web server, the `Dschang HyperLexicon' has already
been used extensively in phonological fieldwork and analysis in Cameroon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707012</id><created>1997-07-22</created><authors><author><keyname>Moennich</keyname><forenames>Uwe</forenames><affiliation>Tuebingen University/SfS</affiliation></author></authors><title>Adjunction As Substitution: An Algebraic Formulation of Regular,
  Context-Free and Tree Adjoining Languages</title><categories>cmp-lg cs.CL</categories><comments>Formal Grammar Conference, Aix-en-Provence, Aug. 97, 11pp., uses
  AMS-LaTeX, natbib</comments><abstract>  This note presents a method of interpreting the tree adjoining languages as
the natural third step in a hierarchy that starts with the regular and the
context-free languages. The central notion in this account is that of a
higher-order substitution. Whereas in traditional presentations of rule systems
for abstract language families the emphasis has been on a first-order
substitution process in which auxiliary variables are replaced by elements of
the carrier of the proper algebra - concatenations of terminal and auxiliary
category symbols in the string case - we lift this process to the level of
operations defined on the elements of the carrier of the algebra. Our own view
is that this change of emphasis provides the adequate platform for a better
understanding of the operation of adjunction. To put it in a nutshell:
Adjoining is not a first-order, but a second-order substitution operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707013</id><created>1997-07-22</created><authors><author><keyname>Moennich</keyname><forenames>Uwe</forenames><affiliation>Tuebingen University/SfS</affiliation></author></authors><title>On Cloning Context-Freeness</title><categories>cmp-lg cs.CL</categories><comments>36pp., uses AMS-LaTeX, tree-dvips, natbib</comments><report-no>Arbeitspapiere des SFB 340, Bericht Nr. 114</report-no><abstract>  To Rogers (1994) we owe the insight that monadic second order predicate logic
with multiple successors (MSO) is well suited in many respects as a realistic
formal base for syntactic theorizing. However, the agreeable formal properties
of this logic come at a cost: MSO is equivalent with the class of regular tree
automata/grammars, and, thereby, with the class of context-free languages.
  This paper outlines one approach towards a solution of MSO's expressivity
problem. On the background of an algebraically refined Chomsky hierarchy, which
allows the definition of several classes of languages--in particular, a whole
hierarchy between CF and CS--via regular tree grammars over unambiguously
derivable alphabets of varying complexity plus their respective
yield-functions, it shows that not only some non-context-free string languages
can be captured by context-free means in this way, but that this approach can
be generalized to the corresponding structures. I.e., non-recognizable sets of
structures can--up to homomorphism--be coded context-freely. Since the class of
languages covered--Fischer's (1968} OI family of indexed languages--includes
all attested instances of non-context-freeness in natural language, there
exists an indirect, to be sure, but completely general way to formally describe
the natural languages using a weak framework like MSO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707014</id><created>1997-07-22</created><authors><author><keyname>Agarwal</keyname><forenames>Rajeev</forenames><affiliation>Texas Instruments, Inc.</affiliation></author></authors><title>Towards a PURE Spoken Dialogue System for Information Access</title><categories>cmp-lg cs.CL</categories><comments>8 pages, 2 encapsulated postscript figures, uses aclap.sty</comments><journal-ref>Proceedings of the ACL/EACL Workshop on &quot;Interactive Spoken Dialog
  Systems: Bringing Speech and NLP Together in Real Applications,&quot; Madrid,
  Spain, pp. 90-97, 1997.</journal-ref><abstract>  With the rapid explosion of the World Wide Web, it is becoming increasingly
possible to easily acquire a wide variety of information such as flight
schedules, yellow pages, used car prices, current stock prices, entertainment
event schedules, account balances, etc. It would be very useful to have spoken
dialogue interfaces for such information access tasks. We identify portability,
usability, robustness, and extensibility as the four primary design objectives
for such systems. In other words, the objective is to develop a PURE (Portable,
Usable, Robust, Extensible) system. A two-layered dialogue architecture for
spoken dialogue systems is presented where the upper layer is
domain-independent and the lower layer is domain-specific. We are implementing
this architecture in a mixed-initiative system that accesses flight
arrival/departure information from the World Wide Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707015</id><created>1997-07-23</created><authors><author><keyname>Brants</keyname><forenames>Thorsten</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author><author><keyname>Skut</keyname><forenames>Wojciech</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author><author><keyname>Krenn</keyname><forenames>Brigitte</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author></authors><title>Tagging Grammatical Functions</title><categories>cmp-lg cs.CL</categories><comments>11 pages, LaTeX, uses aclap.sty, psfig.sty, and rotate.sty</comments><abstract>  This paper addresses issues in automated treebank construction. We show how
standard part-of-speech tagging techniques extend to the more general problem
of structural annotation, especially for determining grammatical functions and
syntactic categories. Annotation is viewed as an interactive process where
manual and automatic processing alternate. Efficiency and accuracy results are
presented. We also discuss further automation steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707016</id><created>1997-07-25</created><authors><author><keyname>Calder</keyname><forenames>Jo</forenames><affiliation>University of Edinburgh Language Technology Group, Human Communication Research Centre and Centre for Cognitive Science</affiliation></author></authors><title>On aligning trees</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uses psfig.tex</comments><abstract>  The increasing availability of corpora annotated for linguistic structure
prompts the question: if we have the same texts, annotated for phrase structure
under two different schemes, to what extent do the annotations agree on
structuring within the text? We suggest the term tree alignment to indicate the
situation where two markup schemes choose to bracket off the same text
elements. We propose a general method for determining agreement between two
analyses. We then describe an efficient implementation, which is also modular
in that the core of the implementation can be reused regardless of the format
of markup used in the corpora. The output of the implementation on the Susanne
and Penn treebank corpora is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707017</id><created>1997-07-28</created><authors><author><keyname>Coleman</keyname><forenames>John</forenames></author><author><keyname>Pierrehumbert</keyname><forenames>Janet</forenames></author></authors><title>Stochastic phonological grammars and acceptability</title><categories>cmp-lg cs.CL</categories><comments>compressed postscript, 8 pages, 1 figure</comments><abstract>  In foundational works of generative phonology it is claimed that subjects can
reliably discriminate between possible but non-occurring words and words that
could not be English. In this paper we examine the use of a probabilistic
phonological parser for words to model experimentally-obtained judgements of
the acceptability of a set of nonsense words. We compared various methods of
scoring the goodness of the parse as a predictor of acceptability. We found
that the probability of the worst part is not the best score of acceptability,
indicating that classical generative phonology and Optimality Theory miss an
important fact, as these approaches do not recognise a mechanism by which the
frequency of well-formed parts may ameliorate the unacceptability of
low-frequency parts. We argue that probabilistic generative grammars are
demonstrably a more psychologically realistic model of phonological competence
than standard generative phonology or Optimality Theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707018</id><created>1997-07-28</created><authors><author><keyname>Coleman</keyname><forenames>John</forenames></author><author><keyname>Dirksen</keyname><forenames>Arthur</forenames></author><author><keyname>Hussain</keyname><forenames>Sarmad</forenames></author><author><keyname>Waals</keyname><forenames>Juliette</forenames></author></authors><title>Multilingual phonological analysis and speech synthesis</title><categories>cmp-lg cs.CL</categories><comments>tex file, 1 postscript figure. Paper is from Proceedings of the 2nd
  meeting of ACL SIGPHON, Santa Cruz, 1997</comments><abstract>  We give an overview of multilingual speech synthesis using the IPOX system.
The first part discusses work in progress for various languages: Tashlhit
Berber, Urdu and Dutch. The second part discusses a multilingual phonological
grammar, which can be adapted to a particular language by setting parameters
and adding language-specific details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707019</id><created>1997-07-28</created><authors><author><keyname>Carberry</keyname><forenames>Sandra</forenames><affiliation>University of Delaware</affiliation></author><author><keyname>Harvey</keyname><forenames>Terrence</forenames><affiliation>University of Delaware</affiliation></author></authors><title>Generating Coherent Messages in Real-time Decision Support: Exploiting
  Discourse Theory for Discourse Practice</title><categories>cmp-lg cs.CL</categories><comments>6 pages</comments><journal-ref>Proceedings of the 19th Annual Conference of the Cognitive Science
  Society (1997)</journal-ref><abstract>  This paper presents a message planner, TraumaGEN, that draws on rhetorical
structure and discourse theory to address the problem of producing integrated
messages from individual critiques, each of which is designed to achieve its
own communicative goal. TraumaGEN takes into account the purpose of the
messages, the situation in which the messages will be received, and the social
role of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9707020</id><created>1997-07-30</created><authors><author><keyname>Skoumalova</keyname><forenames>Hana</forenames><affiliation>Charles University</affiliation></author></authors><title>A Czech Morphological Lexicon</title><categories>cmp-lg cs.CL</categories><comments>7 pages, A4, aclap.sty</comments><journal-ref>Proceedings of the Third Meeting of the ACL Special Interest Group
  in Computational Phonology, pp. 41-47, Madrid, July 1997. ACL</journal-ref><abstract>  In this paper, a treatment of Czech phonological rules in two-level
morphology approach is described. First the possible phonological alternations
in Czech are listed and then their treatment in a practical application of a
Czech morphological lexicon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708001</id><created>1997-08-05</created><authors><author><keyname>Cristea</keyname><forenames>Dan</forenames><affiliation>University &quot;A.I. Cuza&quot;, Iasi, Romania</affiliation></author><author><keyname>Webber</keyname><forenames>Bonnie Lynn</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Expectations in Incremental Discourse Processing</title><categories>cmp-lg cs.CL</categories><comments>9 pages, uses aclap.sty, psfig.tex</comments><journal-ref>Proceedings 35th Annual ACL, Madrid - June 1997</journal-ref><abstract>  The way in which discourse features express connections back to the previous
discourse has been described in the literature in terms of adjoining at the
right frontier of discourse structure. But this does not allow for discourse
features that express expectations about what is to come in the subsequent
discourse. After characterizing these expectations and their distribution in
text, we show how an approach that makes use of substitution as well as
adjoining on a suitably defined right frontier, can be used to both process
expectations and constrain discouse processing in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708002</id><created>1997-08-07</created><authors><author><keyname>Cawsey</keyname><forenames>Alison J.</forenames><affiliation>Herriot-Watt University</affiliation></author><author><keyname>Webber</keyname><forenames>Bonnie L.</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Jones</keyname><forenames>Ray B.</forenames><affiliation>University of Glasgow</affiliation></author></authors><title>Natural Language Generation in Healthcare: Brief Review</title><categories>cmp-lg cs.CL</categories><comments>15 pages, to appear in the Journal of the American Medical
  Informatics Association</comments><abstract>  Good communication is vital in healthcare, both among healthcare
professionals, and between healthcare professionals and their patients. And
well-written documents, describing and/or explaining the information in
structured databases may be easier to comprehend, more edifying and even more
convincing, than the structured data, even when presented in tabular or graphic
form. Documents may be automatically generated from structured data, using
techniques from the field of natural language generation. These techniques are
concerned with how the content, organisation and language used in a document
can be dynamically selected, depending on the audience and context. They have
been used to generate health education materials, explanations and critiques in
decision support systems, and medical reports and progress notes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708003</id><created>1997-08-07</created><authors><author><keyname>Webber</keyname><forenames>Bonnie L.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Structure and Ostension in the Interpretation of Discourse Deixis</title><categories>cmp-lg cs.CL</categories><comments>22 pages, uses psfig</comments><journal-ref>Language and Cognitive Processes 6(2), May 1991, pp. 107-135</journal-ref><abstract>  This paper examines demonstrative pronouns used as deictics to refer to the
interpretation of one or more clauses. Although this usage is frowned upon in
style manuals (for example Strunk and White (1959) state that ``This. The
pronoun 'this', referring to the complete sense of a preceding sentence or
clause, cannot always carry the load and so may produce an imprecise
statement.''), it is nevertheless very common in written text. Handling this
usage poses a problem for Natural Language Understanding systems. The solution
I propose is based on distinguishing between what can be pointed to and what
can be referred to by virtue of pointing. I argue that a restricted set of
discourse segments yield what such demonstrative pronouns can point to and a
restricted set of what Nunberg (1979) has called referring functions yield what
they can refer to by virtue of that pointing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708004</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708004</id><created>1997-08-06</created><authors><author><keyname>Abusch</keyname><forenames>Dorit</forenames><affiliation>IMS, University of Stuttgart</affiliation></author><author><keyname>Rooth</keyname><forenames>Mats</forenames><affiliation>IMS, University of Stuttgart</affiliation></author></authors><title>Epistemic NP Modifiers</title><categories>cmp-lg cs.CL</categories><comments>Final pre-publication version, 27 pages, Postscript. Final version
  appears in the proceedings of SALT VII</comments><report-no>Arbeitspapiere des SFB 340, Bericht Nr. 108, Juni 1997</report-no><abstract>  The paper considers participles such as &quot;unknown&quot;, &quot;identified&quot; and
&quot;unspecified&quot;, which in sentences such as &quot;Solange is staying in an unknown
hotel&quot; have readings equivalent to an indirect question &quot;Solange is staying in
a hotel, and it is not known which hotel it is.&quot; We discuss phenomena including
disambiguation of quantifier scope and a restriction on the set of determiners
which allow the reading in question. Epistemic modifiers are analyzed in a DRT
framework with file (information state) discourse referents. The proposed
semantics uses a predication on files and discourse referents which is related
to recent developments in dynamic modal predicate calculus. It is argued that a
compositional DRT semantics must employ a semantic type of discourse referents,
as opposed to just a type of individuals. A connection is developed between the
scope effects of epistemic modifiers and the scope-disambiguating effect of &quot;a
certain&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708005</id><created>1997-08-11</created><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames></author></authors><title>Centering, Anaphora Resolution, and Discourse Structure</title><categories>cmp-lg cs.CL</categories><comments>35 pages, uses elsart12, lingmacros, named, psfig</comments><journal-ref>Centering In Discourse, eds. Marilyn A. Walker, Aravind K. Joshi
  and Ellen F. Prince, Oxford University Press, 1997</journal-ref><abstract>  Centering was formulated as a model of the relationship between attentional
state, the form of referring expressions, and the coherence of an utterance
within a discourse segment (Grosz, Joshi and Weinstein, 1986; Grosz, Joshi and
Weinstein, 1995). In this chapter, I argue that the restriction of centering to
operating within a discourse segment should be abandoned in order to integrate
centering with a model of global discourse structure. The within-segment
restriction causes three problems. The first problem is that centers are often
continued over discourse segment boundaries with pronominal referring
expressions whose form is identical to those that occur within a discourse
segment. The second problem is that recent work has shown that listeners
perceive segment boundaries at various levels of granularity. If centering
models a universal processing phenomenon, it is implausible that each listener
is using a different centering algorithm.The third issue is that even for
utterances within a discourse segment, there are strong contrasts between
utterances whose adjacent utterance within a segment is hierarchically recent
and those whose adjacent utterance within a segment is linearly recent. This
chapter argues that these problems can be eliminated by replacing Grosz and
Sidner's stack model of attentional state with an alternate model, the cache
model. I show how the cache model is easily integrated with the centering
algorithm, and provide several types of data from naturally occurring
discourses that support the proposed integrated model. Future work should
provide additional support for these claims with an examination of a larger
corpus of naturally occurring discourses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708006</id><created>1997-08-13</created><updated>1997-08-15</updated><authors><author><keyname>Goodman</keyname><forenames>Joshua</forenames><affiliation>Harvard University</affiliation></author></authors><title>Global Thresholding and Multiple Pass Parsing</title><categories>cmp-lg cs.CL</categories><comments>Fixed latex errors; fixed minor errors in published version</comments><journal-ref>Proceedings of the Second Conference on Empirical Methods in
  Natural Language Processing, 11-25</journal-ref><abstract>  We present a variation on classic beam thresholding techniques that is up to
an order of magnitude faster than the traditional method, at the same
performance level. We also present a new thresholding technique, global
thresholding, which, combined with the new beam thresholding, gives an
additional factor of two improvement, and a novel technique, multiple pass
parsing, that can be combined with the others to yield yet another 50%
improvement. We use a new search algorithm to simultaneously optimize the
thresholding parameters of the various algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708007</id><created>1997-08-13</created><updated>1997-08-19</updated><authors><author><keyname>Raman</keyname><forenames>Anand</forenames><affiliation>Comp Sc</affiliation></author><author><keyname>Newman</keyname><forenames>John</forenames><affiliation>Linguistics and Second Language Teaching</affiliation></author><author><keyname>Patrick</keyname><forenames>Jon</forenames><affiliation>Information Systems</affiliation></author></authors><title>A complexity measure for diachronic Chinese phonology</title><categories>cmp-lg cs.CL</categories><comments>uses psfig.sty, aclap.sty, fullname.bst, 9 pages, 2 ps figures</comments><abstract>  This paper addresses the problem of deriving distance measures between parent
and daughter languages with specific relevance to historical Chinese phonology.
The diachronic relationship between the languages is modelled as a
Probabilistic Finite State Automaton. The Minimum Message Length principle is
then employed to find the complexity of this structure. The idea is that this
measure is representative of the amount of dissimilarity between the two
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708008</id><created>1997-08-14</created><authors><author><keyname>Lee</keyname><forenames>Lillian</forenames><affiliation>Cornell University</affiliation></author></authors><title>Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uses aclap.sty and eepic.sty</comments><journal-ref>Proceedings of the 35th ACL/8th EACL, pp 9-15</journal-ref><abstract>  Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG
parsing. We prove a dual result: CFG parsers running in time $O(|G||w|^{3 -
\myeps})$ on a grammar $G$ and a string $w$ can be used to multiply $m \times
m$ Boolean matrices in time $O(m^{3 - \myeps/3})$. In the process we also
provide a formal definition of parsing motivated by an informal notion due to
Lang. Our result establishes one of the first limitations on general CFG
parsing: a fast, practical CFG parser would yield a fast, practical BMM
algorithm, which is not believed to exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708009</id><created>1997-08-18</created><authors><author><keyname>Moeller</keyname><forenames>Jens-Uwe</forenames><affiliation>Natural Language Systems Division, Dept. of Computer Science, Univ. of Hamburg</affiliation></author></authors><title>DIA-MOLE: An Unsupervised Learning Approach to Adaptive Dialogue Models
  for Spoken Dialogue Systems</title><categories>cmp-lg cs.CL</categories><comments>Postscript only</comments><journal-ref>Proc. EUROSPEECH 97, Rhodes, Greek, 2271-2274</journal-ref><abstract>  The DIAlogue MOdel Learning Environment supports an engineering-oriented
approach towards dialogue modelling for a spoken-language interface. Major
steps towards dialogue models is to know about the basic units that are used to
construct a dialogue model and possible sequences. In difference to many other
approaches a set of dialogue acts is not predefined by any theory or manually
during the engineering process, but is learned from data that are available in
an avised spoken dialogue system. The architecture is outlined and the approach
is applied to the domain of appointment scheduling. Even though based on a word
correctness of about 70% predictability of dialogue acts in DIA-MOLE turns out
to be comparable to human-assigned dialogue acts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708010</id><created>1997-08-18</created><authors><author><keyname>Dagan</keyname><forenames>Ido</forenames><affiliation>Bar-Ilan University</affiliation></author><author><keyname>Lee</keyname><forenames>Lillian</forenames><affiliation>Cornell University</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames><affiliation>AT&amp;T Labs -- Research</affiliation></author></authors><title>Similarity-Based Methods For Word Sense Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses psfig.tex and aclap.sty</comments><journal-ref>Proceedings of the 35th ACL/8th EACL, pp 56--63</journal-ref><abstract>  We compare four similarity-based estimation methods against back-off and
maximum-likelihood estimation methods on a pseudo-word sense disambiguation
task in which we controlled for both unigram and bigram frequency. The
similarity-based methods perform up to 40% better on this particular task. We
also conclude that events that occur only once in the training set have major
impact on similarity-based estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708011</id><created>1997-08-19</created><authors><author><keyname>Lee</keyname><forenames>Lillian</forenames><affiliation>Cornell University</affiliation></author></authors><title>Similarity-Based Approaches to Natural Language Processing</title><categories>cmp-lg cs.CL</categories><comments>71 pages (single-spaced)</comments><report-no>Harvard University Technical Report TR-11-97</report-no><abstract>  This thesis presents two similarity-based approaches to sparse data problems.
The first approach is to build soft, hierarchical clusters: soft, because each
event belongs to each cluster with some probability; hierarchical, because
cluster centroids are iteratively split to model finer distinctions. Our second
approach is a nearest-neighbor approach: instead of calculating a centroid for
each class, as in the hierarchical clustering approach, we in essence build a
cluster around each word. We compare several such nearest-neighbor approaches
on a word sense disambiguation task and find that as a whole, their performance
is far superior to that of standard methods. In another set of experiments, we
show that using estimation techniques based on the nearest-neighbor model
enables us to achieve perplexity reductions of more than 20 percent over
standard techniques in the prediction of low-frequency events, and
statistically significant speech recognition error-rate reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708012</id><created>1997-08-19</created><authors><author><keyname>Carroll</keyname><forenames>John</forenames><affiliation>University of Sussex</affiliation></author><author><keyname>Weir</keyname><forenames>David</forenames><affiliation>University of Sussex</affiliation></author></authors><title>Encoding Frequency Information in Lexicalized Grammars</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uses fullname.sty</comments><journal-ref>5th International Workshop on Parsing Technologies (IWPT-97)</journal-ref><abstract>  We address the issue of how to associate frequency information with
lexicalized grammar formalisms, using Lexicalized Tree Adjoining Grammar as a
representative framework. We consider systematically a number of alternative
probabilistic frameworks, evaluating their adequacy from both a theoretical and
empirical perspective using data from existing large treebanks. We also propose
three orthogonal approaches for backing off probability estimates to cope with
the large number of parameters involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9708013</id><created>1997-08-20</created><authors><author><keyname>Sima'an</keyname><forenames>Khalil</forenames><affiliation>University of Utrecht</affiliation></author></authors><title>explanation-based learning of data oriented parsing</title><categories>cmp-lg cs.CL</categories><comments>Appeared in Proceedings of Computational Natural Language Learning
  (CoNLL97), ACL-EACL'97, Madrid, Spain</comments><abstract>  This paper presents a new view of Explanation-Based Learning (EBL) of natural
language parsing. Rather than employing EBL for specializing parsers by
inferring new ones, this paper suggests employing EBL for learning how to
reduce ambiguity only partially.
  The present method consists of an EBL algorithm for learning partial-parsers,
and a parsing algorithm which combines partial-parsers with existing
``full-parsers&quot;. The learned partial-parsers, implementable as Cascades of
Finite State Transducers (CFSTs), recognize and combine constituents
efficiently, prohibiting spurious overgeneration. The parsing algorithm
combines a learned partial-parser with a given full-parser such that the role
of the full-parser is limited to combining the constituents, recognized by the
partial-parser, and to recognizing unrecognized portions of the input sentence.
Besides the reduction of the parse-space prior to disambiguation, the present
method provides a way for refining existing disambiguation models that learn
stochastic grammars from tree-banks.
  We exhibit encouraging empirical results using a pilot implementation:
parse-space is reduced substantially with minimal loss of coverage. The speedup
gain for disambiguation models is exemplified by experiments with the DOP
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709001</id><created>1997-09-08</created><authors><author><keyname>Neuhaus</keyname><forenames>Peter</forenames><affiliation>University of Freiburg</affiliation></author><author><keyname>Broeker</keyname><forenames>Norbert</forenames><affiliation>University of Freiburg</affiliation></author></authors><title>The Complexity of Recognition of Linguistically Adequate Dependency
  Grammars</title><categories>cmp-lg cs.CL</categories><comments>8 pages, requires LaTeX2e, epsfig, latexsym, amsmath</comments><journal-ref>Proc. ACL-EACL 1997, Madrid, Spain, pp.337-343</journal-ref><abstract>  Results of computational complexity exist for a wide range of phrase
structure-based grammar formalisms, while there is an apparent lack of such
results for dependency-based formalisms. We here adapt a result on the
complexity of ID/LP-grammars to the dependency framework. Contrary to previous
studies on heavily restricted dependency grammars, we prove that recognition
(and thus, parsing) of linguistically adequate dependency grammars is
NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709002</id><created>1997-09-12</created><updated>1997-09-13</updated><authors><author><keyname>Siegel</keyname><forenames>Eric V.</forenames></author></authors><title>Learning Methods for Combining Linguistic Indicators to Classify Verbs</title><categories>cmp-lg cs.CL</categories><comments>7 pages, Latex, in the Proceedings of the Second Conference on
  Empirical Methods in Natural Language Processing, Providence, Rhode Island,
  1997</comments><abstract>  Fourteen linguistically-motivated numerical indicators are evaluated for
their ability to categorize verbs as either states or events. The values for
each indicator are computed automatically across a corpus of text. To improve
classification performance, machine learning techniques are employed to combine
multiple indicators. Three machine learning methods are compared for this task:
decision tree induction, a genetic algorithm, and log-linear regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709003</id><created>1997-09-15</created><updated>1997-09-16</updated><authors><author><keyname>Atserias</keyname><forenames>Jordi</forenames><affiliation>Universitat Politecnica de Catalunya</affiliation></author><author><keyname>Climent</keyname><forenames>Salvador</forenames><affiliation>Universitat de Barcelona</affiliation></author><author><keyname>Farreres</keyname><forenames>Xavier</forenames><affiliation>Universitat Politecnica de Catalunya</affiliation></author><author><keyname>Rigau</keyname><forenames>German</forenames><affiliation>Universitat Politecnica de Catalunya</affiliation></author><author><keyname>Rodriguez</keyname><forenames>Horacio</forenames><affiliation>Universitat Politecnica de Catalunya</affiliation></author></authors><title>Combining Multiple Methods for the Automatic Construction of
  Multilingual WordNets</title><categories>cmp-lg cs.CL</categories><comments>7 pages, 4 postscript figures</comments><journal-ref>RANLP'97 Bulgaria</journal-ref><abstract>  This paper explores the automatic construction of a multilingual Lexical
Knowledge Base from preexisting lexical resources. First, a set of automatic
and complementary techniques for linking Spanish words collected from
monolingual and bilingual MRDs to English WordNet synsets are described.
Second, we show how resulting data provided by each method is then combined to
produce a preliminary version of a Spanish WordNet with an accuracy over 85%.
The application of these combinations results on an increment of the extracted
connexions of a 40% without losing accuracy. Both coarse-grained (class level)
and fine-grained (synset assignment level) confidence ratios are used and
evaluated. Finally, the results for the whole process are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709004</id><created>1997-09-15</created><authors><author><keyname>Hidalgo</keyname><forenames>Jose Maria Gomez</forenames></author><author><keyname>Rodriguez</keyname><forenames>Manuel de Buenaga</forenames></author></authors><title>Integrating a Lexical Database and a Training Collection for Text
  Categorization</title><categories>cmp-lg cs.CL</categories><comments>12 pages, 3 figures (2 tables)</comments><journal-ref>ACL/EACL Workshop on Automatic Extraction and Building of Lexical
  Semantic Resources for Natural Language Applications, 1997</journal-ref><abstract>  Automatic text categorization is a complex and useful task for many natural
language processing applications. Recent approaches to text categorization
focus more on algorithms than on resources involved in this operation. In
contrast to this trend, we present an approach based on the integration of
widely available resources as lexical databases and training collections to
overcome current limitations of the task. Our approach makes use of WordNet
synonymy information to increase evidence for bad trained categories. When
testing a direct categorization, a WordNet based one, a training algorithm, and
our integrated approach, the latter exhibits a better perfomance than any of
the others. Incidentally, WordNet based approach perfomance is comparable with
the training approach one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709005</id><created>1997-09-17</created><authors><author><keyname>Tuells</keyname><forenames>Toni</forenames><affiliation>Universitat Pompeu Fabra</affiliation></author></authors><title>A generation algorithm for f-structure representations</title><categories>cmp-lg cs.CL</categories><comments>6 pages</comments><journal-ref>In the Proceedings of RANLP'97 (pages 270-275), Tzigov Chark,
  Bulgaria, 1997</journal-ref><abstract>  This paper shows that previously reported generation algorithms run into
problems when dealing with f-structure representations. A generation algorithm
that is suitable for this type of representations is presented: the Semantic
Kernel Generation (SKG) algorithm. The SKG method has the same processing
strategy as the Semantic Head Driven generation (SHDG) algorithm and relies on
the assumption that it is possible to compute the Semantic Kernel (SK) and non
Semantic Kernel (Non-SK) information for each input structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709006</id><created>1997-09-17</created><authors><author><keyname>Boros</keyname><forenames>Manuela</forenames><affiliation>FORWISS, Erlangen</affiliation></author><author><keyname>Aretoulaki</keyname><forenames>Maria</forenames><affiliation>Chair for Pattern Recognition, University of Erlangen</affiliation></author><author><keyname>Gallwitz</keyname><forenames>Florian</forenames><affiliation>Chair for Pattern Recognition, University of Erlangen</affiliation></author><author><keyname>Noeth</keyname><forenames>Elmar</forenames><affiliation>Chair for Pattern Recognition, University of Erlangen</affiliation></author><author><keyname>Niemann</keyname><forenames>Heinrich</forenames><affiliation>Chair for Pattern Recognition, University of Erlangen</affiliation></author></authors><title>Semantic Processing of Out-Of-Vocabulary Words in a Spoken Dialogue
  System</title><categories>cmp-lg cs.CL</categories><comments>4 pages, 2 eps figures, requires LaTeX2e, uses eurospeech.sty and
  epsfig</comments><journal-ref>Proceedings of EUROSPEECH'97, Vol.4, pp.1887-1890, Rhodes, Greece</journal-ref><abstract>  One of the most important causes of failure in spoken dialogue systems is
usually neglected: the problem of words that are not covered by the system's
vocabulary (out-of-vocabulary or OOV words). In this paper a methodology is
described for the detection, classification and processing of OOV words in an
automatic train timetable information system. The various extensions that had
to be effected on the different modules of the system are reported, resulting
in the design of appropriate dialogue strategies, as are encouraging evaluation
results on the new versions of the word recogniser and the linguistic
processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709007</id><created>1997-09-17</created><authors><author><keyname>Rodriguez</keyname><forenames>Manuel de Buenaga</forenames></author><author><keyname>Hidalgo</keyname><forenames>Jose Maria Gomez</forenames></author><author><keyname>Agudo</keyname><forenames>Belen Diaz</forenames></author></authors><title>Using WordNet to Complement Training Information in Text Categorization</title><categories>cmp-lg cs.CL</categories><comments>16 pages, 1 figure, 3 tables, previously with RANLP latext style</comments><journal-ref>Second International Conference on Recent Advances in Natural
  Language Processing, 1997</journal-ref><abstract>  Automatic Text Categorization (TC) is a complex and useful task for many
natural language applications, and is usually performed through the use of a
set of manually classified documents, a training collection. We suggest the
utilization of additional resources like lexical databases to increase the
amount of information that TC systems make use of, and thus, to improve their
performance. Our approach integrates WordNet information with two training
approaches through the Vector Space Model. The training approaches we test are
the Rocchio (relevance feedback) and the Widrow-Hoff (machine learning)
algorithms. Results obtained from evaluation show that the integration of
WordNet clearly outperforms training approaches, and that an integrated
technique can effectively address the classification of low frequency
categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709008</id><created>1997-09-20</created><authors><author><keyname>Jiang</keyname><forenames>Jay J.</forenames><affiliation>University of Waterloo</affiliation></author><author><keyname>Conrath</keyname><forenames>David W.</forenames><affiliation>McMaster University</affiliation></author></authors><title>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy</title><categories>cmp-lg cs.CL</categories><comments>15 pages, Postscript only</comments><journal-ref>In the Proceedings of ROCLING X, Taiwan, 1997</journal-ref><abstract>  This paper presents a new approach for measuring semantic similarity/distance
between words and concepts. It combines a lexical taxonomy structure with
corpus statistical information so that the semantic distance between nodes in
the semantic space constructed by the taxonomy can be better quantified with
the computational evidence derived from a distributional analysis of corpus
data. Specifically, the proposed measure is a combined approach that inherits
the edge-based approach of the edge counting scheme, which is then enhanced by
the node-based approach of the information content calculation. When tested on
a common data set of word pair similarity ratings, the proposed approach
outperforms other computational models. It gives the highest correlation value
(r = 0.828) with a benchmark based on human similarity judgements, whereas an
upper bound (r = 0.885) is observed when human subjects replicate the same
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709009</id><created>1997-09-22</created><authors><author><keyname>Lyon</keyname><forenames>Caroline</forenames><affiliation>Computer Science Department</affiliation></author><author><keyname>Brown</keyname><forenames>Stephen</forenames><affiliation>Mathematics Department, University of Hertfordshire, UK</affiliation></author></authors><title>Evaluating Parsing Schemes with Entropy Indicators</title><categories>cmp-lg cs.CL</categories><comments>7 pages. LaTeX format, epsfig used, .sty file included</comments><journal-ref>5th Meeting on Mathematics of Language, MOL5, 1997</journal-ref><abstract>  This paper introduces an objective metric for evaluating a parsing scheme. It
is based on Shannon's original work with letter sequences, which can be
extended to part-of-speech tag sequences. It is shown that this regular
language is an inadequate model for natural language, but a representation is
used that models language slightly higher in the Chomsky hierarchy.
  We show how the entropy of parsed and unparsed sentences can be measured. If
the entropy of the parsed sentence is lower, this indicates that some of the
structure of the language has been captured.
  We apply this entropy indicator to support one particular parsing scheme that
effects a top down segmentation. This approach could be used to decompose the
parsing task into computationally more tractable subtasks. It also lends itself
to the extraction of predicate/argument structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709010</id><created>1997-09-23</created><authors><author><keyname>Hahn</keyname><forenames>Udo</forenames><affiliation>Computational Linguistics Lab, Freiburg University</affiliation></author><author><keyname>Neuhaus</keyname><forenames>Peter</forenames><affiliation>Computational Linguistics Lab, Freiburg University</affiliation></author><author><keyname>Broeker</keyname><forenames>Norbert</forenames><affiliation>Institute for Natural Language Processing, Stuttgart University</affiliation></author></authors><title>Message-Passing Protocols for Real-World Parsing -- An Object-Oriented
  Model and its Preliminary Evaluation</title><categories>cmp-lg cs.CL</categories><comments>12 pages, uses epsfig.sty</comments><journal-ref>Proc. Int'l Workshop on Parsing Technologies, 1997, Boston/MA:
  MIT, pp 101-112</journal-ref><abstract>  We argue for a performance-based design of natural language grammars and
their associated parsers in order to meet the constraints imposed by real-world
NLP. Our approach incorporates declarative and procedural knowledge about
language and language use within an object-oriented specification framework. We
discuss several message-passing protocols for parsing and provide reasons for
sacrificing completeness of the parse in favor of efficiency based on a
preliminary empirical evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709011</id><created>1997-09-23</created><authors><author><keyname>Wintner</keyname><forenames>Shuly</forenames><affiliation>Department of Computer Science, Technion, Israel Institute of Technology, Haifa, Israel</affiliation></author><author><keyname>Francez</keyname><forenames>Nissim</forenames><affiliation>Department of Computer Science, Technion, Israel Institute of Technology, Haifa, Israel</affiliation></author></authors><title>Off-line Parsability and the Well-foundedness of Subsumption</title><categories>cmp-lg cs.CL</categories><comments>19 pages, 1 postscript figure, uses fullname.sty</comments><abstract>  Typed feature structures are used extensively for the specification of
linguistic information in many formalisms. The subsumption relation orders TFSs
by their information content. We prove that subsumption of acyclic TFSs is
well-founded, whereas in the presence of cycles general TFS subsumption is not
well-founded. We show an application of this result for parsing, where the
well-foundedness of subsumption is used to guarantee termination for grammars
that are off-line parsable. We define a new version of off-line parsability
that is less strict than the existing one; thus termination is guaranteed for
parsing with a larger set of grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709012</id><created>1997-09-23</created><authors><author><keyname>Lyon</keyname><forenames>Caroline</forenames><affiliation>Computer Science Department, University of Hertfordshire, UK</affiliation></author><author><keyname>Frank</keyname><forenames>Ray</forenames><affiliation>Computer Science Department, University of Hertfordshire, UK</affiliation></author></authors><title>Using Single Layer Networks for Discrete, Sequential Data: An Example
  from Natural Language Processing</title><categories>cmp-lg cs.CL</categories><comments>28 pages, 9 figures, Latex format, uses epsfig, .styfile included</comments><journal-ref>Neural Computing and Applications 5(4), 1997, 196-214</journal-ref><abstract>  A natural language parser which has been successfully implemented is
described. This is a hybrid system, in which neural networks operate within a
rule based framework. It can be accessed via telnet for users to try on their
own text. (For details, contact the author.) Tested on technical manuals, the
parser finds the subject and head of the subject in over 90% of declarative
sentences.
  The neural processing components belong to the class of Generalized Single
Layer Networks (GSLN). In general, supervised, feed-forward networks need more
than one layer to process data. However, in some cases data can be
pre-processed with a non-linear transformation, and then presented in a
linearly separable form for subsequent processing by a single layer net. Such
networks offer advantages of functional transparency and operational speed.
  For our parser, the initial stage of processing maps linguistic data onto a
higher order representation, which can then be analysed by a single layer
network. This transformation is supported by information theoretic analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709013</id><created>1997-09-23</created><authors><author><keyname>Wintner</keyname><forenames>Shuly</forenames><affiliation>Department of Computer Science, the Technion, Haifa, Israel</affiliation></author></authors><title>An Abstract Machine for Unification Grammars</title><categories>cmp-lg cs.CL</categories><comments>Doctoral Thesis, 96 pages, many postscript figures, uses pstricks,
  pst-node, psfig, fullname and a macros file</comments><abstract>  This work describes the design and implementation of an abstract machine,
Amalia, for the linguistic formalism ALE, which is based on typed feature
structures. This formalism is one of the most widely accepted in computational
linguistics and has been used for designing grammars in various linguistic
theories, most notably HPSG. Amalia is composed of data structures and a set of
instructions, augmented by a compiler from the grammatical formalism to the
abstract instructions, and a (portable) interpreter of the abstract
instructions. The effect of each instruction is defined using a low-level
language that can be executed on ordinary hardware.
  The advantages of the abstract machine approach are twofold. From a
theoretical point of view, the abstract machine gives a well-defined
operational semantics to the grammatical formalism. This ensures that grammars
specified using our system are endowed with well defined meaning. It enables,
for example, to formally verify the correctness of a compiler for HPSG, given
an independent definition. From a practical point of view, Amalia is the first
system that employs a direct compilation scheme for unification grammars that
are based on typed feature structures. The use of amalia results in a much
improved performance over existing systems.
  In order to test the machine on a realistic application, we have developed a
small-scale, HPSG-based grammar for a fragment of the Hebrew language, using
Amalia as the development platform. This is the first application of HPSG to a
Semitic language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709014</id><created>1997-09-24</created><authors><author><keyname>Wintner</keyname><forenames>Shuly</forenames><affiliation>Seminar fuer Sprachwissenschaft, Tuebingen</affiliation></author><author><keyname>Gabrilovich</keyname><forenames>Evgeniy</forenames><affiliation>Laboratory for Computational Linguistics, Technion, Israel</affiliation></author><author><keyname>Francez</keyname><forenames>Nissim</forenames><affiliation>Laboratory for Computational Linguistics, Technion, Israel</affiliation></author></authors><title>Amalia -- A Unified Platform for Parsing and Generation</title><categories>cmp-lg cs.CL</categories><comments>8 pages postscript</comments><journal-ref>Proceedings of Recent Advances in Natural Language Programming
  (RANLP97), Tzigov Chark, Bulgaria, 11-13 September 1997, pp. 135-142</journal-ref><abstract>  Contemporary linguistic theories (in particular, HPSG) are declarative in
nature: they specify constraints on permissible structures, not how such
structures are to be computed. Grammars designed under such theories are,
therefore, suitable for both parsing and generation. However, practical
implementations of such theories don't usually support bidirectional processing
of grammars. We present a grammar development system that includes a compiler
of grammars (for parsing and generation) to abstract machine instructions, and
an interpreter for the abstract machine language. The generation compiler
inverts input grammars (designed for parsing) to a form more suitable for
generation. The compiled grammars are then executed by the interpreter using
one control strategy, regardless of whether the grammar is the original or the
inverted version. We thus obtain a unified, efficient platform for developing
reversible grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9709015</id><created>1997-09-26</created><authors><author><keyname>Yaari</keyname><forenames>Yaakov</forenames><affiliation>Bar Ilan University</affiliation></author></authors><title>Segmentation of Expository Texts by Hierarchical Agglomerative Clustering</title><categories>cmp-lg cs.CL</categories><comments>7 pages, Latex2e, 4 postscript figures</comments><journal-ref>RANLP'97, Bulgaria</journal-ref><abstract>  We propose a method for segmentation of expository texts based on
hierarchical agglomerative clustering. The method uses paragraphs as the basic
segments for identifying hierarchical discourse structure in the text, applying
lexical similarity between them as the proximity test. Linear segmentation can
be induced from the identified structure through application of two simple
rules. However the hierarchy can be used also for intelligent exploration of
the text. The proposed segmentation algorithm is evaluated against an accepted
linear segmentation method and shows comparable results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9710001</id><created>1997-10-10</created><authors><author><keyname>Tzoukermann</keyname><forenames>Evelyne</forenames><affiliation>Bell Labs</affiliation></author><author><keyname>Radev</keyname><forenames>Dragomir R.</forenames><affiliation>Columbia University</affiliation></author></authors><title>Use of Weighted Finite State Transducers in Part of Speech Tagging</title><categories>cmp-lg cs.CL</categories><comments>uses psfig, ipamacs</comments><abstract>  This paper addresses issues in part of speech disambiguation using
finite-state transducers and presents two main contributions to the field. One
of them is the use of finite-state machines for part of speech tagging.
Linguistic and statistical information is represented in terms of weights on
transitions in weighted finite-state transducers. Another contribution is the
successful combination of techniques -- linguistic and statistical -- for word
disambiguation, compounded with the notion of word classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9710002</id><created>1997-10-10</created><authors><author><keyname>Tzoukermann</keyname><forenames>Evelyne</forenames><affiliation>AT&amp;T Bell Labs</affiliation></author><author><keyname>Radev</keyname><forenames>Dragomir R.</forenames><affiliation>Columbia University</affiliation></author><author><keyname>Gale</keyname><forenames>William A.</forenames><affiliation>AT&amp;T Bell Labs</affiliation></author></authors><title>Tagging French Without Lexical Probabilities -- Combining Linguistic
  Knowledge And Statistical Learning</title><categories>cmp-lg cs.CL</categories><comments>uses ypsfig</comments><abstract>  This paper explores morpho-syntactic ambiguities for French to develop a
strategy for part-of-speech disambiguation that a) reflects the complexity of
French as an inflected language, b) optimizes the estimation of probabilities,
c) allows the user flexibility in choosing a tagset. The problem in extracting
lexical probabilities from a limited training corpus is that the statistical
model may not necessarily represent the use of a particular word in a
particular context. In a highly morphologically inflected language, this
argument is particularly serious since a word can be tagged with a large number
of parts of speech. Due to the lack of sufficient training data, we argue
against estimating lexical probabilities to disambiguate parts of speech in
unrestricted texts. Instead, we use the strength of contextual probabilities
along with a feature we call ``genotype'', a set of tags associated with a
word. Using this knowledge, we have built a part-of-speech tagger that combines
linguistic and statistical approaches: contextual information is disambiguated
by linguistic rules and n-gram probabilities on parts of speech only are
estimated in order to disambiguate the remaining ambiguous tags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9710003</id><created>1997-10-14</created><authors><author><keyname>Blache</keyname><forenames>Philippe</forenames></author></authors><title>Disambiguating with Controlled Disjunctions</title><categories>cmp-lg cs.CL</categories><comments>requires LaTeX2e, uses tree-dvips, avm</comments><journal-ref>Proceedings of IWPT'97</journal-ref><abstract>  In this paper, we propose a disambiguating technique called controlled
disjunctions. This extension of the so-called named disjunctions relies on the
relations existing between feature values (covariation, control, etc.). We show
that controlled disjunctions can implement different kind of ambiguities in a
consistent and homogeneous way. We describe the integration of controlled
disjunctions into a HPSG feature structure representation. Finally, we present
a direct implementation by means of delayed evaluation and we develop an
example within the functionnal programming paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9710004</id><created>1997-10-14</created><authors><author><keyname>Hammond</keyname><forenames>Michael</forenames><affiliation>University of Arizona</affiliation></author></authors><title>Parsing syllables: modeling OT computationally</title><categories>cmp-lg cs.CL</categories><comments>22pp, postscript</comments><abstract>  In this paper, I propose to implement syllabification in OT as a parser. I
propose several innovations that result in a finite and small candidate set.
The candidate set problem is handled with several moves: i) MAX and DEP
violations are not hypothesized by the parser, ii) candidates are encoded
locally, and iii) EVAL is applied constraint by constraint.
  The parser I propose is implemented in Prolog. It has a number of desirable
consequences. First, it runs and thus provides an existence proof that
syllabification can be implemented in OT. There are a number of other desirable
consequences as well. First, constraints are implemented as finite-state
transducers. Second, the parser makes several interesting claims about the
phonological properties of so-called nonrecoverable insertions and deletions.
Third, the implementation suggests some particular reformulations of some of
the benchmark constraints in the OT arsenal, e.g. *COMPLEX, PARSE, ONSET, and
NOCODA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9710005</id><created>1997-10-16</created><authors><author><keyname>Merlo</keyname><forenames>Paola</forenames><affiliation>U. of Pennsylvania and University of Geneva</affiliation></author><author><keyname>Crocker</keyname><forenames>Matthew</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>Berthouzoz</keyname><forenames>Cathy</forenames><affiliation>University of Geneva</affiliation></author></authors><title>Attaching Multiple Prepositional Phrases: Generalized Backed-off
  Estimation</title><categories>cmp-lg cs.CL</categories><comments>7 pages, appeared in the EMNLP-2 proceedings</comments><abstract>  There has recently been considerable interest in the use of lexically-based
statistical techniques to resolve prepositional phrase attachments. To our
knowledge, however, these investigations have only considered the problem of
attaching the first PP, i.e., in a [V NP PP] configuration. In this paper, we
consider one technique which has been successfully applied to this problem,
backed-off estimation, and demonstrate how it can be extended to deal with the
problem of multiple PP attachment. The multiple PP attachment introduces two
related problems: sparser data (since multiple PPs are naturally rarer), and
greater syntactic ambiguity (more attachment configurations which must be
distinguished). We present and algorithm which solves this problem through
re-use of the relatively rich data obtained from first PP training, in
resolving subsequent PP attachments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9710006</id><created>1997-10-21</created><authors><author><keyname>Di Eugenio</keyname><forenames>Barbara</forenames><affiliation>University of Pittsburgh</affiliation></author><author><keyname>Moore</keyname><forenames>Johanna D.</forenames><affiliation>University of Pittsburgh</affiliation></author><author><keyname>Paolucci</keyname><forenames>Massimo</forenames><affiliation>University of Pittsburgh</affiliation></author></authors><title>Learning Features that Predict Cue Usage</title><categories>cmp-lg cs.CL</categories><comments>10 pages, 2 Postscript figures, uses aclap.sty, psfig.tex</comments><journal-ref>Proceedings of ACL/EACL97, Madrid, 1997</journal-ref><abstract>  Our goal is to identify the features that predict the occurrence and
placement of discourse cues in tutorial explanations in order to aid in the
automatic generation of explanations. Previous attempts to devise rules for
text generation were based on intuition or small numbers of constructed
examples. We apply a machine learning program, C4.5, to induce decision trees
for cue occurrence and placement from a corpus of data coded for a variety of
features previously thought to affect cue usage. Our experiments enable us to
identify the features with most predictive power, and show that machine
learning can be used to induce decision trees useful for text generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9710007</id><created>1997-10-24</created><authors><author><keyname>Poesio</keyname><forenames>Massimo</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>Vieira</keyname><forenames>Renata</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>A Corpus-Based Investigation of Definite Description Use</title><categories>cmp-lg cs.CL</categories><comments>47 pages, uses fullname.sty and palatino.sty</comments><abstract>  We present the results of a study of definite descriptions use in written
texts aimed at assessing the feasibility of annotating corpora with information
about definite description interpretation. We ran two experiments, in which
subjects were asked to classify the uses of definite descriptions in a corpus
of 33 newspaper articles, containing a total of 1412 definite descriptions. We
measured the agreement among annotators about the classes assigned to definite
descriptions, as well as the agreement about the antecedent assigned to those
definites that the annotators classified as being related to an antecedent in
the text. The most interesting result of this study from a corpus annotation
perspective was the rather low agreement (K=0.63) that we obtained using
versions of Hawkins' and Prince's classification schemes; better results
(K=0.76) were obtained using the simplified scheme proposed by Fraurud that
includes only two classes, first-mention and subsequent-mention. The agreement
about antecedents was also not complete. These findings raise questions
concerning the strategy of evaluating systems for definite description
interpretation by comparing their results with a standardized annotation. From
a linguistic point of view, the most interesting observations were the great
number of discourse-new definites in our corpus (in one of our experiments,
about 50% of the definites in the collection were classified as discourse-new,
30% as anaphoric, and 18% as associative/bridging) and the presence of
definites which did not seem to require a complete disambiguation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9710008</id><created>1997-10-30</created><updated>1997-10-30</updated><authors><author><keyname>Wiebe</keyname><forenames>Janyce</forenames><affiliation>New Mexico State University</affiliation></author><author><keyname>Bruce</keyname><forenames>Rebecca</forenames><affiliation>Southern Methodist University</affiliation></author><author><keyname>Duan</keyname><forenames>Lei</forenames><affiliation>Sony Corporation</affiliation></author></authors><title>Probabilistic Event Categorization</title><categories>cmp-lg cs.CL</categories><journal-ref>Recent Advances in Natural Language Processing (RANLP-97),
  European Commission, DG XIII, Tzigov Chark, Bulgaria, September 1997, pp.
  163--170.</journal-ref><abstract>  This paper describes the automation of a new text categorization task. The
categories assigned in this task are more syntactically, semantically, and
contextually complex than those typically assigned by fully automatic systems
that process unseen test data. Our system for assigning these categories is a
probabilistic classifier, developed with a recent method for formulating a
probabilistic model from a predefined set of potential features. This paper
focuses on feature selection. It presents a number of fully automatic features.
It identifies and evaluates various approaches to organizing collocational
properties into features, and presents the results of experiments covarying
type of organization and type of property. We find that one organization is not
best for all kinds of properties, so this is an experimental parameter worth
investigating in NLP systems. In addition, the results suggest a way to take
advantage of properties that are low frequency but strongly indicative of a
class. The problems of recognizing and organizing the various kinds of
contextual information required to perform a linguistically complex
categorization task have rarely been systematically investigated in NLP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711001</id><created>1997-11-11</created><authors><author><keyname>Riezler</keyname><forenames>Stefan</forenames><affiliation>University of Tuebingen</affiliation></author></authors><title>Probabilistic Constraint Logic Programming</title><categories>cmp-lg cs.CL</categories><comments>35 pages, uses sfbart.cls</comments><report-no>Arbeitspapiere des SFB 340, Bericht Nr. 117, Oktober 1997</report-no><abstract>  This paper addresses two central problems for probabilistic processing
models: parameter estimation from incomplete data and efficient retrieval of
most probable analyses. These questions have been answered satisfactorily only
for probabilistic regular and context-free models. We address these problems
for a more expressive probabilistic constraint logic programming model. We
present a log-linear probability model for probabilistic constraint logic
programming. On top of this model we define an algorithm to estimate the
parameters and to select the properties of log-linear models from incomplete
data. This algorithm is an extension of the improved iterative scaling
algorithm of Della-Pietra, Della-Pietra, and Lafferty (1995). Our algorithm
applies to log-linear models in general and is accompanied with suitable
approximation methods when applied to large data spaces. Furthermore, we
present an approach for searching for most probable analyses of the
probabilistic constraint logic programming model. This method can be applied to
the ambiguity resolution problem in natural language processing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711002</id><created>1997-11-11</created><authors><author><keyname>Grimley-Evans</keyname><forenames>Edmund</forenames></author></authors><title>Approximating Context-Free Grammars with a Finite-State Calculus</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX, 2 PostScript figures, aclap.sty</comments><journal-ref>Proceedings of ACL-EACL 97, Madrid, pp 452-459, 1997.</journal-ref><abstract>  Although adequate models of human language for syntactic analysis and
semantic interpretation are of at least context-free complexity, for
applications such as speech processing in which speed is important finite-state
models are often preferred. These requirements may be reconciled by using the
more complex grammar to automatically derive a finite-state approximation which
can then be used as a filter to guide speech recognition or to reject many
hypotheses at an early stage of processing. A method is presented here for
calculating such finite-state approximations from context-free grammars. It is
essentially different from the algorithm introduced by Pereira and Wright
(1991; 1996), is faster in some cases, and has the advantage of being
open-ended and adaptable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711003</id><created>1997-11-16</created><authors><author><keyname>Manning</keyname><forenames>Christopher D.</forenames><affiliation>University of Sydney</affiliation></author><author><keyname>Carpenter</keyname><forenames>Bob</forenames><affiliation>Lucent Technologies Bell Labs</affiliation></author></authors><title>Probabilistic Parsing Using Left Corner Language Models</title><categories>cmp-lg cs.CL</categories><comments>12 pages, uses iwpt97.sty</comments><journal-ref>Proceedings of the Fifth International Workshop on Parsing
  Technologies, MIT, Boston MA, 1997</journal-ref><abstract>  We introduce a novel parser based on a probabilistic version of a left-corner
parser. The left-corner strategy is attractive because rule probabilities can
be conditioned on both top-down goals and bottom-up derivations. We develop the
underlying theory and explain how a grammar can be induced from analyzed data.
We show that the left-corner approach provides an advantage over simple
top-down probabilistic context-free grammars in parsing the Wall Street Journal
using a grammar induced from the Penn Treebank. We also conclude that the Penn
Treebank provides a fairly weak testbed due to the flatness of its bracketings
and to the obvious overgeneration and undergeneration of its induced grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711004</id><created>1997-11-17</created><authors><author><keyname>Miller</keyname><forenames>Corey</forenames></author><author><keyname>Karaali</keyname><forenames>Orhan</forenames></author><author><keyname>Massey</keyname><forenames>Noel</forenames></author></authors><title>Variation and Synthetic Speech</title><categories>cmp-lg cs.CL</categories><comments>18 pages, 2 figures</comments><report-no>Motorola-SSML-1</report-no><abstract>  We describe the approach to linguistic variation taken by the Motorola speech
synthesizer. A pan-dialectal pronunciation dictionary is described, which
serves as the training data for a neural network based letter-to-sound
converter. Subsequent to dictionary retrieval or letter-to-sound generation,
pronunciations are submitted a neural network based postlexical module. The
postlexical module has been trained on aligned dictionary pronunciations and
hand-labeled narrow phonetic transcriptions. This architecture permits the
learning of individual postlexical variation, and can be retrained for each
speaker whose voice is being modeled for synthesis. Learning variation in this
way can result in greater naturalness for the synthetic speech that is produced
by the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711005</id><created>1997-11-19</created><authors><author><keyname>Bateman</keyname><forenames>John A.</forenames><affiliation>Language and Communication Research Centre, Dept. of English Studies, University of Stirling, Scotland</affiliation></author></authors><title>Some apparently disjoint aims and requirements for grammar development
  environments: the case of natural language generation</title><categories>cmp-lg cs.CL</categories><comments>9 pages, EPS figures, uses: aclap.sty, psfig.sty Paper presented at
  the ACL/EACL'97 Madrid Workshop on Computational Environments for Grammar
  Development and Linguistic Engineering</comments><abstract>  Grammar development environments (GDE's) for analysis and for generation have
not yet come together. Despite the fact that analysis-oriented GDE's (such as
ALEP) may include some possibility of sentence generation, the development
techniques and kinds of resources suggested are apparently not those required
for practical, large-scale natural language generation work. Indeed, there is
no use of `standard' (i.e., analysis-oriented) GDE's in current
projects/applications targetting the generation of fluent, coherent texts. This
unsatisfactory situation requires some analysis and explanation, which this
paper attempts using as an example an extensive GDE for generation. The support
provided for distributed large-scale grammar development, multilinguality, and
resource maintenance are discussed and contrasted with analysis-oriented
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711006</id><created>1997-11-19</created><authors><author><keyname>Baggia</keyname><forenames>Paolo</forenames><affiliation>CSELT - Turin, Italy</affiliation></author><author><keyname>Danieli</keyname><forenames>Morena</forenames><affiliation>CSELT - Turin, Italy</affiliation></author><author><keyname>Gerbino</keyname><forenames>Elisabetta</forenames><affiliation>CSELT - Turin, Italy</affiliation></author><author><keyname>Moisa</keyname><forenames>Loreta M.</forenames><affiliation>CSELT - Turin, Italy</affiliation></author><author><keyname>Popovici</keyname><forenames>Cosmin</forenames><affiliation>CSELT - Turin, Italy</affiliation></author></authors><title>Contextual Information and Specific Language Models for Spoken Language
  Understanding</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Latex, uses aclap.sty</comments><journal-ref>Proceedings of SPECOM'97, Cluj-Napoca, Romania, pp. 51-56</journal-ref><abstract>  In this paper we explain how contextual expectations are generated and used
in the task-oriented spoken language understanding system Dialogos. The hard
task of recognizing spontaneous speech on the telephone may greatly benefit
from the use of specific language models during the recognition of callers'
utterances. By 'specific language models' we mean a set of language models that
are trained on contextually appropriated data, and that are used during
different states of the dialogue on the basis of the information sent to the
acoustic level by the dialogue management module. In this paper we describe how
the specific language models are obtained on the basis of contextual
information. The experimental result we report show that recognition and
understanding performance are improved thanks to the use of specific language
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711007</id><created>1997-11-19</created><authors><author><keyname>Popovici</keyname><forenames>Cosmin</forenames><affiliation>ICI - Bucuresti, Romania</affiliation></author><author><keyname>Baggia</keyname><forenames>Paolo</forenames><affiliation>CSELT - Turin, Italy</affiliation></author></authors><title>Language Modelling For Task-Oriented Domains</title><categories>cmp-lg cs.CL</categories><comments>5 pages, LaTeX, 4 eps figures, uses icassp91.sty, and epsf.tex</comments><journal-ref>Proceedings of EUROSPEECH'97, Rhodes, Greece, vol. 3, pp.
  1459-1462</journal-ref><abstract>  This paper is focused on the language modelling for task-oriented domains and
presents an accurate analysis of the utterances acquired by the Dialogos spoken
dialogue system. Dialogos allows access to the Italian Railways timetable by
using the telephone over the public network. The language modelling aspects of
specificity and behaviour to rare events are studied. A technique for getting a
language model more robust, based on sentences generated by grammars, is
presented. Experimental results show the benefit of the proposed technique. The
increment of performance between language models created using grammars and
usual ones, is higher when the amount of training material is limited.
Therefore this technique can give an advantage especially for the development
of language models in a new domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711008</id><created>1997-11-19</created><authors><author><keyname>Danieli</keyname><forenames>Morena</forenames><affiliation>CSELT - Turin, Italy</affiliation></author></authors><title>On the use of expectations for detecting and repairing human-machine
  miscommunication</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX, uses aaai.sty</comments><journal-ref>Proceedings of AAAI-96 Workshop on Detecting, Preventing, and
  Repairing Human-Machine Miscommunications, Portland, OR, pp. 87-93</journal-ref><abstract>  In this paper I describe how miscommunication problems are dealt with in the
spoken language system DIALOGOS. The dialogue module of the system exploits
dialogic expectations in a twofold way: to model what future user utterance
might be about (predictions), and to account how the user's next utterance may
be related to previous ones in the ongoing interaction (pragmatic-based
expectations). The analysis starts from the hypothesis that the occurrence of
miscommunication is concomitant with two pragmatic phenomena: the deviation of
the user from the expected behaviour and the generation of a conversational
implicature. A preliminary evaluation of a large amount of interactions between
subjects and DIALOGOS shows that the system performance is enhanced by the uses
of both predictions and pragmatic-based expectations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711009</id><created>1997-11-19</created><authors><author><keyname>Ueberla</keyname><forenames>Joerg P.</forenames><affiliation>Speech Machines - DRA Malvern</affiliation></author></authors><title>Towards an Improved Performance Measure for Language Models</title><categories>cmp-lg cs.CL</categories><abstract>  In this paper a first attempt at deriving an improved performance measure for
language models, the probability ratio measure (PRM) is described. In a proof
of concept experiment, it is shown that PRM correlates better with recognition
accuracy and can lead to better recognition results when used as the
optimisation criterion of a clustering algorithm. Inspite of the approximations
and limitations of this preliminary work, the results are very encouraging and
should justify more work along the same lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711010</id><created>1997-11-19</created><authors><author><keyname>Henschel</keyname><forenames>Renate</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh</affiliation></author><author><keyname>Bateman</keyname><forenames>John A.</forenames><affiliation>Language and Communication Research Centre, Dept. of English Studies, University of Stirling</affiliation></author></authors><title>Application-driven automatic subgrammar extraction</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses: aclap.sty, epic.sty, put-inserts Paper presented at
  the ACL/EACL'97 Madrid Workshop on Computational Environments for Grammar
  Development and Linguistic Engineering</comments><abstract>  The space and run-time requirements of broad coverage grammars appear for
many applications unreasonably large in relation to the relative simplicity of
the task at hand. On the other hand, handcrafted development of
application-dependent grammars is in danger of duplicating work which is then
difficult to re-use in other contexts of application. To overcome this problem,
we present in this paper a procedure for the automatic extraction of
application-tuned consistent subgrammars from proved large-scale generation
grammars. The procedure has been implemented for large-scale systemic grammars
and builds on the formal equivalence between systemic grammars and typed
unification based grammars. Its evaluation for the generation of encyclopedia
entries is described, and directions of future development, applicability, and
extensions are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711011</id><created>1997-11-20</created><updated>1997-11-21</updated><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames><affiliation>Brown University</affiliation></author></authors><title>The effect of alternative tree representations on tree bank grammars</title><categories>cmp-lg cs.CL</categories><comments>Adds missing bibliography (sorry!), 11 pages, to appear in
  Proceedings of NeMLAP 3, uses epic.sty and eepic.sty</comments><abstract>  The performance of PCFGs estimated from tree banks is sensitive to the
particular way in which linguistic constructions are represented as trees in
the tree bank. This paper presents a theoretical analysis of the effect of
different tree representations for PP attachment on PCFG models, and introduces
a new methodology for empirically examining such effects using tree
transformations. It shows that one transformation, which copies the label of a
parent node onto the labels of its children, can improve the performance of a
PCFG model in terms of labelled precision and recall on held out data from 73%
(precision) and 69% (recall) to 80% and 79% respectively. It also points out
that if only maximum likelihood parses are of interest then many productions
can be ignored, since they are subsumed by combinations of other productions in
the grammar. In the Penn II tree bank grammar, almost 9% of productions are
subsumed in this way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711012</id><created>1997-11-20</created><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames><affiliation>Brown University</affiliation></author></authors><title>Proof Nets and the Complexity of Processing Center-Embedded
  Constructions</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proceedings of LACL 95; uses epic.sty, eepic.sty,
  rotate.sty</comments><abstract>  This paper shows how proof nets can be used to formalize the notion of
``incomplete dependency'' used in psycholinguistic theories of the
unacceptability of center-embedded constructions. Such theories of human
language processing can usually be restated in terms of geometrical constraints
on proof nets. The paper ends with a discussion of the relationship between
these constraints and incremental semantic interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711013</id><created>1997-11-20</created><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames><affiliation>Brown University</affiliation></author></authors><title>Features as Resources in R-LFG</title><categories>cmp-lg cs.CL</categories><comments>21 pages; appears in Proceedings of LFG-97; uses epic.sty, eepic.sty,
  ipa.sty</comments><abstract>  This paper introduces a non-unification-based version of LFG called R-LFG
(Resource-based Lexical Functional Grammar), which combines elements from both
LFG and Linear Logic. The paper argues that a resource sensitive account
provides a simpler treatment of many linguistic uses of non-monotonic devices
in LFG, such as existential constraints and constraint equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9711014</id><created>1997-11-21</created><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames><affiliation>Brown University</affiliation></author></authors><title>Type-driven semantic interpretation and feature dependencies in R-LFG</title><categories>cmp-lg cs.CL</categories><comments>30 pages, to appear in the the ``Glue Language'' volume edited by
  Dalrymple, uses tree-dvips, ipa, epic, eepic, fullname</comments><abstract>  Once one has enriched LFG's formal machinery with the linear logic mechanisms
needed for semantic interpretation as proposed by Dalrymple et. al., it is
natural to ask whether these make any existing components of LFG redundant. As
Dalrymple and her colleagues note, LFG's f-structure completeness and coherence
constraints fall out as a by-product of the linear logic machinery they propose
for semantic interpretation, thus making those f-structure mechanisms
redundant. Given that linear logic machinery or something like it is
independently needed for semantic interpretation, it seems reasonable to
explore the extent to which it is capable of handling feature structure
constraints as well.
  R-LFG represents the extreme position that all linguistically required
feature structure dependencies can be captured by the resource-accounting
machinery of a linear or similiar logic independently needed for semantic
interpretation, making LFG's unification machinery redundant. The goal is to
show that LFG linguistic analyses can be expressed as clearly and perspicuously
using the smaller set of mechanisms of R-LFG as they can using the much larger
set of unification-based mechanisms in LFG: if this is the case then we will
have shown that positing these extra f-structure mechanisms is not
linguistically warranted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712001</id><created>1997-12-08</created><authors><author><keyname>Neumann</keyname><forenames>Guenter</forenames></author></authors><title>Applying Explanation-based Learning to Control and Speeding-up Natural
  Language Generation</title><categories>cmp-lg cs.CL</categories><comments>9 pages; in Proc. of ACL-EACL 1997, Madrid, Spain, pp. 214-221</comments><abstract>  This paper presents a method for the automatic extraction of subgrammars to
control and speeding-up natural language generation NLG. The method is based on
explanation-based learning (EBL). The main advantage for the proposed new
method for NLG is that the complexity of the grammatical decision making
process during NLG can be vastly reduced, because the EBL method supports the
adaption of a NLG system to a particular use of a language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712002</id><created>1997-12-09</created><updated>1997-12-11</updated><authors><author><keyname>Bloedorn</keyname><forenames>Eric</forenames><affiliation>MITRE Corporation and George Mason University</affiliation></author><author><keyname>Mani</keyname><forenames>Inderjeet</forenames><affiliation>MITRE Corporation</affiliation></author><author><keyname>MacMillan</keyname><forenames>T. Richard</forenames><affiliation>MITRE Corporation</affiliation></author></authors><title>Machine Learning of User Profiles: Representational Issues</title><categories>cmp-lg cs.CL cs.LG</categories><comments>6 pages</comments><abstract>  As more information becomes available electronically, tools for finding
information of interest to users becomes increasingly important. The goal of
the research described here is to build a system for generating comprehensible
user profiles that accurately capture user interest with minimum user
interaction. The research described here focuses on the importance of a
suitable generalization hierarchy and representation for learning profiles
which are predictively accurate and comprehensible. In our experiments we
evaluated both traditional features based on weighted term vectors as well as
subject features corresponding to categories which could be drawn from a
thesaurus. Our experiments, conducted in the context of a content-based
profiling system for on-line newspapers on the World Wide Web (the IDD News
Browser), demonstrate the importance of a generalization hierarchy and the
promise of combining natural language processing techniques with machine
learning (ML) to address an information retrieval (IR) problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712003</id><created>1997-12-09</created><authors><author><keyname>Hirst</keyname><forenames>Graeme</forenames><affiliation>University of Toronto</affiliation></author></authors><title>Context as a Spurious Concept</title><categories>cmp-lg cs.CL</categories><abstract>  I take issue with AI formalizations of context, primarily the formalization
by McCarthy and Buvac, that regard context as an undefined primitive whose
formalization can be the same in many different kinds of AI tasks. In
particular, any theory of context in natural language must take the special
nature of natural language into account and cannot regard context simply as an
undefined primitive. I show that there is no such thing as a coherent theory of
context simpliciter -- context pure and simple -- and that context in natural
language is not the same kind of thing as context in KR. In natural language,
context is constructed by the speaker and the interpreter, and both have
considerable discretion in so doing. Therefore, a formalization based on
pre-defined contexts and pre-defined `lifting axioms' cannot account for how
context is used in real-world language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712004</id><created>1997-12-10</created><authors><author><keyname>Mani</keyname><forenames>Inderjeet</forenames><affiliation>MITRE Corporation</affiliation></author><author><keyname>Bloedorn</keyname><forenames>Eric</forenames><affiliation>MITRE Corporation</affiliation></author></authors><title>Multi-document Summarization by Graph Search and Matching</title><categories>cmp-lg cs.CL</categories><comments>7 pages, 5 Postscript figures, uses aaai.sty</comments><abstract>  We describe a new method for summarizing similarities and differences in a
pair of related documents using a graph representation for text. Concepts
denoted by words, phrases, and proper names in the document are represented
positionally as nodes in the graph along with edges corresponding to semantic
relations between items. Given a perspective in terms of which the pair of
documents is to be summarized, the algorithm first uses a spreading activation
technique to discover, in each document, nodes semantically related to the
topic. The activated graphs of each document are then matched to yield a graph
corresponding to similarities and differences between the pair, which is
rendered in natural language. An evaluation of these techniques has been
carried out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712005</id><created>1997-12-12</created><authors><author><keyname>Niwa</keyname><forenames>Yoshiki</forenames><affiliation>Advanced Research Laboratory, Hitachi, Ltd.</affiliation></author><author><keyname>Nishioka</keyname><forenames>Shingo</forenames><affiliation>Advanced Research Laboratory, Hitachi, Ltd.</affiliation></author><author><keyname>Iwayama</keyname><forenames>Makoto</forenames><affiliation>Advanced Research Laboratory, Hitachi, Ltd.</affiliation></author><author><keyname>Takano</keyname><forenames>Akihiko</forenames><affiliation>Advanced Research Laboratory, Hitachi, Ltd.</affiliation></author><author><keyname>Nitta</keyname><forenames>Yoshihiko</forenames><affiliation>Dept. of Economics, Nihon University</affiliation></author></authors><title>Topic Graph Generation for Query Navigation: Use of Frequency Classes
  for Topic Extraction</title><categories>cmp-lg cs.CL</categories><comments>6 pages, 3 figures</comments><journal-ref>Proceedings of NLPRS'97, Natural Language Processing Pacific Rim
  Symposium '97, pages 95-100, Phuket, Thailand, Dec. 1997</journal-ref><abstract>  To make an interactive guidance mechanism for document retrieval systems, we
developed a user-interface which presents users the visualized map of topics at
each stage of retrieval process. Topic words are automatically extracted by
frequency analysis and the strength of the relationships between topic words is
measured by their co-occurrence. A major factor affecting a user's impression
of a given topic word graph is the balance between common topic words and
specific topic words. By using frequency classes for topic word extraction, we
made it possible to select well-balanced set of topic words, and to adjust the
balance of common and specific topic words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712006</id><created>1997-12-23</created><authors><author><keyname>Kilgarriff</keyname><forenames>Adam</forenames><affiliation>ITRI, University of Brighton</affiliation></author></authors><title>&quot;I don't believe in word senses&quot;</title><categories>cmp-lg cs.CL</categories><comments>25 pages</comments><abstract>  Word sense disambiguation assumes word senses. Within the lexicography and
linguistics literature, they are known to be very slippery entities. The paper
looks at problems with existing accounts of `word sense' and describes the
various kinds of ways in which a word's meaning can deviate from its core
meaning. An analysis is presented in which word senses are abstractions from
clusters of corpus citations, in accordance with current lexicographic
practice. The corpus citations, not the word senses, are the basic objects in
the ontology. The corpus citations will be clustered into senses according to
the purposes of whoever or whatever does the clustering. In the absence of such
purposes, word senses do not exist.
  Word sense disambiguation also needs a set of word senses to disambiguate
between. In most recent work, the set has been taken from a general-purpose
lexical resource, with the assumption that the lexical resource describes the
word senses of English/French/..., between which NLP applications will need to
disambiguate. The implication of the paper is, by contrast, that word senses
exist only relative to a task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712007</id><created>1997-12-23</created><authors><author><keyname>Kilgarriff</keyname><forenames>Adam</forenames><affiliation>ITRI, University of Brighton</affiliation></author></authors><title>Foreground and Background Lexicons and Word Sense Disambiguation for
  Information Extraction</title><categories>cmp-lg cs.CL</categories><comments>12 pages</comments><report-no>ITRI-97-04</report-no><journal-ref>Proc. International Workshop on Lexically Driven Information
  Extraction. Frascati, Italy. July 1997. Pp 51--62.</journal-ref><abstract>  Lexicon acquisition from machine-readable dictionaries and corpora is
currently a dynamic field of research, yet it is often not clear how lexical
information so acquired can be used, or how it relates to structured meaning
representations. In this paper I look at this issue in relation to Information
Extraction (hereafter IE), and one subtask for which both lexical and general
knowledge are required, Word Sense Disambiguation (WSD). The analysis is based
on the widely-used, but little-discussed distinction between an IE system's
foreground lexicon, containing the domain's key terms which map onto the
database fields of the output formalism, and the background lexicon, containing
the remainder of the vocabulary. For the foreground lexicon, human lexicography
is required. For the background lexicon, automatic acquisition is appropriate.
  For the foreground lexicon, WSD will occur as a by-product of finding a
coherent semantic interpretation of the input. WSD techniques as discussed in
recent literature are suited only to the background lexicon. Once the
foreground/background distinction is developed, there is a match between what
is possible, given the state of the art in WSD, and what is required, for
high-quality IE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712008</id><created>1997-12-23</created><authors><author><keyname>Kilgarriff</keyname><forenames>Adam</forenames><affiliation>ITRI, University of Brighton</affiliation></author></authors><title>What is word sense disambiguation good for?</title><categories>cmp-lg cs.CL</categories><comments>6 pages</comments><report-no>ITRI-97-08</report-no><journal-ref>Proc. Natural Language Processing Pacific Rim Symposium. Phuket,
  Thailand. December 1997. Pp 209--214.</journal-ref><abstract>  Word sense disambiguation has developed as a sub-area of natural language
processing, as if, like parsing, it was a well-defined task which was a
pre-requisite to a wide range of language-understanding applications. First, I
review earlier work which shows that a set of senses for a word is only ever
defined relative to a particular human purpose, and that a view of word senses
as part of the linguistic furniture lacks theoretical underpinnings. Then, I
investigate whether and how word sense ambiguity is in fact a problem for
different varieties of NLP application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712009</id><created>1997-12-23</created><authors><author><keyname>Heeman</keyname><forenames>Peter A.</forenames><affiliation>University of Rochester</affiliation></author></authors><title>Speech Repairs, Intonational Boundaries and Discourse Markers: Modeling
  Speakers' Utterances in Spoken Dialog</title><categories>cmp-lg cs.CL</categories><comments>280 pages, doctoral dissertation (latex with postscript figures)</comments><abstract>  In this thesis, we present a statistical language model for resolving speech
repairs, intonational boundaries and discourse markers. Rather than finding the
best word interpretation for an acoustic signal, we redefine the speech
recognition problem to so that it also identifies the POS tags, discourse
markers, speech repairs and intonational phrase endings (a major cue in
determining utterance units). Adding these extra elements to the speech
recognition problem actually allows it to better predict the words involved,
since we are able to make use of the predictions of boundary tones, discourse
markers and speech repairs to better account for what word will occur next.
Furthermore, we can take advantage of acoustic information, such as silence
information, which tends to co-occur with speech repairs and intonational
phrase endings, that current language models can only regard as noise in the
acoustic signal. The output of this language model is a much fuller account of
the speaker's turn, with part-of-speech assigned to each word, intonation
phrase endings and discourse markers identified, and speech repairs detected
and corrected. In fact, the identification of the intonational phrase endings,
discourse markers, and resolution of the speech repairs allows the speech
recognizer to model the speaker's utterances, rather than simply the words
involved, and thus it can return a more meaningful analysis of the speaker's
turn for later processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712010</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9712010</id><created>1997-12-24</created><authors><author><keyname>Orsucci</keyname><forenames>F.</forenames></author><author><keyname>Walter</keyname><forenames>K.</forenames></author><author><keyname>Giuliani</keyname><forenames>A.</forenames></author><author><keyname>Webber,</keyname><forenames>C. L.</forenames><suffix>Jr.</suffix></author><author><keyname>Zbilut</keyname><forenames>J. P.</forenames></author></authors><title>Orthographic Structuring of Human Speech and Texts: Linguistic
  Application of Recurrence Quantification Analysis</title><categories>cmp-lg cs.CL</categories><comments>8 pages, 7 figures, submitted to Intl. J. Chaos Theory Applications</comments><abstract>  A methodology based upon recurrence quantification analysis is proposed for
the study of orthographic structure of written texts. Five different
orthographic data sets (20th century Italian poems, 20th century American
poems, contemporary Swedish poems with their corresponding Italian
translations, Italian speech samples, and American speech samples) were
subjected to recurrence quantification analysis, a procedure which has been
found to be diagnostically useful in the quantitative assessment of ordered
series in fields such as physics, molecular dynamics, physiology, and general
signal processing. Recurrence quantification was developed from recurrence
plots as applied to the analysis of nonlinear, complex systems in the physical
sciences, and is based on the computation of a distance matrix of the elements
of an ordered series (in this case the letters consituting selected speech and
poetic texts). From a strictly mathematical view, the results show the
possibility of demonstrating invariance between different language exemplars
despite the apparent low-level of coding (orthography). Comparison with the
actual texts confirms the ability of the method to reveal recurrent structures,
and their complexity. Using poems as a reference standard for judging speech
complexity, the technique exhibits language independence, order dependence and
freedom from pure statistical characteristics of studied sequences, as well as
consistency with easily identifiable texts. Such studies may provide
phenomenological markers of hidden structure as coded by the purely
orthographic level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9801001</id><created>1998-01-14</created><updated>1998-01-20</updated><authors><author><keyname>Ristad</keyname><forenames>Eric Sven</forenames></author><author><keyname>Thomas</keyname><forenames>Robert G.</forenames></author></authors><title>Hierarchical Non-Emitting Markov Models</title><categories>cmp-lg cs.CL</categories><comments>http://www.cs.princeton.edu/~ristad/papers/pu-544-97.ps.gz</comments><report-no>CS-TR-544-97</report-no><abstract>  We describe a simple variant of the interpolated Markov model with
non-emitting state transitions and prove that it is strictly more powerful than
any Markov model. More importantly, the non-emitting model outperforms the
classic interpolated model on the natural language texts under a wide range of
experimental conditions, with only a modest increase in computational
requirements. The non-emitting model is also much less prone to overfitting.
  Keywords: Markov model, interpolated Markov model, hidden Markov model,
mixture modeling, non-emitting state transitions, state-conditional
interpolation, statistical language model, discrete time series, Brown corpus,
Wall Street Journal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9801002</id><created>1998-01-16</created><authors><author><keyname>Heeman</keyname><forenames>Peter A.</forenames><affiliation>Oregon Graduate Institute</affiliation></author><author><keyname>Byron</keyname><forenames>Donna</forenames><affiliation>U. of Rochester</affiliation></author><author><keyname>Allen</keyname><forenames>James F.</forenames><affiliation>U. of Rochester</affiliation></author></authors><title>Identifying Discourse Markers in Spoken Dialog</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses psfig</comments><journal-ref>AAAI 1998 Spring Symposium on Applying Machine Learning to
  Discourse Processing</journal-ref><abstract>  In this paper, we present a method for identifying discourse marker usage in
spontaneous speech based on machine learning. Discourse markers are denoted by
special POS tags, and thus the process of POS tagging can be used to identify
discourse markers. By incorporating POS tagging into language modeling,
discourse markers can be identified during speech recognition, in which the
timeliness of the information can be used to help predict the following words.
We contrast this approach with an alternative machine learning approach
proposed by Litman (1996). This paper also argues that discourse markers can be
used to help the hearer predict the role that the upcoming utterance plays in
the dialog. Thus discourse markers should provide valuable evidence for
automatic dialog act prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9801003</id><created>1998-01-26</created><authors><author><keyname>Bosch</keyname><forenames>Antal van den</forenames><affiliation>ILK / Computational Linguistics, Tilburg University</affiliation></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames><affiliation>ILK / Computational Linguistics, Tilburg University</affiliation></author></authors><title>Do not forget: Full memory in memory-based learning of word
  pronunciation</title><categories>cmp-lg cs.CL</categories><comments>uses conll98, epsf, and ipamacs (WSU IPA)</comments><journal-ref>Proceedings of NeMLaP3/CoNLL98, 195-204</journal-ref><abstract>  Memory-based learning, keeping full memory of learning material, appears a
viable approach to learning NLP tasks, and is often superior in generalisation
accuracy to eager learning approaches that abstract from learning material.
Here we investigate three partial memory-based learning approaches which remove
from memory specific task instance types estimated to be exceptional. The three
approaches each implement one heuristic function for estimating exceptionality
of instance types: (i) typicality, (ii) class prediction strength, and (iii)
friendly-neighbourhood size. Experiments are performed with the memory-based
learning algorithm IB1-IG trained on English word pronunciation. We find that
removing instance types with low prediction strength (ii) is the only tested
method which does not seriously harm generalisation accuracy. We conclude that
keeping full memory of types rather than tokens, and excluding minority
ambiguities appear to be the only performance-preserving optimisations of
memory-based learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9801004</id><created>1998-01-26</created><authors><author><keyname>Bosch</keyname><forenames>Antal van den</forenames><affiliation>ILK / Computational Linguistics, Tilburg University</affiliation></author><author><keyname>Weijters</keyname><forenames>Ton</forenames><affiliation>Dept. of Information Technology, Eindhoven University of Technology</affiliation></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames><affiliation>ILK / Computational Linguistics, Tilburg University</affiliation></author></authors><title>Modularity in inductively-learned word pronunciation systems</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uses nemlap3.sty and epsf and ipamacs (WSU IPA) macros</comments><journal-ref>Proceedings of NeMLaP3/CoNLL98, 185-194</journal-ref><abstract>  In leading morpho-phonological theories and state-of-the-art text-to-speech
systems it is assumed that word pronunciation cannot be learned or performed
without in-between analyses at several abstraction levels (e.g., morphological,
graphemic, phonemic, syllabic, and stress levels). We challenge this assumption
for the case of English word pronunciation. Using IGTree, an inductive-learning
decision-tree algorithms, we train and test three word-pronunciation systems in
which the number of abstraction levels (implemented as sequenced modules) is
reduced from five, via three, to one. The latter system, classifying letter
strings directly as mapping to phonemes with stress markers, yields
significantly better generalisation accuracies than the two multi-module
systems. Analyses of empirical results indicate that positive utility effects
of sequencing modules are outweighed by cascading errors passed on between
modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9801005</id><created>1998-01-26</created><authors><author><keyname>Quesada</keyname><forenames>Jose F.</forenames></author></authors><title>A General, Sound and Efficient Natural Language Parsing Algorithm based
  on Syntactic Constraints Propagation</title><categories>cmp-lg cs.CL</categories><comments>12 pages, 4 Postscript figures, uses epsfig</comments><journal-ref>Proceedings CAEPIA'97, Malaga, Spain. pp. 775-786</journal-ref><abstract>  This paper presents a new context-free parsing algorithm based on a
bidirectional strictly horizontal strategy which incorporates strong top-down
predictions (derivations and adjacencies). From a functional point of view, the
parser is able to propagate syntactic constraints reducing parsing ambiguity.
  From a computational perspective, the algorithm includes different techniques
aimed at the improvement of the manipulation and representation of the
structures used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9802001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9802001</id><created>1998-02-02</created><authors><author><keyname>Kempe</keyname><forenames>Andre</forenames><affiliation>Xerox Research Centre Europe, Grenoble Laboratory, France</affiliation></author></authors><title>Look-Back and Look-Ahead in the Conversion of Hidden Markov Models into
  Finite State Transducers</title><categories>cmp-lg cs.CL</categories><comments>9 pages, A4, LaTeX (+4x eps) gzip tar gzip uuencode</comments><journal-ref>NeMLaP3/CoNLL'98, pp.29-37, Sydney, Australia. January 15-17, 1998</journal-ref><abstract>  This paper describes the conversion of a Hidden Markov Model into a finite
state transducer that closely approximates the behavior of the stochastic
model. In some cases the transducer is equivalent to the HMM. This conversion
is especially advantageous for part-of-speech tagging because the resulting
transducer can be composed with other transducers that encode correction rules
for the most frequent tagging errors. The speed of tagging is also improved.
The described methods have been implemented and successfully tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9802002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9802002</id><created>1998-02-11</created><authors><author><keyname>Padro</keyname><forenames>Lluis</forenames></author></authors><title>A Hybrid Environment for Syntax-Semantic Tagging</title><categories>cmp-lg cs.CL</categories><comments>PhD Thesis. 120 pages</comments><abstract>  The thesis describes the application of the relaxation labelling algorithm to
NLP disambiguation. Language is modelled through context constraint inspired on
Constraint Grammars. The constraints enable the use of a real value statind
&quot;compatibility&quot;. The technique is applied to POS tagging, Shallow Parsing and
Word Sense Disambigation. Experiments and results are reported. The proposed
approach enables the use of multi-feature constraint models, the simultaneous
resolution of several NL disambiguation tasks, and the collaboration of
linguistic and statistical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9803001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9803001</id><created>1998-03-02</created><authors><author><keyname>Hirschman</keyname><forenames>Lynette</forenames></author><author><keyname>Robinson</keyname><forenames>Patricia</forenames></author><author><keyname>Burger</keyname><forenames>John</forenames></author><author><keyname>Vilain</keyname><forenames>Marc</forenames></author></authors><title>Automating Coreference: The Role of Annotated Training Data</title><categories>cmp-lg cs.CL</categories><comments>4 pages, 5 figures. To appear in the AAAI Spring Symposium on
  Applying Machine Learning to Discourse Processing. The Alembic Workbench
  annotation tool described in this paper is available at
  http://www.mitre.org/resources/centers/advanced_info/g04h/workbench.html</comments><abstract>  We report here on a study of interannotator agreement in the coreference task
as defined by the Message Understanding Conference (MUC-6 and MUC-7). Based on
feedback from annotators, we clarified and simplified the annotation
specification. We then performed an analysis of disagreement among several
annotators, concluding that only 16% of the disagreements represented genuine
disagreement about coreference; the remainder of the cases were mostly
typographical errors or omissions, easily reconciled. Initially, we measured
interannotator agreement in the low 80s for precision and recall. To try to
improve upon this, we ran several experiments. In our final experiment, we
separated the tagging of candidate noun phrases from the linking of actual
coreferring expressions. This method shows promise - interannotator agreement
climbed to the low 90s - but it needs more extensive validation. These results
position the research community to broaden the coreference task to multiple
languages, and possibly to different kinds of coreference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9803002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9803002</id><created>1998-03-22</created><authors><author><keyname>Androutsopoulos</keyname><forenames>I.</forenames><affiliation>Microsoft Research Institute, Macquarie University, Sydney</affiliation></author><author><keyname>Ritchie</keyname><forenames>G. D.</forenames><affiliation>Dept. of Artificial Intelligence, Univ. of Edinburgh</affiliation></author><author><keyname>Thanisch</keyname><forenames>P.</forenames><affiliation>Dept. of Computer Science, Univ. of Edinburgh</affiliation></author></authors><title>Time, Tense and Aspect in Natural Language Database Interfaces</title><categories>cmp-lg cs.CL</categories><comments>50 pages. LaTeX2e. Uses: amstex, a4, a4wide, xspace, avm, examples.
  EPS figures included. To appear in the Journal of Natural Language
  Engineering</comments><journal-ref>Natural Language Engineering, 4(3), pp. 229-276, Sept. 1998,
  Cambridge Univ. Press.</journal-ref><abstract>  Most existing natural language database interfaces (NLDBs) were designed to
be used with database systems that provide very limited facilities for
manipulating time-dependent data, and they do not support adequately temporal
linguistic mechanisms (verb tenses, temporal adverbials, temporal subordinate
clauses, etc.). The database community is becoming increasingly interested in
temporal database systems, that are intended to store and manipulate in a
principled manner information not only about the present, but also about the
past and future. When interfacing to temporal databases, supporting temporal
linguistic mechanisms becomes crucial.
  We present a framework for constructing natural language interfaces for
temporal databases (NLTDBs), that draws on research in tense and aspect
theories, temporal logics, and temporal databases. The framework consists of a
temporal intermediate representation language, called TOP, an HPSG grammar that
maps a wide range of questions involving temporal mechanisms to appropriate TOP
expressions, and a provably correct method for translating from TOP to TSQL2,
TSQL2 being a recently proposed temporal extension of the SQL database
language. This framework was employed to implement a prototype NLTDB using ALE
and Prolog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9803003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9803003</id><created>1998-03-27</created><authors><author><keyname>Bikel</keyname><forenames>Daniel M.</forenames><affiliation>BBN</affiliation></author><author><keyname>Miller</keyname><forenames>Scott</forenames><affiliation>BBN</affiliation></author><author><keyname>Schwartz</keyname><forenames>Richard</forenames><affiliation>BBN</affiliation></author><author><keyname>Weischedel</keyname><forenames>Ralph</forenames><affiliation>BBN</affiliation></author></authors><title>Nymble: a High-Performance Learning Name-finder</title><categories>cmp-lg cs.CL</categories><comments>Postscript only, 8 pages</comments><journal-ref>Proceedings of the Fifth Conference on Applied Natural Language
  Processing, 1997, pp. 194-201</journal-ref><abstract>  This paper presents a statistical, learned approach to finding names and
other non-recursive entities in text (as per the MUC-6 definition of the NE
task), using a variant of the standard hidden Markov model. We present our
justification for the problem and our approach, a detailed discussion of the
model itself and finally the successful results of this new approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9804001</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9804001</id><created>1998-04-02</created><authors><author><keyname>Larcheveque</keyname><forenames>John</forenames><affiliation>INRIA, Rocquencourt, France</affiliation></author></authors><title>Graph Interpolation Grammars: a Rule-based Approach to the Incremental
  Parsing of Natural Languages</title><categories>cmp-lg cs.CL</categories><comments>41 pages, Postscript only</comments><report-no>RR-3390</report-no><abstract>  Graph Interpolation Grammars are a declarative formalism with an operational
semantics. Their goal is to emulate salient features of the human parser, and
notably incrementality. The parsing process defined by GIGs incrementally
builds a syntactic representation of a sentence as each successive lexeme is
read. A GIG rule specifies a set of parse configurations that trigger its
application and an operation to perform on a matching configuration. Rules are
partly context-sensitive; furthermore, they are reversible, meaning that their
operations can be undone, which allows the parsing process to be
nondeterministic. These two factors confer enough expressive power to the
formalism for parsing natural languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9804002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9804002</id><created>1998-04-26</created><updated>1998-05-12</updated><authors><author><keyname>Karttunen</keyname><forenames>Lauri</forenames><affiliation>Xerox Research Centre Europe</affiliation></author></authors><title>The Proper Treatment of Optimality in Computational Phonology</title><categories>cmp-lg cs.CL</categories><comments>12 pages. Postscript</comments><journal-ref>Proceedings of FSMNLP'98. International Workshop on Finite-State
  Methods in Natural Language Processing, pages 1-12, June 29 - July 1, 1998.
  Bilkent University. Ankara, Turkey</journal-ref><abstract>  This paper presents a novel formalization of optimality theory. Unlike
previous treatments of optimality in computational linguistics, starting with
Ellison (1994), the new approach does not require any explicit marking and
counting of constraint violations. It is based on the notion of &quot;lenient
composition,&quot; defined as the combination of ordinary composition and priority
union. If an underlying form has outputs that can meet a given constraint,
lenient composition enforces the constraint; if none of the output candidates
meet the constraint, lenient composition allows all of them. For the sake of
greater efficiency, we may &quot;leniently compose&quot; the GEN relation and all the
constraints into a single finite-state transducer that maps each underlying
form directly into its optimal surface realizations, and vice versa, without
ever producing any failing candidates. Seen from this perspective, optimality
theory is surprisingly similar to the two older strains of finite-state
phonology: classical rewrite systems and two-level models. In particular, the
ranking of optimality constraints corresponds to the ordering of rewrite rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9804003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9804003</id><created>1998-04-28</created><authors><author><keyname>van Noord</keyname><forenames>Gertjan</forenames><affiliation>Groningen University</affiliation></author></authors><title>Treatment of Epsilon-Moves in Subset Construction</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings of FSMNLP'98. International Workshop on Finite-State
  Methods in Natural Language Processing, pages 1-12, June 29 - July 1, 1998.
  Bilkent University. Ankara, Turkey.</journal-ref><abstract>  The paper discusses the problem of determinising finite-state automata
containing large numbers of epsilon-moves. Experiments with finite-state
approximations of natural language grammars often give rise to very large
automata with a very large number of epsilon-moves. The paper identifies three
subset construction algorithms which treat epsilon-moves. A number of
experiments has been performed which indicate that the algorithms differ
considerably in practice. Furthermore, the experiments suggest that the average
number of epsilon-moves per state can be used to predict which algorithm is
likely to perform best for a given input automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9804004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9804004</id><created>1998-04-29</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames><affiliation>University of Library and Information Science</affiliation></author></authors><title>Corpus-Based Word Sense Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>PhD Thesis, 108 pages</comments><report-no>TR98-0003, Tokyo Institute of Technology</report-no><abstract>  Resolution of lexical ambiguity, commonly termed ``word sense
disambiguation'', is expected to improve the analytical accuracy for tasks
which are sensitive to lexical semantics. Such tasks include machine
translation, information retrieval, parsing, natural language understanding and
lexicography. Reflecting the growth in utilization of machine readable texts,
word sense disambiguation techniques have been explored variously in the
context of corpus-based approaches. Within one corpus-based framework, that is
the similarity-based method, systems use a database, in which example sentences
are manually annotated with correct word senses. Given an input, systems search
the database for the most similar example to the input. The lexical ambiguity
of a word contained in the input is resolved by selecting the sense annotation
of the retrieved example. In this research, we apply this method of resolution
of verbal polysemy, in which the similarity between two examples is computed as
the weighted average of the similarity between complements governed by a target
polysemous verb. We explore similarity-based verb sense disambiguation focusing
on the following three methods. First, we propose a weighting schema for each
verb complement in the similarity computation. Second, in similarity-based
techniques, the overhead for manual supervision and searching the large-sized
database can be prohibitive. To resolve this problem, we propose a method to
select a small number of effective examples, for system usage. Finally, the
efficiency of our system is highly dependent on the similarity computation
used. To maximize efficiency, we propose a method which integrates the
advantages of previous methods for similarity computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9804005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9804005</id><created>1998-04-30</created><authors><author><keyname>da Costa</keyname><forenames>N. C. A.</forenames><affiliation>University of Sao Paulo</affiliation></author><author><keyname>Doria</keyname><forenames>F. A.</forenames><affiliation>Federal University at Rio de Janeiro</affiliation></author></authors><title>On the existence of certain total recursive functions in nontrivial
  axiom systems, I</title><categories>cmp-lg cs.CL</categories><comments>LaTeX, 16 pages, no figures. This paper was submitted to a major
  journal in the field and rejected. The referee somehow misundesrtood
  Corollary 3.8 and wrongly concluded that the proof had either a gap or an
  error. Can you find whether that error exists?</comments><abstract>  We investigate the existence of a class of ZFC-provably total recursive unary
functions, given certain constraints, and apply some of those results to show
that, for $\Sigma_1$-sound set theory, ZFC$\not\vdash P&lt;NP$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805001</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805001</id><created>1998-05-05</created><authors><author><keyname>Carroll</keyname><forenames>Glenn</forenames><affiliation>IMS, Universit&#xe4;t Stuttgart</affiliation></author><author><keyname>Rooth</keyname><forenames>Mats</forenames><affiliation>IMS, Universit&#xe4;t Stuttgart</affiliation></author></authors><title>Valence Induction with a Head-Lexicalized PCFG</title><categories>cmp-lg cs.CL</categories><comments>10 pages, 5 postscript figures</comments><abstract>  This paper presents an experiment in learning valences (subcategorization
frames) from a 50 million word text corpus, based on a lexicalized
probabilistic context free grammar. Distributions are estimated using a
modified EM algorithm. We evaluate the acquired lexicon both by comparison with
a dictionary and by entropy measures. Results show that our model produces
highly accurate frame distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805002</id><created>1998-05-07</created><authors><author><keyname>Dymetman</keyname><forenames>Marc</forenames><affiliation>Xerox Research Centre Europe, Grenoble</affiliation></author></authors><title>Group Theory and Grammatical Description</title><categories>cmp-lg cs.CL</categories><comments>17 pages (Latex, Postscript). A shorter version of this paper will
  appear in the Coling/ACL 98 Proceedings. See
  http://www.xrce.xerox.com/people/dymetman/dymetman.html</comments><report-no>MLTT-033</report-no><abstract>  This paper presents a model for linguistic description based on group theory.
A grammar in this model, or &quot;G-grammar&quot;, is a collection of lexical expressions
which are products of logical forms, phonological forms, and their inverses.
Phrasal descriptions are obtained by forming products of lexical expressions
and by cancelling contiguous elements which are inverses of each other. We show
applications of this model to parsing and generation, long-distance movement,
and quantifier scoping. We believe that by moving from the free monoid over a
vocabulary V --- standard in formal language studies --- to the free group over
V, deep affinities between linguistic phenomena and classical algebra come to
the surface, and that the consequences of tapping the mathematical connections
thus established could be considerable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805003</id><created>1998-05-08</created><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Models of Co-occurrence</title><categories>cmp-lg cs.CL</categories><report-no>IRCS TR #98-05</report-no><abstract>  A model of co-occurrence in bitext is a boolean predicate that indicates
whether a given pair of word tokens co-occur in corresponding regions of the
bitext space. Co-occurrence is a precondition for the possibility that two
tokens might be mutual translations. Models of co-occurrence are the glue that
binds methods for mapping bitext correspondence with methods for estimating
translation models into an integrated system for exploiting parallel texts.
Different models of co-occurrence are possible, depending on the kind of bitext
map that is available, the language-specific information that is available, and
the assumptions made about the nature of translational equivalence. Although
most statistical translation models are based on models of co-occurrence,
modeling co-occurrence correctly is more difficult than it may at first appear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805004</id><created>1998-05-08</created><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Annotation Style Guide for the Blinker Project</title><categories>cmp-lg cs.CL</categories><report-no>IRCS TR #98-06</report-no><abstract>  This annotation style guide was created by and for the Blinker project at the
University of Pennsylvania. The Blinker project was so named after the
``bilingual linker'' GUI, which was created to enable bilingual annotators to
``link'' word tokens that are mutual translations in parallel texts. The
parallel text chosen for this project was the Bible, because it is probably the
easiest text to obtain in electronic form in multiple languages. The languages
involved were English and French, because, of the languages with which the
project co-ordinator was familiar, these were the two for which a sufficient
number of annotators was likely to be found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805005</id><created>1998-05-08</created><updated>1998-05-10</updated><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Manual Annotation of Translational Equivalence: The Blinker Project</title><categories>cmp-lg cs.CL</categories><report-no>IRCS TR #98-07</report-no><abstract>  Bilingual annotators were paid to link roughly sixteen thousand corresponding
words between on-line versions of the Bible in modern French and modern
English. These annotations are freely available to the research community from
http://www.cis.upenn.edu/~melamed . The annotations can be used for several
purposes. First, they can be used as a standard data set for developing and
testing translation lexicons and statistical translation models. Second,
researchers in lexical semantics will be able to mine the annotations for
insights about cross-linguistic lexicalization patterns. Third, the annotations
can be used in research into certain recently proposed methods for monolingual
word-sense disambiguation. This paper describes the annotated texts, the
specially-designed annotation tool, and the strategies employed to increase the
consistency of the annotations. The annotation process was repeated five times
by different annotators. Inter-annotator agreement rates indicate that the
annotations are reasonably reliable and that the method is easy to replicate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805006</id><created>1998-05-11</created><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Word-to-Word Models of Translational Equivalence</title><categories>cmp-lg cs.CL</categories><report-no>IRCS TR #98-08</report-no><abstract>  Parallel texts (bitexts) have properties that distinguish them from other
kinds of parallel data. First, most words translate to only one other word.
Second, bitext correspondence is noisy. This article presents methods for
biasing statistical translation models to reflect these properties. Analysis of
the expected behavior of these biases in the presence of sparse data predicts
that they will result in more accurate models. The prediction is confirmed by
evaluation with respect to a gold standard -- translation models that are
biased in this fashion are significantly more accurate than a baseline
knowledge-poor model. This article also shows how a statistical translation
model can take advantage of various kinds of pre-existing knowledge that might
be available about particular language pairs. Even the simplest kinds of
language-specific knowledge, such as the distinction between content words and
function words, is shown to reliably boost translation model performance on
some tasks. Statistical models that are informed by pre-existing knowledge
about the model domain combine the best of both the rationalist and empiricist
traditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805007</id><created>1998-05-19</created><authors><author><keyname>Goodman</keyname><forenames>Joshua</forenames><affiliation>Harvard University</affiliation></author></authors><title>Parsing Inside-Out</title><categories>cmp-lg cs.CL</categories><comments>Ph.D. Thesis, 257 pages, 40 postscript figures</comments><abstract>  The inside-outside probabilities are typically used for reestimating
Probabilistic Context Free Grammars (PCFGs), just as the forward-backward
probabilities are typically used for reestimating HMMs. I show several novel
uses, including improving parser accuracy by matching parsing algorithms to
evaluation criteria; speeding up DOP parsing by 500 times; and 30 times faster
PCFG thresholding at a given accuracy level. I also give an elegant,
state-of-the-art grammar formalism, which can be used to compute inside-outside
probabilities; and a parser description formalism, which makes it easy to
derive inside-outside formulas and many others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805008</id><created>1998-05-20</created><authors><author><keyname>Rogers</keyname><forenames>James</forenames><affiliation>University of Central Florida</affiliation></author></authors><title>A Descriptive Characterization of Tree-Adjoining Languages (Full Version)</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses colacl.sty</comments><report-no>CS-TR-98-01</report-no><abstract>  Since the early Sixties and Seventies it has been known that the regular and
context-free languages are characterized by definability in the monadic
second-order theory of certain structures. More recently, these descriptive
characterizations have been used to obtain complexity results for constraint-
and principle-based theories of syntax and to provide a uniform model-theoretic
framework for exploring the relationship between theories expressed in
disparate formal terms. These results have been limited, to an extent, by the
lack of descriptive characterizations of language classes beyond the
context-free. Recently, we have shown that tree-adjoining languages (in a
mildly generalized form) can be characterized by recognition by automata
operating on three-dimensional tree manifolds, a three-dimensional analog of
trees. In this paper, we exploit these automata-theoretic results to obtain a
characterization of the tree-adjoining languages by definability in the monadic
second-order theory of these three-dimensional tree manifolds. This not only
opens the way to extending the tools of model-theoretic syntax to the level of
TALs, but provides a highly flexible mechanism for defining TAGs in terms of
logical constraints.
  This is the full version of a paper to appear in the proceedings of
COLING-ACL'98 as a project note.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805009</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805009</id><created>1998-05-27</created><authors><author><keyname>Yuret</keyname><forenames>Deniz</forenames><affiliation>MIT Artificial Intelligence Laboratory</affiliation></author></authors><title>Discovery of Linguistic Relations Using Lexical Attraction</title><categories>cmp-lg cs.CL</categories><comments>dissertation, 56 pages</comments><abstract>  This work has been motivated by two long term goals: to understand how humans
learn language and to build programs that can understand language. Using a
representation that makes the relevant features explicit is a prerequisite for
successful learning and understanding. Therefore, I chose to represent
relations between individual words explicitly in my model. Lexical attraction
is defined as the likelihood of such relations. I introduce a new class of
probabilistic language models named lexical attraction models which can
represent long distance relations between words and I formalize this new class
of models using information theory.
  Within the framework of lexical attraction, I developed an unsupervised
language acquisition program that learns to identify linguistic relations in a
given sentence. The only explicitly represented linguistic knowledge in the
program is lexical attraction. There is no initial grammar or lexicon built in
and the only input is raw text. Learning and processing are interdigitated. The
processor uses the regularities detected by the learner to impose structure on
the input. This structure enables the learner to detect higher level
regularities. Using this bootstrapping procedure, the program was trained on
100 million words of Associated Press material and was able to achieve 60%
precision and 50% recall in finding relations between content-words. Using
knowledge of lexical attraction, the program can identify the correct relations
in syntactically ambiguous sentences such as ``I saw the Statue of Liberty
flying over New York.''
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805010</id><created>1998-05-28</created><authors><author><keyname>Harvey</keyname><forenames>Terrence</forenames><affiliation>Department of Computer Science, University of Delaware</affiliation></author><author><keyname>Carberry</keyname><forenames>Sandra</forenames><affiliation>Department of Computer Science, University of Delaware</affiliation></author></authors><title>Integrating Text Plans for Conciseness and Coherence</title><categories>cmp-lg cs.CL</categories><comments>7 pages, 7 Postscript figures, uses colacl.sty</comments><journal-ref>Proceedings of the 17th International Conference on Computational
  Linguistics (COLING-ACL '98)</journal-ref><abstract>  Our experience with a critiquing system shows that when the system detects
problems with the user's performance, multiple critiques are often produced.
Analysis of a corpus of actual critiques revealed that even though each
individual critique is concise and coherent, the set of critiques as a whole
may exhibit several problems that detract from conciseness and coherence, and
consequently assimilation. Thus a text planner was needed that could integrate
the text plans for individual communicative goals to produce an overall text
plan representing a concise, coherent message.
  This paper presents our general rule-based system for accomplishing this
task. The system takes as input a \emph{set} of individual text plans
represented as RST-style trees, and produces a smaller set of more complex
trees representing integrated messages that still achieve the multiple
communicative goals of the individual text plans. Domain-independent rules are
used to capture strategies across domains, while the facility for addition of
domain-dependent rules enables the system to be tuned to the requirements of a
particular domain. The system has been tested on a corpus of critiques in the
domain of trauma care.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805011</id><created>1998-05-29</created><authors><author><keyname>Jones</keyname><forenames>Karen Sparck</forenames><affiliation>Computer Laboratory, University of Cambridge</affiliation></author></authors><title>Automatic summarising: factors and directions</title><categories>cmp-lg cs.CL</categories><abstract>  This position paper suggests that progress with automatic summarising demands
a better research methodology and a carefully focussed research strategy. In
order to develop effective procedures it is necessary to identify and respond
to the context factors, i.e. input, purpose, and output factors, that bear on
summarising and its evaluation. The paper analyses and illustrates these
factors and their implications for evaluation. It then argues that this
analysis, together with the state of the art and the intrinsic difficulty of
summarising, imply a nearer-term strategy concentrating on shallow, but not
surface, text analysis and on indicative summarising. This is illustrated with
current work, from which a potentially productive research programme can be
developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9805012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9805012</id><created>1998-05-29</created><authors><author><keyname>Schneider</keyname><forenames>David A.</forenames><affiliation>University of Delaware</affiliation></author><author><keyname>McCoy</keyname><forenames>Kathleen F.</forenames><affiliation>University of Delaware</affiliation></author></authors><title>Recognizing Syntactic Errors in the Writing of Second Language Learners</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses colacl.sty</comments><journal-ref>Proceedings of the 17th International Conference on Computational
  Linguistics (COLING-ACL '98)</journal-ref><abstract>  This paper reports on the recognition component of an intelligent tutoring
system that is designed to help foreign language speakers learn standard
English. The system models the grammar of the learner, with this instantiation
of the system tailored to signers of American Sign Language (ASL). We discuss
the theoretical motivations for the system, various difficulties that have been
encountered in the implementation, as well as the methods we have used to
overcome these problems. Our method of capturing ungrammaticalities involves
using mal-rules (also called 'error productions'). However, the straightforward
addition of some mal-rules causes significant performance problems with the
parser. For instance, the ASL population has a strong tendency to drop pronouns
and the auxiliary verb `to be'. Being able to account for these as sentences
results in an explosion in the number of possible parses for each sentence.
This explosion, left unchecked, greatly hampers the performance of the system.
We discuss how this is handled by taking into account expectations from the
specific population (some of which are captured in our unique user model). The
different representations of lexical items at various points in the acquisition
process are modeled by using mal-rules, which obviates the need for multiple
lexicons. The grammar is evaluated on its ability to correctly diagnose
agreement problems in actual sentences produced by ASL native speakers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806001</id><created>1998-05-31</created><authors><author><keyname>Radev</keyname><forenames>Dragomir R.</forenames><affiliation>Columbia University</affiliation></author></authors><title>Learning Correlations between Linguistic Indicators and Semantic
  Constraints: Reuse of Context-Dependent Descriptions of Entities</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses colacl.sty and acl.bst, uses epsfig. To appear in the
  Proceedings of the Joint 17th International Conference on Computational
  Linguistics 36th Annual Meeting of the Association for Computational
  Linguistics (COLING-ACL'98)</comments><abstract>  This paper presents the results of a study on the semantic constraints
imposed on lexical choice by certain contextual indicators. We show how such
indicators are computed and how correlations between them and the choice of a
noun phrase description of a named entity can be automatically established
using supervised learning. Based on this correlation, we have developed a
technique for automatic lexical choice of descriptions of entities in text
generation. We discuss the underlying relationship between the pragmatics of
choosing an appropriate description that serves a specific purpose in the
automatically generated text and the semantics of the description itself. We
present our work in the framework of the more general concept of reuse of
linguistic structures that are automatically extracted from large corpora. We
present a formal evaluation of our approach and we conclude with some thoughts
on potential applications of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806002</id><created>1998-06-02</created><authors><author><keyname>Samuel</keyname><forenames>Ken</forenames><affiliation>Department of Computer Science, University of Delaware</affiliation></author><author><keyname>Carberry</keyname><forenames>Sandra</forenames><affiliation>Department of Computer Science, University of Delaware</affiliation></author><author><keyname>Vijay-Shanker</keyname><forenames>K.</forenames><affiliation>Department of Computer Science, University of Delaware</affiliation></author></authors><title>Computing Dialogue Acts from Features with Transformation-Based Learning</title><categories>cmp-lg cs.CL</categories><comments>8 pages, 1 Postscript figure, uses aaai.sty and aaai.bst</comments><journal-ref>Applying Machine Learning to Discourse Processing: Papers from the
  1998 AAAI Spring Symposium</journal-ref><abstract>  To interpret natural language at the discourse level, it is very useful to
accurately recognize dialogue acts, such as SUGGEST, in identifying speaker
intentions. Our research explores the utility of a machine learning method
called Transformation-Based Learning (TBL) in computing dialogue acts, because
TBL has a number of advantages over alternative approaches for this
application. We have identified some extensions to TBL that are necessary in
order to address the limitations of the original algorithm and the particular
demands of discourse processing. We use a Monte Carlo strategy to increase the
applicability of the TBL method, and we select features of utterances that can
be used as input to improve the performance of TBL. Our system is currently
being tested on the VerbMobil corpora of spoken dialogues, producing promising
preliminary results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806003</id><created>1998-06-03</created><authors><author><keyname>Samuel</keyname><forenames>Ken</forenames><affiliation>Department of Computer Science, University of Delaware</affiliation></author></authors><title>Lazy Transformation-Based Learning</title><categories>cmp-lg cs.CL</categories><comments>5 pages, 4 Postscript figures, uses aaai.sty and aaai.bst</comments><journal-ref>Proceedings of the 11th International Florida Artificial
  Intelligence Research Symposium Conference</journal-ref><abstract>  We introduce a significant improvement for a relatively new machine learning
method called Transformation-Based Learning. By applying a Monte Carlo strategy
to randomly sample from the space of rules, rather than exhaustively analyzing
all possible rules, we drastically reduce the memory and time costs of the
algorithm, without compromising accuracy on unseen data. This enables
Transformation- Based Learning to apply to a wider range of domains, as it can
effectively consider a larger number of different features and feature
interactions in the data. In addition, the Monte Carlo improvement decreases
the labor demands on the human developer, who no longer needs to develop a
minimal set of rule templates to maintain tractability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806004</id><created>1998-06-05</created><authors><author><keyname>Lee</keyname><forenames>Mark</forenames><affiliation>University of Sheffield</affiliation></author></authors><title>Rationality, Cooperation and Conversational Implicature</title><categories>cmp-lg cs.CL</categories><comments>Presented at the Ninth Irish Conference on Artificial Intelligence
  (1997)</comments><abstract>  Conversational implicatures are usually described as being licensed by the
disobeying or flouting of a Principle of Cooperation. However, the
specification of this principle has proved computationally elusive. In this
paper we suggest that a more useful concept is rationality. Such a concept can
be specified explicitely in planning terms and we argue that speakers perform
utterances as part of the optimal plan for their particular communicative
goals. Such an assumption can be used by the hearer to infer conversational
implicatures implicit in the speaker's utterance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806005</id><created>1998-06-05</created><authors><author><keyname>Lee</keyname><forenames>Mark</forenames><affiliation>University of Sheffield</affiliation></author><author><keyname>Wilks</keyname><forenames>Yorick</forenames><affiliation>University of Sheffield</affiliation></author></authors><title>Eliminating deceptions and mistaken belief to infer conversational
  implicature</title><categories>cmp-lg cs.CL</categories><comments>IJCAI-97 workshop on Collaboration, Cooperation, and Conflict in
  Dialogue Systems</comments><abstract>  Conversational implicatures are usually described as being licensed by the
disobeying or flouting of some principle by the speaker in cooperative
dialogue. However, such work has failed to distinguish cases of the speaker
flouting such a principle from cases where the speaker is either deceptive or
holds a mistaken belief. In this paper, we demonstrate how the three different
cases can be distinguished in terms of the beliefs ascribed to the speaker of
the utterance. We argue that in the act of distinguishing the speaker's
intention and ascribing such beliefs, the intended inference can be made by the
hearer. This theory is implemented in ViewGen, a pre-existing belief modelling
system used in a medical counselling domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806006</id><created>1998-06-08</created><authors><author><keyname>Samuel</keyname><forenames>Ken</forenames><affiliation>Department of Computer and Information Sciences, University of Delaware</affiliation></author><author><keyname>Carberry</keyname><forenames>Sandra</forenames><affiliation>Department of Computer and Information Sciences, University of Delaware</affiliation></author><author><keyname>Vijay-Shanker</keyname><forenames>K.</forenames><affiliation>Department of Computer and Information Sciences, University of Delaware</affiliation></author></authors><title>Dialogue Act Tagging with Transformation-Based Learning</title><categories>cmp-lg cs.CL</categories><comments>7 pages, no Postscript figures, uses colacl.sty and acl.bst</comments><journal-ref>Proceedings of the 17th International Conference on Computational
  Linguistics (COLING-ACL '98)</journal-ref><abstract>  For the task of recognizing dialogue acts, we are applying the
Transformation-Based Learning (TBL) machine learning algorithm. To circumvent a
sparse data problem, we extract values of well-motivated features of
utterances, such as speaker direction, punctuation marks, and a new feature,
called dialogue act cues, which we find to be more effective than cue phrases
and word n-grams in practice. We present strategies for constructing a set of
dialogue act cues automatically by minimizing the entropy of the distribution
of dialogue acts in a training corpus, filtering out irrelevant dialogue act
cues, and clustering semantically-related words. In addition, to address
limitations of TBL, we introduce a Monte Carlo strategy for training
efficiently and a committee method for computing confidence measures. These
ideas are combined in our working implementation, which labels held-out data as
accurately as any other reported system for the dialogue act tagging task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806007</id><created>1998-06-09</created><authors><author><keyname>Samuel</keyname><forenames>Ken</forenames><affiliation>Department of Computer and Information Sciences, University of Delaware</affiliation></author><author><keyname>Carberry</keyname><forenames>Sandra</forenames><affiliation>Department of Computer and Information Sciences, University of Delaware</affiliation></author><author><keyname>Vijay-Shanker</keyname><forenames>K.</forenames><affiliation>Department of Computer and Information Sciences, University of Delaware</affiliation></author></authors><title>An Investigation of Transformation-Based Learning in Discourse</title><categories>cmp-lg cs.CL</categories><comments>9 pages, 3 Postscript figure, uses ml98.sty</comments><journal-ref>Machine Learning: Proceedings of the 15th International Conference</journal-ref><abstract>  This paper presents results from the first attempt to apply
Transformation-Based Learning to a discourse-level Natural Language Processing
task. To address two limitations of the standard algorithm, we developed a
Monte Carlo version of Transformation-Based Learning to make the method
tractable for a wider range of problems without degradation in accuracy, and we
devised a committee method for assigning confidence measures to tags produced
by Transformation-Based Learning. The paper describes these advances, presents
experimental evidence that Transformation-Based Learning is as effective as
alternative approaches (such as Decision Trees and N-Grams) for a discourse
task called Dialogue Act Tagging, and argues that Transformation-Based Learning
has desirable features that make it particularly appealing for the Dialogue Act
Tagging task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806008</id><created>1998-06-10</created><authors><author><keyname>Kim</keyname><forenames>Byeongchang</forenames><affiliation>POSTECH, Korea</affiliation></author><author><keyname>Lee</keyname><forenames>WonIl</forenames><affiliation>POSTECH, Korea</affiliation></author><author><keyname>Lee</keyname><forenames>Geunbae</forenames><affiliation>POSTECH, Korea</affiliation></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames><affiliation>POSTECH, Korea</affiliation></author></authors><title>Unlimited Vocabulary Grapheme to Phoneme Conversion for Korean TTS</title><categories>cmp-lg cs.CL</categories><comments>5 pages, uses colacl.sty and acl.bst, uses epsfig. To appear in the
  Proceedings of the Joint 17th International Conference on Computational
  Linguistics 36th Annual Meeting of the Association for Computational
  Linguistics (COLING-ACL'98)</comments><abstract>  This paper describes a grapheme-to-phoneme conversion method using phoneme
connectivity and CCV conversion rules. The method consists of mainly four
modules including morpheme normalization, phrase-break detection, morpheme to
phoneme conversion and phoneme connectivity check.
  The morpheme normalization is to replace non-Korean symbols into standard
Korean graphemes. The phrase-break detector assigns phrase breaks using
part-of-speech (POS) information. In the morpheme-to-phoneme conversion module,
each morpheme in the phrase is converted into phonetic patterns by looking up
the morpheme phonetic pattern dictionary which contains candidate phonological
changes in boundaries of the morphemes. Graphemes within a morpheme are grouped
into CCV patterns and converted into phonemes by the CCV conversion rules. The
phoneme connectivity table supports grammaticality checking of the adjacent two
phonetic morphemes.
  In the experiments with a corpus of 4,973 sentences, we achieved 99.9% of the
grapheme-to-phoneme conversion performance and 97.5% of the sentence conversion
performance. The full Korean TTS system is now being implemented using this
conversion method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806009</id><created>1998-06-11</created><authors><author><keyname>Benitez</keyname><forenames>Laura</forenames><affiliation>Natural Language Processing Group of Universitat Politecnica de Catalunya and Universitat de Barcelona</affiliation></author><author><keyname>Cervell</keyname><forenames>Sergi</forenames><affiliation>Natural Language Processing Group of Universitat Politecnica de Catalunya and Universitat de Barcelona</affiliation></author><author><keyname>Escudero</keyname><forenames>Gerard</forenames><affiliation>Natural Language Processing Group of Universitat Politecnica de Catalunya and Universitat de Barcelona</affiliation></author><author><keyname>Lopez</keyname><forenames>Monica</forenames><affiliation>Natural Language Processing Group of Universitat Politecnica de Catalunya and Universitat de Barcelona</affiliation></author><author><keyname>Rigau</keyname><forenames>German</forenames><affiliation>Natural Language Processing Group of Universitat Politecnica de Catalunya and Universitat de Barcelona</affiliation></author><author><keyname>Taule</keyname><forenames>Mariona</forenames><affiliation>Natural Language Processing Group of Universitat Politecnica de Catalunya and Universitat de Barcelona</affiliation></author></authors><title>Methods and Tools for Building the Catalan WordNet</title><categories>cmp-lg cs.CL</categories><comments>5 pages, postscript file. In workshop Language Resources for European
  Minority Languages at LREC'98</comments><abstract>  In this paper we introduce the methodology used and the basic phases we
followed to develop the Catalan WordNet, and shich lexical resources have been
employed in its building. This methodology, as well as the tools we made use
of, have been thought in a general way so that they could be applied to any
other language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806010</id><created>1998-06-15</created><authors><author><keyname>Agirre</keyname><forenames>E.</forenames></author><author><keyname>Gojenola</keyname><forenames>K.</forenames></author><author><keyname>Sarasola</keyname><forenames>K.</forenames></author></authors><title>Towards a single proposal is spelling correction</title><categories>cmp-lg cs.CL</categories><abstract>  The study presented here relies on the integrated use of different kinds of
knowledge in order to improve first-guess accuracy in non-word
context-sensitive correction for general unrestricted texts. State of the art
spelling correction systems, e.g. ispell, apart from detecting spelling errors,
also assist the user by offering a set of candidate corrections that are close
to the misspelled word. Based on the correction proposals of ispell, we built
several guessers, which were combined in different ways. Firstly, we evaluated
all possibilities and selected the best ones in a corpus with artificially
generated typing errors. Secondly, the best combinations were tested on texts
with genuine spelling errors. The results for the latter suggest that we can
expect automatic non-word correction for all the errors in a free running text
with 80% precision and a single proposal 98% of the times (1.02 proposals on
average).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806011</id><created>1998-06-16</created><updated>1999-04-15</updated><authors><author><keyname>Argamon</keyname><forenames>Shlomo</forenames></author><author><keyname>Dagan</keyname><forenames>Ido</forenames></author><author><keyname>Krymolowski</keyname><forenames>Yuval</forenames></author></authors><title>A Memory-Based Approach to Learning Shallow Natural Language Patterns</title><categories>cmp-lg cs.CL</categories><comments>27 pages. This is a revised and extended version of the paper
  presented in COLING-ACL '98. To appear in Journal of Experimental and
  Theoretical AI (JETAI) special issue on memory-based learning</comments><acm-class>H.3.1;I.2.6;I.2.7</acm-class><abstract>  Recognizing shallow linguistic patterns, such as basic syntactic
relationships between words, is a common task in applied natural language and
text processing. The common practice for approaching this task is by tedious
manual definition of possible pattern structures, often in the form of regular
expressions or finite automata. This paper presents a novel memory-based
learning method that recognizes shallow patterns in new text based on a
bracketed training corpus. The training data are stored as-is, in efficient
suffix-tree data structures. Generalization is performed on-line at recognition
time by comparing subsequences of the new text to positive and negative
evidence in the corpus. This way, no information in the training is lost, as
can happen in other learning systems that construct a single generalized model
at the time of training. The paper presents experimental results for
recognizing noun phrase, subject-verb and verb-object patterns in English.
Since the learning approach enables easy porting to new domains, we plan to
apply it to syntactic patterns in other languages and to sub-language patterns
for information extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806012</id><created>1998-06-19</created><authors><author><keyname>Hochberg</keyname><forenames>Judith</forenames></author><author><keyname>Scovel</keyname><forenames>Clint</forenames></author><author><keyname>Thomas</keyname><forenames>Timothy</forenames></author><author><keyname>Hall</keyname><forenames>Sam</forenames></author></authors><title>Bayesian Stratified Sampling to Assess Corpus Utility</title><categories>cmp-lg cs.CL</categories><comments>8 pages, 5 figures. To appear in Proceedings of WVLC-6</comments><report-no>98-1922</report-no><abstract>  This paper describes a method for asking statistical questions about a large
text corpus. We exemplify the method by addressing the question, &quot;What
percentage of Federal Register documents are real documents, of possible
interest to a text researcher or analyst?&quot; We estimate an answer to this
question by evaluating 200 documents selected from a corpus of 45,820 Federal
Register documents. Stratified sampling is used to reduce the sampling
uncertainty of the estimate from over 3100 documents to fewer than 1000. The
stratification is based on observed characteristics of real documents, while
the sampling procedure incorporates a Bayesian version of Neyman allocation. A
possible application of the method is to establish baseline statistics used to
estimate recall rates for information retrieval systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806013</id><created>1998-06-21</created><authors><author><keyname>Carroll</keyname><forenames>John</forenames><affiliation>University of Sussex</affiliation></author><author><keyname>Minnen</keyname><forenames>Guido</forenames><affiliation>University of Sussex</affiliation></author><author><keyname>Briscoe</keyname><forenames>Ted</forenames><affiliation>Cambridge University</affiliation></author></authors><title>Can Subcategorisation Probabilities Help a Statistical Parser?</title><categories>cmp-lg cs.CL</categories><comments>9 pages, uses colacl.sty</comments><journal-ref>6th Workshop on Very Large Corpora, Montreal, Canada, 1998</journal-ref><abstract>  Research into the automatic acquisition of lexical information from corpora
is starting to produce large-scale computational lexicons containing data on
the relative frequencies of subcategorisation alternatives for individual
verbal predicates. However, the empirical question of whether this type of
frequency information can in practice improve the accuracy of a statistical
parser has not yet been answered. In this paper we describe an experiment with
a wide-coverage statistical grammar and parser for English and
subcategorisation frequencies acquired from ten million words of text which
shows that this information can significantly improve parse accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806014</id><created>1998-06-22</created><authors><author><keyname>Wilks</keyname><forenames>Yorick</forenames></author><author><keyname>Stevenson</keyname><forenames>Mark</forenames></author></authors><title>Word Sense Disambiguation using Optimised Combinations of Knowledge
  Sources</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses colacl.sty. To appear in the Proceedings of COLING-ACL
  '98</comments><abstract>  Word sense disambiguation algorithms, with few exceptions, have made use of
only one lexical knowledge source. We describe a system which performs
unrestricted word sense disambiguation (on all content words in free text) by
combining different knowledge sources: semantic preferences, dictionary
definitions and subject/domain codes along with part-of-speech tags. The
usefulness of these sources is optimised by means of a learning algorithm. We
also describe the creation of a new sense tagged corpus by combining existing
resources. Tested accuracy of our approach on this corpus exceeds 92%,
demonstrating the viability of all-word disambiguation rather than restricting
oneself to a small sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806015</id><created>1998-06-23</created><authors><author><keyname>Rigau</keyname><forenames>German</forenames><affiliation>Departament de LSI, Universitat Politecnica de Catalunya</affiliation></author><author><keyname>Rodriguez</keyname><forenames>Horacio</forenames><affiliation>Departament de LSI, Universitat Politecnica de Catalunya</affiliation></author><author><keyname>Agirre</keyname><forenames>Eneko</forenames><affiliation>Lengoia eta Informatikoak saila, Euskal Erriko Universitatea</affiliation></author></authors><title>Building Accurate Semantic Taxonomies from Monolingual MRDs</title><categories>cmp-lg cs.CL</categories><comments>7 pages, postscript file. In COLIN-ACL'98</comments><abstract>  This paper presents a method that combines a set of unsupervised algorithms
in order to accurately build large taxonomies from any machine-readable
dictionary (MRD). Our aim is to profit from conventional MRDs, with no explicit
semantic coding. We propose a system that 1) performs fully automatic exraction
of taxonomic links from MRD entries and 2) ranks the extracted relations in a
way that selective manual refinement is allowed. Tested accuracy can reach
around 100% depending on the degree of coverage selected, showing that taxonomy
building is not limited to structured dictionaries such as LDOCE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806016</id><created>1998-06-23</created><authors><author><keyname>Farreres</keyname><forenames>Xavier</forenames><affiliation>Departament de Llenguatges i Sistemes Informatics of the Universitat Politecnica de Catalunya</affiliation></author><author><keyname>Rigau</keyname><forenames>German</forenames><affiliation>Departament de Llenguatges i Sistemes Informatics of the Universitat Politecnica de Catalunya</affiliation></author><author><keyname>Rodriguez</keyname><forenames>Horacio</forenames><affiliation>Departament de Llenguatges i Sistemes Informatics of the Universitat Politecnica de Catalunya</affiliation></author></authors><title>Using WordNet for Building WordNets</title><categories>cmp-lg cs.CL</categories><comments>8 pages, postscript file. In workshop on Usage of WordNet in NLP</comments><abstract>  This paper summarises a set of methodologies and techniques for the fast
construction of multilingual WordNets. The English WordNet is used in this
approach as a backbone for Catalan and Spanish WordNets and as a lexical
knowledge resource for several subtasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806017</id><created>1998-06-24</created><authors><author><keyname>Webber</keyname><forenames>Bonnie Lynn</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Joshi</keyname><forenames>Aravind K.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Anchoring a Lexicalized Tree-Adjoining Grammar for Discourse</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses aclcol.sty</comments><journal-ref>Proceedings of COLING-ACL'98 Workshop on Discourse Relations and
  Discourse Markers. (Reproduced with permission of the Universite de Montreal</journal-ref><abstract>  We here explore a ``fully'' lexicalized Tree-Adjoining Grammar for discourse
that takes the basic elements of a (monologic) discourse to be not simply
clauses, but larger structures that are anchored on variously realized
discourse cues. This link with intra-sentential grammar suggests an account for
different patterns of discourse cues, while the different structures and
operations suggest three separate sources for elements of discourse meaning:
(1) a compositional semantics tied to the basic trees and operations; (2) a
presuppositional semantics carried by cue phrases that freely adjoin to trees;
and (3) general inference, that draws additional, defeasible conclusions that
flesh out what is conveyed compositionally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806018</id><created>1998-06-25</created><authors><author><keyname>Strube</keyname><forenames>Michael</forenames><affiliation>IRCS, University of Pennsylvania</affiliation></author></authors><title>Never Look Back: An Alternative to Centering</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses colacl.sty, epsfig.sty, times.sty, lingmacros.sty</comments><journal-ref>Proceedings of COLING-ACL '98</journal-ref><abstract>  I propose a model for determining the hearer's attentional state which
depends solely on a list of salient discourse entities (S-list). The ordering
among the elements of the S-list covers also the function of the
backward-looking center in the centering model. The ranking criteria for the
S-list are based on the distinction between hearer-old and hearer-new discourse
entities and incorporate preferences for inter- and intra-sentential anaphora.
The model is the basis for an algorithm which operates incrementally, word by
word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806019</id><created>1998-06-25</created><authors><author><keyname>Di Eugenio</keyname><forenames>Barbara</forenames><affiliation>University of Pittsburgh</affiliation></author><author><keyname>Jordan</keyname><forenames>Pamela W.</forenames><affiliation>University of Pittsburgh</affiliation></author><author><keyname>Moore</keyname><forenames>Johanna D.</forenames><affiliation>University of Pittsburgh</affiliation></author><author><keyname>Thomason</keyname><forenames>Richmond H.</forenames><affiliation>University of Pittsburgh</affiliation></author></authors><title>An Empirical Investigation of Proposals in Collaborative Dialogues</title><categories>cmp-lg cs.CL</categories><comments>5 pages, colacl.sty, formulas.sty</comments><journal-ref>Proceedings of COLING-ACL 1998</journal-ref><abstract>  We describe a corpus-based investigation of proposals in dialogue. First, we
describe our DRI compliant coding scheme and report our inter-coder reliability
results. Next, we test several hypotheses about what constitutes a well-formed
proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9806020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9806020</id><created>1998-06-29</created><authors><author><keyname>Stone</keyname><forenames>Matthew</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Webber</keyname><forenames>Bonnie</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Textual Economy through Close Coupling of Syntax and Semantics</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uses QobiTree.tex</comments><journal-ref>Proceedings 1998 Int'l Workshop on Natural Language Generation,
  Niagara-on-the-Lake, Canada, August 1998</journal-ref><abstract>  We focus on the production of efficient descriptions of objects, actions and
events. We define a type of efficiency, textual economy, that exploits the
hearer's recognition of inferential links to material elsewhere within a
sentence. Textual economy leads to efficient descriptions because the material
that supports such inferences has been included to satisfy independent
communicative goals, and is therefore overloaded in Pollack's sense. We argue
that achieving textual economy imposes strong requirements on the
representation and reasoning used in generating sentences. The representation
must support the generator's simultaneous consideration of syntax and
semantics. Reasoning must enable the generator to assess quickly and reliably
at any stage how the hearer will interpret the current sentence, with its
(incomplete) syntax and semantics. We show that these representational and
reasoning requirements are met in the SPUD system for sentence planning and
realization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807001</id><created>1998-07-06</created><authors><author><keyname>Azzam</keyname><forenames>Saliha</forenames><affiliation>University of Sheffield</affiliation></author><author><keyname>Humphreys</keyname><forenames>Kevin</forenames><affiliation>University of Sheffield</affiliation></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames><affiliation>University of Sheffield</affiliation></author></authors><title>Evaluating a Focus-Based Approach to Anaphora Resolution</title><categories>cmp-lg cs.CL</categories><comments>5 pages, uses colacl.sty</comments><journal-ref>Proceedings of COLING-ACL '98</journal-ref><abstract>  We present an approach to anaphora resolution based on a focusing algorithm,
and implemented within an existing MUC (Message Understanding Conference)
Information Extraction system, allowing quantitative evaluation against a
substantial corpus of annotated real-world texts. Extensions to the basic
focusing mechanism can be easily tested, resulting in refinements to the
mechanism and resolution rules. Results are compared with the results of a
simpler heuristic-based approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807002</id><created>1998-07-13</created><authors><author><keyname>Klavans</keyname><forenames>Judith L.</forenames><affiliation>Center for Research on Information Access, Columbia University</affiliation></author><author><keyname>Kan</keyname><forenames>Min-Yen</forenames><affiliation>Computer Science, Columbia University</affiliation></author></authors><title>The Role of Verbs in Document Analysis</title><categories>cmp-lg cs.CL</categories><comments>7 + 1 pages, US Letter, LaTeX (+2 eps figures). To appear in
  Proceedings of the 17th International Conference on Computational Linguistics
  (COLING-ACL '98). Tool license available at
  http://www.cs.columbia.edu/nlp/licenses/verberLicenseDownload.html</comments><journal-ref>Proceedings of the 17th International Conference on Computational
  Linguistics (COLING-ACL '98), Montreal, Canada: Aug. 1998. pp. 680-686.</journal-ref><abstract>  We present results of two methods for assessing the event profile of news
articles as a function of verb type. The unique contribution of this research
is the focus on the role of verbs, rather than nouns. Two algorithms are
presented and evaluated, one of which is shown to accurately discriminate
documents by type and semantic properties, i.e. the event profile. The initial
method, using WordNet (Miller et al. 1990), produced multiple
cross-classification of articles, primarily due to the bushy nature of the verb
tree coupled with the sense disambiguation problem. Our second approach using
English Verb Classes and Alternations (EVCA) Levin (1993) showed that
monosemous categorization of the frequent verbs in WSJ made it possible to
usefully discriminate documents. For example, our results show that articles in
which communication verbs predominate tend to be opinion pieces, whereas
articles with a high percentage of agreement verbs tend to be about mergers or
legal cases. An evaluation is performed on the results using Kendall's Tau. We
present convincing evidence for using verb semantic classes as a discriminant
in document classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807003</id><created>1998-07-14</created><authors><author><keyname>Hardt</keyname><forenames>Daniel</forenames></author></authors><title>Centering in Dynamic Semantics</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings of COLING 96, Copenhagen, Denmark</journal-ref><abstract>  Centering theory posits a discourse center, a distinguished discourse entity
that is the topic of a discourse. A simplified version of this theory is
developed in a Dynamic Semantics framework. In the resulting system, the
mechanism of center shift allows a simple, elegant analysis of a variety of
phenomena involving sloppy identity in ellipsis and ``paycheck pronouns''.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807004</id><created>1998-07-17</created><authors><author><keyname>Li</keyname><forenames>Hang</forenames><affiliation>NEC Corporation</affiliation></author><author><keyname>Abe</keyname><forenames>Naoki</forenames><affiliation>NEC Corporation</affiliation></author></authors><title>Word Clustering and Disambiguation Based on Co-occurrence Data</title><categories>cmp-lg cs.CL</categories><comments>latex file, uses colacl.sty file and 4 eps files, to appear in Proc.
  of COLING-ACL'98, 8 pages</comments><abstract>  We address the problem of clustering words (or constructing a thesaurus)
based on co-occurrence data, and using the acquired word classes to improve the
accuracy of syntactic disambiguation. We view this problem as that of
estimating a joint probability distribution specifying the joint probabilities
of word pairs, such as noun verb pairs. We propose an efficient algorithm based
on the Minimum Description Length (MDL) principle for estimating such a
probability distribution. Our method is a natural extension of those proposed
in (Brown et al 92) and (Li &amp; Abe 96), and overcomes their drawbacks while
retaining their advantages. We then combined this clustering method with the
disambiguation method of (Li &amp; Abe 95) to derive a disambiguation method that
makes use of both automatically constructed thesauruses and a hand-made
thesaurus. The overall disambiguation accuracy achieved by our method is 85.2%,
which compares favorably against the accuracy (82.4%) obtained by the
state-of-the-art disambiguation method of (Brill &amp; Resnik 94).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807005</id><created>1998-07-17</created><authors><author><keyname>Larcheveque</keyname><forenames>John</forenames><affiliation>INRIA</affiliation></author></authors><title>Graph Interpolation Grammars as Context-Free Automata</title><categories>cmp-lg cs.CL</categories><report-no>RR-3456</report-no><abstract>  A derivation step in a Graph Interpolation Grammar has the effect of scanning
an input token. This feature, which aims at emulating the incrementality of the
natural parser, restricts the formal power of GIGs. This contrasts with the
fact that the derivation mechanism involves a context-sensitive device similar
to tree adjunction in TAGs. The combined effect of input-driven derivation and
restricted context-sensitiveness would be conceivably unfortunate if it turned
out that Graph Interpolation Languages did not subsume Context Free Languages
while being partially context-sensitive. This report sets about examining
relations between CFGs and GIGs, and shows that GILs are a proper superclass of
CFLs. It also brings out a strong equivalence between CFGs and GIGs for the
class of CFLs. Thus, it lays the basis for meaningfully investigating the
amount of context-sensitiveness supported by GIGs, but leaves this
investigation for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807006</id><created>1998-07-17</created><authors><author><keyname>Skut</keyname><forenames>Wojciech</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author><author><keyname>Brants</keyname><forenames>Thorsten</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author></authors><title>A Maximum-Entropy Partial Parser for Unrestricted Text</title><categories>cmp-lg cs.CL</categories><comments>9 pages, LaTeX</comments><abstract>  This paper describes a partial parser that assigns syntactic structures to
sequences of part-of-speech tags. The program uses the maximum entropy
parameter estimation method, which allows a flexible combination of different
knowledge sources: the hierarchical structure, parts of speech and phrasal
categories. In effect, the parser goes beyond simple bracketing and recognises
even fairly complex structures. We give accuracy figures for different
applications of the parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807007</id><created>1998-07-17</created><authors><author><keyname>Skut</keyname><forenames>Wojciech</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author><author><keyname>Brants</keyname><forenames>Thorsten</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author></authors><title>Chunk Tagger - Statistical Recognition of Noun Phrases</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX</comments><abstract>  We describe a stochastic approach to partial parsing, i.e., the recognition
of syntactic structures of limited depth. The technique utilises Markov Models,
but goes beyond usual bracketing approaches, since it is capable of recognising
not only the boundaries, but also the internal structure and syntactic category
of simple as well as complex NP's, PP's, AP's and adverbials. We compare
tagging accuracy for different applications and encoding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807008</id><created>1998-07-17</created><authors><author><keyname>Skut</keyname><forenames>Wojciech</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author><author><keyname>Brants</keyname><forenames>Thorsten</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author><author><keyname>Krenn</keyname><forenames>Brigitte</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author><author><keyname>Uszkoreit</keyname><forenames>Hans</forenames><affiliation>Computational Linguistics, Universitity of the Saarland, Germany</affiliation></author></authors><title>A Linguistically Interpreted Corpus of German Newspaper Text</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX</comments><abstract>  In this paper, we report on the development of an annotation scheme and
annotation tools for unrestricted German text. Our representation format is
based on argument structure, but also permits the extraction of other kinds of
representations. We discuss several methodological issues and the analysis of
some phenomena. Additional focus is on the tools developed in our project and
their applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807009</id><created>1998-07-20</created><updated>1998-07-24</updated><authors><author><keyname>Broeker</keyname><forenames>Norbert</forenames><affiliation>IMS, Stuttgart University</affiliation></author></authors><title>A Projection Architecture for Dependency Grammar and How it Compares to
  LFG</title><categories>cmp-lg cs.CL</categories><comments>Proc. LFG'98 (Brisbane/AUS); LaTeX2e with html package</comments><abstract>  This paper explores commonalities and differences between \dachs, a variant
of Dependency Grammar, and Lexical-Functional Grammar. \dachs\ is based on
traditional linguistic insights, but on modern mathematical tools, aiming to
integrate different knowledge systems (from syntax and semantics) via their
coupling to an abstract syntactic primitive, the dependency relation. These
knowledge systems correspond rather closely to projections in LFG. We will
investigate commonalities arising from the usage of the projection approach in
both theories, and point out differences due to the incompatible linguistic
premises. The main difference to LFG lies in the motivation and status of the
dimensions, and the information coded there. We will argue that LFG confounds
different information in one projection, preventing it to achieve a good
separation of alternatives and calling the motivation of the projection into
question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807010</id><created>1998-07-20</created><authors><author><keyname>Turcato</keyname><forenames>Davide</forenames><affiliation>Simon Fraser University and TCC Communications, Canada</affiliation></author></authors><title>Automatically Creating Bilingual Lexicons for Machine Translation from
  Bilingual Text</title><categories>cmp-lg cs.CL</categories><comments>Latex file, uses colacl.sty file, 7 pages</comments><journal-ref>Proceedings of COLING-ACL'98</journal-ref><abstract>  A method is presented for automatically augmenting the bilingual lexicon of
an existing Machine Translation system, by extracting bilingual entries from
aligned bilingual text. The proposed method only relies on the resources
already available in the MT system itself. It is based on the use of bilingual
lexical templates to match the terminal symbols in the parses of the aligned
sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807011</id><created>1998-07-22</created><authors><author><keyname>Ratnaparkhi</keyname><forenames>Adwait</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Statistical Models for Unsupervised Prepositional Phrase Attachment</title><categories>cmp-lg cs.CL</categories><comments>uses colacl.sty</comments><journal-ref>Proceedings of the 17th International Conference on Computational
  Linguistics (COLING-ACL '98)</journal-ref><abstract>  We present several unsupervised statistical models for the prepositional
phrase attachment task that approach the accuracy of the best supervised
methods for this task. Our unsupervised approach uses a heuristic based on
attachment proximity and trains from raw text that is annotated with only
part-of-speech tags and morphological base forms, as opposed to attachment
information. It is therefore less resource-intensive and more portable than
previous corpus-based algorithms proposed for this task. We present results for
prepositional phrase attachment in both English and Spanish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807012</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807012</id><created>1998-07-31</created><authors><author><keyname>Binsted</keyname><forenames>Kim</forenames><affiliation>Sony Computer Science Laboratory</affiliation></author></authors><title>Character design for soccer commmentary</title><categories>cmp-lg cs.CL</categories><comments>uuencoded gzipped tar file. Latex. Uses psfig, times, llncs</comments><abstract>  In this paper we present early work on an animated talking head commentary
system called {\bf Byrne}\footnote{David Byrne is the lead singer of the
Talking Heads.}. The goal of this project is to develop a system which can take
the output from the RoboCup soccer simulator, and generate appropriate
affective speech and facial expressions, based on the character's personality,
emotional state, and the state of play. Here we describe a system which takes
pre-analysed simulator output as input, and which generates text marked-up for
use by a speech generator and a face animation system. We make heavy use of
inter-system standards, so that future versions of Byrne will be able to take
advantage of advances in the technologies that it incorporates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9807013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9807013</id><created>1998-07-31</created><authors><author><keyname>van Halteren</keyname><forenames>Hans</forenames><affiliation>University of Nijmegen</affiliation></author><author><keyname>Zavrel</keyname><forenames>Jakub</forenames><affiliation>Tilburg University</affiliation></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames><affiliation>Tilburg University</affiliation></author></authors><title>Improving Data Driven Wordclass Tagging by System Combination</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX, uses acl.bst, colacl.sty</comments><journal-ref>Proceedings of the 17th International Conference on Computational
  Linguistics (COLING-ACL'98)</journal-ref><abstract>  In this paper we examine how the differences in modelling between different
data driven systems performing the same NLP task can be exploited to yield a
higher accuracy than the best individual system. We do this by means of an
experiment involving the task of morpho-syntactic wordclass tagging. Four
well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation
Rules and Maximum Entropy) are trained on the same corpus data. After
comparison, their outputs are combined using several voting strategies and
second stage classifiers. All combination taggers outperform their best
component, with the best combination showing a 19.1% lower error rate than the
best individual tagger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808001</id><created>1998-08-03</created><authors><author><keyname>Hwa</keyname><forenames>Rebecca</forenames><affiliation>Harvard University</affiliation></author></authors><title>An Empirical Evaluation of Probabilistic Lexicalized Tree Insertion
  Grammars</title><categories>cmp-lg cs.CL</categories><comments>10 pages, 6 encapsulated postscript figures and 2 latex figures, uses
  colacl.sty</comments><report-no>TR-06-98</report-no><journal-ref>Proceedings of COLING-ACL'98</journal-ref><abstract>  We present an empirical study of the applicability of Probabilistic
Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to
Probabilistic Context-Free Grammars (PCFG), to problems in stochastic
natural-language processing. Comparing the performance of PLTIGs with
non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best
aspects of both, with language modeling capability comparable to N-grams, and
improved parsing performance over its non-lexicalized counterpart. Furthermore,
training of PLTIGs displays faster convergence than PCFGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808002</id><created>1998-08-05</created><authors><author><keyname>Gonzalo</keyname><forenames>Julio</forenames><affiliation>UNED, Spain</affiliation></author><author><keyname>Verdejo</keyname><forenames>Felisa</forenames><affiliation>UNED, Spain</affiliation></author><author><keyname>Chugur</keyname><forenames>Irina</forenames><affiliation>UNED, Spain</affiliation></author><author><keyname>Cigarran</keyname><forenames>Juan</forenames><affiliation>UNED, Spain</affiliation></author></authors><title>Indexing with WordNet synsets can improve Text Retrieval</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX2e, 3 eps figures, uses epsfig, colacl.sty</comments><journal-ref>Proceedings of the COLING/ACL'98 Workshop on Usage of WordNet for
  NLP, Montreal, 1998</journal-ref><abstract>  The classical, vector space model for text retrieval is shown to give better
results (up to 29% better in our experiments) if WordNet synsets are chosen as
the indexing space, instead of word forms. This result is obtained for a
manually disambiguated test collection (of queries and documents) derived from
the Semcor semantic concordance. The sensitivity of retrieval performance to
(automatic) disambiguation errors when indexing documents is also measured.
Finally, it is observed that if queries are not disambiguated, indexing by
synsets performs (at best) only as good as standard word indexing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808003</id><created>1998-08-07</created><authors><author><keyname>Resnik</keyname><forenames>Philip</forenames><affiliation>University of Maryland</affiliation></author></authors><title>Parallel Strands: A Preliminary Investigation into Mining the Web for
  Bilingual Text</title><categories>cmp-lg cs.CL</categories><comments>LaTeX2e, 11 pages, 7 eps figures; uses psfig, llncs.cls, theapa.sty.
  An Appendix at http://umiacs.umd.edu/~resnik/amta98/amta98_appendix.html
  contains test data</comments><report-no>UMIACS TR 98-41</report-no><journal-ref>Proceedings of AMTA-98</journal-ref><abstract>  Parallel corpora are a valuable resource for machine translation, but at
present their availability and utility is limited by genre- and
domain-specificity, licensing restrictions, and the basic difficulty of
locating parallel texts in all but the most dominant of the world's languages.
A parallel corpus resource not yet explored is the World Wide Web, which hosts
an abundance of pages in parallel translation, offering a potential solution to
some of these problems and unique opportunities of its own. This paper presents
the necessary first step in that exploration: a method for automatically
finding parallel translated documents on the Web. The technique is conceptually
simple, fully language independent, and scalable, and preliminary evaluation
results indicate that the method may be accurate enough to apply without human
intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808004</id><created>1998-08-12</created><updated>1998-08-13</updated><authors><author><keyname>Aoyama</keyname><forenames>Hideaki</forenames></author><author><keyname>Constable</keyname><forenames>John</forenames></author></authors><title>Word Length Frequency and Distribution in English: Observations, Theory,
  and Implications for the Construction of Verse Lines</title><categories>cmp-lg cs.CL</categories><comments>32 pages, 11 figures, uses epsf.tex</comments><abstract>  Recent observations in the theory of verse and empirical metrics have
suggested that constructing a verse line involves a pattern-matching search
through a source text, and that the number of found elements (complete words
totaling a specified number of syllables) is given by dividing the total number
of words by the mean number of syllables per word in the source text. This
paper makes this latter point explicit mathematically, and in the course of
this demonstration shows that the word length frequency totals in English
output are distributed geometrically (previous researchers reported an adjusted
Poisson distribution), and that the sequential distribution is random at the
global level, with significant non-randomness in the fine structure. Data from
a corpus of just under two million words, and a syllable-count lexicon of
71,000 word-forms is reported. The pattern-matching theory is shown to be
internally coherent, and it is observed that some of the analytic techniques
described here form a satisfactory test for regular (isometric) lineation in a
text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808005</id><created>1998-08-13</created><authors><author><keyname>Ludwig</keyname><forenames>Bernd</forenames><affiliation>FORWISS and Institute of Computer Science, University of Erlangen</affiliation></author><author><keyname>Goerz</keyname><forenames>Guenther</forenames><affiliation>FORWISS and Institute of Computer Science, University of Erlangen</affiliation></author><author><keyname>Niemann</keyname><forenames>Heinrich</forenames><affiliation>FORWISS and Institute of Computer Science, University of Erlangen</affiliation></author></authors><title>Combining Expression and Content in Domains for Dialog Managers</title><categories>cmp-lg cs.CL</categories><comments>5 pages, uses conference.sty</comments><journal-ref>Proceedings of DL '98, pp. 126-130, Trento, Italy</journal-ref><abstract>  We present work in progress on abstracting dialog managers from their domain
in order to implement a dialog manager development tool which takes (among
other data) a domain description as input and delivers a new dialog manager for
the described domain as output. Thereby we will focus on two topics; firstly,
the construction of domain descriptions with description logics and secondly,
the interpretation of utterances in a given domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808006</id><created>1998-08-13</created><authors><author><keyname>Aoyama</keyname><forenames>Hideaki</forenames></author><author><keyname>Constable</keyname><forenames>John</forenames></author></authors><title>Isometric Lineation in English Texts: An Empirical and Mathematical
  Examination of its Character and Consequences</title><categories>cmp-lg cs.CL</categories><comments>PDF file only</comments><abstract>  In this paper we build on earlier observations and theory regarding word
length frequency and sequential distribution to develop a mathematical
characterization of some of the language features distinguishing isometrically
lineated text from unlineated text, in other words the features distinguishing
isometrical verse from prose. It is shown that the frequency of syllables
making complete words produces a flat distribution for prose, while that for
verse exhibits peaks at the line length position and subsequent multiples of
that position. Data from several verse authors is presented, including a
detailed mathematical analysis of the dynamics underlying peak creation, and
comments are offered on the processes by which authors construct lines. We note
that the word-length sequence of prose is random, whereas lineation
necessitates non-random word-length sequencing, and that this has the probable
consequence of introducing a degree of randomness into the otherwise highly
ordered grammatical sequence. In addition we observe that this effect can be
ameliorated by a reduction in the mean word length of the text (confirming
earlier observations that verse tends to use shorter words) and the use of
lines varying from the core isometrical set. The frequency of variant lines is
shown to be coincident with the frequency of polysyllables, suggesting that the
use of variant lines is motivated by polysyllabic word placement. The
restrictive effects of different line lengths, the relationship between
metrical restriction and poetic effect, and the general character of metrical
rules are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808007</id><created>1998-08-19</created><updated>1998-08-19</updated><authors><author><keyname>Yeh</keyname><forenames>Alexander S.</forenames><affiliation>Mitre Corporation</affiliation></author><author><keyname>Vilain</keyname><forenames>Marc B.</forenames><affiliation>Mitre Corporation</affiliation></author></authors><title>Some Properties of Preposition and Subordinate Conjunction Attachments</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses colacl.sty</comments><journal-ref>Proceedings of COLING-ACL '98: 36th Annual Meeting of the
  Association for Computational Linguistics and 17th International Conference
  on Computational Linguistics, Montreal, Canada, 1998. Pages 1436-1442.</journal-ref><abstract>  Determining the attachments of prepositions and subordinate conjunctions is a
key problem in parsing natural language. This paper presents a trainable
approach to making these attachments through transformation sequences and
error-driven learning. Our approach is broad coverage, and accounts for roughly
three times the attachment cases that have previously been handled by
corpus-based techniques. In addition, our approach is based on a simplified
model of syntax that is more consistent with the practice in current
state-of-the-art language processing systems. This paper sketches syntactic and
algorithmic details, and presents experimental results on data sets derived
from the Penn Treebank. We obtain an attachment accuracy of 75.4% for the
general case, the first such corpus-based result to be reported. For the
restricted cases previously studied with corpus-based methods, our approach
yields an accuracy comparable to current work (83.1%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808008</id><created>1998-08-20</created><authors><author><keyname>Bozsahin</keyname><forenames>Cem</forenames><affiliation>Middle East Technical University, Ankara</affiliation></author></authors><title>Deriving the Predicate-Argument Structure for a Free Word Order Language</title><categories>cmp-lg cs.CL</categories><comments>7 pages LaTeX; uses {times,avm,lingmacros,tree-dvips}.sty; minor
  corrections in the abstract</comments><journal-ref>Proceedings of COLING-ACL '98, 167-173</journal-ref><abstract>  In relatively free word order languages, grammatical functions are
intricately related to case marking. Assuming an ordered representation of the
predicate-argument structure, this work proposes a Combinatory Categorial
Grammar formulation of relating surface case cues to categories and types for
correctly placing the arguments in the predicate-argument structure. This is
achieved by assigning case markers GF-encoding categories. Unlike other CG
formulations, type shifting does not proliferate or cause spurious ambiguity.
Categories of all argument-encoding grammatical functions follow from the same
principle of category assignment. Normal order evaluation of the combinatory
form reveals the predicate-argument structure. Application of the method to
Turkish is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808009</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808009</id><created>1998-08-21</created><authors><author><keyname>Broeker</keyname><forenames>Norbert</forenames><affiliation>IMS, Stuttgart University</affiliation></author></authors><title>How to define a context-free backbone for DGs: Implementing a DG in the
  LFG formalism</title><categories>cmp-lg cs.CL</categories><comments>10 pages, LaTeX2e</comments><journal-ref>Proc. COLING-ACL'98 Workshop ``Processing of Dependency-Based
  Grammars'', pp 29-38</journal-ref><abstract>  This paper presents a multidimensional Dependency Grammar (DG), which
decouples the dependency tree from word order, such that surface ordering is
not determined by traversing the dependency tree. We develop the notion of a
\emph{word order domain structure}, which is linked but structurally dissimilar
to the syntactic dependency tree. We then discuss the implementation of such a
DG using constructs from a unification-based phrase-structure approach, namely
Lexical-Functional Grammar (LFG). Particular attention is given to the analysis
of discontinuities in DG in terms of LFG's functional uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808010</id><created>1998-08-21</created><authors><author><keyname>Pagel</keyname><forenames>V.</forenames></author><author><keyname>Lenzo</keyname><forenames>K.</forenames></author><author><keyname>Black</keyname><forenames>A.</forenames></author></authors><title>Letter to Sound Rules for Accented Lexicon Compression</title><categories>cmp-lg cs.CL</categories><comments>4 pages 1 figure</comments><abstract>  This paper presents trainable methods for generating letter to sound rules
from a given lexicon for use in pronouncing out-of-vocabulary words and as a
method for lexicon compression.
  As the relationship between a string of letters and a string of phonemes
representing its pronunciation for many languages is not trivial, we discuss
two alignment procedures, one fully automatic and one hand-seeded which produce
reasonable alignments of letters to phones.
  Top Down Induction Tree models are trained on the aligned entries. We show
how combined phoneme/stress prediction is better than separate prediction
processes, and still better when including in the model the last phonemes
transcribed and part of speech information. For the lexicons we have tested,
our models have a word accuracy (including stress) of 78% for OALD, 62% for CMU
and 94% for BRULEX. The extremely high scores on the training sets allow
substantial size reductions (more than 1/20).
  WWW site: http://tcts.fpms.ac.be/synthesis/mbrdico
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808011</id><created>1998-08-23</created><authors><author><keyname>Cozens</keyname><forenames>Simon</forenames></author></authors><title>Primitive Part-of-Speech Tagging using Word Length and Sentential
  Structure</title><categories>cmp-lg cs.CL</categories><comments>6 pages</comments><abstract>  It has been argued that, when learning a first language, babies use a series
of small clues to aid recognition and comprehension, and that one of these
clues is word length. In this paper we present a statistical part of speech
tagger which trains itself solely on the number of letters in each word in a
sentence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808012</id><created>1998-08-25</created><authors><author><keyname>Broeker</keyname><forenames>Norbert</forenames><affiliation>IMS, Stuttgart University</affiliation></author></authors><title>Separating Surface Order and Syntactic Relations in a Dependency Grammar</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX2e</comments><journal-ref>Proc. COLING-ACL'98, pp 174-180</journal-ref><abstract>  This paper proposes decoupling the dependency tree from word order, such that
surface ordering is not determined by traversing the dependency tree. We
develop the notion of a \emph{word order domain structure}, which is linked but
structurally dissimilar to the syntactic dependency tree. The proposal results
in a lexicalized, declarative, and formally precise description of word order;
features which lack previous proposals for dependency grammars. Contrary to
other lexicalized approaches to word order, our proposal does not require
lexical ambiguities for ordering alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808013</id><created>1998-08-25</created><authors><author><keyname>Hartrumpf</keyname><forenames>Sven</forenames><affiliation>University of Hagen, Germany</affiliation></author></authors><title>Partial Evaluation for Efficient Access to Inheritance Lexicons</title><categories>cmp-lg cs.CL</categories><comments>8 pages; uses amsfonts.sty, avm.sty, booktabs.sty, caption.sty,
  ranlp97.sty, and xy.sty</comments><journal-ref>Proceedings of the 2nd International Conference on Recent Advances
  in Natural Language Processing (RANLP-97), pp. 43-50, Tzigov Chark, Bulgaria,
  September 1997.</journal-ref><abstract>  Multiple default inheritance formalisms for lexicons have attracted much
interest in recent years. I propose a new efficient method to access such
lexicons. After showing two basic strategies for lookup in inheritance
lexicons, a compromise is developed which combines to a large degree (from a
practical point of view) the advantages of both strategies and avoids their
disadvantages. The method is a kind of (off-line) partial evaluation that makes
a subset of inherited information explicit before using the lexicon. I identify
the parts of a lexicon which should be evaluated, and show how partial
evaluation works for inheritance lexicons. Finally, the theoretical results are
confirmed by a complete implementation. Speedups by a factor of 10-100 are
reached.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808014</id><created>1998-08-26</created><authors><author><keyname>Pagel</keyname><forenames>V.</forenames></author><author><keyname>Carbonell</keyname><forenames>N.</forenames></author><author><keyname>Laprie</keyname><forenames>Y.</forenames></author><author><keyname>Vaissiere</keyname><forenames>J.</forenames></author></authors><title>Spotting Prosodic Boundaries in Continuous Speech in French</title><categories>cmp-lg cs.CL</categories><comments>4 pages</comments><abstract>  A radio speech corpus of 9mn has been prosodically marked by a phonetician
expert, and non expert listeners. this corpus is large enough to train and test
an automatic boundary spotting system, namely a time delay neural network fed
with F0 values, vowels and pseudo-syllable durations. Results validate both
prosodic marking and automatic spotting of prosodic events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808015</id><created>1998-08-26</created><authors><author><keyname>Cardie</keyname><forenames>Claire</forenames><affiliation>Cornell University</affiliation></author><author><keyname>Pierce</keyname><forenames>David</forenames><affiliation>Cornell University</affiliation></author></authors><title>Error-Driven Pruning of Treebank Grammars for Base Noun Phrase
  Identification</title><categories>cmp-lg cs.CL</categories><comments>7 pages; 2 eps figures; uses epsf, colacl</comments><journal-ref>Proceedings of COLING-ACL'98, pages 218-224.</journal-ref><abstract>  Finding simple, non-recursive, base noun phrases is an important subtask for
many natural language processing applications. While previous empirical methods
for base NP identification have been rather complex, this paper instead
proposes a very simple algorithm that is tailored to the relative simplicity of
the task. In particular, we present a corpus-based approach for finding base
NPs by matching part-of-speech tag sequences. The training phase of the
algorithm is based on two successful techniques: first the base NP grammar is
read from a ``treebank'' corpus; then the grammar is improved by selecting
rules with high ``benefit'' scores. Using this simple algorithm with a naive
heuristic for matching rules, we achieve surprising accuracy in an evaluation
on the Penn Treebank Wall Street Journal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808016</id><created>1998-08-27</created><authors><author><keyname>Shaw</keyname><forenames>James</forenames><affiliation>Columbia University</affiliation></author></authors><title>Segregatory Coordination and Ellipsis in Text Generation</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses colacl.sty</comments><journal-ref>COLING-ACL'98, pp. 1220-1226</journal-ref><abstract>  In this paper, we provide an account of how to generate sentences with
coordination constructions from clause-sized semantic representations. An
algorithm is developed to generate sentences with ellipsis, gapping,
right-node-raising, and non-constituent coordination constructions. Various
examples from linguistic literature will be used to demonstrate that the
algorithm does its job well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9808017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9808017</id><created>1998-08-31</created><authors><author><keyname>Nederhof</keyname><forenames>Mark-Jan</forenames><affiliation>University of Groningen</affiliation></author><author><keyname>Satta</keyname><forenames>Giorgio</forenames><affiliation>Universita di Padova</affiliation></author></authors><title>A Variant of Earley Parsing</title><categories>cmp-lg cs.CL</categories><comments>12 pages, 1 Postscript figure, uses psfig.tex and llncs.sty</comments><journal-ref>AI*IA 97: Advances in Artificial Intelligence, 5th Congress of the
  Italian Association for Artificial Intelligence, LNAI 1321, Springer Verlag,
  pages 84-95, 1997.</journal-ref><abstract>  The Earley algorithm is a widely used parsing method in natural language
processing applications. We introduce a variant of Earley parsing that is based
on a ``delayed'' recognition of constituents. This allows us to start the
recognition of a constituent only in cases in which all of its subconstituents
have been found within the input string. This is particularly advantageous in
several cases in which partial analysis of a constituent cannot be completed
and in general in all cases of productions sharing some suffix of their
right-hand sides (even for different left-hand side nonterminals). Although the
two algorithms result in the same asymptotic time and space complexity, from a
practical perspective our algorithm improves the time and space requirements of
the original method, as shown by reported experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9809001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9809001</id><created>1998-09-01</created><updated>1998-09-02</updated><authors><author><keyname>Jarvinen</keyname><forenames>Timo</forenames></author><author><keyname>Tapanainen</keyname><forenames>Pasi</forenames></author></authors><title>Towards an implementable dependency grammar</title><categories>cmp-lg cs.CL</categories><comments>10 pages</comments><journal-ref>in CoLing-ACL'98 workshop 'Processing of Dependency-Based
  Grammars', Kahane and Polguere (eds), p. 1-10, Montreal, Canada, 1998</journal-ref><abstract>  The aim of this paper is to define a dependency grammar framework which is
both linguistically motivated and computationally parsable. See the demo at
http://www.conexor.fi/analysers.html#testing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9809002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9809002</id><created>1998-09-09</created><authors><author><keyname>Guarino</keyname><forenames>Nicola</forenames><affiliation>LADSEB-CNR, Padova, Italy</affiliation></author></authors><title>Some Ontological Principles for Designing Upper Level Lexical Resources</title><categories>cmp-lg cs.CL</categories><comments>8 pages - gzipped postscript file - A4 format</comments><journal-ref>in Proc. of First International Conference on Language Resources
  and Evaluation, Rubio, Gallardo, Castro, and Tejada (eds.), p. 527-534,
  Granada, Spain, 1998</journal-ref><abstract>  The purpose of this paper is to explore some semantic problems related to the
use of linguistic ontologies in information systems, and to suggest some
organizing principles aimed to solve such problems. The taxonomic structure of
current ontologies is unfortunately quite complicated and hard to understand,
especially for what concerns the upper levels. I will focus here on the problem
of ISA overloading, which I believe is the main responsible of these
difficulties. To this purpose, I will carefully analyze the ontological nature
of the categories used in current upper-level structures, considering the
necessity of splitting them according to more subtle distinctions or the
opportunity of excluding them because of their limited organizational role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9809003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9809003</id><created>1998-09-14</created><authors><author><keyname>Hale</keyname><forenames>Michael Mc</forenames><affiliation>Air Force Research Laboratory</affiliation></author></authors><title>A Comparison of WordNet and Roget's Taxonomy for Measuring Semantic
  Similarity</title><categories>cmp-lg cs.CL</categories><comments>6 pages, postscript</comments><abstract>  This paper presents the results of using Roget's International Thesaurus as
the taxonomy in a semantic similarity measurement task. Four similarity metrics
were taken from the literature and applied to Roget's The experimental
evaluation suggests that the traditional edge counting approach does
surprisingly well (a correlation of r=0.88 with a benchmark set of human
similarity judgements, with an upper bound of r=0.90 for human subjects
performing the same task.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0001137</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0001137</id><created>2000-01-11</created><updated>2000-05-03</updated><authors><author><keyname>Weigt</keyname><forenames>Martin</forenames></author><author><keyname>Hartmann</keyname><forenames>Alexander K.</forenames></author></authors><title>The number of guards needed by a museum: A phase transition in vertex
  covering of random graphs</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>4 pages, 3 eps-figures, new version to be published in Phys. Rev. Let</comments><journal-ref>Phys. Rev. Lett. 84, 6118 (2000)</journal-ref><doi>10.1103/PhysRevLett.84.6118</doi><abstract>  In this letter we study the NP-complete vertex cover problem on finite
connectivity random graphs. When the allowed size of the cover set is
decreased, a discontinuous transition in solvability and typical-case
complexity occurs. This transition is characterized by means of exact numerical
simulations as well as by analytical replica calculations. The replica
symmetric phase diagram is in excellent agreement with numerical findings up to
average connectivity $e$, where replica symmetry becomes locally unstable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0002331</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0002331</id><created>2000-02-21</created><authors><author><keyname>Mansilla</keyname><forenames>Ricardo</forenames></author></authors><title>From naive to sophisticated behavior in multiagents based financial
  market models</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CE nlin.AO physics.data-an q-fin.TR</categories><comments>16 pages, 4 figures, submitted to Physica A</comments><doi>10.1016/S0378-4371(00)00227-2</doi><abstract>  We discuss the behavior of two magnitudes, physical complexity and mutual
information function of the outcome of a model of heterogeneous, inductive
rational agents inspired in the El Farol Bar problem and the Minority Game. The
first is a measure rooted in Kolmogorov-Chaitin theory and the second one a
measure related with information entropy of Shannon.
  We make extensive computer simulations, as result of which, we propose an
ansatz for physical complexity and establish the dependence of exponent of that
ansatz from the parameters of the model. We discuss the accuracy of our results
and the relationship with the behavior of mutual information function as a
measure of time correlations of agents choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0002469</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0002469</id><created>2000-02-29</created><authors><author><keyname>Korniss</keyname><forenames>G.</forenames></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames></author><author><keyname>Toroczkai</keyname><forenames>Z.</forenames></author><author><keyname>Rikvold</keyname><forenames>P. A.</forenames></author></authors><title>Non-equilibrium Surface Growth and Scalability of Parallel Algorithms
  for Large Asynchronous Systems</title><categories>cond-mat.stat-mech cs.PF physics.comp-ph</categories><comments>to appear in Computer Simulation Studies in Condensed Matter Physics
  XIII, edited by D.P. Landau, S.P. Lewis, and H.-B. Schuttler</comments><journal-ref>Springer Proceedings in Physics, Vol. 86, Computer Simulation
  Studies in Condensed-Matter Physics XIII, edited by D.P. Landau, S.P. Lewis,
  and H.-B. Schuttler (Springer, Berlin, 2001) p. 183.</journal-ref><abstract>  The scalability of massively parallel algorithms is a fundamental question in
computer science. We study the scalability and the efficiency of a conservative
massively parallel algorithm for discrete-event simulations where the discrete
events are Poisson arrivals. The parallel algorithm is applicable to a wide
range of problems, including dynamic Monte Carlo simulations for large
asynchronous systems with short-range interactions. The evolution of the
simulated time horizon is analogous to a growing and fluctuating surface, and
the efficiency of the algorithm corresponds to the density of local minima of
this surface. In one dimension we find that the steady state of the macroscopic
landscape is governed by the Edwards-Wilkinson Hamiltonian, which implies that
the algorithm is scalable. Preliminary results for higher-dimensional logical
topologies are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0006316</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0006316</id><created>2000-06-21</created><authors><author><keyname>Hartmann</keyname><forenames>Alexander K.</forenames></author><author><keyname>Weigt</keyname><forenames>Martin</forenames></author></authors><title>Statistical mechanics perspective on the phase transition in vertex
  covering finite-connectivity random graphs</title><categories>cond-mat.stat-mech cs.CC</categories><comments>24 pages, 9 figures</comments><journal-ref>Theoretical Computer Science 265, 199 (2001)</journal-ref><abstract>  The vertex-cover problem is studied for random graphs $G_{N,cN}$ having $N$
vertices and $cN$ edges. Exact numerical results are obtained by a
branch-and-bound algorithm. It is found that a transition in the coverability
at a $c$-dependent threshold $x=x_c(c)$ appears, where $xN$ is the cardinality
of the vertex cover. This transition coincides with a sharp peak of the typical
numerical effort, which is needed to decide whether there exists a cover with
$xN$ vertices or not. For small edge concentrations $c\ll 0.5$, a cluster
expansion is performed, giving very accurate results in this regime. These
results are extended using methods developed in statistical physics. The so
called annealed approximation reproduces a rigorous bound on $x_c(c)$ which was
known previously. The main part of the paper contains an application of the
replica method. Within the replica symmetric ansatz the threshold $x_c(c)$ and
the critical backbone size $b_c(c)$ can be calculated. For $c&lt;e/2$ the results
show an excellent agreement with the numerical findings. At average vertex
degree $2c=e$, an instability of the simple replica symmetric solution occurs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0009165</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0009165</id><created>2000-09-11</created><updated>2002-02-04</updated><authors><author><keyname>Nemenman</keyname><forenames>Ilya</forenames></author><author><keyname>Bialek</keyname><forenames>William</forenames></author></authors><title>Occam factors and model-independent Bayesian learning of continuous
  distributions</title><categories>cond-mat cs.LG nlin.AO physics.data-an</categories><comments>publication revisions: extended introduction, new references, other
  minor corrections; 6 pages, 6 figures, revtex</comments><journal-ref>Phys. Rev. E, 65 (2), 2002</journal-ref><doi>10.1103/PhysRevE.65.026137</doi><abstract>  Learning of a smooth but nonparametric probability density can be regularized
using methods of Quantum Field Theory. We implement a field theoretic prior
numerically, test its efficacy, and show that the data and the phase space
factors arising from the integration over the model space determine the free
parameter of the theory (&quot;smoothness scale&quot;) self-consistently. This persists
even for distributions that are atypical in the prior and is a step towards a
model-independent theory for learning continuous distributions. Finally, we
point out that a wrong parameterization of a model family may sometimes be
advantageous for small data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0009417</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0009417</id><created>2000-09-27</created><updated>2000-11-28</updated><authors><author><keyname>Weigt</keyname><forenames>Martin</forenames></author><author><keyname>Hartmann</keyname><forenames>Alexander K.</forenames></author></authors><title>Typical solution time for a vertex-covering algorithm on
  finite-connectivity random graphs</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>4 pages, 2 figures, to appear in Phys. Rev. Lett</comments><journal-ref>Phys. Rev. Lett. 86, 1658 (2001)</journal-ref><doi>10.1103/PhysRevLett.86.1658</doi><abstract>  In this letter, we analytically describe the typical solution time needed by
a backtracking algorithm to solve the vertex-cover problem on
finite-connectivity random graphs. We find two different transitions: The first
one is algorithm-dependent and marks the dynamical transition from linear to
exponential solution times. The second one gives the maximum computational
complexity, and is found exactly at the threshold where the system undergoes an
algorithm-independent phase transition in its solvability. Analytical results
are corroborated by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0010337</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0010337</id><created>2000-10-23</created><updated>2001-04-08</updated><authors><author><keyname>Boettcher</keyname><forenames>S.</forenames><affiliation>Emory U.</affiliation></author><author><keyname>Percus</keyname><forenames>A. G.</forenames><affiliation>Los Alamos</affiliation></author></authors><title>Optimization with Extremal Dynamics</title><categories>cond-mat.stat-mech cs.NE math.OC</categories><comments>4 pages, RevTex4, 1 table and 3 ps-figures included, as to appear in
  PRL, related papers available at
  http://www.physics.emory.edu/faculty/boettcher/</comments><journal-ref>Phys. Rev. Lett, 86 (2001) 5211</journal-ref><doi>10.1103/PhysRevLett.86.5211</doi><abstract>  We explore a new general-purpose heuristic for finding high-quality solutions
to hard optimization problems. The method, called extremal optimization, is
inspired by self-organized criticality, a concept introduced to describe
emergent complexity in physical systems. Extremal optimization successively
replaces extremely undesirable variables of a single sub-optimal solution with
new, random ones. Large fluctuations ensue, that efficiently explore many local
optima. With only one adjustable parameter, the heuristic's performance has
proven competitive with more elaborate methods, especially near phase
transitions which are believed to coincide with the hardest instances. We use
extremal optimization to elucidate the phase transition in the 3-coloring
problem, and we provide independent confirmation of previously reported
extrapolations for the ground-state energy of +-J spin glasses in d=3 and 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0011181</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0011181</id><created>2000-11-10</created><updated>2000-12-21</updated><authors><author><keyname>Ricci-Tersenghi</keyname><forenames>F.</forenames></author><author><keyname>Weigt</keyname><forenames>M.</forenames></author><author><keyname>Zecchina</keyname><forenames>R.</forenames></author></authors><title>Simplest random K-satisfiability problem</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>14 pages, 5 figures, to appear in Phys. Rev. E (Feb 2001). v2: minor
  errors and references corrected</comments><journal-ref>Phys. Rev. E 63, 026702 (2001)</journal-ref><doi>10.1103/PhysRevE.63.026702</doi><abstract>  We study a simple and exactly solvable model for the generation of random
satisfiability problems. These consist of $\gamma N$ random boolean constraints
which are to be satisfied simultaneously by $N$ logical variables. In
statistical-mechanics language, the considered model can be seen as a diluted
p-spin model at zero temperature. While such problems become extraordinarily
hard to solve by local search methods in a large region of the parameter space,
still at least one solution may be superimposed by construction. The
statistical properties of the model can be studied exactly by the replica
method and each single instance can be analyzed in polynomial time by a simple
global solution method. The geometrical/topological structures responsible for
dynamic and static phase transitions as well as for the onset of computational
complexity in local search method are thoroughly analyzed. Numerical analysis
on very large samples allows for a precise characterization of the critical
scaling behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0103328</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0103328</id><created>2001-03-15</created><updated>2001-08-15</updated><authors><author><keyname>Franz</keyname><forenames>S.</forenames></author><author><keyname>Leone</keyname><forenames>M.</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>F.</forenames></author><author><keyname>Zecchina</keyname><forenames>R.</forenames></author></authors><title>Exact solutions for diluted spin glasses and optimization problems</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>4 pages, 1 figure, accepted for publication in PRL</comments><journal-ref>Phys. Rev. Lett. 87 (2001) 127209</journal-ref><doi>10.1103/PhysRevLett.87.127209</doi><abstract>  We study the low temperature properties of p-spin glass models with finite
connectivity and of some optimization problems. Using a one-step functional
replica symmetry breaking Ansatz we can solve exactly the saddle-point
equations for graphs with uniform connectivity. The resulting ground state
energy is in perfect agreement with numerical simulations. For fluctuating
connectivity graphs, the same Ansatz can be used in a variational way: For
p-spin models (known as p-XOR-SAT in computer science) it provides the exact
configurational entropy together with the dynamical and static critical
connectivities (for p=3, \gamma_d=0.818 and \gamma_s=0.918 resp.), whereas for
hard optimization problems like 3-SAT or Bicoloring it provides new upper
bounds for their critical thresholds (\gamma_c^{var}=4.396 and
\gamma_c^{var}=2.149 resp.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0104066</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0104066</id><created>2001-04-03</created><updated>2001-07-09</updated><authors><author><keyname>Montemurro</keyname><forenames>Marcelo A.</forenames></author></authors><title>Beyond the Zipf-Mandelbrot law in quantitative linguistics</title><categories>cond-mat.stat-mech cs.CL nlin.AO</categories><comments>6 pages and 7 figures; minor changes in text, added refereces</comments><doi>10.1016/S0378-4371(01)00355-7</doi><abstract>  In this paper the Zipf-Mandelbrot law is revisited in the context of
linguistics. Despite its widespread popularity the Zipf--Mandelbrot law can
only describe the statistical behaviour of a rather restricted fraction of the
total number of words contained in some given corpus. In particular, we focus
our attention on the important deviations that become statistically relevant as
larger corpora are considered and that ultimately could be understood as
salient features of the underlying complex process of language generation.
Finally, it is shown that all the different observed regimes can be accurately
encompassed within a single mathematical framework recently introduced by C.
Tsallis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0104214</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0104214</id><created>2001-04-11</created><authors><author><keyname>Boettcher</keyname><forenames>S.</forenames><affiliation>Emory U.</affiliation></author><author><keyname>Percus</keyname><forenames>A. G.</forenames><affiliation>Los Alamos</affiliation></author></authors><title>Extremal Optimization for Graph Partitioning</title><categories>cond-mat.stat-mech cs.NE math.OC</categories><comments>34 pages, RevTex4, 1 table and 20 ps-figures included, related papers
  available at http://www.physics.emory.edu/faculty/boettcher/</comments><journal-ref>Phys. Rev. E, 64 (2001) 026114</journal-ref><doi>10.1103/PhysRevE.64.026114</doi><abstract>  Extremal optimization is a new general-purpose method for approximating
solutions to hard optimization problems. We study the method in detail by way
of the NP-hard graph partitioning problem. We discuss the scaling behavior of
extremal optimization, focusing on the convergence of the average run as a
function of runtime and system size. The method has a single free parameter,
which we determine numerically and justify using a simple argument. Our
numerical results demonstrate that on random graphs, extremal optimization
maintains consistent accuracy for increasing system sizes, with an
approximation error decreasing over runtime roughly as a power law t^(-0.4). On
geometrically structured graphs, the scaling of results from the average run
suggests that these are far from optimal, with large fluctuations between
individual trials. But when only the best runs are considered, results
consistent with theoretical arguments are recovered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0106096</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0106096</id><created>2001-06-06</created><authors><author><keyname>Albert</keyname><forenames>Reka</forenames></author><author><keyname>Barabasi</keyname><forenames>Albert-Laszlo</forenames></author></authors><title>Statistical mechanics of complex networks</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.NI math-ph math.MP nlin.AO physics.data-an</categories><comments>54 pages, submitted to Reviews of Modern Physics</comments><journal-ref>Reviews of Modern Physics 74, 47 (2002)</journal-ref><doi>10.1103/RevModPhys.74.47</doi><abstract>  Complex networks describe a wide range of systems in nature and society, much
quoted examples including the cell, a network of chemicals linked by chemical
reactions, or the Internet, a network of routers and computers connected by
physical links. While traditionally these systems were modeled as random
graphs, it is increasingly recognized that the topology and evolution of real
networks is governed by robust organizing principles. Here we review the recent
advances in the field of complex networks, focusing on the statistical
mechanics of network topology and dynamics. After reviewing the empirical data
that motivated the recent interest in networks, we discuss the main models and
analytical tools, covering random graphs, small-world and scale-free networks,
as well as the interplay between topology and the network's robustness against
failures and attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0107212</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0107212</id><created>2001-07-11</created><authors><author><keyname>Puniyani</keyname><forenames>Amit R</forenames></author><author><keyname>Lukose</keyname><forenames>Rajan M</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A</forenames></author></authors><title>Intentional Walks on Scale Free Small Worlds</title><categories>cond-mat.soft cond-mat.stat-mech cs.NI physics.data-an</categories><comments>4 pages, 2 figures (10 eps files)</comments><abstract>  We present a novel algorithm that generates scale free small world graphs
such as those found in the World Wide Web,social and metabolic networks. We use
the generated graphs to study the dynamics of a realistic search strategy on
the graphs, and find that they can be navigated in a very short number of
steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0109121</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0109121</id><created>2001-09-06</created><authors><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author><author><keyname>Zimmermann</keyname><forenames>Joerg</forenames></author><author><keyname>Muehlenbein</keyname><forenames>Heinz</forenames></author></authors><title>Coordination of Decisions in a Spatial Agent Model</title><categories>cond-mat.stat-mech cs.MA</categories><comments>submitted for publication in Physica A (31 pages incl. 17 multi-part
  figures)</comments><journal-ref>Physica A, vol. 303, no. 1-2 (2002) pp. 189-216</journal-ref><doi>10.1016/S0378-4371(01)00486-1</doi><abstract>  For a binary choice problem, the spatial coordination of decisions in an
agent community is investigated both analytically and by means of stochastic
computer simulations. The individual decisions are based on different local
information generated by the agents with a finite lifetime and disseminated in
the system with a finite velocity. We derive critical parameters for the
emergence of minorities and majorities of agents making opposite decisions and
investigate their spatial organization. We find that dependent on two essential
parameters describing the local impact and the spatial dissemination of
information, either a definite stable minority/majority relation
(single-attractor regime) or a broad range of possible values (multi-attractor
regime) occurs. In the latter case, the outcome of the decision process becomes
rather diverse and hard to predict, both with respect to the share of the
majority and their spatial distribution. We further investigate how a
dissemination of information on different time scales affects the outcome of
the decision process. We find that a more ``efficient'' information exchange
within a subpopulation provides a suitable way to stabilize their majority
status and to reduce ``diversity'' and uncertainty in the decision process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0109218</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0109218</id><created>2001-09-12</created><authors><author><keyname>Montemurro</keyname><forenames>Marcelo A.</forenames></author><author><keyname>Zanette</keyname><forenames>Damian H.</forenames></author></authors><title>Entropic analysis of the role of words in literary texts</title><categories>cond-mat.stat-mech cs.CL</categories><comments>9 pages, 5 figures</comments><abstract>  Beyond the local constraints imposed by grammar, words concatenated in long
sequences carrying a complex message show statistical regularities that may
reflect their linguistic role in the message. In this paper, we perform a
systematic statistical analysis of the use of words in literary English
corpora. We show that there is a quantitative relation between the role of
content words in literary English and the Shannon information entropy defined
over an appropriate probability distribution. Without assuming any previous
knowledge about the syntactic structure of language, we are able to cluster
certain groups of words according to their specific role in the text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0109313</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0109313</id><created>2001-09-18</created><authors><author><keyname>Majumdar</keyname><forenames>Satya N.</forenames></author><author><keyname>Krapivsky</keyname><forenames>P. L.</forenames></author></authors><title>Extreme Value Statistics and Traveling Fronts: An Application to
  Computer Science</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.DS</categories><comments>16 pages, two-column revtex</comments><journal-ref>Phys. Rev. E 65, 036127 (2002)</journal-ref><doi>10.1103/PhysRevE.65.036127</doi><abstract>  We study the statistics of height and balanced height in the binary search
tree problem in computer science. The search tree problem is first mapped to a
fragmentation problem which is then further mapped to a modified directed
polymer problem on a Cayley tree. We employ the techniques of traveling fronts
to solve the polymer problem and translate back to derive exact asymptotic
properties in the original search tree problem. The second mapping allows us
not only to re-derive the already known results for random binary trees but to
obtain new exact results for search trees where the entries arrive according to
an arbitrary distribution, not necessarily randomly. Besides it allows us to
derive the asymptotic shape of the full probability distribution of height and
not just its moments. Our results are then generalized to $m$-ary search trees
with arbitrary distribution. An attempt has been made to make the article
accessible to both physicists and computer scientists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0110165</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0110165</id><created>2001-10-09</created><updated>2001-12-12</updated><authors><author><keyname>Boettcher</keyname><forenames>S.</forenames><affiliation>Emory U.</affiliation></author><author><keyname>Grigni</keyname><forenames>M.</forenames><affiliation>Emory U.</affiliation></author></authors><title>Jamming Model for the Extremal Optimization Heuristic</title><categories>cond-mat.stat-mech cs.NE physics.comp-ph</categories><comments>9 pages, RevTex4, 7 ps-figures included, as to appear in J. Phys. A,
  related papers available at http://www.physics.emory.edu/faculty/boettcher/</comments><journal-ref>Journal of Physics A: Math. Gen., 35 (2002) 1109</journal-ref><doi>10.1088/0305-4470/35/5/301</doi><abstract>  Extremal Optimization, a recently introduced meta-heuristic for hard
optimization problems, is analyzed on a simple model of jamming. The model is
motivated first by the problem of finding lowest energy configurations for a
disordered spin system on a fixed-valence graph. The numerical results for the
spin system exhibit the same phenomena found in all earlier studies of extremal
optimization, and our analytical results for the model reproduce many of these
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0111153</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0111153</id><created>2001-11-09</created><updated>2002-03-27</updated><authors><author><keyname>Barthel</keyname><forenames>W.</forenames></author><author><keyname>Hartmann</keyname><forenames>A. K.</forenames></author><author><keyname>Leone</keyname><forenames>M.</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>F.</forenames></author><author><keyname>Weigt</keyname><forenames>M.</forenames></author><author><keyname>Zecchina</keyname><forenames>R.</forenames></author></authors><title>Hiding solutions in random satisfiability problems: A statistical
  mechanics approach</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>5 pages, 4 figures, revised version to app. in PRL</comments><journal-ref>Phys. Rev. Lett. 88, 188701 (2002)</journal-ref><doi>10.1103/PhysRevLett.88.188701</doi><abstract>  A major problem in evaluating stochastic local search algorithms for
NP-complete problems is the need for a systematic generation of hard test
instances having previously known properties of the optimal solutions. On the
basis of statistical mechanics results, we propose random generators of hard
and satisfiable instances for the 3-satisfiability problem (3SAT). The design
of the hardest problem instances is based on the existence of a first order
ferromagnetic phase transition and the glassy nature of excited states. The
analytical predictions are corroborated by numerical results obtained from
complete as well as stochastic local algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0112103</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0112103</id><created>2001-12-06</created><authors><author><keyname>Korniss</keyname><forenames>G.</forenames></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames></author><author><keyname>Rikvold</keyname><forenames>P. A.</forenames></author><author><keyname>Guclu</keyname><forenames>H.</forenames></author><author><keyname>Toroczkai</keyname><forenames>Z.</forenames></author></authors><title>Going through Rough Times: from Non-Equilibrium Surface Growth to
  Algorithmic Scalability</title><categories>cond-mat.stat-mech cond-mat.mtrl-sci cs.DC cs.PF physics.comp-ph</categories><comments>to appear in the Proceedings of the MRS, Fall 2001</comments><journal-ref>Materials Research Society Symposium Proceedings Series Vol. 700,
  pp. 297-308, 2002</journal-ref><abstract>  Efficient and faithful parallel simulation of large asynchronous systems is a
challenging computational problem. It requires using the concept of local
simulated times and a synchronization scheme. We study the scalability of
massively parallel algorithms for discrete-event simulations which employ
conservative synchronization to enforce causality. We do this by looking at the
simulated time horizon as a complex evolving system, and we identify its
universal characteristics. We find that the time horizon for the conservative
parallel discrete-event simulation scheme exhibits Kardar-Parisi-Zhang-like
kinetic roughening. This implies that the algorithm is asymptotically scalable
in the sense that the average progress rate of the simulation approaches a
non-zero constant. It also implies, however, that there are diverging memory
requirements associated with such schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0112142</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0112142</id><created>2001-12-08</created><updated>2001-12-19</updated><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Zecchina</keyname><forenames>Riccardo</forenames></author></authors><title>Boosting search by rare events</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CC</categories><comments>4 pages, 3 eps figures. References updated</comments><journal-ref>Phys. Rev. Lett. 88, 178701 (2002)</journal-ref><doi>10.1103/PhysRevLett.88.178701</doi><abstract>  Randomized search algorithms for hard combinatorial problems exhibit a large
variability of performances. We study the different types of rare events which
occur in such out-of-equilibrium stochastic processes and we show how they
cooperate in determining the final distribution of running times. As a
byproduct of our analysis we show how search algorithms are optimized by random
restarts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0201139</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0201139</id><created>2002-01-09</created><authors><author><keyname>Montemurro</keyname><forenames>Marcelo A.</forenames></author><author><keyname>Pury</keyname><forenames>Pedro A.</forenames></author></authors><title>Long-range fractal correlations in literary corpora</title><categories>cond-mat.stat-mech cs.CL nlin.AO</categories><comments>to appear in Fractals</comments><journal-ref>Fractals 10(4), 451-461 (2002)</journal-ref><abstract>  In this paper we analyse the fractal structure of long human-language records
by mapping large samples of texts onto time series. The particular mapping set
up in this work is inspired on linguistic basis in the sense that is retains
{\em the word} as the fundamental unit of communication. The results confirm
that beyond the short-range correlations resulting from syntactic rules acting
at sentence level, long-range structures emerge in large written language
samples that give rise to long-range correlations in the use of words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0202190</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0202190</id><created>2002-02-11</created><authors><author><keyname>McGuire</keyname><forenames>Patrick C.</forenames></author><author><keyname>Bohr</keyname><forenames>Henrik</forenames></author><author><keyname>Clark</keyname><forenames>John W.</forenames></author><author><keyname>Haschke</keyname><forenames>Robert</forenames></author><author><keyname>Pershing</keyname><forenames>Chris</forenames></author><author><keyname>Rafelski</keyname><forenames>Johann</forenames></author></authors><title>Threshold Disorder as a Source of Diverse and Complex Behavior in Random
  Nets</title><categories>cond-mat.dis-nn cs.NE q-bio.NC</categories><comments>20 pages, 14 figures, submitted to Neural Networks</comments><journal-ref>Neural Networks,15(10), pp. 1243-1258 (2002)</journal-ref><abstract>  We study the diversity of complex spatio-temporal patterns in the behavior of
random synchronous asymmetric neural networks (RSANNs). Special attention is
given to the impact of disordered threshold values on limit-cycle diversity and
limit-cycle complexity in RSANNs which have `normal' thresholds by default.
Surprisingly, RSANNs exhibit only a small repertoire of rather complex
limit-cycle patterns when all parameters are fixed. This repertoire of complex
patterns is also rather stable with respect to small parameter changes. These
two unexpected results may generalize to the study of other complex systems. In
order to reach beyond this seemingly-disabling `stable and small' aspect of the
limit-cycle repertoire of RSANNs, we have found that if an RSANN has threshold
disorder above a critical level, then there is a rapid increase of the size of
the repertoire of patterns. The repertoire size initially follows a power-law
function of the magnitude of the threshold disorder. As the disorder increases
further, the limit-cycle patterns themselves become simpler until at a second
critical level most of the limit cycles become simple fixed points.
Nonetheless, for moderate changes in the threshold parameters, RSANNs are found
to display specific features of behavior desired for rapidly-responding
processing systems: accessibility to a large set of complex patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0202383</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0202383</id><created>2002-02-21</created><authors><author><keyname>Goodman</keyname><forenames>Joshua</forenames></author></authors><title>Extended Comment on Language Trees and Zipping</title><categories>cond-mat.stat-mech cs.CL cs.LG</categories><comments>6 pages</comments><abstract>  This is the extended version of a Comment submitted to Physical Review
Letters. I first point out the inappropriateness of publishing a Letter
unrelated to physics. Next, I give experimental results showing that the
technique used in the Letter is 3 times worse and 17 times slower than a simple
baseline. And finally, I review the literature, showing that the ideas of the
Letter are not novel. I conclude by suggesting that Physical Review Letters
should not publish Letters unrelated to physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0203227</identifier>
 <datestamp>2008-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0203227</id><created>2002-03-11</created><updated>2002-04-11</updated><authors><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Goltsev</keyname><forenames>A. V.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>Ising Model on Networks with an Arbitrary Distribution of Connections</title><categories>cond-mat.stat-mech cs.NI hep-lat hep-th math-ph math.MP nlin.SI physics.class-ph</categories><comments>5 pages</comments><journal-ref>Phys.Rev.E66:016104,2002</journal-ref><doi>10.1103/PhysRevE.66.016104</doi><abstract>  We find the exact critical temperature $T_c$ of the nearest-neighbor
ferromagnetic Ising model on an `equilibrium' random graph with an arbitrary
degree distribution $P(k)$. We observe an anomalous behavior of the
magnetization, magnetic susceptibility and specific heat, when $P(k)$ is
fat-tailed, or, loosely speaking, when the fourth moment of the distribution
diverges in infinite networks. When the second moment becomes divergent, $T_c$
approaches infinity, the phase transition is of infinite order, and size effect
is anomalously strong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0203436</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0203436</id><created>2002-03-21</created><authors><author><keyname>Sch&#xfc;rmann</keyname><forenames>Thomas</forenames></author><author><keyname>Grassberger</keyname><forenames>Peter</forenames></author></authors><title>Entropy estimation of symbol sequences</title><categories>cond-mat.stat-mech cs.CL cs.IT math.IT physics.data-an</categories><comments>14 pages, 13 figures, 2 tables</comments><journal-ref>CHAOS Vol. 6, No. 3 (1996) 414-427</journal-ref><doi>10.1063/1.166191</doi><abstract>  We discuss algorithms for estimating the Shannon entropy h of finite symbol
sequences with long range correlations. In particular, we consider algorithms
which estimate h from the code lengths produced by some compression algorithm.
Our interest is in describing their convergence with sequence length, assuming
no limits for the space and time complexities of the compression algorithms. A
scaling law is proposed for extrapolation from finite sample lengths. This is
applied to sequences of dynamical systems in non-trivial chaotic regimes, a 1-D
cellular automaton, and to written English texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0203591</identifier>
 <datestamp>2008-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0203591</id><created>2002-03-28</created><authors><author><keyname>Staliunas</keyname><forenames>Kestutis</forenames></author></authors><title>Anticorrelations and subdiffusion in financial systems</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CE q-fin.ST</categories><abstract>  Statistical dynamics of financial systems is investigated, based on a model
of a randomly coupled equation system driven by a stochastic Langevin force.
Anticorrelations of price returns, and subdiffusion of prices is found from the
model, and and compared with those calculated from historical $/EURO exchange
rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0204102</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0204102</id><created>2002-04-04</created><authors><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>Accelerated growth of networks</title><categories>cond-mat.stat-mech cs.NI</categories><comments>29 pages, 11 figures, a brief review of the accelerated growth
  including applications to econophysics, Contribution to `Handbook of Graphs
  and Networks: From the Genome to the Internet', eds. S. Bornholdt and H.G.
  Schuster (Wiley-VCH, Berlin, 2002), to be published</comments><abstract>  In many real growing networks the mean number of connections per vertex
increases with time. The Internet, the Word Wide Web, collaboration networks,
and many others display this behavior. Such a growth can be called {\em
accelerated}. We show that this acceleration influences distribution of
connections and may determine the structure of a network. We discuss general
consequences of the acceleration and demonstrate its features applying simple
illustrating examples. In particular, we show that the accelerated growth
fairly well explains the structure of the Word Web (the network of interacting
words of human language). Also, we use the models of the accelerated growth of
networks to describe a wealth condensation transition in evolving societies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0204111</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0204111</id><created>2002-04-04</created><updated>2002-12-29</updated><authors><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author><author><keyname>Samukhin</keyname><forenames>A. N.</forenames></author></authors><title>Principles of statistical mechanics of random networks</title><categories>cond-mat.stat-mech cs.NI hep-lat hep-th math-ph math.MP nlin.AO</categories><comments>14 pages, an extended version</comments><journal-ref>Nucl.Phys. B666 (2003) 396-416</journal-ref><doi>10.1016/S0550-3213(03)00504-2</doi><abstract>  We develop a statistical mechanics approach for random networks with
uncorrelated vertices. We construct equilibrium statistical ensembles of such
networks and obtain their partition functions and main characteristics. We find
simple dynamical construction procedures that produce equilibrium uncorrelated
random graphs with an arbitrary degree distribution. In particular, we show
that in equilibrium uncorrelated networks, fat-tailed degree distributions may
exist only starting from some critical average number of connections of a
vertex, in a phase with a condensate of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0204181</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0204181</id><created>2002-04-08</created><updated>2002-06-04</updated><authors><author><keyname>Adamic</keyname><forenames>Lada A.</forenames></author><author><keyname>Lukose</keyname><forenames>Rajan M.</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Local Search in Unstructured Networks</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.NI</categories><comments>v2 includes minor revisions: corrections to Fig. 8's caption and
  references. 23 pages, 10 figures, a review of local search strategies in
  unstructured networks, a contribution to `Handbook of Graphs and Networks:
  From the Genome to the Internet', eds. S. Bornholdt and H.G. Schuster
  (Wiley-VCH, Berlin, 2002), to be published</comments><abstract>  We review a number of message-passing algorithms that can be used to search
through power-law networks. Most of these algorithms are meant to be
improvements for peer-to-peer file sharing systems, and some may also shed some
light on how unstructured social networks with certain topologies might
function relatively efficiently with local information. Like the networks that
they are designed for, these algorithms are completely decentralized, and they
exploit the power-law link distribution in the node degree. We demonstrate that
some of these search algorithms can work well on real Gnutella networks, scale
sub-linearly with the number of nodes, and may help reduce the network search
traffic that tends to cripple such networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0205034</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0205034</id><created>2002-05-02</created><authors><author><keyname>Dean</keyname><forenames>David S.</forenames></author><author><keyname>Majumdar</keyname><forenames>Satya N.</forenames></author></authors><title>Phase Transition in a Random Fragmentation Problem with Applications to
  Computer Science</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.DS math.PR</categories><comments>5 pages RevTeX, 3 figures (.eps)</comments><journal-ref>J. Phys. A. 35, L501 (2002)</journal-ref><doi>10.1088/0305-4470/35/32/101</doi><abstract>  We study a fragmentation problem where an initial object of size x is broken
into m random pieces provided x&gt;x_0 where x_0 is an atomic cut-off.
Subsequently the fragmentation process continues for each of those daughter
pieces whose sizes are bigger than x_0. The process stops when all the
fragments have sizes smaller than x_0. We show that the fluctuation of the
total number of splitting events, characterized by the variance, generically
undergoes a nontrivial phase transition as one tunes the branching number m
through a critical value m=m_c. For m&lt;m_c, the fluctuations are Gaussian where
as for m&gt;m_c they are anomalously large and non-Gaussian. We apply this general
result to analyze two different search algorithms in computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0205336</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0205336</id><created>2002-05-16</created><authors><author><keyname>Majumdar</keyname><forenames>Satya N.</forenames></author><author><keyname>Dean</keyname><forenames>David S.</forenames></author></authors><title>Exact Solution of a Drop-push Model for Percolation</title><categories>cond-mat.stat-mech cs.DS math.PR</categories><comments>4 pages revtex, 2 eps figures</comments><journal-ref>Phys. Rev. Lett., 89, 115701 (2002)</journal-ref><doi>10.1103/PhysRevLett.89.115701</doi><abstract>  Motivated by a computer science algorithm known as `linear probing with
hashing' we study a new type of percolation model whose basic features include
a sequential `dropping' of particles on a substrate followed by their transport
via a `pushing' mechanism. Our exact solution in one dimension shows that,
unlike the ordinary random percolation model, the drop-push model has
nontrivial spatial correlations generated by the dynamics itself. The critical
exponents in the drop-push model are also different from that of the ordinary
percolation. The relevance of our results to computer science is pointed out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0206084</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0206084</id><created>2002-06-06</created><authors><author><keyname>Vazquez</keyname><forenames>A.</forenames></author><author><keyname>Pastor-Satorras</keyname><forenames>R.</forenames></author><author><keyname>Vespignani</keyname><forenames>A.</forenames></author></authors><title>Internet topology at the router and autonomous system level</title><categories>cond-mat.dis-nn cs.NI</categories><comments>8 pages, 6 figures, ACM style package</comments><abstract>  We present a statistical analysis of different metrics characterizing the
topological properties of Internet maps, collected at two different resolution
scales: the router and the autonomous system level. The metrics we consider
allow us to confirm the presence of scale-free signatures in several
statistical distributions, as well as to show in a quantitative way the
hierarchical nature of the Internet. Our findings are relevant for the
development of more accurate Internet topology generators, which should
include, along with the scale-free properties of the connectivity distribution,
the hierarchical signatures unveiled in the present work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0206410</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0206410</id><created>2002-06-21</created><updated>2002-10-21</updated><authors><author><keyname>Guimera</keyname><forenames>R.</forenames></author><author><keyname>Arenas</keyname><forenames>A.</forenames></author><author><keyname>Diaz-Guilera</keyname><forenames>A.</forenames></author><author><keyname>Vega-Redondo</keyname><forenames>F.</forenames></author><author><keyname>Cabrales</keyname><forenames>A.</forenames></author></authors><title>Optimal network topologies for local search with congestion</title><categories>cond-mat.dis-nn cs.NI</categories><comments>4 pages. Final version accepted in PRL</comments><journal-ref>Phys. Rev. Lett. 89, 248701 (2002)</journal-ref><doi>10.1103/PhysRevLett.89.248701</doi><abstract>  The problem of searchability in decentralized complex networks is of great
importance in computer science, economy and sociology. We present a formalism
that is able to cope simultaneously with the problem of search and the
congestion effects that arise when parallel searches are performed, and obtain
expressions for the average search cost--written in terms of the search
algorithm and the topological properties of the network--both in presence and
abscence of congestion. This formalism is used to obtain optimal network
structures for a system using a local search algorithm. It is found that only
two classes of networks can be optimal: star-like configurations, when the
number of parallel searches is small, and homogeneous-isotropic configurations,
when the number of parallel searches is large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0207035</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0207035</id><created>2002-07-01</created><updated>2002-11-16</updated><authors><author><keyname>Vazquez</keyname><forenames>Alexei</forenames></author><author><keyname>Weigt</keyname><forenames>Martin</forenames></author></authors><title>Computational complexity arising from degree correlations in networks</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>4 pages, 1 figure, accepted in Phys. Rev. E</comments><journal-ref>Phys. Rev. E 67, 027101 (2003)</journal-ref><doi>10.1103/PhysRevE.67.027101</doi><abstract>  We apply a Bethe-Peierls approach to statistical-mechanics models defined on
random networks of arbitrary degree distribution and arbitrary correlations
between the degrees of neighboring vertices. Using the NP-hard optimization
problem of finding minimal vertex covers on these graphs, we show that such
correlations may lead to a qualitatively different solution structure as
compared to uncorrelated networks. This results in a higher complexity of the
network in a computational sense: Simple heuristic algorithms fail to find a
minimal vertex cover in the highly correlated case, whereas uncorrelated
networks seem to be simple from the point of view of combinatorial
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0207140</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0207140</id><created>2002-07-04</created><updated>2002-09-19</updated><authors><author><keyname>Mezard</keyname><forenames>M.</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>F.</forenames></author><author><keyname>Zecchina</keyname><forenames>R.</forenames></author></authors><title>Alternative solutions to diluted p-spin models and XORSAT problems</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.DM</categories><comments>14 pages, 14 figures. v3: small errors corrected, simpler notation
  used</comments><journal-ref>J. Stat. Phys. 111, 505 (2003)</journal-ref><abstract>  We derive analytical solutions for p-spin models with finite connectivity at
zero temperature. These models are the statistical mechanics equivalent of
p-XORSAT problems in theoretical computer science. We give a full
characterization of the phase diagram: location of the phase transitions
(static and dynamic), together with a description of the clustering phenomenon
taking place in configurational space. We use two alternative methods: the
cavity approach and a rigorous derivation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0208414</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0208414</id><created>2002-08-21</created><updated>2004-11-02</updated><authors><author><keyname>Claussen</keyname><forenames>Jens Christian</forenames><affiliation>Theoretical Physics, University Kiel</affiliation></author></authors><title>Winner-Relaxing Self-Organizing Maps</title><categories>cond-mat.dis-nn cs.NE nlin.AO q-bio.NC</categories><comments>14 pages (6 figs included). To appear in Neural Computation</comments><journal-ref>Neural Computation 17 (5), 996-1009 (2005)</journal-ref><doi>10.1162/0899766053491922</doi><abstract>  A new family of self-organizing maps, the Winner-Relaxing Kohonen Algorithm,
is introduced as a generalization of a variant given by Kohonen in 1991. The
magnification behaviour is calculated analytically. For the original variant a
magnification exponent of 4/7 is derived; the generalized version allows to
steer the magnification in the wide range from exponent 1/2 to 1 in the
one-dimensional case, thus provides optimal mapping in the sense of information
theory. The Winner Relaxing Algorithm requires minimal extra computations per
learning step and is conveniently easy to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0208460</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0208460</id><created>2002-08-23</created><updated>2002-10-28</updated><authors><author><keyname>Mulet</keyname><forenames>R.</forenames></author><author><keyname>Pagnani</keyname><forenames>A.</forenames></author><author><keyname>Weigt</keyname><forenames>M.</forenames></author><author><keyname>Zecchina</keyname><forenames>R.</forenames></author></authors><title>Coloring random graphs</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CC</categories><comments>4 pages, 1 figure, version to app. in PRL</comments><journal-ref>Phys. Rev. Lett. 89, 268701 (2002)</journal-ref><doi>10.1103/PhysRevLett.89.268701</doi><abstract>  We study the graph coloring problem over random graphs of finite average
connectivity $c$. Given a number $q$ of available colors, we find that graphs
with low connectivity admit almost always a proper coloring whereas graphs with
high connectivity are uncolorable. Depending on $q$, we find the precise value
of the critical average connectivity $c_q$. Moreover, we show that below $c_q$
there exist a clustering phase $c\in [c_d,c_q]$ in which ground states
spontaneously divide into an exponential number of clusters and where the
proliferation of metastable states is responsible for the onset of complexity
in local search algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0208488</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0208488</id><created>2002-08-26</created><authors><author><keyname>Fekete</keyname><forenames>A.</forenames></author><author><keyname>Marodi</keyname><forenames>M.</forenames></author><author><keyname>Vattay</keyname><forenames>G.</forenames></author></authors><title>On the Prospects of Chaos Aware Traffic Modeling</title><categories>cond-mat.dis-nn cs.NI nlin.CD</categories><comments>11 pages</comments><abstract>  In this paper the chaotic properties of the TCP congestion avoidance
mechanism are investigated. The analysis focuses on the origin of the complex
behavior appearing in deterministic TCP/IP networks. From the traffic modeling
point of view the understanding of the mechanism generating chaos is essential,
since present models are unable to cope with this phenomena. Using the basic
tools of chaos theory in our study, the main characteristics of chaotic
dynamics are revealed. The dynamics of packet loss events is studied by a
simple symbolic description. The cellular structure of the phase space of
congestion windows is shown. This implies periodic behavior for large time
scales. Chaotic behavior in short time scales and periodicity for larger times
makes it necessary to develop models that account for both. Thus a simple model
that describes the congestion window dynamics according to fluid equations, but
handles the packet loss events separately is introduced. This model can
reproduce the basic features observed in realistic packet level simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0209111</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0209111</id><created>2002-09-04</created><authors><author><keyname>Gudkov</keyname><forenames>Vladimir</forenames></author><author><keyname>Johnson</keyname><forenames>Joseph E.</forenames></author><author><keyname>Nussinov</keyname><forenames>Shmuel</forenames></author></authors><title>Approaches to Network Classification</title><categories>cond-mat.stat-mech cs.DM hep-ph math.CO physics.comp-ph</categories><comments>RevTeX4</comments><abstract>  We introduce a novel approach to description of networks/graphs. It is based
on an analogue physical model which is dynamically evolved. This evolution
depends on the connectivity matrix and readily brings out many qualitative
features of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0209112</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0209112</id><created>2002-09-04</created><updated>2002-09-06</updated><authors><author><keyname>Gudkov</keyname><forenames>Vladimir</forenames></author><author><keyname>Nussinov</keyname><forenames>Shmuel</forenames></author></authors><title>Graph equivalence and characterization via a continuous evolution of a
  physical analog</title><categories>cond-mat.stat-mech cs.DM hep-ph math.CO physics.comp-ph</categories><comments>RevTeX4</comments><abstract>  A general novel approach mapping discrete, combinatorial, graph-theoretic
problems onto ``physical'' models - namely $n$ simplexes in $n-1$ dimensions -
is applied to the graph equivalence problem. It is shown to solve this long
standing problem in polynomial, short, time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0209419</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0209419</id><created>2002-09-17</created><authors><author><keyname>Gudkov</keyname><forenames>Vladimir</forenames></author><author><keyname>Nussinov</keyname><forenames>Shmuel</forenames></author><author><keyname>Nussinov</keyname><forenames>Zohar</forenames></author></authors><title>A Novel Approach Applied to the Largest Clique Problem</title><categories>cond-mat cs.DM math.CO physics.comp-ph</categories><comments>20 pages, 15 figures</comments><abstract>  A novel approach to complex problems has been previously applied to graph
classification and the graph equivalence problem. Here we apply it to the NP
complete problem of finding the largest perfect clique within a graph $G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0211605</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0211605</id><created>2002-11-26</created><authors><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author><author><keyname>Behera</keyname><forenames>Laxmidhar</forenames></author><author><keyname>Muehlenbein</keyname><forenames>Heinz</forenames></author></authors><title>Evolution of Cooperation in a Spatial Prisoner's Dilemma</title><categories>cond-mat.stat-mech cs.GT nlin.CG</categories><comments>33 pages, 22 multipart figures, for related papers see
  http://www.ais.fraunhofer.de/~frank/papers.html</comments><abstract>  We investigate the spatial distribution and the global frequency of agents
who can either cooperate or defect. The agent interaction is described by a
deterministic, non-iterated prisoner's dilemma game, further each agent only
locally interacts with his neighbors. Based on a detailed analysis of the local
payoff structures we derive critical conditions for the invasion or the spatial
coexistence of cooperators and defectors. These results are concluded in a
phase diagram that allows to identify five regimes, each characterized by a
distinct spatiotemporal dynamics and a corresponding final spatial structure.
In addition to the complete invasion of defectors, we find coexistence regimes
with either a majority of cooperators in large spatial domains, or a minority
of cooperators organized in small non-stationary domains or in small clusters.
The analysis further allowed a verification of computer simulation results by
Nowak and May (1993). Eventually, we present simulation results of a true
5-person game on a lattice. This modification leads to non-uniform spatial
interactions that may even enhance the effect of cooperation. Keywords:
Prisoner's dilemma; cooperation; spatial 5-person game
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0212451</identifier>
 <datestamp>2010-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0212451</id><created>2002-12-18</created><updated>2003-09-27</updated><authors><author><keyname>Braunstein</keyname><forenames>A.</forenames></author><author><keyname>Mezard</keyname><forenames>M.</forenames></author><author><keyname>Weigt</keyname><forenames>M.</forenames></author><author><keyname>Zecchina</keyname><forenames>R.</forenames></author></authors><title>Constraint Satisfaction by Survey Propagation</title><categories>cond-mat.dis-nn cs.CC</categories><comments>8 pages, 5 figures</comments><journal-ref>Advances in Neural Information Processing Systems. Vol 9. Oxford
  University Press; 2005. 424</journal-ref><abstract>  Survey Propagation is an algorithm designed for solving typical instances of
random constraint satisfiability problems. It has been successfully tested on
random 3-SAT and random $G(n,\frac{c}{n})$ graph 3-coloring, in the hard region
of the parameter space. Here we provide a generic formalism which applies to a
wide class of discrete Constraint Satisfaction Problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0301035</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0301035</id><created>2003-01-03</created><updated>2003-08-13</updated><authors><author><keyname>Aldous</keyname><forenames>David</forenames></author><author><keyname>Percus</keyname><forenames>Allon G.</forenames></author></authors><title>Scaling and Universality in Continuous Length Combinatorial Optimization</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.DM</categories><comments>5 pages; 3 figures</comments><report-no>LA-UR-02-7322</report-no><doi>10.1073/pnas.1635191100</doi><abstract>  We consider combinatorial optimization problems defined over random
ensembles, and study how solution cost increases when the optimal solution
undergoes a small perturbation delta. For the minimum spanning tree, the
increase in cost scales as delta^2; for the mean-field and Euclidean minimum
matching and traveling salesman problems in dimension d&gt;=2, the increase scales
as delta^3; this is observed in Monte Carlo simulations in d=2,3,4 and in
theoretical analysis of a mean-field model. We speculate that the scaling
exponent could serve to classify combinatorial optimization problems into a
small number of distinct categories, similar to universality classes in
statistical physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0301271</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0301271</id><created>2003-01-15</created><updated>2003-05-07</updated><authors><author><keyname>Barthel</keyname><forenames>Wolfgang</forenames></author><author><keyname>Hartmann</keyname><forenames>Alexander K.</forenames></author><author><keyname>Weigt</keyname><forenames>Martin</forenames></author></authors><title>Solving satisfiability problems by fluctuations: The dynamics of
  stochastic local search algorithms</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CC</categories><comments>21 pages, 18 figures, revised version, to app. in PRE (2003)</comments><journal-ref>Phys. Rev. E 67, 066104 (2003)</journal-ref><doi>10.1103/PhysRevE.67.066104</doi><abstract>  Stochastic local search algorithms are frequently used to numerically solve
hard combinatorial optimization or decision problems. We give numerical and
approximate analytical descriptions of the dynamics of such algorithms applied
to random satisfiability problems. We find two different dynamical regimes,
depending on the number of constraints per variable: For low constraintness,
the problems are solved efficiently, i.e. in linear time. For higher
constraintness, the solution times become exponential. We observe that the
dynamical behavior is characterized by a fast equilibration and fluctuations
around this equilibrium. If the algorithm runs long enough, an exponentially
rare fluctuation towards a solution appears.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0301272</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0301272</id><created>2003-01-15</created><updated>2003-01-29</updated><authors><author><keyname>Semerjian</keyname><forenames>Guilhem</forenames></author><author><keyname>Monasson</keyname><forenames>Remi</forenames></author></authors><title>Relaxation and Metastability in the RandomWalkSAT search procedure</title><categories>cond-mat cs.CC</categories><comments>23 pages, 11 figures. A mistake in sec. IV.B has been corrected</comments><report-no>LPT-ENS 02/66</report-no><journal-ref>Phys. Rev. E 67, 066103 (2003)</journal-ref><doi>10.1103/PhysRevE.67.066103</doi><abstract>  An analysis of the average properties of a local search resolution procedure
for the satisfaction of random Boolean constraints is presented. Depending on
the ratio alpha of constraints per variable, resolution takes a time T_res
growing linearly (T_res \sim tau(alpha) N, alpha &lt; alpha_d) or exponentially
(T_res \sim exp(N zeta(alpha)), alpha &gt; alpha_d) with the size N of the
instance. The relaxation time tau(alpha) in the linear phase is calculated
through a systematic expansion scheme based on a quantum formulation of the
evolution operator. For alpha &gt; alpha_d, the system is trapped in some
metastable state, and resolution occurs from escape from this state through
crossing of a large barrier. An annealed calculation of the height zeta(alpha)
of this barrier is proposed. The polynomial/exponentiel cross-over alpha_d is
not related to the onset of clustering among solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0301307</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0301307</id><created>2003-01-16</created><authors><author><keyname>Tsallis</keyname><forenames>Constantino</forenames></author><author><keyname>Anteneodo</keyname><forenames>Celia</forenames></author><author><keyname>Borland</keyname><forenames>Lisa</forenames></author><author><keyname>Osorio</keyname><forenames>Roberto</forenames></author></authors><title>Nonextensive statistical mechanics and economics</title><categories>cond-mat.stat-mech cs.CE q-fin.ST</categories><comments>6 pages and 6 figures. Contribution to the International Econophysics
  Conference, held in Bali, Indonesia (29-31 August 2002). To appear in Physica
  A. It includes a partial reply to a recent criticism by D.H. Zanette and M.A.
  Montemurro, cond-mat/0212327</comments><journal-ref>Physica A 324, 89 (2003).</journal-ref><doi>10.1016/S0378-4371(03)00042-6</doi><abstract>  Ergodicity, this is to say, dynamics whose time averages coincide with
ensemble averages, naturally leads to Boltzmann-Gibbs (BG) statistical
mechanics, hence to standard thermodynamics. This formalism has been at the
basis of an enormous success in describing, among others, the particular
stationary state corresponding to thermal equilibrium. There are, however, vast
classes of complex systems which accomodate quite badly, or even not at all,
within the BG formalism. Such dynamical systems exhibit, in one way or another,
nonergodic aspects. In order to be able to theoretically study at least some of
these systems, a formalism was proposed 14 years ago, which is sometimes
referred to as nonextensive statistical mechanics. We briefly introduce this
formalism, its foundations and applications. Furthermore, we provide some
bridging to important economical phenomena, such as option pricing, return and
volume distributions observed in the financial markets, and the fascinating and
ubiquitous concept of risk aversion. One may summarize the whole approach by
saying that BG statistical mechanics is based on the entropy $S_{BG}=-k \sum_i
p_i \ln p_i$, and typically provides {\it exponential laws} for describing
stationary states and basic time-dependent phenomena, while nonextensive
statistical mechanics is instead based on the entropic form
$S_q=k(1-\sum_ip_i^q)/(q-1)$ (with $S_1=S_{BG}$), and typically provides, for
the same type of description, (asymptotic) {\it power laws}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0301459</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0301459</id><created>2003-01-23</created><authors><author><keyname>Tumer</keyname><forenames>Kagan</forenames></author><author><keyname>Wolpert</keyname><forenames>David</forenames></author></authors><title>Collectives for the Optimal Combination of Imperfect Objects</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.MA nlin.AO</categories><comments>4 pages</comments><abstract>  In this letter we summarize some recent theoretical work on the design of
collectives, i.e., of systems containing many agents, each of which can be
viewed as trying to maximize an associated private utility, where there is also
a world utility rating the behavior of that overall system that the designer of
the collective wishes to optimize. We then apply algorithms based on that work
on a recently suggested testbed for such optimization problems (Challet &amp;
Johnson, PRL, vol 89, 028701 2002). This is the problem of finding the
combination of imperfect nano-scale objects that results in the best aggregate
object. We present experimental results showing that these algorithms
outperform conventional methods by more than an order of magnitude in this
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0302050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0302050</id><created>2003-02-03</created><authors><author><keyname>Korniss</keyname><forenames>G.</forenames></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames></author><author><keyname>Guclu</keyname><forenames>H.</forenames></author><author><keyname>Toroczkai</keyname><forenames>Z.</forenames></author><author><keyname>Rikvold</keyname><forenames>P. A.</forenames></author></authors><title>Suppressing Roughness of Virtual Times in Parallel Discrete-Event
  Simulations</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.DC physics.comp-ph</categories><journal-ref>Science 299, 677 (2003)</journal-ref><doi>10.1126/science.1079382</doi><abstract>  In a parallel discrete-event simulation (PDES) scheme, tasks are distributed
among processing elements (PEs), whose progress is controlled by a
synchronization scheme. For lattice systems with short-range interactions, the
progress of the conservative PDES scheme is governed by the Kardar-Parisi-Zhang
equation from the theory of non-equilibrium surface growth. Although the
simulated (virtual) times of the PEs progress at a nonzero rate, their standard
deviation (spread) diverges with the number of PEs, hindering efficient data
collection. We show that weak random interactions among the PEs can make this
spread nondivergent. The PEs then progress at a nonzero, near-uniform rate
without requiring global synchronizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0302536</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0302536</id><created>2003-02-26</created><authors><author><keyname>Borgs</keyname><forenames>C.</forenames></author><author><keyname>Chayes</keyname><forenames>J. T.</forenames></author><author><keyname>Mertens</keyname><forenames>S.</forenames></author><author><keyname>Pittel</keyname><forenames>B.</forenames></author></authors><title>Phase Diagram for the Constrained Integer Partitioning Problem</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC math.PR</categories><comments>62 pages, 8 figures</comments><abstract>  We consider the problem of partitioning $n$ integers into two subsets of
given cardinalities such that the discrepancy, the absolute value of the
difference of their sums, is minimized. The integers are i.i.d. random
variables chosen uniformly from the set $\{1,...,M\}$. We study how the typical
behavior of the optimal partition depends on $n,M$ and the bias $s$, the
difference between the cardinalities of the two subsets in the partition. In
particular, we rigorously establish this typical behavior as a function of the
two parameters $\kappa:=n^{-1}\log_2M$ and $b:=|s|/n$ by proving the existence
of three distinct ``phases'' in the $\kappa b$-plane, characterized by the
value of the discrepancy and the number of optimal solutions: a ``perfect
phase'' with exponentially many optimal solutions with discrepancy 0 or 1; a
``hard phase'' with minimal discrepancy of order $Me^{-\Theta(n)}$; and a
``sorted phase'' with an unique optimal partition of order $Mn$, obtained by
putting the $(s+n)/2$ smallest integers in one subset. Our phase diagram covers
all but a relatively small region in the $\kappa b$-plane. We also show that
the three phases can be alternatively characterized by the number of basis
solutions of the associated linear programming problem, and by the fraction of
these basis solutions whose $\pm 1$-valued components form optimal integer
partitions of the subproblem with the corresponding weights. We show in
particular that this fraction is one in the sorted phase, and exponentially
small in both the perfect and hard phases, and strictly exponentially smaller
in the hard phase than in the perfect phase. Open problems are discussed, and
numerical experiments are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0303089</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0303089</id><created>2003-03-05</created><updated>2004-12-27</updated><authors><author><keyname>Gontis</keyname><forenames>Vygintas</forenames></author><author><keyname>Kaulakys</keyname><forenames>Bronislovas</forenames></author></authors><title>Multiplicative point process as a model of trading activity</title><categories>cond-mat.stat-mech cs.CE math.SP nlin.AO nlin.CD q-fin.TR</categories><comments>10 pages, 3 figures</comments><journal-ref>Gontis V., Kaulakys B., Physica A 343 (2004) 505-514</journal-ref><doi>10.1016/j.physa.2004.05.080</doi><abstract>  Signals consisting of a sequence of pulses show that inherent origin of the
1/f noise is a Brownian fluctuation of the average interevent time between
subsequent pulses of the pulse sequence. In this paper we generalize the model
of interevent time to reproduce a variety of self-affine time series exhibiting
power spectral density S(f) scaling as a power of the frequency f. Furthermore,
we analyze the relation between the power-law correlations and the origin of
the power-law probability distribution of the signal intensity. We introduce a
stochastic multiplicative model for the time intervals between point events and
analyze the statistical properties of the signal analytically and numerically.
Such model system exhibits power-law spectral density S(f)~1/f**beta for
various values of beta, including beta=1/2, 1 and 3/2. Explicit expressions for
the power spectra in the low frequency limit and for the distribution density
of the interevent time are obtained. The counting statistics of the events is
analyzed analytically and numerically, as well. The specific interest of our
analysis is related with the financial markets, where long-range correlations
of price fluctuations largely depend on the number of transactions. We analyze
the spectral density and counting statistics of the number of transactions. The
model reproduces spectral properties of the real markets and explains the
mechanism of power-law distribution of trading activity. The study provides
evidence that the statistical properties of the financial markets are enclosed
in the statistics of the time interval between trades. A multiplicative point
process serves as a consistent model generating this statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0304132</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0304132</id><created>2003-04-05</created><authors><author><keyname>Ting</keyname><forenames>Juhi-Lian Julian</forenames></author></authors><title>Causalities of the Taiwan Stock Market</title><categories>cond-mat.stat-mech cs.CE q-fin.ST</categories><comments>8 pages, 15 figures</comments><report-no>almost Bali 2002, Physica A 2003</report-no><journal-ref>Physica A 324, 285-295 (2003)</journal-ref><doi>10.1016/S0378-4371(02)01842-3</doi><abstract>  Volatility, fitting with first order Landau expansion, stationarity, and
causality of the Taiwan stock market (TAIEX) are investigated based on daily
records. Instead of consensuses that consider stock market index change as a
random time series we propose the market change as a dual time series consists
of the index and the corresponding volume. Therefore, causalities between these
two time series are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0305097</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0305097</id><created>2003-05-06</created><authors><author><keyname>Majumdar</keyname><forenames>Satya N.</forenames></author></authors><title>Traveling Front Solutions to Directed Diffusion Limited Aggregation,
  Digital Search Trees and the Lempel-Ziv Data Compression Algorithm</title><categories>cond-mat.stat-mech cs.DS</categories><comments>Revtex, 11 pages, 3 .eps figures included</comments><journal-ref>Phys. Rev. E 68, 026103 (2003)</journal-ref><doi>10.1103/PhysRevE.68.026103</doi><abstract>  We use the traveling front approach to derive exact asymptotic results for
the statistics of the number of particles in a class of directed diffusion
limited aggregation models on a Cayley tree. We point out that some aspects of
these models are closely connected to two different problems in computer
science, namely the digital search tree problem in data structures and the
Lempel-Ziv algorithm for data compression. The statistics of the number of
particles studied here is related to the statistics of height in digital search
trees which, in turn, is related to the statistics of the length of the longest
word formed by the Lempel-Ziv algorithm. Implications of our results to these
computer science problems are pointed out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0305508</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0305508</id><created>2003-05-21</created><authors><author><keyname>Gorban</keyname><forenames>A. N.</forenames></author><author><keyname>Rossiev</keyname><forenames>A. A.</forenames></author><author><keyname>Wunsch</keyname><forenames>D. C.</forenames><suffix>II</suffix></author></authors><title>Neural network modeling of data with gaps: method of principal curves,
  Carleman's formula, and other</title><categories>cond-mat.dis-nn cs.NE physics.data-an</categories><comments>28 pages, 7 figures,The talk was given at the USA-NIS Neurocomputing
  opportunities workshop, Washington DC, July 1999 (Associated with IJCNN'99)</comments><abstract>  A method of modeling data with gaps by a sequence of curves has been
developed. The new method is a generalization of iterative construction of
singular expansion of matrices with gaps. Under discussion are three versions
of the method featuring clear physical interpretation: linear - modeling the
data by a sequence of linear manifolds of small dimension; quasilinear -
constructing &quot;principal curves: (or &quot;principal surfaces&quot;), univalently
projected on the linear principal components; essentially non-linear - based on
constructing &quot;principal curves&quot;: (principal strings and beams) employing the
variation principle; the iteration implementation of this method is close to
Kohonen self-organizing maps. The derived dependencies are extrapolated by
Carleman's formulas. The method is interpreted as a construction of neural
network conveyor designed to solve the following problems: to fill gaps in
data; to repair data - to correct initial data values in such a way as to make
the constructed models work best; to construct a calculator to fill gaps in the
data line fed to the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0305527</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0305527</id><created>2003-05-22</created><updated>2004-11-15</updated><authors><author><keyname>Senashova</keyname><forenames>M. Yu.</forenames></author><author><keyname>Gorban</keyname><forenames>A. N.</forenames></author><author><keyname>Wunsch</keyname><forenames>D. C.</forenames><suffix>II</suffix></author></authors><title>Back-propagation of accuracy</title><categories>cond-mat.dis-nn cs.NA cs.NE</categories><comments>4 pages, 5 figures, The talk given on ICNN97 (The 1997 IEEE
  International Conference on Neural Networks, Houston, USA)</comments><abstract>  In this paper we solve the problem: how to determine maximal allowable
errors, possible for signals and parameters of each element of a network
proceeding from the condition that the vector of output signals of the network
should be calculated with given accuracy? &quot;Back-propagation of accuracy&quot; is
developed to solve this problem. The calculation of allowable errors for each
element of network by back-propagation of accuracy is surprisingly similar to a
back-propagation of error, because it is the backward signals motion, but at
the same time it is very different because the new rules of signals
transformation in the passing back through the elements are different. The
method allows us to formulate the requirements to the accuracy of calculations
and to the realization of technical devices, if the requirements to the
accuracy of output signals of the network are known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0305575</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0305575</id><created>2003-05-23</created><updated>2003-10-13</updated><authors><author><keyname>Myers</keyname><forenames>C. R.</forenames></author></authors><title>Software systems as complex networks: structure, function, and
  evolvability of software collaboration graphs</title><categories>cond-mat.stat-mech cs.SE</categories><comments>16 pages, 8 figures; changed content, corrected typos</comments><journal-ref>Phys. Rev. E 68, 046116 (2003)</journal-ref><doi>10.1103/PhysRevE.68.046116</doi><abstract>  Software systems emerge from mere keystrokes to form intricate functional
networks connecting many collaborating modules, objects, classes, methods, and
subroutines. Building on recent advances in the study of complex networks, I
have examined software collaboration graphs contained within several
open-source software systems, and have found them to reveal scale-free,
small-world networks similar to those identified in other technological,
sociological, and biological systems. I present several measures of these
network topologies, and discuss their relationship to software engineering
practices. I also present a simple model of software system evolution based on
refactoring processes which captures some of the salient features of the
observed systems. Some implications of object-oriented design for questions
about network robustness, evolvability, degeneracy, and organization are
discussed in the wake of these findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0305582</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0305582</id><created>2003-05-25</created><authors><author><keyname>Cohen</keyname><forenames>R.</forenames></author><author><keyname>Dolev</keyname><forenames>D.</forenames></author><author><keyname>Havlin</keyname><forenames>S.</forenames></author><author><keyname>Kalisky</keyname><forenames>T.</forenames></author><author><keyname>Mokryn</keyname><forenames>O.</forenames></author><author><keyname>Shavitt</keyname><forenames>Y.</forenames></author></authors><title>On the Tomography of Networks and Multicast Trees</title><categories>cond-mat cond-mat.dis-nn cs.NI</categories><report-no>TR2002-49 HUJI (2002)</report-no><doi>10.1103/PhysRevE.74.066108</doi><abstract>  In this paper we model the tomography of scale free networks by studying the
structure of layers around an arbitrary network node. We find, both
analytically and empirically, that the distance distribution of all nodes from
a specific network node consists of two regimes. The first is characterized by
rapid growth, and the second decays exponentially. We also show that the nodes
degree distribution at each layer is a power law with an exponential cut-off.
We obtain similar results for the layers surrounding the root of multicast
trees cut from such networks, as well as the Internet. All of our results were
obtained both analytically and on empirical Interenet data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0305681</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0305681</id><created>2003-05-29</created><updated>2004-11-23</updated><authors><author><keyname>Gorban</keyname><forenames>A. N.</forenames></author><author><keyname>Zinovyev</keyname><forenames>A. Yu.</forenames></author><author><keyname>Popova</keyname><forenames>T. G.</forenames></author></authors><title>Seven clusters in genomic triplet distributions</title><categories>cond-mat.dis-nn cs.CV physics.bio-ph physics.data-an q-bio.GN</categories><comments>Correction of URL. 16 pages, 5 figures. The software and datasets are
  available at http://www.ihes.fr/~zinovyev/bullet and
  http://www.ihes.fr/~zinovyev/7clusters Paper also available at
  http://www.bioinfo.de/isb/2003/03/0039</comments><journal-ref>In Silico Biology, 3 (2003), 0039, 471-482</journal-ref><abstract>  In several recent papers new gene-detection algorithms were proposed for
detecting protein-coding regions without requiring learning dataset of already
known genes. The fact that unsupervised gene-detection is possible closely
connected to existence of a cluster structure in oligomer frequency
distributions. In this paper we study cluster structure of several genomes in
the space of their triplet frequencies, using pure data exploration strategy.
Several complete genomic sequences were analyzed, using visualization of tables
of triplet frequencies in a sliding window. The distribution of 64-dimensional
vectors of triplet frequencies displays a well-detectable cluster structure.
The structure was found to consist of seven clusters, corresponding to
protein-coding information in three possible phases in one of the two
complementary strands and in the non-coding regions with high accuracy (higher
than 90% on the nucleotide level). Visualizing and understanding the structure
allows to analyze effectively performance of different gene-prediction tools.
Since the method does not require extraction of ORFs, it can be applied even
for unassembled genomes. The information content of the triplet distributions
and the validity of the mean-field models are analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0306222</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0306222</id><created>2003-06-09</created><authors><author><keyname>Kolakowska</keyname><forenames>A.</forenames></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames></author><author><keyname>Rikvold</keyname><forenames>Per Arne</forenames></author></authors><title>Update statistics in conservative parallel discrete event simulations of
  asynchronous systems</title><categories>cond-mat cs.DC</categories><comments>15 pages, 12 figures</comments><acm-class>F.1.1;F.1.2;G.3;I.6;C.4;B.4.4</acm-class><journal-ref>Phys. Rev. E 68, 046705 (2003)</journal-ref><doi>10.1103/PhysRevE.68.046705</doi><abstract>  We model the performance of an ideal closed chain of L processing elements
that work in parallel in an asynchronous manner. Their state updates follow a
generic conservative algorithm. The conservative update rule determines the
growth of a virtual time surface. The physics of this growth is reflected in
the utilization (the fraction of working processors) and in the interface
width. We show that it is possible to nake an explicit connection between the
utilization and the macroscopic structure of the virtual time interface. We
exploit this connection to derive the theoretical probability distribution of
updates in the system within an approximate model. It follows that the
theoretical lower bound for the computational speed-up is s=(L+1)/4 for L&gt;3.
Our approach uses simple statistics to count distinct surface configuration
classes consistent with the model growth rule. It enables one to compute
analytically microscopic properties of an interface, which are unavailable by
continuum methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0306509</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0306509</id><created>2003-06-19</created><updated>2003-11-24</updated><authors><author><keyname>Challet</keyname><forenames>Damien</forenames></author><author><keyname>Lombardoni</keyname><forenames>Andrea</forenames></author></authors><title>Bug propagation and debugging in asymmetric software structures</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.SE</categories><comments>4 pages, 4 figures</comments><doi>10.1103/PhysRevE.70.046109</doi><abstract>  Software dependence networks are shown to be scale-free and asymmetric. We
then study how software components are affected by the failure of one of them,
and the inverse problem of locating the faulty component. Software at all
levels is fragile with respect to the failure of a random single component.
Locating a faulty component is easy if the failures only affect their nearest
neighbors, while it is hard if the failures propagate further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0306511</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0306511</id><created>2003-06-19</created><updated>2005-09-05</updated><authors><author><keyname>Challet</keyname><forenames>Damien</forenames></author><author><keyname>Du</keyname><forenames>Yann Le</forenames></author></authors><title>Closed source versus open source in a model of software bug dynamics</title><categories>cond-mat.stat-mech cs.SE</categories><comments>13 pages, 8 figures. Better introduction of the model. New section of
  analytical results. Added a discussion on how to fit the model to real data.
  To appear in International Journal of Reliability, Quality and Safety
  Engineering</comments><abstract>  We introduce a simple microscopic description of software bug dynamics where
users, programmers and a maintainer interact through a given program, with a
particular emphasis on bug creation, detection and fixing. When the program is
written from scratch, the first phase of development is characterized by a fast
decline of the number of bugs, followed by a slow phase where most bugs have
been fixed, hence, are hard to find. Releasing immediately bug fixes speeds up
the debugging process, which substantiates bazaar open-source methodology. We
provide a mathematical analysis that supports our numerical simulations.
Finally, we apply our model to Linux history and determine the existence of a
lower bound to the quality of its programmers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0306609</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0306609</id><created>2003-06-24</created><authors><author><keyname>de Moura</keyname><forenames>Alessandro P. S.</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author><author><keyname>Motter</keyname><forenames>Adilson E.</forenames></author></authors><title>Signatures of small-world and scale-free properties in large computer
  programs</title><categories>cond-mat.dis-nn cs.GL</categories><comments>4 pages, 1 figure, to appear in Phys. Rev. E</comments><journal-ref>Phys. Rev. E 68, 017102 (2003)</journal-ref><doi>10.1103/PhysRevE.68.017102</doi><abstract>  A large computer program is typically divided into many hundreds or even
thousands of smaller units, whose logical connections define a network in a
natural way. This network reflects the internal structure of the program, and
defines the ``information flow'' within the program. We show that, (1) due to
its growth in time this network displays a scale-free feature in that the
probability of the number of links at a node obeys a power-law distribution,
and (2) as a result of performance optimization of the program the network has
a small-world structure. We believe that these features are generic for large
computer programs. Our work extends the previous studies on growing networks,
which have mostly been for physical networks, to the domain of computer
software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0307058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0307058</id><created>2003-07-02</created><authors><author><keyname>Seyed-allaei</keyname><forenames>Hamed</forenames></author></authors><title>Finite size scaling approach to dynamic storage allocation problem</title><categories>cond-mat.stat-mech cs.DS</categories><comments>9 pages, 4 figures, will apear in Physica A</comments><journal-ref>Physica A: Statistical Mechanics and its Applications, Volume 327,
  Issues 3-4, 15 September 2003, Pages 563-569</journal-ref><doi>10.1016/S0378-4371(03)00509-0</doi><abstract>  It is demonstrated how dynamic storage allocation algorithms can be analyzed
in terms of finite size scaling. The method is illustrated in the three simple
cases of the it first-fit, next-fit and it best-fit algorithms, and the system
works at full capacity. The analysis is done from two different points of view
- running speed and employed memory. In both cases, and for all algorithms, it
is shown that a simple scaling function exists and the relevant exponents are
calculated. The method can be applied on similar problems as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0307083</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0307083</id><created>2003-07-03</created><authors><author><keyname>Gorban</keyname><forenames>A. N.</forenames></author><author><keyname>Mirkes</keyname><forenames>Eu. M.</forenames></author><author><keyname>Tsaregorodtsev</keyname><forenames>V. G.</forenames></author></authors><title>Generation of Explicit Knowledge from Empirical Data through Pruning of
  Trainable Neural Networks</title><categories>cond-mat cs.NE physics.data-an</categories><comments>9 pages, The talk was given at the IJCNN '99 (Washington DC, July
  1999)</comments><abstract>  This paper presents a generalized technology of extraction of explicit
knowledge from data. The main ideas are 1) maximal reduction of network
complexity (not only removal of neurons or synapses, but removal all the
unnecessary elements and signals and reduction of the complexity of elements),
2) using of adjustable and flexible pruning process (the pruning sequence
shouldn't be predetermined - the user should have a possibility to prune
network on his own way in order to achieve a desired network structure for the
purpose of extraction of rules of desired type and form), and 3) extraction of
rules not in predetermined but any desired form. Some considerations and notes
about network architecture and training process and applicability of currently
developed pruning techniques and rule extraction algorithms are discussed. This
technology, being developed by us for more than 10 years, allowed us to create
dozens of knowledge-based expert systems. In this paper we present a
generalized three-step technology of extraction of explicit knowledge from
empirical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0307201</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0307201</id><created>2003-07-09</created><authors><author><keyname>Gorshenev</keyname><forenames>A. A.</forenames><affiliation>Department of Theoretical Physics State University of Saint-Petersburg, Saint-Petersburg, Russia</affiliation></author><author><keyname>Pis'mak</keyname><forenames>Yu. M.</forenames><affiliation>Department of Theoretical Physics State University of Saint-Petersburg, Saint-Petersburg, Russia</affiliation></author></authors><title>Punctuated Equilibrium in Software Evolution</title><categories>cond-mat.stat-mech cs.SE</categories><comments>4 pages, LaTeX, 2 Postscript figures</comments><doi>10.1103/PhysRevE.70.067103</doi><abstract>  The approach based on paradigm of self-organized criticality proposed for
experimental investigation and theoretical modelling of software evolution. The
dynamics of modifications studied for three free, open source programs Mozilla,
Free-BSD and Emacs using the data from version control systems. Scaling laws
typical for the self-organization criticality found. The model of software
evolution presenting the natural selection principle is proposed. The results
of numerical and analytical investigation of the model are presented. They are
in a good agreement with the data collected for the real-world software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0307630</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0307630</id><created>2003-07-24</created><authors><author><keyname>Wolpert</keyname><forenames>David H.</forenames></author></authors><title>Product Distribution Field Theory</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.MA nlin.AO</categories><comments>4 pages, submitted</comments><abstract>  This paper presents a novel way to approximate a distribution governing a
system of coupled particles with a product of independent distributions. The
approach is an extension of mean field theory that allows the independent
distributions to live in a different space from the system, and thereby capture
statistical dependencies in that system. It also allows different Hamiltonians
for each independent distribution, to facilitate Monte Carlo estimation of
those distributions. The approach leads to a novel energy-minimization
algorithm in which each coordinate Monte Carlo estimates an associated
spectrum, and then independently sets its state by sampling a Boltzmann
distribution across that spectrum. It can also be used for high-dimensional
numerical integration, (constrained) combinatorial optimization, and adaptive
distributed control. This approach also provides a simple, physics-based
derivation of the powerful approximate energy-minimization algorithms
semi-formally derived in \cite{wowh00, wotu02c, wolp03a}. In addition it
suggests many improvements to those algorithms, and motivates a new (bounded
rationality) game theory equilibrium concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0308147</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0308147</id><created>2003-08-07</created><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>Federico</forenames></author></authors><title>Instability of one-step replica-symmetry-broken phase in satisfiability
  problems</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>26 pages, 7 eps figures</comments><journal-ref>J. Phys. A 37, 2073 (2004)</journal-ref><doi>10.1088/0305-4470/37/6/008</doi><abstract>  We reconsider the one-step replica-symmetry-breaking (1RSB) solutions of two
random combinatorial problems: k-XORSAT and k-SAT. We present a general method
for establishing the stability of these solutions with respect to further steps
of replica-symmetry breaking. Our approach extends the ideas of [A.Montanari
and F. Ricci-Tersenghi, Eur.Phys.J. B 33, 339 (2003)] to more general
combinatorial problems.
  It turns out that 1RSB is always unstable at sufficiently small clauses
density alpha or high energy. In particular, the recent 1RSB solution to 3-SAT
is unstable at zero energy for alpha&lt; alpha_m, with alpha_m\approx 4.153. On
the other hand, the SAT-UNSAT phase transition seems to be correctly described
within 1RSB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0308288</identifier>
 <datestamp>2008-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0308288</id><created>2003-08-14</created><authors><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author><author><keyname>Fall</keyname><forenames>Kevin</forenames></author><author><keyname>Yang</keyname><forenames>Xiaowei</forenames></author></authors><title>Compact Routing on Internet-Like Graphs</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.NI</categories><comments>29 pages, 16 figures</comments><report-no>IRB-TR-03-10</report-no><journal-ref>INFOCOM 2004</journal-ref><doi>10.1109/INFCOM.2004.1354495</doi><abstract>  The Thorup-Zwick (TZ) routing scheme is the first generic stretch-3 routing
scheme delivering a nearly optimal local memory upper bound. Using both direct
analysis and simulation, we calculate the stretch distribution of this routing
scheme on random graphs with power-law node degree distributions, $P_k \sim
k^{-\gamma}$. We find that the average stretch is very low and virtually
independent of $\gamma$. In particular, for the Internet interdomain graph,
$\gamma \sim 2.1$, the average stretch is around 1.1, with up to 70% of paths
being shortest. As the network grows, the average stretch slowly decreases. The
routing table is very small, too. It is well below its upper bounds, and its
size is around 50 records for $10^4$-node networks. Furthermore, we find that
both the average shortest path length (i.e. distance) $\bar{d}$ and width of
the distance distribution $\sigma$ observed in the real Internet inter-AS graph
have values that are very close to the minimums of the average stretch in the
$\bar{d}$- and $\sigma$-directions. This leads us to the discovery of a unique
critical quasi-stationary point of the average TZ stretch as a function of
$\bar{d}$ and $\sigma$. The Internet distance distribution is located in a
close neighborhood of this point. This observation suggests the analytical
structure of the average stretch function may be an indirect indicator of some
hidden optimization criteria influencing the Internet's interdomain topology
evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0310227</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0310227</id><created>2003-10-09</created><authors><author><keyname>Achlioptas</keyname><forenames>Dimitris</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Random k-SAT: Two Moments Suffice to Cross a Sharp Threshold</title><categories>cond-mat.stat-mech cs.CC math.CO math.PR</categories><abstract>  Many NP-complete constraint satisfaction problems appear to undergo a &quot;phase
transition'' from solubility to insolubility when the constraint density passes
through a critical threshold. In all such cases it is easy to derive upper
bounds on the location of the threshold by showing that above a certain density
the first moment (expectation) of the number of solutions tends to zero. We
show that in the case of certain symmetric constraints, considering the second
moment of the number of solutions yields nearly matching lower bounds for the
location of the threshold. Specifically, we prove that the threshold for both
random hypergraph 2-colorability (Property B) and random Not-All-Equal k-SAT is
2^{k-1} ln 2 -O(1). As a corollary, we establish that the threshold for random
k-SAT is of order Theta(2^k), resolving a long-standing open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0310600</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0310600</id><created>2003-10-26</created><authors><author><keyname>Wu</keyname><forenames>Fang</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Finding Communities in Linear Time: A Physics Approach</title><categories>cond-mat.stat-mech cs.DS</categories><doi>10.1140/epjb/e2004-00125-x</doi><abstract>  We present a method that allows for the discovery of communities within
graphs of arbitrary size in times that scale linearly with their size. This
method avoids edge cutting and is based on notions of voltage drops across
networks that are both intuitive and easy to solve regardless of the complexity
of the graph involved. We additionally show how this algorithm allows for the
swift discovery of the community surrounding a given node without having to
extract all the communities out of a graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0311552</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0311552</id><created>2003-11-24</created><authors><author><keyname>Ben-Naim</keyname><forenames>E.</forenames></author><author><keyname>Krapivsky</keyname><forenames>P. L.</forenames></author><author><keyname>Redner</keyname><forenames>S.</forenames></author></authors><title>Extremal Properties of Random Structures</title><categories>cond-mat.stat-mech cs.DS math.PR</categories><comments>24 pages, 8 figures, review</comments><journal-ref>Lecture Notes in Physics 650, 211 (2004)</journal-ref><doi>10.1007/b98716</doi><abstract>  The extremal characteristics of random structures, including trees, graphs,
and networks, are discussed. A statistical physics approach is employed in
which extremal properties are obtained through suitably defined rate equations.
A variety of unusual time dependences and system-size dependences for basic
extremal properties are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0312019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0312019</id><created>2003-12-01</created><authors><author><keyname>Kokado</keyname><forenames>Satoshi</forenames></author><author><keyname>Harigaya</keyname><forenames>Kikuo</forenames></author></authors><title>A theoretical investigation of ferromagnetic tunnel junctions with
  4-valued conductances</title><categories>cond-mat.mes-hall cond-mat.mtrl-sci cs.CE physics.ins-det quant-ph</categories><comments>9 pages, 3 figures, accepted for publication in J. Phys.: Condens.
  Matter</comments><journal-ref>J. Phys.: Condens. Matter 15 (2003) 8797-8804</journal-ref><doi>10.1088/0953-8984/15/50/012</doi><abstract>  In considering a novel function in ferromagnetic tunnel junctions consisting
of ferromagnet(FM)/barrier/FM junctions, we theoretically investigate multiple
valued (or multi-level) cell property, which is in principle realized by
sensing conductances of four states recorded with magnetization configurations
of two FMs; that is, (up,up), (up,down), (down,up), (down,down). To obtain such
4-valued conductances, we propose FM1/spin-polarized barrier/FM2 junctions,
where the FM1 and FM2 are different ferromagnets, and the barrier has spin
dependence. The proposed idea is applied to the case of the barrier having
localized spins. Assuming that all the localized spins are pinned parallel to
magnetization axes of the FM1 and FM2, 4-valued conductances are explicitly
obtained for the case of many localized spins. Furthermore, objectives for an
ideal spin-polarized barrier are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0312483</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0312483</id><created>2003-12-18</created><updated>2004-06-08</updated><authors><author><keyname>Braunstein</keyname><forenames>A.</forenames></author><author><keyname>Zecchina</keyname><forenames>R.</forenames></author></authors><title>Survey Propagation as local equilibrium equations</title><categories>cond-mat.dis-nn cs.CC</categories><comments>13 pages, 3 figures</comments><journal-ref>J. Stat. Mech., P06007 (2004)</journal-ref><doi>10.1088/1742-5468/2004/06/P06007</doi><abstract>  It has been shown experimentally that a decimation algorithm based on Survey
Propagation (SP) equations allows to solve efficiently some combinatorial
problems over random graphs. We show that these equations can be derived as
sum-product equations for the computation of marginals in an extended space
where the variables are allowed to take an additional value -- $*$ -- when they
are not forced by the combinatorial constraints. An appropriate ``local
equilibrium condition'' cost/energy function is introduced and its entropy is
shown to coincide with the expected logarithm of the number of clusters of
solutions as computed by SP. These results may help to clarify the geometrical
notion of clusters assumed by SP for the random K-SAT or random graph coloring
(where it is conjectured to be exact) and helps to explain which kind of
clustering operation or approximation is enforced in general/small sized models
in which it is known to be inexact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0312603</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0312603</id><created>2003-12-23</created><updated>2005-03-03</updated><authors><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author><author><keyname>Travieso</keyname><forenames>Gonzalo</forenames></author><author><keyname>Ruggiero</keyname><forenames>Carlos Antonio</forenames></author></authors><title>Complex Grid Computing</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.DC</categories><comments>5 pages, 2 figures</comments><journal-ref>Eur. Phys. J. B, 44 (2005) p.119</journal-ref><doi>10.1140/epjb/e2005-00107-6</doi><abstract>  This article investigates the performance of grid computing systems whose
interconnections are given by random and scale-free complex network models.
Regular networks, which are common in parallel computing architectures, are
also used as a standard for comparison. The processing load is assigned to the
processing nodes on demand, and the efficiency of the overall computing is
quantified in terms of the respective speed-ups. It is found that random
networks allow higher computing efficiency than their scale-free counterparts
as a consequence of the smaller number of isolated clusters implied by the
former model. At the same time, for fixed cluster sizes, the scale free model
tend to provide slightly better efficiency. Two modifications of the random and
scale free paradigms, where new connections tend to favor more recently added
nodes, are proposed and shown to be more effective for grid computing than the
standard models. A well-defined correlation is observed between the topological
properties of the network and their respective computing efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0401065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0401065</id><created>2004-01-06</created><authors><author><keyname>Petermann</keyname><forenames>Thomas</forenames></author><author><keyname>Rios</keyname><forenames>Paolo De Los</forenames></author></authors><title>Exploration of scale-free networks</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.NI</categories><comments>To appear in Eur. Phys. J. B; 5 pages, 4 figures</comments><journal-ref>Eur. Phys. J. B 38, 201 (2004)</journal-ref><doi>10.1140/epjb/e2004-00021-5</doi><abstract>  The increased availability of data on real networks has favoured an explosion
of activity in the elaboration of models able to reproduce both qualitatively
and quantitatively the measured properties. What has been less explored is the
reliability of the data, and whether the measurement technique biases them.
Here we show that tree-like explorations (similar in principle to traceroute)
can indeed change the measured exponents of a scale-free network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0401191</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0401191</id><created>2004-01-12</created><updated>2004-10-19</updated><authors><author><keyname>Shafee</keyname><forenames>Fariel</forenames></author></authors><title>Chaos and Annealing in Social networks</title><categories>cond-mat.dis-nn cs.GT nlin.AO</categories><comments>New simulations added. Accepted for Verhulst 200, 2004</comments><abstract>  In this work we compare social clusters with spin clusters and compare
different properties. We also try to compare phase changes in market and labor
stratification with phase changes of spin clusters. Then we compare the
requisites for redrawing the boundaries of social clusters with respect to
energy minimization and efficiency. We finally do a simulation experiment and
show that by choosing suitable link matrices for agents and attributes of the
same and of different agents it is possible to have at the same time behavior
similar to chaos or punctuated equilibrium in some attributes or fairly regular
oscillations of preferences for other attributes, using greatest utility or
efficiency as a criterion for change in conflicting social networks with
different agents having different preferences with respect to the attributes in
the agent himself or with similar attributes in other agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0401229</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0401229</id><created>2004-01-14</created><authors><author><keyname>Shchur</keyname><forenames>L. N.</forenames></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames></author></authors><title>On the Evolution of Time Horizons in Parallel and Grid Simulations</title><categories>cond-mat.stat-mech cs.DC</categories><comments>24 pages with 11 figures</comments><journal-ref>Phys. Rev. E 70 (2004) 026703</journal-ref><doi>10.1103/PhysRevE.70.026703</doi><abstract>  We analyze the evolution of the local simulation times (LST) in Parallel
Discrete Event Simulations. The new ingredients introduced are i) we associate
the LST with the nodes and not with the processing elements, and 2) we propose
to minimize the exchange of information between different processing elements
by freezing the LST on the boundaries between processing elements for some time
of processing and then releasing them by a wide-stream memory exchange between
processing elements. Highlights of our approach are i) it keeps the highest
level of processor time utilization during the algorithm evolution, ii) it
takes a reasonable time for the memory exchange excluding the time-consuming
and complicated process of message exchange between processors, and iii) the
communication between processors is decoupled from the calculations performed
on a processor. The effectiveness of our algorithm grows with the number of
nodes (or threads). This algorithm should be applicable for any parallel
simulation with short-range interactions, including parallel or grid
simulations of partial differential equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0402143</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0402143</id><created>2004-02-04</created><authors><author><keyname>Boykin</keyname><forenames>P. Oscar</forenames></author><author><keyname>Roychowdhury</keyname><forenames>Vwani</forenames></author></authors><title>Personal Email Networks: An Effective Anti-Spam Tool</title><categories>cond-mat.dis-nn cs.NI</categories><comments>8 pages, 7 figures</comments><journal-ref>IEEE Computer, Vol. 38, No. 4, pages 61-68, April 2005</journal-ref><abstract>  We provide an automated graph theoretic method for identifying individual
users' trusted networks of friends in cyberspace. We routinely use our social
networks to judge the trustworthiness of outsiders, i.e., to decide where to
buy our next car, or to find a good mechanic for it. In this work, we show that
an email user may similarly use his email network, constructed solely from
sender and recipient information available in the email headers, to distinguish
between unsolicited commercial emails, commonly called &quot;spam&quot;, and emails
associated with his circles of friends. We exploit the properties of social
networks to construct an automated anti-spam tool which processes an individual
user's personal email network to simultaneously identify the user's core
trusted networks of friends, as well as subnetworks generated by spams. In our
empirical studies of individual mail boxes, our algorithm classified
approximately 53% of all emails as spam or non-spam, with 100% accuracy. Some
of the emails are left unclassified by this network analysis tool. However, one
can exploit two of the following useful features. First, it requires no user
intervention or supervised training; second, it results in no false negatives
i.e., spam being misclassified as non-spam, or vice versa. We demonstrate that
these two features suggest that our algorithm may be used as a platform for a
comprehensive solution to the spam problem when used in concert with more
sophisticated, but more cumbersome, content-based filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0402268</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0402268</id><created>2004-02-10</created><authors><author><keyname>Berger</keyname><forenames>N.</forenames><affiliation>Microsoft Research, Redmond WA, USA</affiliation></author><author><keyname>Borgs</keyname><forenames>C.</forenames><affiliation>Microsoft Research, Redmond WA, USA</affiliation></author><author><keyname>Chayes</keyname><forenames>J. T.</forenames><affiliation>Microsoft Research, Redmond WA, USA</affiliation></author><author><keyname>D'Souza</keyname><forenames>R. M.</forenames><affiliation>Microsoft Research, Redmond WA, USA</affiliation></author><author><keyname>Kleinberg</keyname><forenames>R. D.</forenames><affiliation>M.I.T. CSAIL, Cambridge MA, USA</affiliation></author></authors><title>Competition-Induced Preferential Attachment</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.NI</categories><comments>Submitted to Intnl. Colloq. on Automata, Languages and Programming
  (ICALP 2004)</comments><journal-ref>Proceedings of the 31st International Colloquium on Automata,
  Languages and Programming, 208-221 (2004).</journal-ref><abstract>  Models based on preferential attachment have had much success in reproducing
the power law degree distributions which seem ubiquitous in both natural and
engineered systems. Here, rather than assuming preferential attachment, we give
an explanation of how it can arise from a more basic underlying mechanism of
competition between opposing forces.
  We introduce a family of one-dimensional geometric growth models, constructed
iteratively by locally optimizing the tradeoffs between two competing metrics.
This family admits an equivalent description as a graph process with no
reference to the underlying geometry. Moreover, the resulting graph process is
shown to be preferential attachment with an upper cutoff. We rigorously
determine the degree distribution for the family of random graph models,
showing that it obeys a power law up to a finite threshold and decays
exponentially above this threshold.
  We also introduce and rigorously analyze a generalized version of our graph
process, with two natural parameters, one corresponding to the cutoff and the
other a ``fertility'' parameter. Limiting cases of this process include the
standard Barabasi-Albert preferential attachment model and the uniform
attachment model. In the general case, we prove that the process has a power
law degree distribution up to a cutoff, and establish monotonicity of the power
as a function of the two parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0402508</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0402508</id><created>2004-02-19</created><authors><author><keyname>Wolpert</keyname><forenames>David H.</forenames></author></authors><title>Information Theory - The Bridge Connecting Bounded Rational Game Theory
  and Statistical Physics</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.GT cs.MA nlin.AO</categories><comments>17 pages, no figures, accepted for publication</comments><abstract>  A long-running difficulty with conventional game theory has been how to
modify it to accommodate the bounded rationality of all real-world players. A
recurring issue in statistical physics is how best to approximate joint
probability distributions with decoupled (and therefore far more tractable)
distributions. This paper shows that the same information theoretic
mathematical structure, known as Product Distribution (PD) theory, addresses
both issues. In this, PD theory not only provides a principled formulation of
bounded rationality and a set of new types of mean field theory in statistical
physics. It also shows that those topics are fundamentally one and the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0402581</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0402581</id><created>2004-02-24</created><updated>2004-09-14</updated><authors><author><keyname>Baronchelli</keyname><forenames>A.</forenames></author><author><keyname>Caglioti</keyname><forenames>E.</forenames></author><author><keyname>Loreto</keyname><forenames>V.</forenames></author><author><keyname>Pizzi</keyname><forenames>E.</forenames></author></authors><title>Dictionary based methods for information extraction</title><categories>cond-mat.stat-mech cond-mat.other cs.IR q-bio.GN q-bio.OT</categories><comments>7 pages, Latex, elsart style</comments><journal-ref>Physica A - Vol 342/1-2 pp 294-300 (2004)</journal-ref><doi>10.1016/j.physa.2004.01.072</doi><abstract>  In this paper we present a general method for information extraction that
exploits the features of data compression techniques. We first define and focus
our attention on the so-called &quot;dictionary&quot; of a sequence. Dictionaries are
intrinsically interesting and a study of their features can be of great
usefulness to investigate the properties of the sequences they have been
extracted from (e.g. DNA strings). We then describe a procedure of string
comparison between dictionary-created sequences (or &quot;artificial texts&quot;) that
gives very good results in several contexts. We finally present some results on
self-consistent classification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0403233</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0403233</id><created>2004-03-09</created><updated>2006-01-25</updated><authors><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Caglioti</keyname><forenames>Emanuele</forenames></author><author><keyname>Loreto</keyname><forenames>Vittorio</forenames></author></authors><title>Artificial Sequences and Complexity Measures</title><categories>cond-mat.stat-mech cs.CL cs.IR cs.IT math.IT</categories><comments>Revised version, with major changes, of previous &quot;Data Compression
  approach to Information Extraction and Classification&quot; by A. Baronchelli and
  V. Loreto. 15 pages; 5 figures</comments><journal-ref>J. Stat. Mech., P04002 (2005)</journal-ref><doi>10.1088/1742-5468/2005/04/P04002</doi><abstract>  In this paper we exploit concepts of information theory to address the
fundamental problem of identifying and defining the most suitable tools to
extract, in a automatic and agnostic way, information from a generic string of
characters. We introduce in particular a class of methods which use in a
crucial way data compression techniques in order to define a measure of
remoteness and distance between pairs of sequences of characters (e.g. texts)
based on their relative information content. We also discuss in detail how
specific features of data compression techniques could be used to introduce the
notion of dictionary of a given sequence and of Artificial Text and we show how
these new tools can be used for information extraction purposes. We point out
the versatility and generality of our method that applies to any kind of
corpora of character strings independently of the type of coding behind them.
We consider as a case study linguistic motivated problems and we present
results for automatic language recognition, authorship attribution and self
consistent-classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0403341</identifier>
 <datestamp>2009-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0403341</id><created>2004-03-13</created><updated>2004-07-09</updated><authors><author><keyname>Kolakowska</keyname><forenames>A.</forenames></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames></author><author><keyname>Verma</keyname><forenames>P. S.</forenames></author></authors><title>Roughening of the (1+1) interfaces in two-component surface growth with
  an admixture of random deposition</title><categories>cond-mat.mtrl-sci cs.DC cs.OH physics.comp-ph</categories><comments>16 pages, 16 figures, 77 references</comments><journal-ref>Phys. Rev. E, Vol.70, 051602 (2004)</journal-ref><doi>10.1103/PhysRevE.70.051602</doi><abstract>  We simulate competitive two-component growth on a one dimensional substrate
of $L$ sites. One component is a Poisson-type deposition that generates
Kardar-Parisi-Zhang (KPZ) correlations. The other is random deposition (RD). We
derive the universal scaling function of the interface width for this model and
show that the RD admixture acts as a dilatation mechanism to the fundamental
time and height scales, but leaves the KPZ correlations intact. This
observation is generalized to other growth models. It is shown that the
flat-substrate initial condition is responsible for the existence of an early
non-scaling phase in the interface evolution. The length of this initial phase
is a non-universal parameter, but its presence is universal. In application to
parallel and distributed computations, the important consequence of the derived
scaling is the existence of the upper bound for the desynchronization in a
conservative update algorithm for parallel discrete-event simulations. It is
shown that such algorithms are generally scalable in a ring communication
topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0403453</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0403453</id><created>2004-03-17</created><authors><author><keyname>Ben-Naim</keyname><forenames>E.</forenames></author><author><keyname>Krapivsky</keyname><forenames>P. L.</forenames></author></authors><title>Unicyclic Components in Random Graphs</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.DS math.PR</categories><comments>4 pages, 2 figures</comments><journal-ref>J. Phys. A 37, L189 (2004)</journal-ref><doi>10.1088/0305-4470/37/18/L01</doi><abstract>  The distribution of unicyclic components in a random graph is obtained
analytically. The number of unicyclic components of a given size approaches a
self-similar form in the vicinity of the gelation transition. At the gelation
point, this distribution decays algebraically, U_k ~ 1/(4k) for k&gt;&gt;1. As a
result, the total number of unicyclic components grows logarithmically with the
system size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0403725</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0403725</id><created>2004-03-30</created><updated>2004-07-28</updated><authors><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Pagnani</keyname><forenames>Andrea</forenames></author><author><keyname>Weigt</keyname><forenames>Martin</forenames></author></authors><title>Threshold values, stability analysis and high-q asymptotics for the
  coloring problem on random graphs</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>23 pages, 10 figures. Replaced with accepted version</comments><journal-ref>Phys. Rev. E 70, 046705 (2004)</journal-ref><doi>10.1103/PhysRevE.70.046705</doi><abstract>  We consider the problem of coloring Erdos-Renyi and regular random graphs of
finite connectivity using q colors. It has been studied so far using the cavity
approach within the so-called one-step replica symmetry breaking (1RSB) ansatz.
We derive a general criterion for the validity of this ansatz and, applying it
to the ground state, we provide evidence that the 1RSB solution gives exact
threshold values c_q for the q-COL/UNCOL phase transition. We also study the
asymptotic thresholds for q &gt;&gt; 1 finding c_q = 2qlog(q)-log(q)-1+o(1) in
perfect agreement with rigorous mathematical bounds, as well as the nature of
excited states, and give a global phase diagram of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0404424</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0404424</id><created>2004-04-18</created><updated>2005-04-18</updated><authors><author><keyname>Lipowski</keyname><forenames>Adam</forenames></author><author><keyname>Lipowska</keyname><forenames>Dorota</forenames></author></authors><title>Travelling Salesman Problem with a Center</title><categories>cond-mat.stat-mech cs.CC</categories><comments>4 pages, minor changes, accepted in Phys.Rev.E</comments><journal-ref>Phys. Rev. E 71, 067701 (2005)</journal-ref><doi>10.1103/PhysRevE.71.067701</doi><abstract>  We study a travelling salesman problem where the path is optimized with a
cost function that includes its length $L$ as well as a certain measure $C$ of
its distance from the geometrical center of the graph. Using simulated
annealing (SA) we show that such a problem has a transition point that
separates two phases differing in the scaling behaviour of $L$ and $C$, in
efficiency of SA, and in the shape of minimal paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0404593</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0404593</id><created>2004-04-24</created><updated>2004-07-24</updated><authors><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>The shortest path to complex networks</title><categories>cond-mat.stat-mech cs.NI nlin.AO q-bio.MN</categories><comments>25 pages, a contribution to `Complex Systems and Inter-disciplinary
  Science', v.1, eds. N. Johnson, J. Efstathiou, and F. Reed-Tsochas (World
  Scientific, 2004), to be published</comments><abstract>  1. The birth of network science. 2. What are random networks? 3. Adjacency
matrix. 4. Degree distribution. 5. What are simple networks? Classical random
graphs. 6. Birth of the giant component. 7. Topology of the Web. 8.Uncorrelated
networks. 9. What are small worlds? 10. Real networks are mesoscopic objects.
11. What are complex networks? 12. The configuration model. 13. The absence of
degree--degree correlations. 14.Networks with correlated degrees.15.Clustering.
16. What are small-world networks? 17. `Small worlds' is not the same as
`small-world networks'. 18. Fat-tailed degree distributions. 19.Reasons for the
fat-tailed degree distributions. 20. Preferential linking. 21. Condensation of
edges. 22. Cut-offs of degree distributions. 23. Reasons for correlations in
networks. 24. Classical random graphs cannot be used for comparison with real
networks. 25. How to measure degree--degree correlations. 26. Assortative and
disassortative mixing. 27. Disassortative mixing does not mean that vertices of
high degrees rarely connect to each other. 28. Reciprocal links in directed
nets. 29. Ultra-small-world effect. 30. Tree ansatz. 31.Ultraresilience against
random failures. 32. When correlated nets are ultraresilient. 33. Vulnerability
of complex networks. 34. The absence of an epidemic threshold. 35. Search based
on local information. 36.Ultraresilience disappears in finite nets. 37.Critical
behavior of cooperative models on networks. 38. Berezinskii-Kosterlitz-Thouless
phase transitions in networks. 39.Cascading failures. 40.Cliques &amp; communities.
41. Betweenness. 42.Extracting communities. 43. Optimal paths. 44.Distributions
of the shortest-path length &amp; of the loop's length are narrow. 45. Diffusion on
networks. 46. What is modularity? 47.Hierarchical organization of networks. 48.
Convincing modelling of real-world networks:Is it possible? 49. The small Web..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0405319</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0405319</id><created>2004-05-14</created><updated>2006-03-20</updated><authors><author><keyname>Deroulers</keyname><forenames>Christophe</forenames><affiliation>LPTENS</affiliation></author><author><keyname>Monasson</keyname><forenames>R&#xe9;mi</forenames><affiliation>LPTENS</affiliation></author></authors><title>Critical behaviour of combinatorial search algorithms, and the
  unitary-propagation universality class</title><categories>cond-mat.stat-mech cs.CC</categories><comments>7 pages; 3 figures</comments><proxy>ccsd ccsd-00001564</proxy><report-no>preprint LPTENS 04/25</report-no><journal-ref>Europhysics Letters 68 (2004) 153-159</journal-ref><doi>10.1209/epl/i2004-10177-6</doi><abstract>  The probability P(alpha, N) that search algorithms for random Satisfiability
problems successfully find a solution is studied as a function of the ratio
alpha of constraints per variable and the number N of variables. P is shown to
be finite if alpha lies below an algorithm--dependent threshold alpha\_A, and
exponentially small in N above. The critical behaviour is universal for all
algorithms based on the widely-used unitary propagation rule: P[ (1 + epsilon)
alpha\_A, N] ~ exp[-N^(1/6) Phi(epsilon N^(1/3)) ]. Exponents are related to
the critical behaviour of random graphs, and the scaling function Phi is
exactly calculated through a mapping onto a diffusion-and-death problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0406152</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0406152</id><created>2004-06-07</created><authors><author><keyname>Sarshar</keyname><forenames>Nima</forenames></author><author><keyname>Boykin</keyname><forenames>P. Oscar</forenames></author><author><keyname>Roychowdhury</keyname><forenames>Vwani P.</forenames></author></authors><title>Scalable Percolation Search in Power Law Networks</title><categories>cond-mat.dis-nn cs.NI</categories><journal-ref>Best paper award Fourth International Conference on Peer-to-Peer
  Computing, pp. 2-9, 2004</journal-ref><abstract>  We introduce a scalable searching algorithm for finding nodes and contents in
random networks with Power-Law (PL) and heavy-tailed degree distributions. The
network is searched using a probabilistic broadcast algorithm, where a query
message is relayed on each edge with probability just above the bond
percolation threshold of the network. We show that if each node caches its
directory via a short random walk, then the total number of {\em accessible
contents exhibits a first-order phase transition}, ensuring very high hit rates
just above the percolation threshold. In any random PL network of size, $N$,
and exponent, $2 \leq \tau &lt; 3$, the total traffic per query scales
sub-linearly, while the search time scales as $O(\log N)$. In a PL network with
exponent, $\tau \approx 2$, {\em any content or node} can be located in the
network with {\em probability approaching one} in time $O(\log N)$, while
generating traffic that scales as $O(\log^2 N)$, if the maximum degree,
$k_{max}$, is unconstrained, and as $O(N^{{1/2}+\epsilon})$ (for any
$\epsilon&gt;0$) if $ k_{max}=O(\sqrt{N})$. Extensive large-scale simulations show
these scaling laws to be precise. We discuss how this percolation search
algorithm can be directly adapted to solve the well-known scaling problem in
unstructured Peer-to-Peer (P2P) networks. Simulations of the protocol on sample
large-scale subnetworks of existing P2P services show that overall traffic can
be reduced by almost two-orders of magnitude, without any significant loss in
search performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0406404</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0406404</id><created>2004-06-17</created><updated>2004-06-22</updated><authors><author><keyname>Dall'Asta</keyname><forenames>Luca</forenames></author><author><keyname>Alvarez-Hamelin</keyname><forenames>Ignacio</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author><author><keyname>Vazquez</keyname><forenames>Alexei</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>A statistical approach to the traceroute-like exploration of networks:
  theory and simulations</title><categories>cond-mat.stat-mech cs.NI</categories><journal-ref>CAAN 2004, LNCS 3405, p. 140 (2005) .</journal-ref><abstract>  Mapping the Internet generally consists in sampling the network from a
limited set of sources by using &quot;traceroute&quot;-like probes. This methodology,
akin to the merging of different spanning trees to a set of destinations, has
been argued to introduce uncontrolled sampling biases that might produce
statistical properties of the sampled graph which sharply differ from the
original ones. Here we explore these biases and provide a statistical analysis
of their origin. We derive a mean-field analytical approximation for the
probability of edge and vertex detection that exploits the role of the number
of sources and targets and allows us to relate the global topological
properties of the underlying network with the statistical accuracy of the
sampled graph. In particular we find that the edge and vertex detection
probability is depending on the betweenness centrality of each element. This
allows us to show that shortest path routed sampling provides a better
characterization of underlying graphs with scale-free topology. We complement
the analytical discussion with a throughout numerical investigation of
simulated mapping strategies in different network models. We show that sampled
graphs provide a fair qualitative characterization of the statistical
properties of the original networks in a fair range of different strategies and
exploration parameters. The numerical study also allows the identification of
intervals of the exploration parameters that optimize the fraction of nodes and
edges discovered in the sampled graph. This finding might hint the steps toward
more efficient mapping strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0406765</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0406765</id><created>2004-06-30</created><updated>2004-07-12</updated><authors><author><keyname>Serrano</keyname><forenames>M. Angeles</forenames></author><author><keyname>Boguna</keyname><forenames>Marian</forenames></author><author><keyname>Diaz-Guilera</keyname><forenames>Albert</forenames></author></authors><title>Competition and adaptation in an Internet evolution model</title><categories>cond-mat.dis-nn cs.NI</categories><comments>Minor content changes and inset of fig.2</comments><journal-ref>Physical Review Letters 94, 038701 (2005)</journal-ref><doi>10.1103/PhysRevLett.94.038701</doi><abstract>  We model the evolution of the Internet at the Autonomous System level as a
process of competition for users and adaptation of bandwidth capability. We
find the exponent of the degree distribution as a simple function of the growth
rates of the number of autonomous systems and the total number of connections
in the Internet, both empirically measurable quantities. This fact place our
model apart from others in which this exponent depends on parameters that need
to be adjusted in a model dependent way. Our approach also accounts for a high
level of clustering as well as degree-degree correlations, both with the same
hierarchical structure present in the real Internet. Further, it also
highlights the interplay between bandwidth, connectivity and traffic of the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0407439</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0407439</id><created>2004-07-16</created><authors><author><keyname>Kokado</keyname><forenames>Satoshi</forenames></author><author><keyname>Harigaya</keyname><forenames>Kikuo</forenames></author></authors><title>A Theoretical Study on Spin-Dependent Transport of &quot;Ferromagnet/Carbon
  Nanotube Encapsulating Magnetic Atoms/Ferromagnet&quot; Junctions with 4-Valued
  Conductances</title><categories>cond-mat.mes-hall cond-mat.mtrl-sci cs.CE physics.chem-ph</categories><comments>10 pages, 4 figures, accepted for publication in J. Phys.: Condens.
  Matter</comments><journal-ref>J. Phys.: Condens. Matter 16, 5605 (2004)</journal-ref><doi>10.1088/0953-8984/16/30/020</doi><abstract>  As a novel function of ferromagnet (FM)/spacer/FM junctions, we theoretically
investigate multiple-valued (or multi-level) cell property, which is in
principle realized by sensing conductances of four states recorded with
magnetization configurations of two FMs; (up,up), (up,down), (down,up),
(down,down). In order to sense all the states, 4-valued conductances
corresponding to the respective states are necessary. We previously proposed
that 4-valued conductances are obtained in FM1/spin-polarized spacer (SPS)/FM2
junctions, where FM1 and FM2 have different spin polarizations, and the spacer
depends on spin [J. Phys.: Condens. Matter 15, 8797 (2003)]. In this paper, an
ideal SPS is considered as a single-wall armchair carbon nanotube encapsulating
magnetic atoms, where the nanotube shows on-resonance or off-resonance at the
Fermi level according to its length. The magnitude of the obtained 4-valued
conductances has an opposite order between the on-resonant nanotube and the
off-resonant one, and this property can be understood by considering electronic
states of the nanotube. Also, the magnetoresistance ratio between (up,up) and
(down,down) can be larger than the conventional one between parallel and
anti-parallel configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0408190</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0408190</id><created>2004-08-09</created><updated>2012-10-17</updated><authors><author><keyname>Jia</keyname><forenames>Haixia</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author></authors><title>From spin glasses to hard satisfiable formulas</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.AI</categories><comments>This is now somewhat out of date, but we thought it would make sense
  to cross-list to cs.AI as an example of hard SAT problems. The preliminary
  version (without results on RRT) appeared in SAT 2004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a highly structured family of hard satisfiable 3-SAT formulas
corresponding to an ordered spin-glass model from statistical physics. This
model has provably &quot;glassy&quot; behavior; that is, it has many local optima with
large energy barriers between them, so that local search algorithms get stuck
and have difficulty finding the true ground state, i.e., the unique satisfying
assignment. We test the hardness of our formulas with two Davis-Putnam solvers,
Satz and zChaff, the recently introduced Survey Propagation (SP), and two local
search algorithms, Walksat and Record-to-Record Travel (RRT). We compare our
formulas to random 3-XOR-SAT formulas and to two other generators of hard
satisfiable instances, the minimum disagreement parity formulas of Crawford et
al., and Hirsch's hgen. For the complete solvers the running time of our
formulas grows exponentially in sqrt(n), and exceeds that of random 3-XOR-SAT
formulas for small problem sizes. SP is unable to solve our formulas with as
few as 25 variables. For Walksat, our formulas appear to be harder than any
other known generator of satisfiable instances. Finally, our formulas can be
solved efficiently by RRT but only if the parameter d is tuned to the height of
the barriers between local minima, and we use this parameter to measure the
barrier heights in random 3-XOR-SAT formulas as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0408370</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0408370</id><created>2004-08-16</created><authors><author><keyname>Troyer</keyname><forenames>Matthias</forenames></author><author><keyname>Wiese</keyname><forenames>Uwe-Jens</forenames></author></authors><title>Computational complexity and fundamental limitations to fermionic
  quantum Monte Carlo simulations</title><categories>cond-mat.stat-mech cond-mat.str-el cs.CC hep-lat physics.comp-ph</categories><comments>4 pages</comments><journal-ref>Phys.Rev.Lett. 94 (2005) 170201</journal-ref><doi>10.1103/PhysRevLett.94.170201</doi><abstract>  Quantum Monte Carlo simulations, while being efficient for bosons, suffer
from the &quot;negative sign problem'' when applied to fermions - causing an
exponential increase of the computing time with the number of particles. A
polynomial time solution to the sign problem is highly desired since it would
provide an unbiased and numerically exact method to simulate correlated quantum
systems. Here we show, that such a solution is almost certainly unattainable by
proving that the sign problem is NP-hard, implying that a generic solution of
the sign problem would also solve all problems in the complexity class NP
(nondeterministic polynomial) in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0409532</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0409532</id><created>2004-09-21</created><authors><author><keyname>Bovier</keyname><forenames>Anton</forenames><affiliation>WIAS-Berlin, TU-Berlin</affiliation></author><author><keyname>Kurkova</keyname><forenames>Irina</forenames><affiliation>U Paris 6</affiliation></author></authors><title>Poisson convergence in the restricted $k$-partioning problem</title><categories>cond-mat.dis-nn cs.CC math.PR</categories><comments>31pp, AMSTeX</comments><report-no>WIAS-preprint 964</report-no><abstract>  The randomized $k$-number partitioning problem is the task to distribute $N$
i.i.d. random variables into $k$ groups in such a way that the sums of the
variables in each group are as similar as possible. The restricted
$k$-partitioning problem refers to the case where the number of elements in
each group is fixed to $N/k$. In the case $k=2$ it has been shown that the
properly rescaled differences of the two sums in the close to optimal
partitions converge to a Poisson point process, as if they were independent
random variables. We generalize this result to the case $k&gt;2$ in the restricted
problem and show that the vector of differences between the $k$ sums converges
to a $k-1$-dimensional Poisson point process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0410059</identifier>
 <datestamp>2008-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0410059</id><created>2004-10-04</created><authors><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Accuracy and Scaling Phenomena in Internet Mapping</title><categories>cond-mat.dis-nn cs.NI physics.soc-ph</categories><comments>4 pages, 3 figures; supercedes cond-mat/0407339 and contains scaling
  results on the accuracy of multi-source traceroute studies</comments><journal-ref>Phys. Rev. Lett. 94, 018701 (2005)</journal-ref><doi>10.1103/PhysRevLett.94.018701</doi><abstract>  A great deal of effort has been spent measuring topological features of the
Internet. However, it was recently argued that sampling based on taking paths
or traceroutes through the network from a small number of sources introduces a
fundamental bias in the observed degree distribution. We examine this bias
analytically and experimentally. For Erdos-Renyi random graphs with mean degree
c, we show analytically that traceroute sampling gives an observed degree
distribution P(k) ~ 1/k for k &lt; c, even though the underlying degree
distribution is Poisson. For graphs whose degree distributions have power-law
tails P(k) ~ k^-alpha, traceroute sampling from a small number of sources can
significantly underestimate the value of \alpha when the graph has a large
excess (i.e., many more edges than vertices). We find that in order to obtain a
good estimate of alpha it is necessary to use a number of sources which grows
linearly in the average degree of the underlying graph. Based on these
observations we comment on the accuracy of the published values of alpha for
the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0410270</identifier>
 <datestamp>2010-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0410270</id><created>2004-10-12</created><updated>2005-08-15</updated><authors><author><keyname>Furuichi</keyname><forenames>Shigeru</forenames></author></authors><title>On uniqueness theorems for Tsallis entropy and Tsallis relative entropy</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>this was merged by two manuscripts (arXiv:cond-mat/0410270 and
  arXiv:cond-mat/0410271), and will be published from IEEE TIT</comments><journal-ref>IEEE Trans. on Information Theory, Vol.51(2005),pp.3638-3645</journal-ref><abstract>  The uniqueness theorem for Tsallis entropy was presented in {\it H.Suyari,
IEEE Trans. Inform. Theory, Vol.50, pp.1783-1787 (2004)} by introducing the
generalized Shannon-Khinchin's axiom. In the present paper, this result is
generalized and simplified as follows: {\it Generalization}: The uniqueness
theorem for Tsallis relative entropy is shown by means of the generalized
Hobson's axiom. {\it Simplification}: The uniqueness theorem for Tsallis
entropy is shown by means of the generalized Faddeev's axiom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0410271</identifier>
 <datestamp>2010-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0410271</id><created>2004-10-12</created><updated>2010-12-26</updated><authors><author><keyname>Furuichi</keyname><forenames>Shigeru</forenames></author></authors><title>A generalized Faddeev's axiom and the uniqueness theorem for Tsallis
  entropy</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>This paper has been withdrawn by the author. The contents were
  unified in cond-mat/0410270</comments><abstract>  The uniequness theorem for the Tsallis entropy by introducing the generalized
Faddeev's axiom is proven. Our result improves the recent result, the
uniqueness theorem for Tsallis entropy by the generalized Shannon-Khinchin's
axiom in \cite{Suy}, in the sence that our axiom is simpler than his one, as
similar that Faddeev's axiom is simpler than Shannon-Khinchin's one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0410460</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0410460</id><created>2004-10-19</created><updated>2005-03-15</updated><authors><author><keyname>Makovetskiy</keyname><forenames>S. D.</forenames><affiliation>Kharkiv National University of Radio Electronics, Ukraine</affiliation></author><author><keyname>Makovetskii</keyname><forenames>D. N.</forenames><affiliation>Institute of Radio-Physics and Electronics of Natl. Acad. Sci., Ukraine</affiliation></author></authors><title>A Computational Study of Rotating Spiral Waves and Spatio-Temporal
  Transient Chaos in a Deterministic Three-Level Active System</title><categories>cond-mat.other cs.NE nlin.CG</categories><comments>24 pages (LaTeX2e file) and 14 figures (separate PNG-files); Version
  2 - minor changes and corrections, new references</comments><abstract>  Spatio-temporal dynamics of a deterministic three-level cellular automaton
(TLCA) of Zykov-Mikhailov type (Sov. Phys. - Dokl., 1986, Vol.31, No.1, P.51)
is studied numerically. Evolution of spatial structures is investigated both
for the original Zykov-Mikhailov model (which is applicable to, for example,
Belousov-Zhabotinskii chemical reactions) and for proposed by us TLCA, which is
a generalization of Zykov-Mikhailov model for the case of two-channel
diffusion. Such the TLCA is a minimal model for an excitable medium of
microwave phonon laser, called phaser (D. N. Makovetskii, Tech. Phys., 2004,
Vol.49, No.2, P.224; cond-mat/0402640). The most interesting observed forms of
TLCA dynamics are as follows: (a) spatio-temporal transient chaos in form of
highly bottlenecked collective evolution of excitations by rotating spiral
waves (RSW) with variable topological charges; (b) competition of left-handed
and right-handed RSW with unexpected features, including self-induced
alteration of integral effective topological charge; (c) transient chimera
states, i.e. coexistence of regular and chaotic domains in TLCA patterns; (d)
branching of TLCA states with different symmetry which may lead to full
restoring of symmetry of imperfect starting pattern. Phenomena (a) and (c) are
directly related to phaser dynamics features observed earlier in real
experiments at liquid helium temperatures on corundum crystals doped by
iron-group ions. ACM: F.1.1, I.6, J.2; PACS:05.65.+b, 07.05.Tp, 82.20.Wt
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0410498</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0410498</id><created>2004-10-20</created><authors><author><keyname>Majumdar</keyname><forenames>Satya N.</forenames></author><author><keyname>Dean</keyname><forenames>David S.</forenames></author><author><keyname>Krapivsky</keyname><forenames>P. L.</forenames></author></authors><title>Understanding Search Trees via Statistical Physics</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.DS</categories><comments>11 pages, 8 .eps figures included. Invited contribution to
  STATPHYS-22 held at Bangalore (India) in July 2004. To appear in the
  proceedings of STATPHYS-22</comments><doi>10.1007/BF02704178</doi><abstract>  We study the random m-ary search tree model (where m stands for the number of
branches of a search tree), an important problem for data storage in computer
science, using a variety of statistical physics techniques that allow us to
obtain exact asymptotic results. In particular, we show that the probability
distributions of extreme observables associated with a random search tree such
as the height and the balanced height of a tree have a traveling front
structure. In addition, the variance of the number of nodes needed to store a
data string of a given size N is shown to undergo a striking phase transition
at a critical value of the branching ratio m_c=26. We identify the mechanism of
this phase transition, show that it is generic and occurs in various other
problems as well. New results are obtained when each element of the data string
is a D-dimensional vector. We show that this problem also has a phase
transition at a critical dimension, D_c= \pi/\sin^{-1}(1/\sqrt{8})=8.69363...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0410594</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0410594</id><created>2004-10-22</created><updated>2005-04-08</updated><authors><author><keyname>Lipowski</keyname><forenames>Adam</forenames></author><author><keyname>Ferreira</keyname><forenames>Antonio L.</forenames></author></authors><title>A model of student's dilemma</title><categories>cond-mat.other cond-mat.stat-mech cs.MA physics.soc-ph</categories><comments>4 pages, minor changes, accepted in Physica A</comments><journal-ref>Physica A 354-C, pp.539-546 (2005)</journal-ref><doi>10.1016/j.physa.2005.03.009</doi><abstract>  Each year perhaps millions of young people face the following dilemma: should
I continue my education or rather start working with already acquired skills.
Right decision must take into account somebody's own abilities, accessibility
to education institutions, competition, and potential benefits. A multi-agent,
evolutionary model of this dilemma predicts a transition between stratified and
homogeneous phases, evolution that diminishes fitness, fewer applicants per
seat for decreased capacity of the university, and presence of poor students at
\'elite universities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0411077</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0411077</id><created>2004-11-03</created><updated>2012-11-05</updated><authors><author><keyname>Zhou</keyname><forenames>Haijun</forenames></author></authors><title>Long Range Frustrations in a Spin Glass Model of the Vertex Cover
  Problem</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>An erratum is added to the main text. 5 pages, 5 figures</comments><journal-ref>Physical Review Letters 94, 217203 (2005)</journal-ref><doi>10.1103/PhysRevLett.94.217203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a spin glass system on a random graph, some vertices have their spins
changing among different configurations of a ground--state domain. Long range
frustrations may exist among these unfrozen vertices in the sense that certain
combinations of spin values for these vertices may never appear in any
configuration of this domain. We present a mean field theory to tackle such
long range frustrations and apply it to the NP-hard minimum vertex cover
(hard-core gas condensation) problem. Our analytical results on the
ground-state energy density and on the fraction of frozen vertices are in good
agreement with known numerical and mathematical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0411079</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0411079</id><created>2004-11-03</created><updated>2005-05-19</updated><authors><author><keyname>Zhou</keyname><forenames>Haijun</forenames></author></authors><title>Long range frustration in finite connectivity spin glasses: A mean field
  theory and its application to the random $K$-satisfiability problem</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>Final version. Published in New Journal of Physics, freely available
  at http://www.njp.org/</comments><journal-ref>New Journal of Physics 7: 123 (2005)</journal-ref><doi>10.1088/1367-2630/7/1/123</doi><abstract>  Shortened abstract: A mean field theory of long range frustration is
constructed for spin glass systems with quenched randomness of vertex--vertex
connections and of spin--spin coupling strengths. This theory is applied to a
spin glass model of the random $K$-satisfiability problem (K=2 or K=3).
  The zero--temperature phase diagram of the $\pm J$ Viana--Bray model is also
determined, which is identical to that of the random 2-SAT problem. The
predicted phase transition between a non-frustrated and a long--rangely
frustrated spin glass phase might also be observable in real materials at a
finite temperature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0412460</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0412460</id><created>2004-12-17</created><updated>2005-05-16</updated><authors><author><keyname>Niven</keyname><forenames>Robert K.</forenames></author></authors><title>Exact Maxwell-Boltzmann, Bose-Einstein and Fermi-Dirac Statistics</title><categories>cond-mat.stat-mech cs.IT math.IT quant-ph</categories><comments>18 pages; 6 figures; accepted for publication by Physics Letters A,
  13/5/05</comments><doi>10.1016/j.physleta.2005.05.063</doi><abstract>  The exact Maxwell-Boltzmann (MB), Bose-Einstein (BE) and Fermi-Dirac (FD)
entropies and probabilistic distributions are derived by the combinatorial
method of Boltzmann, without Stirling's approximation. The new entropy measures
are explicit functions of the probability and degeneracy of each state, and the
total number of entities, N. By analysis of the cost of a &quot;binary decision&quot;,
exact BE and FD statistics are shown to have profound consequences for the
behaviour of quantum mechanical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0412587</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0412587</id><created>2004-12-21</created><authors><author><keyname>Kokado</keyname><forenames>Satoshi</forenames></author><author><keyname>Harigaya</keyname><forenames>Kikuo</forenames></author></authors><title>Spin dependent transport of ``nonmagnetic metal/zigzag nanotube
  encapsulating magnetic atoms/nonmagnetic metal'' junctions</title><categories>cond-mat.mes-hall cond-mat.mtrl-sci cs.CE physics.chem-ph quant-ph</categories><comments>4 pages, 3 figures, accepted for publication in Synth. Metals</comments><journal-ref>Synthetic Metals, Volume 152, Issues 1-3, 20 September 2005, Pages
  465-468</journal-ref><doi>10.1016/j.synthmet.2005.07.187</doi><abstract>  Towards a novel magnetoresistance (MR) device with a carbon nanotube, we
propose ``nonmagnetic metal/zigzag nanotube encapsulating magnetic
atoms/nonmagnetic metal'' junctions. We theoretically investigate how
spin-polarized edges of the nanotube and the encapsulated magnetic atoms
influence on transport. When the on-site Coulomb energy divided by the
magnitude of transfer integral, $U/|t|$, is larger than 0.8, large MR effect
due to the direction of spins of magnetic atoms, which has the magnitude of the
MR ratio of about 100%, appears reflecting such spin-polarized edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0412723</identifier>
 <datestamp>2008-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0412723</id><created>2004-12-28</created><authors><author><keyname>Gontis</keyname><forenames>Vygintas</forenames></author><author><keyname>Kaulakys</keyname><forenames>Bronislovas</forenames></author></authors><title>Modelling financial markets by the multiplicative sequence of trades</title><categories>cond-mat.stat-mech cs.CE math.SP physics.data-an q-fin.ST</categories><comments>6 pages, 2 figures</comments><journal-ref>Gontis V., Kaulakys B., Physica A 344 (2004) 128-133</journal-ref><doi>10.1016/j.physa.2004.06.153</doi><abstract>  We introduce the stochastic multiplicative point process modelling trading
activity of financial markets. Such a model system exhibits power-law spectral
density S(f) ~ 1/f**beta, scaled as power of frequency for various values of
beta between 0.5 and 2. Furthermore, we analyze the relation between the
power-law autocorrelations and the origin of the power-law probability
distribution of the trading activity. The model reproduces the spectral
properties of trading activity and explains the mechanism of power-law
distribution in real markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0501169</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0501169</id><created>2005-01-08</created><updated>2005-10-18</updated><authors><author><keyname>Li</keyname><forenames>Lun</forenames></author><author><keyname>Alderson</keyname><forenames>David</forenames></author><author><keyname>Tanaka</keyname><forenames>Reiko</forenames></author><author><keyname>Doyle</keyname><forenames>John C.</forenames></author><author><keyname>Willinger</keyname><forenames>Walter</forenames></author></authors><title>Towards a Theory of Scale-Free Graphs: Definition, Properties, and
  Implications (Extended Version)</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.NI math.CO q-bio.MN</categories><comments>44 pages, 16 figures. The primary version is to appear in Internet
  Mathematics (2005)</comments><report-no>CIT-CDS-04-006</report-no><abstract>  Although the ``scale-free'' literature is large and growing, it gives neither
a precise definition of scale-free graphs nor rigorous proofs of many of their
claimed properties. In fact, it is easily shown that the existing theory has
many inherent contradictions and verifiably false claims. In this paper, we
propose a new, mathematically precise, and structural definition of the extent
to which a graph is scale-free, and prove a series of results that recover many
of the claimed properties while suggesting the potential for a rich and
interesting theory. With this definition, scale-free (or its opposite,
scale-rich) is closely related to other structural graph properties such as
various notions of self-similarity (or respectively, self-dissimilarity).
Scale-free graphs are also shown to be the likely outcome of random
construction processes, consistent with the heuristic definitions implicit in
existing random graph approaches. Our approach clarifies much of the confusion
surrounding the sensational qualitative claims in the scale-free literature,
and offers rigorous and quantitative alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0501707</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0501707</id><created>2005-01-28</created><authors><author><keyname>Seitz</keyname><forenames>Sakari</forenames></author><author><keyname>Alava</keyname><forenames>Mikko</forenames></author><author><keyname>Orponen</keyname><forenames>Pekka</forenames></author></authors><title>Focused Local Search for Random 3-Satisfiability</title><categories>cond-mat.stat-mech cs.CC</categories><comments>20 pages, lots of figures</comments><doi>10.1088/1742-5468/2005/06/P06006</doi><abstract>  A local search algorithm solving an NP-complete optimisation problem can be
viewed as a stochastic process moving in an 'energy landscape' towards
eventually finding an optimal solution. For the random 3-satisfiability
problem, the heuristic of focusing the local moves on the presently
unsatisfiedclauses is known to be very effective: the time to solution has been
observed to grow only linearly in the number of variables, for a given
clauses-to-variables ratio $\alpha$ sufficiently far below the critical
satisfiability threshold $\alpha_c \approx 4.27$. We present numerical results
on the behaviour of three focused local search algorithms for this problem,
considering in particular the characteristics of a focused variant of the
simple Metropolis dynamics. We estimate the optimal value for the
``temperature'' parameter $\eta$ for this algorithm, such that its linear-time
regime extends as close to $\alpha_c$ as possible. Similar parameter
optimisation is performed also for the well-known WalkSAT algorithm and for the
less studied, but very well performing Focused Record-to-Record Travel method.
We observe that with an appropriate choice of parameters, the linear time
regime for each of these algorithms seems to extend well into ratios $\alpha &gt;
4.2$ -- much further than has so far been generally assumed. We discuss the
statistics of solution times for the algorithms, relate their performance to
the process of ``whitening'', and present some conjectures on the shape of
their computational phase diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0502205</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0502205</id><created>2005-02-08</created><updated>2005-02-08</updated><authors><author><keyname>Berger</keyname><forenames>N.</forenames></author><author><keyname>Borgs</keyname><forenames>C.</forenames></author><author><keyname>Chayes</keyname><forenames>J. T.</forenames></author><author><keyname>D'Souza</keyname><forenames>R. M.</forenames></author><author><keyname>Kleinberg</keyname><forenames>R. D.</forenames></author></authors><title>Degree Distribution of Competition-Induced Preferential Attachment
  Graphs</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.NI math.PR</categories><comments>24 pages, one figure. To appear in the journal: Combinatorics,
  Probability and Computing. Note, this is a long version, with complete
  proofs, of the paper &quot;Competition-Induced Preferential Attachment&quot;
  (cond-mat/0402268)</comments><abstract>  We introduce a family of one-dimensional geometric growth models, constructed
iteratively by locally optimizing the tradeoffs between two competing metrics,
and show that this family is equivalent to a family of preferential attachment
random graph models with upper cutoffs. This is the first explanation of how
preferential attachment can arise from a more basic underlying mechanism of
local competition. We rigorously determine the degree distribution for the
family of random graph models, showing that it obeys a power law up to a finite
threshold and decays exponentially above this threshold.
  We also rigorously analyze a generalized version of our graph process, with
two natural parameters, one corresponding to the cutoff and the other a
``fertility'' parameter. We prove that the general model has a power-law degree
distribution up to a cutoff, and establish monotonicity of the power as a
function of the two parameters. Limiting cases of the general model include the
standard preferential attachment model without cutoff and the uniform
attachment model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0503087</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0503087</id><created>2005-03-03</created><updated>2006-03-29</updated><authors><author><keyname>Achlioptas</keyname><forenames>Dimitris</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author><author><keyname>Kempe</keyname><forenames>David</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>On the Bias of Traceroute Sampling; or, Power-law Degree Distributions
  in Regular Graphs</title><categories>cond-mat.dis-nn cs.NI math.CO math.PR</categories><comments>Long-format version (19 pages); includes small correction to section
  6.1</comments><journal-ref>Proc. 37th ACM Symposium on Theory of Computing (STOC) 2005</journal-ref><abstract>  Understanding the structure of the Internet graph is a crucial step for
building accurate network models and designing efficient algorithms for
Internet applications. Yet, obtaining its graph structure is a surprisingly
difficult task, as edges cannot be explicitly queried. Instead, empirical
studies rely on traceroutes to build what are essentially single-source,
all-destinations, shortest-path trees. These trees only sample a fraction of
the network's edges, and a recent paper by Lakhina et al. found empirically
that the resuting sample is intrinsically biased. For instance, the observed
degree distribution under traceroute sampling exhibits a power law even when
the underlying degree distribution is Poisson.
  In this paper, we study the bias of traceroute sampling systematically, and,
for a very general class of underlying degree distributions, calculate the
likely observed distributions explicitly. To do this, we use a continuous-time
realization of the process of exposing the BFS tree of a random graph with a
given degree distribution, calculate the expected degree distribution of the
tree, and show that it is sharply concentrated. As example applications of our
machinery, we show how traceroute sampling finds power-law degree distributions
in both delta-regular and Poisson-distributed random graphs. Thus, our work
puts the observations of Lakhina et al. on a rigorous footing, and extends them
to nearly arbitrary degree distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0503627</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0503627</id><created>2005-03-26</created><updated>2006-07-28</updated><authors><author><keyname>Lubachevsky</keyname><forenames>Boris D.</forenames></author></authors><title>How to Simulate Billiards and Similar Systems</title><categories>cond-mat.mtrl-sci cs.DS math.DS</categories><comments>29 pages. 10 figures</comments><journal-ref>Journal of Computational Physics, v.94 n.2, p.255-283, June 1991</journal-ref><doi>10.1016/0021-9991(91)90222-7</doi><abstract>  An N-component continuous-time dynamic system is considered whose components
evolve autonomously all the time except for in discrete asynchronous instances
of pairwise interactions. Examples include chaotically colliding billiard balls
and combat models. A new efficient serial event-driven algorithm is described
for simulating such systems. Rather than maintaining and updating the global
state of the system, the algorithm tries to examine only essential events,
i.e., component interactions. The events are processed in a non-decreasing
order of time; new interactions are scheduled on the basis of the examined
interactions using preintegrated equations of the evolutions of the components.
If the components are distributed uniformly enough in the evolution space, so
that this space can be subdivided into small sectors such that only O(1)
sectors and O(1)$components are in the neighborhood of a sector, then the
algorithm spends time O (log N) for processing an event which is the
asymptotical minimum. The algorithm uses a simple strategy for handling data:
only two states are maintained for each simulated component. Fast data access
in this strategy assures the practical efficiency of the algorithm. It works
noticeably faster than other algorithms proposed for this model.
  Key phrases: collision detection, dense packing, molecular dynamics, hard
spheres, granular flow
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0504025</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0504025</id><created>2005-04-01</created><authors><author><keyname>Kaulakys</keyname><forenames>B.</forenames></author><author><keyname>Gontis</keyname><forenames>V.</forenames></author><author><keyname>Alaburda</keyname><forenames>M.</forenames></author></authors><title>Point process model of 1/f noise versus a sum of Lorentzians</title><categories>cond-mat.stat-mech astro-ph cond-mat.dis-nn cs.CE math.ST nlin.AO physics.data-an q-bio.NC stat.TH</categories><comments>23 pages, 10 figures, to be published in Phys. Rev. E</comments><journal-ref>Phys.Rev. E71 (2005) 051105</journal-ref><doi>10.1103/PhysRevE.71.051105</doi><abstract>  We present a simple point process model of $1/f^{\beta}$ noise, covering
different values of the exponent $\beta$. The signal of the model consists of
pulses or events. The interpulse, interevent, interarrival, recurrence or
waiting times of the signal are described by the general Langevin equation with
the multiplicative noise and stochastically diffuse in some interval resulting
in the power-law distribution. Our model is free from the requirement of a wide
distribution of relaxation times and from the power-law forms of the pulses. It
contains only one relaxation rate and yields $1/f^ {\beta}$ spectra in a wide
range of frequency. We obtain explicit expressions for the power spectra and
present numerical illustrations of the model. Further we analyze the relation
of the point process model of $1/f$ noise with the Bernamont-Surdin-McWhorter
model, representing the signals as a sum of the uncorrelated components. We
show that the point process model is complementary to the model based on the
sum of signals with a wide-range distribution of the relaxation times. In
contrast to the Gaussian distribution of the signal intensity of the sum of the
uncorrelated components, the point process exhibits asymptotically a power-law
distribution of the signal intensity. The developed multiplicative point
process model of $1/f^{\beta}$ noise may be used for modeling and analysis of
stochastic processes in different systems with the power-law distribution of
the intensity of pulsing signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0504070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0504070</id><created>2005-04-04</created><authors><author><keyname>Mezard</keyname><forenames>M.</forenames></author><author><keyname>Mora</keyname><forenames>T.</forenames></author><author><keyname>Zecchina</keyname><forenames>R.</forenames></author></authors><title>Clustering of solutions in the random satisfiability problem</title><categories>cond-mat.dis-nn cs.CC</categories><comments>4 pages, 1 figure</comments><journal-ref>Phys. Rev. Lett. 94, 197205 (2005)</journal-ref><doi>10.1103/PhysRevLett.94.197205</doi><abstract>  Using elementary rigorous methods we prove the existence of a clustered phase
in the random $K$-SAT problem, for $K\geq 8$. In this phase the solutions are
grouped into clusters which are far away from each other. The results are in
agreement with previous predictions of the cavity method and give a rigorous
confirmation to one of its main building blocks. It can be generalized to other
systems of both physical and computational interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0504185</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0504185</id><created>2005-04-07</created><updated>2005-05-11</updated><authors><author><keyname>Rezaei</keyname><forenames>Behnam A.</forenames></author><author><keyname>Sarshar</keyname><forenames>Nima</forenames></author><author><keyname>Boykin</keyname><forenames>P. Oscar</forenames></author><author><keyname>Roychowdhury</keyname><forenames>Vwani P.</forenames></author></authors><title>Disaster Management in Scale-Free Networks: Recovery from and Protection
  Against Intentional Attacks</title><categories>cond-mat.stat-mech cs.DS cs.NI physics.data-an physics.soc-ph</categories><comments>12 pages, to be submitted to PRE</comments><abstract>  Susceptibility of scale free Power Law (PL) networks to attacks has been
traditionally studied in the context of what may be termed as {\em
instantaneous attacks}, where a randomly selected set of nodes and edges are
deleted while the network is kept {\em static}. In this paper, we shift the
focus to the study of {\em progressive} and instantaneous attacks on {\em
reactive} grown and random PL networks, which can respond to attacks and take
remedial steps. In the process, we present several techniques that managed
networks can adopt to minimize the damages during attacks, and also to
efficiently recover from the aftermath of successful attacks. For example, we
present (i) compensatory dynamics that minimize the damages inflicted by
targeted progressive attacks, such as linear-preferential deletions of nodes in
grown PL networks; the resulting dynamic naturally leads to the emergence of
networks with PL degree distributions with exponential cutoffs; (ii)
distributed healing algorithms that can scale the maximum degree of nodes in a
PL network using only local decisions, and (iii) efficient means of creating
giant connected components in a PL network that has been fragmented by attacks
on a large number of high-degree nodes. Such targeted attacks are considered to
be a major vulnerability of PL networks; however, our results show that the
introduction of only a small number of random edges, through a {\em reverse
percolation} process, can restore connectivity, which in turn allows
restoration of other topological properties of the original network. Thus, the
scale-free nature of the networks can itself be effectively utilized for
protection and recovery purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0505193</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0505193</id><created>2005-05-08</created><updated>2005-09-23</updated><authors><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author><author><keyname>Povolotsky</keyname><forenames>A. M.</forenames></author><author><keyname>Samukhin</keyname><forenames>A. N.</forenames></author></authors><title>Organization of complex networks without multiple connections</title><categories>cond-mat.stat-mech cs.NI hep-lat physics.soc-ph</categories><comments>5 pages, 2 figures</comments><journal-ref>Phys.Rev.Lett. 95 (2005) 195701</journal-ref><doi>10.1103/PhysRevLett.95.195701</doi><abstract>  We find a new structural feature of equilibrium complex random networks
without multiple and self-connections. We show that if the number of
connections is sufficiently high, these networks contain a core of highly
interconnected vertices. The number of vertices in this core varies in the
range between $const N^{1/2}$ and $const N^{2/3}$, where $N$ is the number of
vertices in a network. At the birth point of the core, we obtain the
size-dependent cut-off of the distribution of the number of connections and
find that its position differs from earlier estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0506002</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0506002</id><created>2005-05-31</created><authors><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Goltsev</keyname><forenames>A. V.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>Correlations in interacting systems with a network topology</title><categories>cond-mat.stat-mech cs.NI math-ph math.MP physics.soc-ph</categories><comments>5 pages</comments><journal-ref>Phys. Rev. E 72, 066130 (2005)</journal-ref><doi>10.1103/PhysRevE.72.066130</doi><abstract>  We study pair correlations in cooperative systems placed on complex networks.
We show that usually in these systems, the correlations between two interacting
objects (e.g., spins), separated by a distance $\ell$, decay, on average,
faster than $1/(\ell z_\ell)$. Here $z_\ell$ is the mean number of the
$\ell$-th nearest neighbors of a vertex in a network. This behavior, in
particular, leads to a dramatic weakening of correlations between second and
more distant neighbors on networks with fat-tailed degree distributions, which
have a divergent number $z_2$ in the infinite network limit. In this case, only
the pair correlations between the nearest neighbors are observable. We obtain
the pair correlation function of the Ising model on a complex network and also
derive our results in the framework of a phenomenological approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0506037</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0506037</id><created>2005-06-01</created><authors><author><keyname>Stepanov</keyname><forenames>M. G.</forenames><affiliation>LANL</affiliation></author><author><keyname>Chernyak</keyname><forenames>V.</forenames><affiliation>Wayne State</affiliation></author><author><keyname>Chertkov</keyname><forenames>M.</forenames><affiliation>LANL</affiliation></author><author><keyname>Vasic</keyname><forenames>B.</forenames><affiliation>U. of Arizona</affiliation></author></authors><title>Diagnosis of weaknesses in modern error correction codes: a physics
  approach</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT</categories><comments>9 pages, 8 figures</comments><report-no>LA-UR-05-2591</report-no><doi>10.1103/PhysRevLett.95.228701</doi><abstract>  One of the main obstacles to the wider use of the modern error-correction
codes is that, due to the complex behavior of their decoding algorithms, no
systematic method which would allow characterization of the Bit-Error-Rate
(BER) is known. This is especially true at the weak noise where many systems
operate and where coding performance is difficult to estimate because of the
diminishingly small number of errors. We show how the instanton method of
physics allows one to solve the problem of BER analysis in the weak noise range
by recasting it as a computationally tractable minimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0506053</identifier>
 <datestamp>2008-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0506053</id><created>2005-06-02</created><updated>2007-09-19</updated><authors><author><keyname>Daud&#xe9;</keyname><forenames>Herv&#xe9;</forenames><affiliation>LATP</affiliation></author><author><keyname>Mezard</keyname><forenames>Marc</forenames><affiliation>LPTMS</affiliation></author><author><keyname>Mora</keyname><forenames>Thierry</forenames><affiliation>LPTMS</affiliation></author><author><keyname>Zecchina</keyname><forenames>Riccardo</forenames><affiliation>POLITO</affiliation></author></authors><title>Pairs of SAT Assignment in Random Boolean Formulae</title><categories>cond-mat.dis-nn cs.CC</categories><proxy>ccsd ccsd-00005109</proxy><journal-ref>Theoretical Computer Science 393 (2008) 260-279</journal-ref><doi>10.1016/j.tcs.2008.01.005</doi><abstract>  We investigate geometrical properties of the random K-satisfiability problem
using the notion of x-satisfiability: a formula is x-satisfiable if there exist
two SAT assignments differing in Nx variables. We show the existence of a sharp
threshold for this property as a function of the clause density. For large
enough K, we prove that there exists a region of clause density, below the
satisfiability threshold, where the landscape of Hamming distances between SAT
assignments experiences a gap: pairs of SAT-assignments exist at small x, and
around x=1/2, but they donot exist at intermediate values of x. This result is
consistent with the clustering scenario which is at the heart of the recent
heuristic analysis of satisfiability using statistical physics analysis (the
cavity method), and its algorithmic counterpart (the survey propagation
algorithm). The method uses elementary probabilistic arguments (first and
second moment methods), and might be useful in other problems of computational
and physical interest where similar phenomena appear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0506330</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0506330</id><created>2005-06-14</created><updated>2005-06-15</updated><authors><author><keyname>Bogacz</keyname><forenames>L.</forenames></author><author><keyname>Burda</keyname><forenames>Z.</forenames></author><author><keyname>Janke</keyname><forenames>W.</forenames></author><author><keyname>Waclaw</keyname><forenames>B.</forenames></author></authors><title>A program generating homogeneous random graphs with given weights</title><categories>cond-mat.dis-nn cs.NI physics.comp-ph</categories><comments>19 pages, 3 figures</comments><journal-ref>Comp. Phys. Comm. 173 (2005) 162-174</journal-ref><doi>10.1016/j.cpc.2005.07.010</doi><abstract>  We present a program package which generates homogeneous random graphs with
probabilities prescribed by the user. The statistical weight of a labeled graph
$\alpha$ is given in the form $W(\alpha)=\prod_{i=1}^N p(q_i)$, where $p(q)$ is
an arbitrary user function and $q_i$ are the degrees of the graph nodes. The
program can be used to generate two types of graphs (simple graphs and
pseudo-graphs) from three types of ensembles (micro-canonical, canonical and
grand-canonical).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0506652</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0506652</id><created>2005-06-24</created><updated>2005-09-14</updated><authors><author><keyname>Ciliberti</keyname><forenames>Stefano</forenames></author><author><keyname>Mezard</keyname><forenames>Marc</forenames></author></authors><title>The theoretical capacity of the Parity Source Coder</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT</categories><comments>Added references, minor changes</comments><journal-ref>J. Stat Mech P10003 (2005)</journal-ref><doi>10.1088/1742-5468/2005/10/P10003</doi><abstract>  The Parity Source Coder is a protocol for data compression which is based on
a set of parity checks organized in a sparse random network. We consider here
the case of memoryless unbiased binary sources. We show that the theoretical
capacity saturate the Shannon limit at large K. We also find that the first
corrections to the leading behavior are exponentially small, so that the
behavior at finite K is very close to the optimal one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0507451</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0507451</id><created>2005-07-19</created><updated>2005-11-02</updated><authors><author><keyname>Mezard</keyname><forenames>Marc</forenames></author><author><keyname>Palassini</keyname><forenames>Matteo</forenames></author><author><keyname>Rivoire</keyname><forenames>Olivier</forenames></author></authors><title>Landscape of solutions in constraint satisfaction problems</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>4 pages, 4 figures. Replaced with published version</comments><journal-ref>Phys. Rev. Lett. 95, 200202 (2005)</journal-ref><doi>10.1103/PhysRevLett.95.200202</doi><abstract>  We present a theoretical framework for characterizing the geometrical
properties of the space of solutions in constraint satisfaction problems,
together with practical algorithms for studying this structure on particular
instances. We apply our method to the coloring problem, for which we obtain the
total number of solutions and analyze in detail the distribution of distances
between solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0508125</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0508125</id><created>2005-08-04</created><updated>2006-03-20</updated><authors><author><keyname>Deroulers</keyname><forenames>Christophe</forenames><affiliation>LPTENS</affiliation></author><author><keyname>Monasson</keyname><forenames>R&#xe9;mi</forenames><affiliation>LPTENS</affiliation></author></authors><title>Criticality and Universality in the Unit-Propagation Search Rule</title><categories>cond-mat.stat-mech cs.CC</categories><comments>30 pages, 13 figures</comments><proxy>ccsd ccsd-00007778</proxy><report-no>LPTENS-05/24</report-no><journal-ref>European Physical Journal B 49 (2006) 339-369</journal-ref><doi>10.1140/epjb/e2006-00072-6</doi><abstract>  The probability Psuccess(alpha, N) that stochastic greedy algorithms
successfully solve the random SATisfiability problem is studied as a function
of the ratio alpha of constraints per variable and the number N of variables.
These algorithms assign variables according to the unit-propagation (UP) rule
in presence of constraints involving a unique variable (1-clauses), to some
heuristic (H) prescription otherwise. In the infinite N limit, Psuccess
vanishes at some critical ratio alpha\_H which depends on the heuristic H. We
show that the critical behaviour is determined by the UP rule only. In the case
where only constraints with 2 and 3 variables are present, we give the phase
diagram and identify two universality classes: the power law class, where
Psuccess[alpha\_H (1+epsilon N^{-1/3}), N] ~ A(epsilon)/N^gamma; the stretched
exponential class, where Psuccess[alpha\_H (1+epsilon N^{-1/3}), N] ~
exp[-N^{1/6} Phi(epsilon)]. Which class is selected depends on the
characteristic parameters of input data. The critical exponent gamma is
universal and calculated; the scaling functions A and Phi weakly depend on the
heuristic H and are obtained from the solutions of reaction-diffusion equations
for 1-clauses. Computation of some non-universal corrections allows us to match
numerical results with good precision. The critical behaviour for constraints
with &gt;3 variables is given. Our results are interpreted in terms of dynamical
graph percolation and we argue that they should apply to more general
situations where UP is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0508152</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0508152</id><created>2005-08-05</created><authors><author><keyname>Avizrats</keyname><forenames>Yaniv S.</forenames></author><author><keyname>Feinberg</keyname><forenames>Joshua</forenames></author><author><keyname>Fishman</keyname><forenames>Shmuel</forenames></author></authors><title>A Universal Scaling Theory for Complexity of Analog Computation</title><categories>cond-mat.other cs.CC</categories><comments>4 pages, 2 eps figures</comments><abstract>  We discuss the computational complexity of solving linear programming
problems by means of an analog computer. The latter is modeled by a dynamical
system which converges to the optimal vertex solution. We analyze various
probability ensembles of linear programming problems. For each one of these we
obtain numerically the probability distribution functions of certain quantities
which measure the complexity. Remarkably, in the asymptotic limit of very large
problems, each of these probability distribution functions reduces to a
universal scaling function, depending on a single scaling variable and
independent of the details of its parent probability ensemble. These functions
are reminiscent of the scaling functions familiar in the theory of phase
transitions. The results reported here extend analytical and numerical results
obtained recently for the Gaussian ensemble.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0508216</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0508216</id><created>2005-08-09</created><authors><author><keyname>Pelizzola</keyname><forenames>Alessandro</forenames></author></authors><title>Cluster Variation Method in Statistical Physics and Probabilistic
  Graphical Models</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>36 pages, 17 figures</comments><journal-ref>J. Phys. A 38, R309 (2005)</journal-ref><doi>10.1088/0305-4470/38/33/R01</doi><abstract>  The cluster variation method (CVM) is a hierarchy of approximate variational
techniques for discrete (Ising--like) models in equilibrium statistical
mechanics, improving on the mean--field approximation and the Bethe--Peierls
approximation, which can be regarded as the lowest level of the CVM. In recent
years it has been applied both in statistical physics and to inference and
optimization problems formulated in terms of probabilistic graphical models.
  The foundations of the CVM are briefly reviewed, and the relations with
similar techniques are discussed. The main properties of the method are
considered, with emphasis on its exactness for particular models and on its
asymptotic properties.
  The problem of the minimization of the variational free energy, which arises
in the CVM, is also addressed, and recent results about both provably
convergent and message-passing algorithms are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0509102</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0509102</id><created>2005-09-05</created><updated>2006-02-28</updated><authors><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Goltsev</keyname><forenames>A. V.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>k-core organization of complex networks</title><categories>cond-mat.stat-mech cs.NI math-ph math.MP physics.soc-ph</categories><comments>5 pages, 3 figures</comments><journal-ref>Phys. Rev. Lett. 96, 040601 (2006)</journal-ref><doi>10.1103/PhysRevLett.96.040601</doi><abstract>  We analytically describe the architecture of randomly damaged uncorrelated
networks as a set of successively enclosed substructures -- k-cores. The k-core
is the largest subgraph where vertices have at least k interconnections. We
find the structure of k-cores, their sizes, and their birth points -- the
bootstrap percolation thresholds. We show that in networks with a finite mean
number z_2 of the second-nearest neighbors, the emergence of a k-core is a
hybrid phase transition. In contrast, if z_2 diverges, the networks contain an
infinite sequence of k-cores which are ultra-robust against random damage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0510064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0510064</id><created>2005-10-04</created><authors><author><keyname>Majumdar</keyname><forenames>Satya N.</forenames></author></authors><title>Brownian Functionals in Physics and Computer Science</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.DS</categories><comments>21 pages, 5 .eps figures included</comments><journal-ref>Current Science, vol-89, 2076 (2005).</journal-ref><abstract>  This is a brief review on Brownian functionals in one dimension and their
various applications, a contribution to the special issue ``The Legacy of
Albert Einstein&quot; of Current Science. After a brief description of Einstein's
original derivation of the diffusion equation, this article provides a
pedagogical introduction to the path integral methods leading to the derivation
of the celebrated Feynman-Kac formula. The usefulness of this technique in
calculating the statistical properties of Brownian functionals is illustrated
with several examples in physics and probability theory, with particular
emphasis on applications in computer science. The statistical properties of
&quot;first-passage Brownian functionals&quot; and their applications are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0510429</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0510429</id><created>2005-10-17</created><updated>2005-10-24</updated><authors><author><keyname>Dean</keyname><forenames>David S.</forenames></author><author><keyname>Majumdar</keyname><forenames>Satya N.</forenames></author></authors><title>Phase Transition in the Aldous-Shields Model of Growing Trees</title><categories>cond-mat.stat-mech cs.DS math.PR</categories><comments>Latex 17 pages, 6 figures</comments><journal-ref>J. Stat. Phys 124, 1351 (2006).</journal-ref><doi>10.1007/s10955-006-9193-9</doi><abstract>  We study analytically the late time statistics of the number of particles in
a growing tree model introduced by Aldous and Shields. In this model, a cluster
grows in continuous time on a binary Cayley tree, starting from the root, by
absorbing new particles at the empty perimeter sites at a rate proportional to
c^{-l} where c is a positive parameter and l is the distance of the perimeter
site from the root. For c=1, this model corresponds to random binary search
trees and for c=2 it corresponds to digital search trees in computer science.
By introducing a backward Fokker-Planck approach, we calculate the mean and the
variance of the number of particles at large times and show that the variance
undergoes a `phase transition' at a critical value c=sqrt{2}. While for
c&gt;sqrt{2} the variance is proportional to the mean and the distribution is
normal, for c&lt;sqrt{2} the variance is anomalously large and the distribution is
non-Gaussian due to the appearance of extreme fluctuations. The model is
generalized to one where growth occurs on a tree with $m$ branches and, in this
more general case, we show that the critical point occurs at c=sqrt{m}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0511159</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0511159</id><created>2005-11-07</created><updated>2005-12-09</updated><authors><author><keyname>Braunstein</keyname><forenames>Alfredo</forenames></author><author><keyname>Zecchina</keyname><forenames>Riccardo</forenames></author></authors><title>Learning by message-passing in networks of discrete synapses</title><categories>cond-mat.dis-nn cs.LG q-bio.NC</categories><comments>4 pages, 3 figures; references updated and minor corrections;
  accepted in PRL</comments><journal-ref>Phys. Rev. Lett. 96, 030201 (2006)</journal-ref><doi>10.1103/PhysRevLett.96.030201</doi><abstract>  We show that a message-passing process allows to store in binary &quot;material&quot;
synapses a number of random patterns which almost saturates the information
theoretic bounds. We apply the learning algorithm to networks characterized by
a wide range of different connection topologies and of size comparable with
that of biological systems (e.g. $n\simeq10^{5}-10^{6}$). The algorithm can be
turned into an on-line --fault tolerant-- learning protocol of potential
interest in modeling aspects of synaptic plasticity and in building
neuromorphic devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0512017</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0512017</id><created>2005-12-01</created><updated>2007-04-20</updated><authors><author><keyname>Niven</keyname><forenames>Robert K.</forenames></author></authors><title>Combinatorial Information Theory: I. Philosophical Basis of
  Cross-Entropy and Entropy</title><categories>cond-mat.stat-mech cs.IT math-ph math.IT math.MP physics.data-an</categories><comments>45 pp; 1 figure; REVTex; updated version 5 (incremental changes)</comments><abstract>  This study critically analyses the information-theoretic, axiomatic and
combinatorial philosophical bases of the entropy and cross-entropy concepts.
The combinatorial basis is shown to be the most fundamental (most primitive) of
these three bases, since it gives (i) a derivation for the Kullback-Leibler
cross-entropy and Shannon entropy functions, as simplified forms of the
multinomial distribution subject to the Stirling approximation; (ii) an
explanation for the need to maximize entropy (or minimize cross-entropy) to
find the most probable realization; and (iii) new, generalized definitions of
entropy and cross-entropy - supersets of the Boltzmann principle - applicable
to non-multinomial systems. The combinatorial basis is therefore of much
broader scope, with far greater power of application, than the
information-theoretic and axiomatic bases. The generalized definitions underpin
a new discipline of ``{\it combinatorial information theory}'', for the
analysis of probabilistic systems of any type.
  Jaynes' generic formulation of statistical mechanics for multinomial systems
is re-examined in light of the combinatorial approach. (abbreviated abstract)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0601021</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0601021</id><created>2006-01-02</created><updated>2006-02-20</updated><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author><author><keyname>Woelki</keyname><forenames>Marko</forenames></author><author><keyname>Schreckenberg</keyname><forenames>Michael</forenames></author></authors><title>Characterizing correlations of flow oscillations at bottlenecks</title><categories>cond-mat.stat-mech cs.MA</categories><journal-ref>J. Stat. Mech. (2006) P02005</journal-ref><doi>10.1088/1742-5468/2006/02/P02005</doi><abstract>  &quot;Oscillations&quot; occur in quite different kinds of many-particle-systems when
two groups of particles with different directions of motion meet or intersect
at a certain spot. We present a model of pedestrian motion that is able to
reproduce oscillations with different characteristics. The Wald-Wolfowitz test
and Gillis' correlated random walk are shown to hold observables that can be
used to characterize different kinds of oscillations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0601487</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0601487</id><created>2006-01-20</created><updated>2006-03-07</updated><authors><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Chernyak</keyname><forenames>Vladimir Y.</forenames></author></authors><title>Loop Calculus in Statistical Physics and Information Science</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT</categories><comments>4 pages, submitted to Phys.Rev.Lett. Changes: More general model,
  Simpler derivation</comments><report-no>LAUR-06-0443</report-no><journal-ref>Phys. Rev. E 73, 065102(R) (2006)</journal-ref><doi>10.1103/PhysRevE.73.065102</doi><abstract>  Considering a discrete and finite statistical model of a general position we
introduce an exact expression for the partition function in terms of a finite
series. The leading term in the series is the Bethe-Peierls (Belief
Propagation)-BP contribution, the rest are expressed as loop-contributions on
the factor graph and calculated directly using the BP solution. The series
unveils a small parameter that often makes the BP approximation so successful.
Applications of the loop calculus in statistical physics and information
science are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0601573</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0601573</id><created>2006-01-25</created><authors><author><keyname>Parisi</keyname><forenames>G.</forenames></author><author><keyname>Zamponi</keyname><forenames>F.</forenames></author></authors><title>Amorphous packings of hard spheres in large space dimension</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.GM math.IT</categories><comments>11 pages, 1 figure, includes feynmf diagrams</comments><journal-ref>J.Stat.Mech. (2006) P03017</journal-ref><doi>10.1088/1742-5468/2006/03/P03017</doi><abstract>  In a recent paper (cond-mat/0506445) we derived an expression for the
replicated free energy of a liquid of hard spheres based on the HNC free energy
functional. An approximate equation of state for the glass and an estimate of
the random close packing density were obtained in d=3. Here we show that the
HNC approximation is not needed: the same expression can be obtained from the
full diagrammatic expansion of the replicated free energy. Then, we consider
the asymptotics of this expression when the space dimension d is very large. In
this limit, the entropy of the hard sphere liquid has been computed exactly.
Using this solution, we derive asymptotic expressions for the glass transition
density and for the random close packing density for hard spheres in large
space dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0602183</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0602183</id><created>2006-02-07</created><authors><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Pellicoro</keyname><forenames>Mario</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author></authors><title>Nonlinear parametric model for Granger causality of time series</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.LG physics.med-ph q-bio.QM</categories><comments>4 pages 5 figures</comments><doi>10.1103/PhysRevE.73.066216</doi><abstract>  We generalize a previously proposed approach for nonlinear Granger causality
of time series, based on radial basis function. The proposed model is not
constrained to be additive in variables from the two time series and can
approximate any function of these variables, still being suitable to evaluate
causality. Usefulness of this measure of causality is shown in a physiological
example and in the study of the feed-back loop in a model of excitatory and
inhibitory neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0602345</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0602345</id><created>2006-02-14</created><authors><author><keyname>Makovetskiy</keyname><forenames>S. D.</forenames></author></authors><title>Numerical Modeling of Coexistence, Competition and Collapse of Rotating
  Spiral Waves in Three-Level Excitable Media with Discrete Active Centers and
  Absorbing Boundaries</title><categories>cond-mat.other cs.NE nlin.CG</categories><comments>12 pages (LaTeX2e file) and 3 figures (separate PNG-files)</comments><abstract>  Spatio-temporal dynamics of excitable media with discrete three-level active
centers (ACs) and absorbing boundaries is studied numerically by means of a
deterministic three-level model (see S. D. Makovetskiy and D. N. Makovetskii,
on-line preprint cond-mat/0410460 ), which is a generalization of Zykov-
Mikhailov model (see Sov. Phys. -- Doklady, 1986, Vol.31, No.1, P.51) for the
case of two-channel diffusion of excitations. In particular, we revealed some
qualitatively new features of coexistence, competition and collapse of rotating
spiral waves (RSWs) in three-level excitable media under conditions of strong
influence of the second channel of diffusion. Part of these features are caused
by unusual mechanism of RSWs evolution when RSW's cores get into the surface
layer of an active medium (i.~e. the layer of ACs resided at the absorbing
boundary). Instead of well known scenario of RSW collapse, which takes place
after collision of RSW's core with absorbing boundary, we observed complicated
transformations of the core leading to nonlinear ''reflection'' of the RSW from
the boundary or even to birth of several new RSWs in the surface layer. To our
knowledge, such nonlinear ''reflections'' of RSWs and resulting die hard
vorticity in excitable media with absorbing boundaries were unknown earlier.
ACM classes: F.1.1, I.6, J.2; PACS numbers: 05.65.+b, 07.05.Tp, 82.20.Wt
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0602351</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0602351</id><created>2006-02-15</created><authors><author><keyname>Donetti</keyname><forenames>Luca</forenames></author><author><keyname>Hurtado</keyname><forenames>Pablo I.</forenames></author><author><keyname>Munoz</keyname><forenames>Miguel A.</forenames></author></authors><title>Synchronization in Network Structures: Entangled Topology as Optimal
  Architecture for Network Design</title><categories>cond-mat.dis-nn cs.NI</categories><comments>8 pages, 3 figs., to appear in Lecture Notes in Computer Science</comments><journal-ref>Lecture Notes in Computer Science 3993, 1075 (2006)</journal-ref><abstract>  In these notes we study synchronizability of dynamical processes defined on
complex networks as well as its interplay with network topology. Building from
a recent work by Barahona and Pecora [Phys. Rev. Lett. 89, 054101 (2002)], we
use a simulated annealing algorithm to construct optimally-synchronizable
networks. The resulting structures, known as entangled networks, are
characterized by an extremely homogeneous and interwoven topology: degree,
distance, and betweenness distributions are all very narrow, with short average
distances, large loops, and small modularity. Entangled networks exhibit an
excellent (almost optimal) performance with respect to other flow or
connectivity properties such as robustness, random walk minimal first-passage
times, and good searchability. All this converts entangled networks in a
powerful concept with optimal properties in many respects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0602611</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0602611</id><created>2006-02-26</created><updated>2006-02-28</updated><authors><author><keyname>Goltsev</keyname><forenames>A. V.</forenames></author><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>k-core (bootstrap) percolation on complex networks: Critical phenomena
  and nonlocal effects</title><categories>cond-mat.stat-mech cs.NI math-ph math.MP physics.soc-ph</categories><comments>11 pages, 8 figures</comments><journal-ref>Phys. Rev. E 73, 056101 (2006)</journal-ref><doi>10.1103/PhysRevE.73.056101</doi><abstract>  We develop the theory of the k-core (bootstrap) percolation on uncorrelated
random networks with arbitrary degree distributions. We show that the k-core
percolation is an unusual, hybrid phase transition with a jump emergence of the
k-core as at a first order phase transition but also with a critical
singularity as at a continuous transition. We describe the properties of the
k-core, explain the meaning of the order parameter for the k-core percolation,
and reveal the origin of the specific critical phenomena. We demonstrate that a
so-called ``corona'' of the k-core plays a crucial role (corona is a subset of
vertices in the k-core which have exactly k neighbors in the k-core). It turns
out that the k-core percolation threshold is at the same time the percolation
threshold of finite corona clusters. The mean separation of vertices in corona
clusters plays the role of the correlation length and diverges at the critical
point. We show that a random removal of even one vertex from the k-core may
result in the collapse of a vast region of the k-core around the removed
vertex. The mean size of this region diverges at the critical point. We find an
exact mapping of the k-core percolation to a model of cooperative relaxation.
This model undergoes critical relaxation with a divergent rate at some critical
moment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0602661</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0602661</id><created>2006-02-28</created><authors><author><keyname>Parisi</keyname><forenames>G.</forenames></author><author><keyname>Zamponi</keyname><forenames>F.</forenames></author></authors><title>On the high density behavior of Hamming codes with fixed minimum
  distance</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT</categories><comments>15 pages, 6 figures</comments><journal-ref>J.Stat.Phys. 123, 1145 (2006)</journal-ref><doi>10.1007/s10955-006-9142-7</doi><abstract>  We discuss the high density behavior of a system of hard spheres of diameter
d on the hypercubic lattice of dimension n, in the limit n -&gt; oo, d -&gt; oo,
d/n=delta. The problem is relevant for coding theory. We find a solution to the
equations describing the liquid up to very large values of the density, but we
show that this solution gives a negative entropy for the liquid phase when the
density is large enough. We then conjecture that a phase transition towards a
different phase might take place, and we discuss possible scenarios for this
transition. Finally we discuss the relation between our results and known
rigorous bounds on the maximal density of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0603189</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0603189</id><created>2006-03-07</created><updated>2007-07-01</updated><authors><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Chernyak</keyname><forenames>Vladimir Y.</forenames></author></authors><title>Loop series for discrete statistical models on graphs</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT</categories><comments>20 pages, 3 figures</comments><report-no>LAUR-06-1221</report-no><journal-ref>J. Stat. Mech. (2006) P06009</journal-ref><doi>10.1088/1742-5468/2006/06/P06009</doi><abstract>  In this paper we present derivation details, logic, and motivation for the
loop calculus introduced in \cite{06CCa}. Generating functions for three
inter-related discrete statistical models are each expressed in terms of a
finite series. The first term in the series corresponds to the Bethe-Peierls
(Belief Propagation)-BP contribution, the other terms are labeled by loops on
the factor graph. All loop contributions are simple rational functions of spin
correlation functions calculated within the BP approach. We discuss two
alternative derivations of the loop series. One approach implements a set of
local auxiliary integrations over continuous fields with the BP contribution
corresponding to an integrand saddle-point value. The integrals are replaced by
sums in the complimentary approach, briefly explained in \cite{06CCa}. A local
gauge symmetry transformation that clarifies an important invariant feature of
the BP solution, is revealed in both approaches. The partition function remains
invariant while individual terms change under the gauge transformation. The
requirement for all individual terms to be non-zero only for closed loops in
the factor graph (as opposed to paths with loose ends) is equivalent to fixing
the first term in the series to be exactly equal to the BP contribution.
Further applications of the loop calculus to problems in statistical physics,
computer and information sciences are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0603350</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0603350</id><created>2006-03-13</created><updated>2006-05-05</updated><authors><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames></author></authors><title>The number of matchings in random graphs</title><categories>cond-mat.dis-nn cs.CC math.CO</categories><comments>17 pages, 6 figures, to be published in Journal of Statistical
  Mechanics</comments><journal-ref>J. Stat. Mech. (2006) P05003</journal-ref><doi>10.1088/1742-5468/2006/05/P05003</doi><abstract>  We study matchings on sparse random graphs by means of the cavity method. We
first show how the method reproduces several known results about maximum and
perfect matchings in regular and Erdos-Renyi random graphs. Our main new result
is the computation of the entropy, i.e. the leading order of the logarithm of
the number of solutions, of matchings with a given size. We derive both an
algorithm to compute this entropy for an arbitrary graph with a girth that
diverges in the large size limit, and an analytic result for the entropy in
regular and Erdos-Renyi random graph ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0603861</identifier>
 <datestamp>2007-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0603861</id><created>2006-03-31</created><authors><author><keyname>Danila</keyname><forenames>Bogdan</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author><author><keyname>Earl</keyname><forenames>Samuel</forenames></author><author><keyname>Marsh</keyname><forenames>John A.</forenames></author><author><keyname>Toroczkai</keyname><forenames>Zoltan</forenames></author><author><keyname>Bassler</keyname><forenames>Kevin E.</forenames></author></authors><title>Congestion-gradient driven transport on complex networks</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.NI</categories><comments>11 pages, 8 figures</comments><journal-ref>Phys Rev E 74, 046114 (2006)</journal-ref><doi>10.1103/PhysRevE.74.046114</doi><abstract>  We present a study of transport on complex networks with routing based on
local information. Particles hop from one node of the network to another
according to a set of routing rules with different degrees of congestion
awareness, ranging from random diffusion to rigid congestion-gradient driven
flow. Each node can be either source or destination for particles and all nodes
have the same routing capacity, which are features of ad-hoc wireless networks.
It is shown that the transport capacity increases when a small amount of
congestion awareness is present in the routing rules, and that it then
decreases as the routing rules become too rigid when the flow becomes strictly
congestion-gradient driven. Therefore, an optimum value of the congestion
awareness exists in the routing rules. It is also shown that, in the limit of a
large number of nodes, networks using routing based on local information jam at
any nonzero load. Finally, we study the correlation between congestion at node
level and a betweenness centrality measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0604267</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0604267</id><created>2006-04-11</created><authors><author><keyname>Hatchett</keyname><forenames>Jonathan PL</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Survey propagation for the cascading Sourlas code</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>14 pages, 5 figures</comments><doi>10.1088/0305-4470/39/34/005</doi><abstract>  We investigate how insights from statistical physics, namely survey
propagation, can improve decoding of a particular class of sparse error
correcting codes. We show that a recently proposed algorithm, time averaged
belief propagation, is in fact intimately linked to a specific survey
propagation for which Parisi's replica symmetry breaking parameter is set to
zero, and that the latter is always superior to belief propagation in the high
connectivity limit. We briefly look at further improvements available by going
to the second level of replica symmetry breaking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0604569</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0604569</id><created>2006-04-25</created><authors><author><keyname>Klein</keyname><forenames>Einat</forenames></author><author><keyname>Gross</keyname><forenames>Noam</forenames></author><author><keyname>Kopelowitz</keyname><forenames>Evi</forenames></author><author><keyname>Rosenbluh</keyname><forenames>Michael</forenames></author><author><keyname>Khaykovich</keyname><forenames>Lev</forenames></author><author><keyname>Kinzel</keyname><forenames>Wolfgang</forenames></author><author><keyname>Kanter</keyname><forenames>Ido</forenames></author></authors><title>Public-channel cryptography based on mutual chaos pass filters</title><categories>cond-mat.stat-mech cs.CR</categories><doi>10.1103/PhysRevE.74.046201</doi><abstract>  We study the mutual coupling of chaotic lasers and observe both
experimentally and in numeric simulations, that there exists a regime of
parameters for which two mutually coupled chaotic lasers establish isochronal
synchronization, while a third laser coupled unidirectionally to one of the
pair, does not synchronize. We then propose a cryptographic scheme, based on
the advantage of mutual-coupling over unidirectional coupling, where all the
parameters of the system are public knowledge. We numerically demonstrate that
in such a scheme the two communicating lasers can add a message signal
(compressed binary message) to the transmitted coupling signal, and recover the
message in both directions with high fidelity by using a mutual chaos pass
filter procedure. An attacker however, fails to recover an errorless message
even if he amplifies the coupling signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0605190</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0605190</id><created>2006-05-08</created><updated>2006-09-08</updated><authors><author><keyname>Weigt</keyname><forenames>Martin</forenames></author><author><keyname>Zhou</keyname><forenames>Haijun</forenames></author></authors><title>Message passing for vertex covers</title><categories>cond-mat.stat-mech cs.DS</categories><comments>25 pages, 9 figures - version accepted for publication in PRE</comments><journal-ref>Phys. Rev. E 74, 046110 (2006)</journal-ref><doi>10.1103/PhysRevE.74.046110</doi><abstract>  Constructing a minimal vertex cover of a graph can be seen as a prototype for
a combinatorial optimization problem under hard constraints. In this paper, we
develop and analyze message passing techniques, namely warning and survey
propagation, which serve as efficient heuristic algorithms for solving these
computational hard problems. We show also, how previously obtained results on
the typical-case behavior of vertex covers of random graphs can be recovered
starting from the message passing equations, and how they can be extended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0605570</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0605570</id><created>2006-05-23</created><authors><author><keyname>Thistleton</keyname><forenames>William</forenames></author><author><keyname>Marsh</keyname><forenames>John A.</forenames></author><author><keyname>Nelson</keyname><forenames>Kenric</forenames></author><author><keyname>Tsallis</keyname><forenames>Constantino</forenames></author></authors><title>Generalized Box-Muller method for generating q-Gaussian random deviates</title><categories>cond-mat.stat-mech cs.MS</categories><comments>14 pages including 8 figures and a code</comments><abstract>  The q-Gaussian distribution is known to be an attractor of certain correlated
systems, and is the distribution which, under appropriate constraints,
maximizes the entropy Sq, basis of nonextensive statistical mechanics. This
theory is postulated as a natural extension of the standard (Boltzmann-Gibbs)
statistical mechanics, and may explain the ubiquitous appearance of
heavy-tailed distributions in both natural and man-made systems. The q-Gaussian
distribution is also used as a numerical tool, for example as a visiting
distribution in Generalized Simulated Annealing. We develop and present a
simple, easy to implement numerical method for generating random deviates from
a q-Gaussian distribution based upon a generalization of the well known
Box-Muller method. Our method is suitable for a larger range of q values, q&lt;3,
than has previously appeared in the literature, and can generate deviates from
q-Gaussian distributions of arbitrary width and center. MATLAB code showing a
straightforward implementation is also included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0606125</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0606125</id><created>2006-06-05</created><authors><author><keyname>Dall'Asta</keyname><forenames>Luca</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author></authors><title>Microscopic activity patterns in the Naming Game</title><categories>cond-mat.dis-nn cs.MA physics.soc-ph</categories><comments>submitted to J. Phys. A</comments><journal-ref>J. Phys. A: Math. Gen. 39 14851-14867 (2006)</journal-ref><doi>10.1088/0305-4470/39/48/002</doi><abstract>  The models of statistical physics used to study collective phenomena in some
interdisciplinary contexts, such as social dynamics and opinion spreading, do
not consider the effects of the memory on individual decision processes. On the
contrary, in the Naming Game, a recently proposed model of Language formation,
each agent chooses a particular state, or opinion, by means of a memory-based
negotiation process, during which a variable number of states is collected and
kept in memory. In this perspective, the statistical features of the number of
states collected by the agents becomes a relevant quantity to understand the
dynamics of the model, and the influence of topological properties on
memory-based models. By means of a master equation approach, we analyze the
internal agent dynamics of Naming Game in populations embedded on networks,
finding that it strongly depends on very general topological properties of the
system (e.g. average and fluctuations of the degree). However, the influence of
topological properties on the microscopic individual dynamics is a general
phenomenon that should characterize all those social interactions that can be
modeled by memory-based negotiation processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0606128</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0606128</id><created>2006-06-06</created><authors><author><keyname>Ramezanpour</keyname><forenames>A.</forenames></author><author><keyname>Moghimi-Araghi</keyname><forenames>S.</forenames></author></authors><title>Simplifying Random Satisfiability Problem by Removing Frustrating
  Interactions</title><categories>cond-mat.stat-mech cs.CC</categories><comments>21 pages, 16 figures</comments><doi>10.1103/PhysRevE.74.041105</doi><abstract>  How can we remove some interactions in a constraint satisfaction problem
(CSP) such that it still remains satisfiable? In this paper we study a modified
survey propagation algorithm that enables us to address this question for a
prototypical CSP, i.e. random K-satisfiability problem. The average number of
removed interactions is controlled by a tuning parameter in the algorithm. If
the original problem is satisfiable then we are able to construct satisfiable
subproblems ranging from the original one to a minimal one with minimum
possible number of interactions. The minimal satisfiable subproblems will
provide directly the solutions of the original problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0606696</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0606696</id><created>2006-06-27</created><authors><author><keyname>Mora</keyname><forenames>Thierry</forenames></author><author><keyname>Rivoire</keyname><forenames>Olivier</forenames></author></authors><title>Statistical mechanics of error exponents for error-correcting codes</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT</categories><comments>32 pages, 13 figures</comments><journal-ref>Phys. Rev. E 74, 056110 (2006)</journal-ref><doi>10.1103/PhysRevE.74.056110</doi><abstract>  Error exponents characterize the exponential decay, when increasing message
length, of the probability of error of many error-correcting codes. To tackle
the long standing problem of computing them exactly, we introduce a general,
thermodynamic, formalism that we illustrate with maximum-likelihood decoding of
low-density parity-check (LDPC) codes on the binary erasure channel (BEC) and
the binary symmetric channel (BSC). In this formalism, we apply the cavity
method for large deviations to derive expressions for both the average and
typical error exponents, which differ by the procedure used to select the codes
from specified ensembles. When decreasing the noise intensity, we find that two
phase transitions take place, at two different levels: a glass to ferromagnetic
transition in the space of codewords, and a paramagnetic to glass transition in
the space of codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0607017</identifier>
 <datestamp>2007-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0607017</id><created>2006-07-01</created><updated>2006-07-08</updated><authors><author><keyname>Danila</keyname><forenames>Bogdan</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author><author><keyname>Marsh</keyname><forenames>John A.</forenames></author><author><keyname>Bassler</keyname><forenames>Kevin E.</forenames></author></authors><title>Optimal routing on complex networks</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.NI</categories><comments>4 pages, 5 figures</comments><journal-ref>Phys Rev E 74, 046106 (2006)</journal-ref><doi>10.1103/PhysRevE.74.046106</doi><abstract>  We present a novel heuristic algorithm for routing optimization on complex
networks. Previously proposed routing optimization algorithms aim at avoiding
or reducing link overload. Our algorithm balances traffic on a network by
minimizing the maximum node betweenness with as little path lengthening as
possible, thus being useful in cases when networks are jamming due to queuing
overload. By using the resulting routing table, a network can sustain
significantly higher traffic without jamming than in the case of traditional
shortest path routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0607290</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0607290</id><created>2006-07-11</created><updated>2006-07-17</updated><authors><author><keyname>Bayati</keyname><forenames>Mohsen</forenames></author><author><keyname>Nair</keyname><forenames>Chandra</forenames></author></authors><title>A rigorous proof of the cavity method for counting matchings</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>11 pages, 1 figure</comments><abstract>  In this paper we rigorously prove the validity of the cavity method for the
problem of counting the number of matchings in graphs with large girth. Cavity
method is an important heuristic developed by statistical physicists that has
lead to the development of faster distributed algorithms for problems in
various combinatorial optimization problems. The validity of the approach has
been supported mostly by numerical simulations. In this paper we prove the
validity of cavity method for the problem of counting matchings using rigorous
techniques. We hope that these rigorous approaches will finally help us
establish the validity of the cavity method in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0607454</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0607454</id><created>2006-07-18</created><updated>2006-07-20</updated><authors><author><keyname>Venkatesan</keyname><forenames>R. C.</forenames></author></authors><title>Encryption of Covert Information into Multiple Statistical Distributions</title><categories>cond-mat.stat-mech cs.CR</categories><comments>18 pages, 4 figures. Three sentences expanded to emphasize detail.
  Typos corrected</comments><doi>10.1016/j.physleta.2007.05.117</doi><abstract>  A novel strategy to encrypt covert information (code) via unitary projections
into the null spaces of ill-conditioned eigenstructures of multiple host
statistical distributions, inferred from incomplete constraints, is presented.
The host pdf's are inferred using the maximum entropy principle. The projection
of the covert information is dependent upon the pdf's of the host statistical
distributions. The security of the encryption/decryption strategy is based on
the extreme instability of the encoding process. A self-consistent procedure to
derive keys for both symmetric and asymmetric cryptography is presented. The
advantages of using a multiple pdf model to achieve encryption of covert
information are briefly highlighted. Numerical simulations exemplify the
efficacy of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0608312</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0608312</id><created>2006-08-14</created><updated>2007-01-16</updated><authors><author><keyname>Rizzo</keyname><forenames>T.</forenames></author><author><keyname>Wemmenhove</keyname><forenames>B.</forenames></author><author><keyname>Kappen</keyname><forenames>H. J.</forenames></author></authors><title>On Cavity Approximations for Graphical Models</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT</categories><comments>Extension to factor graphs and comments on related work added</comments><doi>10.1103/PhysRevE.76.011102</doi><abstract>  We reformulate the Cavity Approximation (CA), a class of algorithms recently
introduced for improving the Bethe approximation estimates of marginals in
graphical models. In our new formulation, which allows for the treatment of
multivalued variables, a further generalization to factor graphs with arbitrary
order of interaction factors is explicitly carried out, and a message passing
algorithm that implements the first order correction to the Bethe approximation
is described. Furthermore we investigate an implementation of the CA for
pairwise interactions. In all cases considered we could confirm that CA[k] with
increasing $k$ provides a sequence of approximations of markedly increasing
precision. Furthermore in some cases we could also confirm the general
expectation that the approximation of order $k$, whose computational complexity
is $O(N^{k+1})$ has an error that scales as $1/N^{k+1}$ with the size of the
system. We discuss the relation between this approach and some recent
developments in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0609098</identifier>
 <datestamp>2007-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0609098</id><created>2006-09-05</created><authors><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Synchronization in Weighted Uncorrelated Complex Networks in a Noisy
  Environment: Optimization and Connections with Transport Efficiency</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.NI</categories><comments>Papers on related research can be found at
  http://www.rpi.edu/~korniss/Research/</comments><journal-ref>Phys. Rev. E 75, 051121 (2007)</journal-ref><doi>10.1103/PhysRevE.75.051121</doi><abstract>  Motivated by synchronization problems in noisy environments, we study the
Edwards-Wilkinson process on weighted uncorrelated scale-free networks. We
consider a specific form of the weights, where the strength (and the associated
cost) of a link is proportional to $(k_{i}k_{j})^{\beta}$ with $k_{i}$ and
$k_{j}$ being the degrees of the nodes connected by the link. Subject to the
constraint that the total network cost is fixed, we find that in the mean-field
approximation on uncorrelated scale-free graphs, synchronization is optimal at
$\beta^{*}$$=$-1. Numerical results, based on exact numerical diagonalization
of the corresponding network Laplacian, confirm the mean-field results, with
small corrections to the optimal value of $\beta^{*}$. Employing our recent
connections between the Edwards-Wilkinson process and resistor networks, and
some well-known connections between random walks and resistor networks, we also
pursue a naturally related problem of optimizing performance in queue-limited
communication networks utilizing local weighted routing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0609099</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0609099</id><created>2006-09-05</created><authors><author><keyname>Mora</keyname><forenames>Thierry</forenames><affiliation>LPTMS</affiliation></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames><affiliation>LPTMS</affiliation></author></authors><title>Geometrical organization of solutions to random linear Boolean equations</title><categories>cond-mat.dis-nn cs.CC</categories><comments>20 pages</comments><proxy>ccsd ccsd-00091334</proxy><journal-ref>Journal of Statistical Mechanics: Theory and Experiment (2006)
  P10007</journal-ref><doi>10.1088/1742-5468/2006/10/P10007</doi><abstract>  The random XORSAT problem deals with large random linear systems of Boolean
variables. The difficulty of such problems is controlled by the ratio of number
of equations to number of variables. It is known that in some range of values
of this parameter, the space of solutions breaks into many disconnected
clusters. Here we study precisely the corresponding geometrical organization.
In particular, the distribution of distances between these clusters is computed
by the cavity method. This allows to study the `x-satisfiability' threshold,
the critical density of equations where there exist two solutions at a given
distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0609584</identifier>
 <datestamp>2007-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0609584</id><created>2006-09-22</created><updated>2007-07-03</updated><authors><author><keyname>Bauke</keyname><forenames>Heiko</forenames></author><author><keyname>Mertens</keyname><forenames>Stephan</forenames></author></authors><title>Random numbers for large scale distributed Monte Carlo simulations</title><categories>cond-mat.other cs.DC</categories><journal-ref>Physical Review E, vol. 75, nr. 6 (2007), article 066701</journal-ref><doi>10.1103/PhysRevE.75.066701</doi><abstract>  Monte Carlo simulations are one of the major tools in statistical physics,
complex system science, and other fields, and an increasing number of these
simulations is run on distributed systems like clusters or grids. This raises
the issue of generating random numbers in a parallel, distributed environment.
In this contribution we demonstrate that multiple linear recurrences in finite
fields are an ideal method to produce high quality pseudorandom numbers in
sequential and parallel algorithms. Their known weakness (failure of sampling
points in high dimensions) can be overcome by an appropriate delinearization
that preserves all desirable properties of the underlying linear sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0611567</identifier>
 <datestamp>2009-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0611567</id><created>2006-11-21</created><updated>2009-02-09</updated><authors><author><keyname>Venkatesan</keyname><forenames>R. C.</forenames></author><author><keyname>Plastino</keyname><forenames>A.</forenames></author></authors><title>Generalized Statistics Framework for Rate Distortion Theory</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>31 pages, 2 figures. Minor formatting and typographical changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational principles for the rate distortion (RD) theory in lossy
compression are formulated within the ambit of the generalized nonextensive
statistics of Tsallis, for values of the nonextensivity parameter satisfying $
0 &lt; q &lt; 1 $ and $ q &gt; 1 $. Alternating minimization numerical schemes to
evaluate the nonextensive RD function, are derived. Numerical simulations
demonstrate the efficacy of generalized statistics RD models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0611717</identifier>
 <datestamp>2007-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0611717</id><created>2006-11-28</created><updated>2007-07-23</updated><authors><author><keyname>Baronchelli</keyname><forenames>A.</forenames></author><author><keyname>Dall'Asta</keyname><forenames>L.</forenames></author><author><keyname>Barrat</keyname><forenames>A.</forenames></author><author><keyname>Loreto</keyname><forenames>V.</forenames></author></authors><title>Non-equilibrium phase transition in negotiation dynamics</title><categories>cond-mat.stat-mech cs.MA physics.soc-ph q-bio.PE</categories><comments>4 pages, 4 figures</comments><journal-ref>Phys. Rev. E 76, 051102 (2007)</journal-ref><doi>10.1103/PhysRevE.76.051102</doi><abstract>  We introduce a model of negotiation dynamics whose aim is that of mimicking
the mechanisms leading to opinion and convention formation in a population of
individuals. The negotiation process, as opposed to ``herding-like'' or
``bounded confidence'' driven processes, is based on a microscopic dynamics
where memory and feedback play a central role. Our model displays a
non-equilibrium phase transition from an absorbing state in which all agents
reach a consensus to an active stationary state characterized either by
polarization or fragmentation in clusters of agents with different opinions. We
show the exystence of at least two different universality classes, one for the
case with two possible opinions and one for the case with an unlimited number
of opinions. The phase transition is studied analytically and numerically for
various topologies of the agents' interaction network. In both cases the
universality classes do not seem to depend on the specific interaction
topology, the only relevant feature being the total number of different
opinions ever present in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0612365</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0612365</id><created>2006-12-14</created><updated>2007-02-26</updated><authors><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>Federico</forenames></author><author><keyname>Semerjian</keyname><forenames>Guilhem</forenames></author><author><keyname>Zdeborova</keyname><forenames>Lenka</forenames></author></authors><title>Gibbs States and the Set of Solutions of Random Constraint Satisfaction
  Problems</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CC</categories><comments>6 pages, 6 figures, slightly revised version</comments><journal-ref>Proc. Natl. Acad. Sci. 104, 10318 (2007)</journal-ref><doi>10.1073/pnas.0703685104</doi><abstract>  An instance of a random constraint satisfaction problem defines a random
subset S (the set of solutions) of a large product space (the set of
assignments). We consider two prototypical problem ensembles (random
k-satisfiability and q-coloring of random regular graphs), and study the
uniform measure with support on S. As the number of constraints per variable
increases, this measure first decomposes into an exponential number of pure
states (&quot;clusters&quot;), and subsequently condensates over the largest such states.
Above the condensation point, the mass carried by the n largest states follows
a Poisson-Dirichlet process.
  For typical large instances, the two transitions are sharp. We determine for
the first time their precise location. Further, we provide a formal definition
of each phase transition in terms of different notions of correlation between
distinct variables in the problem.
  The degree of correlation naturally affects the performances of many
search/sampling algorithms. Empirical evidence suggests that local Monte Carlo
Markov Chain strategies are effective up to the clustering phase transition,
and belief propagation up to the condensation point. Finally, refined message
passing techniques (such as survey propagation) may beat also this threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0701184</identifier>
 <datestamp>2007-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0701184</id><created>2007-01-09</created><authors><author><keyname>Danila</keyname><forenames>Bogdan</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author><author><keyname>Marsh</keyname><forenames>John A.</forenames></author><author><keyname>Bassler</keyname><forenames>Kevin E.</forenames></author></authors><title>Transport optimization on complex networks</title><categories>cond-mat.dis-nn cs.NI</categories><comments>19 pages, 7 figures</comments><journal-ref>Chaos 17 (2), 026102 (2007)</journal-ref><doi>10.1063/1.2731718</doi><abstract>  We present a comparative study of the application of a recently introduced
heuristic algorithm to the optimization of transport on three major types of
complex networks. The algorithm balances network traffic iteratively by
minimizing the maximum node betweenness with as little path lengthening as
possible. We show that by using this optimal routing, a network can sustain
significantly higher traffic without jamming than in the case of shortest path
routing. A formula is proved that allows quick computation of the average
number of hops along the path and of the average travel times once the
betweennesses of the nodes are computed. Using this formula, we show that
routing optimization preserves the small-world character exhibited by networks
under shortest path routing, and that it significantly reduces the average
travel time on congested networks with only a negligible increase in the
average travel time at low loads. Finally, we study the correlation between the
weights of the links in the case of optimal routing and the betweennesses of
the nodes connected by them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0701218</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0701218</id><created>2007-01-10</created><updated>2007-01-15</updated><authors><author><keyname>Venkatesan</keyname><forenames>R. C.</forenames></author></authors><title>Generalized Statistics Framework for Rate Distortion Theory with Bregman
  Divergences</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>5 pages + 2 figures. Typos corrected</comments><abstract>  A variational principle for the rate distortion (RD) theory with Bregman
divergences is formulated within the ambit of the generalized (nonextensive)
statistics of Tsallis. The Tsallis-Bregman RD lower bound is established.
Alternate minimization schemes for the generalized Bregman RD (GBRD) theory are
derived. A computational strategy to implement the GBRD model is presented. The
efficacy of the GBRD model is exemplified with the aid of numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0701319</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0701319</id><created>2007-01-15</created><authors><author><keyname>Venkatesan</keyname><forenames>R. C.</forenames></author></authors><title>Statistical Cryptography using a Fisher-Schr\&quot;{o}dinger Model</title><categories>cond-mat.stat-mech cs.CR</categories><comments>8 pages + 2 figures</comments><abstract>  A principled procedure to infer a hierarchy of statistical distributions
possessing ill-conditioned eigenstructures, from incomplete constraints, is
presented. The inference process of the \textit{pdf}'s employs the Fisher
information as the measure of uncertainty, and, utilizes a semi-supervised
learning paradigm based on a measurement-response model. The principle
underlying the learning paradigm involves providing a quantum mechanical
connotation to statistical processes. The inferred \textit{pdf}'s constitute a
statistical host that facilitates the encryption/decryption of covert
information (code). A systematic strategy to encrypt/decrypt code via unitary
projections into the \textit{null spaces} of the ill-conditioned
eigenstructures, is presented. Numerical simulations exemplify the efficacy of
the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0702421</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0702421</id><created>2007-02-18</created><authors><author><keyname>Maneva</keyname><forenames>Elitza</forenames></author><author><keyname>Meltzer</keyname><forenames>Talya</forenames></author><author><keyname>Raymond</keyname><forenames>Jack</forenames></author><author><keyname>Sportiello</keyname><forenames>Andrea</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>A Hike in the Phases of the 1-in-3 Satisfiability</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CC</categories><comments>2 pages, introductory level, proceed. for the Les Houches Session
  LXXXV 2006 on Complex Systems</comments><abstract>  We summarise our results for the random $\epsilon$--1-in-3 satisfiability
problem, where $\epsilon$ is a probability of negation of the variable. We
employ both rigorous and heuristic methods to describe the SAT/UNSAT and
Hard/Easy transitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0702546</identifier>
 <datestamp>2007-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0702546</id><created>2007-02-23</created><updated>2007-06-18</updated><authors><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Kurchan</keyname><forenames>Jorge</forenames></author></authors><title>A Landscape Analysis of Constraint Satisfaction Problems</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CC nlin.CD</categories><comments>17 pages, 69 citations, 12 figures</comments><journal-ref>Phys. Rev. E 76, 021122 (2007)</journal-ref><doi>10.1103/PhysRevE.76.021122</doi><abstract>  We discuss an analysis of Constraint Satisfaction problems, such as Sphere
Packing, K-SAT and Graph Coloring, in terms of an effective energy landscape.
Several intriguing geometrical properties of the solution space become in this
light familiar in terms of the well-studied ones of rugged (glassy) energy
landscapes. A `benchmark' algorithm naturally suggested by this construction
finds solutions in polynomial time up to a point beyond the `clustering' and in
some cases even the `thermodynamic' transitions. This point has a simple
geometric meaning and can be in principle determined with standard Statistical
Mechanical methods, thus pushing the analytic bound up to which problems are
guaranteed to be easy. We illustrate this for the graph three and four-coloring
problem. For Packing problems the present discussion allows to better
characterize the `J-point', proposed as a systematic definition of Random Close
Packing, and to place it in the context of other theories of glasses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0702613</identifier>
 <datestamp>2007-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0702613</id><created>2007-02-26</created><authors><author><keyname>Marinari</keyname><forenames>Enzo</forenames></author><author><keyname>Semerjian</keyname><forenames>Guilhem</forenames></author><author><keyname>Van Kerrebroeck</keyname><forenames>Valery</forenames></author></authors><title>Finding long cycles in graphs</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CC math.PR</categories><journal-ref>Phys. Rev. E 75, 066708 (2007)</journal-ref><doi>10.1103/PhysRevE.75.066708</doi><abstract>  We analyze the problem of discovering long cycles inside a graph. We propose
and test two algorithms for this task. The first one is based on recent
advances in statistical mechanics and relies on a message passing procedure.
The second follows a more standard Monte Carlo Markov Chain strategy. Special
attention is devoted to Hamiltonian cycles of (non-regular) random graphs of
minimal connectivity equal to three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/0703351</identifier>
 <datestamp>2010-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/0703351</id><created>2007-03-13</created><updated>2007-12-25</updated><authors><author><keyname>Fedichkin</keyname><forenames>L.</forenames></author><author><keyname>Katz</keyname><forenames>E.</forenames></author><author><keyname>Privman</keyname><forenames>V.</forenames></author></authors><title>Error Correction and Digitalization Concepts in Biochemical Computing</title><categories>cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci cs.CE q-bio.BM quant-ph</categories><comments>14 pages in PDF</comments><journal-ref>Journal of Computational and Theoretical Nanoscience 5, 36-43
  (2008)</journal-ref><abstract>  We offer a theoretical design of new systems that show promise for digital
biochemical computing, including realizations of error correction by utilizing
redundancy, as well as signal rectification. The approach includes information
processing using encoded DNA sequences, DNAzyme biocatalyzed reactions and the
use of DNA-functionalized magnetic nanoparticles. Digital XOR and NAND logic
gates and copying (fanout) are designed using the same components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9703191</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9703191</id><created>1997-03-21</created><authors><author><keyname>H&#xe4;kkinen</keyname><forenames>J.</forenames><affiliation>Theoretical Physics, Lund U.</affiliation></author><author><keyname>Lagerholm</keyname><forenames>M.</forenames><affiliation>Theoretical Physics, Lund U.</affiliation></author><author><keyname>Peterson</keyname><forenames>C.</forenames><affiliation>Theoretical Physics, Lund U.</affiliation></author><author><keyname>S&#xf6;derberg</keyname><forenames>B.</forenames><affiliation>Theoretical Physics, Lund U.</affiliation></author></authors><title>A Potts Neuron Approach to Communication Routing</title><categories>cond-mat.dis-nn cs.NI hep-lat</categories><comments>10 pages LaTeX</comments><report-no>LU TP 97-02</report-no><journal-ref>Neural Computation 10, 1587-1599 (1998)</journal-ref><abstract>  A feedback neural network approach to communication routing problems is
developed with emphasis on Multiple Shortest Path problems, with several
requests for transmissions between distinct start- and endnodes. The basic
ingredients are a set of Potts neurons for each request, with interactions
designed to minimize path lengths and to prevent overloading of network arcs.
The topological nature of the problem is conveniently handled using a
propagator matrix approach. Although the constraints are global, the
algorithmic steps are based entirely on local information, facilitating
distributed implementations. In the polynomially solvable single-request case
the approach reduces to a fuzzy version of the Bellman-Ford algorithm. The
approach is evaluated for synthetic problems of varying sizes and load levels,
by comparing with exact solutions from a branch-and-bound method. With very few
exceptions, the Potts approach gives legal solutions of very high quality. The
computational demand scales merely as the product of the numbers of requests,
nodes, and arcs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9808130</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9808130</id><created>1998-08-12</created><authors><author><keyname>Novik</keyname><forenames>Keir E.</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author></authors><title>Finite-difference methods for simulation models incorporating
  non-conservative forces</title><categories>cond-mat.soft cs.NA math.NA physics.chem-ph physics.comp-ph physics.flu-dyn</categories><comments>27 pages RevTeX, 9 figures, J. Chem. Phys. (in press)</comments><doi>10.1063/1.477413</doi><abstract>  We discuss algorithms applicable to the numerical solution of second-order
ordinary differential equations by finite-differences. We make particular
reference to the solution of the dissipative particle dynamics fluid model, and
present extensive results comparing one of the algorithms discussed with the
standard method of solution. These results show the successful modeling of
phase separation and surface tension in a binary immiscible fluid mixture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9810144</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9810144</id><created>1998-10-13</created><updated>1999-01-26</updated><authors><author><keyname>Svenson</keyname><forenames>Pontus</forenames></author><author><keyname>Nordahl</keyname><forenames>Mats G.</forenames></author></authors><title>Relaxation in graph coloring and satisfiability problems</title><categories>cond-mat.dis-nn cs.AI</categories><comments>13 pages, 22 figures. Several changes to text, figures added, section
  on feromagnetic model moved to a separate publication. Accepted for
  publication in Phys Rev. E</comments><report-no>Goteborg ITP 98-15</report-no><journal-ref>Phys. Rev. E 59(4) 3983-3999 (1999)</journal-ref><doi>10.1103/PhysRevE.59.3983</doi><abstract>  Using T=0 Monte Carlo simulation, we study the relaxation of graph coloring
(K-COL) and satisfiability (K-SAT), two hard problems that have recently been
shown to possess a phase transition in solvability as a parameter is varied. A
change from exponentially fast to power law relaxation, and a transition to
freezing behavior are found. These changes take place for smaller values of the
parameter than the solvability transition. Results for the coloring problem for
colorable and clustered graphs and for the fraction of persistent spins for
satisfiability are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9810347</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9810347</id><created>1998-10-26</created><updated>1999-03-31</updated><authors><author><keyname>Beccaria</keyname><forenames>Matteo</forenames></author><author><keyname>Presilla</keyname><forenames>Carlo</forenames></author><author><keyname>De Angelis</keyname><forenames>Gian Fabrizio</forenames></author><author><keyname>Jona-Lasinio</keyname><forenames>Giovanni</forenames></author></authors><title>An exact representation of the fermion dynamics in terms of Poisson
  processes and its connection with Monte Carlo algorithms</title><categories>cond-mat cs.DS hep-lat math-ph math.MP quant-ph</categories><comments>4 pages, 1 PostScript figure, REVTeX</comments><journal-ref>Europhys.Lett. 48 (1999) 243-249</journal-ref><doi>10.1209/epl/i1999-00472-2</doi><abstract>  We present a simple derivation of a Feynman-Kac type formula to study
fermionic systems. In this approach the real time or the imaginary time
dynamics is expressed in terms of the evolution of a collection of Poisson
processes. A computer implementation of this formula leads to a family of
algorithms parametrized by the values of the jump rates of the Poisson
processes. From these an optimal algorithm can be chosen which coincides with
the Green Function Monte Carlo method in the limit when the latter becomes
exact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9812344</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9812344</id><created>1998-12-21</created><authors><author><keyname>Korniss</keyname><forenames>G.</forenames><affiliation>Florida State U.</affiliation></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames><affiliation>Florida State U.</affiliation></author><author><keyname>Rikvold</keyname><forenames>P. A.</forenames><affiliation>Florida State U.</affiliation></author></authors><title>Parallelization of a Dynamic Monte Carlo Algorithm: a Partially
  Rejection-Free Conservative Approach</title><categories>cond-mat.stat-mech cond-mat.mtrl-sci cs.DC physics.comp-ph</categories><comments>17 pages, 7 figures, RevTex; submitted to the Journal of
  Computational Physics</comments><report-no>FSU-SCRI-98-131</report-no><journal-ref>J. Comput. Phys. 153, 488 (1999)</journal-ref><doi>10.1006/jcph.1999.6291</doi><abstract>  We experiment with a massively parallel implementation of an algorithm for
simulating the dynamics of metastable decay in kinetic Ising models. The
parallel scheme is directly applicable to a wide range of stochastic cellular
automata where the discrete events (updates) are Poisson arrivals. For high
performance, we utilize a continuous-time, asynchronous parallel version of the
n-fold way rejection-free algorithm. Each processing element carries an lxl
block of spins, and we employ the fast SHMEM-library routines on the Cray T3E
distributed-memory parallel architecture. Different processing elements have
different local simulated times. To ensure causality, the algorithm handles the
asynchrony in a conservative fashion. Despite relatively low utilization and an
intricate relationship between the average time increment and the size of the
spin blocks, we find that for sufficiently large l the algorithm outperforms
its corresponding parallel Metropolis (non-rejection-free) counterpart. As an
example application, we present results for metastable decay in a model
ferromagnetic or ferroelectric film, observed with a probe of area smaller than
the total system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9902011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9902011</id><created>1999-02-01</created><authors><author><keyname>Tuckwell</keyname><forenames>Henry C.</forenames></author></authors><title>Cortical Potential Distributions and Cognitive Information Processing</title><categories>cond-mat.dis-nn adap-org cond-mat.stat-mech cs.NE math-ph math.MP nlin.AO physics.bio-ph q-bio</categories><comments>4 pages</comments><report-no>B3E/99/001</report-no><abstract>  The use of cortical field potentials rather than the details of spike trains
as the basis for cognitive information processing is proposed. This results in
a space of cognitive elements with natural metrics. Sets of spike trains may
also be considered to be points in a multidimensional metric space. The
closeness of sets of spike trains in such a space implies the closeness of
points in the resulting function space of potential distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9906017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9906017</id><created>1999-06-01</created><authors><author><keyname>Corona</keyname><forenames>Ricardo Mansilla</forenames></author></authors><title>Algorithmic Complexity in Minority Game</title><categories>cond-mat.stat-mech adap-org chao-dyn cs.CC nlin.AO nlin.CD</categories><comments>12 pages, 6 figures included</comments><abstract>  In this paper we introduce a new approach for the study of the complex
behavior of Minority Game using the tools of algorithmic complexity, physical
entropy and information theory. We show that physical complexity and mutual
information function strongly depend on memory size of the agents and yields
more information about the complex features of the stream of binary outcomes of
the game than volatility itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9906206</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9906206</id><created>1999-06-14</created><authors><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author><author><keyname>Koulakov</keyname><forenames>Alexei A.</forenames></author></authors><title>Ocular dominance patterns in mammalian visual cortex: A wire length
  minimization approach</title><categories>cond-mat.soft cond-mat.dis-nn cs.NE physics.bio-ph q-bio</categories><comments>9 pages, submitted to Journal of Neuroscience</comments><abstract>  We propose a theory for ocular dominance (OD) patterns in mammalian primary
visual cortex. This theory is based on the premise that OD pattern is an
adaptation to minimize the length of intra-cortical wiring. Thus we can
understand the existing OD patterns by solving a wire length minimization
problem. We divide all the neurons into two classes: left-eye dominated and
right-eye dominated. We find that segregation of neurons into monocular regions
reduces wire length if the number of connections with the neurons of the same
class differs from that with the other class. The shape of the regions depends
on the relative fraction of neurons in the two classes. If the numbers are
close we find that the optimal OD pattern consists of interdigitating stripes.
If one class is less numerous than the other, the optimal OD pattern consists
of patches of the first class neurons in the sea of the other class neurons. We
predict the transition from stripes to patches when the fraction of neurons
dominated by the ipsilateral eye is about 40%. This prediction agrees with the
data in macaque and Cebus monkeys. This theory can be applied to other binary
cortical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9907038</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9907038</id><created>1999-07-02</created><updated>1999-09-10</updated><authors><author><keyname>Albert</keyname><forenames>Reka</forenames><affiliation>University of Notre Dame</affiliation></author><author><keyname>Jeong</keyname><forenames>Hawoong</forenames><affiliation>University of Notre Dame</affiliation></author><author><keyname>Barabasi</keyname><forenames>Albert-Laszlo</forenames><affiliation>University of Notre Dame</affiliation></author></authors><title>The diameter of the world wide web</title><categories>cond-mat.dis-nn adap-org cond-mat.stat-mech cs.NI math-ph math.MP nlin.AO physics.comp-ph</categories><comments>5 pages, 1 figure, updated with most recent results on the size of
  the www</comments><journal-ref>Nature 401, 130-131 (1999)</journal-ref><doi>10.1038/43601</doi><abstract>  Despite its increasing role in communication, the world wide web remains the
least controlled medium: any individual or institution can create websites with
unrestricted number of documents and links. While great efforts are made to map
and characterize the Internet's infrastructure, little is known about the
topology of the web. Here we take a first step to fill this gap: we use local
connectivity measurements to construct a topological model of the world wide
web, allowing us to explore and characterize its large scale properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9907343</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9907343</id><created>1999-07-22</created><updated>1999-11-15</updated><authors><author><keyname>Biroli</keyname><forenames>Giulio</forenames></author><author><keyname>Monasson</keyname><forenames>Remi</forenames></author><author><keyname>Weigt</keyname><forenames>Martin</forenames></author></authors><title>A variational description of the ground state structure in random
  satisfiability problems</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>24 pages, 6 eps figures, to be published in Europ. Phys. J. B</comments><report-no>LPTENS 99/22</report-no><journal-ref>Eur. Phys. J. B 14, 551 (2000)</journal-ref><doi>10.1007/s100510051065</doi><abstract>  A variational approach to finite connectivity spin-glass-like models is
developed and applied to describe the structure of optimal solutions in random
satisfiability problems. Our variational scheme accurately reproduces the known
replica symmetric results and also allows for the inclusion of replica symmetry
breaking effects. For the 3-SAT problem, we find two transitions as the ratio
$\alpha$ of logical clauses per Boolean variables increases. At the first one
$\alpha_s \simeq 3.96$, a non-trivial organization of the solution space in
geometrically separated clusters emerges. The multiplicity of these clusters as
well as the typical distances between different solutions are calculated. At
the second threshold $\alpha_c \simeq 4.48$, satisfying assignments disappear
and a finite fraction $B_0 \simeq 0.13$ of variables are overconstrained and
take the same values in all optimal (though unsatisfying) assignments. These
values have to be compared to $\alpha_c \simeq 4.27, B_0 \simeq 0.4$ obtained
from numerical experiments on small instances. Within the present variational
approach, the SAT-UNSAT transition naturally appears as a mixture of a first
and a second order transition. For the mixed $2+p$-SAT with $p&lt;2/5$, the
behavior is as expected much simpler: a unique smooth transition from SAT to
UNSAT takes place at $\alpha_c=1/(1-p)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cond-mat/9909114</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cond-mat/9909114</id><created>1999-09-07</created><updated>2000-02-01</updated><authors><author><keyname>Korniss</keyname><forenames>G.</forenames></author><author><keyname>Toroczkai</keyname><forenames>Z.</forenames></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames></author><author><keyname>Rikvold</keyname><forenames>P. A.</forenames></author></authors><title>From Massively Parallel Algorithms and Fluctuating Time Horizons to
  Non-equilibrium Surface Growth</title><categories>cond-mat.stat-mech cs.DC physics.comp-ph</categories><comments>RevTex, 4 pages, 3 figures</comments><report-no>FSU-SCRI-99-58</report-no><journal-ref>Phys. Rev. Lett. 84, 1351 (2000).</journal-ref><doi>10.1103/PhysRevLett.84.1351</doi><abstract>  We study the asymptotic scaling properties of a massively parallel algorithm
for discrete-event simulations where the discrete events are Poisson arrivals.
The evolution of the simulated time horizon is analogous to a non-equilibrium
surface. Monte Carlo simulations and a coarse-grained approximation indicate
that the macroscopic landscape in the steady state is governed by the
Edwards-Wilkinson Hamiltonian. Since the efficiency of the algorithm
corresponds to the density of local minima in the associated surface, our
results imply that the algorithm is asymptotically scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001001</id><created>2000-01-03</created><authors><author><keyname>Vlasov</keyname><forenames>Alexander Yu.</forenames><affiliation>FRC/IRH, St.-Petersburg, Russia</affiliation></author></authors><title>Von Neumann Quantum Logic vs. Classical von Neumann Architecture?</title><categories>cs.OH quant-ph</categories><comments>4 pages LaTeXe, two columns, 1 inline logo, submitted to III
  International Conference on Soft Computing and Measurements SCM'2000</comments><acm-class>J.2; I.5.1; F.4</acm-class><abstract>  The name of John von Neumann is common both in quantum mechanics and computer
science. Are they really two absolutely unconnected areas? Many works devoted
to quantum computations and communications are serious argument to suggest
about existence of such a relation, but it is impossible to touch the new and
active theme in a short review. In the paper are described the structures and
models of linear algebra and just due to their generality it is possible to use
universal description of very different areas as quantum mechanics and theory
of Bayesian image analysis, associative memory, neural networks, fuzzy logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001002</id><created>2000-01-04</created><authors><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames></author></authors><title>Minimum Description Length and Compositionality</title><categories>cs.CL cs.AI</categories><acm-class>I.2.7</acm-class><journal-ref>H.Bunt and R.Muskens(Eds.) &quot;Computing Meaning&quot; Vol.1. Kluwer 1999.
  pp.113-128</journal-ref><abstract>  We present a non-vacuous definition of compositionality. It is based on the
idea of combining the minimum description length principle with the original
definition of compositionality (that is, that the meaning of the whole is a
function of the meaning of the parts).
  The new definition is intuitive and allows us to distinguish between
compositional and non-compositional semantics, and between idiomatic and
non-idiomatic expressions. It is not ad hoc, since it does not make any
references to non-intrinsic properties of meaning functions (like being a
polynomial). Moreover, it allows us to compare different meaning functions with
respect to how compositional they are. It bridges linguistic and corpus-based,
statistical approaches to natural language understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001003</id><created>2000-01-06</created><authors><author><keyname>Kiselyov</keyname><forenames>Oleg</forenames></author></authors><title>Why C++ is not very fit for GUI programming</title><categories>cs.PL</categories><comments>Previous version of this paper appeared in Proc. MacHack'95</comments><acm-class>D.3.3; D.1.5</acm-class><abstract>  With no intent of starting a holy war, this paper lists several annoying C++
birthmarks that the author has come across developing GUI class libraries.
C++'s view of classes, instances and hierarchies appears tantalizingly close to
GUI concepts of controls, widgets, window classes and subwindows. OO models of
C++ and of a window system are however different. C++ was designed to be a
&quot;static&quot; language with a lexical name scoping, static type checking and
hierarchies defined at compile time. Screen objects on the other hand are
inherently dynamic; they usually live well beyond the procedure/block that
created them; the hierarchy of widgets is defined to a large extent by layout,
visibility and event flow. Many GUI fundamentals such as dynamic and geometric
hierarchies of windows and controls, broadcasting and percolation of events are
not supported directly by C++ syntax or execution semantics (or supported as
&quot;exceptions&quot; -- pun intended). Therefore these features have to be emulated in
C++ GUI code. This leads to duplication of a graphical toolkit or a window
manager functionality, code bloat, engaging in unsafe practices and forgoing of
many strong C++ features (like scoping rules and compile-time type checking).
This paper enumerates a few major C++/GUI sores and illustrates them on simple
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001004</id><created>2000-01-07</created><authors><author><keyname>Akuzawa</keyname><forenames>Toshinao</forenames><affiliation>RIKEN BSI</affiliation></author></authors><title>Multiplicative Algorithm for Orthgonal Groups and Independent Component
  Analysis</title><categories>cs.LG</categories><comments>11 pages, 2 figures</comments><acm-class>G.1.6</acm-class><abstract>  The multiplicative Newton-like method developed by the author et al. is
extended to the situation where the dynamics is restricted to the orthogonal
group. A general framework is constructed without specifying the cost function.
Though the restriction to the orthogonal groups makes the problem somewhat
complicated, an explicit expression for the amount of individual jumps is
obtained. This algorithm is exactly second-order-convergent. The global
instability inherent in the Newton method is remedied by a
Levenberg-Marquardt-type variation. The method thus constructed can readily be
applied to the independent component analysis. Its remarkable performance is
illustrated by a numerical simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001005</identifier>
 <datestamp>2009-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001005</id><created>2000-01-07</created><authors><author><keyname>De Cnodder</keyname><forenames>Stefaan</forenames></author><author><keyname>Elloumi</keyname><forenames>Omar</forenames></author><author><keyname>Pauwels</keyname><forenames>Kenny</forenames></author></authors><title>Effect of different packet sizes on RED performance</title><categories>cs.NI</categories><comments>Effect of different packet size on RED performance number of pages: 9
  Submitted to IEEE Communication Letters</comments><acm-class>C.2.1</acm-class><abstract>  We consider the adaptation of random early detection (RED) as an active queue
management algorithm for TCP traffic in Internet gateways where different
maximum transfer units (MTUs) are used. We studied the two existing RED
variants and point out a weakness in both. The first variant where the drop
probability is independent from the packet size discriminates connections with
smaller MTUs. The second variant results in a very high Packet Loss Ratio
(PLR), and as a consequence low goodput, for connections with higher MTUs. We
show that fairness in terms of loss and goodput can be supplied through an
appropriate setting of the RED algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001006</id><created>2000-01-09</created><authors><author><keyname>Lappin</keyname><forenames>Shalom</forenames><affiliation>King's College, London</affiliation></author><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames><affiliation>IBM T.J. Watson Research Center</affiliation></author></authors><title>Compositionality, Synonymy, and the Systematic Representation of Meaning</title><categories>cs.CL cs.LO</categories><comments>Submitted to &quot;Linguistics and Philosophy&quot;</comments><acm-class>I.2.7; F.4.1</acm-class><abstract>  In a recent issue of Linguistics and Philosophy Kasmi and Pelletier (1998)
(K&amp;P), and Westerstahl (1998) criticize Zadrozny's (1994) argument that any
semantics can be represented compositionally. The argument is based upon
Zadrozny's theorem that every meaning function m can be encoded by a function
\mu such that (i) for any expression E of a specified language L, m(E) can be
recovered from \mu(E), and (ii) \mu is a homomorphism from the syntactic
structures of L to interpretations of L.
  In both cases, the primary motivation for the objections brought against
Zadrozny's argument is the view that his encoding of the original meaning
function does not properly reflect the synonymy relations posited for the
language.
  In this paper, we argue that these technical criticisms do not go through. In
particular, we prove that \mu properly encodes synonymy relations, i.e. if two
expressions are synonymous, then their compositional meanings are identical.
This corrects some misconceptions about the function \mu, e.g. Janssen (1997).
  We suggest that the reason that semanticists have been anxious to preserve
compositionality as a significant constraint on semantic theory is that it has
been mistakenly regarded as a condition that must be satisfied by any theory
that sustains a systematic connection between the meaning of an expression and
the meanings of its parts. Recent developments in formal and computational
semantics show that systematic theories of meanings need not be compositional.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001007</id><created>2000-01-10</created><authors><author><keyname>De Cnodder</keyname><forenames>Stefaan</forenames></author><author><keyname>Elloumi</keyname><forenames>Omar</forenames></author><author><keyname>Pauwels</keyname><forenames>Kenny</forenames></author></authors><title>RED behavior with different packet sizes</title><categories>cs.NI</categories><comments>13 pages, submitted to IEEE symposium on computer communications
  (ISCC2000)</comments><acm-class>C.2.1</acm-class><abstract>  We consider the adaptation of random early detection (RED) as a buffer
management algorithm for TCP traffic in Internet gateways where different
maximum transfer units (MTUs) are used. We studied the two RED variants
described in [4] and point out a weakness in both. The first variant where drop
probability is independent from the packet size discriminates connections with
smaller MTUs. The second variant results in a very high packet loss ratio
(PLR), and as a consequence low goodput, for connections with higher MTUs. We
show that fairness in terms of loss and goodput can be supplied through an
appropriate setting of the RED algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001008</id><created>2000-01-12</created><updated>2003-06-20</updated><authors><author><keyname>Vidal</keyname><forenames>Jose M.</forenames></author><author><keyname>Durfee</keyname><forenames>Edmund H.</forenames></author></authors><title>Predicting the expected behavior of agents that learn about agents: the
  CLRI framework</title><categories>cs.MA cs.LG</categories><acm-class>I.2.11</acm-class><journal-ref>Autonomous Agents and Multi-Agent Systems Journal, January 2003</journal-ref><abstract>  We describe a framework and equations used to model and predict the behavior
of multi-agent systems (MASs) with learning agents. A difference equation is
used for calculating the progression of an agent's error in its decision
function, thereby telling us how the agent is expected to fare in the MAS. The
equation relies on parameters which capture the agent's learning abilities,
such as its change rate, learning rate and retention rate, as well as relevant
aspects of the MAS such as the impact that agents have on each other. We
validate the framework with experimental results using reinforcement learning
agents in a market system, as well as with other experimental results gathered
from the AI literature. Finally, we use PAC-theory to show how to calculate
bounds on the values of the learning parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001009</id><created>2000-01-12</created><authors><author><keyname>Mateev</keyname><forenames>Nikolay</forenames></author><author><keyname>Menon</keyname><forenames>Vijay</forenames></author><author><keyname>Pingali</keyname><forenames>Keshav</forenames></author></authors><title>Fractal Symbolic Analysis</title><categories>cs.PL</categories><comments>13 pages, 19 figures</comments><acm-class>D.3.4</acm-class><abstract>  Restructuring compilers use dependence analysis to prove that the meaning of
a program is not changed by a transformation. A well-known limitation of
dependence analysis is that it examines only the memory locations read and
written by a statement, and does not assume any particular interpretation for
the operations in that statement. Exploiting the semantics of these operations
enables a wider set of transformations to be used, and is critical for
optimizing important codes such as LU factorization with pivoting.
  Symbolic execution of programs enables the exploitation of such semantic
properties, but it is intractable for all but the simplest programs. In this
paper, we propose a new form of symbolic analysis for use in restructuring
compilers. Fractal symbolic analysis compares a program and its transformed
version by repeatedly simplifying these programs until symbolic analysis
becomes tractable, ensuring that equality of simplified programs is sufficient
to guarantee equality of the original programs. We present a prototype
implementation of fractal symbolic analysis, and show how it can be used to
optimize the cache performance of LU factorization with pivoting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001010</id><created>2000-01-14</created><authors><author><keyname>Molla</keyname><forenames>D.</forenames></author><author><keyname>Berri</keyname><forenames>J.</forenames></author><author><keyname>Hess</keyname><forenames>M.</forenames></author></authors><title>A Real World Implementation of Answer Extraction</title><categories>cs.CL</categories><comments>5 pages</comments><acm-class>H.3.1; I.2.3; I.2.7</acm-class><journal-ref>Proc. of 9th International Conference and Workshop on Database and
  Expert Systems. Workshop &quot;Natural Language and Information Systems&quot;
  (NLIS'98). Vienna: 1998</journal-ref><abstract>  In this paper we describe ExtrAns, an answer extraction system. Answer
extraction (AE) aims at retrieving those exact passages of a document that
directly answer a given user question. AE is more ambitious than information
retrieval and information extraction in that the retrieval results are phrases,
not entire documents, and in that the queries may be arbitrarily specific. It
is less ambitious than full-fledged question answering in that the answers are
not generated from a knowledge base but looked up in the text of documents. The
current version of ExtrAns is able to parse unedited Unix &quot;man pages&quot;, and
derive the logical form of their sentences. User queries are also translated
into logical forms. A theorem prover then retrieves the relevant phrases, which
are presented through selective highlighting in their context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001011</id><created>2000-01-14</created><authors><author><keyname>Cranor</keyname><forenames>Lorrie Faith</forenames></author></authors><title>Agents of Choice: Tools that Facilitate Notice and Choice about Web Site
  Data Practices</title><categories>cs.CY</categories><comments>8 pages</comments><acm-class>K.4.1</acm-class><journal-ref>Proceedings of the 21st International Conference on Privacy and
  Personal Data Protection, 13-15 September 1999, Hong Kong SAR, China, p.
  19-25</journal-ref><abstract>  A variety of tools have been introduced recently that are designed to help
people protect their privacy on the Internet. These tools perform many
different functions in-cluding encrypting and/or anonymizing communications,
preventing the use of persistent identifiers such as cookies, automatically
fetching and analyzing web site privacy policies, and displaying
privacy-related information to users. This paper discusses the set of privacy
tools that aim specifically at facilitating notice and choice about Web site
data practices. While these tools may also have components that perform other
functions such as encryption, or they may be able to work in conjunction with
other privacy tools, the primary pur-pose of these tools is to help make users
aware of web site privacy practices and to make it easier for users to make
informed choices about when to provide data to web sites. Examples of such
tools include the Platform for Privacy Preferences (P3P) and various
infomediary services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001012</id><created>2000-01-18</created><authors><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Measures of Distributional Similarity</title><categories>cs.CL</categories><comments>9 pages, 3 figures</comments><acm-class>I.2.7</acm-class><journal-ref>37th Annual Meeting of the ACL, 1999, pp. 25-32</journal-ref><abstract>  We study distributional similarity measures for the purpose of improving
probability estimation for unseen cooccurrences. Our contributions are
three-fold: an empirical comparison of a broad range of measures; a
classification of similarity functions based on the information that they
incorporate; and the introduction of a novel function that is superior at
evaluating potential proxy distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001013</identifier>
 <datestamp>2012-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001013</id><created>2000-01-19</created><updated>2000-06-23</updated><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author></authors><title>Query Complexity: Worst-Case Quantum Versus Average-Case Classical</title><categories>cs.CC quant-ph</categories><comments>Withdrawn. The results in the paper only work for a certain subclass
  of Boolean functions, in which block sensitivity has properties similar to
  those of ordinary sensitivity. They don't work in general</comments><acm-class>F.1.2</acm-class><abstract>  In this note we investigate the relationship between worst-case quantum query
complexity and average-case classical query complexity. Specifically, we show
that if a quantum computer can evaluate a total Boolean function f with bounded
error using T queries in the worst case, then a deterministic classical
computer can evaluate f using O(T^5) queries in the average case, under a
uniform distribution of inputs. If f is monotone, we show furthermore that only
O(T^3) queries are needed. Previously, Beals et al. (1998) showed that if a
quantum computer can evaluate f with bounded error using T queries in the worst
case, then a deterministic classical computer can evaluate f using O(T^6)
queries in the worst case, or O(T^4) if f is monotone. The optimal bound is
conjectured to be O(T^2), but improving on O(T^6) remains an open problem.
Relating worst-case quantum complexity to average-case classical complexity may
suggest new ways to reduce the polynomial gap in the ordinary worst-case versus
worst-case setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001014</id><created>2000-01-19</created><updated>2004-01-15</updated><authors><author><keyname>de Wolf</keyname><forenames>Ronald</forenames><affiliation>CWI Amsterdam</affiliation></author></authors><title>Nondeterministic Quantum Query and Quantum Communication Complexities</title><categories>cs.CC quant-ph</categories><comments>19 pages, Latex</comments><acm-class>E.4; F.1.1; F.1.2; F.1.3; F.2.0</acm-class><journal-ref>SIAM Journal on Computing, 32(3):681-699, 2003</journal-ref><abstract>  We study nondeterministic quantum algorithms for Boolean functions f. Such
algorithms have positive acceptance probability on input x iff f(x)=1. In the
setting of query complexity, we show that the nondeterministic quantum
complexity of a Boolean function is equal to its ``nondeterministic
polynomial'' degree. We also prove a quantum-vs-classical gap of 1 vs n for
nondeterministic query complexity for a total function. In the setting of
communication complexity, we show that the nondeterministic quantum complexity
of a two-party function is equal to the logarithm of the rank of a
nondeterministic version of the communication matrix. This implies that the
quantum communication complexities of the equality and disjointness functions
are n+1 if we do not allow any error probability. We also exhibit a total
function in which the nondeterministic quantum communication complexity is
exponentially smaller than its classical counterpart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001015</id><created>2000-01-19</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Lakemeyer</keyname><forenames>Gerhard</forenames></author></authors><title>Multi-Agent Only Knowing</title><categories>cs.AI cs.LO</categories><comments>To appear, Journal of Logic and Computation</comments><acm-class>I.2.4, F.4.1</acm-class><abstract>  Levesque introduced a notion of ``only knowing'', with the goal of capturing
certain types of nonmonotonic reasoning. Levesque's logic dealt with only the
case of a single agent. Recently, both Halpern and Lakemeyer independently
attempted to extend Levesque's logic to the multi-agent case. Although there
are a number of similarities in their approaches, there are some significant
differences. In this paper, we reexamine the notion of only knowing, going back
to first principles. In the process, we simplify Levesque's completeness proof,
and point out some problems with the earlier definitions. This leads us to
reconsider what the properties of only knowing ought to be. We provide an axiom
system that captures our desiderata, and show that it has a semantics that
corresponds to it. The axiom system has an added feature of interest: it
includes a modal operator for satisfiability, and thus provides a complete
axiomatization for satisfiability in the logic K45.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001016</id><created>2000-01-21</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author></authors><title>Take-home Complexity</title><categories>cs.CY cs.GL</categories><comments>Updated version of Complexity Theory Column 20, Sigact News
  29(2):9-13, 1998</comments><acm-class>K.3.2; F.1.3</acm-class><abstract>  We discuss the use of projects in first-year graduate complexity theory
courses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001017</id><created>2000-01-21</created><authors><author><keyname>Hlusek</keyname><forenames>Radoslav</forenames></author></authors><title>Bezier Curves Intersection Using Relief Perspective</title><categories>cs.CG cs.GR</categories><comments>8 pages, 2 figures, to appear in Proceedings of WSCG'2000 in Plzen,
  Czech Republic</comments><acm-class>F.2.2; I.3.5; J.6</acm-class><abstract>  Presented paper describes the method for finding the intersection of class
space rational Bezier curves. The problem curve/curve intersection belongs
among basic geometric problems and the aim of this article is to describe the
new technique to solve the problem using relief perspective and Bezier
clipping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001018</id><created>2000-01-23</created><authors><author><keyname>Ingber</keyname><forenames>Lester</forenames></author></authors><title>Adaptive simulated annealing (ASA): Lessons learned</title><categories>cs.MS cs.CE</categories><comments>26 PostScript pages</comments><acm-class>G.1.6</acm-class><journal-ref>Control and Cybernetics 25 (1996) 33-54</journal-ref><abstract>  Adaptive simulated annealing (ASA) is a global optimization algorithm based
on an associated proof that the parameter space can be sampled much more
efficiently than by using other previous simulated annealing algorithms. The
author's ASA code has been publicly available for over two years. During this
time the author has volunteered to help people via e-mail, and the feedback
obtained has been used to further develop the code. Some lessons learned, in
particular some which are relevant to other simulated annealing algorithms, are
described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001019</id><created>2000-01-24</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>PushPush is NP-hard in 2D</title><categories>cs.CG cs.DM</categories><comments>18 pages, 13 figures, 1 table. Improves cs.CG/9911013</comments><report-no>Smith Technical Report 065</report-no><acm-class>F.2.2; G.2.m</acm-class><abstract>  We prove that a particular pushing-blocks puzzle is intractable in 2D,
improving an earlier result that established intractability in 3D [OS99]. The
puzzle, inspired by the game *PushPush*, consists of unit square blocks on an
integer lattice. An agent may push blocks (but never pull them) in attempting
to move between given start and goal positions. In the PushPush version, the
agent can only push one block at a time, and moreover, each block, when pushed,
slides the maximal extent of its free range. We prove this version is NP-hard
in 2D by reduction from SAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001020</id><created>2000-01-24</created><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames><affiliation>CLSP, The Johns Hopkins University</affiliation></author></authors><title>Exploiting Syntactic Structure for Natural Language Modeling</title><categories>cs.CL</categories><comments>Advisor: Frederick Jelinek, Ph.D. Thesis, 122 pages; removed unused
  .eps file</comments><acm-class>G.3, I.2.7, I.5.1, I.5.4</acm-class><abstract>  The thesis presents an attempt at using the syntactic structure in natural
language for improved language models for speech recognition. The structured
language model merges techniques in automatic parsing and language modeling
using an original probabilistic parameterization of a shift-reduce parser. A
maximum likelihood reestimation procedure belonging to the class of
expectation-maximization algorithms is employed for training the model.
Experiments on the Wall Street Journal, Switchboard and Broadcast News corpora
show improvement in both perplexity and word error rate - word lattice
rescoring - over the standard 3-gram language model. The significance of the
thesis lies in presenting an original approach to language modeling that uses
the hierarchical - syntactic - structure in natural language to improve on
current 3-gram modeling techniques for large vocabulary speech recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001021</id><created>2000-01-24</created><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames><affiliation>CLSP The Johns Hopkins University</affiliation></author><author><keyname>Jelinek</keyname><forenames>Frederick</forenames><affiliation>CLSP The Johns Hopkins University</affiliation></author></authors><title>Refinement of a Structured Language Model</title><categories>cs.CL</categories><comments>10 pages</comments><acm-class>G.3, I.2.7, I.5.1, I.5.4</acm-class><journal-ref>Proceedings of the International Conference on Advances in Pattern
  Recognition, 1998, pp. 275-284, Plymouth, UK</journal-ref><abstract>  A new language model for speech recognition inspired by linguistic analysis
is presented. The model develops hidden hierarchical structure incrementally
and uses it to extract meaningful information from the word history - thus
enabling the use of extended distance dependencies - in an attempt to
complement the locality of currently used n-gram Markov models. The model, its
probabilistic parametrization, a reestimation algorithm for the model
parameters and a set of experiments meant to evaluate its potential for speech
recognition are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001022</id><created>2000-01-24</created><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames><affiliation>CLSP The Johns Hopkins University</affiliation></author><author><keyname>Jelinek</keyname><forenames>Frederick</forenames><affiliation>CLSP The Johns Hopkins University</affiliation></author></authors><title>Recognition Performance of a Structured Language Model</title><categories>cs.CL</categories><comments>4 pages</comments><acm-class>G.3, I.2.7, I.5.1, I.5.4</acm-class><journal-ref>Proceedings of Eurospeech, 1999, pp. 1567-1570, Budapest, Hungary</journal-ref><abstract>  A new language model for speech recognition inspired by linguistic analysis
is presented. The model develops hidden hierarchical structure incrementally
and uses it to extract meaningful information from the word history - thus
enabling the use of extended distance dependencies - in an attempt to
complement the locality of currently used trigram models. The structured
language model, its probabilistic parameterization and performance in a
two-pass speech recognizer are presented. Experiments on the SWITCHBOARD corpus
show an improvement in both perplexity and word error rate over conventional
trigram models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001023</id><created>2000-01-25</created><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames><affiliation>CLSP, The Johns Hopkins University</affiliation></author><author><keyname>Jelinek</keyname><forenames>Frederick</forenames><affiliation>CLSP, The Johns Hopkins University</affiliation></author></authors><title>Structured Language Modeling for Speech Recognition</title><categories>cs.CL</categories><comments>4 pages + 2 pages of ERRATA</comments><acm-class>G.3, I.2.7, I.5.1, I.5.4</acm-class><journal-ref>Proceedings of NLDB'99, Klagenfurt, Austria</journal-ref><abstract>  A new language model for speech recognition is presented. The model develops
hidden hierarchical syntactic-like structure incrementally and uses it to
extract meaningful information from the word history, thus complementing the
locality of currently used trigram models. The structured language model (SLM)
and its performance in a two-pass speech recognizer --- lattice decoding ---
are presented. Experiments on the WSJ corpus show an improvement in both
perplexity (PPL) and word error rate (WER) over conventional trigram models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001024</id><created>2000-01-25</created><authors><author><keyname>Schlei</keyname><forenames>B. R.</forenames></author><author><keyname>Prasad</keyname><forenames>L.</forenames></author></authors><title>A Parallel Algorithm for Dilated Contour Extraction from Bilevel Images</title><categories>cs.CV</categories><comments>5 pages, including 3 figures. For additional detail check
  http://www.nis.lanl.gov/~bschlei/labvis/index.html</comments><report-no>Los Alamos Preprint LA-UR-00-309</report-no><acm-class>I.2.10, D.1.3, G.1.2</acm-class><abstract>  We describe a simple, but efficient algorithm for the generation of dilated
contours from bilevel images. The initial part of the contour extraction is
explained to be a good candidate for parallel computer code generation. The
remainder of the algorithm is of linear nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001025</id><created>2000-01-28</created><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Computational Geometry Column 38</title><categories>cs.CG cs.CV</categories><comments>3 pages, 1 figure, 18 refs</comments><acm-class>F.2.2; I.5.3</acm-class><abstract>  Recent results on curve reconstruction are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001026</id><created>2000-01-28</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author></authors><title>A Logic for SDSI's Linked Local Name Spaces</title><categories>cs.CR cs.LO</categories><comments>To appear, Journal of Computer Security</comments><acm-class>D.4.6, K.6.5, C.2.0, F,4,1</acm-class><abstract>  Abadi has introduced a logic to explicate the meaning of local names in SDSI,
the Simple Distributed Security Infrastructure proposed by Rivest and Lampson.
Abadi's logic does not correspond precisely to SDSI, however; it draws
conclusions about local names that do not follow from SDSI's name resolution
algorithm. Moreover, its semantics is somewhat unintuitive. This paper presents
the Logic of Local Name Containment, which does not suffer from these
deficiencies. It has a clear semantics and provides a tight characterization of
SDSI name resolution. The semantics is shown to be closely related to that of
logic programs, leading to an approach to the efficient implementation of
queries concerning local names. A complete axiomatization of the logic is also
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0001027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0001027</id><created>2000-01-28</created><authors><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames><affiliation>Santa Fe Institute</affiliation></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames><affiliation>Santa Fe Institute</affiliation></author></authors><title>Pattern Discovery and Computational Mechanics</title><categories>cs.LG cs.NE</categories><comments>12 pages, 3 figures; submitted to the Proceedings of the 17th
  International Conference on Machine Learning (differs slightly in pagination
  and citation format from that version)</comments><report-no>SFI 00-01-008</report-no><acm-class>I.2.6; F.1.3; G.3; H.1.1</acm-class><abstract>  Computational mechanics is a method for discovering, describing and
quantifying patterns, using tools from statistical physics. It constructs
optimal, minimal models of stochastic processes and their underlying causal
structures. These models tell us about the intrinsic computation embedded
within a process---how it stores and transforms information. Here we summarize
the mathematics of computational mechanics, especially recent optimality and
uniqueness results. We also expound the principles and motivations underlying
computational mechanics, emphasizing its connections to the minimum description
length principle, PAC theory, and other aspects of machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002001</id><created>2000-02-03</created><updated>2000-12-13</updated><authors><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Computing large and small stable models</title><categories>cs.LO cs.AI</categories><comments>This paper is a full version of the conference paper of the same
  title that was published in the Proceedings of the 1999 International
  Conference on Logic Programming, Las Cruces, New Mexico, MIT Press, pp.
  169-183. The proofs of the results in Section 4 were replaced by more elegant
  and general ones</comments><acm-class>I.2.3;I.2.4</acm-class><journal-ref>Theory and Practice of Logic Programming, 2(1), pp. 2002</journal-ref><abstract>  In this paper, we focus on the problem of existence and computing of small
and large stable models. We show that for every fixed integer k, there is a
linear-time algorithm to decide the problem LSM (large stable models problem):
does a logic program P have a stable model of size at least |P|-k. In contrast,
we show that the problem SSM (small stable models problem) to decide whether a
logic program P has a stable model of size at most k is much harder. We present
two algorithms for this problem but their running time is given by polynomials
of order depending on k. We show that the problem SSM is fixed-parameter
intractable by demonstrating that it is W[2]-hard. This result implies that it
is unlikely, an algorithm exists to compute stable models of size at most k
that would run in time O(n^c), where c is a constant independent of k. We also
provide an upper bound on the fixed-parameter complexity of the problem SSM by
showing that it belongs to the class W[3].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002002</id><created>2000-02-03</created><authors><author><keyname>Denecker</keyname><forenames>Marc</forenames></author><author><keyname>Marek</keyname><forenames>Victor W.</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Uniform semantic treatment of default and autoepistemic logics</title><categories>cs.AI</categories><comments>Proceedings of the Seventh International Conference on Principles of
  Knowledge Representation and Reasoning (KR2000); 11 pages</comments><acm-class>I.2.3; I.2.4</acm-class><journal-ref>Artificial Intelligence Journal, 143 (2003), pp. 79--122</journal-ref><abstract>  We revisit the issue of connections between two leading formalisms in
nonmonotonic reasoning: autoepistemic logic and default logic. For each logic
we develop a comprehensive semantic framework based on the notion of a belief
pair. The set of all belief pairs together with the so called knowledge
ordering forms a complete lattice. For each logic, we introduce several
semantics by means of fixpoints of operators on the lattice of belief pairs.
Our results elucidate an underlying isomorphism of the respective semantic
constructions. In particular, we show that the interpretation of defaults as
modal formulas proposed by Konolige allows us to represent all semantics for
default logic in terms of the corresponding semantics for autoepistemic logic.
Thus, our results conclusively establish that default logic can indeed be
viewed as a fragment of autoepistemic logic. However, as we also demonstrate,
the semantics of Moore and Reiter are given by different operators and occupy
different locations in their corresponding families of semantics. This result
explains the source of the longstanding difficulty to formally relate these two
semantics. In the paper, we also discuss approximating skeptical reasoning with
autoepistemic and default logics and establish constructive principles behind
such approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002003</id><created>2000-02-04</created><authors><author><keyname>East</keyname><forenames>Deborah</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>On the accuracy and running time of GSAT</title><categories>cs.AI</categories><comments>Proceedings of the 9th Portuguese Conference on Artificial
  Intelligence (EPIA'99), Lecture Notes in Artificial Intelligence, vol. 1695,
  Springer-Verlag, 1999</comments><acm-class>I.2.8</acm-class><abstract>  Randomized algorithms for deciding satisfiability were shown to be effective
in solving problems with thousands of variables. However, these algorithms are
not complete. That is, they provide no guarantee that a satisfying assignment,
if one exists, will be found. Thus, when studying randomized algorithms, there
are two important characteristics that need to be considered: the running time
and, even more importantly, the accuracy --- a measure of likelihood that a
satisfying assignment will be found, provided one exists. In fact, we argue
that without a reference to the accuracy, the notion of the running time for
randomized algorithms is not well-defined. In this paper, we introduce a formal
notion of accuracy. We use it to define a concept of the running time. We use
both notions to study the random walk strategy GSAT algorithm. We investigate
the dependence of accuracy on properties of input formulas such as
clause-to-variable ratio and the number of satisfying assignments. We
demonstrate that the running time of GSAT grows exponentially in the number of
variables of the input formula for randomly generated 3-CNF formulas and for
the formulas encoding 3- and 4-colorability of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002004</id><created>2000-02-04</created><authors><author><keyname>Bryans</keyname><forenames>Jeremy</forenames></author><author><keyname>Bowman</keyname><forenames>Howard</forenames></author><author><keyname>Derrick</keyname><forenames>John</forenames></author></authors><title>Stochastic Model Checking for Multimedia</title><categories>cs.MM cs.LO</categories><comments>35 pages; 6 figures</comments><acm-class>F.3.1; F.4.1; G.3</acm-class><abstract>  Modern distributed systems include a class of applications in which
non-functional requirements are important. In particular, these applications
include multimedia facilities where real time constraints are crucial to their
correct functioning. In order to specify such systems it is necessary to
describe that events occur at times given by probability distributions and
stochastic automata have emerged as a useful technique by which such systems
can be specified and verified.
  However, stochastic descriptions are very general, in particular they allow
the use of general probability distribution functions, and therefore their
verification can be complex. In the last few years, model checking has emerged
as a useful verification tool for large systems.
  In this paper we describe two model checking algorithms for stochastic
automata. These algorithms consider how properties written in a simple
probabilistic real-time logic can be checked against a given stochastic
automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002005</id><created>2000-02-08</created><authors><author><keyname>Mohapatra</keyname><forenames>Pradosh Kumar</forenames></author></authors><title>Fully Sequential and Distributed Dynamic Algorithms for Minimum Spanning
  Trees</title><categories>cs.DC cs.DS</categories><comments>32 pages, 4 figures</comments><acm-class>G.2.2;F.2.2</acm-class><abstract>  In this paper, we present a fully-dynamic distributed algorithm for
maintaining a minimum spanning tree on general graphs with positive real edge
weights. The goal of a dynamic MST algorithm is to update efficiently the
minimum spanning tree after dynamic changes like edge weight changes, rather
than having to recompute it from scatch each time. The first part of the paper
surveys various algorithms available today both in sequential and distributed
environments to solve static MST problem. We also present some of the efficient
sequential algorithms for computing dynamic MST like the Frederickson's
algorithm and Eppstein's sparsification technique. Lastly we present our new
sequential and distributed algorithms for dynamic MST problem. To our
knowledge, this is the first of the distributed algorithms for computing
dynamic MSTs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002006</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002006</id><created>2000-02-09</created><authors><author><keyname>Akuzawa</keyname><forenames>Toshinao</forenames><affiliation>RIKEN BSI</affiliation></author><author><keyname>Murata</keyname><forenames>Noboru</forenames><affiliation>RIKEN BSI</affiliation></author></authors><title>Multiplicative Nonholonomic/Newton -like Algorithm</title><categories>cs.LG</categories><comments>12 pages</comments><acm-class>G.1.6</acm-class><doi>10.1016/S0960-0779(00)00077-1</doi><abstract>  We construct new algorithms from scratch, which use the fourth order cumulant
of stochastic variables for the cost function. The multiplicative updating rule
here constructed is natural from the homogeneous nature of the Lie group and
has numerous merits for the rigorous treatment of the dynamics. As one
consequence, the second order convergence is shown. For the cost function,
functions invariant under the componentwise scaling are choosen. By identifying
points which can be transformed to each other by the scaling, we assume that
the dynamics is in a coset space. In our method, a point can move toward any
direction in this coset. Thus, no prewhitening is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002007</id><created>2000-02-11</created><authors><author><keyname>Litkowski</keyname><forenames>K.</forenames></author></authors><title>Requirements of Text Processing Lexicons</title><categories>cs.CL</categories><comments>HTML File (72k, about 20 pages). Paper for which extended abstract
  appears in ACL proceedings of 1980</comments><acm-class>H.3.1; I.2.7; I.2.4</acm-class><journal-ref>Proceedings of the 18th Annual Meeting of the Association for
  Computational Linguistics, Philadelphia, PA (1980), pp. 153-4</journal-ref><abstract>  As text processing systems expand in scope, they will require ever larger
lexicons along with a parsing capability for discriminating among many senses
of a word. Existing systems do not incorporate such subtleties in meaning for
their lexicons. Ordinary dictionaries contain such information, but are largely
untapped. When the contents of dictionaries are scrutinized, they reveal many
requirements that must be satisfied in representing meaning and in developing
semantic parsers. These requirements were identified in research designed to
find primitive verb concepts. The requirements are outlined and general
procedures for satisfying them through the use of ordinary dictionaries are
described, illustrated by building frames for and examining the definitions of
&quot;change&quot; and its uses as a hypernym in other definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002008</id><created>2000-02-15</created><authors><author><keyname>Gates</keyname><forenames>R.</forenames></author><author><keyname>Katis</keyname><forenames>P.</forenames></author><author><keyname>Sabadini</keyname><forenames>N.</forenames></author><author><keyname>Walters</keyname><forenames>R. F. C.</forenames></author></authors><title>On Automata with Boundary</title><categories>cs.DC</categories><comments>41 pages, 22 figures. Uses Paul Taylor's diagrams macros, see
  http://www.ctan.org/tex-archive/macros/generic/diagrams/taylor/</comments><report-no>C/TR00-01</report-no><acm-class>D.2.2; D.2.4; F.1.1</acm-class><abstract>  We present a theory of automata with boundary for designing, modelling and
analysing distributed systems. Notions of behaviour, design and simulation
appropriate to the theory are defined. The problem of model checking for
deadlock detection is discussed, and an algorithm for state space reduction in
exhaustive search, based on the theory presented here, is described. Three
examples of the application of the theory are given, one in the course of the
development of the ideas and two as illustrative examples of the use of the
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002009</id><created>2000-02-16</created><authors><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Syntactic Autonomy: Why There is no Autonomy without Symbols and How
  Self-Organization Might Evolve Them</title><categories>cs.AI</categories><acm-class>A.m</acm-class><abstract>  Two different types of agency are discussed based on dynamically coherent and
incoherent couplings with an environment respectively. I propose that until a
private syntax (syntactic autonomy) is discovered by dynamically coherent
agents, there are no significant or interesting types of closure or autonomy.
When syntactic autonomy is established, then, because of a process of
description-based selected self-organization, open-ended evolution is enabled.
At this stage, agents depend, in addition to dynamics, on localized, symbolic
memory, thus adding a level of dynamical incoherence to their interaction with
the environment. Furthermore, it is the appearance of syntactic autonomy which
enables much more interesting types of closures amongst agents which share the
same syntax. To investigate how we can study the emergence of syntax from
dynamical systems, experiments with cellular automata leading to emergent
computation to solve non-trivial tasks are discussed. RNA editing is also
mentioned as a process that may have been used to obtain a primordial
biological code necessary open-ended evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002010</id><created>2000-02-16</created><authors><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author><author><keyname>Bollen</keyname><forenames>Johan</forenames></author></authors><title>Biologically Motivated Distributed Designs for Adaptive Knowledge
  Management</title><categories>cs.IR</categories><comments>To appear in Design Principles for the Immune System and Other
  Distributed Autonomous Systems. i. Cohen and L. Segel (Eds.). Oxford
  University Press</comments><acm-class>H.3, H.1, H.2, I.2</acm-class><abstract>  We discuss how distributed designs that draw from biological network
metaphors can largely improve the current state of information retrieval and
knowledge management of distributed information systems. In particular, two
adaptive recommendation systems named TalkMine and @ApWeb are discussed in more
detail. TalkMine operates at the semantic level of keywords. It leads different
databases to learn new and adapt existing keywords to the categories recognized
by its communities of users using distributed algorithms. @ApWeb operates at
the structural level of information resources, namely citation or hyperlink
structure. It relies on collective behavior to adapt such structure to the
expectations of users. TalkMine and @ApWeb are currently being implemented for
the research library of the Los Alamos National Laboratory under the Active
Recommendation Project. Together they define a biologically motivated
information retrieval system, recommending simultaneously at the level of user
knowledge categories expressed in keywords, and at the level of individual
documents and their associations to other documents. Rather than passive
information retrieval, with this system, users obtain an active, evolving
interaction with information resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002011</id><created>2000-02-17</created><authors><author><keyname>Maxemchuk</keyname><forenames>N. F.</forenames></author><author><keyname>Shur</keyname><forenames>D. H.</forenames></author></authors><title>An Internet Multicast System for the Stock Market</title><categories>cs.NI</categories><comments>19 pages, postscript</comments><acm-class>C.2</acm-class><abstract>  We are moving toward a distributed, international, twenty-four hour,
electronic stock exchange. The exchange will use the global Internet, or
internet technology. This system is a natural application of multicast because
there are a large number of receivers that should receive the same information
simultaneously.
  The data requirements for the stock exchange are discussed. The current
multicast protocols lack the reliability, fairness, and scalability needed in
this application. We describe a distributed architecture together with a
reliable multicast protocol, a modification of the RMP protocol, that has
characteristics appropriate for this application.
  The architecture is used in three applications: In the first, we construct a
unified stock ticker of the transactions that are being conducted on the
various physical and electronic exchanges. Our objective is to deliver the the
same combined ticker reliably and simultaneously to all receivers, anywhere in
the world. In the second, we construct a unified sequence of buy and sell
offers that are delivered to a single exchange or a collection of exchanges.
Our objective is to give all traders the same fair access to an exchange
independent of their relative distances to the exchange or the loss
characteristics of the international network. In the third, we construct a
distributed, electronic trading floor that can replace the current exchanges.
This application uses the innovations from the first two applications to
combine their fairness attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002012</id><created>2000-02-17</created><authors><author><keyname>Li</keyname><forenames>Ming</forenames></author><author><keyname>Ma</keyname><forenames>Bin</forenames></author><author><keyname>Wang</keyname><forenames>Lusheng</forenames></author></authors><title>On The Closest String and Substring Problems</title><categories>cs.CE cs.CC</categories><acm-class>F.2;J.3</acm-class><abstract>  The problem of finding a center string that is `close' to every given string
arises and has many applications in computational biology and coding theory.
This problem has two versions: the Closest String problem and the Closest
Substring problem. Assume that we are given a set of strings ${\cal S}=\{s_1,
s_2, ..., s_n\}$ of strings, say, each of length $m$. The Closest String
problem asks for the smallest $d$ and a string $s$ of length $m$ which is
within Hamming distance $d$ to each $s_i\in {\cal S}$. This problem comes from
coding theory when we are looking for a code not too far away from a given set
of codes. The problem is NP-hard. Berman et al give a polynomial time algorithm
for constant $d$. For super-logarithmic $d$, Ben-Dor et al give an efficient
approximation algorithm using linear program relaxation technique. The best
polynomial time approximation has ratio 4/3 for all $d$ given by Lanctot et al
and Gasieniec et al. The Closest Substring problem looks for a string $t$ which
is within Hamming distance $d$ away from a substring of each $s_i$. This
problem only has a $2- \frac{2}{2|\Sigma|+1}$ approximation algorithm
previously Lanctot et al and is much more elusive than the Closest String
problem, but it has many applications in finding conserved regions, genetic
drug target identification, and genetic probes in molecular biology. Whether
there are efficient approximation algorithms for both problems are major open
questions in this area. We present two polynomial time approxmation algorithms
with approximation ratio $1+ \epsilon$ for any small $\epsilon$ to settle both
questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002013</id><created>2000-02-18</created><authors><author><keyname>Loyer</keyname><forenames>Y.</forenames></author><author><keyname>Spyratos</keyname><forenames>N.</forenames></author><author><keyname>Stamate</keyname><forenames>D.</forenames></author></authors><title>Computing and Comparing Semantics of Programs in Multi-valued Logics</title><categories>cs.LO cs.DB</categories><comments>10 pages, 1 figure, A preliminary version of this paper appeared in
  the form of an extended abstract in the conference Mathematical Foundations
  of Computer Science (MFCS'99)</comments><acm-class>H.0;I.2.3</acm-class><abstract>  The different semantics that can be assigned to a logic program correspond to
different assumptions made concerning the atoms whose logical values cannot be
inferred from the rules. Thus, the well founded semantics corresponds to the
assumption that every such atom is false, while the Kripke-Kleene semantics
corresponds to the assumption that every such atom is unknown. In this paper,
we propose to unify and extend this assumption-based approach by introducing
parameterized semantics for logic programs. The parameter holds the value that
one assumes for all atoms whose logical values cannot be inferred from the
rules. We work within multi-valued logic with bilattice structure, and we
consider the class of logic programs defined by Fitting.
  Following Fitting's approach, we define a simple operator that allows us to
compute the parameterized semantics, and to compare and combine semantics
obtained for different values of the parameter. The semantics proposed by
Fitting corresponds to the value false. We also show that our approach captures
and extends the usual semantics of conventional logic programs thereby unifying
their computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002014</id><created>2000-02-24</created><authors><author><keyname>Ghrist</keyname><forenames>Robert</forenames></author><author><keyname>Koditschek</keyname><forenames>Daniel</forenames></author></authors><title>Safe cooperative robot dynamics on graphs</title><categories>cs.RO cs.AI</categories><comments>18 pages, 5 figures</comments><acm-class>I.2.9</acm-class><abstract>  This paper initiates the use of vector fields to design, optimize, and
implement reactive schedules for safe cooperative robot patterns on planar
graphs. We consider Automated Guided Vehicles (AGV's) operating upon a
predefined network of pathways. In contrast to the case of locally Euclidean
configuration spaces, regularization of collisions is no longer a local
procedure, and issues concerning the global topology of configuration spaces
must be addressed. The focus of the present inquiry is the achievement of safe,
efficient, cooperative patterns in the simplest nontrivial example (a pair of
robots on a Y-network) by means of a state-event heirarchical controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002015</id><created>2000-02-24</created><authors><author><keyname>Nicolas</keyname><forenames>P.</forenames><affiliation>University of Angers, France</affiliation></author><author><keyname>Saubion</keyname><forenames>F.</forenames><affiliation>University of Angers, France</affiliation></author><author><keyname>Stephan</keyname><forenames>I.</forenames><affiliation>University of Angers, France</affiliation></author></authors><title>Genetic Algorithms for Extension Search in Default Logic</title><categories>cs.AI cs.LO</categories><comments>8 pages, 3 figures, 2 tables</comments><acm-class>F.4.1</acm-class><abstract>  A default theory can be characterized by its sets of plausible conclusions,
called its extensions. But, due to the theoretical complexity of Default Logic
(Sigma_2p-complete), the problem of finding such an extension is very difficult
if one wants to deal with non trivial knowledge bases. Based on the principle
of natural selection, Genetic Algorithms have been quite successfully applied
to combinatorial problems and seem useful for problems with huge search spaces
and when no tractable algorithm is available. The purpose of this paper is to
show that techniques issued from Genetic Algorithms can be used in order to
build an efficient default reasoning system. After providing a formal
description of the components required for an extension search based on Genetic
Algorithms principles, we exhibit some experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002016</id><created>2000-02-27</created><updated>2001-03-02</updated><authors><author><keyname>Shen</keyname><forenames>Yi-Dong</forenames></author><author><keyname>Yuan</keyname><forenames>Li-Yan</forenames></author><author><keyname>You</keyname><forenames>Jia-Huai</forenames></author></authors><title>SLT-Resolution for the Well-Founded Semantics</title><categories>cs.AI cs.PL</categories><comments>Slight modification</comments><acm-class>D.3.1; F.4.1; I.2.3</acm-class><journal-ref>Journal of Automated Reasoning 28(1):53-97, 2002</journal-ref><abstract>  Global SLS-resolution and SLG-resolution are two representative mechanisms
for top-down evaluation of the well-founded semantics of general logic
programs. Global SLS-resolution is linear for query evaluation but suffers from
infinite loops and redundant computations. In contrast, SLG-resolution resolves
infinite loops and redundant computations by means of tabling, but it is not
linear. The principal disadvantage of a non-linear approach is that it cannot
be implemented using a simple, efficient stack-based memory structure nor can
it be easily extended to handle some strictly sequential operators such as cuts
in Prolog.
  In this paper, we present a linear tabling method, called SLT-resolution, for
top-down evaluation of the well-founded semantics. SLT-resolution is a
substantial extension of SLDNF-resolution with tabling. Its main features
include: (1) It resolves infinite loops and redundant computations while
preserving the linearity. (2) It is terminating, and sound and complete w.r.t.
the well-founded semantics for programs with the bounded-term-size property
with non-floundering queries. Its time complexity is comparable with
SLG-resolution and polynomial for function-free logic programs. (3) Because of
its linearity for query evaluation, SLT-resolution bridges the gap between the
well-founded semantics and standard Prolog implementation techniques. It can be
implemented by an extension to any existing Prolog abstract machines such as
WAM or ATOAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002017</id><created>2000-02-26</created><authors><author><keyname>Kromer</keyname><forenames>V.</forenames></author></authors><title>An Usage Measure Based on Psychophysical Relations</title><categories>cs.CL</categories><comments>9 pages, 1 figure, 1 table</comments><acm-class>I.2.7</acm-class><abstract>  A new word usage measure is proposed. It is based on psychophysical relations
and allows to reveal words by its degree of &quot;importance&quot; for making basic
dictionaries of sublanguages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0002018</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0002018</id><created>2000-02-29</created><updated>2006-04-22</updated><authors><author><keyname>Musliu</keyname><forenames>Nysret</forenames></author><author><keyname>Gaertner</keyname><forenames>Johannes</forenames></author><author><keyname>Slany</keyname><forenames>Wolfgang</forenames></author></authors><title>Efficient generation of rotating workforce schedules</title><categories>cs.OH</categories><comments>24 pages, uses dbairep.sty</comments><report-no>dbai-tr-2000-35</report-no><acm-class>B.2.4</acm-class><abstract>  Generating high-quality schedules for a rotating workforce is a critical task
in all settings where a certain staffing level must be guaranteed beyond the
capacity of single employees, such as for instance in industrial plants,
hospitals, or airline companies. Results from ergonomics \cite{BEST91} indicate
that rotating workforce schedules have a profound impact on the health and
social life of employees as well as on their performance at work. Moreover,
rotating workforce schedules must satisfy legal requirements and should also
meet the objectives of the employing organization. We describe our solution to
this problem. A basic design decision was to aim at quickly obtaining
high-quality schedules for realistically sized problems while maintaining human
control. The interaction between the decision maker and the algorithm therefore
consists in four steps: (1) choosing a set of lengths of work blocks (a work
block is a sequence of consecutive days of work shifts), (2) choosing a
particular sequence of work and days-off blocks among those that have optimal
weekend characteristics, (3) enumerating possible shift sequences for the
chosen work blocks subject to shift change constraints and bounds on sequences
of shifts, and (4) assignment of shift sequences to work blocks while
fulfilling the staffing requirements. The combination of constraint
satisfaction and problem-oriented intelligent backtracking algorithms in each
of the four steps allows to find good solutions for real-world problems in
acceptable time. Computational results from real-world problems and from
benchmark examples found in the literature confirm the viability of our
approach. The algorithms are now part of a commercial shift scheduling software
package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003001</id><created>2000-03-01</created><authors><author><keyname>Mueller</keyname><forenames>Erik T.</forenames></author></authors><title>Making news understandable to computers</title><categories>cs.IR</categories><acm-class>I.7.2</acm-class><abstract>  Computers and devices are largely unaware of events taking place in the
world. This could be changed if news were made available in a
computer-understandable form. In this paper we present XML documents called
NewsForms that represent the key points of 17 types of news events. We discuss
the benefits of computer-understandable news and present the NewsExtract
program for converting text news stories into NewsForms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003002</id><created>2000-03-01</created><updated>2000-05-02</updated><authors><author><keyname>Guimond</keyname><forenames>Louis-Sebastien</forenames></author><author><keyname>Masakova</keyname><forenames>Zuzana</forenames></author><author><keyname>Patera</keyname><forenames>Jiri</forenames></author><author><keyname>Pelantova</keyname><forenames>Edita</forenames></author></authors><title>Combining Random Number Generators using Quasicrystals</title><categories>cs.DM</categories><acm-class>G.3</acm-class><abstract>  This paper has been withdrawn by the author(s),
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003003</id><created>2000-03-01</created><authors><author><keyname>Mueller</keyname><forenames>Erik T.</forenames></author></authors><title>Prospects for in-depth story understanding by computer</title><categories>cs.AI cs.CL</categories><acm-class>I.2.7</acm-class><abstract>  While much research on the hard problem of in-depth story understanding by
computer was performed starting in the 1970s, interest shifted in the 1990s to
information extraction and word sense disambiguation. Now that a degree of
success has been achieved on these easier problems, I propose it is time to
return to in-depth story understanding. In this paper I examine the shift away
from story understanding, discuss some of the major problems in building a
story understanding system, present some possible solutions involving a set of
interacting understanding agents, and provide pointers to useful tools and
resources for building story understanding systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003004</id><created>2000-03-01</created><authors><author><keyname>Mueller</keyname><forenames>Erik T.</forenames></author></authors><title>A database and lexicon of scripts for ThoughtTreasure</title><categories>cs.AI cs.CL</categories><acm-class>I.2.7; I.2.4</acm-class><abstract>  Since scripts were proposed in the 1970's as an inferencing mechanism for AI
and natural language processing programs, there have been few attempts to build
a database of scripts. This paper describes a database and lexicon of scripts
that has been added to the ThoughtTreasure commonsense platform. The database
provides the following information about scripts: sequence of events, roles,
props, entry conditions, results, goals, emotions, places, duration, frequency,
and cost. English and French words and phrases are linked to script concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003005</id><created>2000-03-02</created><authors><author><keyname>Roy</keyname><forenames>Prasan</forenames></author><author><keyname>Ramamritham</keyname><forenames>Krithi</forenames></author><author><keyname>Seshadri</keyname><forenames>S.</forenames></author><author><keyname>Shenoy</keyname><forenames>Pradeep</forenames></author><author><keyname>Sudarshan</keyname><forenames>S.</forenames></author></authors><title>Don't Trash your Intermediate Results, Cache 'em</title><categories>cs.DB</categories><comments>22 pages, 4 figures</comments><acm-class>H.2.4;H.2.7</acm-class><abstract>  In data warehouse and data mart systems, queries often take a long time to
execute due to their complex nature. Query response times can be greatly
improved by caching final/intermediate results of previous queries, and using
them to answer later queries. In this paper we describe a caching system called
Exchequer which incorporates several novel features including optimization
aware cache maintenance and the use of a cache aware optimizer. In contrast, in
existing work, the module that makes cost-benefit decisions is part of the
cache manager and works independent of the optimizer which essentially
reconsiders these decisions while finding the best plan for a query. In our
work, the optimizer takes the decisions for the cache manager. Furthermore,
existing approaches are either restricted to cube (slice/point) queries, or
cache just the query results. On the other hand, our work is extens ible and in
fact presents a data-model independent framework and algorithm. Our
experimental results attest to the efficacy of our cache management techniques
and show that over a wide range of parameters (a) Exchequer's query response
times are lower by more than 30% compared to the best performing competitor,
and (b) Exchequer can deliver the same response time as its competitor with
just one tenth of the cache size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003006</id><created>2000-03-02</created><authors><author><keyname>Mistry</keyname><forenames>Hoshi</forenames></author><author><keyname>Roy</keyname><forenames>Prasan</forenames></author><author><keyname>Ramamritham</keyname><forenames>Krithi</forenames></author><author><keyname>Sudarshan</keyname><forenames>S.</forenames></author></authors><title>Materialized View Selection and Maintenance Using Multi-Query
  Optimization</title><categories>cs.DB</categories><comments>22 pages, 7 figures</comments><acm-class>H.2.4;H.2.7</acm-class><abstract>  Because the presence of views enhances query performance, materialized views
are increasingly being supported by commercial database/data warehouse systems.
Whenever the data warehouse is updated, the materialized views must also be
updated. However, whereas the amount of data entering a warehouse, the query
loads, and the need to obtain up-to-date responses are all increasing, the time
window available for making the warehouse up-to-date is shrinking. These trends
necessitate efficient techniques for the maintenance of materialized views.
  In this paper, we show how to find an efficient plan for maintenance of a
{\em set} of views, by exploiting common subexpressions between different view
maintenance expressions. These common subexpressions may be materialized
temporarily during view maintenance. Our algorithms also choose
subexpressions/indices to be materialized permanently (and maintained along
with other materialized views), to speed up view maintenance. While there has
been much work on view maintenance in the past, our novel contributions lie in
exploiting a recently developed framework for multiquery optimization to
efficiently find good view maintenance plans as above. In addition to faster
view maintenance, our algorithms can also be used to efficiently select
materialized views to speed up workloads containing queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003007</id><created>2000-03-05</created><authors><author><keyname>Satoh</keyname><forenames>Ken</forenames></author><author><keyname>Okamoto</keyname><forenames>Hidenori</forenames></author></authors><title>Computing Circumscriptive Databases by Integer Programming: Revisited
  (Extended Abstract)</title><categories>cs.AI cs.LO</categories><acm-class>I.2.3</acm-class><abstract>  In this paper, we consider a method of computing minimal models in
circumscription using integer programming in propositional logic and
first-order logic with domain closure axioms and unique name axioms. This kind
of treatment is very important since this enable to apply various technique
developed in operations research to nonmonotonic reasoning.
  Nerode et al. (1995) are the first to propose a method of computing
circumscription using integer programming. They claimed their method was
correct for circumscription with fixed predicate, but we show that their method
does not correctly reflect their claim. We show a correct method of computing
all the minimal models not only with fixed predicates but also with varied
predicates and we extend our method to compute prioritized circumscription as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003008</id><created>2000-03-05</created><authors><author><keyname>Satoh</keyname><forenames>Ken</forenames></author></authors><title>Consistency Management of Normal Logic Program by Top-down Abductive
  Proof Procedure</title><categories>cs.AI</categories><acm-class>I.2.3</acm-class><abstract>  This paper presents a method of computing a revision of a function-free
normal logic program. If an added rule is inconsistent with a program, that is,
if it leads to a situation such that no stable model exists for a new program,
then deletion and addition of rules are performed to avoid inconsistency. We
specify a revision by translating a normal logic program into an abductive
logic program with abducibles to represent deletion and addition of rules. To
compute such deletion and addition, we propose an adaptation of our top-down
abductive proof procedure to compute a relevant abducibles to an added rule. We
compute a minimally revised program, by choosing a minimal set of abducibles
among all the sets of abducibles computed by a top-down proof procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003009</id><created>2000-03-06</created><authors><author><keyname>Kern-Isberner</keyname><forenames>Gabriele</forenames></author></authors><title>Conditional indifference and conditional preservation</title><categories>cs.AI cs.LO</categories><comments>Workshop Nonmonotonic Reasoning 2000, Belief Revision, at KR 2000, 10
  pages</comments><acm-class>I.2.0; I.2.3; I.2.6</acm-class><abstract>  The idea of preserving conditional beliefs emerged recently as a new paradigm
apt to guide the revision of epistemic states. Conditionals are substantially
different from propositional beliefs and need specific treatment. In this
paper, we present a new approach to conditionals, capturing particularly well
their dynamic part as revision policies. We thoroughly axiomatize a principle
of conditional preservation as an indifference property with respect to
conditional structures of worlds. This principle is developed in a
semi-quantitative setting, so as to reveal its fundamental meaning for belief
revision in quantitative as well as in qualitative frameworks. In fact, it is
shown to cover other proposed approaches to conditional preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003010</identifier>
 <datestamp>2009-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003010</id><created>2000-03-06</created><authors><author><keyname>Steinmacher-Burow</keyname><forenames>Burkhard D.</forenames></author></authors><title>TSIA: A Dataflow Model</title><categories>cs.PL</categories><acm-class>D.3.2;D.3.3</acm-class><abstract>  The Task System and Item Architecture (TSIA) is a model for transparent
application execution. In many real-world projects, a TSIA provides a simple
application with a transparent reliable, distributed, heterogeneous, adaptive,
dynamic, real-time, parallel, secure or other execution. TSIA is suitable for
many applications, not just for the simple applications served to date. This
presentation shows that TSIA is a dataflow model - a long-standing model for
transparent parallel execution. The advances to the dataflow model include a
simple semantics, as well as support for input/output, for modifiable items and
for other such effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003011</id><created>2000-03-06</created><updated>2000-06-16</updated><authors><author><keyname>Shapiro</keyname><forenames>Stuart C.</forenames></author><author><keyname>Johnson</keyname><forenames>Frances L.</forenames></author></authors><title>Automatic Belief Revision in SNePS</title><categories>cs.AI cs.LO</categories><comments>Slightly revised page 3, right column, 3rd complete paragraph to fix
  formatting problem</comments><report-no>2000-01</report-no><acm-class>I.2.3; I.2.4</acm-class><abstract>  SNePS is a logic- and network- based knowledge representation, reasoning, and
acting system, based on a monotonic, paraconsistent, first-order term logic,
with compositional intensional semantics. It has an ATMS-style facility for
belief contraction, and an acting component, including a well-defined syntax
and semantics for primitive and composite acts, as well as for ``rules'' that
allow for acting in support of reasoning and reasoning in support of acting.
SNePS has been designed to support natural language competent cognitive agents.
  When the current version of SNePS detects an explicit contradiction, it
interacts with the user, providing information that helps the user decide what
to remove from the knowledge base in order to remove the contradiction. The
forthcoming SNePS 2.6 will also do automatic belief contraction if the
information in the knowledge base warrents it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003012</id><created>2000-03-06</created><authors><author><keyname>Pollock</keyname><forenames>John L.</forenames></author></authors><title>Defeasible Reasoning in OSCAR</title><categories>cs.AI</categories><comments>Nonmonotonic Reasoning Workshop, 2000</comments><acm-class>F.4.1</acm-class><abstract>  This is a system description for the OSCAR defeasible reasoner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003013</id><created>2000-03-06</created><authors><author><keyname>Antoniou</keyname><forenames>G.</forenames></author><author><keyname>Billigton</keyname><forenames>D.</forenames></author><author><keyname>Governatori</keyname><forenames>G.</forenames></author><author><keyname>Maher</keyname><forenames>M. J.</forenames></author></authors><title>A flexible framework for defeasible logics</title><categories>cs.AI cs.LO</categories><comments>Proceedings of 8th International Workshop on Non-Monotonic Reasoning,
  April 9-11, 2000, Breckenridge, Colorado</comments><acm-class>I.2.3; D.1.6</acm-class><abstract>  Logics for knowledge representation suffer from over-specialization: while
each logic may provide an ideal representation formalism for some problems, it
is less than optimal for others. A solution to this problem is to choose from
several logics and, when necessary, combine the representations. In general,
such an approach results in a very difficult problem of combination. However,
if we can choose the logics from a uniform framework then the problem of
combining them is greatly simplified. In this paper, we develop such a
framework for defeasible logics. It supports all defeasible logics that satisfy
a strong negation principle. We use logic meta-programs as the basis for the
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003014</id><created>2000-03-06</created><updated>2000-03-08</updated><authors><author><keyname>Lau</keyname><forenames>Raymond</forenames><affiliation>Queensland University of Technology</affiliation></author><author><keyname>ter Hofstede</keyname><forenames>Arthur H. M.</forenames><affiliation>Queensland University of Technology</affiliation></author><author><keyname>Bruza</keyname><forenames>Peter D.</forenames><affiliation>Distributed Systems Technology Centre</affiliation></author></authors><title>Applying Maxi-adjustment to Adaptive Information Filtering Agents</title><categories>cs.AI cs.MA</categories><comments>The 8th Intl. Workshop on Non-Monotonic Reasoning NMR'2000, Belief
  Change, 9 pages</comments><acm-class>I.2.3;I.2.11;H.3.3</acm-class><abstract>  Learning and adaptation is a fundamental property of intelligent agents. In
the context of adaptive information filtering, a filtering agent's beliefs
about a user's information needs have to be revised regularly with reference to
the user's most current information preferences. This learning and adaptation
process is essential for maintaining the agent's filtering performance. The AGM
belief revision paradigm provides a rigorous foundation for modelling rational
and minimal changes to an agent's beliefs. In particular, the maxi-adjustment
method, which follows the AGM rationale of belief change, offers a sound and
robust computational mechanism to develop adaptive agents so that learning
autonomy of these agents can be enhanced. This paper describes how the
maxi-adjustment method is applied to develop the learning components of
adaptive information filtering agents, and discusses possible difficulties of
applying such a framework to these agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003015</id><created>2000-03-07</created><updated>2000-03-08</updated><authors><author><keyname>Meyer</keyname><forenames>Thomas</forenames></author></authors><title>On the semantics of merging</title><categories>cs.AI cs.LO</categories><comments>7 pages, 9 figures, paper to be presented at NMR'2000</comments><acm-class>I.2.3; I.2.4; I.2.11</acm-class><abstract>  Intelligent agents are often faced with the problem of trying to merge
possibly conflicting pieces of information obtained from different sources into
a consistent view of the world. We propose a framework for the modelling of
such merging operations with roots in the work of Spohn (1988, 1991). Unlike
most approaches we focus on the merging of epistemic states, not knowledge
bases. We construct a number of plausible merging operations and measure them
against various properties that merging operations ought to satisfy. Finally,
we discuss the connection between merging and the use of infobases Meyer (1999)
and Meyer et al. (2000).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003016</id><created>2000-03-07</created><authors><author><keyname>Dupre'</keyname><forenames>Daniele Theseider</forenames><affiliation>Dipartimento di Scienze e Tecnologie Avanzate - Universita' del Piemonte Orientale, Alessandria, Italy</affiliation></author></authors><title>Abductive and Consistency-Based Diagnosis Revisited: a Modeling
  Perspective</title><categories>cs.AI</categories><comments>5 pages, 8th Int. Workshop on Nonmonotonic Reasoning, 2000</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  Diagnostic reasoning has been characterized logically as consistency-based
reasoning or abductive reasoning. Previous analyses in the literature have
shown, on the one hand, that choosing the (in general more restrictive)
abductive definition may be appropriate or not, depending on the content of the
knowledge base [Console&amp;Torasso91], and, on the other hand, that, depending on
the choice of the definition the same knowledge should be expressed in
different form [Poole94].
  Since in Model-Based Diagnosis a major problem is finding the right way of
abstracting the behavior of the system to be modeled, this paper discusses the
relation between modeling, and in particular abstraction in the model, and the
notion of diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003017</id><created>2000-03-07</created><updated>2000-03-08</updated><authors><author><keyname>Booth</keyname><forenames>Richard</forenames></author></authors><title>The lexicographic closure as a revision process</title><categories>cs.AI cs.LO</categories><comments>7 pages, Nonmonotonic Reasoning Workshop 2000 (special session on
  belief change), at KR2000</comments><acm-class>I.2.3</acm-class><abstract>  The connections between nonmonotonic reasoning and belief revision are
well-known. A central problem in the area of nonmonotonic reasoning is the
problem of default entailment, i.e., when should an item of default information
representing &quot;if A is true then, normally, B is true&quot; be said to follow from a
given set of items of such information. Many answers to this question have been
proposed but, surprisingly, virtually none have attempted any explicit
connection to belief revision. The aim of this paper is to give an example of
how such a connection can be made by showing how the lexicographic closure of a
set of defaults may be conceptualised as a process of iterated revision by sets
of sentences. Specifically we use the revision process of Nayak.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003018</id><created>2000-03-07</created><authors><author><keyname>Stephan</keyname><forenames>I.</forenames><affiliation>University of Angers, France</affiliation></author><author><keyname>Saubion</keyname><forenames>F.</forenames><affiliation>University of Angers, France</affiliation></author><author><keyname>Nicolas</keyname><forenames>P.</forenames><affiliation>University of Angers, France</affiliation></author></authors><title>Description of GADEL</title><categories>cs.AI cs.LO</categories><comments>System Descriptions and Demonstrations at Nonmonotonic Reasoning
  Workshop, 2000 6 pages, 2 figures, 5 tables</comments><acm-class>F.4.1</acm-class><abstract>  This article describes the first implementation of the GADEL system : a
Genetic Algorithm for Default Logic. The goal of GADEL is to compute extensions
in Reiter's default logic. It accepts every kind of finite propositional
default theories and is based on evolutionary principles of Genetic Algorithms.
Its first experimental results on certain instances of the problem show that
this new approach of the problem can be successful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003019</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003019</id><created>2000-03-07</created><authors><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Extending Classical Logic with Inductive Definitions</title><categories>cs.LO cs.AI</categories><comments>9 pages to be presented at NMR2000, Breckenridge, April 2000</comments><acm-class>I.2.3;I.2.4;F.4.1</acm-class><abstract>  The goal of this paper is to extend classical logic with a generalized notion
of inductive definition supporting positive and negative induction, to
investigate the properties of this logic, its relationships to other logics in
the area of non-monotonic reasoning, logic programming and deductive databases,
and to show its application for knowledge representation by giving a typology
of definitional knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003020</id><created>2000-03-07</created><updated>2000-03-10</updated><authors><author><keyname>Kakas</keyname><forenames>Antonis</forenames></author></authors><title>ACLP: Integrating Abduction and Constraint Solving</title><categories>cs.AI</categories><comments>6 pages</comments><acm-class>I.2.4;F.4.1</acm-class><abstract>  ACLP is a system which combines abductive reasoning and constraint solving by
integrating the frameworks of Abductive Logic Programming (ALP) and Constraint
Logic Programming (CLP). It forms a general high-level knowledge representation
environment for abductive problems in Artificial Intelligence and other areas.
In ACLP, the task of abduction is supported and enhanced by its non-trivial
integration with constraint solving facilitating its application to complex
problems. The ACLP system is currently implemented on top of the CLP language
of ECLiPSe as a meta-interpreter exploiting its underlying constraint solver
for finite domains. It has been applied to the problems of planning and
scheduling in order to test its computational effectiveness compared with the
direct use of the (lower level) constraint solving framework of CLP on which it
is built. These experiments provide evidence that the abductive framework of
ACLP does not compromise significantly the computational efficiency of the
solutions. Other experiments show the natural ability of ACLP to accommodate
easily and in a robust way new or changing requirements of the original
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003021</id><created>2000-03-07</created><authors><author><keyname>Chopra</keyname><forenames>Samir</forenames></author><author><keyname>Georgatos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Parikh</keyname><forenames>Rohit</forenames></author></authors><title>Relevance Sensitive Non-Monotonic Inference on Belief Sequences</title><categories>cs.AI</categories><acm-class>I.2.3</acm-class><abstract>  We present a method for relevance sensitive non-monotonic inference from
belief sequences which incorporates insights pertaining to prioritized
inference and relevance sensitive, inconsistency tolerant belief revision.
 Our model uses a finite, logically open sequence of propositional formulas as
a representation for beliefs and defines a notion of inference from
maxiconsistent subsets of formulas guided by two orderings: a temporal
sequencing and an ordering based on relevance relations between the conclusion
and formulas in the sequence. The relevance relations are ternary (using
context as a parameter) as opposed to standard binary axiomatizations. The
inference operation thus defined easily handles iterated revision by
maintaining a revision history, blocks the derivation of inconsistent answers
from a possibly inconsistent sequence and maintains the distinction between
explicit and implicit beliefs. In doing so, it provides a finitely presented
formalism and a plausible model of reasoning for automated agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003022</id><created>2000-03-08</created><authors><author><keyname>Arlo-Costa</keyname><forenames>Horacio</forenames></author></authors><title>Hypothetical revision and matter-of-fact supposition</title><categories>cs.AI cs.CL</categories><comments>9 pages. Presented at the Special Session on Belief change: theory
  and practice of the 8th Intl. Workshop on Non-Monotonic Reasoning NMR'2000</comments><acm-class>I.2.4; I.2.6; I.2.7; I.2.11</acm-class><abstract>  The paper studies the notion of supposition encoded in non-Archimedean
conditional probability (and revealed in the acceptance of the so-called
indicative conditionals). The notion of qualitative change of view that thus
arises is axiomatized and compared with standard notions like AGM and UPDATE.
Applications in the following fields are discussed: (1) theory of games and
decisions, (2) causal models, (3) non-monotonic logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003023</id><created>2000-03-08</created><authors><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author></authors><title>Probabilistic Default Reasoning with Conditional Constraints</title><categories>cs.AI</categories><comments>8 pages; to appear in Proceedings of the Eighth International
  Workshop on Nonmonotonic Reasoning, Special Session on Uncertainty Frameworks
  in Nonmonotonic Reasoning, Breckenridge, Colorado, USA, 9-11 April 2000</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  We propose a combination of probabilistic reasoning from conditional
constraints with approaches to default reasoning from conditional knowledge
bases. In detail, we generalize the notions of Pearl's entailment in system Z,
Lehmann's lexicographic entailment, and Geffner's conditional entailment to
conditional constraints. We give some examples that show that the new notions
of z-, lexicographic, and conditional entailment have similar properties like
their classical counterparts. Moreover, we show that the new notions of z-,
lexicographic, and conditional entailment are proper generalizations of both
their classical counterparts and the classical notion of logical entailment for
conditional constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003024</id><created>2000-03-08</created><authors><author><keyname>Delgrande</keyname><forenames>James P.</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author><author><keyname>Tompits</keyname><forenames>Hans</forenames></author></authors><title>A Compiler for Ordered Logic Programs</title><categories>cs.AI</categories><acm-class>I.2.3</acm-class><abstract>  This paper describes a system, called PLP, for compiling ordered logic
programs into standard logic programs under the answer set semantics. In an
ordered logic program, rules are named by unique terms, and preferences among
rules are given by a set of dedicated atoms. An ordered logic program is
transformed into a second, regular, extended logic program wherein the
preferences are respected, in that the answer sets obtained in the transformed
theory correspond with the preferred answer sets of the original theory. Since
the result of the translation is an extended logic program, existing logic
programming systems can be used as underlying reasoning engine. In particular,
PLP is conceived as a front-end to the logic programming systems dlv and
smodels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003025</id><created>2000-03-08</created><authors><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames><affiliation>Katholieke Universiteit Leuven</affiliation></author></authors><title>Logic Programming for Describing and Solving Planning Problems</title><categories>cs.AI cs.LO</categories><comments>8 pages, no figures, Eighth International Workshop on Nonmonotonic
  Reasoning, special track on Representing Actions and Planning</comments><acm-class>D.1.6; D.3.1; F.4.1; I.2.3</acm-class><abstract>  A logic programming paradigm which expresses solutions to problems as stable
models has recently been promoted as a declarative approach to solving various
combinatorial and search problems, including planning problems. In this
paradigm, all program rules are considered as constraints and solutions are
stable models of the rule set. This is a rather radical departure from the
standard paradigm of logic programming. In this paper we revisit abductive
logic programming and argue that it allows a programming style which is as
declarative as programming based on stable models. However, within abductive
logic programming, one has two kinds of rules. On the one hand predicate
definitions (which may depend on the abducibles) which are nothing else than
standard logic programs (with their non-monotonic semantics when containing
with negation); on the other hand rules which constrain the models for the
abducibles. In this sense abductive logic programming is a smooth extension of
the standard paradigm of logic programming, not a radical departure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003026</id><created>2000-03-08</created><authors><author><keyname>Pelov</keyname><forenames>Nikolay</forenames></author><author><keyname>De Mot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames></author></authors><title>A Comparison of Logic Programming Approaches for Representation and
  Solving of Constraint Satisfaction Problems</title><categories>cs.LO</categories><comments>9 pages, 3 figures submitted to NMR 2000, April 9-11, Breckenridge,
  Colorado</comments><acm-class>I.2.3: Logic programming, Nonmonotonic reasoning; I.2.4; F.4.1:
  Logic and constraint programming; Experiments</acm-class><abstract>  Many logic programming based approaches can be used to describe and solve
combinatorial search problems. On the one hand there are definite programs and
constraint logic programs that compute a solution as an answer substitution to
a query containing the variables of the constraint satisfaction problem. On the
other hand there are approaches based on stable model semantics, abduction, and
first-order logic model generation that compute solutions as models of some
theory. This paper compares these different approaches from point of view of
knowledge representation (how declarative are the programs) and from point of
view of performance (how good are they at solving typical problems).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003027</id><created>2000-03-08</created><authors><author><keyname>Van Nuffelen</keyname><forenames>Bert</forenames></author></authors><title>SLDNFA-system</title><categories>cs.AI</categories><comments>6 pages conference:NMR2000, special track on System descriptions and
  demonstration</comments><acm-class>F.4.1;I.2.3;I.2.4</acm-class><abstract>  The SLDNFA-system results from the LP+ project at the K.U.Leuven, which
investigates logics and proof procedures for these logics for declarative
knowledge representation. Within this project inductive definition logic
(ID-logic) is used as representation logic. Different solvers are being
developed for this logic and one of these is SLDNFA. A prototype of the system
is available and used for investigating how to solve efficiently problems
represented in ID-logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003028</id><created>2000-03-08</created><authors><author><keyname>Delgrande</keyname><forenames>James P.</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author><author><keyname>Tompits</keyname><forenames>Hans</forenames></author></authors><title>Logic Programs with Compiled Preferences</title><categories>cs.AI</categories><acm-class>I.2.3</acm-class><abstract>  We describe an approach for compiling preferences into logic programs under
the answer set semantics. An ordered logic program is an extended logic program
in which rules are named by unique terms, and in which preferences among rules
are given by a set of dedicated atoms. An ordered logic program is transformed
into a second, regular, extended logic program wherein the preferences are
respected, in that the answer sets obtained in the transformed theory
correspond with the preferred answer sets of the original theory. Our approach
allows both the specification of static orderings (as found in most previous
work), in which preferences are external to a logic program, as well as
orderings on sets of rules. In large part then, we are interested in describing
a general methodology for uniformly incorporating preference information in a
logic program. Since the result of our translation is an extended logic
program, we can make use of existing implementations, such as dlv and smodels.
To this end, we have developed a compiler, available on the web, as a front-end
for these programming systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003029</id><created>2000-03-08</created><authors><author><keyname>Mellouli</keyname><forenames>Nedra</forenames></author><author><keyname>Bouchon-Meunier</keyname><forenames>Bernadette</forenames></author></authors><title>Fuzzy Approaches to Abductive Inference</title><categories>cs.AI</categories><comments>7 pages and 8 files</comments><acm-class>Artificial intelligence and nonmonotonic reasoning and belief
  revision</acm-class><abstract>  This paper proposes two kinds of fuzzy abductive inference in the framework
of fuzzy rule base. The abductive inference processes described here depend on
the semantic of the rule. We distinguish two classes of interpretation of a
fuzzy rule, certainty generation rules and possible generation rules. In this
paper we present the architecture of abductive inference in the first class of
interpretation. We give two kinds of problem that we can resolve by using the
proposed models of inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003030</id><created>2000-03-08</created><authors><author><keyname>Van Nuffelen</keyname><forenames>Bert</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Problem solving in ID-logic with aggregates: some experiments</title><categories>cs.AI</categories><comments>9 pages conference: NMR2000, special track on abductive reasoning</comments><acm-class>F.4.1;I.2.4;I.2.3</acm-class><abstract>  The goal of the LP+ project at the K.U.Leuven is to design an expressive
logic, suitable for declarative knowledge representation, and to develop
intelligent systems based on Logic Programming technology for solving
computational problems using the declarative specifications. The ID-logic is an
integration of typed classical logic and a definition logic. Different
abductive solvers for this language are being developed. This paper is a report
of the integration of high order aggregates into ID-logic and the consequences
on the solver SLDNFA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003031</id><created>2000-03-08</created><authors><author><keyname>Vodislav</keyname><forenames>Carmen</forenames></author><author><keyname>Mercer</keyname><forenames>Robert E.</forenames></author></authors><title>Optimal Belief Revision</title><categories>cs.AI</categories><comments>NMR'2000 Workshop 6 pages</comments><acm-class>I.2.3</acm-class><abstract>  We propose a new approach to belief revision that provides a way to change
knowledge bases with a minimum of effort. We call this way of revising belief
states optimal belief revision. Our revision method gives special attention to
the fact that most belief revision processes are directed to a specific
informational objective. This approach to belief change is founded on notions
such as optimal context and accessibility. For the sentential model of belief
states we provide both a formal description of contexts as sub-theories
determined by three parameters and a method to construct contexts. Next, we
introduce an accessibility ordering for belief sets, which we then use for
selecting the best (optimal) contexts with respect to the processing effort
involved in the revision. Then, for finitely axiomatizable knowledge bases, we
characterize a finite accessibility ranking from which the accessibility
ordering for the entire base is generated and show how to determine the ranking
of an arbitrary sentence in the language. Finally, we define the adjustment of
the accessibility ranking of a revised base of a belief set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003032</id><created>2000-03-08</created><authors><author><keyname>Grosskreutz</keyname><forenames>Henrik</forenames></author><author><keyname>Lakemeyer</keyname><forenames>Gerhard</forenames></author></authors><title>cc-Golog: Towards More Realistic Logic-Based Robot Controllers</title><categories>cs.AI</categories><acm-class>I.2.3;I.2.8</acm-class><abstract>  High-level robot controllers in realistic domains typically deal with
processes which operate concurrently, change the world continuously, and where
the execution of actions is event-driven as in ``charge the batteries as soon
as the voltage level is low''. While non-logic-based robot control languages
are well suited to express such scenarios, they fare poorly when it comes to
projecting, in a conspicuous way, how the world evolves when actions are
executed. On the other hand, a logic-based control language like \congolog,
based on the situation calculus, is well-suited for the latter. However, it has
problems expressing event-driven behavior. In this paper, we show how these
problems can be overcome by first extending the situation calculus to support
continuous change and event-driven behavior and then presenting \ccgolog, a
variant of \congolog which is based on the extended situation calculus. One
benefit of \ccgolog is that it narrows the gap in expressiveness compared to
non-logic-based control languages while preserving a semantically well-founded
projection mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003033</id><created>2000-03-08</created><authors><author><keyname>Niemela</keyname><forenames>Ilkka</forenames></author><author><keyname>Simons</keyname><forenames>Patrik</forenames></author><author><keyname>Syrjanen</keyname><forenames>Tommi</forenames></author></authors><title>Smodels: A System for Answer Set Programming</title><categories>cs.AI</categories><comments>Proceedings of the 8th International Workshop on Non-Monotonic
  Reasoning, April 9-11, 2000, Breckenridge, Colorado 4 pages, uses aaai.sty</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  The Smodels system implements the stable model semantics for normal logic
programs. It handles a subclass of programs which contain no function symbols
and are domain-restricted but supports extensions including built-in functions
as well as cardinality and weight constraints. On top of this core engine more
involved systems can be built. As an example, we have implemented total and
partial stable model computation for disjunctive logic programs. An interesting
application method is based on answer set programming, i.e., encoding an
application problem as a set of rules so that its solutions are captured by the
stable models of the rules. Smodels has been applied to a number of areas
including planning, model checking, reachability analysis, product
configuration, dynamic constraint satisfaction, and feature interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003034</id><created>2000-03-08</created><updated>2000-03-09</updated><authors><author><keyname>Kakas</keyname><forenames>Antonis</forenames></author><author><keyname>Miller</keyname><forenames>Rob</forenames></author><author><keyname>Toni</keyname><forenames>Francesca</forenames></author></authors><title>E-RES: A System for Reasoning about Actions, Events and Observations</title><categories>cs.AI</categories><comments>Proceedings of the 8th International Workshop on Non-Monotonic
  Reasoning, April 9-11, 2000, Breckenridge, Colorado. 6 pages</comments><acm-class>I.2.4; F.4.1</acm-class><abstract>  E-RES is a system that implements the Language E, a logic for reasoning about
narratives of action occurrences and observations. E's semantics is
model-theoretic, but this implementation is based on a sound and complete
reformulation of E in terms of argumentation, and uses general computational
techniques of argumentation frameworks. The system derives sceptical
non-monotonic consequences of a given reformulated theory which exactly
correspond to consequences entailed by E's model-theory. The computation relies
on a complimentary ability of the system to derive credulous non-monotonic
consequences together with a set of supporting assumptions which is sufficient
for the (credulous) conclusion to hold. E-RES allows theories to contain
general action laws, statements about action occurrences, observations and
statements of ramifications (or universal laws). It is able to derive
consequences both forward and backward in time. This paper gives a short
overview of the theoretical basis of E-RES and illustrates its use on a variety
of examples. Currently, E-RES is being extended so that the system can be used
for planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003035</id><created>2000-03-08</created><authors><author><keyname>Brewka</keyname><forenames>Gerhard</forenames></author></authors><title>Declarative Representation of Revision Strategies</title><categories>cs.AI cs.LO</categories><acm-class>I.2.3</acm-class><abstract>  In this paper we introduce a nonmonotonic framework for belief revision in
which reasoning about the reliability of different pieces of information based
on meta-knowledge about the information is possible, and where revision
strategies can be described declaratively. The approach is based on a
Poole-style system for default reasoning in which entrenchment information is
represented in the logical language. A notion of inference based on the least
fixed point of a monotone operator is used to make sure that all theories
possess a consistent set of conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003036</id><created>2000-03-08</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author><author><keyname>Pfeifer</keyname><forenames>Gerald</forenames></author></authors><title>DLV - A System for Declarative Problem Solving</title><categories>cs.AI cs.LO</categories><comments>6 pages, 1 figure, 1 table</comments><acm-class>D.1.6; D.3.2; I.2.4; F.4.1</acm-class><abstract>  DLV is an efficient logic programming and non-monotonic reasoning (LPNMR)
system with advanced knowledge representation mechanisms and interfaces to
classic relational database systems.
  Its core language is disjunctive datalog (function-free disjunctive logic
programming) under the Answer Set Semantics with integrity constraints, both
default and strong (or explicit) negation, and queries. Integer arithmetics and
various built-in predicates are also supported.
  In addition DLV has several frontends, namely brave and cautious reasoning,
abductive diagnosis, consistency-based diagnosis, a subset of SQL3, planning
with action languages, and logic programming with inheritance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003037</id><created>2000-03-08</created><authors><author><keyname>Egly</keyname><forenames>Uwe</forenames></author><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Tompits</keyname><forenames>Hans</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>QUIP - A Tool for Computing Nonmonotonic Reasoning Tasks</title><categories>cs.AI</categories><acm-class>I.2.3</acm-class><abstract>  In this paper, we outline the prototype of an automated inference tool,
called QUIP, which provides a uniform implementation for several nonmonotonic
reasoning formalisms. The theoretical basis of QUIP is derived from well-known
results about the computational complexity of nonmonotonic logics and exploits
a representation of the different reasoning tasks in terms of quantified
boolean formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003038</id><created>2000-03-08</created><authors><author><keyname>Watson</keyname><forenames>Richard</forenames></author></authors><title>A Splitting Set Theorem for Epistemic Specifications</title><categories>cs.AI</categories><comments>To be published in Proceedings of NMR 2000 Workshop. 6 pages</comments><acm-class>F.4.1; I.2.3</acm-class><abstract>  Over the past decade a considerable amount of research has been done to
expand logic programming languages to handle incomplete information. One such
language is the language of epistemic specifications. As is usual with logic
programming languages, the problem of answering queries is intractable in the
general case. For extended disjunctive logic programs, an idea that has proven
useful in simplifying the investigation of answer sets is the use of splitting
sets. In this paper we will present an extended definition of splitting sets
that will be applicable to epistemic specifications. Furthermore, an extension
of the splitting set theorem will be presented. Also, a characterization of
stratified epistemic specifications will be given in terms of splitting sets.
This characterization leads us to an algorithmic method of computing world
views of a subclass of epistemic logic programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003039</id><created>2000-03-08</created><authors><author><keyname>Hietalahti</keyname><forenames>Maarit</forenames></author><author><keyname>Massacci</keyname><forenames>Fabio</forenames></author><author><keyname>Niemela</keyname><forenames>Ilkka</forenames></author></authors><title>DES: a Challenge Problem for Nonmonotonic Reasoning Systems</title><categories>cs.AI</categories><comments>10 pages, 1 Postscript figure, uses aaai.sty and graphicx.sty</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  The US Data Encryption Standard, DES for short, is put forward as an
interesting benchmark problem for nonmonotonic reasoning systems because (i) it
provides a set of test cases of industrial relevance which shares features of
randomly generated problems and real-world problems, (ii) the representation of
DES using normal logic programs with the stable model semantics is simple and
easy to understand, and (iii) this subclass of logic programs can be seen as an
interesting special case for many other formalizations of nonmonotonic
reasoning. In this paper we present two encodings of DES as logic programs: a
direct one out of the standard specifications and an optimized one extending
the work of Massacci and Marraro. The computational properties of the encodings
are studied by using them for DES key search with the Smodels system as the
implementation of the stable model semantics. Results indicate that the
encodings and Smodels are quite competitive: they outperform state-of-the-art
SAT-checkers working with an optimized encoding of DES into SAT and are
comparable with a SAT-checker that is customized and tuned for the optimized
SAT encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003040</id><created>2000-03-08</created><authors><author><keyname>Johnson</keyname><forenames>Frances L.</forenames></author><author><keyname>Shapiro</keyname><forenames>Stuart C.</forenames></author></authors><title>Implementing Integrity Constraints in an Existing Belief Revision System</title><categories>cs.AI cs.LO</categories><comments>8 pages, for the Belief Change Workshop at NMR2000 colocated with
  KR2000</comments><report-no>2000-03</report-no><acm-class>I.2.3;I.2.4</acm-class><abstract>  SNePS is a mature knowledge representation, reasoning, and acting system that
has long contained a belief revision subsystem, called SNeBR. SNeBR is
triggered when an explicit contradiction is introduced into the SNePS belief
space, either because of a user's new assertion, or because of a user's query.
SNeBR then makes the user decide what belief to remove from the belief space in
order to restore consistency, although it provides information to help the user
in making that decision. We have recently added automatic belief revision to
SNeBR, by which, under certain circumstances, SNeBR decides by itself which
belief to remove, and then informs the user of the decision and its
consequences. We have used the well-known belief revision integrity constraints
as a guide in designing automatic belief revision, taking into account,
however, that SNePS's belief space is not deductively closed, and that it would
be infeasible to form the deductive closure in order to decide what belief to
remove. This paper briefly describes SNeBR both before and after this revision,
discusses how we adapted the integrity constraints for this purpose, and gives
an example of the new SNeBR in action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003041</id><created>2000-03-08</created><authors><author><keyname>Bovens</keyname><forenames>Luc</forenames></author><author><keyname>Hartmann</keyname><forenames>Stephan</forenames></author></authors><title>Coherence, Belief Expansion and Bayesian Networks</title><categories>cs.AI cs.LO</categories><comments>6 pages, 2 figures, paper presented at the 8th Intl. Workshop on
  Non-Monotonic Reasoning NMR'2000 (April 9-11), Breckenridge, Colorado</comments><acm-class>F.4.0; G.3</acm-class><abstract>  We construct a probabilistic coherence measure for information sets which
determines a partial coherence ordering. This measure is applied in
constructing a criterion for expanding our beliefs in the face of new
information. A number of idealizations are being made which can be relaxed by
an appeal to Bayesian Networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003042</id><created>2000-03-08</created><authors><author><keyname>Babovich</keyname><forenames>Yuliya</forenames></author><author><keyname>Erdem</keyname><forenames>Esra</forenames></author><author><keyname>Lifschitz</keyname><forenames>Vladimir</forenames></author></authors><title>Fages' Theorem and Answer Set Programming</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><abstract>  We generalize a theorem by Francois Fages that describes the relationship
between the completion semantics and the answer set semantics for logic
programs with negation as failure. The study of this relationship is important
in connection with the emergence of answer set programming. Whenever the two
semantics are equivalent, answer sets can be computed by a satisfiability
solver, and the use of answer set solvers such as smodels and dlv is
unnecessary. A logic programming representation of the blocks world due to
Ilkka Niemelae is discussed as an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003043</id><created>2000-03-08</created><authors><author><keyname>Ipeirotis</keyname><forenames>Panagiotis</forenames></author><author><keyname>Gravano</keyname><forenames>Luis</forenames></author><author><keyname>Sahami</keyname><forenames>Mehran</forenames></author></authors><title>Automatic Classification of Text Databases through Query Probing</title><categories>cs.DB cs.IR</categories><comments>7 pages, 1 figure</comments><report-no>CUCS-004-00</report-no><acm-class>H.3</acm-class><abstract>  Many text databases on the web are &quot;hidden&quot; behind search interfaces, and
their documents are only accessible through querying. Search engines typically
ignore the contents of such search-only databases. Recently, Yahoo-like
directories have started to manually organize these databases into categories
that users can browse to find these valuable resources. We propose a novel
strategy to automate the classification of search-only text databases. Our
technique starts by training a rule-based document classifier, and then uses
the classifier's rules to generate probing queries. The queries are sent to the
text databases, which are then classified based on the number of matches that
they produce for each query. We report some initial exploratory experiments
that show that our approach is promising to automatically characterize the
contents of text databases accessible on the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003044</id><created>2000-03-09</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>On the tractable counting of theory models and its application to belief
  revision and truth maintenance</title><categories>cs.AI</categories><acm-class>I.2.3</acm-class><abstract>  We introduced decomposable negation normal form (DNNF) recently as a
tractable form of propositional theories, and provided a number of powerful
logical operations that can be performed on it in polynomial time. We also
presented an algorithm for compiling any conjunctive normal form (CNF) into
DNNF and provided a structure-based guarantee on its space and time complexity.
We present in this paper a linear-time algorithm for converting an ordered
binary decision diagram (OBDD) representation of a propositional theory into an
equivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a
subclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the
previous complexity guarantees on compiling DNNF continue to hold for this
stricter subclass, which has stronger properties. In particular, we present a
new operation on d-DNNF which allows us to count its models under the
assertion, retraction and flipping of every literal by traversing the d-DNNF
twice. That is, after such traversal, we can test in constant-time: the
entailment of any literal by the d-DNNF, and the consistency of the d-DNNF
under the retraction or flipping of any literal. We demonstrate the
significance of these new operations by showing how they allow us to implement
linear-time, complete truth maintenance systems and linear-time, complete
belief revision systems for two important classes of propositional theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003045</id><created>2000-03-09</created><authors><author><keyname>Verbaeten</keyname><forenames>Sofie</forenames></author><author><keyname>De Schreye</keyname><forenames>Danny</forenames></author><author><keyname>Sagonas</keyname><forenames>Konstantinos</forenames></author></authors><title>Termination Proofs for Logic Programs with Tabling</title><categories>cs.LO</categories><comments>48 pages, 6 figures, submitted to ACM Transactions on Computational
  Logic (TOCL)</comments><acm-class>I.2.2; I.2.3; D.1.6</acm-class><abstract>  Tabled logic programming is receiving increasing attention in the Logic
Programming community. It avoids many of the shortcomings of SLD execution and
provides a more flexible and often extremely efficient execution mechanism for
logic programs. In particular, tabled execution of logic programs terminates
more often than execution based on SLD-resolution. In this article, we
introduce two notions of universal termination of logic programming with
Tabling: quasi-termination and (the stronger notion of) LG-termination. We
present sufficient conditions for these two notions of termination, namely
quasi-acceptability and LG-acceptability, and we show that these conditions are
also necessary in case the tabling is well-chosen. Starting from these
conditions, we give modular termination proofs, i.e., proofs capable of
combining termination proofs of separate programs to obtain termination proofs
of combined programs. Finally, in the presence of mode information, we state
sufficient conditions which form the basis for automatically proving
termination in a constraint-based way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003046</id><created>2000-03-09</created><authors><author><keyname>Shen</keyname><forenames>Yi-Dong</forenames></author><author><keyname>Yuan</keyname><forenames>Li-Yan</forenames></author><author><keyname>You</keyname><forenames>Jia-Huai</forenames></author><author><keyname>Zhou</keyname><forenames>Neng-Fa</forenames></author></authors><title>Linear Tabulated Resolution Based on Prolog Control Strategy</title><categories>cs.AI cs.LO</categories><comments>To appear as the first accepted paper in Theory and Practice of Logic
  Programming (http://www.cwi.nl/projects/alp/TPLP)</comments><acm-class>F.4.1; I.2.3</acm-class><journal-ref>Theory and Practice of Logic Programming 1(1):71-103, 2001</journal-ref><abstract>  Infinite loops and redundant computations are long recognized open problems
in Prolog. Two ways have been explored to resolve these problems: loop checking
and tabling. Loop checking can cut infinite loops, but it cannot be both sound
and complete even for function-free logic programs. Tabling seems to be an
effective way to resolve infinite loops and redundant computations. However,
existing tabulated resolutions, such as OLDT-resolution, SLG- resolution, and
Tabulated SLS-resolution, are non-linear because they rely on the
solution-lookup mode in formulating tabling. The principal disadvantage of
non-linear resolutions is that they cannot be implemented using a simple
stack-based memory structure like that in Prolog. Moreover, some strictly
sequential operators such as cuts may not be handled as easily as in Prolog.
  In this paper, we propose a hybrid method to resolve infinite loops and
redundant computations. We combine the ideas of loop checking and tabling to
establish a linear tabulated resolution called TP-resolution. TP-resolution has
two distinctive features: (1) It makes linear tabulated derivations in the same
way as Prolog except that infinite loops are broken and redundant computations
are reduced. It handles cuts as effectively as Prolog. (2) It is sound and
complete for positive logic programs with the bounded-term-size property. The
underlying algorithm can be implemented by an extension to any existing Prolog
abstract machines such as WAM or ATOAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003047</id><created>2000-03-09</created><authors><author><keyname>Hoelldobler</keyname><forenames>Steffen</forenames></author><author><keyname>Stoerr</keyname><forenames>Hans-Peter</forenames></author></authors><title>BDD-based reasoning in the fluent calculus - first results</title><categories>cs.AI</categories><comments>9 pages; Workshop on Nonmonotonic Reasoning 2000 (NMR 2000)</comments><acm-class>I.2.8; I.2.3; F.4.1</acm-class><abstract>  The paper reports on first preliminary results and insights gained in a
project aiming at implementing the fluent calculus using methods and techniques
based on binary decision diagrams. After reporting on an initial experiment
showing promising results we discuss our findings concerning various techniques
and heuristics used to speed up the reasoning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003048</id><created>2000-03-09</created><authors><author><keyname>Cabalar</keyname><forenames>Pedro</forenames></author><author><keyname>Cabarcos</keyname><forenames>Manuel</forenames></author><author><keyname>Otero</keyname><forenames>Ramon P.</forenames></author></authors><title>PAL: Pertinence Action Language</title><categories>cs.AI cs.LO</categories><comments>5 pages</comments><acm-class>I.2.4; F.4.1</acm-class><abstract>  The current document contains a brief description of a system for Reasoning
about Actions and Change called PAL (Pertinence Action Language) which makes
use of several reasoning properties extracted from a Temporal Expert Systems
tool called Medtool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003049</id><created>2000-03-09</created><authors><author><keyname>Kakas</keyname><forenames>Antonis</forenames></author><author><keyname>Miller</keyname><forenames>Rob</forenames></author><author><keyname>Toni</keyname><forenames>Francesca</forenames></author></authors><title>Planning with Incomplete Information</title><categories>cs.AI</categories><comments>Proceedings of the 8th International Workshop on Non-Monotonic
  Reasoning, April 9-11, 2000, Breckenridge, Colorado</comments><acm-class>I.2.3;I.2.4;I.2.8</acm-class><abstract>  Planning is a natural domain of application for frameworks of reasoning about
actions and change. In this paper we study how one such framework, the Language
E, can form the basis for planning under (possibly) incomplete information. We
define two types of plans: weak and safe plans, and propose a planner, called
the E-Planner, which is often able to extend an initial weak plan into a safe
plan even though the (explicit) information available is incomplete, e.g. for
cases where the initial state is not completely known. The E-Planner is based
upon a reformulation of the Language E in argumentation terms and a natural
proof theory resulting from the reformulation. It uses an extension of this
proof theory by means of abduction for the generation of plans and adopts
argumentation-based techniques for extending weak plans into safe plans. We
provide representative examples illustrating the behaviour of the E-Planner, in
particular for cases where the status of fluents is incompletely known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003050</id><created>2000-03-10</created><authors><author><keyname>Artosi</keyname><forenames>Alberto</forenames></author><author><keyname>Governatori</keyname><forenames>Guido</forenames></author></authors><title>A tableau methodology for deontic conditional logics</title><categories>cs.LO cs.AI</categories><comments>17 pages</comments><acm-class>F.4.1; I.2.3; I.2.4</acm-class><journal-ref>Deon'98. 4th International Workshop on Deontic Logic in Computer
  Science. CIRFID, Bologna, 1998, 75-91</journal-ref><abstract>  In this paper we present a theorem proving methodology for a restricted but
significant fragment of the conditional language made up of (boolean
combinations of) conditional statements with unnested antecedents. The method
is based on the possible world semantics for conditional logics. The KEM label
formalism, designed to account for the semantics of normal modal logics, is
easily adapted to the semantics of conditional logics by simply indexing labels
with formulas. The inference rules are provided by the propositional system KE+
- a tableau-like analytic proof system devised to be used both as a refutation
and a direct method of proof - enlarged with suitable elimination rules for the
conditional connective. The theorem proving methodology we are going to present
can be viewed as a first step towards developing an appropriate algorithmic
framework for several conditional logics for (defeasible) conditional
obligation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003051</id><created>2000-03-10</created><authors><author><keyname>Wassermann</keyname><forenames>Renata</forenames></author></authors><title>Local Diagnosis</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><abstract>  In an earlier work, we have presented operations of belief change which only
affect the relevant part of a belief base. In this paper, we propose the
application of the same strategy to the problem of model-based diangosis. We
first isolate the subset of the system description which is relevant for a
given observation and then solve the diagnosis problem for this subset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003052</id><created>2000-03-11</created><updated>2000-04-03</updated><authors><author><keyname>Delgrande</keyname><forenames>James</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>A Consistency-Based Model for Belief Change: Preliminary Report</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><abstract>  We present a general, consistency-based framework for belief change.
Informally, in revising K by A, we begin with A and incorporate as much of K as
consistently possible. Formally, a knowledge base K and sentence A are
expressed, via renaming propositions in K, in separate languages. Using a
maximization process, we assume the languages are the same insofar as
consistently possible. Lastly, we express the resultant knowledge base in a
single language. There may be more than one way in which A can be so extended
by K: in choice revision, one such ``extension'' represents the revised state;
alternately revision consists of the intersection of all such extensions.
  The most general formulation of our approach is flexible enough to express
other approaches to revision and update, the merging of knowledge bases, and
the incorporation of static and dynamic integrity constraints. Our framework
differs from work based on ordinal conditional functions, notably with respect
to iterated revision. We argue that the approach is well-suited for
implementation: the choice revision operator gives better complexity results
than general revision; the approach can be expressed in terms of a finite
knowledge base; and the scope of a revision can be restricted to just those
propositions mentioned in the sentence for revision A.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003053</id><created>2000-03-11</created><authors><author><keyname>Lim</keyname><forenames>Lek-Heng</forenames></author></authors><title>Security of the Cao-Li Public Key Cryptosystem</title><categories>cs.CR math.NT</categories><comments>4 pages, article in its published form available from
  http://iel.ihs.com:80</comments><acm-class>E.3</acm-class><journal-ref>Electronics Letters, Volume 34 Number 2, pp. 170-172, January 22
  1998</journal-ref><abstract>  We show that the Cao-Li cryptosystem proposed in \cite{CL1} is not secure.
Its private key can be reconstructed from its public key using elementary means
such as LU-decomposition and Euclidean algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003054</id><created>2000-03-11</created><authors><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author></authors><title>A Problem-Specific Fault-Tolerance Mechanism for Asynchronous,
  Distributed Systems</title><categories>cs.DC</categories><comments>17 pages, 6 figures</comments><report-no>TR-00-01, The University of Chicago</report-no><acm-class>C.2.4</acm-class><abstract>  The idle computers on a local area, campus area, or even wide area network
represent a significant computational resource---one that is, however, also
unreliable, heterogeneous, and opportunistic. This type of resource has been
used effectively for embarrassingly parallel problems but not for more tightly
coupled problems. We describe an algorithm that allows branch-and-bound
problems to be solved in such environments. In designing this algorithm, we
faced two challenges: (1) scalability, to effectively exploit the variably
sized pools of resources available, and (2) fault tolerance, to ensure the
reliability of services. We achieve scalability through a fully decentralized
algorithm, by using a membership protocol for managing dynamically available
resources. However, this fully decentralized design makes achieving reliability
even more challenging. We guarantee fault tolerance in the sense that the loss
of up to all but one resource will not affect the quality of the solution. For
propagating information efficiently, we use epidemic communication for both the
membership protocol and the fault-tolerance mechanism. We have developed a
simulation framework that allows us to evaluate design alternatives. Results
obtained in this framework suggest that our techniques can execute scalably and
reliably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003055</id><created>2000-03-13</created><authors><author><keyname>Brants</keyname><forenames>Thorsten</forenames><affiliation>Saarland University, Germany</affiliation></author></authors><title>TnT - A Statistical Part-of-Speech Tagger</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of ANLP-2000, Seattle, WA</journal-ref><abstract>  Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.
Contrary to claims found elsewhere in the literature, we argue that a tagger
based on Markov models performs at least as well as other current approaches,
including the Maximum Entropy framework. A recent comparison has even shown
that TnT performs significantly better for the tested corpora. We describe the
basic model of TnT, the techniques used for smoothing and for handling unknown
words. Furthermore, we present evaluations on two corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003056</id><created>2000-03-13</created><authors><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>A note on the Declarative reading(s) of Logic Programming</title><categories>cs.LO cs.AI</categories><comments>6 pages; poster at NMR2000, Breckenridge, April 2000</comments><acm-class>I.2.3;I.2.4;F.4.1</acm-class><abstract>  This paper analyses the declarative readings of logic programming. Logic
programming - and negation as failure - has no unique declarative reading. One
common view is that logic programming is a logic for default reasoning, a
sub-formalism of default logic or autoepistemic logic. In this view, negation
as failure is a modal operator. In an alternative view, a logic program is
interpreted as a definition. In this view, negation as failure is classical
objective negation. From a commonsense point of view, there is definitely a
difference between these views. Surprisingly though, both types of declarative
readings lead to grosso modo the same model semantics. This note investigates
the causes for this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003057</id><created>2000-03-13</created><authors><author><keyname>Castro</keyname><forenames>L.</forenames></author><author><keyname>Warren</keyname><forenames>D.</forenames></author></authors><title>XNMR: A tool for knowledge bases exploration</title><categories>cs.LO cs.AI</categories><comments>2 pages; no figures; NMR2000 Systems Description</comments><acm-class>D.1.6</acm-class><abstract>  XNMR is a system designed to explore the results of combining the
well-founded semantics system XSB with the stable-models evaluator SMODELS. Its
main goal is to work as a tool for fast and interactive exploration of
knowledge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003058</id><created>2000-03-13</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>A note on knowledge-based programs and specifications</title><categories>cs.DC cs.LO</categories><comments>To appear, Distributed Computing</comments><acm-class>D.1.3; F.3.1; D.2.1</acm-class><abstract>  Knowledge-based program are programs with explicit tests for knowledge. They
have been used successfully in a number of applications. Sanders has pointed
out what seem to be a counterintuitive property of knowledge-based programs.
Roughly speaking, they do not satisfy a certain monotonicity property, while
standard programs (ones without tests for knowledge) do. It is shown that there
are two ways of defining the monotonicity property, which agree for standard
programs. Knowledge-based programs satisfy the first, but do not satisfy the
second. It is further argued by example that the fact that they do not satisfy
the second is actually a feature, not a problem. Moreover, once we allow the
more general class of knowledge-based specifications, standard programs do not
satisfy the monotonicity property either.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003059</id><created>2000-03-13</created><authors><author><keyname>Williams</keyname><forenames>Mary-Anne</forenames></author><author><keyname>Sims</keyname><forenames>Aidan</forenames></author></authors><title>SATEN: An Object-Oriented Web-Based Revision and Extraction Engine</title><categories>cs.AI</categories><comments>The implementation of SATEN can be found at
  http://cafe.newcastle.edu.au/saten</comments><acm-class>I.2.3</acm-class><abstract>  SATEN is an object-oriented web-based extraction and belief revision engine.
It runs on any computer via a Java 1.1 enabled browser such as Netscape 4.
SATEN performs belief revision based on the AGM approach. The extraction and
belief revision reasoning engines operate on a user specified ranking of
information. One of the features of SATEN is that it can be used to integrate
mutually inconsistent commensuate rankings into a consistent ranking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003060</id><created>2000-03-14</created><authors><author><keyname>Busemann</keyname><forenames>Stephan</forenames></author><author><keyname>Schmeier</keyname><forenames>Sven</forenames></author><author><keyname>Arens</keyname><forenames>Roman G.</forenames></author></authors><title>Message Classification in the Call Center</title><categories>cs.CL</categories><comments>8 pages with 2 figures</comments><acm-class>I.2.6; I.2.7; I.7.5</acm-class><journal-ref>Proceedings of ANLP-2000, Seattle, WA</journal-ref><abstract>  Customer care in technical domains is increasingly based on e-mail
communication, allowing for the reproduction of approved solutions. Identifying
the customer's problem is often time-consuming, as the problem space changes if
new products are launched. This paper describes a new approach to the
classification of e-mail requests based on shallow text processing and machine
learning techniques. It is implemented within an assistance system for call
center agents that is used in a commercial setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003061</id><created>2000-03-14</created><authors><author><keyname>East</keyname><forenames>Deborah</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>dcs: An Implementation of DATALOG with Constraints</title><categories>cs.AI</categories><comments>6 pages (AAAI format), 4 ps figures; System descriptions and
  demonstration Session, 8th Intl. Workshop on Non-Monotonic Reasoning</comments><acm-class>I.2.3;I.2.4;I.2.8;F.4.1;F.2.2</acm-class><abstract>  Answer-set programming (ASP) has emerged recently as a viable programming
paradigm. We describe here an ASP system, DATALOG with constraints or DC, based
on non-monotonic logic. Informally, DC theories consist of propositional
clauses (constraints) and of Horn rules. The semantics is a simple and natural
extension of the semantics of the propositional logic. However, thanks to the
presence of Horn rules in the system, modeling of transitive closure becomes
straightforward. We describe the syntax, use and implementation of DC and
provide experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003062</id><created>2000-03-14</created><updated>2001-01-15</updated><authors><author><keyname>McDowell</keyname><forenames>Raymond C.</forenames></author><author><keyname>Miller</keyname><forenames>Dale A.</forenames></author></authors><title>Reasoning with Higher-Order Abstract Syntax in a Logical Framework</title><categories>cs.LO cs.PL</categories><comments>56 pages, 21 tables; revised in light of reviewer comments; to appear
  in ACM Transactions on Computational Logic</comments><acm-class>D.3.1; F.3.1; D.2.4; F.4.1</acm-class><abstract>  Logical frameworks based on intuitionistic or linear logics with higher-type
quantification have been successfully used to give high-level, modular, and
formal specifications of many important judgments in the area of programming
languages and inference systems. Given such specifications, it is natural to
consider proving properties about the specified systems in the framework: for
example, given the specification of evaluation for a functional programming
language, prove that the language is deterministic or that evaluation preserves
types. One challenge in developing a framework for such reasoning is that
higher-order abstract syntax (HOAS), an elegant and declarative treatment of
object-level abstraction and substitution, is difficult to treat in proofs
involving induction. In this paper, we present a meta-logic that can be used to
reason about judgments coded using HOAS; this meta-logic is an extension of a
simple intuitionistic logic that admits higher-order quantification over simply
typed lambda-terms (key ingredients for HOAS) as well as induction and a notion
of definition. We explore the difficulties of formal meta-theoretic analysis of
HOAS encodings by considering encodings of intuitionistic and linear logics,
and formally derive the admissibility of cut for important subsets of these
logics. We then propose an approach to avoid the apparent tradeoff between the
benefits of higher-order abstract syntax and the ability to analyze the
resulting encodings. We illustrate this approach through examples involving the
simple functional and imperative programming languages PCF and PCF:=. We
formally derive such properties as unicity of typing, subject reduction,
determinacy of evaluation, and the equivalence of transition semantics and
natural semantics presentations of evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003063</id><created>2000-03-14</created><updated>2000-05-02</updated><authors><author><keyname>Guimond</keyname><forenames>Louis-Sebastien</forenames></author><author><keyname>Patera</keyname><forenames>Jan</forenames></author><author><keyname>Patera</keyname><forenames>Jiri</forenames></author></authors><title>Statistics and implementation of APRNGs</title><categories>cs.DM</categories><comments>This paper has been temporarily withdrawn by the author(s)</comments><acm-class>G3</acm-class><abstract>  This paper has been temporarily withdrawn by the author(s),
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003064</id><created>2000-03-15</created><authors><author><keyname>Kiselyov</keyname><forenames>Oleg</forenames></author></authors><title>A network file system over HTTP: remote access and modification of files
  and &quot;files&quot;</title><categories>cs.OS cs.NI</categories><comments>This present document combines a paper and a Freenix Track talk
  presented at a 1999 USENIX Annual Technical Conference, June 6-11, 1999;
  Monterey, CA, USA; 6 HTML files. The paper alone appeared in Proc. FREENIX
  Track: 1999 USENIX Annual Technical Conference, June 6-11,1999; Monterey, CA,
  USA, pp. 75-80</comments><acm-class>D.4.3; D.4.4; E.5; C.2.2; C.2.4</acm-class><abstract>  The goal of the present HTTPFS project is to enable access to remote files,
directories, and other containers through an HTTP pipe. HTTPFS system permits
retrieval, creation and modification of these resources as if they were regular
files and directories on a local filesystem. The remote host can be any UNIX or
Win9x/WinNT box that is capable of running a Perl CGI script and accessible
either directly or via a web proxy or a gateway. HTTPFS runs entirely in user
space.
  The current implementation fully supports reading as well as creating,
writing, appending, and truncating of files on a remote HTTP host. HTTPFS
provides an isolation level for concurrent file access stronger than the one
mandated by POSIX file system semantics, closer to that of AFS. Both an API
with familiar open(), read(), write(), close(), etc. calls, and an interactive
interface, via the popular Midnight Commander file browser, are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003065</id><created>2000-03-15</created><authors><author><keyname>Kiselyov</keyname><forenames>Oleg</forenames></author><author><keyname>Fisher</keyname><forenames>Paul</forenames></author></authors><title>Image Compression with Iterated Function Systems, Finite Automata and
  Zerotrees: Grand Unification</title><categories>cs.CV</categories><comments>This is a full paper submitted to Data Compression Conference '96; 10
  pages; The abstract of this paper was published in Proc. DCC'96: Data
  Compression Conference, March 31 - April 3, 1996, Snowbird, Utah, IEEE
  Computer Society Press, Los Alamitos, California, 1996, p.443</comments><acm-class>I.4.2; I.4.10; G.1.2</acm-class><abstract>  Fractal image compression, Culik's image compression and zerotree prediction
coding of wavelet image decomposition coefficients succeed only because typical
images being compressed possess a significant degree of self-similarity.
Besides the common concept, these methods turn out to be even more tightly
related, to the point of algorithmical reducibility of one technique to
another. The goal of the present paper is to demonstrate these relations.
  The paper offers a plain-term interpretation of Culik's image compression, in
regular image processing terms, without resorting to finite state machines and
similar lofty language. The interpretation is shown to be algorithmically
related to an IFS fractal image compression method: an IFS can be exactly
transformed into Culik's image code. Using this transformation, we will prove
that in a self-similar (part of an) image any zero wavelet coefficient is the
root of a zerotree, or its branch.
  The paper discusses the zerotree coding of (wavelet/projection) coefficients
as a common predictor/corrector, applied vertically through different layers of
a multiresolutional decomposition, rather than within the same view. This
interpretation leads to an insight into the evolution of image compression
techniques: from a causal single-layer prediction, to non-causal same-view
predictions (wavelet decomposition among others) and to a causal cross-layer
prediction (zero-trees, Culik's method).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003066</id><created>2000-03-15</created><authors><author><keyname>Hoagland</keyname><forenames>James A.</forenames></author></authors><title>Specifying and Implementing Security Policies Using LaSCO, the Language
  for Security Constraints on Objects</title><categories>cs.CR</categories><comments>Ph.D. disseration, UC Davis, Computer Science, March 2000. In color
  but looks okay in black and white</comments><acm-class>D.4.6</acm-class><abstract>  In this dissertation, we present LaSCO, the Language for Security Constraints
on Objects, a new approach to expressing security policies using policy graphs
and present a method for enforcing policies so expressed. Other approaches for
stating security policies fall short of what is desirable with respect to
either policy clarity, executability, or the precision with which a policy may
be expressed. However, LaSCO is designed to have those three desirable
properties of a security policy language as well as: relevance for many
different systems, statement of policies at an appropriate level of detail,
user friendliness for both casual and expert users, and amenability to formal
reasoning. In LaSCO, the constraints of a policy are stated as directed graphs
annotated with expressions describing the situation under which the policy
applies and what the requirement is. LaSCO may be used for such diverse
applications as executing programs, file systems, operating systems,
distributed systems, and networks.
  Formal operational semantics have been defined for LaSCO. An architecture for
implementing LaSCO on any system, is presented along with an implementation of
the system-independent portion in Perl. Using this, we have implemented LaSCO
for Java programs, preventing Java programs from violating policy. A GUI to
facilitate writing policies is provided. We have studied applying LaSCO to a
network as viewed by GrIDS, a distributed intrusion detection system for large
networks, and propose a design. We conclude that LaSCO has characteristics that
enable its use on different types of systems throughout the process of
precisely expressing a policy, understanding the implications of a policy, and
implementing it on a system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003067</id><created>2000-03-17</created><authors><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames><affiliation>Katholieke Universiteit Leuven, Belgium</affiliation></author><author><keyname>Vandecasteele</keyname><forenames>Henk</forenames><affiliation>Katholieke Universiteit Leuven, Belgium</affiliation></author><author><keyname>de Waal</keyname><forenames>D. Andre</forenames><affiliation>Potchefstroom University for Christian Higher Education, South Africa</affiliation></author><author><keyname>Denecker</keyname><forenames>Marc</forenames><affiliation>Katholieke Universiteit Leuven, Belgium</affiliation></author></authors><title>Detecting Unsolvable Queries for Definite Logic Programs</title><categories>cs.LO cs.AI</categories><comments>32 pages including appendix. A preliminary version appeared in
  proceedings PLILP/ALP98 (Springer LNCS 1490) This version, without appendix
  appeared in Journal Functional and Logic Programming 1999</comments><acm-class>D.1.6; F.3.1; F.4.1</acm-class><journal-ref>Journal of Functional and Logic Programming, Vol. 1999, 1-35, 1999</journal-ref><abstract>  In solving a query, the SLD proof procedure for definite programs sometimes
searches an infinite space for a non existing solution. For example, querying a
planner for an unreachable goal state. Such programs motivate the development
of methods to prove the absence of a solution. Considering the definite program
and the query ``&lt;- Q'' as clauses of a first order theory, one can apply model
generators which search for a finite interpretation in which the program
clauses as well as the clause ``false &lt;- Q'' are true. This paper develops a
new approach which exploits the fact that all clauses are definite. It is based
on a goal directed abductive search in the space of finite pre-interpretations
for a pre-interpretation such that ``Q'' is false in the least model of the
program based on it. Several methods for efficiently searching the space of
pre-interpretations are presented. Experimental results confirm that our
approach find solutions with less search than with the use of a first order
model generator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003068</id><created>2000-03-17</created><authors><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames><affiliation>Katholieke Universiteit Leuven, Belgium</affiliation></author><author><keyname>Leuschel</keyname><forenames>Michael</forenames><affiliation>Katholieke Universiteit Leuven, Belgium</affiliation></author><author><keyname>Sagonas</keyname><forenames>Konstantinos</forenames><affiliation>Katholieke Universiteit Leuven, Belgium</affiliation></author></authors><title>A Polyvariant Binding-Time Analysis for Off-line Partial Deduction</title><categories>cs.PL cs.LO</categories><comments>19 pages (including appendix) Paper (without appendix) appeared in
  Programming Languages and Systems, Proceedings of the European Symposium on
  Programming (ESOP'98), Part of ETAPS'98 (Chris Hankin, eds.), LNCS, vol.
  1381, 1998, pp. 27-41</comments><acm-class>D.3.0; D.1.6; F.3.1</acm-class><abstract>  We study the notion of binding-time analysis for logic programs. We formalise
the unfolding aspect of an on-line partial deduction system as a Prolog
program. Using abstract interpretation, we collect information about the
run-time behaviour of the program. We use this information to make the control
decisions about the unfolding at analysis time and to turn the on-line system
into an off-line system. We report on some initial experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003069</id><created>2000-03-20</created><authors><author><keyname>Pelov</keyname><forenames>Nikolay</forenames></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames></author></authors><title>Proving Failure of Queries for Definite Logic Programs Using XSB-Prolog</title><categories>cs.LO</categories><comments>18 pages, 1 figure, 3 tables presented at LPAR'99, Tbilisi, Georgia</comments><acm-class>F.3.1; I.2.3</acm-class><abstract>  Proving failure of queries for definite logic programs can be done by
constructing a finite model of the program in which the query is false. A
general purpose model generator for first order logic can be used for this. A
recent paper presented at PLILP98 shows how the peculiarities of definite
programs can be exploited to obtain a better solution. There a procedure is
described which combines abduction with tabulation and uses a meta-interpreter
for heuristic control of the search. The current paper shows how similar
results can be obtained by direct execution under the standard tabulation of
the XSB-Prolog system. The loss of control is compensated for by better
intelligent backtracking and more accurate failure analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003070</id><created>2000-03-20</created><authors><author><keyname>Etalle</keyname><forenames>S.</forenames></author><author><keyname>Mountjoy</keyname><forenames>J.</forenames></author></authors><title>The (Lazy) Functional Side of Logic Programming</title><categories>cs.PL cs.LO</categories><comments>23 pages</comments><report-no>CS-00-02</report-no><acm-class>D.1.1; D.1.6; D.3.2; F.3.3</acm-class><abstract>  The possibility of translating logic programs into functional ones has long
been a subject of investigation. Common to the many approaches is that the
original logic program, in order to be translated, needs to be well-moded and
this has led to the common understanding that these programs can be considered
to be the ``functional part'' of logic programs. As a consequence of this it
has become widely accepted that ``complex'' logical variables, the possibility
of a dynamic selection rule, and general properties of non-well-moded programs
are exclusive features of logic programs. This is not quite true, as some of
these features are naturally found in lazy functional languages. We readdress
the old question of what features are exclusive to the logic programming
paradigm by defining a simple translation applicable to a wider range of logic
programs, and demonstrate that the current circumscription is unreasonably
restrictive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003071</id><created>2000-03-21</created><authors><author><keyname>Volkstorf</keyname><forenames>Charlie</forenames></author></authors><title>Axiomatic Synthesis of Computer Programs and Computability Theorems</title><categories>cs.LO</categories><comments>for submission to Journal of the ACM</comments><acm-class>D.1.2; D.2.1; F.1.1; F.2.1; F.3.1; F.4.1; H.2.3; H.2.4; H.3.3; I.2.2</acm-class><abstract>  We introduce a set of eight universal Rules of Inference by which computer
programs with known properties (axioms) are transformed into new programs with
known properties (theorems). Axioms are presented to formalize a segment of
Number Theory, DataBase retrieval and Computability Theory. The resulting
Program Calculus is used to generate programs to (1) Determine if one number is
a factor of another. (2) List all employees who earn more than their manager.
(3) List the set of programs that halt no on themselves, thus proving that it
is recursively enumerable. The well-known fact that the set of programs that do
not halt yes on themselves is not recursively enumerable is formalized as a
program requirement that has no solution, an Incompleteness Axiom. Thus, any
axioms (programs) which could be used to generate this program are themselves
unattainable. Such proofs are presented to formally generate several additional
theorems, including (4) The halting problem is unsolvable.
  Open problems and future research is discussed, including the use of
temporary sort files, programs that calculate statistics (such as counts and
sums), the synthesis of programs to solve other well-known problems from Number
Theory, Logic, DataBase retrieval and Computability Theory, application to
Programming Language Semantics, and the formalization of incompleteness results
from Logic and the semantic paradoxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003072</id><created>2000-03-22</created><authors><author><keyname>Lee</keyname><forenames>Jason W. H.</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Tay</keyname><forenames>Y. C.</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Tung</keyname><forenames>Anthony K. H.</forenames><affiliation>National University of Singapore</affiliation></author></authors><title>MOO: A Methodology for Online Optimization through Mining the Offline
  Optimum</title><categories>cs.DS cs.LG</categories><comments>12 pages, 4 figures</comments><report-no>Research Report No. 743</report-no><acm-class>F.2.2;H.2.8;F.1.2</acm-class><abstract>  Ports, warehouses and courier services have to decide online how an arriving
task is to be served in order that cost is minimized (or profit maximized).
These operators have a wealth of historical data on task assignments; can these
data be mined for knowledge or rules that can help the decision-making?
  MOO is a novel application of data mining to online optimization. The idea is
to mine (logged) expert decisions or the offline optimum for rules that can be
used for online decisions. It requires little knowledge about the task
distribution and cost structure, and is applicable to a wide range of problems.
  This paper presents a feasibility study of the methodology for the well-known
k-server problem. Experiments with synthetic data show that optimization can be
recast as classification of the optimum decisions; the resulting heuristic can
achieve the optimum for strong request patterns, consistently outperforms other
heuristics for weak patterns, and is robust despite changes in cost model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003073</id><created>2000-03-22</created><authors><author><keyname>Baral</keyname><forenames>Chitta</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Proceedings of the 8th International Workshop on Non-Monotonic
  Reasoning, NMR'2000</title><categories>cs.AI cs.LO</categories><comments>Contributing editors: Marc Denecker, Antonis Kakas, Francesca Toni -
  Abductive Reasoning; Samir Chopra, Mary-Anne Williams - Belief change: theory
  and practice; Vladimir Lifschitz, Alessandro Provetti - Representing actions
  and planning; Juergen Dix - System demonstrations and presentations; Salem
  Benferhat, Henri Prade - Uncertainty frameworks in NMR</comments><acm-class>I2.2;I2.3;I2.4;I2.8;F4.1</acm-class><abstract>  The papers gathered in this collection were presented at the 8th
International Workshop on Nonmonotonic Reasoning, NMR2000. The series was
started by John McCarthy in 1978. The first international NMR workshop was held
at Mohonk Mountain House, New Paltz, New York in June, 1984, and was organized
by Ray Reiter and Bonnie Webber.
  In the last 10 years the area of nonmonotonic reasoning has seen a number of
important developments. Significant theoretical advances were made in the
understanding of general abstract principles underlying nonmonotonicity. Key
results on the expressibility and computational complexity of nonmonotonic
logics were established. The role of nonmonotonic reasoning in belief revision,
abduction, reasoning about action, planing and uncertainty was further
clarified. Several successful NMR systems were built and used in applications
such as planning, scheduling, logic programming and constraint satisfaction.
  The papers in the proceedings reflect these recent advances in the field.
They are grouped into sections corresponding to special sessions as they were
held at the workshop:
  1. General NMR track
  2. Abductive reasonig
  3. Belief revision: theory and practice
  4. Representing action and planning
  5. Systems descriptions and demonstrations
  6. Uncertainty frameworks in NMR
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003074</id><created>2000-03-23</created><authors><author><keyname>Bouma</keyname><forenames>Gosse</forenames></author></authors><title>A Finite State and Data-Oriented Method for Grapheme to Phoneme
  Conversion</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of NAACL-2000, Seattle, WA</journal-ref><abstract>  A finite-state method, based on leftmost longest-match replacement, is
presented for segmenting words into graphemes, and for converting graphemes
into phonemes. A small set of hand-crafted conversion rules for Dutch achieves
a phoneme accuracy of over 93%. The accuracy of the system is further improved
by using transformation-based learning. The phoneme accuracy of the best system
(using a large set of rule templates and a `lazy' variant of Brill's algoritm),
trained on only 40K words, reaches 99% accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003075</id><created>2000-03-23</created><authors><author><keyname>Burgess</keyname><forenames>Mark</forenames></author></authors><title>On the theory of system administration</title><categories>cs.OH</categories><comments>About 38 pages american size, 4 figures</comments><acm-class>H.1.m</acm-class><abstract>  This paper describes necessary elements for constructing theoretical models
of network and system administration. Armed with a theoretical model it becomes
possible to determine best practices and optimal strategies in a way which
objectively relates policies and assumptions to results obtained. It is
concluded that a mixture of automation and human, or other intelligent
incursion is required to fully implement system policy with current technology.
Some aspects of the author's immunity model for automated system administration
are explained, as an example. A theoretical framework makes the prediction that
the optimal balance between resource availability and garbage collection
strategies is encompassed by the immunity model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003076</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003076</id><created>2000-03-24</created><updated>2001-05-23</updated><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Monfroy</keyname><forenames>Eric</forenames></author></authors><title>Constraint Programming viewed as Rule-based Programming</title><categories>cs.AI cs.PL</categories><comments>39 pages. To appear in Theory and Practice of Logic Programming
  Journal</comments><acm-class>D.3.3;I.2.2;I.2.3</acm-class><abstract>  We study here a natural situation when constraint programming can be entirely
reduced to rule-based programming. To this end we explain first how one can
compute on constraint satisfaction problems using rules represented by simple
first-order formulas. Then we consider constraint satisfaction problems that
are based on predefined, explicitly given constraints. To solve them we first
derive rules from these explicitly given constraints and limit the computation
process to a repeated application of these rules, combined with labeling.We
consider here two types of rules. The first type, that we call equality rules,
leads to a new notion of local consistency, called {\em rule consistency} that
turns out to be weaker than arc consistency for constraints of arbitrary arity
(called hyper-arc consistency in \cite{MS98b}). For Boolean constraints rule
consistency coincides with the closure under the well-known propagation rules
for Boolean constraints. The second type of rules, that we call membership
rules, yields a rule-based characterization of arc consistency. To show
feasibility of this rule-based approach to constraint programming we show how
both types of rules can be automatically generated, as {\tt CHR} rules of
\cite{fruhwirth-constraint-95}. This yields an implementation of this approach
to programming by means of constraint logic programming. We illustrate the
usefulness of this approach to constraint programming by discussing various
examples, including Boolean constraints, two typical examples of many valued
logics, constraints dealing with Waltz's language for describing polyhedral
scenes, and Allen's qualitative approach to temporal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003077</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003077</id><created>2000-03-24</created><authors><author><keyname>East</keyname><forenames>Deborah</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>DATALOG with constraints - an answer-set programming system</title><categories>cs.AI</categories><comments>6 pages, 5 figures, will appear in Proceedings of AAAI-2000</comments><acm-class>D.1.6;F.2.2;F.4.2;I.2.8;I.6.5</acm-class><abstract>  Answer-set programming (ASP) has emerged recently as a viable programming
paradigm well attuned to search problems in AI, constraint satisfaction and
combinatorics. Propositional logic is, arguably, the simplest ASP system with
an intuitive semantics supporting direct modeling of problem constraints.
However, for some applications, especially those requiring that transitive
closure be computed, it requires additional variables and results in large
theories. Consequently, it may not be a practical computational tool for such
problems. On the other hand, ASP systems based on nonmonotonic logics, such as
stable logic programming, can handle transitive closure computation efficiently
and, in general, yield very concise theories as problem representations. Their
semantics is, however, more complex. Searching for the middle ground, in this
paper we introduce a new nonmonotonic logic, DATALOG with constraints or DC.
Informally, DC theories consist of propositional clauses (constraints) and of
Horn rules. The semantics is a simple and natural extension of the semantics of
the propositional logic. However, thanks to the presence of Horn rules in the
system, modeling of transitive closure becomes straightforward. We describe the
syntax and semantics of DC, and study its properties. We discuss an
implementation of DC and present results of experimental study of the
effectiveness of DC, comparing it with CSAT, a satisfiability checker and
SMODELS implementation of stable logic programming. Our results show that DC is
competitive with the other two approaches, in case of many search problems,
often yielding much more efficient solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003078</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003078</id><created>2000-03-24</created><authors><author><keyname>Plotnikov</keyname><forenames>Anatoly D.</forenames></author></authors><title>About the finding of independent vertices of a graph</title><categories>cs.DS</categories><comments>8 pages, 2 figures</comments><acm-class>F.2.2;G.2.1;G.2.2</acm-class><journal-ref>About the finding of independent vertices of a graph, Journal
  &quot;Kibernetika&quot;, No. 1, 1989, p. 119 - 121</journal-ref><abstract>  We examine the Maximum Independent Set Problem in an undirected graph. The
main result is that this problem can be considered as the solving the same
problem in a subclass of the weighted normal twin-orthogonal graphs. The
problem is formulated which is dual to the problem above. It is shown that, for
trivial twin-orthogonal graphs, any of its maximal independent set is also
maximum one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003079</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003079</id><created>2000-03-26</created><authors><author><keyname>Siebert</keyname><forenames>Andreas</forenames></author></authors><title>Differential Invariants under Gamma Correction</title><categories>cs.CV</categories><comments>8 pages, 12 figures</comments><acm-class>I.4.7</acm-class><journal-ref>Vision Interface 2000, Montreal, 2000</journal-ref><abstract>  This paper presents invariants under gamma correction and similarity
transformations. The invariants are local features based on differentials which
are implemented using derivatives of the Gaussian. The use of the proposed
invariant representation is shown to yield improved correlation results in a
template matching scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003080</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003080</id><created>2000-03-28</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>Some Remarks on Boolean Constraint Propagation</title><categories>cs.AI</categories><comments>14 pages. To appear in: New Trends in Constraints, Papers from the
  Joint ERCIM/Compulog-Net Workshop Cyprus, October 25-27, 1999.
  Springer-Verlag Lecture Notes in Artificial Intelligence</comments><acm-class>D.3.2;D.3.3</acm-class><abstract>  We study here the well-known propagation rules for Boolean constraints. First
we propose a simple notion of completeness for sets of such rules and establish
a completeness result. Then we show an equivalence in an appropriate sense
between Boolean constraint propagation and unit propagation, a form of
resolution for propositional logic.
  Subsequently we characterize one set of such rules by means of the notion of
hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify
the status of a similar, though different, set of rules introduced in (Simonis
1989a) and more fully in (Codognet and Diaz 1996).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003081</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003081</id><created>2000-03-29</created><authors><author><keyname>Gotoh</keyname><forenames>Yoshihiko</forenames></author><author><keyname>Renals</keyname><forenames>Steve</forenames></author></authors><title>Variable Word Rate N-grams</title><categories>cs.CL</categories><comments>4 pages, 4 figures, ICASSP-2000</comments><acm-class>I.2.7</acm-class><abstract>  The rate of occurrence of words is not uniform but varies from document to
document. Despite this observation, parameters for conventional n-gram language
models are usually derived using the assumption of a constant word rate. In
this paper we investigate the use of variable word rate assumption, modelled by
a Poisson distribution or a continuous mixture of Poissons. We present an
approach to estimating the relative frequencies of words or n-grams taking
prior information of their occurrences into account. Discounting and smoothing
schemes are also considered. Using the Broadcast News task, the approach
demonstrates a reduction of perplexity up to 10%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003082</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003082</id><created>2000-03-29</created><authors><author><keyname>Antoniou</keyname><forenames>G.</forenames></author><author><keyname>Billington</keyname><forenames>D.</forenames></author><author><keyname>Governatori</keyname><forenames>G.</forenames></author><author><keyname>Maher</keyname><forenames>M. J.</forenames></author></authors><title>Representation results for defeasible logic</title><categories>cs.LO cs.AI</categories><comments>30 pages, 1 figure</comments><acm-class>F.4.1; I.2.3; I.2.4</acm-class><abstract>  The importance of transformations and normal forms in logic programming, and
generally in computer science, is well documented. This paper investigates
transformations and normal forms in the context of Defeasible Logic, a simple
but efficient formalism for nonmonotonic reasoning based on rules and
priorities. The transformations described in this paper have two main benefits:
on one hand they can be used as a theoretical tool that leads to a deeper
understanding of the formalism, and on the other hand they have been used in
the development of an efficient implementation of defeasible logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003083</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003083</id><created>2000-03-30</created><authors><author><keyname>Choi</keyname><forenames>Freddy Y. Y.</forenames><affiliation>University of Manchester</affiliation></author></authors><title>Advances in domain independent linear text segmentation</title><categories>cs.CL</categories><comments>8 pages, 8 figures. To appear in Proceedings of NAACL00, Seattle.
  Software and experiment packages available from author's homepage:
  http://www.cs.man.ac.uk/~choif</comments><acm-class>I.2.7</acm-class><abstract>  This paper describes a method for linear text segmentation which is twice as
accurate and over seven times as fast as the state-of-the-art (Reynar, 1998).
Inter-sentence similarity is replaced by rank in the local context. Boundary
locations are discovered by divisive clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0003084</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0003084</id><created>2000-03-30</created><authors><author><keyname>Gotoh</keyname><forenames>Yoshihiko</forenames></author><author><keyname>Renals</keyname><forenames>Steve</forenames></author></authors><title>Information Extraction from Broadcast News</title><categories>cs.CL</categories><comments>12 pages, 3 figures, Philosophical Transactions of the Royal Society
  of London, series A: Mathematical, Physical and Engineering Sciences, vol.
  358, 2000</comments><acm-class>I.2.7</acm-class><doi>10.1098/rsta.2000.0587</doi><abstract>  This paper discusses the development of trainable statistical models for
extracting content from television and radio news broadcasts. In particular we
concentrate on statistical finite state models for identifying proper names and
other named entities in broadcast speech. Two models are presented: the first
represents name class information as a word attribute; the second represents
both word-word and class-class transitions explicitly. A common n-gram based
formulation is used for both models. The task of named entity identification is
characterized by relatively sparse training data and issues related to
smoothing are discussed. Experiments are reported using the DARPA/NIST Hub-4E
evaluation for North American Broadcast News.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004001</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004001</id><created>2000-04-03</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>A Theory of Universal Artificial Intelligence based on Algorithmic
  Complexity</title><categories>cs.AI cs.IT cs.LG math.IT</categories><comments>62 pages, LaTeX</comments><acm-class>I.2; F.1.3; E.4</acm-class><abstract>  Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental prior probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown prior distribution. We combine both ideas and
get a parameterless theory of universal Artificial Intelligence. We give strong
arguments that the resulting AIXI model is the most intelligent unbiased agent
possible. We outline for a number of problem classes, including sequence
prediction, strategic games, function minimization, reinforcement and
supervised learning, how the AIXI model can formally solve them. The major
drawback of the AIXI model is that it is uncomputable. To overcome this
problem, we construct a modified algorithm AIXI-tl, which is still effectively
more intelligent than any other time t and space l bounded agent. The
computation time of AIXI-tl is of the order tx2^l. Other discussed topics are
formal definitions of intelligence order relations, the horizon problem and
relations of the AIXI theory to other AI approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004002</id><created>2000-04-05</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Schaerf</keyname><forenames>Andrea</forenames></author></authors><title>Programming in Alma-0, or Imperative and Declarative Programming
  Reconciled</title><categories>cs.LO cs.AI cs.PL</categories><comments>With updated references with respect to the published version</comments><acm-class>D.3.2;F.3.2;F.3.3;I.2.8;I.5.5</acm-class><journal-ref>Frontiers of Combining Systems 2, Research Studies Press Ltd, D.
  M. Gabbay and M. de Rijke (editors), pages 1-16, 1999</journal-ref><abstract>  In (Apt et al, TOPLAS 1998) we introduced the imperative programming language
Alma-0 that supports declarative programming. In this paper we illustrate the
hybrid programming style of Alma-0 by means of various examples that complement
those presented in (Apt et al, TOPLAS 1998). The presented Alma-0 programs
illustrate the versatility of the language and show that ``don't know''
nondeterminism can be naturally combined with assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004003</id><created>2000-04-10</created><updated>2000-04-26</updated><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Searching for Spaceships</title><categories>cs.AI nlin.CG</categories><comments>17 pages, 13 figures. This revision adds Paul Tooke's new c/6
  &quot;dragon&quot;, and corrects one URL. For more information about the software and
  patterns described here, see http://www.ics.uci.edu/~eppstein/ca/</comments><acm-class>I.2.8; F.1.1</acm-class><journal-ref>More Games of No Chance, MSRI Publications 42, 2002, pp. 433-453</journal-ref><abstract>  We describe software that searches for spaceships in Conway's Game of Life
and related two-dimensional cellular automata. Our program searches through a
state space related to the de Bruijn graph of the automaton, using a method
that combines features of breadth first and iterative deepening search, and
includes fast bit-parallel graph reachability and path enumeration algorithms
for finding the successors of each state. Successful results include a new 2c/7
spaceship in Life, found by searching a space with 2^126 states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004004</id><created>2000-04-13</created><authors><author><keyname>Boisvert</keyname><forenames>Ronald F.</forenames></author></authors><title>Mathematical Software: Past, Present, and Future</title><categories>cs.MS</categories><comments>To appear in the Proceedings of the International Symposium on
  Computational Sciences, Purdue University, May 21-22, 1999. 20 pages</comments><acm-class>G.4</acm-class><abstract>  This paper provides some reflections on the field of mathematical software on
the occasion of John Rice's 65th birthday. I describe some of the common themes
of research in this field and recall some significant events in its evolution.
Finally, I raise a number of issues that are of concern to future developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004005</id><created>2000-04-16</created><authors><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author></authors><title>Exact Phase Transitions in Random Constraint Satisfaction Problems</title><categories>cs.AI cs.CC cs.DM</categories><comments>See http://www.jair.org/ for any accompanying files</comments><acm-class>I.2.8; G.3</acm-class><journal-ref>Journal of Artificial Intelligence Research, Vol 12, (2000),
  93-103.</journal-ref><abstract>  In this paper we propose a new type of random CSP model, called Model RB,
which is a revision to the standard Model B. It is proved that phase
transitions from a region where almost all problems are satisfiable to a region
where almost all problems are unsatisfiable do exist for Model RB as the number
of variables approaches infinity. Moreover, the critical values at which the
phase transitions occur are also known exactly. By relating the hardness of
Model RB to Model B, it is shown that there exist a lot of hard instances in
Model RB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004006</id><created>2000-04-17</created><authors><author><keyname>Ferrucci</keyname><forenames>F.</forenames></author><author><keyname>Pacini</keyname><forenames>G.</forenames></author><author><keyname>Sessa</keyname><forenames>M. I.</forenames></author></authors><title>On Redundancy Elimination Tolerant Scheduling Rules</title><categories>cs.PL</categories><comments>53 pages, to appear on TPLP</comments><acm-class>D.1.6</acm-class><abstract>  In (Ferrucci, Pacini and Sessa, 1995) an extended form of resolution, called
Reduced SLD resolution (RSLD), is introduced. In essence, an RSLD derivation is
an SLD derivation such that redundancy elimination from resolvents is performed
after each rewriting step. It is intuitive that redundancy elimination may have
positive effects on derivation process. However, undesiderable effects are also
possible. In particular, as shown in this paper, program termination as well as
completeness of loop checking mechanisms via a given selection rule may be
lost. The study of such effects has led us to an analysis of selection rule
basic concepts, so that we have found convenient to move the attention from
rules of atom selection to rules of atom scheduling. A priority mechanism for
atom scheduling is built, where a priority is assigned to each atom in a
resolvent, and primary importance is given to the event of arrival of new atoms
from the body of the applied clause at rewriting time. This new computational
model proves able to address the study of redundancy elimination effects,
giving at the same time interesting insights into general properties of
selection rules. As a matter of fact, a class of scheduling rules, namely the
specialisation independent ones, is defined in the paper by using not trivial
semantic arguments. As a quite surprising result, specialisation independent
scheduling rules turn out to coincide with a class of rules which have an
immediate structural characterisation (named stack-queue rules). Then we prove
that such scheduling rules are tolerant to redundancy elimination, in the sense
that neither program termination nor completeness of equality loop check is
lost passing from SLD to RSLD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004007</id><created>2000-04-17</created><authors><author><keyname>Frick</keyname><forenames>Markus</forenames></author><author><keyname>Grohe</keyname><forenames>Martin</forenames></author></authors><title>Deciding first-order properties of locally tree-decomposable structures</title><categories>cs.DS cs.CC cs.DB</categories><acm-class>F.2.2; G.2.2; H.2.4; F.1.3</acm-class><abstract>  We introduce the concept of a class of graphs, or more generally, relational
structures, being locally tree-decomposable. There are numerous examples of
locally tree-decomposable classes, among them the class of planar graphs and
all classes of bounded valence or of bounded tree-width. We also consider a
slightly more general concept of a class of structures having bounded local
tree-width.
  We show that for each property P of structures that is definable in
first-order logic and for each locally tree-decomposable class C of graphs,
there is a linear time algorithm deciding whether a given structure A in C has
property P. For classes C of bounded local tree-width, we show that for every
k\ge 1 there is an algorithm that solves the same problem in time
O(n^{1+(1/k)}) (where n is the cardinality of the input structure).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004008</id><created>2000-04-17</created><authors><author><keyname>Breck</keyname><forenames>Eric</forenames><affiliation>The MITRE Corporation</affiliation></author><author><keyname>Burger</keyname><forenames>John D.</forenames><affiliation>The MITRE Corporation</affiliation></author><author><keyname>Ferro</keyname><forenames>Lisa</forenames><affiliation>The MITRE Corporation</affiliation></author><author><keyname>Hirschman</keyname><forenames>Lynette</forenames><affiliation>The MITRE Corporation</affiliation></author><author><keyname>House</keyname><forenames>David</forenames><affiliation>The MITRE Corporation</affiliation></author><author><keyname>Light</keyname><forenames>Marc</forenames><affiliation>The MITRE Corporation</affiliation></author><author><keyname>Mani</keyname><forenames>Inderjeet</forenames><affiliation>The MITRE Corporation</affiliation></author></authors><title>How to Evaluate your Question Answering System Every Day and Still Get
  Real Work Done</title><categories>cs.CL cs.IR</categories><comments>6 pages, 3 figures, to appear in Proceedings of the Second
  International Conference on Language Resources and Evaluation (LREC 2000)</comments><acm-class>I.2.7; H.3.4</acm-class><abstract>  In this paper, we report on Qaviar, an experimental automated evaluation
system for question answering applications. The goal of our research was to
find an automatically calculated measure that correlates well with human
judges' assessment of answer correctness in the context of question answering
tasks. Qaviar judges the response by computing recall against the stemmed
content words in the human-generated answer key. It counts the answer correct
if it exceeds agiven recall threshold. We determined that the answer
correctness predicted by Qaviar agreed with the human 93% to 95% of the time.
41 question-answering systems were ranked by both Qaviar and human assessors,
and these rankings correlated with a Kendall's Tau measure of 0.920, compared
to a correlation of 0.956 between human assessors on the same data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004009</id><created>2000-04-17</created><updated>2000-05-12</updated><authors><author><keyname>Benson</keyname><forenames>David B.</forenames></author></authors><title>Separating the complexity classes NL and NP</title><categories>cs.CC</categories><comments>Withdrawn</comments><acm-class>F.1.3; F.4.1</acm-class><abstract>  Withdrawn since -order- was overlooked. First order reductions without order
are much too weak to separate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004010</id><created>2000-04-18</created><authors><author><keyname>Weaver</keyname><forenames>Lex</forenames></author></authors><title>Design and Evaluation of Mechanisms for a Multicomputer Object Store</title><categories>cs.DC cs.DB</categories><comments>1994 Honours thesis, 134 pages</comments><acm-class>H.3.4; H.2.4</acm-class><abstract>  Multicomputers have traditionally been viewed as powerful compute engines.
It is from this perspective that they have been applied to various problems in
order to achieve significant performance gains. There are many applications for
which this compute intensive approach is only a partial solution. CAD, virtual
reality, simulation, document management and analysis all require timely access
to large amounts of data. This thesis investigates the use of the object store
paradigm to harness the large distributed memories found on multicomputers. The
design, implementation, and evaluation of a distributed object server on the
Fujitsu AP1000 is described. The performance of the distributed object server
under example applications, mainly physical simulation problems, is used to
evaluate solutions to the problems of client space recovery, object migration,
and coherence maintenance.
  The distributed object server follows the client-server model, allows object
replication, and uses binary semaphores as a concurrency control measure.
Instrumentation of the server under these applications supports several
conclusions: client space recovery should be dynamically controlled by the
application, predictively prefetching object replicas yields benefits in
restricted circumstances, object migration by storage unit (segment) is not
generally suitable where there are many objects per storage unit, and binary
semaphores are an expensive concurrency control measure in this environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004011</id><created>2000-04-19</created><authors><author><keyname>Steinmacher-Burow</keyname><forenames>Burkhard D.</forenames></author></authors><title>Task Frames</title><categories>cs.PL</categories><acm-class>D.3.3;D.3.4</acm-class><abstract>  Forty years ago Dijkstra introduced the current conventional execution of
routines. It places activation frames onto a stack. Each frame is the internal
state of an executing routine. The resulting application execution is not
easily helped by an external system. This presentation proposes an alternative
execution of routines. It places task frames onto the stack. A task frame is
the call of a routine to be executed. The feasibility of the alternative
execution is demonstrated by a crude implementation. As described elsewhere, an
application which executes in terms of tasks can be provided by an external
system with a transparent reliable, distributed, heterogeneous, adaptive,
dynamic, real-time, parallel, secure or other execution. By extending the crude
implementation, this presentation outlines a simple transparent parallel
execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004012</id><created>2000-04-21</created><authors><author><keyname>Etievent</keyname><forenames>Emmanuel</forenames></author><author><keyname>Lebourgeois</keyname><forenames>Frank</forenames></author><author><keyname>Jolion</keyname><forenames>Jean-Michel</forenames></author></authors><title>Assisted Video Sequences Indexing : Motion Analysis Based on Interest
  Points</title><categories>cs.CV</categories><comments>HTML, 8 pages, 6 figures, http://rfv.insa-lyon.fr/~etievent/</comments><report-no>RR9903</report-no><acm-class>I.4.8; I.4.9</acm-class><journal-ref>Iciap 99, Venezia, 27-29 sept., 1059-1062</journal-ref><abstract>  This work deals with content-based video indexing. Our viewpoint is
semi-automatic analysis of compressed video. We consider the possible
applications of motion analysis and moving object detection : assisting moving
object indexing, summarising videos, and allowing image and motion queries. We
propose an approach based on interest points. As first results, we test and
compare the stability of different types of interest point detectors in
compressed sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004013</id><created>2000-04-23</created><authors><author><keyname>Weaver</keyname><forenames>Lex</forenames></author><author><keyname>Lynes</keyname><forenames>Andrew</forenames></author></authors><title>Sorting Integers on the AP1000</title><categories>cs.DC</categories><comments>1994 Project Report, 23 pages</comments><acm-class>C.1.4</acm-class><abstract>  Sorting is one of the classic problems of computer science. Whilst well
understood on sequential machines, the diversity of architectures amongst
parallel systems means that algorithms do not perform uniformly on all
platforms. This document describes the implementation of a radix based
algorithm for sorting positive integers on a Fujitsu AP1000 Supercomputer,
which was constructed as an entry in the Joint Symposium on Parallel Processing
(JSPP) 1994 Parallel Software Contest (PSC94). Brief consideration is also
given to a full radix sort conducted in parallel across the machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004014</id><created>2000-04-25</created><updated>2001-01-11</updated><authors><author><keyname>Baker</keyname><forenames>Mark</forenames><affiliation>University of Portsmouth, UK</affiliation></author></authors><title>Cluster Computing White Paper</title><categories>cs.DC cs.AR cs.NI</categories><comments>119 page white paper - Edited by Mark Baker (University of
  Portsmouth), version 2.0</comments><acm-class>B.4.1 B.4.3 C.0 C.1.2 C.1.4 C.2 C.4 D.1.3 D.1.5 D.2.11 D.3.1 D.4.1
  D.4.6</acm-class><abstract>  Cluster computing is not a new area of computing. It is, however, evident
that there is a growing interest in its usage in all areas where applications
have traditionally used parallel or distributed computing platforms. The
growing interest has been fuelled in part by the availability of powerful
microprocessors and high-speed networks as off-the-shelf commodity components
as well as in part by the rapidly maturing software components available to
support high performance and high availability applications.
  This White Paper has been broken down into eleven sections, each of which has
been put together by academics and industrial researchers who are both experts
in their fields and where willing to volunteer their time and effort to put
together this White Paper. The status of this paper is draft and we are at the
stage of publicizing its presence and making a Request For Comments (RFC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004015</id><created>2000-04-27</created><updated>2001-07-10</updated><authors><author><keyname>Bauer</keyname><forenames>Christian</forenames></author><author><keyname>Frink</keyname><forenames>Alexander</forenames></author><author><keyname>Kreckel</keyname><forenames>Richard</forenames></author></authors><title>Introduction to the GiNaC Framework for Symbolic Computation within the
  C++ Programming Language</title><categories>cs.SC hep-ph physics.comp-ph</categories><report-no>MZ-TH/00-17</report-no><acm-class>I.1.1; I.1.3</acm-class><journal-ref>J. Symbolic Computation (2002) 33, 1-12</journal-ref><abstract>  The traditional split-up into a low level language and a high level language
in the design of computer algebra systems may become obsolete with the advent
of more versatile computer languages. We describe GiNaC, a special-purpose
system that deliberately denies the need for such a distinction. It is entirely
written in C++ and the user can interact with it directly in that language. It
was designed to provide efficient handling of multivariate polynomials,
algebras and special functions that are needed for loop calculations in
theoretical quantum field theory. It also bears some potential to become a more
general purpose symbolic package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0004016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0004016</id><created>2000-04-27</created><authors><author><keyname>Sardinha</keyname><forenames>Tony Berber</forenames><affiliation>Catholic U. of Sao Paulo, Brazil</affiliation></author></authors><title>Looking at discourse in a corpus: The role of lexical cohesion</title><categories>cs.CL</categories><comments>5 pages, Paper presented at AILA 99, Tokyo, Japan</comments><acm-class>I.2.7</acm-class><abstract>  This paper is aimed at reporting on the development and application of a
computer model for discourse analysis through segmentation. Segmentation refers
to the principled division of texts into contiguous constituents. Other studies
have looked at the application of a number of models to the analysis of
discourse by computer. The segmentation procedure developed for the present
investigation is called LSM ('Link Set Median'). It was applied to three corpus
of 300 texts from three different genres. The results obtained by application
of the LSM procedure on the corpus were then compared to segmentation carried
out at random. Statistical analyses suggested that LSM significantly
outperformed random segmentation, thus indicating that the segmentation was
meaningful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005001</id><created>2000-05-03</created><authors><author><keyname>Chen</keyname><forenames>Liang</forenames><affiliation>Utsunomiya University, Japan</affiliation></author><author><keyname>Tokuda</keyname><forenames>Naoyuki</forenames><affiliation>Utsunomiya University, Japan</affiliation></author></authors><title>Robustness of Regional Matching Scheme over Global Matching Scheme</title><categories>cs.CV</categories><comments>16 pages, Latex, 7 EPS figures, using esub2acm.cls and epsf.tex</comments><report-no>UU-TOKUDALAB-00-03</report-no><acm-class>I.2.10; I.5.2; H.1</acm-class><abstract>  The paper has established and verified the theory prevailing widely among
image and pattern recognition specialists that the bottom-up indirect regional
matching process is the more stable and the more robust than the global
matching process against concentrated types of noise represented by clutter,
outlier or occlusion in the imagery. We have demonstrated this by analyzing the
effect of concentrated noise on a typical decision making process of a
simplified two candidate voting model where our theorem establishes the lower
bounds to a critical breakdown point of election (or decision) result by the
bottom-up matching process are greater than the exact bound of the global
matching process implying that the former regional process is capable of
accommodating a higher level of noise than the latter global process before the
result of decision overturns. We present a convincing experimental verification
supporting not only the theory by a white-black flag recognition problem in the
presence of localized noise but also the validity of the conjecture by a facial
recognition problem that the theorem remains valid for other decision making
processes involving an important dimension-reducing transform such as principal
component analysis or a Gabor transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005002</id><created>2000-05-03</created><authors><author><keyname>Heering</keyname><forenames>Jan</forenames></author></authors><title>Application Software, Domain-Specific Languages, and Language Design
  Assistants</title><categories>cs.PL</categories><comments>To be presented at SSGRR 2000, L'Aquila, Italy</comments><report-no>SEN-R0010 (CWI, Amsterdam)</report-no><acm-class>D.3</acm-class><journal-ref>in Proceedings SSGRR 2000 International Conference on Advances in
  Infrastructure for Electronic Business, Science, and Education on the
  Internet</journal-ref><abstract>  While application software does the real work, domain-specific languages
(DSLs) are tools to help produce it efficiently, and language design assistants
in turn are meta-tools to help produce DSLs quickly. DSLs are already in wide
use (HTML for web pages, Excel macros for spreadsheet applications, VHDL for
hardware design, ...), but many more will be needed for both new as well as
existing application domains. Language design assistants to help develop them
currently exist only in the basic form of language development systems. After a
quick look at domain-specific languages, and especially their relationship to
application libraries, we survey existing language development systems and give
an outline of future language design assistants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005003</id><created>2000-05-03</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>CoRR: A Computing Research Repository</title><categories>cs.DL cs.CY</categories><comments>This article is based on ``The Computing Research Repository:
  Promoting the Rapid Dissemination of Computer Science Research'', Joseph Y.
  Halpern and Carl Lagoze (http://xxx.lanl.gov/abs/cs.DL/9812020), but focuses
  on somewhat different issues, more geared to the Computer Documentation
  community. It will appear in the ACM Journal of Computer Documentation</comments><acm-class>H.3.7; K.4.0</acm-class><abstract>  Discusses how CoRR was set up and some policy issues involved with setting up
such a repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005004</id><created>2000-05-03</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>A response to the commentaries on CoRR</title><categories>cs.DL cs.CY</categories><comments>This is a response to the commentaries on &quot;CoRR: A Computing Research
  Repository&quot; (http://xxx.lanl.gov/abs/cs.DL/0005003). The original article,
  the Commentaries,and my response, will appear in the ACM Journal of Computer
  Documentation</comments><acm-class>H.3.7; K.4.0</acm-class><abstract>  This is a response to the commentaries on &quot;CoRR: A Computing Research
Repository&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005005</id><created>2000-05-04</created><authors><author><keyname>King</keyname><forenames>Davis</forenames></author><author><keyname>Rossignac</keyname><forenames>Jarek</forenames></author><author><keyname>Szymczak</keyname><forenames>Andrzej</forenames></author></authors><title>Connectivity Compression for Irregular Quadrilateral Meshes</title><categories>cs.GR cs.CG cs.DS</categories><report-no>GVU Tech Report GIT-GVU-99-36</report-no><acm-class>I.3.5; G2.2; E.4; J.6</acm-class><abstract>  Applications that require Internet access to remote 3D datasets are often
limited by the storage costs of 3D models. Several compression methods are
available to address these limits for objects represented by triangle meshes.
Many CAD and VRML models, however, are represented as quadrilateral meshes or
mixed triangle/quadrilateral meshes, and these models may also require
compression. We present an algorithm for encoding the connectivity of such
quadrilateral meshes, and we demonstrate that by preserving and exploiting the
original quad structure, our approach achieves encodings 30 - 80% smaller than
an approach based on randomly splitting quads into triangles. We present both a
code with a proven worst-case cost of 3 bits per vertex (or 2.75 bits per
vertex for meshes without valence-two vertices) and entropy-coding results for
typical meshes ranging from 0.3 to 0.9 bits per vertex, depending on the
regularity of the mesh. Our method may be implemented by a rule for a
particular splitting of quads into triangles and by using the compression and
decompression algorithms introduced in [Rossignac99] and
[Rossignac&amp;Szymczak99]. We also present extensions to the algorithm to compress
meshes with holes and handles and meshes containing triangles and other
polygons as well as quads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005006</id><created>2000-05-06</created><authors><author><keyname>Pedersen</keyname><forenames>Ted</forenames><affiliation>University of Minnesota Duluth</affiliation></author></authors><title>A Simple Approach to Building Ensembles of Naive Bayesian Classifiers
  for Word Sense Disambiguation</title><categories>cs.CL</categories><comments>7 pages, Latex, uses colnaacl.sty. Appears in Proceedings of NAACL,
  pages 63-69, May 2000, Seattle, WA</comments><acm-class>I.2.7</acm-class><abstract>  This paper presents a corpus-based approach to word sense disambiguation that
builds an ensemble of Naive Bayesian classifiers, each of which is based on
lexical features that represent co--occurring words in varying sized windows of
context. Despite the simplicity of this approach, empirical results
disambiguating the widely studied nouns line and interest show that such an
ensemble achieves accuracy rivaling the best previously published results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005007</id><created>2000-05-07</created><authors><author><keyname>Kling</keyname><forenames>Rob</forenames></author><author><keyname>McKim</keyname><forenames>Geoffrey</forenames></author><author><keyname>Fortuna</keyname><forenames>Joanna</forenames></author><author><keyname>King</keyname><forenames>Adam</forenames></author></authors><title>Scientific Collaboratories as Socio-Technical Interaction Networks: A
  Theoretical Approach</title><categories>cs.CY</categories><acm-class>H.5.3</acm-class><abstract>  Collaboratories refer to laboratories where scientists can work together
while they are in distant locations from each other and from key equipment.
They have captured the interest both of CSCW researchers and of science funders
who wish to optimize the use of rare scientific equipment and expertise. We
examine the kind of CSCW conceptions that help us best understand the character
of working relationships in these scientific collaboratories. Our model,
inspired by actor-network theory, considers technologies as Socio-technical
Interaction Networks (STINs). This model provides a rich understanding of the
scientific collaboratories, and also a more complete understanding of the
conditions and activities that support collaborative work in them. We
illustrate the significance of STIN models with several cases drawn from the
fields of high energy physics and materials science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005008</id><created>2000-05-08</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>A Denotational Semantics for First-Order Logic</title><categories>cs.PL cs.AI</categories><comments>17 pages. Invited talk at the Computational Logic Conference (CL
  2000). To appear in Springer-Verlag Lecture Notes in Computer Science</comments><acm-class>F.3.2; D.3.2</acm-class><abstract>  In Apt and Bezem [AB99] (see cs.LO/9811017) we provided a computational
interpretation of first-order formulas over arbitrary interpretations. Here we
complement this work by introducing a denotational semantics for first-order
logic. Additionally, by allowing an assignment of a non-ground term to a
variable we introduce in this framework logical variables.
  The semantics combines a number of well-known ideas from the areas of
semantics of imperative programming languages and logic programming. In the
resulting computational view conjunction corresponds to sequential composition,
disjunction to ``don't know'' nondeterminism, existential quantification to
declaration of a local variable, and negation to the ``negation as finite
failure'' rule. The soundness result shows correctness of the semantics with
respect to the notion of truth. The proof resembles in some aspects the proof
of the soundness of the SLDNF-resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005009</id><created>2000-05-08</created><authors><author><keyname>Tobies</keyname><forenames>Stephan</forenames></author></authors><title>PSPACE Reasoning for Graded Modal Logics</title><categories>cs.LO cs.AI cs.CC cs.DS</categories><acm-class>F.4.1</acm-class><journal-ref>Journal of Logic and Computation, Vol. 11 No. 1, pp 85-106 2001</journal-ref><abstract>  We present a PSPACE algorithm that decides satisfiability of the graded modal
logic Gr(K_R)---a natural extension of propositional modal logic K_R by
counting expressions---which plays an important role in the area of knowledge
representation. The algorithm employs a tableaux approach and is the first
known algorithm which meets the lower bound for the complexity of the problem.
Thus, we exactly fix the complexity of the problem and refute an
ExpTime-hardness conjecture. We extend the results to the logic Gr(K_(R \cap
I)), which augments Gr(K_R) with inverse relations and intersection of
accessibility relations. This establishes a kind of ``theoretical benchmark''
that all algorithmic approaches can be measured against.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005010</id><created>2000-05-08</created><authors><author><keyname>Simons</keyname><forenames>Patrik</forenames></author></authors><title>Extending and Implementing the Stable Model Semantics</title><categories>cs.LO cs.AI</categories><comments>109 pages, 30 figures, dissertation for the degree of Doctor of
  Technology</comments><report-no>HUT-TCS-A58</report-no><acm-class>I.2.3; I.2.8; F.4.1</acm-class><abstract>  An algorithm for computing the stable model semantics of logic programs is
developed. It is shown that one can extend the semantics and the algorithm to
handle new and more expressive types of rules. Emphasis is placed on the use of
efficient implementation techniques. In particular, an implementation of
lookahead that safely avoids testing every literal for failure and that makes
the use of lookahead feasible is presented. In addition, a good heuristic is
derived from the principle that the search space should be minimized.
  Due to the lack of competitive algorithms and implementations for the
computation of stable models, the system is compared with three satisfiability
solvers. This shows that the heuristic can be improved by breaking ties, but
leaves open the question of how to break them. It also demonstrates that the
more expressive rules of the stable model semantics make the semantics clearly
preferable over propositional logic when a problem has a more compact logic
program representation. Conjunctive normal form representations are never more
compact than logic program ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005011</id><created>2000-05-08</created><authors><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author></authors><title>An Average Analysis of Backtracking on Random Constraint Satisfaction
  Problems</title><categories>cs.CC cs.AI</categories><comments>20 pages, submitted to Annals of Mathematics and Artificial
  Intelligence</comments><acm-class>F.2.2; I.2.8</acm-class><journal-ref>Annals of Mathematics and Artificial Intelligence, 33:21-37, 2001.</journal-ref><abstract>  In this paper we propose a random CSP model, called Model GB, which is a
natural generalization of standard Model B. It is proved that Model GB in which
each constraint is easy to satisfy exhibits non-trivial behaviour (not
trivially satisfiable or unsatisfiable) as the number of variables approaches
infinity. A detailed analysis to obtain an asymptotic estimate (good to 1+o(1))
of the average number of nodes in a search tree used by the backtracking
algorithm on Model GB is also presented. It is shown that the average number of
nodes required for finding all solutions or proving that no solution exists
grows exponentially with the number of variables. So this model might be an
interesting distribution for studying the nature of hard instances and
evaluating the performance of CSP algorithms. In addition, we further
investigate the behaviour of the average number of nodes as r (the ratio of
constraints to variables) varies. The results indicate that as r increases,
random CSP instances get easier and easier to solve, and the base for the
average number of nodes that is exponential in r tends to 1 as r approaches
infinity. Therefore, although the average number of nodes used by the
backtracking algorithm on random CSP is exponential, many CSP instances will be
very easy to solve when r is sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005012</id><created>2000-05-09</created><authors><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author><author><keyname>Tobies</keyname><forenames>Stephan</forenames></author></authors><title>Reasoning with Axioms: Theory and Pratice</title><categories>cs.LO cs.AI</categories><comments>This paper appeard in the Proceedings of KR'2000</comments><acm-class>F.4.1, I.2.3, I.2.4</acm-class><abstract>  When reasoning in description, modal or temporal logics it is often useful to
consider axioms representing universal truths in the domain of discourse.
Reasoning with respect to an arbitrary set of axioms is hard, even for
relatively inexpressive logics, and it is essential to deal with such axioms in
an efficient manner if implemented systems are to be effective in real
applications. This is particularly relevant to Description Logics, where
subsumption reasoning with respect to a terminology is a fundamental problem.
Two optimisation techniques that have proved to be particularly effective in
dealing with terminologies are lazy unfolding and absorption. In this paper we
seek to improve our theoretical understanding of these important techniques. We
define a formal framework that allows the techniques to be precisely described,
establish conditions under which they can be safely applied, and prove that,
provided these conditions are respected, subsumption testing algorithms will
still function correctly. These results are used to show that the procedures
used in the FaCT system are correct and, moreover, to show how efficiency can
be significantly improved, while still retaining the guarantee of correctness,
by relaxing the safety conditions for absorption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005013</id><created>2000-05-09</created><authors><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author><author><keyname>Sattler</keyname><forenames>Ulrike</forenames></author><author><keyname>Tobies</keyname><forenames>Stephan</forenames></author></authors><title>Practical Reasoning for Very Expressive Description Logics</title><categories>cs.LO cs.AI</categories><acm-class>F.4.1, I.2.3, I.2.4</acm-class><journal-ref>Logic Journal of the IGPL 8(3):239-264, May 2000</journal-ref><abstract>  Description Logics (DLs) are a family of knowledge representation formalisms
mainly characterised by constructors to build complex concepts and roles from
atomic ones. Expressive role constructors are important in many applications,
but can be computationally problematical. We present an algorithm that decides
satisfiability of the DL ALC extended with transitive and inverse roles and
functional restrictions with respect to general concept inclusion axioms and
role hierarchies; early experiments indicate that this algorithm is well-suited
for implementation. Additionally, we show that ALC extended with just
transitive and inverse roles is still in PSPACE. We investigate the limits of
decidability for this family of DLs, showing that relaxing the constraints
placed on the kinds of roles used in number restrictions leads to the
undecidability of all inference problems. Finally, we describe a number of
optimisation techniques that are crucial in obtaining implementations of the
decision procedures, which, despite the worst-case complexity of the problem,
exhibit good performance with real-life problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005014</id><created>2000-05-10</created><authors><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author><author><keyname>Sattler</keyname><forenames>Ulrike</forenames></author><author><keyname>Tobies</keyname><forenames>Stephan</forenames></author></authors><title>Practical Reasoning for Expressive Description Logics</title><categories>cs.LO cs.AI</categories><comments>This paper appeared in the Proceedings of LPAR'99</comments><acm-class>F.4.1, I.2.3, I.2.4</acm-class><abstract>  Description Logics (DLs) are a family of knowledge representation formalisms
mainly characterised by constructors to build complex concepts and roles from
atomic ones. Expressive role constructors are important in many applications,
but can be computationally problematical. We present an algorithm that decides
satisfiability of the DL ALC extended with transitive and inverse roles, role
hierarchies, and qualifying number restrictions. Early experiments indicate
that this algorithm is well-suited for implementation. Additionally, we show
that ALC extended with just transitive and inverse roles is still in PSPACE.
Finally, we investigate the limits of decidability for this family of DLs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005015</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005015</id><created>2000-05-10</created><authors><author><keyname>Sang</keyname><forenames>Erik F. Tjong Kim</forenames></author></authors><title>Noun Phrase Recognition by System Combination</title><categories>cs.CL</categories><comments>6 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of NAACL 2000, Seattle, WA, USA</journal-ref><abstract>  The performance of machine learning algorithms can be improved by combining
the output of different systems. In this paper we apply this idea to the
recognition of noun phrases.We generate different classifiers by using
different representations of the data. By combining the results with voting
techniques described in (Van Halteren et.al. 1998) we manage to improve the
best reported performances on standard data sets for base noun phrases and
arbitrary noun phrases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005016</id><created>2000-05-10</created><authors><author><keyname>Broeker</keyname><forenames>Norbert</forenames></author></authors><title>Improving Testsuites via Instrumentation</title><categories>cs.CL</categories><comments>6 pages, LaTeX2e</comments><acm-class>D.2.5</acm-class><journal-ref>Proc. ANLP--NAACL, Seattle/WA, Apr29--May4 2000, pp.325-330</journal-ref><abstract>  This paper explores the usefulness of a technique from software engineering,
namely code instrumentation, for the development of large-scale natural
language grammars. Information about the usage of grammar rules in test
sentences is used to detect untested rules, redundant test sentences, and
likely causes of overgeneration. Results show that less than half of a
large-coverage grammar for German is actually tested by two large testsuites,
and that 10-30% of testing time is redundant. The methodology applied can be
seen as a re-use of grammar writing knowledge for testsuite compilation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005017</id><created>2000-05-11</created><authors><author><keyname>Horrock</keyname><forenames>Ian</forenames></author><author><keyname>Sattler</keyname><forenames>Ulrike</forenames></author><author><keyname>Tobies</keyname><forenames>Stephan</forenames></author></authors><title>Reasoning with Individuals for the Description Logic SHIQ</title><categories>cs.LO cs.AI</categories><comments>To appear at CADE-17</comments><acm-class>F.4.1, I.2.3, I.2.4</acm-class><abstract>  While there has been a great deal of work on the development of reasoning
algorithms for expressive description logics, in most cases only Tbox reasoning
is considered. In this paper we present an algorithm for combined Tbox and Abox
reasoning in the SHIQ description logic. This algorithm is of particular
interest as it can be used to decide the problem of (database) conjunctive
query containment w.r.t. a schema. Moreover, the realisation of an efficient
implementation should be relatively straightforward as it can be based on an
existing highly optimised implementation of the Tbox algorithm in the FaCT
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005018</id><created>2000-05-11</created><updated>2001-07-30</updated><authors><author><keyname>Bossi</keyname><forenames>Annalisa</forenames></author><author><keyname>Cocco</keyname><forenames>Nicoletta</forenames></author><author><keyname>Etalle</keyname><forenames>Sandro</forenames></author><author><keyname>Rossi</keyname><forenames>Sabina</forenames></author></authors><title>On Modular Termination Proofs of General Logic Programs</title><categories>cs.LO cs.PL</categories><comments>29 pages. To appear in Theory and Practice of Logic Programming</comments><report-no>University of Venice Technical Report CS2000-8</report-no><acm-class>D.2; D.3; F.3.1; F.3.2</acm-class><abstract>  We propose a modular method for proving termination of general logic programs
(i.e., logic programs with negation). It is based on the notion of acceptable
programs, but it allows us to prove termination in a truly modular way. We
consider programs consisting of a hierarchy of modules and supply a general
result for proving termination by dealing with each module separately. For
programs which are in a certain sense well-behaved, namely well-moded or
well-typed programs, we derive both a simple verification technique and an
iterative proof method. Some examples show how our system allows for greatly
simplified proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005019</id><created>2000-05-12</created><authors><author><keyname>Aliod</keyname><forenames>Diego Moll'a</forenames></author><author><keyname>Hess</keyname><forenames>Michael</forenames></author></authors><title>On the Scalability of the Answer Extraction System &quot;ExtrAns&quot;</title><categories>cs.CL</categories><comments>5 pages</comments><acm-class>H.3.1; I.2.3; I.2.7</acm-class><journal-ref>Applications of Natural Language to Information Systems (NLDB'99).
  Klagenfurt, Austria, 1999, 219-224</journal-ref><abstract>  This paper reports on the scalability of the answer extraction system
ExtrAns. An answer extraction system locates the exact phrases in the documents
that contain the explicit answers to the user queries. Answer extraction
systems are therefore more convenient than document retrieval systems in
situations where the user wants to find specific information in limited time.
  ExtrAns performs answer extraction over UNIX manpages. It has been
constructed by combining available linguistic resources and implementing only a
few modules from scratch. A resolution procedure between the minimal logical
form of the user query and the minimal logical forms of the manpage sentences
finds the answers to the queries. These answers are displayed to the user,
together with pointers to the respective manpages, and the exact phrases that
contribute to the answer are highlighted.
  This paper shows that the increase in response times is not a big issue when
scaling the system up from 30 to 500 documents, and that the response times for
500 documents are still acceptable for a real-time answer extraction system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005020</id><created>2000-05-12</created><authors><author><keyname>Radev</keyname><forenames>Dragomir R.</forenames><affiliation>University of Michigan</affiliation></author><author><keyname>Jing</keyname><forenames>Hongyan</forenames><affiliation>Columbia University</affiliation></author><author><keyname>Budzikowska</keyname><forenames>Malgorzata</forenames><affiliation>IBM TJ Watson Research Center</affiliation></author></authors><title>Centroid-based summarization of multiple documents: sentence extraction,
  utility-based evaluation, and user studies</title><categories>cs.CL cs.AI cs.DL cs.HC cs.IR</categories><comments>10 pages Corpus availability at http://perun.si.umich.edu/~radev/mds</comments><acm-class>H.3.1; H.3.4; H.3.7; H.5.2; I.2.7</acm-class><journal-ref>NAACL/ANLP Workshop on Automatic Summarization, Seattle, WA, April
  30, 2000</journal-ref><abstract>  We present a multi-document summarizer, called MEAD, which generates
summaries using cluster centroids produced by a topic detection and tracking
system. We also describe two new techniques, based on sentence utility and
subsumption, which we have applied to the evaluation of both single and
multiple document summaries. Finally, we describe two user studies that test
our models of multi-document summarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005021</id><created>2000-05-14</created><authors><author><keyname>Guergachi</keyname><forenames>A.</forenames></author></authors><title>Modeling the Uncertainty in Complex Engineering Systems</title><categories>cs.AI cs.LG</categories><comments>24 pages using ACM style file</comments><acm-class>I.2.6;I.6.4;J.2;G.3</acm-class><abstract>  Existing procedures for model validation have been deemed inadequate for many
engineering systems. The reason of this inadequacy is due to the high degree of
complexity of the mechanisms that govern these systems. It is proposed in this
paper to shift the attention from modeling the engineering system itself to
modeling the uncertainty that underlies its behavior. A mathematical framework
for modeling the uncertainty in complex engineering systems is developed. This
framework uses the results of computational learning theory. It is based on the
premise that a system model is a learning machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005022</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005022</id><created>2000-05-17</created><updated>2000-06-12</updated><authors><author><keyname>Rocchesso</keyname><forenames>Davide</forenames></author></authors><title>Fractionally-addressed delay lines</title><categories>cs.SD</categories><comments>11 pages, 19 figures, to be published in IEEE Transactions on Speech
  and Audio Processing Corrected ACM-class</comments><acm-class>H.5.5</acm-class><journal-ref>IEEE Transactions on Speech and Audio Processing, vol. 8, no. 6,
  november 2000, pp. 717-727</journal-ref><abstract>  While traditional implementations of variable-length digital delay lines are
based on a circular buffer accessed by two pointers, we propose an
implementation where a single fractional pointer is used both for read and
write operations. On modern general-purpose architectures, the proposed method
is nearly as efficient as the popularinterpolated circular buffer, and it
behaves well for delay-length modulations commonly found in digital audio
effects. The physical interpretation of the new implementation shows that it is
suitable for simulating tension or density modulations in wave-propagating
media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005023</id><created>2000-05-19</created><authors><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Panizzi</keyname><forenames>Emanuele</forenames></author><author><keyname>Proietti</keyname><forenames>Benedetto</forenames></author></authors><title>C++ programming language for an abstract massively parallel SIMD
  architecture</title><categories>cs.PL</categories><comments>10 pages</comments><acm-class>D.3.3; D.2.1</acm-class><abstract>  The aim of this work is to define and implement an extended C++ language to
support the SIMD programming paradigm. The C++ programming language has been
extended to express all the potentiality of an abstract SIMD machine consisting
of a central Control Processor and a N-dimensional toroidal array of Numeric
Processors. Very few extensions have been added to the standard C++ with the
goal of minimising the effort for the programmer in learning a new language and
to keep very high the performance of the compiled code. The proposed language
has been implemented as a porting of the GNU C++ Compiler on a SIMD
supercomputer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005024</id><created>2000-05-22</created><updated>2000-05-22</updated><authors><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author></authors><title>The SAT Phase Transition</title><categories>cs.AI cs.CC</categories><comments>13 pages, 3 figures</comments><acm-class>F.2.m; I.2.8</acm-class><journal-ref>The SAT Phase Transition. Science in China, Series E,
  42(5):494-501, 1999</journal-ref><abstract>  Phase transition is an important feature of SAT problem. For random k-SAT
model, it is proved that as r (ratio of clauses to variables) increases, the
structure of solutions will undergo a sudden change like satisfiability phase
transition when r reaches a threshold point. This phenomenon shows that the
satisfying truth assignments suddenly shift from being relatively different
from each other to being very similar to each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005025</id><created>2000-05-22</created><authors><author><keyname>Walther</keyname><forenames>Markus</forenames><affiliation>University of Marburg</affiliation></author></authors><title>Finite-State Reduplication in One-Level Prosodic Morphology</title><categories>cs.CL</categories><comments>7 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proc. NAACL-2000, Seattle/WA, pp.296-302</journal-ref><abstract>  Reduplication, a central instance of prosodic morphology, is particularly
challenging for state-of-the-art computational morphology, since it involves
copying of some part of a phonological string. In this paper I advocate a
finite-state method that combines enriched lexical representations via
intersection to implement the copying. The proposal includes a
resource-conscious variant of automata and can benefit from the existence of
lazy algorithms. Finally, the implementation of a complex case from Koasati is
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005026</id><created>2000-05-24</created><authors><author><keyname>Sobrado</keyname><forenames>Igor</forenames><affiliation>University of Oviedo</affiliation></author></authors><title>A One-Time Pad based Cipher for Data Protection in Distributed
  Environments</title><categories>cs.CR cs.DC cs.IR cs.NI</categories><comments>19 pages, 8 PostScript figures (attached), uses ACM/LaTeX macros
  esub2acm.{bst|cls} and pifont package</comments><report-no>FFUOV-00/03</report-no><acm-class>C.2.1; C.2.4; H.3; H.3.4</acm-class><abstract>  A one-time pad (OTP) based cipher to insure both data protection and
integrity when mobile code arrives to a remote host is presented. Data
protection is required when a mobile agent could retrieve confidential
information that would be encrypted in untrusted nodes of the network; in this
case, information management could not rely on carrying an encryption key. Data
integrity is a prerequisite because mobile code must be protected against
malicious hosts that, by counterfeiting or removing collected data, could cover
information to the server that has sent the agent. The algorithm described in
this article seems to be simple enough, so as to be easily implemented. This
scheme is based on a non-interactive protocol and allows a remote host to
change its own data on-the-fly and, at the same time, protecting information
against handling by other hosts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005027</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005027</id><created>2000-05-26</created><authors><author><keyname>Wolf</keyname><forenames>David R.</forenames></author></authors><title>A Bayesian Reflection on Surfaces</title><categories>cs.CV cs.DS cs.LG math.PR nlin.AO physics.data-an</categories><comments>34 pages, 1 figure, abbreviated versions presented: Bayesian
  Statistics, Valencia, Spain, 1998; Maximum Entropy and Bayesian Methods,
  Garching, Germany, 1998</comments><acm-class>G.3;I.2.4;I.2.6;I.2.10;I.4.1;I.4.4;I.4.5;I.4.10</acm-class><journal-ref>Entropy, Vol.1, Issue 4, 69-98, 1999. http://www.mdpi.org/entropy/</journal-ref><doi>10.3390/e1040069</doi><abstract>  The topic of this paper is a novel Bayesian continuous-basis field
representation and inference framework. Within this paper several problems are
solved: The maximally informative inference of continuous-basis fields, that is
where the basis for the field is itself a continuous object and not
representable in a finite manner; the tradeoff between accuracy of
representation in terms of information learned, and memory or storage capacity
in bits; the approximation of probability distributions so that a maximal
amount of information about the object being inferred is preserved; an
information theoretic justification for multigrid methodology. The maximally
informative field inference framework is described in full generality and
denoted the Generalized Kalman Filter. The Generalized Kalman Filter allows the
update of field knowledge from previous knowledge at any scale, and new data,
to new knowledge at any other scale. An application example instance, the
inference of continuous surfaces from measurements (for example, camera image
data), is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005028</id><created>2000-05-26</created><authors><author><keyname>Skraparlis</keyname><forenames>Dimitrios</forenames></author></authors><title>A method for command identification, using modified collision free
  hashing with addition &amp; rotation iterative hash functions (part 1)</title><categories>cs.HC cs.IR</categories><comments>28 pages, includes code</comments><acm-class>B.4.2; H.5.2</acm-class><abstract>  This paper proposes a method for identification of a user`s fixed string set
(which can be a command/instruction set for a terminal or microprocessor). This
method is fast and has very small memory requirements, compared to a
traditional full string storage and compare method. The user feeds characters
into a microcontroller via a keyboard or another microprocessor sends commands
and the microcontroller hashes the input in order to identify valid commands,
ensuring no collisions between hashed valid strings, while applying further
criteria to narrow collision between random and valid strings. The method
proposed narrows the possibility of the latter kind of collision, achieving
small code and memory-size utilization and very fast execution. Hashing is
achieved using additive &amp; rotating hash functions in an iterative form, which
can be very easily implemented in simple microcontrollers and microprocessors.
Such hash functions are presented and compared according to their efficiency
for a given string/command set, using the program found in the appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005029</id><created>2000-05-30</created><authors><author><keyname>Radev</keyname><forenames>Dragomir R.</forenames><affiliation>University of Michigan</affiliation></author><author><keyname>Prager</keyname><forenames>John</forenames><affiliation>IBM TJ Watson Research Center</affiliation></author><author><keyname>Samn</keyname><forenames>Valerie</forenames><affiliation>Teachers College, Columbia University</affiliation></author></authors><title>Ranking suspected answers to natural language questions using predictive
  annotation</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>H.3.1;H.3.3;H.3.4;H.5.2;I.2.7</acm-class><journal-ref>ANLP'00, Seattle, WA, May 2000</journal-ref><abstract>  In this paper, we describe a system to rank suspected answers to natural
language questions. We process both corpus and query using a new technique,
predictive annotation, which augments phrases in texts with labels anticipating
their being targets of certain kinds of questions. Given a natural language
question, an IR system returns a set of matching passages, which are then
analyzed and ranked according to various criteria described in this paper. We
provide an evaluation of the techniques based on results from the TREC Q&amp;A
evaluation in which our system participated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005030</id><created>2000-05-30</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Axiomatizing Causal Reasoning</title><categories>cs.AI cs.LO</categories><comments>An earlier version of this paper appeared in UAI '98</comments><acm-class>I.2.4; F.4.1</acm-class><journal-ref>Journal of AI Research, Vol. 12, 2000, pp. 317--337</journal-ref><abstract>  Causal models defined in terms of a collection of equations, as defined by
Pearl, are axiomatized here. Axiomatizations are provided for three
successively more general classes of causal models: (1) the class of recursive
theories (those without feedback), (2) the class of theories where the
solutions to the equations are unique, (3) arbitrary theories (where the
equations may not have solutions and, if they do, they are not necessarily
unique). It is shown that to reason about causality in the most general third
class, we must extend the language used by Galles and Pearl. In addition, the
complexity of the decision procedures is characterized for all the languages
and classes of models considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005031</identifier>
 <datestamp>2011-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005031</id><created>2000-05-30</created><updated>2011-06-15</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Conditional Plausibility Measures and Bayesian Networks</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><journal-ref>Journal Of Artificial Intelligence Research, Volume 14, pages
  359-389, 2001</journal-ref><doi>10.1613/jair.817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general notion of algebraic conditional plausibility measures is defined.
Probability measures, ranking functions, possibility measures, and (under the
appropriate definitions) sets of probability measures can all be viewed as
defining algebraic conditional plausibility measures. It is shown that
algebraic conditional plausibility measures can be represented using Bayesian
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005032</id><created>2000-05-31</created><authors><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author></authors><title>Computational Complexity and Phase Transitions</title><categories>cs.CC cs.DS</categories><comments>A (slightly) revised version of the paper submitted to the 15th IEEE
  Conference on Computational Complexity</comments><acm-class>F.2.2</acm-class><abstract>  Phase transitions in combinatorial problems have recently been shown to be
useful in locating &quot;hard&quot; instances of combinatorial problems. The connection
between computational complexity and the existence of phase transitions has
been addressed in Statistical Mechanics and Artificial Intelligence, but not
studied rigorously.
  We take a step in this direction by investigating the existence of sharp
thresholds for the class of generalized satisfiability problems defined by
Schaefer. In the case when all constraints are clauses we give a complete
characterization of such problems that have a sharp threshold.
  While NP-completeness does not imply (even in this restricted case) the
existence of a sharp threshold, it &quot;almost implies&quot; this, since clausal
generalized satisfiability problems that lack a sharp threshold are either
  1. polynomial time solvable, or
  2. predicted, with success probability lower bounded by some positive
constant by across all the probability range, by a single, trivial procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0005033</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0005033</id><created>2000-05-31</created><authors><author><keyname>Panizzi</keyname><forenames>Emanuele</forenames></author><author><keyname>Pastorelli</keyname><forenames>Bernardo</forenames></author></authors><title>Multimethods and separate static typechecking in a language with
  C++-like object model</title><categories>cs.PL</categories><comments>15 pages, 18 figures</comments><report-no>UAQ DIE R.99-33</report-no><acm-class>D.1.5; D.3.3; D.3.4</acm-class><abstract>  The goal of this paper is the description and analysis of multimethod
implementation in a new object-oriented, class-based programming language
called OOLANG. The implementation of the multimethod typecheck and selection,
deeply analyzed in the paper, is performed in two phases in order to allow
static typechecking and separate compilation of modules. The first phase is
performed at compile time, while the second is executed at link time and does
not require the modules' source code. OOLANG has syntax similar to C++; the
main differences are the absence of pointers and the realization of
polymorphism through subsumption. It adopts the C++ object model and supports
multiple inheritance as well as virtual base classes. For this reason, it has
been necessary to define techniques for realigning argument and return value
addresses when performing multimethod invocations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006001</id><created>2000-05-31</created><authors><author><keyname>Philip</keyname><forenames>Ninan Sajeeth</forenames></author><author><keyname>Joseph</keyname><forenames>K. Babu</forenames></author></authors><title>Boosting the Differences: A fast Bayesian classifier neural network</title><categories>cs.CV</categories><comments>latex 18pages no figures</comments><report-no>IDA2000</report-no><acm-class>I1.2;F.1.1;F1.2;C1.3</acm-class><abstract>  A Bayesian classifier that up-weights the differences in the attribute values
is discussed. Using four popular datasets from the UCI repository, some
interesting features of the network are illustrated. The network is suitable
for classification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006002</id><created>2000-05-31</created><authors><author><keyname>Philip</keyname><forenames>Ninan Sajeeth</forenames></author><author><keyname>Joseph</keyname><forenames>K. Babu</forenames></author></authors><title>Distorted English Alphabet Identification : An application of Difference
  Boosting Algorithm</title><categories>cs.CV</categories><comments>latex 14pages no figures</comments><report-no>ADCOM2000</report-no><acm-class>I1.2;F.1.1;F1.2;C1.3</acm-class><abstract>  The difference-boosting algorithm is used on letters dataset from the UCI
repository to classify distorted raster images of English alphabets. In
contrast to rather complex networks, the difference-boosting is found to
produce comparable or better classification efficiency on this complex problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006003</id><created>2000-06-01</created><authors><author><keyname>Henderson</keyname><forenames>John C.</forenames></author><author><keyname>Brill</keyname><forenames>Eric</forenames></author></authors><title>Exploiting Diversity in Natural Language Processing: Combining Parsers</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the Fourth Conference on Empirical Methods in
  Natural Language Processing (EMNLP-99), pages 187-194. College Park,
  Maryland, USA. June, 1999</journal-ref><abstract>  Three state-of-the-art statistical parsers are combined to produce more
accurate parses, as well as new bounds on achievable Treebank parsing accuracy.
Two general approaches are presented and two combination techniques are
described for each approach. Both parametric and non-parametric models are
explored. The resulting parsers surpass the best previously published
performance results for the Penn Treebank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006004</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006004</id><created>2000-06-01</created><authors><author><keyname>Mondal</keyname><forenames>S. A.</forenames></author></authors><title>A Note on &quot;Optimal Static Load Balancing in Distributed Computer
  Systems&quot;</title><categories>cs.DC</categories><comments>18 pages</comments><acm-class>C.4 Performance of Systems: modeling techniques; D.4.8 Perfomance:
  modeling and prediction</acm-class><abstract>  The problem of minimizing mean response time of generic jobs submitted to a
heterogenous distributed computer systems is considered in this paper. A static
load balancing strategy, in which decision of redistribution of loads does not
depend on the state of the system, is used for this purpose. The article is
closely related to a previous article on the same topic. The present article
points out number of inconsistencies in the previous article, provides a new
formulation, and discusses the impact of new findings, based on the improved
formulation, on the results of the previous article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006005</id><created>2000-06-02</created><authors><author><keyname>Marsland</keyname><forenames>Stephen</forenames></author><author><keyname>Nehmzow</keyname><forenames>Ulrich</forenames></author><author><keyname>Shapiro</keyname><forenames>Jonathan</forenames></author></authors><title>Novelty Detection for Robot Neotaxis</title><categories>cs.RO cs.NE nlin.AO</categories><comments>7 pages, 5 figures. In Proceedings of the Second International
  Conference on Neural Computation, 2000</comments><acm-class>I.2.6</acm-class><abstract>  The ability of a robot to detect and respond to changes in its environment is
potentially very useful, as it draws attention to new and potentially important
features. We describe an algorithm for learning to filter out previously
experienced stimuli to allow further concentration on novel features. The
algorithm uses a model of habituation, a biological process which causes a
decrement in response with repeated presentation. Experiments with a mobile
robot are presented in which the robot detects the most novel stimulus and
turns towards it (`neotaxis').
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006006</id><created>2000-06-02</created><authors><author><keyname>Marsland</keyname><forenames>Stephen</forenames></author><author><keyname>Nehmzow</keyname><forenames>Ulrich</forenames></author><author><keyname>Shapiro</keyname><forenames>Jonathan</forenames></author></authors><title>A Real-Time Novelty Detector for a Mobile Robot</title><categories>cs.RO cs.NE</categories><comments>8 pages, 6 figures. In Proceedings of EUREL European Advanced
  Robotics Systems Masterclass and Conference, 2000</comments><acm-class>I.2.6</acm-class><abstract>  Recognising new or unusual features of an environment is an ability which is
potentially very useful to a robot. This paper demonstrates an algorithm which
achieves this task by learning an internal representation of `normality' from
sonar scans taken as a robot explores the environment. This model of the
environment is used to evaluate the novelty of each sonar scan presented to it
with relation to the model. Stimuli which have not been seen before, and
therefore have more novelty, are highlighted by the filter. The filter has the
ability to forget about features which have been learned, so that stimuli which
are seen only rarely recover their response over time. A number of robot
experiments are presented which demonstrate the operation of the filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006007</id><created>2000-06-02</created><authors><author><keyname>Marsland</keyname><forenames>Stephen</forenames></author><author><keyname>Nehmzow</keyname><forenames>Ulrich</forenames></author><author><keyname>Shapiro</keyname><forenames>Jonathan</forenames></author></authors><title>Novelty Detection on a Mobile Robot Using Habituation</title><categories>cs.RO cs.NE nlin.AO</categories><comments>10 pages, 6 figures. In From Animals to Animats, The Sixth
  International Conference on Simulation of Adaptive Behaviour, Paris, 2000</comments><acm-class>I.2.6</acm-class><abstract>  In this paper a novelty filter is introduced which allows a robot operating
in an un structured environment to produce a self-organised model of its
surroundings and to detect deviations from the learned model. The environment
is perceived using the rob ot's 16 sonar sensors. The algorithm produces a
novelty measure for each sensor scan relative to the model it has learned. This
means that it highlights stimuli which h ave not been previously experienced.
The novelty filter proposed uses a model of hab ituation. Habituation is a
decrement in behavioural response when a stimulus is pre sented repeatedly.
Robot experiments are presented which demonstrate the reliable o peration of
the filter in a number of environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006008</id><created>2000-06-02</created><authors><author><keyname>Dwork</keyname><forenames>Cynthia</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Waarts</keyname><forenames>O.</forenames></author></authors><title>Performing work efficiently in the presence of faults</title><categories>cs.DC</categories><acm-class>C.2.4</acm-class><journal-ref>SIAM Journal on Computing 27:5, 1998, pp. 1457--1491</journal-ref><abstract>  We consider a system of t synchronous processes that communicate only by
sending messages to one another, and that together must perform $n$ independent
units of work. Processes may fail by crashing; we want to guarantee that in
every execution of the protocol in which at least one process survives, all n
units of work will be performed. We consider three parameters: the number of
messages sent, the total number of units of work performed (including
multiplicities), and time. We present three protocols for solving the problem.
All three are work-optimal, doing O(n+t) work. The first has moderate costs in
the remaining two parameters, sending O(t\sqrt{t}) messages, and taking O(n+t)
time. This protocol can be easily modified to run in any completely
asynchronous system equipped with a failure detection mechanism. The second
sends only O(t log{t}) messages, but its running time is large (exponential in
n and t). The third is essentially time-optimal in the (usual) case in which
there are no failures, and its time complexity degrades gracefully as the
number of failures increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006009</id><created>2000-06-02</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>Knowledge and common knowledge in a distributed environment</title><categories>cs.DC cs.AI</categories><comments>This paper is copyrighted by ACM and appears in the ACM Digital
  Library</comments><acm-class>C.2.2, C.2.4, D.2.4, I.2.4, F.3.1, F.3.1</acm-class><journal-ref>Journal of the ACM 37:3, 1990, pp. 549--587</journal-ref><abstract>  Reasoning about knowledge seems to play a fundamental role in distributed
systems. Indeed, such reasoning is a central part of the informal intuitive
arguments used in the design of distributed protocols. Communication in a
distributed system can be viewed as the act of transforming the system's state
of knowledge. This paper presents a general framework for formalizing and
reasoning about knowledge in distributed systems. We argue that states of
knowledge of groups of processors are useful concepts for the design and
analysis of distributed protocols. In particular, distributed knowledge
corresponds to knowledge that is ``distributed'' among the members of the
group, while common knowledge corresponds to a fact being ``publicly known''.
The relationship between common knowledge and a variety of desirable actions in
a distributed system is illustrated. Furthermore, it is shown that, formally
speaking, in practical systems common knowledge cannot be attained. A number of
weaker variants of common knowledge that are attainable in many cases of
interest are introduced and investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006010</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006010</id><created>2000-06-05</created><authors><author><keyname>Asperti</keyname><forenames>Andrea</forenames></author><author><keyname>Roversi</keyname><forenames>Luca</forenames></author></authors><title>Light Affine Logic (Proof Nets, Programming Notation, P-Time Correctness
  and Completeness)</title><categories>cs.LO</categories><report-no>RT-54-2000</report-no><acm-class>F.4.1; F.1.1; I.2.3</acm-class><abstract>  This paper is a structured introduction to Light Affine Logic, and to its
intuitionistic fragment. Light Affine Logic has a polynomially costing cut
elimination (P-Time correctness), and encodes all P-Time Turing machines
(P-Time completeness). P-Time correctness is proved by introducing the Proof
nets for Intuitionistic Light Affine Logic. P-Time completeness is demonstrated
in full details thanks to a very compact program notation. On one side, the
proof of P-Time correctness describes how the complexity of cut elimination is
controlled, thanks to a suitable cut elimination strategy that exploits
structural properties of the Proof nets. This allows to have a good catch on
the meaning of the ``paragraph'' modality, which is a peculiarity of light
logics. On the other side, the proof of P-Time completeness, together with a
lot of programming examples, gives a flavor of the non trivial task of
programming with resource limitations, using Intuitionistic Light Affine Logic
derivations as programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006011</id><created>2000-06-05</created><authors><author><keyname>Henderson</keyname><forenames>John C.</forenames></author><author><keyname>Brill</keyname><forenames>Eric</forenames></author></authors><title>Bagging and Boosting a Treebank Parser</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 1st Meeting of the North American Chapter of
  the Association for Computational Linguistics (NAACL-2000), pages 34-41</journal-ref><abstract>  Bagging and boosting, two effective machine learning techniques, are applied
to natural language parsing. Experiments using these techniques with a
trainable statistical parser are described. The best resulting system provides
roughly as large of a gain in F-measure as doubling the corpus size. Error
analysis of the result of the boosting technique reveals some inconsistent
annotations in the Penn Treebank, suggesting a semi-automatic method for
finding inconsistent treebank annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006012</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006012</id><created>2000-06-05</created><authors><author><keyname>Henderson</keyname><forenames>John C.</forenames></author></authors><title>Exploiting Diversity for Natural Language Parsing</title><categories>cs.CL</categories><comments>Ph.D. Thesis, Johns Hopkins University. Advisor: Eric Brill. 169 pages</comments><acm-class>I.2.7</acm-class><abstract>  The popularity of applying machine learning methods to computational
linguistics problems has produced a large supply of trainable natural language
processing systems. Most problems of interest have an array of off-the-shelf
products or downloadable code implementing solutions using various techniques.
Where these solutions are developed independently, it is observed that their
errors tend to be independently distributed. This thesis is concerned with
approaches for capitalizing on this situation in a sample problem domain, Penn
Treebank-style parsing.
  The machine learning community provides techniques for combining outputs of
classifiers, but parser output is more structured and interdependent than
classifications. To address this discrepancy, two novel strategies for
combining parsers are used: learning to control a switch between parsers and
constructing a hybrid parse from multiple parsers' outputs.
  Off-the-shelf parsers are not developed with an intention to perform well in
a collaborative ensemble. Two techniques are presented for producing an
ensemble of parsers that collaborate. All of the ensemble members are created
using the same underlying parser induction algorithm, and the method for
producing complementary parsers is only loosely constrained by that chosen
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006013</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006013</id><created>2000-06-07</created><authors><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author><author><keyname>Koutsias</keyname><forenames>John</forenames></author><author><keyname>Chandrinos</keyname><forenames>Konstantinos V.</forenames></author><author><keyname>Paliouras</keyname><forenames>George</forenames></author><author><keyname>Spyropoulos</keyname><forenames>Constantine D.</forenames></author></authors><title>An evaluation of Naive Bayesian anti-spam filtering</title><categories>cs.CL cs.AI</categories><comments>9 pages</comments><acm-class>H.4.3; I.2.6; I.2.7; I.5.4; K.4.1</acm-class><journal-ref>Proceedings of the workshop on Machine Learning in the New
  Information Age, G. Potamias, V. Moustakis and M. van Someren (eds.), 11th
  European Conference on Machine Learning, Barcelona, Spain, pp. 9-17, 2000</journal-ref><abstract>  It has recently been argued that a Naive Bayesian classifier can be used to
filter unsolicited bulk e-mail (&quot;spam&quot;). We conduct a thorough evaluation of
this proposal on a corpus that we make publicly available, contributing towards
standard benchmarks. At the same time we investigate the effect of
attribute-set size, training-corpus size, lemmatization, and stop-lists on the
filter's performance, issues that had not been previously explored. After
introducing appropriate cost-sensitive evaluation measures, we reach the
conclusion that additional safety nets are needed for the Naive Bayesian
anti-spam filter to be viable in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006014</id><created>2000-06-08</created><authors><author><keyname>Gunther</keyname><forenames>Neil J.</forenames></author></authors><title>Solaris System Resource Manager: All I Ever Wanted Was My Unfair
  Advantage (And Why You Can't Have It!)</title><categories>cs.PF cs.OS</categories><comments>17 pages, Updated since the 25th Computer Measurement Group
  Conference, Reno NV, Dec.5-10, 1999</comments><acm-class>C.4;D.4.1;D.4.8</acm-class><journal-ref>Proc. CMG'99 Conf. p.194-205</journal-ref><abstract>  Traditional UNIX time-share schedulers attempt to be fair to all users by
employing a round-robin style algorithm for allocating CPU time. Unfortunately,
a loophole exists whereby the scheduler can be biased in favor of a greedy user
running many short CPU-time processes. This loophole is not a defect but an
intrinsic property of the round-robin scheduler that ensures responsiveness to
the short CPU demands associated with multiple interactive users. A new
generation of UNIX system resource management software constrains the scheduler
to be equitable to all users regardless of the number of processes each may be
running. This &quot;fair-share&quot; scheduling draws on the concept of pro rating
resource &quot;shares&quot; across users and groups and then dynamically adjusting CPU
usage to meet those share proportions. The simple notion of statically
allocating these shares, however, belies the potential consequences for
performance as measured by user response time and service level targets. We
demonstrate this point by modeling several simple share allocation scenarios
and analyzing the corresponding performance effects. A brief comparison of
commercial system resource management implementations from HP, IBM, and SUN is
also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006015</id><created>2000-06-08</created><updated>2000-06-13</updated><authors><author><keyname>Gunther</keyname><forenames>Neil J.</forenames></author></authors><title>UNIX Resource Managers: Capacity Planning and Resource Issues</title><categories>cs.PF cs.OS</categories><comments>12 pages. Fixed formatting problem. To be presented at the SAGE-AU
  Conference, Bond University, Gold Coast, Australia, July 7, 2000</comments><acm-class>C.4;D.4.1;D.4.8</acm-class><abstract>  The latest implementations of commercial UNIX to offer mainframe style
capacity management on enterprise servers include: AIX Workload Manager (WLM),
HP-UX Process Resource Manager (PRM), Solaris Resource Manager (SRM), as well
as SGI and Compaq. The ability to manage server capacity is achieved by making
significant modifications to the standard UNIX operating system so that
processes are inherently tied to specific users. Those users, in turn, are
granted only a certain fraction of system resources. Resource usage is
monitored and compared with each users grant to ensure that the assigned
entitlement constraints are met. In this paper, we begin by clearing up some of
the confusion that has surrounded the motivation and the terminology behind the
new technology. The common theme across each of the commercial implementations
is the introduction of the fair-share scheduler. After reviewing some potential
performance pitfalls, we present capacity planning guidelines for migrating to
automated UNIX resource management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006016</id><created>2000-06-08</created><authors><author><keyname>Gunther</keyname><forenames>Neil J.</forenames></author></authors><title>The X-Files: Investigating Alien Performance in a Thin-client World</title><categories>cs.PF cs.DC</categories><comments>13 pages; Invited Lecture at the High Performance Computing
  Conference, University of Tromso, Norway, June 27-30, 1999</comments><acm-class>C.2.4;C.4;D.4.8;D.4.9;H.3.4;H.5.2;I.6.8</acm-class><journal-ref>Proc. Hiper'99 Vol.1, p.156</journal-ref><abstract>  Many scientific applications use the X11 window environment; an open source
windows GUI standard employing a client/server architecture. X11 promotes:
distributed computing, thin-client functionality, cheap desktop displays,
compatibility with heterogeneous servers, remote services and administration,
and greater maturity than newer web technologies. This paper details the
author's investigations into close encounters with alien performance in
X11-based seismic applications running on a 200-node cluster, backed by 2 TB of
mass storage. End-users cited two significant UFOs (Unidentified Faulty
Operations) i) long application launch times and ii) poor interactive response
times. The paper is divided into three major sections describing Close
Encounters of the 1st Kind: citings of UFO experiences, the 2nd Kind: recording
evidence of a UFO, and the 3rd Kind: contact and analysis. UFOs do exist and
this investigation presents a real case study for evaluating workload analysis
and other diagnostic tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006017</id><created>2000-06-09</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames></author><author><keyname>Hockey</keyname><forenames>Beth Ann</forenames></author><author><keyname>James</keyname><forenames>Frankie</forenames></author></authors><title>Turning Speech Into Scripts</title><categories>cs.CL</categories><comments>Working notes from AAAI Spring Symposium</comments><acm-class>H.5.2; I.2.7</acm-class><journal-ref>AAAI Spring Symposium on Natural Dialogues with Practical Robotic
  Devices, March 20-22, 2000. Stanford, CA</journal-ref><abstract>  We describe an architecture for implementing spoken natural language dialogue
interfaces to semi-autonomous systems, in which the central idea is to
transform the input speech signal through successive levels of representation
corresponding roughly to linguistic knowledge, dialogue knowledge, and domain
knowledge. The final representation is an executable program in a simple
scripting language equivalent to a subset of Cshell. At each stage of the
translation process, an input is transformed into an output, producing as a
byproduct a &quot;meta-output&quot; which describes the nature of the transformation
performed. We show how consistent use of the output/meta-output distinction
permits a simple and perspicuous treatment of apparently diverse topics
including resolution of pronouns, correction of user misconceptions, and
optimization of scripts. The methods described have been concretely realized in
a prototype speech interface to a simulation of the Personal Satellite
Assistant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006018</identifier>
 <datestamp>2008-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006018</id><created>2000-06-09</created><authors><author><keyname>James</keyname><forenames>Frankie</forenames></author><author><keyname>Rayner</keyname><forenames>Manny</forenames></author><author><keyname>Hockey</keyname><forenames>Beth Ann</forenames></author></authors><title>Accuracy, Coverage, and Speed: What Do They Mean to Users?</title><categories>cs.CL cs.HC</categories><comments>Position paper for CHI 2000 Workshop on Natural-Language Interaction</comments><acm-class>H.5.2; I.2.7</acm-class><abstract>  Speech is becoming increasingly popular as an interface modality, especially
in hands- and eyes-busy situations where the use of a keyboard or mouse is
difficult. However, despite the fact that many have hailed speech as being
inherently usable (since everyone already knows how to talk), most users of
speech input are left feeling disappointed by the quality of the interaction.
Clearly, there is much work to be done on the design of usable spoken
interfaces. We believe that there are two major problems in the design of
speech interfaces, namely, (a) the people who are currently working on the
design of speech interfaces are, for the most part, not interface designers and
therefore do not have as much experience with usability issues as we in the CHI
community do, and (b) speech, as an interface modality, has vastly different
properties than other modalities, and therefore requires different usability
measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006019</id><created>2000-06-09</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames></author><author><keyname>Hockey</keyname><forenames>Beth Ann</forenames></author><author><keyname>James</keyname><forenames>Frankie</forenames></author></authors><title>A Compact Architecture for Dialogue Management Based on Scripts and
  Meta-Outputs</title><categories>cs.CL</categories><acm-class>I.2.7; H.5.2</acm-class><journal-ref>Language Technology Joint Conference ANLP-NAACL 2000. 29 April - 4
  May 2000, Seattle, WA</journal-ref><abstract>  We describe an architecture for spoken dialogue interfaces to semi-autonomous
systems that transforms speech signals through successive representations of
linguistic, dialogue, and domain knowledge. Each step produces an output, and a
meta-output describing the transformation, with an executable program in a
simple scripting language as the final result. The output/meta-output
distinction permits perspicuous treatment of diverse tasks such as resolving
pronouns, correcting user misconceptions, and optimizing scripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006020</id><created>2000-06-09</created><authors><author><keyname>Hockey</keyname><forenames>Beth Ann</forenames></author><author><keyname>Rayner</keyname><forenames>Manny</forenames></author><author><keyname>James</keyname><forenames>Frankie</forenames></author></authors><title>A Comparison of the XTAG and CLE Grammars for English</title><categories>cs.CL</categories><comments>5th International Workshop on Tree Adjoining Grammars and Related
  Formalisms. 25-27 May 2000, Paris, France</comments><acm-class>I.2.7</acm-class><abstract>  When people develop something intended as a large broad-coverage grammar,
they usually have a more specific goal in mind. Sometimes this goal is covering
a corpus; sometimes the developers have theoretical ideas they wish to
investigate; most often, work is driven by a combination of these two main
types of goal. What tends to happen after a while is that the community of
people working with the grammar starts thinking of some phenomena as
``central'', and makes serious efforts to deal with them; other phenomena are
labelled ``marginal'', and ignored. Before long, the distinction between
``central'' and ``marginal'' becomes so ingrained that it is automatic, and
people virtually stop thinking about the ``marginal'' phenomena. In practice,
the only way to bring the marginal things back into focus is to look at what
other people are doing and compare it with one's own work. In this paper, we
will take two large grammars, XTAG and the CLE, and examine each of them from
the other's point of view. We will find in both cases not only that important
things are missing, but that the perspective offered by the other grammar
suggests simple and practical ways of filling in the holes. It turns out that
there is a pleasing symmetry to the picture. XTAG has a very good treatment of
complement structure, which the CLE to some extent lacks; conversely, the CLE
offers a powerful and general account of adjuncts, which the XTAG grammar does
not fully duplicate. If we examine the way in which each grammar does the thing
it is good at, we find that the relevant methods are quite easy to port to the
other framework, and in fact only involve generalization and systematization of
existing mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006021</id><created>2000-06-09</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames></author><author><keyname>Hockey</keyname><forenames>Beth Ann</forenames></author><author><keyname>James</keyname><forenames>Frankie</forenames></author><author><keyname>Bratt</keyname><forenames>Elizabeth O.</forenames></author><author><keyname>Goldwater</keyname><forenames>Sharon</forenames></author><author><keyname>Gawron</keyname><forenames>Mark</forenames></author></authors><title>Compiling Language Models from a Linguistically Motivated Unification
  Grammar</title><categories>cs.CL</categories><comments>To be published in COLING 2000</comments><acm-class>I.2.7</acm-class><abstract>  Systems now exist which are able to compile unification grammars into
language models that can be included in a speech recognizer, but it is so far
unclear whether non-trivial linguistically principled grammars can be used for
this purpose. We describe a series of experiments which investigate the
question empirically, by incrementally constructing a grammar and discovering
what problems emerge when successively larger versions are compiled into finite
state graph representations and used as language models for a medium-vocabulary
recognition task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006022</id><created>2000-06-10</created><authors><author><keyname>Helmy</keyname><forenames>Ahmed</forenames><affiliation>University of Southern California</affiliation></author></authors><title>Multicast-based Architecture for IP Mobility: Simulation Analysis and
  Comparison with Basic Mobile IP</title><categories>cs.NI cs.PF</categories><comments>14 pages, 14 pages (.pdf file)</comments><acm-class>C.2.1; C.2.2</acm-class><abstract>  With the introduction of a newer generation of wireless devices and
technologies, the need for an efficient architecture for IP mobility is
becoming more apparent. Several architectures have been proposed to support IP
mobility. Most studies, however, show that current architectures, in general,
fall short from satisfying the performance requirements for wireless
applications, mainly audio. Other studies have shown performance improvement by
using multicast to reduce latency and packet loss during handoff. In this
study, we propose a multicast-based architecture to support IP mobility. We
evaluate our approach through simulation, and we compare it to mainstream
approaches for IP mobility, mainly, the Mobile IP protocol. Comparison is
performed according to the required performance criteria, such as smooth
handoff and efficient routing.
  Our simulation results show significant improvement for the proposed
architecture. On average, basic Mobile IP consumes almost twice as much network
bandwidth, and experiences more than twice as much end-to-end and handoff
delays, as does our proposed architecture. Furthermore, we propose an extension
to Mobile IP to support our architecture with minimal modification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006023</id><created>2000-06-11</created><updated>2000-10-26</updated><authors><author><keyname>Stolcke</keyname><forenames>A.</forenames></author><author><keyname>Ries</keyname><forenames>K.</forenames></author><author><keyname>Coccaro</keyname><forenames>N.</forenames></author><author><keyname>Shriberg</keyname><forenames>E.</forenames></author><author><keyname>Bates</keyname><forenames>R.</forenames></author><author><keyname>Jurafsky</keyname><forenames>D.</forenames></author><author><keyname>Taylor</keyname><forenames>P.</forenames></author><author><keyname>Martin</keyname><forenames>R.</forenames></author><author><keyname>Van Ess-Dykema</keyname><forenames>C.</forenames></author><author><keyname>Meteer</keyname><forenames>M.</forenames></author></authors><title>Dialogue Act Modeling for Automatic Tagging and Recognition of
  Conversational Speech</title><categories>cs.CL</categories><comments>35 pages, 5 figures. Changes in copy editing (note title spelling
  changed)</comments><acm-class>I.2.7</acm-class><journal-ref>Computational Linguistics 26(3), 339-373, September 2000</journal-ref><abstract>  We describe a statistical approach for modeling dialogue acts in
conversational speech, i.e., speech-act-like units such as Statement, Question,
Backchannel, Agreement, Disagreement, and Apology. Our model detects and
predicts dialogue acts based on lexical, collocational, and prosodic cues, as
well as on the discourse coherence of the dialogue act sequence. The dialogue
model is based on treating the discourse structure of a conversation as a
hidden Markov model and the individual dialogue acts as observations emanating
from the model states. Constraints on the likely sequence of dialogue acts are
modeled via a dialogue act n-gram. The statistical dialogue grammar is combined
with word n-grams, decision trees, and neural networks modeling the
idiosyncratic lexical and prosodic manifestations of each dialogue act. We
develop a probabilistic integration of speech recognition with dialogue
modeling, to improve both speech recognition and dialogue act classification
accuracy. Models are trained and evaluated using a large hand-labeled database
of 1,155 conversations from the Switchboard corpus of spontaneous
human-to-human telephone speech. We achieved good dialogue act labeling
accuracy (65% based on errorful, automatically recognized words and prosody,
and 71% based on word transcripts, compared to a chance baseline accuracy of
35% and human accuracy of 84%) and a small reduction in word recognition error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006024</id><created>2000-06-11</created><authors><author><keyname>Shriberg</keyname><forenames>E.</forenames></author><author><keyname>Bates</keyname><forenames>R.</forenames></author><author><keyname>Stolcke</keyname><forenames>A.</forenames></author><author><keyname>Taylor</keyname><forenames>P.</forenames></author><author><keyname>Jurafsky</keyname><forenames>D.</forenames></author><author><keyname>Ries</keyname><forenames>K.</forenames></author><author><keyname>Coccaro</keyname><forenames>N.</forenames></author><author><keyname>Martin</keyname><forenames>R.</forenames></author><author><keyname>Meteer</keyname><forenames>M.</forenames></author><author><keyname>Van Ess-Dykema</keyname><forenames>C.</forenames></author></authors><title>Can Prosody Aid the Automatic Classification of Dialog Acts in
  Conversational Speech?</title><categories>cs.CL</categories><comments>55 pages, 10 figures</comments><acm-class>I.2.7</acm-class><journal-ref>Language and Speech 41(3-4), 439-487, 1998</journal-ref><abstract>  Identifying whether an utterance is a statement, question, greeting, and so
forth is integral to effective automatic understanding of natural dialog.
Little is known, however, about how such dialog acts (DAs) can be automatically
classified in truly natural conversation. This study asks whether current
approaches, which use mainly word information, could be improved by adding
prosodic information. The study is based on more than 1000 conversations from
the Switchboard corpus. DAs were hand-annotated, and prosodic features
(duration, pause, F0, energy, and speaking rate) were automatically extracted
for each DA. In training, decision trees based on these features were inferred;
trees were then applied to unseen test data to evaluate performance.
Performance was evaluated for prosody models alone, and after combining the
prosody models with word information -- either from true words or from the
output of an automatic speech recognizer. For an overall classification task,
as well as three subtasks, prosody made significant contributions to
classification. Feature-specific analyses further revealed that although
canonical features (such as F0 for questions) were important, less obvious
features could compensate if canonical features were removed. Finally, in each
task, integrating the prosodic model with a DA-specific statistical language
model improved performance over that of the language model alone, especially
for the case of recognized words. Results suggest that DAs are redundantly
marked in natural conversation, and that a variety of automatically extractable
prosodic features could aid dialog processing in speech applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006025</id><created>2000-06-11</created><authors><author><keyname>Stolcke</keyname><forenames>A.</forenames></author></authors><title>Entropy-based Pruning of Backoff Language Models</title><categories>cs.CL</categories><comments>5 pages. Typos in published version fixed</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings DARPA Broadcast News Transcription and Understanding
  Workshop, pp. 270-274, Lansdowne, VA, 1998</journal-ref><abstract>  A criterion for pruning parameters from N-gram backoff language models is
developed, based on the relative entropy between the original and the pruned
model. It is shown that the relative entropy resulting from pruning a single
N-gram can be computed exactly and efficiently for backoff models. The relative
entropy measure can be expressed as a relative change in training set
perplexity. This leads to a simple pruning criterion whereby all N-grams that
change perplexity by less than a threshold are removed from the model.
Experiments show that a production-quality Hub4 LM can be reduced to 26% its
original size without increasing recognition error. We also compare the
approach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and
show that their approach can be interpreted as an approximation to the relative
entropy criterion. Experimentally, both approaches select similar sets of
N-grams (about 85% overlap), with the exact relative entropy criterion giving
marginally better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006026</id><created>2000-06-12</created><updated>2000-06-21</updated><authors><author><keyname>Fontana</keyname><forenames>Federico</forenames></author><author><keyname>Rocchesso</keyname><forenames>Davide</forenames></author></authors><title>Online Correction of Dispersion Error in 2D Waveguide Meshes</title><categories>cs.SD cs.NA</categories><comments>4 pages, 5 figures, to appear in the Proceedings of the International
  Computer Music Conference, 2000. Corrected first reference</comments><acm-class>H.5.5</acm-class><abstract>  An elastic ideal 2D propagation medium, i.e., a membrane, can be simulated by
models discretizing the wave equation on the time-space grid (finite difference
methods), or locally discretizing the solution of the wave equation (waveguide
meshes). The two approaches provide equivalent computational structures, and
introduce numerical dispersion that induces a misalignment of the modes from
their theoretical positions. Prior literature shows that dispersion can be
arbitrarily reduced by oversizing and oversampling the mesh, or by adpting
offline warping techniques. In this paper we propose to reduce numerical
dispersion by embedding warping elements, i.e., properly tuned allpass filters,
in the structure. The resulting model exhibits a significant reduction in
dispersion, and requires less computational resources than a regular mesh
structure having comparable accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006027</id><created>2000-06-13</created><authors><author><keyname>Nugues</keyname><forenames>Pierre</forenames></author></authors><title>Verbal Interactions in Virtual Worlds</title><categories>cs.CL cs.HC</categories><comments>Position paper for CHI 2000 Workshop on Natural-Language Interaction,
  The Hague, 22 figures</comments><acm-class>H.5.2; I.2.7</acm-class><abstract>  We first discuss respective advantages of language interaction in virtual
worlds and of using 3D images in dialogue systems. Then, we describe an example
of a verbal interaction system in virtual reality: Ulysse. Ulysse is a
conversational agent that helps a user navigate in virtual worlds. It has been
designed to be embedded in the representation of a participant of a virtual
conference and it responds positively to motion orders. Ulysse navigates the
user's viewpoint on his/her behalf in the virtual world. On tests we carried
out, we discovered that users, novices as well as experienced ones have
difficulties moving in a 3D environment. Agents such as Ulysse enable a user to
carry out navigation motions that would have been impossible with classical
interaction devices. From the whole Ulysse system, we have stripped off a
skeleton architecture that we have ported to VRML, Java, and Prolog. We hope
this skeleton helps the design of language applications in virtual worlds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006028</id><created>2000-06-13</created><authors><author><keyname>Ratnaparkhi</keyname><forenames>Adwait</forenames></author></authors><title>Trainable Methods for Surface Natural Language Generation</title><categories>cs.CL</categories><comments>LaTeX, 8 pages</comments><acm-class>I.2.7; I.2.6</acm-class><journal-ref>Proceedings of the 1st Meeting of the North American Chapter of
  the Association for Computational Linguistics (NAACL 2000). Pages 194--201</journal-ref><abstract>  We present three systems for surface natural language generation that are
trainable from annotated corpora. The first two systems, called NLG1 and NLG2,
require a corpus marked only with domain-specific semantic attributes, while
the last system, called NLG3, requires a corpus marked with both semantic
attributes and syntactic dependency information. All systems attempt to produce
a grammatical natural language phrase from a domain-specific semantic
representation. NLG1 serves a baseline system and uses phrase frequencies to
generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy
probability models to individually generate each word in the phrase. The
systems NLG2 and NLG3 learn to determine both the word choice and the word
order of the phrase. We present experiments in which we generate phrases to
describe flights in the air travel domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006029</id><created>2000-06-17</created><authors><author><keyname>Helmy</keyname><forenames>Ahmed</forenames><affiliation>University of Southern California</affiliation></author><author><keyname>Gupta</keyname><forenames>Sandeep</forenames><affiliation>University of Southern California</affiliation></author><author><keyname>Estrin</keyname><forenames>Deborah</forenames><affiliation>University of Southern California</affiliation></author><author><keyname>Cerpa</keyname><forenames>Alberto</forenames><affiliation>University of Southern California</affiliation></author><author><keyname>Yu</keyname><forenames>Yan</forenames><affiliation>University of Southern California</affiliation></author></authors><title>Systematic Performance Evaluation of Multipoint Protocols</title><categories>cs.NI cs.DS</categories><comments>23 pages, 12 figures</comments><acm-class>C.2.2; C.2.0</acm-class><abstract>  The advent of multipoint (multicast-based) applications and the growth and
complexity of the Internet has complicated network protocol design and
evaluation.
  In this paper, we present a method for automatic synthesis of worst and best
case scenarios for multipoint protocol performance evaluation.
  Our method uses a fault-oriented test generation (FOTG) algorithm for
searching the protocol and system state space to synthesize these scenarios.
The algorithm is based on a global finite state machine (FSM) model. We extend
the algorithm with timing semantics to handle end-to-end delays and address
performance criteria. We introduce the notion of a virtual LAN to represent
delays of the underlying multicast distribution tree.
  As a case study, we use our method to evaluate variants of the timer
suppression mechanism, used in various multipoint protocols, with respect to
two performance criteria: overhead of response messages and response time.
Simulation results for reliable multicast protocols show that our method
provides a scalable way for synthesizing worst-case scenarios automatically. We
expect our method to serve as a model for applying systematic scenario
generation to other multipoint protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006030</id><created>2000-06-20</created><updated>2000-06-20</updated><authors><author><keyname>Bojinov</keyname><forenames>Hristo</forenames></author><author><keyname>Casal</keyname><forenames>Arancha</forenames></author><author><keyname>Hogg</keyname><forenames>Tad</forenames></author></authors><title>Multiagent Control of Self-reconfigurable Robots</title><categories>cs.RO cs.DC cs.MA</categories><comments>15 pages, 10 color figures, including low-resolution photos of
  prototype hardware</comments><acm-class>I.2.9; I.2.11; H.3.4</acm-class><journal-ref>Artificial Intelligence 142:99-120 (2002)</journal-ref><abstract>  We demonstrate how multiagent systems provide useful control techniques for
modular self-reconfigurable (metamorphic) robots. Such robots consist of many
modules that can move relative to each other, thereby changing the overall
shape of the robot to suit different tasks. Multiagent control is particularly
well-suited for tasks involving uncertain and changing environments. We
illustrate this approach through simulation experiments of Proteo, a
metamorphic robot system currently under development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006031</id><created>2000-06-21</created><authors><author><keyname>Shen</keyname><forenames>Yi-Dong</forenames></author><author><keyname>Yuan</keyname><forenames>Li-Yan</forenames></author><author><keyname>You</keyname><forenames>Jia-Huai</forenames></author></authors><title>Verifying Termination of General Logic Programs with Concrete Queries</title><categories>cs.AI cs.LO</categories><comments>28 pages, 8 figures</comments><acm-class>D.3.1; F.4.1; I.2.3</acm-class><abstract>  We introduce a method of verifying termination of logic programs with respect
to concrete queries (instead of abstract query patterns). A necessary and
sufficient condition is established and an algorithm for automatic verification
is developed. In contrast to existing query pattern-based approaches, our
method has the following features: (1) It applies to all general logic programs
with non-floundering queries. (2) It is very easy to automate because it does
not need to search for a level mapping or a model, nor does it need to compute
an interargument relation based on additional mode or type information. (3) It
bridges termination analysis with loop checking, the two problems that have
been studied separately in the past despite their close technical relation with
each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006032</id><created>2000-06-23</created><authors><author><keyname>Grefenstette</keyname><forenames>Gregory</forenames></author><author><keyname>Nioche</keyname><forenames>Julien</forenames></author></authors><title>Estimation of English and non-English Language Use on the WWW</title><categories>cs.CL cs.GL cs.HC</categories><acm-class>J.5; H.5.2; I.2.7; H.3.5; H.5.3</acm-class><journal-ref>Proceedings of RIAO'2000, &quot;Content-Based Multimedia Information
  Access&quot;, Paris, April 12-14,2000, pp. 237-246</journal-ref><abstract>  The World Wide Web has grown so big, in such an anarchic fashion, that it is
difficult to describe. One of the evident intrinsic characteristics of the
World Wide Web is its multilinguality. Here, we present a technique for
estimating the size of a language-specific corpus given the frequency of
commonly occurring words in the corpus. We apply this technique to estimating
the number of words available through Web browsers for given languages.
Comparing data from 1996 to data from 1999 and 2000, we calculate the growth of
a number of European languages on the Web. As expected, non-English languages
are growing at a faster pace than English, though the position of English is
still dominant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006033</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006033</id><created>2000-06-23</created><authors><author><keyname>Smaus</keyname><forenames>Jan-Georg</forenames></author><author><keyname>Hill</keyname><forenames>Patricia M.</forenames></author><author><keyname>King</keyname><forenames>Andy</forenames></author></authors><title>Verifying Termination and Error-Freedom of Logic Programs with block
  Declarations</title><categories>cs.LO cs.PL</categories><comments>to be published in Theory and Practice of Logic Programming, 40
  pages, 10 figures</comments><acm-class>F.3.1; D.3.2</acm-class><abstract>  We present verification methods for logic programs with delay declarations.
The verified properties are termination and freedom from errors related to
built-ins. Concerning termination, we present two approaches. The first
approach tries to eliminate the well-known problem of speculative output
bindings. The second approach is based on identifying the predicates for which
the textual position of an atom using this predicate is irrelevant with respect
to termination. Three features are distinctive of this work: it allows for
predicates to be used in several modes; it shows that block declarations, which
are a very simple delay construct, are sufficient to ensure the desired
properties; it takes the selection rule into account, assuming it to be as in
most Prolog implementations. The methods can be used to verify existing
programs and assist in writing new programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006034</id><created>2000-06-25</created><authors><author><keyname>Glynn</keyname><forenames>Kevin</forenames></author><author><keyname>Sulzmann</keyname><forenames>Martin</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author></authors><title>Type Classes and Constraint Handling Rules</title><categories>cs.PL</categories><comments>14 pages, Workshop on Rule-Based Constraint Reasoning and Programming
  (http://www.informatik.uni-muenchen.de/~fruehwir/cl2000r.html)</comments><report-no>TR2000/7</report-no><acm-class>D.3.3; F.3.1</acm-class><abstract>  Type classes are an elegant extension to traditional, Hindley-Milner based
typing systems. They are used in modern, typed languages such as Haskell to
support controlled overloading of symbols. Haskell 98 supports only
single-parameter and constructor type classes. Other extensions such as
multi-parameter type classes are highly desired but are still not officially
supported by Haskell. Subtle issues arise with extensions, which may lead to a
loss of feasible type inference or ambiguous programs. A proper logical basis
for type class systems seems to be missing. Such a basis would allow extensions
to be characterised and studied rigorously. We propose to employ Constraint
Handling Rules as a tool to study and develop type class systems in a uniform
way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006035</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006035</id><created>2000-06-26</created><updated>2006-08-03</updated><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>On the Development of the Intersection of a Plane with a Polytope</title><categories>cs.CG cs.DM</categories><comments>11 pages, 8 figures. Earlier version replaced after I discovered
  Schur's 1921 theorem, whose proof can be followed to establish the key
  generalization of Cauchy's arm lemma, my Theorem 1. Paper revised
  accordingly. New (2006) version corrects two errors in the proofs found by
  Raghavan Dhandapani</comments><report-no>Smith Technical Report 068</report-no><acm-class>F.2.2</acm-class><abstract>  Define a ``slice'' curve as the intersection of a plane with the surface of a
polytope, i.e., a convex polyhedron in three dimensions. We prove that a slice
curve develops on a plane without self-intersection. The key tool used is a
generalization of Cauchy's arm lemma to permit nonconvex ``openings'' of a
planar convex chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006036</id><created>2000-06-27</created><authors><author><keyname>Shriberg</keyname><forenames>E.</forenames></author><author><keyname>Stolcke</keyname><forenames>A.</forenames></author><author><keyname>Hakkani-Tur</keyname><forenames>D.</forenames></author><author><keyname>Tur</keyname><forenames>G.</forenames></author></authors><title>Prosody-Based Automatic Segmentation of Speech into Sentences and Topics</title><categories>cs.CL</categories><comments>30 pages, 9 figures. To appear in Speech Communication 32(1-2),
  Special Issue on Accessing Information in Spoken Audio, September 2000</comments><acm-class>I.2.7</acm-class><journal-ref>Speech Communication 32(1-2), 127-154, September 2000</journal-ref><abstract>  A crucial step in processing speech audio data for information extraction,
topic detection, or browsing/playback is to segment the input into sentence and
topic units. Speech segmentation is challenging, since the cues typically
present for segmenting text (headers, paragraphs, punctuation) are absent in
spoken language. We investigate the use of prosody (information gleaned from
the timing and melody of speech) for these tasks. Using decision tree and
hidden Markov modeling techniques, we combine prosodic cues with word-based
approaches, and evaluate performance on two speech corpora, Broadcast News and
Switchboard. Results show that the prosodic model alone performs on par with,
or better than, word-based statistical language models -- for both true and
automatically recognized words in news speech. The prosodic model achieves
comparable performance with significantly less training data, and requires no
hand-labeling of prosodic events. Across tasks and corpora, we obtain a
significant improvement over word-only models using a probabilistic combination
of prosodic and lexical information. Inspection reveals that the prosodic
models capture language-independent boundary indicators described in the
literature. Finally, cue usage is task and corpus dependent. For example, pause
and pitch features are highly informative for segmenting news speech, whereas
pause, duration and word-based cues dominate for natural conversation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006037</id><created>2000-06-27</created><authors><author><keyname>Haas</keyname><forenames>Zygmunt</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Wicker</keyname><forenames>Stephen B.</forenames></author></authors><title>A Decision-Theoretic Approach to Resource Allocation in Wireless
  Multimedia Networks</title><categories>cs.NI</categories><comments>To appear, Dial M for Mobility, 2000</comments><acm-class>C.2.1; C.2.2</acm-class><abstract>  The allocation of scarce spectral resources to support as many user
applications as possible while maintaining reasonable quality of service is a
fundamental problem in wireless communication. We argue that the problem is
best formulated in terms of decision theory. We propose a scheme that takes
decision-theoretic concerns (like preferences) into account and discuss the
difficulties and subtleties involved in applying standard techniques from the
theory of Markov Decision Processes (MDPs) in constructing an algorithm that is
decision-theoretically optimal. As an example of the proposed framework, we
construct such an algorithm under some simplifying assumptions. Additionally,
we present analysis and simulation results that show that our algorithm meets
its design goals. Finally, we investigate how far from optimal one well-known
heuristic is. The main contribution of our results is in providing insight and
guidance for the design of near-optimal admission-control policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006038</id><created>2000-06-28</created><authors><author><keyname>Gerdemann</keyname><forenames>Dale</forenames></author><author><keyname>van Noord</keyname><forenames>Gertjan</forenames></author></authors><title>Approximation and Exactness in Finite State Optimality Theory</title><categories>cs.CL</categories><comments>10 pages, 1 figure, Finite-State Phonology : SIGPHON 2000, Fifth
  Meeting of the ACL Special Interest Group in Computational Phonology, COLING
  2000</comments><acm-class>I.2.7</acm-class><abstract>  Previous work (Frank and Satta 1998; Karttunen, 1998) has shown that
Optimality Theory with gradient constraints generally is not finite state. A
new finite-state treatment of gradient constraints is presented which improves
upon the approximation of Karttunen (1998). The method turns out to be exact,
and very compact, for the syllabification analysis of Prince and Smolensky
(1993).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006039</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006039</id><created>2000-06-28</created><authors><author><keyname>Drioli</keyname><forenames>Carlo</forenames></author><author><keyname>Rocchesso</keyname><forenames>Davide</forenames></author></authors><title>Orthogonal Least Squares Algorithm for the Approximation of a Map and
  its Derivatives with a RBF Network</title><categories>cs.NE cs.SD</categories><comments>8 pages, 8 figures, submitted to IEEE Trans. on Systems, Man, and
  Cybernetics</comments><acm-class>I.2.6</acm-class><abstract>  Radial Basis Function Networks (RBFNs) are used primarily to solve
curve-fitting problems and for non-linear system modeling. Several algorithms
are known for the approximation of a non-linear curve from a sparse data set by
means of RBFNs. However, there are no procedures that permit to define
constrains on the derivatives of the curve. In this paper, the Orthogonal Least
Squares algorithm for the identification of RBFNs is modified to provide the
approximation of a non-linear 1-in 1-out map along with its derivatives, given
a set of training data. The interest on the derivatives of non-linear functions
concerns many identification and control tasks where the study of system
stability and robustness is addressed. The effectiveness of the proposed
algorithm is demonstrated by a study on the stability of a single loop feedback
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006040</id><created>2000-06-28</created><authors><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author></authors><title>Correlation over Decomposed Signals: A Non-Linear Approach to Fast and
  Effective Sequences Comparison</title><categories>cs.CV cs.DS q-bio</categories><comments>7 pages, 1 figure</comments><acm-class>I.5.4; F.2.2; I.5.4; J.3</acm-class><abstract>  A novel non-linear approach to fast and effective comparison of sequences is
presented, compared to the traditional cross-correlation operator, and
illustrated with respect to DNA sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006041</id><created>2000-06-29</created><authors><author><keyname>Atserias</keyname><forenames>Jordi</forenames></author><author><keyname>Castellon</keyname><forenames>Irene</forenames></author><author><keyname>Civit</keyname><forenames>Montse</forenames></author><author><keyname>Rigau</keyname><forenames>German</forenames></author></authors><title>Using a Diathesis Model for Semantic Parsing</title><categories>cs.CL cs.AI</categories><comments>8 pages</comments><acm-class>I.2.7;I.5</acm-class><journal-ref>Proceedins of VEXTAL.1999 pg 385-392</journal-ref><abstract>  This paper presents a semantic parsing approach for unrestricted texts.
Semantic parsing is one of the major bottlenecks of Natural Language
Understanding (NLU) systems and usually requires building expensive resources
not easily portable to other domains. Our approach obtains a case-role
analysis, in which the semantic roles of the verb are identified. In order to
cover all the possible syntactic realisations of a verb, our system combines
their argument structure with a set of general semantic labelled diatheses
models. Combining them, the system builds a set of syntactic-semantic patterns
with their own role-case representation. Once the patterns are build, we use an
approximate tree pattern-matching algorithm to identify the most reliable
pattern for a sentence. The pattern matching is performed between the
syntactic-semantic patterns and the feature-structure tree representing the
morphological, syntactical and semantic information of the analysed sentence.
For sentences assigned to the correct model, the semantic parsing system we are
presenting identifies correctly more than 73% of possible semantic case-roles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006042</id><created>2000-06-29</created><authors><author><keyname>Atserias</keyname><forenames>Jordi</forenames></author><author><keyname>Castellon</keyname><forenames>Irene</forenames></author><author><keyname>Civit</keyname><forenames>Montse</forenames></author><author><keyname>Rigau</keyname><forenames>German</forenames></author></authors><title>Semantic Parsing based on Verbal Subcategorization</title><categories>cs.CL cs.AI</categories><comments>12 pages, extended version of the paper. Spanish version of the paper
  also available from authors home page</comments><acm-class>I.2.7;I.5</acm-class><journal-ref>Conference on Intelligence text Processing and Computational
  Linguistics, CICLing 2000. pg 330-340</journal-ref><abstract>  The aim of this work is to explore new methodologies on Semantic Parsing for
unrestricted texts. Our approach follows the current trends in Information
Extraction (IE) and is based on the application of a verbal subcategorization
lexicon (LEXPIR) by means of complex pattern recognition techniques. LEXPIR is
framed on the theoretical model of the verbal subcategorization developed in
the Pirapides project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006043</id><created>2000-06-30</created><authors><author><keyname>Piechowiak</keyname><forenames>S.</forenames></author><author><keyname>Rodriguez</keyname><forenames>J.</forenames></author></authors><title>Constraint compiling into rules formalism constraint compiling into
  rules formalism for dynamic CSPs computing</title><categories>cs.AI</categories><comments>14 pages</comments><acm-class>F.4.1</acm-class><abstract>  In this paper we present a rule based formalism for filtering variables
domains of constraints. This formalism is well adapted for solving dynamic CSP.
We take diagnosis as an instance problem to illustrate the use of these rules.
A diagnosis problem is seen like finding all the minimal sets of constraints to
be relaxed in the constraint network that models the device to be diagnosed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006044</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006044</id><created>2000-06-30</created><authors><author><keyname>Beesley</keyname><forenames>Kenneth R.</forenames></author><author><keyname>Karttunen</keyname><forenames>Lauri</forenames></author></authors><title>Finite-State Non-Concatenative Morphotactics</title><categories>cs.CL</categories><comments>SIGPHON-2000, Proceedings of the Fifth Workshop of the ACL Special
  Interest Group in Computational Phonology, p. 1-12. Aug. 6, 2000. Luxembourg</comments><acm-class>A0;F1.1;J5</acm-class><abstract>  Finite-state morphology in the general tradition of the Two-Level and Xerox
implementations has proved very successful in the production of robust
morphological analyzer-generators, including many large-scale commercial
systems. However, it has long been recognized that these implementations have
serious limitations in handling non-concatenative phenomena. We describe a new
technique for constructing finite-state transducers that involves reapplying
the regular-expression compiler to its own output. Implemented in an algorithm
called compile-replace, this technique has proved useful for handling
non-concatenative phenomena; and we demonstrate it on Malay full-stem
reduplication and Arabic stem interdigitation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006045</id><created>2000-06-30</created><authors><author><keyname>Ribeiro</keyname><forenames>Carlos</forenames></author><author><keyname>Zuquete</keyname><forenames>Andre</forenames></author><author><keyname>Ferreira</keyname><forenames>Paulo</forenames></author><author><keyname>Guedes</keyname><forenames>Paulo</forenames></author></authors><title>Security Policy Consistency</title><categories>cs.LO cs.CR</categories><comments>To appear in the first CL2000 workshop on Rule-Based Constraint
  Reasoning and Programming</comments><acm-class>F.4.1; D.4.6; K.6.5</acm-class><abstract>  With the advent of wide security platforms able to express simultaneously all
the policies comprising an organization's global security policy, the problem
of inconsistencies within security policies become harder and more relevant.
  We have defined a tool based on the CHR language which is able to detect
several types of inconsistencies within and between security policies and other
specifications, namely workflow specifications.
  Although the problem of security conflicts has been addressed by several
authors, to our knowledge none has addressed the general problem of security
inconsistencies, on its several definitions and target specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006046</identifier>
 <datestamp>2010-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006046</id><created>2000-06-30</created><authors><author><keyname>Beigel</keyname><forenames>Richard</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>3-Coloring in Time O(1.3289^n)</title><categories>cs.DS</categories><comments>31 pages, 22 figures. An earlier version of this paper was presented
  at the 36th IEEE Symp. Foundations of Comp. Sci., 1995, and appears as ECCC
  TR 95-033</comments><acm-class>F.2.2</acm-class><journal-ref>J. Algorithms 54:2 (2005) 168-204</journal-ref><doi>10.1016/j.jalgor.2004.06.008</doi><abstract>  We consider worst case time bounds for NP-complete problems including 3-SAT,
3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a
constraint satisfaction (CSP) formulation of these problems. 3-SAT is
equivalent to (2,3)-CSP while the other problems above are special cases of
(3,2)-CSP; there is also a natural duality transformation from (a,b)-CSP to
(b,a)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the
time bounds for solving the other problems listed above. Our techniques involve
a mixture of Davis-Putnam-style backtracking with more sophisticated matching
and network flow based ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0006047</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0006047</id><created>2000-06-30</created><authors><author><keyname>Schlei</keyname><forenames>B. R.</forenames></author><author><keyname>Prasad</keyname><forenames>L.</forenames></author><author><keyname>Skourikhine</keyname><forenames>A. N.</forenames></author></authors><title>Geometric Morphology of Granular Materials</title><categories>cs.CV</categories><comments>6 pages, 9 figures. For more information visit
  http://www.nis.lanl.gov/~bschlei/labvis/index.html</comments><report-no>LA-UR-00-2839</report-no><acm-class>I.2.10;I.4.6;I.4.10</acm-class><doi>10.1117/12.404821</doi><abstract>  We present a new method to transform the spectral pixel information of a
micrograph into an affine geometric description, which allows us to analyze the
morphology of granular materials. We use spectral and pulse-coupled neural
network based segmentation techniques to generate blobs, and a newly developed
algorithm to extract dilated contours. A constrained Delaunay tesselation of
the contour points results in a triangular mesh. This mesh is the basic
ingredient of the Chodal Axis Transform, which provides a morphological
decomposition of shapes. Such decomposition allows for grain separation and the
efficient computation of the statistical features of granular materials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007001</id><created>2000-07-03</created><authors><author><keyname>Teran</keyname><forenames>Oswaldo</forenames><affiliation>Manchester Metropolitan University. Manchester. UK</affiliation><affiliation>Universidad de Los Andes. Merida. Venezuela</affiliation></author><author><keyname>Edmonds</keyname><forenames>Bruce</forenames><affiliation>Manchester Metropolitan University. Manchester. UK</affiliation></author><author><keyname>Wallis</keyname><forenames>Steve</forenames><affiliation>Manchester Metropolitan University. Manchester. UK</affiliation></author></authors><title>Constraint Exploration and Envelope of Simulation Trajectories</title><categories>cs.PL cs.AI cs.LO</categories><comments>15 pages, To be presented at the First Workshop on Rule-Based
  Constraint Reasoning and Programming at the First International Conference on
  Computational Logic, London, UK, 24th to 28th July, 2000</comments><acm-class>D.3.3; F.3.1; F.4.1</acm-class><abstract>  The implicit theory that a simulation represents is precisely not in the
individual choices but rather in the 'envelope' of possible trajectories - what
is important is the shape of the whole envelope. Typically a huge amount of
computation is required when experimenting with factors bearing on the dynamics
of a simulation to tease out what affects the shape of this envelope. In this
paper we present a methodology aimed at systematically exploring this envelope.
We propose a method for searching for tendencies and proving their necessity
relative to a range of parameterisations of the model and agents' choices, and
to the logic of the simulation language. The exploration consists of a forward
chaining generation of the trajectories associated to and constrained by such a
range of parameterisations and choices. Additionally, we propose a
computational procedure that helps implement this exploration by translating a
Multi Agent System simulation into a constraint-based search over possible
trajectories by 'compiling' the simulation rules into a more specific form,
namely by partitioning the simulation rules using appropriate modularity in the
simulation. An example of this procedure is exhibited.
  Keywords: Constraint Search, Constraint Logic Programming, Proof, Emergence,
Tendencies
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007002</id><created>2000-07-03</created><updated>2003-06-20</updated><authors><author><keyname>Benhamou</keyname><forenames>Frederic</forenames></author><author><keyname>Goualard</keyname><forenames>Frederic</forenames></author><author><keyname>Languenou</keyname><forenames>Eric</forenames></author><author><keyname>Christie</keyname><forenames>Marc</forenames></author></authors><title>Interval Constraint Solving for Camera Control and Motion Planning</title><categories>cs.AI cs.NA</categories><comments>35 pages, 13 figures, revised and extended version of a paper
  published in the proceedings of CP '00</comments><acm-class>D.3.3;D.2.2;G.1.0;H.5.1</acm-class><abstract>  Many problems in robust control and motion planning can be reduced to either
find a sound approximation of the solution space determined by a set of
nonlinear inequalities, or to the ``guaranteed tuning problem'' as defined by
Jaulin and Walter, which amounts to finding a value for some tuning parameter
such that a set of inequalities be verified for all the possible values of some
perturbation vector. A classical approach to solve these problems, which
satisfies the strong soundness requirement, involves some quantifier
elimination procedure such as Collins' Cylindrical Algebraic Decomposition
symbolic method. Sound numerical methods using interval arithmetic and local
consistency enforcement to prune the search space are presented in this paper
as much faster alternatives for both soundly solving systems of nonlinear
inequalities, and addressing the guaranteed tuning problem whenever the
perturbation vector has dimension one. The use of these methods in camera
control is investigated, and experiments with the prototype of a declarative
modeller to express camera motion using a cinematic language are reported and
commented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007003</id><created>2000-07-03</created><authors><author><keyname>Yeates</keyname><forenames>Stuart</forenames></author><author><keyname>Bainbridge</keyname><forenames>David</forenames></author><author><keyname>Witten</keyname><forenames>Ian H.</forenames></author></authors><title>Using compression to identify acronyms in text</title><categories>cs.DL cs.IR</categories><comments>10 pages. A short form published in DCC2000</comments><report-no>Working Paper 00/01</report-no><acm-class>H.3.7</acm-class><abstract>  Text mining is about looking for patterns in natural language text, and may
be defined as the process of analyzing text to extract information from it for
particular purposes. In previous work, we claimed that compression is a key
technology for text mining, and backed this up with a study that showed how
particular kinds of lexical tokens---names, dates, locations, etc.---can be
identified and located in running text, using compression models to provide the
leverage necessary to distinguish different token types (Witten et al., 1999)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007004</id><created>2000-07-04</created><authors><author><keyname>Zunino</keyname><forenames>Alejandro</forenames></author><author><keyname>Amandi</keyname><forenames>Analia</forenames></author></authors><title>Brainstorm/J: a Java Framework for Intelligent Agents</title><categories>cs.AI</categories><comments>15 pages. To be published in Proceedings of the Second Argentinian
  Symposium on Artificial Intelligence (ASAI'2000 - 29th JAIIO). September
  2000. Tandil, Buenos Aires, Argentina. See
  http://www.exa.unicen.edu.ar/~azunino</comments><acm-class>I.2.11</acm-class><abstract>  Despite the effort of many researchers in the area of multi-agent systems
(MAS) for designing and programming agents, a few years ago the research
community began to take into account that common features among different MAS
exists. Based on these common features, several tools have tackled the problem
of agent development on specific application domains or specific types of
agents. As a consequence, their scope is restricted to a subset of the huge
application domain of MAS. In this paper we propose a generic infrastructure
for programming agents whose name is Brainstorm/J. The infrastructure has been
implemented as an object oriented framework. As a consequence, our approach
supports a broader scope of MAS applications than previous efforts, being
flexible and reusable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007005</id><created>2000-07-04</created><authors><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author><author><keyname>Estrin</keyname><forenames>Deborah</forenames></author><author><keyname>Gupta</keyname><forenames>Sandeep</forenames></author></authors><title>Systematic Testing of Multicast Routing Protocols: Analysis of Forward
  and Backward Search Techniques</title><categories>cs.NI cs.DS</categories><comments>26 pages, 20 figures</comments><acm-class>C.2.2, C.2.0</acm-class><abstract>  In this paper, we present a new methodology for developing systematic and
automatic test generation algorithms for multipoint protocols. These algorithms
attempt to synthesize network topologies and sequences of events that stress
the protocol's correctness or performance. This problem can be viewed as a
domain-specific search problem that suffers from the state space explosion
problem. One goal of this work is to circumvent the state space explosion
problem utilizing knowledge of network and fault modeling, and multipoint
protocols. The two approaches investigated in this study are based on forward
and backward search techniques. We use an extended finite state machine (FSM)
model of the protocol. The first algorithm uses forward search to perform
reduced reachability analysis. Using domain-specific information for multicast
routing over LANs, the algorithm complexity is reduced from exponential to
polynomial in the number of routers. This approach, however, does not fully
automate topology synthesis. The second algorithm, the fault-oriented test
generation, uses backward search for topology synthesis and uses backtracking
to generate event sequences instead of searching forward from initial states.
Using these algorithms, we have conducted studies for correctness of the
multicast routing protocol PIM. We propose to extend these algorithms to study
end-to-end multipoint protocols using a virtual LAN that represents delays of
the underlying multicast distribution tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007006</id><created>2000-07-05</created><authors><author><keyname>Kaper</keyname><forenames>Hans G.</forenames><affiliation>Argonne National Laboratory</affiliation></author><author><keyname>Tipei</keyname><forenames>Sever</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author><author><keyname>Wright</keyname><forenames>Jeff M.</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author></authors><title>DISCO: An object-oriented system for music composition and sound design</title><categories>cs.SD cs.DS cs.SE</categories><comments>4 pages, no figures; to be published in Proc. Int'l Computer Music
  Conference 2000 (Berlin, August 2000)</comments><acm-class>H.5.5</acm-class><abstract>  This paper describes an object-oriented approach to music composition and
sound design. The approach unifies the processes of music making and instrument
building by using similar logic, objects, and procedures. The composition
modules use an abstract representation of musical data, which can be easily
mapped onto different synthesis languages or a traditionally notated score. An
abstract base class is used to derive classes on different time scales. Objects
can be related to act across time scales, as well as across an entire piece,
and relationships between similar objects can replicate traditional music
operations or introduce new ones. The DISCO (Digital Instrument for
Sonification and Composition) system is an open-ended work in progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007007</id><created>2000-07-05</created><authors><author><keyname>Kaper</keyname><forenames>Hans G.</forenames><affiliation>Argonne National Laboratory</affiliation></author><author><keyname>Tipei</keyname><forenames>Sever</forenames><affiliation>Argonne National Laboratory</affiliation></author><author><keyname>Wiebel</keyname><forenames>Elizabeth</forenames><affiliation>Argonne National Laboratory</affiliation></author></authors><title>Data sonification and sound visualization</title><categories>cs.SD cs.HC cs.MM</categories><comments>25 pages, 5 figures, 3 tables; preprint of the published paper
  (differing in details)</comments><report-no>ANL/MCS-P738-0199</report-no><acm-class>H.5.5</acm-class><journal-ref>Computing in Science and Engineering, Vol. 1 No. 4, July-August
  1999, pp. 48-58</journal-ref><abstract>  This article describes a collaborative project between researchers in the
Mathematics and Computer Science Division at Argonne National Laboratory and
the Computer Music Project of the University of Illinois at Urbana-Champaign.
The project focuses on the use of sound for the exploration and analysis of
complex data sets in scientific computing. The article addresses digital sound
synthesis in the context of DIASS (Digital Instrument for Additive Sound
Synthesis) and sound visualization in a virtual-reality environment by means of
M4CAVE. It describes the procedures and preliminary results of some experiments
in scientific sonification and sound visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007008</id><created>2000-07-06</created><authors><author><keyname>Brand</keyname><forenames>M. G. J. van den</forenames></author><author><keyname>Heering</keyname><forenames>J.</forenames></author><author><keyname>Klint</keyname><forenames>P.</forenames></author><author><keyname>Olivier</keyname><forenames>P. A.</forenames></author></authors><title>Compiling Language Definitions: The ASF+SDF Compiler</title><categories>cs.PL cs.SE</categories><comments>36 pages, 5 figures</comments><report-no>SEN-R0014</report-no><acm-class>D.3.1; D.3.2; D.3.4; F.4.2</acm-class><journal-ref>ACM Transactions on Programming Languages and Systems 24 4 (July
  2002) 334-368</journal-ref><abstract>  The ASF+SDF Meta-Environment is an interactive language development
environment whose main application areas are definition of domain-specific
languages, generation of program analysis and transformation tools, production
of software renovation tools, and general specification and prototyping. It
uses conditional rewrite rules to define the dynamic semantics and other
tool-oriented aspects of languages, so the effectiveness of the generated tools
is critically dependent on the quality of the rewrite rule implementation.
  The ASF+SDF rewrite rule compiler generates C code, thus taking advantage of
C's portability and the sophisticated optimization capabilities of current C
compilers as well as avoiding potential abstract machine interface bottlenecks.
It can handle large (10 000+ rule) language definitions and uses an efficient
run-time storage scheme capable of handling large (1 000 000+ node) terms. Term
storage uses maximal subterm sharing (hash-consing), which turns out to be more
effective in the case of ASF+SDF than in Lisp or SML. Extensive benchmarking
has shown the time and space performance of the generated code to be as good as
or better than that of the best current rewrite rule and functional language
compilers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007009</id><created>2000-07-06</created><authors><author><keyname>Daciuk</keyname><forenames>Jan</forenames></author><author><keyname>Mihov</keyname><forenames>Stoyan</forenames></author><author><keyname>Watson</keyname><forenames>Bruce</forenames></author><author><keyname>Watson</keyname><forenames>Richard</forenames></author></authors><title>Incremental construction of minimal acyclic finite-state automata</title><categories>cs.CL</categories><comments>14 pages, 7 figures</comments><acm-class>I.2.7</acm-class><journal-ref>Computational Linguistics, Vol. 26, Number 1, March 2000</journal-ref><abstract>  In this paper, we describe a new method for constructing minimal,
deterministic, acyclic finite-state automata from a set of strings. Traditional
methods consist of two phases: the first to construct a trie, the second one to
minimize it. Our approach is to construct a minimal automaton in a single phase
by adding new strings one by one and minimizing the resulting automaton
on-the-fly. We present a general algorithm as well as a specialization that
relies upon the lexicographical ordering of the input strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007010</id><created>2000-07-07</created><authors><author><keyname>Escudero</keyname><forenames>Gerard</forenames></author><author><keyname>Marquez</keyname><forenames>Lluis</forenames></author><author><keyname>Rigau</keyname><forenames>German</forenames></author></authors><title>Boosting Applied to Word Sense Disambiguation</title><categories>cs.CL cs.AI</categories><comments>12 pages</comments><acm-class>I.2.7;I.2.6</acm-class><journal-ref>Proceedings of the 11th European Conference on Machine Learning,
  ECML'2000 pp. 129-141</journal-ref><abstract>  In this paper Schapire and Singer's AdaBoost.MH boosting algorithm is applied
to the Word Sense Disambiguation (WSD) problem. Initial experiments on a set of
15 selected polysemous words show that the boosting approach surpasses Naive
Bayes and Exemplar-based approaches, which represent state-of-the-art accuracy
on supervised WSD. In order to make boosting practical for a real learning
domain of thousands of words, several ways of accelerating the algorithm by
reducing the feature space are studied. The best variant, which we call
LazyBoosting, is tested on the largest sense-tagged corpus available containing
192,800 examples of the 191 most frequent and ambiguous English words. Again,
boosting compares favourably to the other benchmark algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007011</id><created>2000-07-07</created><authors><author><keyname>Escudero</keyname><forenames>Gerard</forenames></author><author><keyname>Marquez</keyname><forenames>Lluis</forenames></author><author><keyname>Rigau</keyname><forenames>German</forenames></author></authors><title>Naive Bayes and Exemplar-Based approaches to Word Sense Disambiguation
  Revisited</title><categories>cs.CL cs.AI</categories><comments>5 pages</comments><acm-class>I.2.7;I.2.6</acm-class><journal-ref>Proceedings of the 14th European Conference on Artificial
  Intelligence, ECAI'2000 pp. 421-425</journal-ref><abstract>  This paper describes an experimental comparison between two standard
supervised learning methods, namely Naive Bayes and Exemplar-based
classification, on the Word Sense Disambiguation (WSD) problem. The aim of the
work is twofold. Firstly, it attempts to contribute to clarify some confusing
information about the comparison between both methods appearing in the related
literature. In doing so, several directions have been explored, including:
testing several modifications of the basic learning algorithms and varying the
feature space. Secondly, an improvement of both algorithms is proposed, in
order to deal with large attribute sets. This modification, which basically
consists in using only the positive information appearing in the examples,
allows to improve greatly the efficiency of the methods, with no loss in
accuracy. The experiments have been performed on the largest sense-tagged
corpus available containing the most frequent and ambiguous English words.
Results show that the Exemplar-based approach to WSD is generally superior to
the Bayesian approach, especially when a specific metric for dealing with
symbolic attributes is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007012</id><created>2000-07-07</created><authors><author><keyname>Wolinski</keyname><forenames>Francis</forenames></author><author><keyname>Vichot</keyname><forenames>Frantz</forenames></author><author><keyname>Stricker</keyname><forenames>Mathieu</forenames></author></authors><title>Using Learning-based Filters to Detect Rule-based Filtering Obsolescence</title><categories>cs.CL cs.AI</categories><comments>13 pages, 12 figures, Content-based Multimedia Information Access,
  RIAO 2000</comments><acm-class>H.3.3; I.2.6</acm-class><abstract>  For years, Caisse des Depots et Consignations has produced information
filtering applications. To be operational, these applications require high
filtering performances which are achieved by using rule-based filters. With
this technique, an administrator has to tune a set of rules for each topic.
However, filters become obsolescent over time. The decrease of their
performances is due to diachronic polysemy of terms that involves a loss of
precision and to diachronic polymorphism of concepts that involves a loss of
recall.
  To help the administrator to maintain his filters, we have developed a method
which automatically detects filtering obsolescence. It consists in making a
learning-based control filter using a set of documents which have already been
categorised as relevant or not relevant by the rule-based filter. The idea is
to supervise this filter by processing a differential comparison of its
outcomes with those of the control one.
  This method has many advantages. It is simple to implement since the training
set used by the learning is supplied by the rule-based filter. Thus, both the
making and the use of the control filter are fully automatic. With automatic
detection of obsolescence, learning-based filtering finds a rich application
which offers interesting prospects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007013</id><created>2000-07-07</created><authors><author><keyname>Penn</keyname><forenames>Gerald</forenames></author></authors><title>Applying Constraint Handling Rules to HPSG</title><categories>cs.CL cs.PL</categories><comments>To appear, Proceedings of First Workshop on Rule-Based Constraint
  Reasoning and Programming, CL2000; 14 pages</comments><acm-class>I.2.7; D.3.3</acm-class><abstract>  Constraint Handling Rules (CHR) have provided a realistic solution to an
over-arching problem in many fields that deal with constraint logic
programming: how to combine recursive functions or relations with constraints
while avoiding non-termination problems. This paper focuses on some other
benefits that CHR, specifically their implementation in SICStus Prolog, have
provided to computational linguists working on grammar design tools. CHR rules
are applied by means of a subsumption check and this check is made only when
their variables are instantiated or bound. The former functionality is at best
difficult to simulate using more primitive coroutining statements such as
SICStus when/2, and the latter simply did not exist in any form before CHR.
  For the sake of providing a case study in how these can be applied to grammar
development, we consider the Attribute Logic Engine (ALE), a Prolog
preprocessor for logic programming with typed feature structures, and its
extension to a complete grammar development system for Head-driven Phrase
Structure Grammar (HPSG), a popular constraint-based linguistic theory that
uses typed feature structures. In this context, CHR can be used not only to
extend the constraint language of feature structure descriptions to include
relations in a declarative way, but also to provide support for constraints
with complex antecedents and constraints on the co-occurrence of feature values
that are necessary to interpret the type system of HPSG properly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007014</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007014</id><created>2000-07-08</created><authors><author><keyname>O'Donnell</keyname><forenames>Michael J.</forenames></author><author><keyname>Bisnovatyi</keyname><forenames>Ilia</forenames></author></authors><title>The Sound Manifesto</title><categories>cs.SD</categories><comments>To appear in the conference on Critical Technologies for the Future
  of Computing, part of SPIE's International Symposium on Optical Science and
  Technology, 30 July to 4 August 2000, San Diego, CA</comments><acm-class>H.5.2; H.5.5; H.5.1</acm-class><doi>10.1117/12.409214</doi><abstract>  Computing practice today depends on visual output to drive almost all user
interaction. Other senses, such as audition, may be totally neglected, or used
tangentially, or used in highly restricted specialized ways. We have excellent
audio rendering through D-A conversion, but we lack rich general facilities for
modeling and manipulating sound comparable in quality and flexibility to
graphics. We need co-ordinated research in several disciplines to improve the
use of sound as an interactive information channel.
  Incremental and separate improvements in synthesis, analysis, speech
processing, audiology, acoustics, music, etc. will not alone produce the
radical progress that we seek in sonic practice. We also need to create a new
central topic of study in digital audio research. The new topic will assimilate
the contributions of different disciplines on a common foundation. The key
central concept that we lack is sound as a general-purpose information channel.
We must investigate the structure of this information channel, which is driven
by the co-operative development of auditory perception and physical sound
production. Particular audible encodings, such as speech and music, illuminate
sonic information by example, but they are no more sufficient for a
characterization than typography is sufficient for a characterization of visual
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007015</id><created>2000-07-10</created><authors><author><keyname>Herman</keyname><forenames>Ted</forenames></author></authors><title>Phase Clocks for Transient Fault Repair</title><categories>cs.DC</categories><comments>22 pages, LaTeX</comments><report-no>TR99-08</report-no><acm-class>C.2.4; D.4.5</acm-class><abstract>  Phase clocks are synchronization tools that implement a form of logical time
in distributed systems. For systems tolerating transient faults by self-repair
of damaged data, phase clocks can enable reasoning about the progress of
distributed repair procedures. This paper presents a phase clock algorithm
suited to the model of transient memory faults in asynchronous systems with
read/write registers. The algorithm is self-stabilizing and guarantees accuracy
of phase clocks within O(k) time following an initial state that is k-faulty.
Composition theorems show how the algorithm can be used for the timing of
distributed procedures that repair system outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007016</id><created>2000-07-11</created><authors><author><keyname>Stricker</keyname><forenames>Mathieu</forenames></author><author><keyname>Vichot</keyname><forenames>Frantz</forenames></author><author><keyname>Dreyfus</keyname><forenames>Gerard</forenames></author><author><keyname>Wolinski</keyname><forenames>Francis</forenames></author></authors><title>Two Steps Feature Selection and Neural Network Classification for the
  TREC-8 Routing</title><categories>cs.CL cs.AI</categories><comments>5 pages, 1 figure, Eighth International Text REtrieval Conference
  (TREC-8)</comments><acm-class>H.3.3; K.3.2</acm-class><abstract>  For the TREC-8 routing, one specific filter is built for each topic. Each
filter is a classifier trained to recognize the documents that are relevant to
the topic. When presented with a document, each classifier estimates the
probability for the document to be relevant to the topic for which it has been
trained. Since the procedure for building a filter is topic-independent, the
system is fully automatic.
  By making use of a sample of documents that have previously been evaluated as
relevant or not relevant to a particular topic, a term selection is performed,
and a neural network is trained. Each document is represented by a vector of
frequencies of a list of selected terms. This list depends on the topic to be
filtered; it is constructed in two steps. The first step defines the
characteristic words used in the relevant documents of the corpus; the second
one chooses, among the previous list, the most discriminant ones. The length of
the vector is optimized automatically for each topic. At the end of the term
selection, a vector of typically 25 words is defined for the topic, so that
each document which has to be processed is represented by a vector of term
frequencies.
  This vector is subsequently input to a classifier that is trained from the
same sample. After training, the classifier estimates for each document of a
test set its probability of being relevant; for submission to TREC, the top
1000 documents are ranked in order of decreasing relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007017</id><created>2000-07-13</created><authors><author><keyname>Schweiger</keyname><forenames>R.</forenames><affiliation>University Giessen</affiliation></author><author><keyname>Hoelzer</keyname><forenames>S.</forenames><affiliation>Giessen</affiliation></author><author><keyname>Dudeck</keyname><forenames>J.</forenames><affiliation>Giessen</affiliation></author></authors><title>Fuzzy data: XML may handle it</title><categories>cs.IR</categories><comments>15 pages, 6 figures</comments><acm-class>H3.2</acm-class><abstract>  Data modeling is one of the most difficult tasks in application engineering.
The engineer must be aware of the use cases and the required application
services and at a certain point of time he has to fix the data model which
forms the base for the application services. However, once the data model has
been fixed it is difficult to consider changing needs. This might be a problem
in specific domains, which are as dynamic as the healthcare domain. With fuzzy
data we address all those data that are difficult to organize in a single
database. In this paper we discuss a gradual and pragmatic approach that uses
the XML technology to conquer more model flexibility. XML may provide the clue
between unstructured text data and structured database solutions and shift the
paradigm from &quot;organizing the data along a given model&quot; towards &quot;organizing the
data along user requirements&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007018</id><created>2000-07-13</created><authors><author><keyname>Zavrel</keyname><forenames>Jakub</forenames></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames></author></authors><title>Bootstrapping a Tagged Corpus through Combination of Existing
  Heterogeneous Taggers</title><categories>cs.CL</categories><comments>4 pages</comments><acm-class>I.2.7; I.2.6</acm-class><journal-ref>Proceedings of the 2nd International Conference on Language
  Resources and Evaluation (LREC 2000), pp. 17--20</journal-ref><abstract>  This paper describes a new method, Combi-bootstrap, to exploit existing
taggers and lexical resources for the annotation of corpora with new tagsets.
Combi-bootstrap uses existing resources as features for a second level machine
learning module, that is trained to make the mapping to the new tagset on a
very small sample of annotated corpus material. Experiments show that
Combi-bootstrap: i) can integrate a wide variety of existing resources, and ii)
achieves much higher accuracy (up to 44.7 % error reduction) than both the best
single tagger and an ensemble tagger constructed out of the same small training
sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007019</id><created>2000-07-13</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Examples, Counterexamples, and Enumeration Results for Foldings and
  Unfoldings between Polygons and Polytopes</title><categories>cs.CG cs.DM</categories><comments>54 pages, 33 figures</comments><report-no>Smith Technical Report 069</report-no><acm-class>F.2.2</acm-class><abstract>  We investigate how to make the surface of a convex polyhedron (a polytope) by
folding up a polygon and gluing its perimeter shut, and the reverse process of
cutting open a polytope and unfolding it to a polygon. We explore basic
enumeration questions in both directions: Given a polygon, how many foldings
are there? Given a polytope, how many unfoldings are there to simple polygons?
Throughout we give special attention to convex polygons, and to regular
polygons. We show that every convex polygon folds to an infinite number of
distinct polytopes, but that their number of combinatorially distinct gluings
is polynomial. There are, however, simple polygons with an exponential number
of distinct gluings.
  In the reverse direction, we show that there are polytopes with an
exponential number of distinct cuttings that lead to simple unfoldings. We
establish necessary conditions for a polytope to have convex unfoldings,
implying, for example, that among the Platonic solids, only the tetrahedron has
a convex unfolding. We provide an inventory of the polytopes that may unfold to
regular polygons, showing that, for n&gt;6, there is essentially only one class of
such polytopes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007020</id><created>2000-07-13</created><updated>2000-07-14</updated><authors><author><keyname>Givan</keyname><forenames>Robert</forenames></author><author><keyname>McAllester</keyname><forenames>David</forenames></author></authors><title>Polynomial-time Computation via Local Inference Relations</title><categories>cs.LO cs.AI cs.PL</categories><comments>22 pages. Appeared in Knowledge Representation and Reasoning,1993.
  Submitted to ACM Transactions on Computational Logic Correction: original
  conference appearance was 1992, not 1993</comments><acm-class>I.2.2; I.2.3; I.2.4; F.4.m</acm-class><abstract>  We consider the concept of a local set of inference rules. A local rule set
can be automatically transformed into a rule set for which bottom-up evaluation
terminates in polynomial time. The local-rule-set transformation gives
polynomial-time evaluation strategies for a large variety of rule sets that
cannot be given terminating evaluation strategies by any other known automatic
technique. This paper discusses three new results. First, it is shown that
every polynomial-time predicate can be defined by an (unstratified) local rule
set. Second, a new machine-recognizable subclass of the local rule sets is
identified. Finally we show that locality, as a property of rule sets, is
undecidable in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007021</id><created>2000-07-13</created><updated>2000-09-03</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>PushPush and Push-1 are NP-hard in 2D</title><categories>cs.CG cs.DM</categories><comments>10 pages, 11 figures. Corrects an error in the conference version:
  Proc. of the 12th Canadian Conference on Computational Geometry, August 2000,
  pp. 211-219</comments><acm-class>F.2.2</acm-class><abstract>  We prove that two pushing-blocks puzzles are intractable in 2D. One of our
constructions improves an earlier result that established intractability in 3D
[OS99] for a puzzle inspired by the game PushPush. The second construction
answers a question we raised in [DDO00] for a variant we call Push-1. Both
puzzles consist of unit square blocks on an integer lattice; all blocks are
movable. An agent may push blocks (but never pull them) in attempting to move
between given start and goal positions. In the PushPush version, the agent can
only push one block at a time, and moreover when a block is pushed it slides
the maximal extent of its free range. In the Push-1 version, the agent can only
push one block one square at a time, the minimal extent---one square. Both
NP-hardness proofs are by reduction from SAT, and rely on a common
construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007022</id><created>2000-07-13</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Day</keyname><forenames>David</forenames></author><author><keyname>Garofolo</keyname><forenames>John</forenames></author><author><keyname>Henderson</keyname><forenames>John</forenames></author><author><keyname>Laprun</keyname><forenames>Christophe</forenames></author><author><keyname>Liberman</keyname><forenames>Mark</forenames></author></authors><title>ATLAS: A flexible and extensible architecture for linguistic annotation</title><categories>cs.CL</categories><comments>8 pages, 9 figures</comments><acm-class>E.2; H.2.1; H.3.3; H.3.4; H.3.7; I.2.7</acm-class><journal-ref>Proceedings of the Second International Conference on Language
  Resources and Evaluation, pp. 1699-1706, Paris: European Language Resources
  Association, 2000</journal-ref><abstract>  We describe a formal model for annotating linguistic artifacts, from which we
derive an application programming interface (API) to a suite of tools for
manipulating these annotations. The abstract logical model provides for a range
of storage formats and promotes the reuse of tools that interact through this
API. We focus first on ``Annotation Graphs,'' a graph model for annotations on
linear signals (such as text and speech) indexed by intervals, for which
efficient database storage and querying techniques are applicable. We note how
a wide range of existing annotated corpora can be mapped to this annotation
graph model. This model is then generalized to encompass a wider variety of
linguistic ``signals,'' including both naturally occuring phenomena (as
recorded in images, video, multi-modal interactions, etc.), as well as the
derived resources that are increasingly important to the engineering of natural
language processing systems (such as word lists, dictionaries, aligned
bilingual corpora, etc.). We conclude with a review of the current efforts
towards implementing key pieces of this architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007023</id><created>2000-07-13</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Buneman</keyname><forenames>Peter</forenames></author><author><keyname>Tan</keyname><forenames>Wang-Chiew</forenames></author></authors><title>Towards a query language for annotation graphs</title><categories>cs.CL cs.DB</categories><comments>8 pages, 10 figures</comments><acm-class>E.1; E.2; H.2.1; H.2.3; H.2.8; H.3.1; H.3.3; I.2.7</acm-class><journal-ref>Proceedings of the Second International Conference on Language
  Resources and Evaluation, pp. 807-814, Paris: European Language Resources
  Association, 2000</journal-ref><abstract>  The multidimensional, heterogeneous, and temporal nature of speech databases
raises interesting challenges for representation and query. Recently,
annotation graphs have been proposed as a general-purpose representational
framework for speech databases. Typical queries on annotation graphs require
path expressions similar to those used in semistructured query languages.
However, the underlying model is rather different from the customary graph
models for semistructured data: the graph is acyclic and unrooted, and both
temporal and inclusion relationships are important. We develop a query language
and describe optimization techniques for an underlying relational
representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007024</id><created>2000-07-13</created><authors><author><keyname>Graff</keyname><forenames>David</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>Many uses, many annotations for large speech corpora: Switchboard and
  TDT as case studies</title><categories>cs.CL</categories><comments>7 pages, 2 figures</comments><acm-class>E.2; H.2.5; I.2.7</acm-class><journal-ref>Proceedings of the Second International Conference on Language
  Resources and Evaluation, pp. 427-433, Paris: European Language Resources
  Association, 2000</journal-ref><abstract>  This paper discusses the challenges that arise when large speech corpora
receive an ever-broadening range of diverse and distinct annotations. Two case
studies of this process are presented: the Switchboard Corpus of telephone
conversations and the TDT2 corpus of broadcast news. Switchboard has undergone
two independent transcriptions and various types of additional annotation, all
carried out as separate projects that were dispersed both geographically and
chronologically. The TDT2 corpus has also received a variety of annotations,
but all directly created or managed by a core group. In both cases, issues
arise involving the propagation of repairs, consistency of references, and the
ability to integrate annotations having different formats and levels of detail.
We describe a general framework whereby these issues can be addressed
successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007025</id><created>2000-07-13</created><authors><author><keyname>Glasser</keyname><forenames>Christian</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author></authors><title>A Moment of Perfect Clarity I: The Parallel Census Technique</title><categories>cs.CC</categories><comments>8 pages</comments><acm-class>F.1.3; F.1.2</acm-class><abstract>  We discuss the history and uses of the parallel census technique---an elegant
tool in the study of certain computational objects having polynomially bounded
census functions. A sequel will discuss advances (including Cai, Naik, and
Sivakumar [CNS95] and Glasser [Gla00]), some related to the parallel census
technique and some due to other approaches, in the complexity-class collapses
that follow if NP has sparse hard sets under reductions weaker than (full)
truth-table reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007026</id><created>2000-07-13</created><authors><author><keyname>Ansari</keyname><forenames>Suhail</forenames></author><author><keyname>Kohavi</keyname><forenames>Ron</forenames></author><author><keyname>Mason</keyname><forenames>Llew</forenames></author><author><keyname>Zheng</keyname><forenames>Zijian</forenames></author></authors><title>Integrating E-Commerce and Data Mining: Architecture and Challenges</title><categories>cs.LG cs.AI cs.CV cs.DB</categories><comments>KDD workshop: WebKDD 2000</comments><acm-class>I.2.6;H.2.8</acm-class><journal-ref>WEBKDD'2000 workshop: Web Mining for E-Commerce -- Challenges and
  Opportunities</journal-ref><abstract>  We show that the e-commerce domain can provide all the right ingredients for
successful data mining and claim that it is a killer domain for data mining. We
describe an integrated architecture, based on our expe-rience at Blue Martini
Software, for supporting this integration. The architecture can dramatically
reduce the pre-processing, cleaning, and data understanding effort often
documented to take 80% of the time in knowledge discovery projects. We
emphasize the need for data collection at the application server layer (not the
web server) in order to support logging of data and metadata that is essential
to the discovery process. We describe the data transformation bridges required
from the transaction processing systems and customer event streams (e.g.,
clickstreams) to the data warehouse. We detail the mining workbench, which
needs to provide multiple views of the data through reporting, data mining
algorithms, visualization, and OLAP. We con-clude with a set of challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007027</id><created>2000-07-14</created><authors><author><keyname>Frumkin</keyname><forenames>Michael A.</forenames></author><author><keyname>Van der Wijngaart</keyname><forenames>Rob F.</forenames></author></authors><title>Efficient cache use for stencil operations on structured discretization
  grids</title><categories>cs.PF cs.CC</categories><comments>tex .tar.gz file, including ps file, 16 pagest, 5 figures, 2
  Appendicies</comments><acm-class>C.4;B.8</acm-class><abstract>  We derive tight bounds on cache misses for evaluation of explicit stencil
operators on structured grids. Our lower bound is based on the isoperimetrical
property of the discrete octahedron. Our upper bound is based on good surface
to volume ratio of a parallelepiped spanned by a reduced basis of the inter-
ference lattice of a grid. Measurements show that our algorithm typically
reduces the number of cache misses by factor of three relative to a compiler
optimized code. We show that stencil calculations on grids whose interference
lattice have a short vector feature abnormally high numbers of cache misses. We
call such grids unfavorable and suggest to avoid these in computations by
appropriate padding. By direct measurements on MIPS R10000 we show a good
correlation of abnormally high cache misses and unfavorable three-dimensional
grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007028</id><created>2000-07-18</created><authors><author><keyname>Lin</keyname><forenames>Po-Han</forenames></author></authors><title>Base Encryption: Dynamic algorithms, Keys, and Symbol Set</title><categories>cs.CR cs.CC</categories><comments>html page</comments><acm-class>E, E.3</acm-class><abstract>  All the current modern encryption algorithms utilize fixed symbols for
plaintext and cyphertext. What I mean by fixed is that there is a set and
limited number of symbols to represent the characters, numbers, and
punctuations. In addition, they are usually the same (the plaintext symbols
have the same and equivalent counterpart in the cyphertext symbols). Almost all
the encryption algorithms rely on a predefined keyspace and length for the
encryption/decription keys, and it is usually fixed (number of bits). In
addition, the algorithms used by the encryptions are static. There is a
predefined number of operatiors, and a predefined order (loops included) of
operations. The algorithm stays the same, and the plaintext and cyphertext
along with the key are churned through this cypherblock.
  Base Encryption does the opposite: It utilizes the novel concepts of base
conversion, symbol remapping, and dynamic algorithms (dynamic operators and
dynamic operations). Base Encryption solves the weakness in todays encryption
schemes, namely... Fixed symbols (base) Fixed keylengths Fixed algorithms
(fixed number of operations and operators)
  Unique features... Immune from plain-text-attacks. Immune from
brute-force-attacks. Can utilize throwaway algorithms (as opposed to throw away
keys). Plug-And-Play engine (other cyphers can be augmentated to it)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007029</id><created>2000-07-18</created><authors><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author></authors><title>Dimension-Dependent behavior in the satisfability of random k-Horn
  formulae</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><abstract>  We determine the asymptotical satisfiability probability of a random
at-most-k-Horn formula, via a probabilistic analysis of a simple version,
called PUR, of positive unit resolution. We show that for k=k(n)-&gt;oo the
problem can be ``reduced'' to the case k(n)=n, that was solved in
cs.DS/9912001. On the other hand, in the case k= a constant the behavior of PUR
is modeled by a simple queuing chain, leading to a closed-form solution when
k=2. Our analysis predicts an ``easy-hard-easy'' pattern in this latter case.
Under a rescaled parameter, the graphs of satisfaction probability
corresponding to finite values of k converge to the one for the uniform case, a
``dimension-dependent behavior'' similar to the one found experimentally by
Kirkpatrick and Selman (Science'94) for k-SAT. The phenomenon is qualitatively
explained by a threshold property for the number of iterations of PUR makes on
random satisfiable Horn formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007030</id><created>2000-07-19</created><updated>2002-09-02</updated><authors><author><keyname>Griffioen</keyname><forenames>W. O. D.</forenames></author><author><keyname>Vaandrager</keyname><forenames>F. W.</forenames></author></authors><title>A theory of normed simulations</title><categories>cs.LO</categories><comments>31 pages, 10figures</comments><report-no>CSI-R0013</report-no><acm-class>F.1.1; F.3.1</acm-class><abstract>  In existing simulation proof techniques, a single step in a lower-level
specification may be simulated by an extended execution fragment in a
higher-level one. As a result, it is cumbersome to mechanize these techniques
using general purpose theorem provers. Moreover, it is undecidable whether a
given relation is a simulation, even if tautology checking is decidable for the
underlying specification logic. This paper introduces various types of normed
simulations. In a normed simulation, each step in a lower-level specification
can be simulated by at most one step in the higher-level one, for any related
pair of states. In earlier work we demonstrated that normed simulations are
quite useful as a vehicle for the formalization of refinement proofs via
theorem provers. Here we show that normed simulations also have pleasant
theoretical properties: (1) under some reasonable assumptions, it is decidable
whether a given relation is a normed forward simulation, provided tautology
checking is decidable for the underlying logic; (2) at the semantic level,
normed forward and backward simulations together form a complete proof method
for establishing behavior inclusion, provided that the higher-level
specification has finite invisible nondeterminism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007031</id><created>2000-07-21</created><updated>2000-07-24</updated><authors><author><keyname>Kromer</keyname><forenames>Victor</forenames></author></authors><title>Parameter-free Model of Rank Polysemantic Distribution</title><categories>cs.CL</categories><comments>3 pages, no figures</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 4th conference of the International
  Quantitative Linguistics Association (QUALICO 2000). Prague, August 24-26,
  2000. P. 21-22.The full version (in Russian) is available in Web Journal
  FCCL. See URL http://fccl.ksu.ru/fcclpap.htm</journal-ref><abstract>  A model of rank polysemantic distribution with a minimal number of fitting
parameters is offered. In an ideal case a parameter-free description of the
dependence on the basis of one or several immediate features of the
distribution is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007032</id><created>2000-07-21</created><authors><author><keyname>Georgatos</keyname><forenames>Konstantinos</forenames></author></authors><title>Knowledge on Treelike Spaces</title><categories>cs.LO cs.AI</categories><comments>31 pages</comments><acm-class>F.4.1;I.2.0</acm-class><journal-ref>Studia Logica, 1(59), 1997</journal-ref><abstract>  This paper presents a bimodal logic for reasoning about knowledge during
knowledge acquisition. One of the modalities represents (effort during)
non-deterministic time and the other represents knowledge. The semantics of
this logic are tree-like spaces which are a generalization of semantics used
for modeling branching time and historical necessity. A finite system of axiom
schemes is shown to be canonically complete for the formentioned spaces. A
characterization of the satisfaction relation implies the small model property
and decidability for this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007033</id><created>2000-07-21</created><authors><author><keyname>Georgatos</keyname><forenames>Konstantinos</forenames></author></authors><title>To Preference via Entrenchment</title><categories>cs.LO cs.AI</categories><comments>16 pages</comments><acm-class>F.4.1;I.2.3</acm-class><journal-ref>Annals Of Pure And Applied Logic, (96)1-3, pages 141-155, 1999</journal-ref><abstract>  We introduce a simple generalization of Gardenfors and Makinson's epistemic
entrenchment called partial entrenchment. We show that preferential inference
can be generated as the sceptical counterpart of an inference mechanism defined
directly on partial entrenchment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007034</id><created>2000-07-25</created><authors><author><keyname>Png</keyname><forenames>Ivan</forenames></author></authors><title>The Competitiveness of On-Line vis-a-vis Conventional Retailing: A
  Preliminary Study</title><categories>cs.OH</categories><comments>11th NEC Research Symposium</comments><acm-class>K.4.4</acm-class><abstract>  Previous research has directly studied whether on-line retailing is more
competitive than conventional retail markets. The evidence from books and music
CDs is mixed. Here, I use an indirect approach to compare the competitiveness
of on-line with conventional markets. Focusing on the retail market for books,
I identify a peculiarity in the pricing of bestsellers relative to other
titles. Supposing that competitive barriers are lower in on-line retailing, I
analyze how the lower barriers would affect the relative pricing of
bestsellers. The empirical data indicates that on-line retailing is more
competitive than conventional retailing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007035</id><created>2000-07-25</created><authors><author><keyname>Daude</keyname><forenames>J.</forenames></author><author><keyname>Padro</keyname><forenames>L.</forenames></author><author><keyname>Rigau</keyname><forenames>G.</forenames></author></authors><title>Mapping WordNets Using Structural Information</title><categories>cs.CL</categories><comments>8 pages, uses epsfig. To appear in ACL'2000 proceedings</comments><acm-class>I.2.7</acm-class><journal-ref>38th Anual Meeting of the Association for Computational
  Linguistics (ACL'2000). Hong Kong, October 2000.</journal-ref><abstract>  We present a robust approach for linking already existing lexical/semantic
hierarchies. We used a constraint satisfaction algorithm (relaxation labeling)
to select --among a set of candidates-- the node in a target taxonomy that
bests matches each node in a source taxonomy. In particular, we use it to map
the nominal part of WordNet 1.5 onto WordNet 1.6, with a very high precision
and a very low remaining ambiguity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007036</id><created>2000-07-25</created><authors><author><keyname>Martins</keyname><forenames>J. F.</forenames><affiliation>EST-IPS, Setubal</affiliation></author><author><keyname>Dente</keyname><forenames>J. A.</forenames><affiliation>IST, Lisboa</affiliation></author><author><keyname>Pires</keyname><forenames>A. J.</forenames><affiliation>EST-IPS, Setubal</affiliation></author><author><keyname>Mendes</keyname><forenames>R. Vilela</forenames><affiliation>GFM, UL, Lisboa</affiliation></author></authors><title>Language identification of controlled systems: Modelling, control and
  anomaly detection</title><categories>cs.CL</categories><comments>27 pages Latex, 18 figures</comments><acm-class>I.2.8</acm-class><journal-ref>IEEE Trans. in Systems, Man and Cybernetics 31 (2001) 234</journal-ref><abstract>  Formal language techniques have been used in the past to study autonomous
dynamical systems. However, for controlled systems, new features are needed to
distinguish between information generated by the system and input control. We
show how the modelling framework for controlled dynamical systems leads
naturally to a formulation in terms of context-dependent grammars. A learning
algorithm is proposed for on-line generation of the grammar productions, this
formulation being then used for modelling, control and anomaly detection.
Practical applications are described for electromechanical drives. Grammatical
interpolation techniques yield accurate results and the pattern detection
capabilities of the language-based formulation makes it a promising technique
for the early detection of anomalies or faulty behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007037</id><created>2000-07-26</created><authors><author><keyname>Georgatos</keyname><forenames>Konstantinos</forenames></author></authors><title>Knowledge Theoretic Properties of Topological Spaces</title><categories>cs.LO</categories><comments>14 pages</comments><acm-class>F.4.1</acm-class><journal-ref>In Knowledge Representation and Uncertainty. M. Masuch and L.
  Polos, Eds. Lecture Notes in Artificial Intelligence, vol. 808, pages
  147-159, Springer-Verlag, 1994</journal-ref><abstract>  We study the topological models of a logic of knowledge for topological
reasoning, introduced by Larry Moss and Rohit Parikh. Among our results is a
solution of a conjecture by the formentioned authors, finite satisfiability
property and decidability for the theory of topological models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007038</id><created>2000-07-26</created><authors><author><keyname>Georgatos</keyname><forenames>Konstantinos</forenames></author></authors><title>Modal Logics for Topological Spaces</title><categories>cs.LO cs.AI</categories><comments>25 pages, extened abstract of PHD Dissertation</comments><acm-class>F.4.1</acm-class><abstract>  In this thesis we shall present two logical systems, MP and MP, for the
purpose of reasoning about knowledge and effort. These logical systems will be
interpreted in a spatial context and therefore, the abstract concepts of
knowledge and effort will be defined by concrete mathematical concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007039</id><created>2000-07-26</created><authors><author><keyname>Georgatos</keyname><forenames>Konstantinos</forenames></author></authors><title>Ordering-based Representations of Rational Inference</title><categories>cs.LO cs.AI</categories><comments>26 pages, appeared in conference proceedings, contains proofs</comments><acm-class>F.4.1;I.2.3</acm-class><journal-ref>In the Proceedings of the European Workshop on Logics in AI (JELIA
  '96). J.J. Alferes, L.M. Pereira and E. Orlowska, Eds. Lecture Notes in
  Artificial Intelligence, vol. 1126, pages 176-191, Springer-Verlag, 1996</journal-ref><abstract>  Rational inference relations were introduced by Lehmann and Magidor as the
ideal systems for drawing conclusions from a conditional base. However, there
has been no simple characterization of these relations, other than its original
representation by preferential models. In this paper, we shall characterize
them with a class of total preorders of formulas by improving and extending
Gardenfors and Makinson's results for expectation inference relations. A second
representation is application-oriented and is obtained by considering a class
of consequence operators that grade sets of defaults according to our reliance
on them. The finitary fragment of this class of consequence operators has been
employed by recent default logic formalisms based on maxiconsistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007040</id><created>2000-07-26</created><authors><author><keyname>Georgatos</keyname><forenames>Konstantinos</forenames></author></authors><title>Entrenchment Relations: A Uniform Approach to Nonmonotonicity</title><categories>cs.LO cs.AI</categories><comments>22 pages, a paper in preliminary form that appeared later in
  conference proceedings</comments><acm-class>F.4.1;I.2.3</acm-class><journal-ref>In the Proceedings of the International Joint Conference on
  Qualitative and Quantitative Practical Reasoning (ESCQARU/FAPR 97), Lecture
  Notes in Artificial Intelligence, vol. 1244, pages 282-297, Springer-Verlag,
  1997</journal-ref><abstract>  We show that Gabbay's nonmonotonic consequence relations can be reduced to a
new family of relations, called entrenchment relations. Entrenchment relations
provide a direct generalization of epistemic entrenchment and expectation
ordering introduced by Gardenfors and Makinson for the study of belief revision
and expectation inference, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007041</id><created>2000-07-26</created><authors><author><keyname>Amati</keyname><forenames>Gianni</forenames></author><author><keyname>Georgatos</keyname><forenames>Konstantinos</forenames></author></authors><title>Relevance as Deduction: A Logical View of Information Retrieval</title><categories>cs.IR cs.LO</categories><comments>6 pages, Abstract</comments><report-no>TR-1996-29</report-no><acm-class>I.2.3;H.3.0;H.3.3</acm-class><journal-ref>In F. Crestani and M. Lalmas, editors, Proceedings of the Second
  Workshop on Information Retrieval, Uncertainty and Logic WIRUL'96, pages
  21--26. University of Glasgow, Glasgow, Scotland, 1996</journal-ref><abstract>  The problem of Information Retrieval is, given a set of documents D and a
query q, providing an algorithm for retrieving all documents in D relevant to
q. However, retrieval should depend and be updated whenever the user is able to
provide as an input a preferred set of relevant documents; this process is
known as em relevance feedback. Recent work in IR has been paying great
attention to models which employ a logical approach; the advantage being that
one can have a simple computable characterization of retrieval on the basis of
a pure logical analysis of retrieval. Most of the logical models make use of
probabilities or similar belief functions in order to introduce the inductive
component whereby uncertainty is treated. Their general paradigm is the
following: em find the nature of conditional $d\imp q$ and then define a
probability on the top of it. We just reverse this point of view; first use the
numerical information, frequencies or probabilities, then define your own
logical consequence. More generally, we claim that retrieval is a form of
deduction. We introduce a simple but powerful logical framework of relevance
feedback, derived from the well founded area of nonmonotonic logic. This
description can help us evaluate, describe and compare from a theoretical point
of view previous approaches based on conditionals or probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007042</id><created>2000-07-29</created><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Computational Geometry Column 39</title><categories>cs.CG cs.DM</categories><comments>4 pages, 2 figures. To appear in SIGACT News and in Int. J. Comp.
  Geom. Appl</comments><acm-class>F.2.2</acm-class><journal-ref>SIGACT News 31(3): 47-49 (2000)</journal-ref><abstract>  The resolution of a decades-old open problem is described: polygonal chains
cannot lock in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007043</id><created>2000-07-30</created><authors><author><keyname>Nath</keyname><forenames>Suman Kumar</forenames></author><author><keyname>Chowdhury</keyname><forenames>Rezaul Alam</forenames></author><author><keyname>Kaykobad</keyname><forenames>M.</forenames></author></authors><title>Min-Max Fine Heaps</title><categories>cs.DS</categories><comments>6 pages, pdf file</comments><acm-class>E.1</acm-class><abstract>  In this paper we present a new data structure for double ended priority
queue, called min-max fine heap, which combines the techniques used in fine
heap and traditional min-max heap. The standard operations on this proposed
structure are also presented, and their analysis indicates that the new
structure outperforms the traditional one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0007044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0007044</id><created>2000-07-31</created><updated>2001-06-11</updated><authors><author><keyname>Gal</keyname><forenames>Avigdor</forenames></author><author><keyname>Eckstein</keyname><forenames>Jonathan</forenames></author></authors><title>Managing Periodically Updated Data in Relational Databases: A Stochastic
  Modeling Approach</title><categories>cs.DB</categories><report-no>RRR-37-2000</report-no><acm-class>H.2.4</acm-class><abstract>  Recent trends in information management involve the periodic transcription of
data onto secondary devices in a networked environment, and the proper
scheduling of these transcriptions is critical for efficient data management.
To assist in the scheduling process, we are interested in modeling the
reduction of consistency over time between a relation and its replica, termed
obsolescence of data. The modeling is based on techniques from the field of
stochastic processes, and provides several stochastic models for content
evolution in the base relations of a database, taking referential integrity
constraints into account. These models are general enough to accommodate most
of the common scenarios in databases, including batch insertions and life spans
both with and without memory. As an initial &quot;proof of concept&quot; of the
applicability of our approach, we validate the insertion portion of our model
framework via experiments with real data feeds. We also discuss a set of
transcription protocols which make use of the proposed stochastic model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008001</id><created>2000-08-01</created><authors><author><keyname>Bryant</keyname><forenames>Randal E.</forenames></author><author><keyname>Velev</keyname><forenames>Miroslav N.</forenames></author></authors><title>Boolean Satisfiability with Transitivity Constraints</title><categories>cs.LO</categories><comments>Submitted to ACM Transactions on Computational Logic</comments><acm-class>I2.3; G2.2</acm-class><abstract>  We consider a variant of the Boolean satisfiability problem where a subset E
of the propositional variables appearing in formula Fsat encode a symmetric,
transitive, binary relation over N elements. Each of these relational
variables, e[i,j], for 1 &lt;= i &lt; j &lt;= N, expresses whether or not the relation
holds between elements i and j. The task is to either find a satisfying
assignment to Fsat that also satisfies all transitivity constraints over the
relational variables (e.g., e[1,2] &amp; e[2,3] ==&gt; e[1,3]), or to prove that no
such assignment exists. Solving this satisfiability problem is the final and
most difficult step in our decision procedure for a logic of equality with
uninterpreted functions. This procedure forms the core of our tool for
verifying pipelined microprocessors.
  To use a conventional Boolean satisfiability checker, we augment the set of
clauses expressing Fsat with clauses expressing the transitivity constraints.
We consider methods to reduce the number of such clauses based on the sparse
structure of the relational variables.
  To use Ordered Binary Decision Diagrams (OBDDs), we show that for some sets
E, the OBDD representation of the transitivity constraints has exponential size
for all possible variable orderings. By considering only those relational
variables that occur in the OBDD representation of Fsat, our experiments show
that we can readily construct an OBDD representation of the relevant
transitivity constraints and thus solve the constrained satisfiability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008002</id><created>2000-08-02</created><authors><author><keyname>Latapy</keyname><forenames>M.</forenames></author><author><keyname>Mantaci</keyname><forenames>R.</forenames></author><author><keyname>Morvan</keyname><forenames>M.</forenames></author><author><keyname>Phan</keyname><forenames>H. D.</forenames></author></authors><title>Structure of some sand pile model</title><categories>cs.DM cs.DS</categories><comments>To appear in Theoretical Computer Science</comments><report-no>LIAFA Technical Report 99/22</report-no><acm-class>G.2.1; J.2</acm-class><abstract>  SPM (Sand Pile Model) is a simple discrete dynamical system used in physics
to represent granular objects. It is deeply related to integer partitions, and
many other combinatorics problems, such as tilings or rewriting systems. The
evolution of the system started with n stacked grains generates a lattice,
denoted by SPM(n). We study here the structure of this lattice. We first
explain how it can be constructed, by showing its strong self-similarity
property. Then, we define SPM(infini), a natural extension of SPM when one
starts with an infinite number of grains. Again, we give an efficient
construction algorithm and a coding of this lattice using a self-similar tree.
The two approaches give different recursive formulae for the cardinal of
SPM(n), where no closed formula have ever been found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008003</id><created>2000-08-07</created><authors><author><keyname>Busemann</keyname><forenames>Stephan</forenames></author></authors><title>Interfacing Constraint-Based Grammars and Generation Algorithms</title><categories>cs.CL</categories><comments>8 pages, uses colacl.sty</comments><acm-class>I.2.7</acm-class><journal-ref>Proc. Workshop on Analysis for Generation, 1st International
  Natural Language Generation Conference, Mitzpe Ramon, Israel, June 12, 2000.
  pp. 14-21</journal-ref><abstract>  Constraint-based grammars can, in principle, serve as the major linguistic
knowledge source for both parsing and generation. Surface generation starts
from input semantics representations that may vary across grammars. For many
declarative grammars, the concept of derivation implicitly built in is that of
parsing. They may thus not be interpretable by a generation algorithm. We show
that linguistically plausible semantic analyses can cause severe problems for
semantic-head-driven approaches for generation (SHDG). We use SeReal, a variant
of SHDG and the DISCO grammar of German as our source of examples. We propose a
new, general approach that explicitly accounts for the interface between the
grammar and the generation algorithm by adding a control-oriented layer to the
linguistic knowledge base that reorganizes the semantics in a way suitable for
generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008004</id><created>2000-08-08</created><authors><author><keyname>Yeh</keyname><forenames>Alexander</forenames></author></authors><title>Comparing two trainable grammatical relations finders</title><categories>cs.CL</categories><comments>5 pages, uses colacl.sty</comments><acm-class>I.2.7</acm-class><journal-ref>18th International Conference on Computational Linguistics (COLING
  2000), pages 1146-1150, Saarbruecken, Germany, July, 2000</journal-ref><abstract>  Grammatical relationships (GRs) form an important level of natural language
processing, but different sets of GRs are useful for different purposes.
Therefore, one may often only have time to obtain a small training corpus with
the desired GR annotations. On such a small training corpus, we compare two
systems. They use different learning techniques, but we find that this
difference by itself only has a minor effect. A larger factor is that in
English, a different GR length measure appears better suited for finding simple
argument GRs than for finding modifier GRs. We also find that partitioning the
data may help memory-based learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008005</id><created>2000-08-08</created><authors><author><keyname>Yeh</keyname><forenames>Alexander</forenames></author></authors><title>More accurate tests for the statistical significance of result
  differences</title><categories>cs.CL</categories><comments>7 pages, uses colacl.sty</comments><acm-class>I.2.7</acm-class><journal-ref>18th International Conference on Computational Linguistics (COLING
  2000), pages 947-953, Saarbruecken, Germany, July, 2000</journal-ref><abstract>  Statistical significance testing of differences in values of metrics like
recall, precision and balanced F-score is a necessary part of empirical natural
language processing. Unfortunately, we find in a set of experiments that many
commonly used tests often underestimate the significance and so are less likely
to detect differences that exist between different techniques. This
underestimation comes from an independence assumption that is often violated.
We point out some useful tests that do not make this assumption, including
computationally-intensive randomization tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008006</id><created>2000-08-09</created><authors><author><keyname>Hazelhurst</keyname><forenames>Scott</forenames></author></authors><title>Algorithms for Analysing Firewall and Router Access Lists</title><categories>cs.NI</categories><comments>12 pages; revised and shortened version appeared in Workshop on
  Dependable IP Systems and Platforms, In Proc ICDSN, June 2000</comments><report-no>TR-Wits-CS-1999-5</report-no><acm-class>C.2.0; E.2.0</acm-class><abstract>  Network firewalls and routers use a rule database to decide which packets
will be allowed from one network onto another. By filtering packets the
firewalls and routers can improve security and performance. However, as the
size of the rule list increases, it becomes difficult to maintain and validate
the rules, and lookup latency may increase significantly. Ordered binary
decision diagrams (BDDs) - a compact method of representing and manipulating
boolean expressions - are a potential method of representing the rules. This
paper presents a new algorithm for representing such lists as a BDD and then
shows how the resulting boolean expression can be used to analyse rule sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008007</id><created>2000-08-09</created><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames></author><author><keyname>Resnik</keyname><forenames>Philip</forenames></author></authors><title>Tagger Evaluation Given Hierarchical Tag Sets</title><categories>cs.CL</categories><comments>preprint is 7 pages, laid out differently than printed version</comments><acm-class>G.3; I.2.7; J.5</acm-class><journal-ref>Computers and the Humanities 34(1-2). Special issue on SENSEVAL.
  pp. 79-84</journal-ref><abstract>  We present methods for evaluating human and automatic taggers that extend
current practice in three ways. First, we show how to evaluate taggers that
assign multiple tags to each test instance, even if they do not assign
probabilities. Second, we show how to accommodate a common property of manually
constructed ``gold standards'' that are typically used for objective
evaluation, namely that there is often more than one correct answer. Third, we
show how to measure performance when the set of possible tags is
tree-structured in an IS-A hierarchy. To illustrate how our methods can be used
to measure inter-annotator agreement, we show how to compute the kappa
coefficient over hierarchical tag sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008008</id><created>2000-08-11</created><updated>2002-04-06</updated><authors><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author></authors><title>On the Average Similarity Degree between Solutions of Random k-SAT and
  Random CSPs</title><categories>cs.AI cs.CC cs.DM</categories><comments>22 pages, the final version to appear in Discrete Applied Mathematics</comments><acm-class>F.2.2; I.2.8</acm-class><journal-ref>Discrete Applied Mathematics, 136(2004):125-149.</journal-ref><abstract>  To study the structure of solutions for random k-SAT and random CSPs, this
paper introduces the concept of average similarity degree to characterize how
solutions are similar to each other. It is proved that under certain
conditions, as r (i.e. the ratio of constraints to variables) increases, the
limit of average similarity degree when the number of variables approaches
infinity exhibits phase transitions at a threshold point, shifting from a
smaller value to a larger value abruptly. For random k-SAT this phenomenon will
occur when k&gt;4 . It is further shown that this threshold point is also a
singular point with respect to r in the asymptotic estimate of the second
moment of the number of solutions. Finally, we discuss how this work is helpful
to understand the hardness of solving random instances and a possible
application of it to the design of search algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008009</id><created>2000-08-15</created><authors><author><keyname>Spiliopoulou</keyname><forenames>Myra</forenames><affiliation>Institue of Information Systems, Humboldt University Berlin</affiliation></author><author><keyname>Pohle</keyname><forenames>Carsten</forenames><affiliation>Institue of Information Systems, Humboldt University Berlin</affiliation></author></authors><title>Data Mining to Measure and Improve the Success of Web Sites</title><categories>cs.LG cs.DB</categories><comments>24 pages, 4 postscript figures and 4 figures containing only text. To
  be published in the Journal of Data Mining and Knowledge Discovery (Kluwer
  Academic Publishers), Special Issue on E-Commerce; subject to some revision</comments><acm-class>I.2.6; H.2.8</acm-class><abstract>  For many companies, competitiveness in e-commerce requires a successful
presence on the web. Web sites are used to establish the company's image, to
promote and sell goods and to provide customer support. The success of a web
site affects and reflects directly the success of the company in the electronic
market. In this study, we propose a methodology to improve the ``success'' of
web sites, based on the exploitation of navigation pattern discovery. In
particular, we present a theory, in which success is modelled on the basis of
the navigation behaviour of the site's users. We then exploit WUM, a navigation
pattern discovery miner, to study how the success of a site is reflected in the
users' behaviour. With WUM we measure the success of a site's components and
obtain concrete indications of how the site should be improved. We report on
our first experiments with an online catalog, the success of which we have
studied. Our mining analysis has shown very promising results, on the basis of
which the site is currently undergoing concrete improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008010</id><created>2000-08-16</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Cortes</keyname><forenames>Carmen</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Dujmovic</keyname><forenames>Vida</forenames></author><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author><author><keyname>Meijer</keyname><forenames>Henk</forenames></author><author><keyname>Overmars</keyname><forenames>Mark</forenames></author><author><keyname>Palop</keyname><forenames>Belen</forenames></author><author><keyname>Ramaswami</keyname><forenames>Suneeta</forenames></author><author><keyname>Toussaint</keyname><forenames>Godfried T.</forenames></author></authors><title>Flipturning polygons</title><categories>cs.CG cs.DM math.MG</categories><comments>26 pages, 32 figures, see also
  http://www.uiuc.edu/~jeffe/pubs/flipturn.html</comments><acm-class>F.2.2; G.2</acm-class><abstract>  A flipturn is an operation that transforms a nonconvex simple polygon into
another simple polygon, by rotating a concavity 180 degrees around the midpoint
of its bounding convex hull edge. Joss and Shannon proved in 1973 that a
sequence of flipturns eventually transforms any simple polygon into a convex
polygon. This paper describes several new results about such flipturn
sequences. We show that any orthogonal polygon is convexified after at most n-5
arbitrary flipturns, or at most 5(n-4)/6 well-chosen flipturns, improving the
previously best upper bound of (n-1)!/2. We also show that any simple polygon
can be convexified by at most n^2-4n+1 flipturns, generalizing earlier results
of Ahn et al. These bounds depend critically on how degenerate cases are
handled; we carefully explore several possibilities. We describe how to
maintain both a simple polygon and its convex hull in O(log^4 n) time per
flipturn, using a data structure of size O(n). We show that although flipturn
sequences for the same polygon can have very different lengths, the shape and
position of the final convex polygon is the same for all sequences and can be
computed in O(n log n) time. Finally, we demonstrate that finding the longest
convexifying flipturn sequence of a simple polygon is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008011</id><created>2000-08-16</created><authors><author><keyname>Zwick</keyname><forenames>Uri</forenames></author></authors><title>All Pairs Shortest Paths using Bridging Sets and Rectangular Matrix
  Multiplication</title><categories>cs.DS</categories><comments>27 pages, 19 figures, a preliminary version appeared in FOCS'98 under
  a slightly different title</comments><acm-class>F.2.2;G.2.2;G.3</acm-class><abstract>  We present two new algorithms for solving the {\em All Pairs Shortest Paths}
(APSP) problem for weighted directed graphs. Both algorithms use fast matrix
multiplication algorithms.
  The first algorithm solves the APSP problem for weighted directed graphs in
which the edge weights are integers of small absolute value in $\Ot(n^{2+\mu})$
time, where $\mu$ satisfies the equation $\omega(1,\mu,1)=1+2\mu$ and
$\omega(1,\mu,1)$ is the exponent of the multiplication of an $n\times n^\mu$
matrix by an $n^\mu \times n$ matrix. Currently, the best available bounds on
$\omega(1,\mu,1)$, obtained by Coppersmith, imply that $\mu&lt;0.575$. The running
time of our algorithm is therefore $O(n^{2.575})$. Our algorithm improves on
the $\Ot(n^{(3+\omega)/2})$ time algorithm, where $\omega=\omega(1,1,1)&lt;2.376$
is the usual exponent of matrix multiplication, obtained by Alon, Galil and
Margalit, whose running time is only known to be $O(n^{2.688})$.
  The second algorithm solves the APSP problem {\em almost} exactly for
directed graphs with {\em arbitrary} non-negative real weights. The algorithm
runs in $\Ot((n^\omega/\eps)\log (W/\eps))$ time, where $\eps&gt;0$ is an error
parameter and W is the largest edge weight in the graph, after the edge weights
are scaled so that the smallest non-zero edge weight in the graph is 1. It
returns estimates of all the distances in the graph with a stretch of at most
$1+\eps$. Corresponding paths can also be found efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008012</id><created>2000-08-17</created><authors><author><keyname>Sang</keyname><forenames>Erik F. Tjong Kim</forenames></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames></author><author><keyname>Dejean</keyname><forenames>Herve</forenames></author><author><keyname>Koeling</keyname><forenames>Rob</forenames></author><author><keyname>Krymolowski</keyname><forenames>Yuval</forenames></author><author><keyname>Punyakanok</keyname><forenames>Vasin</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author></authors><title>Applying System Combination to Base Noun Phrase Identification</title><categories>cs.CL</categories><comments>7 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of COLING 2000, Saarbruecken, Germany</journal-ref><abstract>  We use seven machine learning algorithms for one task: identifying base noun
phrases. The results have been processed by different system combination
methods and all of these outperformed the best individual result. We have
applied the seven learners with the best combinator, a majority vote of the top
five systems, to a standard data set and managed to improve the best published
result for this data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008013</id><created>2000-08-18</created><authors><author><keyname>Hoste</keyname><forenames>Veronique</forenames></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames></author><author><keyname>Sang</keyname><forenames>Erik Tjong Kim</forenames></author><author><keyname>Gillis</keyname><forenames>Steven</forenames></author></authors><title>Meta-Learning for Phonemic Annotation of Corpora</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of ICML-2000, Stanford University, CA, USA</journal-ref><abstract>  We apply rule induction, classifier combination and meta-learning (stacked
classifiers) to the problem of bootstrapping high accuracy automatic annotation
of corpora with pronunciation information. The task we address in this paper
consists of generating phonemic representations reflecting the Flemish and
Dutch pronunciations of a word on the basis of its orthographic representation
(which in turn is based on the actual speech recordings). We compare several
possible approaches to achieve the text-to-pronunciation mapping task:
memory-based learning, transformation-based learning, rule induction, maximum
entropy modeling, combination of classifiers in stacked learning, and stacking
of meta-learners. We are interested both in optimal accuracy and in obtaining
insight into the linguistic regularities involved. As far as accuracy is
concerned, an already high accuracy level (93% for Celex and 86% for Fonilex at
word level) for single classifiers is boosted significantly with additional
error reductions of 31% and 38% respectively using combination of classifiers,
and a further 5% using combination of meta-learners, bringing overall word
level accuracy to 96% for the Dutch variant and 92% for the Flemish variant. We
also show that the application of machine learning methods indeed leads to
increased insight into the linguistic regularities determining the variation
between the two pronunciation variants studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008014</id><created>2000-08-18</created><authors><author><keyname>De Pauw</keyname><forenames>Guy</forenames></author></authors><title>Aspects of Pattern-Matching in Data-Oriented Parsing</title><categories>cs.CL</categories><comments>7 pages, 3 figures</comments><acm-class>I.2.6;I.2.7;I.5.4</acm-class><journal-ref>Proceedings of the 18th International Conference on Computational
  Linguistics</journal-ref><abstract>  Data-Oriented Parsing (dop) ranks among the best parsing schemes, pairing
state-of-the art parsing accuracy to the psycholinguistic insight that larger
chunks of syntactic structures are relevant grammatical and probabilistic
units. Parsing with the dop-model, however, seems to involve a lot of CPU
cycles and a considerable amount of double work, brought on by the concept of
multiple derivations, which is necessary for probabilistic processing, but
which is not convincingly related to a proper linguistic backbone. It is
however possible to re-interpret the dop-model as a pattern-matching model,
which tries to maximize the size of the substructures that construct the parse,
rather than the probability of the parse. By emphasizing this memory-based
aspect of the dop-model, it is possible to do away with multiple derivations,
opening up possibilities for efficient Viterbi-style optimizations, while still
retaining acceptable parsing accuracy through enhanced context-sensitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008015</id><created>2000-08-18</created><authors><author><keyname>Walther</keyname><forenames>Markus</forenames><affiliation>University of Marburg</affiliation></author></authors><title>Temiar Reduplication in One-Level Prosodic Morphology</title><categories>cs.CL</categories><comments>9 pages, 2 figures. Finite-State Phonology: SIGPHON-2000, Proceedings
  of the Fifth Workshop of the ACL Special Interest Group in Computational
  Phonology, pp.13-21. Aug. 6, 2000. Luxembourg</comments><acm-class>I.2.7</acm-class><abstract>  Temiar reduplication is a difficult piece of prosodic morphology. This paper
presents the first computational analysis of Temiar reduplication, using the
novel finite-state approach of One-Level Prosodic Morphology originally
developed by Walther (1999b, 2000). After reviewing both the data and the basic
tenets of One-level Prosodic Morphology, the analysis is laid out in some
detail, using the notation of the FSA Utilities finite-state toolkit (van Noord
1997). One important discovery is that in this approach one can easily define a
regular expression operator which ambiguously scans a string in the left- or
rightward direction for a certain prosodic property. This yields an elegant
account of base-length-dependent triggering of reduplication as found in
Temiar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008016</id><created>2000-08-21</created><authors><author><keyname>Spilker</keyname><forenames>Joerg</forenames></author><author><keyname>Klarner</keyname><forenames>Martin</forenames></author><author><keyname>Goerz</keyname><forenames>Guenther</forenames></author></authors><title>Processing Self Corrections in a speech to speech system</title><categories>cs.CL cs.AI</categories><comments>5 pages, 2 figures</comments><acm-class>I 2.7</acm-class><journal-ref>Proceedings of COLING 2000, Saarbruecken, Germany; 31.7-4.8; pp
  1116-1120</journal-ref><abstract>  Speech repairs occur often in spontaneous spoken dialogues. The ability to
detect and correct those repairs is necessary for any spoken language system.
We present a framework to detect and correct speech repairs where all relevant
levels of information, i.e., acoustics, lexis, syntax and semantics can be
integrated. The basic idea is to reduce the search space for repairs as soon as
possible by cascading filters that involve more and more features. At first an
acoustic module generates hypotheses about the existence of a repair. Second a
stochastic model suggests a correction for every hypothesis. Well scored
corrections are inserted as new paths in the word lattice. Finally a lattice
parser decides on accepting the rep air.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008017</id><created>2000-08-21</created><authors><author><keyname>Roark</keyname><forenames>Brian</forenames></author><author><keyname>Johnson</keyname><forenames>Mark</forenames></author></authors><title>Efficient probabilistic top-down and left-corner parsing</title><categories>cs.CL</categories><comments>8 pages, 3 tables, 3 figures</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 37th Annual Meeting of the Association for
  Computational Linguistics, 1999, pages 421-428</journal-ref><abstract>  This paper examines efficient predictive broad-coverage parsing without
dynamic programming. In contrast to bottom-up methods, depth-first top-down
parsing produces partial parses that are fully connected trees spanning the
entire left context, from which any kind of non-local dependency or partial
semantic interpretation can in principle be read. We contrast two predictive
parsing approaches, top-down and left-corner parsing, and find both to be
viable. In addition, we find that enhancement with non-local information not
only improves parser accuracy, but also substantially improves the search
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008018</id><created>2000-08-22</created><authors><author><keyname>Senizergues</keyname><forenames>G.</forenames></author></authors><title>The Bisimulation Problem for equational graphs of finite out-degree</title><categories>cs.LO cs.DM</categories><comments>98 pages, 4 figures, submitted to JACM</comments><acm-class>F.1.1;F.4.2;F.4.3;G.2.2</acm-class><abstract>  The &quot;bisimulation problem&quot; for equational graphs of finite out-degree is
shown to be decidable. We reduce this problem to the bisimulation problem for
deterministic rational (vectors of) boolean series on the alphabet of a dpda M.
We then exhibit a complete formal system for deducing equivalent pairs of such
vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008019</id><created>2000-08-22</created><authors><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author><author><keyname>Koutsias</keyname><forenames>John</forenames></author><author><keyname>Chandrinos</keyname><forenames>Konstantinos V.</forenames></author><author><keyname>Spyropoulos</keyname><forenames>Constantine D.</forenames></author></authors><title>An Experimental Comparison of Naive Bayesian and Keyword-Based Anti-Spam
  Filtering with Personal E-mail Messages</title><categories>cs.CL cs.IR cs.LG</categories><acm-class>H.4.3; I.2.6; I.2.7; I.5.4; K.4.1</acm-class><journal-ref>Proceedings of the 23rd Annual International ACM SIGIR Conference
  on Research and Development in Information Retrieval, N.J. Belkin, P.
  Ingwersen and M.-K. Leong (Eds.), Athens, Greece, July 24-28, 2000, pages
  160-167</journal-ref><abstract>  The growing problem of unsolicited bulk e-mail, also known as &quot;spam&quot;, has
generated a need for reliable anti-spam e-mail filters. Filters of this type
have so far been based mostly on manually constructed keyword patterns. An
alternative approach has recently been proposed, whereby a Naive Bayesian
classifier is trained automatically to detect spam messages. We test this
approach on a large collection of personal e-mail messages, which we make
publicly available in &quot;encrypted&quot; form contributing towards standard
benchmarks. We introduce appropriate cost-sensitive measures, investigating at
the same time the effect of attribute-set size, training-corpus size,
lemmatization, and stop lists, issues that have not been explored in previous
experiments. Finally, the Naive Bayesian filter is compared, in terms of
performance, to a filter that uses keyword patterns, and which is part of a
widely used e-mail reader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008020</id><created>2000-08-22</created><authors><author><keyname>Ciaramita</keyname><forenames>Massimiliano</forenames></author><author><keyname>Johnson</keyname><forenames>Mark</forenames></author></authors><title>Explaining away ambiguity: Learning verb selectional preference with
  Bayesian networks</title><categories>cs.CL cs.AI</categories><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 18th International Conference on Computational
  Linguistics, Saarbrucken, Germany, Vol.1, 2000, p.187</journal-ref><abstract>  This paper presents a Bayesian model for unsupervised learning of verb
selectional preferences. For each verb the model creates a Bayesian network
whose architecture is determined by the lexical hierarchy of Wordnet and whose
parameters are estimated from a list of verb-object pairs found from a corpus.
``Explaining away'', a well-known property of Bayesian networks, helps the
model deal in a natural fashion with word sense ambiguity in the training data.
On a word sense disambiguation test our model performed better than other state
of the art systems for unsupervised learning of selectional preferences.
Computational complexity problems, ways of improving this approach and methods
for implementing ``explaining away'' in other graphical frameworks are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008021</id><created>2000-08-22</created><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames></author><author><keyname>Roark</keyname><forenames>Brian</forenames></author></authors><title>Compact non-left-recursive grammars using the selective left-corner
  transform and factoring</title><categories>cs.CL</categories><comments>7 pages, 5 tables, 2 figures</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 18th International Conference on Computational
  Linguistics (COLING), 2000, pages 355-361</journal-ref><abstract>  The left-corner transform removes left-recursion from (probabilistic)
context-free grammars and unification grammars, permitting simple top-down
parsing techniques to be used. Unfortunately the grammars produced by the
standard left-corner transform are usually much larger than the original. The
selective left-corner transform described in this paper produces a transformed
grammar which simulates left-corner recognition of a user-specified set of the
original productions, and top-down recognition of the others. Combined with two
factorizations, it produces non-left-recursive grammars that are not much
larger than the original.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008022</id><created>2000-08-22</created><authors><author><keyname>Mu&#xf1;oz</keyname><forenames>Marcia</forenames></author><author><keyname>Punyakanok</keyname><forenames>Vasin</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author><author><keyname>Zimak</keyname><forenames>Dav</forenames></author></authors><title>A Learning Approach to Shallow Parsing</title><categories>cs.LG cs.CL</categories><comments>LaTex 2e, 11 pages, 2 eps figures, 1 bbl file, uses colacl.sty</comments><acm-class>I.2.6; I.2.7</acm-class><journal-ref>Proceedings of EMNLP-VLC'99, pages 168-178</journal-ref><abstract>  A SNoW based learning approach to shallow parsing tasks is presented and
studied experimentally. The approach learns to identify syntactic patterns by
combining simple predictors to produce a coherent inference. Two instantiations
of this approach are studied and experimental results for Noun-Phrases (NP) and
Subject-Verb (SV) phrases that compare favorably with the best published
results are presented. In doing that, we compare two ways of modeling the
problem of learning to recognize patterns and suggest that shallow parsing
patterns are better learned using open/close predictors than using
inside/outside predictors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008023</id><created>2000-08-23</created><authors><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author><author><keyname>Dale</keyname><forenames>Robert</forenames></author></authors><title>Selectional Restrictions in HPSG</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 18th International Conference on Computational
  Linguistics (COLING), Saarbrucken, Germany, 31 July - 4 August 2000, pages
  15-20</journal-ref><abstract>  Selectional restrictions are semantic sortal constraints imposed on the
participants of linguistic constructions to capture contextually-dependent
constraints on interpretation. Despite their limitations, selectional
restrictions have proven very useful in natural language applications, where
they have been used frequently in word sense disambiguation, syntactic
disambiguation, and anaphora resolution. Given their practical value, we
explore two methods to incorporate selectional restrictions in the HPSG theory,
assuming that the reader is familiar with HPSG. The first method employs HPSG's
Background feature and a constraint-satisfaction component pipe-lined after the
parser. The second method uses subsorts of referential indices, and blocks
readings that violate selectional restrictions during parsing. While
theoretically less satisfactory, we have found the second method particularly
useful in the development of practical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008024</id><created>2000-08-23</created><authors><author><keyname>Osborne</keyname><forenames>Miles</forenames></author></authors><title>Estimation of Stochastic Attribute-Value Grammars using an Informative
  Sample</title><categories>cs.CL</categories><comments>6 pages, 2 figures. Coling 2000, Saarbr\&quot;{u}cken, Germany. pp
  586--592</comments><acm-class>I.2.6</acm-class><journal-ref>Coling 2000, Saarbr\&quot;{u}cken, Germany. pp 586--592</journal-ref><abstract>  We argue that some of the computational complexity associated with estimation
of stochastic attribute-value grammars can be reduced by training upon an
informative subset of the full training set. Results using the parsed Wall
Street Journal corpus show that in some circumstances, it is possible to obtain
better estimation results using an informative sample than when training upon
all the available material. Further experimentation demonstrates that with
unlexicalised models, a Gaussian Prior can reduce overfitting. However, when
models are lexicalised and contain overlapping features, overfitting does not
seem to be a problem, and a Gaussian Prior makes minimal difference to
performance. Our approach is applicable for situations when there are an
infeasibly large number of parses in the training set, or else for when
recovery of these parses from a packed representation is itself computationally
expensive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008025</id><created>2000-08-23</created><updated>2001-07-27</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Phutball Endgames are Hard</title><categories>cs.CC cs.GT</categories><comments>9 pages, 8 figures. Revised to include additional references on the
  complexity of checkers</comments><acm-class>F.1.3,K.8.0</acm-class><journal-ref>More Games of No Chance, MSRI Publications 42, 2002, pp. 351-360</journal-ref><abstract>  We show that, in John Conway's board game Phutball (or Philosopher's
Football), it is NP-complete to determine whether the current player has a move
that immediately wins the game. In contrast, the similar problems of
determining whether there is an immediately winning move in checkers, or a move
that kings a man, are both solvable in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008026</id><created>2000-08-24</created><authors><author><keyname>Roark</keyname><forenames>Brian</forenames></author><author><keyname>Charniak</keyname><forenames>Eugene</forenames></author></authors><title>Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon
  construction</title><categories>cs.CL</categories><comments>7 pages, 1 figure, 5 tables</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 36th Annual Meeting of the Association for
  Computational Linguistics and 17th International Conference on Computational
  Linguistics (COLING-ACL), 1998, pages 1110-1116</journal-ref><abstract>  Generating semantic lexicons semi-automatically could be a great time saver,
relative to creating them by hand. In this paper, we present an algorithm for
extracting potential entries for a category from an on-line corpus, based upon
a small set of exemplars. Our algorithm finds more correct terms and fewer
incorrect ones than previous work in this area. Additionally, the entries that
are generated potentially provide broader coverage of the category than would
occur to an individual coding them by hand. Our algorithm finds many terms not
included within Wordnet (many more than previous algorithms), and could be
viewed as an ``enhancer'' of existing broad-coverage resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008027</id><created>2000-08-24</created><authors><author><keyname>Roark</keyname><forenames>Brian</forenames></author><author><keyname>Charniak</keyname><forenames>Eugene</forenames></author></authors><title>Measuring efficiency in high-accuracy, broad-coverage statistical
  parsing</title><categories>cs.CL</categories><comments>8 pages, 4 figures, 2 tables</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the COLING 2000 Workshop on Efficiency in
  Large-Scale Parsing Systems, 2000, pages 29-36</journal-ref><abstract>  Very little attention has been paid to the comparison of efficiency between
high accuracy statistical parsers. This paper proposes one machine-independent
metric that is general enough to allow comparisons across very different
parsing architectures. This metric, which we call ``events considered'',
measures the number of ``events'', however they are defined for a particular
parser, for which a probability must be calculated, in order to find the parse.
It is applicable to single-pass or multi-stage parsers. We discuss the
advantages of the metric, and demonstrate its usefulness by using it to compare
two parsers which differ in several fundamental ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008028</id><created>2000-08-25</created><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames></author><author><keyname>Geman</keyname><forenames>Stuart</forenames></author><author><keyname>Canon</keyname><forenames>Stephen</forenames></author><author><keyname>Chi</keyname><forenames>Zhiyi</forenames></author><author><keyname>Riezler</keyname><forenames>Stefan</forenames></author></authors><title>Estimators for Stochastic ``Unification-Based'' Grammars</title><categories>cs.CL</categories><comments>7 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proc 37th Annual Conference of the Association for Computational
  Linguistics, 1999, pages 535-541</journal-ref><abstract>  Log-linear models provide a statistically sound framework for Stochastic
``Unification-Based'' Grammars (SUBGs) and stochastic versions of other kinds
of grammars. We describe two computationally-tractable ways of estimating the
parameters of such grammars from a training corpus of syntactic analyses, and
apply these to estimate a stochastic version of Lexical-Functional Grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008029</id><created>2000-08-25</created><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames></author><author><keyname>Riezler</keyname><forenames>Stefan</forenames></author></authors><title>Exploiting auxiliary distributions in stochastic unification-based
  grammars</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proc 1st NAACL, 2000, pages 154-161</journal-ref><abstract>  This paper describes a method for estimating conditional probability
distributions over the parses of ``unification-based'' grammars which can
utilize auxiliary distributions that are estimated by other means. We show how
this can be used to incorporate information about lexical selectional
preferences gathered from other sources into Stochastic ``Unification-based''
Grammars (SUBGs). While we apply this estimator to a Stochastic
Lexical-Functional Grammar, the method is general, and should be applicable to
stochastic versions of HPSGs, categorial grammars and transformational
grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008030</id><created>2000-08-28</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Yamamoto</keyname><forenames>Atsumu</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Metonymy Interpretation Using X NO Y Examples</title><categories>cs.CL</categories><comments>8 pages. Computation and Language</comments><acm-class>I.2.7</acm-class><journal-ref>SNLP2000, Chiang Mai, Thailand, May 10, 2000</journal-ref><abstract>  We developed on example-based method of metonymy interpretation. One
advantages of this method is that a hand-built database of metonymy is not
necessary because it instead uses examples in the form ``Noun X no Noun Y (Noun
Y of Noun X).'' Another advantage is that we will be able to interpret
newly-coined metonymic sentences by using a new corpus. We experimented with
metonymy interpretation and obtained a precision rate of 66% when using this
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008031</id><created>2000-08-28</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Uchimoto</keyname><forenames>Kiyotaka</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Bunsetsu Identification Using Category-Exclusive Rules</title><categories>cs.CL</categories><comments>7 pages. Computation and Language</comments><acm-class>I.2.7</acm-class><journal-ref>COLING'2000, Saarbrucken, Germany, August, 2000</journal-ref><abstract>  This paper describes two new bunsetsu identification methods using supervised
learning. Since Japanese syntactic analysis is usually done after bunsetsu
identification, bunsetsu identification is important for analyzing Japanese
sentences. In experiments comparing the four previously available
machine-learning methods (decision tree, maximum-entropy method, example-based
approach and decision list) and two new methods using category-exclusive rules,
the new method using the category-exclusive rules with the highest similarity
performed best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008032</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008032</id><created>2000-08-28</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Uchimoto</keyname><forenames>Kiyotaka</forenames></author><author><keyname>Ozaku</keyname><forenames>Hiromi</forenames></author><author><keyname>Utiyama</keyname><forenames>Masao</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Japanese Probabilistic Information Retrieval Using Location and Category
  Information</title><categories>cs.CL</categories><comments>7,8 pages. Computation and Language. IRAL'2000, Hong Kong, September
  30, 2000</comments><acm-class>H.3.3; I.2.7</acm-class><abstract>  Robertson's 2-poisson information retrieve model does not use location and
category information. We constructed a framework using location and category
information in a 2-poisson model. We submitted two systems based on this
framework to the IREX contest, Japanese language information retrieval contest
held in Japan in 1999. For precision in the A-judgement measure they scored
0.4926 and 0.4827, the highest values among the 15 teams and 22 systems that
participated in the IREX contest. We describe our systems and the comparative
experiments done when various parameters were changed. These experiments
confirmed the effectiveness of using location and category information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008033</id><created>2000-08-28</created><authors><author><keyname>Bond</keyname><forenames>Francis</forenames></author><author><keyname>Ogura</keyname><forenames>Kentaro</forenames></author><author><keyname>Uchino</keyname><forenames>Hajime</forenames></author></authors><title>Temporal Expressions in Japanese-to-English Machine Translation</title><categories>cs.CL</categories><comments>8 pages, slightly reformatted to avoid obscure style file</comments><acm-class>I.2.7</acm-class><journal-ref>Seventh International Conference on Theoretical and Methodological
  Issues in Machine Translation: TMI-97, Santa Fe, July 1997, pp 55--62</journal-ref><abstract>  This paper describes in outline a method for translating Japanese temporal
expressions into English. We argue that temporal expressions form a special
subset of language that is best handled as a special module in machine
translation. The paper deals with problems of lexical idiosyncrasy as well as
the choice of articles and prepositions within temporal expressions. In
addition temporal expressions are considered as parts of larger structures, and
the question of whether to translate them as noun phrases or adverbials is
addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008034</id><created>2000-08-30</created><authors><author><keyname>Riezler</keyname><forenames>Stefan</forenames></author><author><keyname>Prescher</keyname><forenames>Detlef</forenames></author><author><keyname>Kuhn</keyname><forenames>Jonas</forenames></author><author><keyname>Johnson</keyname><forenames>Mark</forenames></author></authors><title>Lexicalized Stochastic Modeling of Constraint-Based Grammars using
  Log-Linear Measures and EM Training</title><categories>cs.CL</categories><comments>8 pages, uses acl2000.sty</comments><acm-class>I.2.6; I.2.7</acm-class><journal-ref>Proceedings of the 38th Annual Meeting of the ACL, 2000</journal-ref><abstract>  We present a new approach to stochastic modeling of constraint-based grammars
that is based on log-linear models and uses EM for estimation from unannotated
data. The techniques are applied to an LFG grammar for German. Evaluation on an
exact match task yields 86% precision for an ambiguity rate of 5.4, and 90%
precision on a subcat frame match for an ambiguity rate of 25. Experimental
comparison to training from a parsebank shows a 10% gain from EM training.
Also, a new class-based grammar lexicalization is presented, showing a 10% gain
over unlexicalized models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008035</id><created>2000-08-30</created><authors><author><keyname>Prescher</keyname><forenames>Detlef</forenames></author><author><keyname>Riezler</keyname><forenames>Stefan</forenames></author><author><keyname>Rooth</keyname><forenames>Mats</forenames></author></authors><title>Using a Probabilistic Class-Based Lexicon for Lexical Ambiguity
  Resolution</title><categories>cs.CL</categories><comments>7 pages, uses colacl.sty</comments><acm-class>I.2.6, I.2.7</acm-class><journal-ref>Proceedings of the 18th COLING, 2000</journal-ref><abstract>  This paper presents the use of probabilistic class-based lexica for
disambiguation in target-word selection. Our method employs minimal but precise
contextual information for disambiguation. That is, only information provided
by the target-verb, enriched by the condensed information of a probabilistic
class-based lexicon, is used. Induction of classes and fine-tuning to verbal
arguments is done in an unsupervised manner by EM-based clustering techniques.
The method shows promising results in an evaluation on real-world translations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0008036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0008036</id><created>2000-08-30</created><authors><author><keyname>Riezler</keyname><forenames>Stefan</forenames></author></authors><title>Probabilistic Constraint Logic Programming. Formal Foundations of
  Quantitative and Statistical Inference in Constraint-Based Natural Language
  Processing</title><categories>cs.CL</categories><comments>PhD Thesis, 144 pages, University of Tuebingen, 1998</comments><acm-class>I.2.6; I.2.7</acm-class><abstract>  In this thesis, we present two approaches to a rigorous mathematical and
algorithmic foundation of quantitative and statistical inference in
constraint-based natural language processing. The first approach, called
quantitative constraint logic programming, is conceptualized in a clear logical
framework, and presents a sound and complete system of quantitative inference
for definite clauses annotated with subjective weights. This approach combines
a rigorous formal semantics for quantitative inference based on subjective
weights with efficient weight-based pruning for constraint-based systems. The
second approach, called probabilistic constraint logic programming, introduces
a log-linear probability distribution on the proof trees of a constraint logic
program and an algorithm for statistical inference of the parameters and
properties of such probability models from incomplete, i.e., unparsed data. The
possibility of defining arbitrary properties of proof trees as properties of
the log-linear probability model and efficiently estimating appropriate
parameter values for them permits the probabilistic modeling of arbitrary
context-dependencies in constraint logic programs. The usefulness of these
ideas is evaluated empirically in a small-scale experiment on finding the
correct parses of a constraint-based grammar. In addition, we address the
problem of computational intractability of the calculation of expectations in
the inference task and present various techniques to approximately solve this
task. Moreover, we present an approximate heuristic technique for searching for
the most probable analysis in probabilistic constraint logic programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009001</id><created>2000-09-05</created><updated>2002-02-25</updated><authors><author><keyname>Soklakov</keyname><forenames>Andrei N.</forenames><affiliation>Royal Holloway, University of London</affiliation></author></authors><title>Complexity analysis for algorithmically simple strings</title><categories>cs.LG</categories><comments>10 pages</comments><acm-class>E.4; F.2; I.2</acm-class><abstract>  Given a reference computer, Kolmogorov complexity is a well defined function
on all binary strings. In the standard approach, however, only the asymptotic
properties of such functions are considered because they do not depend on the
reference computer. We argue that this approach can be more useful if it is
refined to include an important practical case of simple binary strings.
Kolmogorov complexity calculus may be developed for this case if we restrict
the class of available reference computers. The interesting problem is to
define a class of computers which is restricted in a {\it natural} way modeling
the real-life situation where only a limited class of computers is physically
available to us. We give an example of what such a natural restriction might
look like mathematically, and show that under such restrictions some error
terms, even logarithmic in complexity, can disappear from the standard
complexity calculus.
  Keywords: Kolmogorov complexity; Algorithmic information theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009002</id><created>2000-09-07</created><authors><author><keyname>Watrous</keyname><forenames>John</forenames><affiliation>University of Calgary</affiliation></author></authors><title>Succinct quantum proofs for properties of finite groups</title><categories>cs.CC quant-ph</categories><comments>16 pages, to appear in FOCS'00</comments><acm-class>F.1.3;F.1.2</acm-class><abstract>  In this paper we consider a quantum computational variant of nondeterminism
based on the notion of a quantum proof, which is a quantum state that plays a
role similar to a certificate in an NP-type proof. Specifically, we consider
quantum proofs for properties of black-box groups, which are finite groups
whose elements are encoded as strings of a given length and whose group
operations are performed by a group oracle. We prove that for an arbitrary
group oracle there exist succinct (polynomial-length) quantum proofs for the
Group Non-Membership problem that can be checked with small error in polynomial
time on a quantum computer. Classically this is impossible--it is proved that
there exists a group oracle relative to which this problem does not have
succinct proofs that can be checked classically with bounded error in
polynomial time (i.e., the problem is not in MA relative to the group oracle
constructed). By considering a certain subproblem of the Group Non-Membership
problem we obtain a simple proof that there exists an oracle relative to which
BQP is not contained in MA. Finally, we show that quantum proofs for
non-membership and classical proofs for various other group properties can be
combined to yield succinct quantum proofs for other group properties not having
succinct proofs in the classical setting, such as verifying that a number
divides the order of a group and verifying that a group is not a simple group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009003</id><created>2000-09-08</created><authors><author><keyname>Sarkar</keyname><forenames>Anoop</forenames></author><author><keyname>Zeman</keyname><forenames>Daniel</forenames></author></authors><title>Automatic Extraction of Subcategorization Frames for Czech</title><categories>cs.CL</categories><comments>7 pages. Another version under the name &quot;Learning Verb
  Subcategorization from Corpora: Counting Frame Subsets&quot;, authors: Zeman,
  Sarkar, in proceedings of LREC 2000, Athens, Greece</comments><acm-class>I.2.7, G.3</acm-class><journal-ref>Proceedings of the 18th International Conference on Computational
  Linguistics (Coling 2000), Universit</journal-ref><abstract>  We present some novel machine learning techniques for the identification of
subcategorization information for verbs in Czech. We compare three different
statistical techniques applied to this problem. We show how the learning
algorithm can be used to discover previously unknown subcategorization frames
from the Czech Prague Dependency Treebank. The algorithm can then be used to
label dependents of a verb in the Czech treebank as either arguments or
adjuncts. Using our techniques, we ar able to achieve 88% precision on unseen
parsed text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009004</id><created>2000-09-13</created><authors><author><keyname>Carr</keyname><forenames>Les</forenames></author><author><keyname>Hitchcock</keyname><forenames>Steve</forenames></author><author><keyname>Hall</keyname><forenames>Wendy</forenames></author><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author></authors><title>A usage based analysis of CoRR</title><categories>cs.DL</categories><comments>This is a commentary on &quot;CoRR: A Computing Research Repository&quot; by
  Joseph Y. Halpern (cs.DL/0005003). See also Halpern's response to this and
  other commentaries (cs.DL/0005004)</comments><acm-class>H.4.3</acm-class><journal-ref>ACM Journal of Computer Documentation, Vol. 24, No. 2, May 2000,
  54-59</journal-ref><abstract>  Based on an empirical analysis of author usage of CoRR, and of its
predecessor in the Los Alamos eprint archives, it is shown that CoRR has not
yet been able to match the early growth of the Los Alamos physics archives.
Some of the reasons are implicit in Halpern's paper, and we explore them
further here. In particular we refer to the need to promote CoRR more
effectively for its intended community - computer scientists in universities,
industrial research labs and in government. We take up some points of detail on
this new world of open archiving concerning central versus distributed
self-archiving, publication, the restructuring of the journal publishers'
niche, peer review and copyright.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009005</identifier>
 <datestamp>2011-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009005</id><created>2000-09-13</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Wang</keyname><forenames>Joseph</forenames></author></authors><title>Fast Approximation of Centrality</title><categories>cs.DS cond-mat.dis-nn cs.SI</categories><comments>2 pages. To appear in 12th ACM/SIAM Symp. Discrete Algorithms (SODA
  2001)</comments><acm-class>F.2.2</acm-class><journal-ref>J. Graph Algorithms &amp; Applications 8(1):27-38, 2004</journal-ref><abstract>  Social studies researchers use graphs to model group activities in social
networks. An important property in this context is the centrality of a vertex:
the inverse of the average distance to each other vertex. We describe a
randomized approximation algorithm for centrality in weighted graphs. For
graphs exhibiting the small world phenomenon, our method estimates the
centrality of all vertices with high probability within a (1+epsilon) factor in
near-linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009006</id><created>2000-09-13</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Improved Algorithms for 3-Coloring, 3-Edge-Coloring, and Constraint
  Satisfaction</title><categories>cs.DS</categories><comments>11 pages, 7 figures. To appear in 12th ACM/SIAM Symp. Discrete
  Algorithms (SODA 2001). This extended abstract summarizes results from
  cs.DS/0006046 &quot;3-coloring in time O(1.3289^n)&quot; (with Richard Beigel) that
  were found after our FOCS 1995 paper on the same subject</comments><acm-class>F.2.2</acm-class><abstract>  We consider worst case time bounds for NP-complete problems including 3-SAT,
3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a
constraint satisfaction (CSP) formulation of these problems; 3-SAT is
equivalent to (2,3)-CSP while the other problems above are special cases of
(3,2)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the
time bounds for solving the other problems listed above. Our techniques involve
a mixture of Davis-Putnam-style backtracking with more sophisticated matching
and network flow based ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009007</id><created>2000-09-13</created><authors><author><keyname>Provost</keyname><forenames>Foster</forenames></author><author><keyname>Fawcett</keyname><forenames>Tom</forenames></author></authors><title>Robust Classification for Imprecise Environments</title><categories>cs.LG</categories><comments>24 pages, 12 figures. To be published in Machine Learning Journal.
  For related papers, see http://www.hpl.hp.com/personal/Tom_Fawcett/ROCCH/</comments><acm-class>I.2.6</acm-class><abstract>  In real-world environments it usually is difficult to specify target
operating conditions precisely, for example, target misclassification costs.
This uncertainty makes building robust classification systems problematic. We
show that it is possible to build a hybrid classifier that will perform at
least as well as the best available classifier for any target conditions. In
some cases, the performance of the hybrid actually can surpass that of the best
known classifier. This robust performance extends across a wide variety of
comparison frameworks, including the optimization of metrics such as accuracy,
expected cost, lift, precision, recall, and workforce utilization. The hybrid
also is efficient to build, to store, and to update. The hybrid is based on a
method for the comparison of classifier performance that is robust to imprecise
class distributions and misclassification costs. The ROC convex hull (ROCCH)
method combines techniques from ROC analysis, decision analysis and
computational geometry, and adapts them to the particulars of analyzing learned
classifiers. The method is efficient and incremental, minimizes the management
of classifier performance data, and allows for clear visual comparisons and
sensitivity analyses. Finally, we point to empirical evidence that a robust
hybrid classifier indeed is needed for many real-world problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009008</id><created>2000-09-18</created><authors><author><keyname>Sang</keyname><forenames>Erik F. Tjong Kim</forenames></author><author><keyname>Buchholz</keyname><forenames>Sabine</forenames></author></authors><title>Introduction to the CoNLL-2000 Shared Task: Chunking</title><categories>cs.CL</categories><comments>6 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of CoNLL-2000 and LLL-2000, Lisbon, Portugal</journal-ref><abstract>  We describe the CoNLL-2000 shared task: dividing text into syntactically
related non-overlapping groups of words, so-called text chunking. We give
background information on the data sets, present a general overview of the
systems that have taken part in the shared task and briefly discuss their
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009009</id><created>2000-09-18</created><authors><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author><author><keyname>Paliouras</keyname><forenames>Georgios</forenames></author><author><keyname>Karkaletsis</keyname><forenames>Vangelis</forenames></author><author><keyname>Sakkis</keyname><forenames>Georgios</forenames></author><author><keyname>Spyropoulos</keyname><forenames>Constantine D.</forenames></author><author><keyname>Stamatopoulos</keyname><forenames>Panagiotis</forenames></author></authors><title>Learning to Filter Spam E-Mail: A Comparison of a Naive Bayesian and a
  Memory-Based Approach</title><categories>cs.CL cs.IR cs.LG</categories><acm-class>H.4.3; I.2.6; I.2.7; I.5.4; K.4.1</acm-class><journal-ref>Proceedings of the workshop &quot;Machine Learning and Textual
  Information Access&quot;, 4th European Conference on Principles and Practice of
  Knowledge Discovery in Databases (PKDD-2000), H. Zaragoza, P. Gallinari and
  M. Rajman (Eds.), Lyon, France, September 2000, pp. 1-13</journal-ref><abstract>  We investigate the performance of two machine learning algorithms in the
context of anti-spam filtering. The increasing volume of unsolicited bulk
e-mail (spam) has generated a need for reliable anti-spam filters. Filters of
this type have so far been based mostly on keyword patterns that are
constructed by hand and perform poorly. The Naive Bayesian classifier has
recently been suggested as an effective method to construct automatically
anti-spam filters with superior performance. We investigate thoroughly the
performance of the Naive Bayesian filter on a publicly available corpus,
contributing towards standard benchmarks. At the same time, we compare the
performance of the Naive Bayesian filter to an alternative memory-based
learning approach, after introducing suitable cost-sensitive evaluation
measures. Both methods achieve very accurate spam filtering, outperforming
clearly the keyword-based filter of a widely used e-mail reader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009010</id><created>2000-09-18</created><updated>2000-10-10</updated><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author></authors><title>Computing Crossing Numbers in Quadratic Time</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2;G.2.2</acm-class><abstract>  We show that for every fixed non-negative integer k there is a quadratic time
algorithm that decides whether a given graph has crossing number at most k and,
if this is the case, computes a drawing of the graph in the plane with at most
k crossings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009011</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009011</id><created>2000-09-18</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author></authors><title>Anaphora Resolution in Japanese Sentences Using Surface Expressions and
  Examples</title><categories>cs.CL</categories><comments>156 pages. Doctoral thesis in Kyoto University, December 1996,
  supervised by M. Nagao</comments><acm-class>I.2.7</acm-class><abstract>  Anaphora resolution is one of the major problems in natural language
processing. It is also one of the important tasks in machine translation and
man/machine dialogue. We solve the problem by using surface expressions and
examples. Surface expressions are the words in sentences which provide clues
for anaphora resolution. Examples are linguistic data which are actually used
in conversations and texts. The method using surface expressions and examples
is a practical method. This thesis handles almost all kinds of anaphora: i. The
referential property and number of a noun phrase ii. Noun phrase direct
anaphora iii. Noun phrase indirect anaphora iv. Pronoun anaphora v. Verb phrase
ellipsis
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009012</id><created>2000-09-19</created><authors><author><keyname>Monz</keyname><forenames>Christof</forenames></author></authors><title>Modeling Ambiguity in a Multi-Agent System</title><categories>cs.CL cs.AI cs.MA</categories><comments>7 pages</comments><acm-class>F.4.1; I.2.7</acm-class><abstract>  This paper investigates the formal pragmatics of ambiguous expressions by
modeling ambiguity in a multi-agent system. Such a framework allows us to give
a more refined notion of the kind of information that is conveyed by ambiguous
expressions. We analyze how ambiguity affects the knowledge of the dialog
participants and, especially, what they know about each other after an
ambiguous sentence has been uttered. The agents communicate with each other by
means of a TELL-function, whose application is constrained by an implementation
of some of Grice's maxims. The information states of the multi-agent system
itself are represented as a Kripke structures and TELL is an update function on
those structures. This framework enables us to distinguish between the
information conveyed by ambiguous sentences vs. the information conveyed by
disjunctions, and between semantic ambiguity vs. perceived ambiguity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009013</id><created>2000-09-19</created><updated>2000-09-22</updated><authors><author><keyname>Efrat</keyname><forenames>Alon</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Pattern Matching for sets of segments</title><categories>cs.CG</categories><comments>To appear in the 12 ACM Symposium on Discrete Algorithms, Jan 2001</comments><acm-class>F.2.2</acm-class><abstract>  In this paper we present algorithms for a number of problems in geometric
pattern matching where the input consist of a collections of segments in the
plane. Our work consists of two main parts. In the first, we address problems
and measures that relate to collections of orthogonal line segments in the
plane. Such collections arise naturally from problems in mapping buildings and
robot exploration.
  We propose a new measure of segment similarity called a \emph{coverage
measure}, and present efficient algorithms for maximising this measure between
sets of axis-parallel segments under translations. Our algorithms run in time
$O(n^3\polylog n)$ in the general case, and run in time $O(n^2\polylog n)$ for
the case when all segments are horizontal. In addition, we show that when
restricted to translations that are only vertical, the Hausdorff distance
between two sets of horizontal segments can be computed in time roughly
$O(n^{3/2}{\sl polylog}n)$. These algorithms form significant improvements over
the general algorithm of Chew et al. that takes time $O(n^4 \log^2 n)$. In the
second part of this paper we address the problem of matching polygonal chains.
We study the well known \Frd, and present the first algorithm for computing the
\Frd under general translations. Our methods also yield algorithms for
computing a generalization of the \Fr distance, and we also present a simple
approximation algorithm for the \Frd that runs in time $O(n^2\polylog n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009014</id><created>2000-09-20</created><authors><author><keyname>Aiello</keyname><forenames>Marco</forenames></author><author><keyname>Monz</keyname><forenames>Christof</forenames></author><author><keyname>Todoran</keyname><forenames>Leon</forenames></author></authors><title>Combining Linguistic and Spatial Information for Document Analysis</title><categories>cs.CL cs.DL</categories><comments>Appeared in: J. Mariani and D. Harman (Eds.) Proceedings of RIAO'2000
  Content-Based Multimedia Information Access, CID, 2000. pp. 266-275</comments><acm-class>H.3.5; H.3.6; H.3.7; I.2.7; I.7</acm-class><abstract>  We present a framework to analyze color documents of complex layout. In
addition, no assumption is made on the layout. Our framework combines in a
content-driven bottom-up approach two different sources of information: textual
and spatial. To analyze the text, shallow natural language processing tools,
such as taggers and partial parsers, are used. To infer relations of the
logical layout we resort to a qualitative spatial calculus closely related to
Allen's calculus. We evaluate the system against documents from a color journal
and present the results of extracting the reading order from the journal's
pages. In this case, our analysis is successful as it extracts the intended
reading order from the document.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009015</id><created>2000-09-20</created><authors><author><keyname>Monz</keyname><forenames>Christof</forenames></author><author><keyname>de Rijke</keyname><forenames>Maarten</forenames></author></authors><title>A Tableaux Calculus for Ambiguous Quantification</title><categories>cs.CL</categories><comments>In: H. de Swart (editor). Automated Reasoning with Analytic Tableaux
  and Related Methods, Tableaux'98 LNAI 1397, Springer, 1998, pp. 232-246</comments><acm-class>F.4.1 I.2.7</acm-class><abstract>  Coping with ambiguity has recently received a lot of attention in natural
language processing. Most work focuses on the semantic representation of
ambiguous expressions. In this paper we complement this work in two ways.
First, we provide an entailment relation for a language with ambiguous
expressions. Second, we give a sound and complete tableaux calculus for
reasoning with statements involving ambiguous quantification. The calculus
interleaves partial disambiguation steps with steps in a traditional deductive
process, so as to minimize and postpone branching in the proof process, and
thereby increases its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009016</id><created>2000-09-20</created><authors><author><keyname>Monz</keyname><forenames>Christof</forenames></author></authors><title>Contextual Inference in Computational Semantics</title><categories>cs.CL cs.AI</categories><acm-class>F.4.1; I.2.7</acm-class><journal-ref>In: P. Bouquet, P. Brezillon, L. Serafini, M. Benerecetti, F.
  Castellani (Eds.) 2nd International and Interdisciplinary Conference on
  Modeling and Using Context (CONTEXT'99). Lecture Notes in Artificial
  Intelligence 1688, Springer, 1999, pages 242-255</journal-ref><abstract>  In this paper, an application of automated theorem proving techniques to
computational semantics is considered. In order to compute the presuppositions
of a natural language discourse, several inference tasks arise. Instead of
treating these inferences independently of each other, we show how integrating
techniques from formal approaches to context into deduction can help to compute
presuppositions more efficiently. Contexts are represented as Discourse
Representation Structures and the way they are nested is made explicit. In
addition, a tableau calculus is present which keeps track of contextual
information, and thereby allows to avoid carrying out redundant inference steps
as it happens in approaches that neglect explicit nesting of contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009017</id><created>2000-09-21</created><authors><author><keyname>Monz</keyname><forenames>Christof</forenames></author><author><keyname>de Rijke</keyname><forenames>Maarten</forenames></author></authors><title>A Tableau Calculus for Pronoun Resolution</title><categories>cs.CL cs.AI</categories><comments>16 pages</comments><acm-class>F.4.1; I.2.7</acm-class><journal-ref>In: N.V. Murray (ed.) Automated Reasoning with Analytic Tableaux
  and Related Methods. Lecture Notes in Artificial Intelligence 1617, Springer,
  1999, pages 247-262</journal-ref><abstract>  We present a tableau calculus for reasoning in fragments of natural language.
We focus on the problem of pronoun resolution and the way in which it
complicates automated theorem proving for natural language processing. A method
for explicitly manipulating contextual information during deduction is
proposed, where pronouns are resolved against this context during deduction. As
a result, pronoun resolution and deduction can be interleaved in such a way
that pronouns are only resolved if this is licensed by a deduction rule; this
helps us to avoid the combinatorial complexity of total pronoun disambiguation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009018</id><created>2000-09-21</created><authors><author><keyname>Monz</keyname><forenames>Christof</forenames></author><author><keyname>de Rijke</keyname><forenames>Maarten</forenames></author></authors><title>A Resolution Calculus for Dynamic Semantics</title><categories>cs.CL cs.AI</categories><comments>15 pages</comments><acm-class>F.4.1; I.2.7</acm-class><journal-ref>In: J. Dix, L. Farinas del Cerro, and U. Furbach (eds.) Logics in
  Artificial Intelligence (JELIA'98). Lecture Notes in Artificial Intelligence
  1489, Springer, 1998, pp. 184-198</journal-ref><abstract>  This paper applies resolution theorem proving to natural language semantics.
The aim is to circumvent the computational complexity triggered by natural
language ambiguities like pronoun binding, by interleaving pronoun binding with
resolution deduction. Therefore disambiguation is only applied to expression
that actually occur during derivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009019</id><created>2000-09-21</created><authors><author><keyname>Monz</keyname><forenames>Christof</forenames></author></authors><title>Computing Presuppositions by Contextual Reasoning</title><categories>cs.AI cs.CL</categories><comments>5 pages</comments><acm-class>F.4.1; I.2.7</acm-class><journal-ref>In: P. Brezillon, R. Turner, J-C. Pomerol and E. Turner (Eds.)
  Proceedings of the AAAI-99 Workshop on Reasoning in Context for AI
  Applications, AAAI Press, 1999, pp. 75-79</journal-ref><abstract>  This paper describes how automated deduction methods for natural language
processing can be applied more efficiently by encoding context in a more
elaborate way. Our work is based on formal approaches to context, and we
provide a tableau calculus for contextual reasoning. This is explained by
considering an example from the problem area of presupposition projection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009020</id><created>2000-09-22</created><authors><author><keyname>Baker</keyname><forenames>Mark</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Hyde</keyname><forenames>Dan</forenames></author></authors><title>Cluster Computing: A High-Performance Contender</title><categories>cs.DC cs.AR</categories><acm-class>C0</acm-class><abstract>  When you first heard people speak of Piles of PCs, the first thing that came
to mind may have been a cluttered computer room with processors, monitors, and
snarls of cables all around. Collections of computers have undoubtedly become
more sophisticated than in the early days of shared drives and modem
connections. No matter what you call them, Clusters of Workstations (COW),
Networks of Workstations (NOW), Workstation Clusters (WCs), Clusters of PCs
(CoPs), clusters of computers are now filling the processing niche once
occupied by more powerful stand-alone machines. This article discusses the need
for cluster computing technology, Technologies, Components, and Applications,
Supercluster Systems and Issues, The Need for a New Task Force, and Cluster
Computing Educational Resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009021</id><created>2000-09-22</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Abramson</keyname><forenames>David</forenames></author><author><keyname>Giddy</keyname><forenames>Jon</forenames></author></authors><title>Nimrod/G: An Architecture of a Resource Management and Scheduling System
  in a Global Computational Grid</title><categories>cs.DC</categories><acm-class>C0</acm-class><journal-ref>HPC Asia 2000, IEEE Press</journal-ref><abstract>  The availability of powerful microprocessors and high-speed networks as
commodity components has enabled high performance computing on distributed
systems (wide-area cluster computing). In this environment, as the resources
are usually distributed geographically at various levels (department,
enterprise, or worldwide) there is a great challenge in integrating,
coordinating and presenting them as a single resource to the user; thus forming
a computational grid. Another challenge comes from the distributed ownership of
resources with each resource having its own access policy, cost, and mechanism.
  The proposed Nimrod/G grid-enabled resource management and scheduling system
builds on our earlier work on Nimrod and follows a modular and component-based
architecture enabling extensibility, portability, ease of development, and
interoperability of independently developed components. It uses the Globus
toolkit services and can be easily extended to operate with any other emerging
grid middleware services. It focuses on the management and scheduling of
computations over dynamic resources scattered geographically across the
Internet at department, enterprise, or global level with particular emphasis on
developing scheduling schemes based on the concept of computational economy for
a real test bed, namely, the Globus testbed (GUSTO).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009022</id><created>2000-09-22</created><authors><author><keyname>Escudero</keyname><forenames>Gerard</forenames></author><author><keyname>Marquez</keyname><forenames>Lluis</forenames></author><author><keyname>Rigau</keyname><forenames>German</forenames></author></authors><title>A Comparison between Supervised Learning Algorithms for Word Sense
  Disambiguation</title><categories>cs.CL cs.AI</categories><comments>6 pages</comments><acm-class>I.2.7;I.2.6</acm-class><journal-ref>Proceedings of the 4th Conference on Computational Natural
  Language Learning, CoNLL'2000, pp. 31-36</journal-ref><abstract>  This paper describes a set of comparative experiments, including cross-corpus
evaluation, between five alternative algorithms for supervised Word Sense
Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNoW,
Decision Lists, and Boosting. Two main conclusions can be drawn: 1) The
LazyBoosting algorithm outperforms the other four state-of-the-art algorithms
in terms of accuracy and ability to tune to new domains; 2) The domain
dependence of WSD systems seems very strong and suggests that some kind of
adaptation or tuning is required for cross-corpus application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009023</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009023</id><created>2000-09-22</created><authors><author><keyname>Brodsky</keyname><forenames>Alex</forenames></author><author><keyname>Durocher</keyname><forenames>Stephane</forenames></author><author><keyname>Gethner</keyname><forenames>Ellen</forenames></author></authors><title>The Rectilinear Crossing Number of K_10 is 62</title><categories>cs.DM math.CO</categories><comments>17 Pages, colour figures</comments><acm-class>G.2.2</acm-class><journal-ref>Electronic Journal of Combinatorics. 8(1):R23 1-30. 2001</journal-ref><abstract>  A drawing of a graph G in the plane is said to be a rectilinear drawing of G
if the edges are required to be line segments (as opposed to Jordan curves). We
assume no three vertices are collinear. The rectilinear crossing number of G is
the fewest number of edge crossings attainable over all rectilinear drawings of
G. Thanks to Richard Guy, exact values of the rectilinear crossing number of
K_n, the complete graph on n vertices, for n = 3,...,9, are known (Guy 1972,
White and Beinke 1978, Finch 2000, Sloanes A014540). Since 1971, thanks to the
work of David Singer (1971, Gardiner 1986), the rectilinear crossing number of
K_10 has been known to be either 61 or 62, a deceptively innocent and
tantalizing statement. The difficulty of determining the correct value is
evidenced by the fact that Singer's result has withstood the test of time. In
this paper we use a purely combinatorial argument to show that the rectilinear
crossing number of K_10 is 62. Moreover, using this result, we improve an
asymptotic lower bound for a related problem. Finally, we close with some new
and old open questions that were provoked, in part, by the results of this
paper, and by the tangled history of the problem itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009024</id><created>2000-09-25</created><authors><author><keyname>Bern</keyname><forenames>Marshall</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Computing the Depth of a Flat</title><categories>cs.CG</categories><comments>6 pages, 1 figure, 2-page version to appear in ACM/SIAM SODA 2001</comments><acm-class>F.2.2; g.3</acm-class><abstract>  We give algorithms for computing the regression depth of a k-flat for a set
of n points in R^d. The running time is O(n^(d-2) + n log n) when 0 &lt; k &lt; d-1,
faster than the best time bound for hyperplane regression or for data depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009025</id><created>2000-09-27</created><authors><author><keyname>Bod</keyname><forenames>Rens</forenames></author></authors><title>Parsing with the Shortest Derivation</title><categories>cs.CL</categories><comments>7 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings COLING'2000, with a minor correction</journal-ref><abstract>  Common wisdom has it that the bias of stochastic grammars in favor of shorter
derivations of a sentence is harmful and should be redressed. We show that the
common wisdom is wrong for stochastic grammars that use elementary trees
instead of context-free rules, such as Stochastic Tree-Substitution Grammars
used by Data-Oriented Parsing models. For such grammars a non-probabilistic
metric based on the shortest derivation outperforms a probabilistic metric on
the ATIS and OVIS corpora, while it obtains very competitive results on the
Wall Street Journal corpus. This paper also contains the first published
experiments with DOP on the Wall Street Journal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009026</id><created>2000-09-27</created><authors><author><keyname>Bod</keyname><forenames>Rens</forenames></author></authors><title>An improved parser for data-oriented lexical-functional analysis</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings ACL'2000, Hong Kong</journal-ref><abstract>  We present an LFG-DOP parser which uses fragments from LFG-annotated
sentences to parse new sentences. Experiments with the Verbmobil and Homecentre
corpora show that (1) Viterbi n best search performs about 100 times faster
than Monte Carlo search while both achieve the same accuracy; (2) the DOP
hypothesis which states that parse accuracy increases with increasing fragment
size is confirmed for LFG-DOP; (3) LFG-DOP's relative frequency estimator
performs worse than a discounted frequency estimator; and (4) LFG-DOP
significantly outperforms Tree-DOP is evaluated on tree structures only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009027</id><created>2000-09-28</created><authors><author><keyname>Even-Zohar</keyname><forenames>Yair</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author></authors><title>A Classification Approach to Word Prediction</title><categories>cs.CL cs.AI cs.LG</categories><comments>8 pages</comments><acm-class>I.2.6;I.2.7</acm-class><journal-ref>NAACL 2000</journal-ref><abstract>  The eventual goal of a language model is to accurately predict the value of a
missing word given its context. We present an approach to word prediction that
is based on learning a representation for each word as a function of words and
linguistics predicates in its context. This approach raises a few new questions
that we address. First, in order to learn good word representations it is
necessary to use an expressive representation of the context. We present a way
that uses external knowledge to generate expressive context representations,
along with a learning method capable of handling the large number of features
generated this way that can, potentially, contribute to each prediction.
Second, since the number of words ``competing'' for each prediction is large,
there is a need to ``focus the attention'' on a smaller subset of these. We
exhibit the contribution of a ``focus of attention'' mechanism to the
performance of the word predictor. Finally, we describe a large scale
experimental study in which the approach presented is shown to yield
significant improvements in word prediction tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009028</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009028</id><created>2000-09-28</created><authors><author><keyname>Brodsky</keyname><forenames>Alex</forenames></author><author><keyname>Durocher</keyname><forenames>Stephane</forenames></author><author><keyname>Gethner</keyname><forenames>Ellen</forenames></author></authors><title>Toward the Rectilinear Crossing Number of $K_n$: New Drawings, Upper
  Bounds, and Asymptotics</title><categories>cs.DM cs.CG math.CO</categories><comments>13 Pages</comments><acm-class>F.2.2;G.2.1;G.2.2</acm-class><journal-ref>Discrete Mathematics. 262(1-3):59-77. 2003</journal-ref><doi>10.1016/S0012-365X(02)00491-0</doi><abstract>  Scheinerman and Wilf (1994) assert that `an important open problem in the
study of graph embeddings is to determine the rectilinear crossing number of
the complete graph K_n.' A rectilinear drawing of K_n is an arrangement of n
vertices in the plane, every pair of which is connected by an edge that is a
line segment. We assume that no three vertices are collinear, and that no three
edges intersect in a point unless that point is an endpoint of all three. The
rectilinear crossing number of K_n is the fewest number of edge crossings
attainable over all rectilinear drawings of K_n.
  For each n we construct a rectilinear drawing of K_n that has the fewest
number of edge crossings and the best asymptotics known to date. Moreover, we
give some alternative infinite families of drawings of K_n with good
asymptotics. Finally, we mention some old and new open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009029</id><created>2000-09-29</created><authors><author><keyname>Huntbach</keyname><forenames>Matthew</forenames></author></authors><title>The Concurrent Language Aldwych</title><categories>cs.PL</categories><comments>Presented at RULE 2000, First International Workshop on Rule-Based
  Programming, Montreal, Canada</comments><acm-class>D.3.3</acm-class><abstract>  Aldwych is proposed as the foundation of a general purpose language for
parallel applications. It works on a rule-based principle, and has aspects
variously of concurrent functional, logic and object-oriented languages, yet it
forms an integrated whole. It is intended to be applicable both for small-scale
parallel programming, and for large-scale open systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0009030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0009030</id><created>2000-09-29</created><authors><author><keyname>Xiao</keyname><forenames>Yong</forenames></author><author><keyname>Ariola</keyname><forenames>Zena M.</forenames></author><author><keyname>Mauny</keyname><forenames>Michel</forenames></author></authors><title>From Syntactic Theories to Interpreters: A Specification Language and
  Its Compilation</title><categories>cs.PL cs.SE</categories><comments>Accepted in Rule-based Programming Workshop, 2000, 16 pages</comments><acm-class>D.2.1; D.1.2; D.3.1; F.3.1; F.3.2; I.2.2</acm-class><abstract>  Recent years have seen an increasing need of high-level specification
languages and tools generating code from specifications. In this paper, we
introduce a specification language, {\splname}, which is tailored to the
writing of syntactic theories of language semantics. More specifically, the
language supports specifying primitive notions such as dynamic constraints,
contexts, axioms, and inference rules. We also introduce a system which
generates interpreters from {\splname} specifications. A prototype system is
implemented and has been tested on a number of examples, including a syntactic
theory for Verilog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010001</id><created>2000-09-30</created><authors><author><keyname>Branco</keyname><forenames>P. J. Costa</forenames></author><author><keyname>Dente</keyname><forenames>J. A.</forenames></author></authors><title>Design of an Electro-Hydraulic System Using Neuro-Fuzzy Techniques</title><categories>cs.RO cs.LG</categories><comments>35 pages</comments><acm-class>C.3; C.4; F.1.1; I.2.6; I.2.9; I.6.5; J.2, J.7</acm-class><journal-ref>In: Fusion of Neural Networks, Fuzzy Sets &amp; Genetic Algorithms:
  Industrial Applications, Chapter 4, CRC Press, Boca Raton, Florida, USA.,
  1999</journal-ref><abstract>  Increasing demands in performance and quality make drive systems fundamental
parts in the progressive automation of industrial processes. Their conventional
models become inappropriate and have limited scope if one requires a precise
and fast performance. So, it is important to incorporate learning capabilities
into drive systems in such a way that they improve their accuracy in realtime,
becoming more autonomous agents with some degree of intelligence. To
investigate this challenge, this chapter presents the development of a learning
control system that uses neuro-fuzzy techniques in the design of a tracking
controller to an experimental electro-hydraulic actuator. We begin the chapter
by presenting the neuro-fuzzy modeling process of the actuator. This part
surveys the learning algorithm, describes the laboratorial system, and presents
the modeling steps as the choice of actuator representative variables, the
acquisition of training and testing data sets, and the acquisition of the
neuro-fuzzy inverse-model of the actuator. In the second part of the chapter,
we use the extracted neuro-fuzzy model and its learning capabilities to design
the actuator position controller based on the feedback-error-learning
technique. Through a set of experimental results, we show the generalization
properties of the controller, its learning capability in actualizing in
realtime the initial neuro-fuzzy inverse-model, and its compensation action
improving the electro-hydraulics tracking performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010002</id><created>2000-09-30</created><authors><author><keyname>Branco</keyname><forenames>P. J. Costa</forenames></author><author><keyname>Dente</keyname><forenames>J. A.</forenames></author></authors><title>Noise Effects in Fuzzy Modelling Systems</title><categories>cs.NE cs.LG</categories><comments>6 pages</comments><acm-class>I.2.6; I.5.1; I.5.2</acm-class><journal-ref>In: Computational Intelligence and Applications, pp. 103-108,
  World Scientific and Engineering Society Press, Danvers, USA, 1999</journal-ref><abstract>  Noise is source of ambiguity for fuzzy systems. Although being an important
aspect, the effects of noise in fuzzy modeling have been little investigated.
This paper presents a set of tests using three well-known fuzzy modeling
algorithms. These evaluate perturbations in the extracted rule-bases caused by
noise polluting the learning data, and the corresponding deformations in each
learned functional relation. We present results to show: 1) how these fuzzy
modeling systems deal with noise; 2) how the established fuzzy model structure
influences noise sensitivity of each algorithm; and 3) whose characteristics of
the learning algorithms are relevant to noise attenuation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010003</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010003</id><created>2000-09-30</created><authors><author><keyname>Henriques</keyname><forenames>L.</forenames></author><author><keyname>Rolim</keyname><forenames>L.</forenames></author><author><keyname>Suemitsu</keyname><forenames>W.</forenames></author><author><keyname>Branco</keyname><forenames>P. J. Costa</forenames></author><author><keyname>Dente</keyname><forenames>J. A.</forenames></author></authors><title>Torque Ripple Minimization in a Switched Reluctance Drive by Neuro-Fuzzy
  Compensation</title><categories>cs.RO cs.LG</categories><comments>To be published in IEEE Trans. on Magnetics, 2000</comments><acm-class>I.2.9; I.2.6</acm-class><doi>10.1109/20.908911</doi><abstract>  Simple power electronic drive circuit and fault tolerance of converter are
specific advantages of SRM drives, but excessive torque ripple has limited its
use to special applications. It is well known that controlling the current
shape adequately can minimize the torque ripple. This paper presents a new
method for shaping the motor currents to minimize the torque ripple, using a
neuro-fuzzy compensator. In the proposed method, a compensating signal is added
to the output of a PI controller, in a current-regulated speed control loop.
Numerical results are presented in this paper, with an analysis of the effects
of changing the form of the membership function of the neuro-fuzzy compensator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010004</id><created>2000-09-30</created><authors><author><keyname>Branco</keyname><forenames>P. J. Costa</forenames></author><author><keyname>Dente</keyname><forenames>J. A.</forenames></author></authors><title>A Fuzzy Relational Identification Algorithm and Its Application to
  Predict The Behaviour of a Motor Drive System</title><categories>cs.RO cs.LG</categories><comments>12 pages</comments><acm-class>I.2.9; I.2.6</acm-class><journal-ref>In: Fuzzy Sets and Systems, Vol. 109, No. 3, pp. 41-52, Elsevier,
  2000</journal-ref><abstract>  Fuzzy relational identification builds a relational model describing systems
behaviour by a nonlinear mapping between its variables. In this paper, we
propose a new fuzzy relational algorithm based on simplified max-min relational
equation. The algorithm presents an adaptation method applied to gravity-center
of each fuzzy set based on error integral value between measured and predicted
system output, and uses the concept of time-variant universe of discourses. The
identification algorithm also includes a method to attenuate noise influence in
extracted system relational model using a fuzzy filtering mechanism. The
algorithm is applied to one-step forward prediction of a simulated and
experimental motor drive system. The identified model has its input-output
variables (stator-reference current and motor speed signal) treated as fuzzy
sets, whereas the relations existing between them are described by means of a
matrix R defining the relational model extracted by the algorithm. The results
show the good potentialities of the algorithm in predict the behaviour of the
system and attenuate through the fuzzy filtering method possible noise
distortions in the relational model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010005</id><created>2000-10-02</created><authors><author><keyname>Homan</keyname><forenames>Christopher M.</forenames></author></authors><title>Low Ambiguity in Strong, Total, Associative, One-Way Functions</title><categories>cs.CC</categories><comments>18 pages, one tex file, one bbl file</comments><acm-class>f.1.3</acm-class><abstract>  Rabi and Sherman present a cryptographic paradigm based on associative,
one-way functions that are strong (i.e., hard to invert even if one of their
arguments is given) and total. Hemaspaandra and Rothe proved that such powerful
one-way functions exist exactly if (standard) one-way functions exist, thus
showing that the associative one-way function approach is as plausible as
previous approaches. In the present paper, we study the degree of ambiguity of
one-way functions. Rabiand Sherman showed that no associative one-way function
(over a universe having at least two elements) can be unambiguous (i.e.,
one-to-one). Nonetheless, we prove that if standard, unambiguous, one-way
functions exist, then there exist strong, total, associative, one-way functions
that are $\mathcal{O}(n)$-to-one. This puts a reasonable upper bound on the
ambiguity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010006</id><created>2000-10-02</created><authors><author><keyname>Kohavi</keyname><forenames>Ron</forenames></author><author><keyname>Provost</keyname><forenames>Foster</forenames></author></authors><title>Applications of Data Mining to Electronic Commerce</title><categories>cs.LG cs.DB</categories><comments>Editorial for special issue</comments><acm-class>I.2.6;H.2.8</acm-class><abstract>  Electronic commerce is emerging as the killer domain for data mining
technology.
  The following are five desiderata for success. Seldom are they they all
present in one data mining application.
  1. Data with rich descriptions. For example, wide customer records with many
potentially useful fields allow data mining algorithms to search beyond obvious
correlations.
  2. A large volume of data. The large model spaces corresponding to rich data
demand many training instances to build reliable models.
  3. Controlled and reliable data collection. Manual data entry and integration
from legacy systems both are notoriously problematic; fully automated
collection is considerably better.
  4. The ability to evaluate results. Substantial, demonstrable return on
investment can be very convincing.
  5. Ease of integration with existing processes. Even if pilot studies show
potential benefit, deploying automated solutions to previously manual processes
is rife with pitfalls. Building a system to take advantage of the mined
knowledge can be a substantial undertaking. Furthermore, one often must deal
with social and political issues involved in the automation of a previously
manual business process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010007</id><created>2000-10-02</created><authors><author><keyname>Sen</keyname><forenames>Sandeep</forenames></author><author><keyname>Chatterjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Dumir</keyname><forenames>Neeraj</forenames></author></authors><title>Towards a Theory of Cache-Efficient Algorithms</title><categories>cs.AR cs.DS</categories><acm-class>B.3.2;C.0;F.1.1</acm-class><abstract>  We describe a model that enables us to analyze the running time of an
algorithm in a computer with a memory hierarchy with limited associativity, in
terms of various cache parameters. Our model, an extension of Aggarwal and
Vitter's I/O model, enables us to establish useful relationships between the
cache complexity and the I/O complexity of computations. As a corollary, we
obtain cache-optimal algorithms for some fundamental problems like sorting,
FFT, and an important subclass of permutations in the single-level cache model.
We also show that ignoring associativity concerns could lead to inferior
performance, by analyzing the average-case cache behavior of mergesort. We
further extend our model to multiple levels of cache with limited associativity
and present optimal algorithms for matrix transpose and sorting. Our techniques
may be used for systematic exploitation of the memory hierarchy starting from
the algorithm design stage, and dealing with the hitherto unresolved problem of
limited associativity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010008</id><created>2000-10-03</created><authors><author><keyname>Cichon</keyname><forenames>E. A.</forenames></author><author><keyname>Marion</keyname><forenames>J-Y.</forenames></author></authors><title>The Light Lexicographic path Ordering</title><categories>cs.PL cs.CC</categories><acm-class>F.1.3; I.2.2</acm-class><abstract>  We introduce syntactic restrictions of the lexicographic path ordering to
obtain the Light Lexicographic Path Ordering. We show that the light
lexicographic path ordering leads to a characterisation of the functions
computable in space bounded by a polynomial in the size of the inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010009</id><created>2000-10-03</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>An Approach to the Implementation of Overlapping Rules in Standard ML</title><categories>cs.PL</categories><comments>13 pages. Presented at RULE 2000, First International Workshop on
  Rule-Based Programming, Montreal, Canada</comments><acm-class>D.3.3</acm-class><abstract>  We describe an approach to programming rule-based systems in Standard ML,
with a focus on so-called overlapping rules, that is rules that can still be
active when other rules are fired. Such rules are useful when implementing
rule-based reactive systems, and to that effect we show a simple implementation
of Loyall's Active Behavior Trees, used to control goal-directed agents in the
Oz virtual environment. We discuss an implementation of our framework using a
reactive library geared towards implementing those kind of systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010010</id><created>2000-10-03</created><authors><author><keyname>Martins</keyname><forenames>J. F.</forenames></author><author><keyname>Branco</keyname><forenames>P. J. Costa</forenames></author><author><keyname>Pires</keyname><forenames>A. J.</forenames></author><author><keyname>Dente</keyname><forenames>J. A.</forenames></author></authors><title>Fault Detection using Immune-Based Systems and Formal Language
  Algorithms</title><categories>cs.CE cs.LG</categories><comments>To appear as an Invited paper in IEEE Conference on Decision and
  Control (CDC2000), 6 pages</comments><acm-class>I.2.6; I.2.7</acm-class><abstract>  This paper describes two approaches for fault detection: an immune-based
mechanism and a formal language algorithm. The first one is based on the
feature of immune systems in distinguish any foreign cell from the body own
cell. The formal language approach assumes the system as a linguistic source
capable of generating a certain language, characterised by a grammar. Each
algorithm has particular characteristics, which are analysed in the paper,
namely in what cases they can be used with advantage. To test their
practicality, both approaches were applied on the problem of fault detection in
an induction motor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010011</id><created>2000-10-06</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Pasanen</keyname><forenames>Kari</forenames></author><author><keyname>Rothe</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>If P \neq NP then Some Strongly Noninvertible Functions are Invertible</title><categories>cs.CC</categories><comments>Extended and updated version of UR-CS-TR-00-737</comments><acm-class>F.1.3; F.2.2</acm-class><abstract>  Rabi, Rivest, and Sherman alter the standard notion of noninvertibility to a
new notion they call strong noninvertibility, and show -- via explicit
cryptographic protocols for secret-key agreement ([RS93,RS97] attribute this to
Rivest and Sherman) and digital signatures [RS93,RS97] -- that strongly
noninvertible functions would be very useful components in protocol design.
Their definition of strong noninvertibility has a small twist (``respecting the
argument given'') that is needed to ensure cryptographic usefulness. In this
paper, we show that this small twist has a large, unexpected consequence:
Unless P=NP, some strongly noninvertible functions are invertible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010012</id><created>2000-10-06</created><authors><author><keyname>Mangu</keyname><forenames>L.</forenames></author><author><keyname>Brill</keyname><forenames>E.</forenames></author><author><keyname>Stolcke</keyname><forenames>A.</forenames></author></authors><title>Finding consensus in speech recognition: word error minimization and
  other applications of confusion networks</title><categories>cs.CL</categories><comments>35 pages, 8 figures</comments><acm-class>I.2.7</acm-class><journal-ref>Computer Speech and Language 14(4), 373-400, October 2000</journal-ref><abstract>  We describe a new framework for distilling information from word lattices to
improve the accuracy of speech recognition and obtain a more perspicuous
representation of a set of alternative hypotheses. In the standard MAP decoding
approach the recognizer outputs the string of words corresponding to the path
with the highest posterior probability given the acoustics and a language
model. However, even given optimal models, the MAP decoder does not necessarily
minimize the commonly used performance metric, word error rate (WER). We
describe a method for explicitly minimizing WER by extracting word hypotheses
with the highest posterior probabilities from word lattices. We change the
standard problem formulation by replacing global search over a large set of
sentence hypotheses with local search over a small set of word candidates. In
addition to improving the accuracy of the recognizer, our method produces a new
representation of the set of candidate hypotheses that specifies the sequence
of word-level confusions in a compact lattice format. We study the properties
of confusion networks and examine their use for other tasks, such as lattice
compression, word spotting, confidence annotation, and reevaluation of
recognition hypotheses using higher-level knowledge sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010013</id><created>2000-10-09</created><authors><author><keyname>Rodriguez</keyname><forenames>Diego</forenames><affiliation>University of Oviedo</affiliation></author><author><keyname>Sobrado</keyname><forenames>Igor</forenames><affiliation>University of Oviedo</affiliation></author></authors><title>A Public-key based Information Management Model for Mobile Agents</title><categories>cs.CR cs.DC cs.IR cs.NI</categories><comments>7 pages, 0 PostScript figures, uses IEEE/LaTeX macros
  IEEEtran.{bst|cls}</comments><report-no>FFUOV-00/04</report-no><acm-class>C.2.1; C.2.4; H.3; H.3.4</acm-class><abstract>  Mobile code based computing requires development of protection schemes that
allow digital signature and encryption of data collected by the agents in
untrusted hosts. These algorithms could not rely on carrying encryption keys if
these keys could be stolen or used to counterfeit data by hostile hosts and
agents. As a consequence, both information and keys must be protected in a way
that only authorized hosts, that is the host that provides information and the
server that has sent the mobile agent, could modify (by changing or removing)
retrieved data. The data management model proposed in this work allows the
information collected by the agents to be protected against handling by other
hosts in the information network. It has been done by using standard public-key
cryptography modified to support protection of data in distributed environments
without requiring an interactive protocol with the host that has dropped the
agent. Their significance stands on the fact that it is the first model that
supports a full-featured protection of mobile agents allowing remote hosts to
change its own information if required before agent returns to its originating
server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010014</id><created>2000-10-10</created><authors><author><keyname>Skorik</keyname><forenames>Sergei</forenames></author><author><keyname>Berthommier</keyname><forenames>Frederic</forenames></author></authors><title>On a cepstrum-based speech detector robust to white noise</title><categories>cs.CL cs.CV cs.HC</categories><comments>4 pages pdf format, requires Acrobat Reader v 4.0 or later</comments><acm-class>I.2.7; I.2.1; I.2.10; H.5.5</acm-class><abstract>  We study effects of additive white noise on the cepstral representation of
speech signals. Distribution of each individual cepstrum coefficient of speech
is shown to depend strongly on noise and to overlap significantly with the
cepstrum distribution of noise. Based on these studies, we suggest a scalar
quantity, V, equal to the sum of weighted cepstral coefficients, which is able
to classify frames containing speech against noise-like frames. The
distributions of V for speech and noise frames are reasonably well separated
above SNR = 5 dB, demonstrating the feasibility of robust speech detector based
on V.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010015</id><created>2000-10-10</created><authors><author><keyname>Wu</keyname><forenames>Pei-Chi</forenames></author></authors><title>On Exponential-Time Completeness of the Circularity Problem for
  Attribute Grammars</title><categories>cs.PL cs.CC</categories><comments>7 pages</comments><acm-class>D.3.1; D.3.4; F.2.2; F.4.2</acm-class><abstract>  Attribute grammars (AGs) are a formal technique for defining semantics of
programming languages. Existing complexity proofs on the circularity problem of
AGs are based on automata theory, such as writing pushdown acceptor and
alternating Turing machines. They reduced the acceptance problems of above
automata, which are exponential-time (EXPTIME) complete, to the AG circularity
problem. These proofs thus show that the circularity problem is EXPTIME-hard,
at least as hard as the most difficult problems in EXPTIME. However, none has
given a proof for the EXPTIME-completeness of the problem. This paper first
presents an alternating Turing machine for the circularity problem. The
alternating Turing machine requires polynomial space. Thus, the circularity
problem is in EXPTIME and is then EXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010016</id><created>2000-10-10</created><authors><author><keyname>Hoffmann</keyname><forenames>Berthold</forenames></author><author><keyname>Minas</keyname><forenames>Mark</forenames></author></authors><title>Towards rule-based visual programming of generic visual systems</title><categories>cs.PL</categories><comments>15 pages, 16 figures contribution to the First International Workshop
  on Rule-Based Programming (RULE'2000), September 19, 2000, Montreal, Canada</comments><acm-class>D.1.7; D.3.3</acm-class><abstract>  This paper illustrates how the diagram programming language DiaPlan can be
used to program visual systems. DiaPlan is a visual rule-based language that is
founded on the computational model of graph transformation. The language
supports object-oriented programming since its graphs are hierarchically
structured. Typing allows the shape of these graphs to be specified recursively
in order to increase program security. Thanks to its genericity, DiaPlan allows
to implement systems that represent and manipulate data in arbitrary diagram
notations. The environment for the language exploits the diagram editor
generator DiaGen for providing genericity, and for implementing its user
interface and type checker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010017</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010017</id><created>2000-10-10</created><updated>2001-01-08</updated><authors><author><keyname>Rocchesso</keyname><forenames>Davide</forenames></author><author><keyname>Dutilleux</keyname><forenames>Pierre</forenames></author></authors><title>Generalization of a 3-D resonator model for the simulation of spherical
  enclosures</title><categories>cs.SD</categories><comments>39 pages, 16 figures, 6 tables. Accepted for publication in Applied
  Signal Processing</comments><acm-class>H.5.5</acm-class><doi>10.1155/S1110865701000105</doi><abstract>  A rectangular enclosure has such an even distribution of resonances that it
can be accurately and efficiently modelled using a feedback delay network.
Conversely, a non rectangular shape such as a sphere has a distribution of
resonances that challenges the construction of an efficient model. This work
proposes an extension of the already known feedback delay network structure to
model the resonant properties of a sphere. A specific frequency distribution of
resonances can be approximated, up to a certain frequency, by inserting an
allpass filter of moderate order after each delay line of a feedback delay
network. The structure used for rectangular boxes is therefore augmented with a
set of allpass filters allowing parametric control over the enclosure size and
the boundary properties. This work was motivated by informal listening tests
which have shown that it is possible to identify a basic shape just from the
distribution of its audible resonances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010018</id><created>2000-10-11</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author></authors><title>Internet Packet Filter Management and Rectangle Geometry</title><categories>cs.CG cs.NI</categories><comments>9 pages, 2 figures. To appear at 12th ACM/SIAM Symp. Discrete
  Algorithms (SODA 2001)</comments><acm-class>F.2.2</acm-class><abstract>  We consider rule sets for internet packet routing and filtering, where each
rule consists of a range of source addresses, a range of destination addresses,
a priority, and an action. A given packet should be handled by the action from
the maximum priority rule that matches its source and destination. We describe
new data structures for quickly finding the rule matching an incoming packet,
in near-linear space, and a new algorithm for determining whether a rule set
contains any conflicts, in time O(n^{3/2}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010019</id><created>2000-10-11</created><authors><author><keyname>Canetti</keyname><forenames>Ran</forenames></author><author><keyname>Goldreich</keyname><forenames>Oded</forenames></author><author><keyname>Halevi</keyname><forenames>Shai</forenames></author></authors><title>The Random Oracle Methodology, Revisited</title><categories>cs.CR</categories><comments>31 pages</comments><acm-class>D.4.6; K.6.5</acm-class><journal-ref>In Proceedings of 30th Annual ACM Symposium on the Theory of
  Computing, pages 209-218, May 1998, ACM</journal-ref><abstract>  We take a critical look at the relationship between the security of
cryptographic schemes in the Random Oracle Model, and the security of the
schemes that result from implementing the random oracle by so called
&quot;cryptographic hash functions&quot;. The main result of this paper is a negative
one: There exist signature and encryption schemes that are secure in the Random
Oracle Model, but for which any implementation of the random oracle results in
insecure schemes.
  In the process of devising the above schemes, we consider possible
definitions for the notion of a &quot;good implementation&quot; of a random oracle,
pointing out limitations and challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010020</id><created>2000-10-11</created><authors><author><keyname>Yeh</keyname><forenames>Alexander</forenames></author></authors><title>Using existing systems to supplement small amounts of annotated
  grammatical relations training data</title><categories>cs.CL</categories><comments>7 pages, uses acl2000.sty</comments><acm-class>I.2.7</acm-class><journal-ref>38th Annual Meeting of the Association for Computational
  Linguistics (ACL-2000), pages 126-132, Hong Kong, October, 2000</journal-ref><abstract>  Grammatical relationships (GRs) form an important level of natural language
processing, but different sets of GRs are useful for different purposes.
Therefore, one may often only have time to obtain a small training corpus with
the desired GR annotations. To boost the performance from using such a small
training corpus on a transformation rule learner, we use existing systems that
find related types of annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010021</id><created>2000-10-14</created><updated>2000-10-16</updated><authors><author><keyname>Aspnes</keyname><forenames>James</forenames></author><author><keyname>Fischer</keyname><forenames>David F.</forenames></author><author><keyname>Fischer</keyname><forenames>Michael J.</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Kumar</keyname><forenames>Alok</forenames></author></authors><title>Towards Understanding the Predictability of Stock Markets from the
  Perspective of Computational Complexity</title><categories>cs.CE cs.CC</categories><comments>The conference version will appear in SODA 2001</comments><acm-class>F.1.3;F.2.2;G.2.1;G.2.3;G.3;J.1</acm-class><abstract>  This paper initiates a study into the century-old issue of market
predictability from the perspective of computational complexity. We develop a
simple agent-based model for a stock market where the agents are traders
equipped with simple trading strategies, and their trades together determine
the stock prices. Computer simulations show that a basic case of this model is
already capable of generating price graphs which are visually similar to the
recent price movements of high tech stocks. In the general model, we prove that
if there are a large number of traders but they employ a relatively small
number of strategies, then there is a polynomial-time algorithm for predicting
future price movements with high accuracy. On the other hand, if the number of
strategies is large, market prediction becomes complete in two new
computational complexity classes CPP and BCPP, which are between P^NP[O(log n)]
and PP. These computational completeness results open up a novel possibility
that the price graph of an actual stock could be sufficiently deterministic for
various prediction goals but appear random to all polynomial-time prediction
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010022</id><created>2000-10-15</created><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Kalai</keyname><forenames>Adam</forenames></author><author><keyname>Wasserman</keyname><forenames>Hal</forenames></author></authors><title>Noise-Tolerant Learning, the Parity Problem, and the Statistical Query
  Model</title><categories>cs.LG cs.AI cs.DS</categories><acm-class>I.2.6</acm-class><abstract>  We describe a slightly sub-exponential time algorithm for learning parity
functions in the presence of random classification noise. This results in a
polynomial-time algorithm for the case of parity functions that depend on only
the first O(log n log log n) bits of input. This is the first known instance of
an efficient noise-tolerant algorithm for a concept class that is provably not
learnable in the Statistical Query model of Kearns. Thus, we demonstrate that
the set of problems learnable in the statistical query model is a strict subset
of those problems learnable in the presence of noise in the PAC model.
  In coding-theory terms, what we give is a poly(n)-time algorithm for decoding
linear k by n codes in the presence of random noise for the case of k = c log n
loglog n for some c &gt; 0. (The case of k = O(log n) is trivial since one can
just individually check each of the 2^k possible messages and choose the one
that yields the closest codeword.)
  A natural extension of the statistical query model is to allow queries about
statistical properties that involve t-tuples of examples (as opposed to single
examples). The second result of this paper is to show that any class of
functions learnable (strongly or weakly) with t-wise queries for t = O(log n)
is also weakly learnable with standard unary queries. Hence this natural
extension to the statistical query model does not increase the set of weakly
learnable functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010023</id><created>2000-10-16</created><authors><author><keyname>Bulitko</keyname><forenames>Vadim</forenames></author></authors><title>Oracle Complexity and Nontransitivity in Pattern Recognition</title><categories>cs.CC cs.AI cs.CV cs.DS</categories><acm-class>F.2.2; F.4.1; I.2.10; I.5.2</acm-class><abstract>  Different mathematical models of recognition processes are known. In the
present paper we consider a pattern recognition algorithm as an oracle
computation on a Turing machine. Such point of view seems to be useful in
pattern recognition as well as in recursion theory. Use of recursion theory in
pattern recognition shows connection between a recognition algorithm comparison
problem and complexity problems of oracle computation. That is because in many
cases we can take into account only the number of sign computations or in other
words volume of oracle information needed. Therefore, the problem of
recognition algorithm preference can be formulated as a complexity optimization
problem of oracle computation. Furthermore, introducing a certain &quot;natural&quot;
preference relation on a set of recognizing algorithms, we discover it to be
nontransitive. This relates to the well known nontransitivity paradox in
probability theory.
  Keywords: Pattern Recognition, Recursion Theory, Nontransitivity, Preference
Relation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010024</id><created>2000-10-17</created><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Martinez</keyname><forenames>David</forenames></author></authors><title>Exploring automatic word sense disambiguation with decision lists and
  the Web</title><categories>cs.CL</categories><comments>9 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Procedings of the COLING 2000 Workshop on Semantic Annotation and
  Intelligent Content</journal-ref><abstract>  The most effective paradigm for word sense disambiguation, supervised
learning, seems to be stuck because of the knowledge acquisition bottleneck. In
this paper we take an in-depth study of the performance of decision lists on
two publicly available corpora and an additional corpus automatically acquired
from the Web, using the fine-grained highly polysemous senses in WordNet.
Decision lists are shown a versatile state-of-the-art technique. The
experiments reveal, among other facts, that SemCor can be an acceptable (0.7
precision for polysemous words) starting point for an all-words system. The
results on the DSO corpus show that for some highly polysemous words 0.7
precision seems to be the current state-of-the-art limit. On the other hand,
independently constructed hand-tagged corpora are not mutually useful, and a
corpus automatically acquired from the Web is shown to fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010025</id><created>2000-10-17</created><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Ansa</keyname><forenames>Olatz</forenames></author><author><keyname>Arregi</keyname><forenames>Xabier</forenames></author><author><keyname>Artola</keyname><forenames>Xabier</forenames></author><author><keyname>de Ilarraza</keyname><forenames>Arantza Diaz</forenames></author><author><keyname>Lersundi</keyname><forenames>Mikel</forenames></author><author><keyname>Martinez</keyname><forenames>David</forenames></author><author><keyname>Sarasola</keyname><forenames>Kepa</forenames></author><author><keyname>Urizar</keyname><forenames>Ruben</forenames></author></authors><title>Extraction of semantic relations from a Basque monolingual dictionary
  using Constraint Grammar</title><categories>cs.CL</categories><comments>11 pages. PostScript format</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of EURALEX 2000</journal-ref><abstract>  This paper deals with the exploitation of dictionaries for the semi-automatic
construction of lexicons and lexical knowledge bases. The final goal of our
research is to enrich the Basque Lexical Database with semantic information
such as senses, definitions, semantic relations, etc., extracted from a Basque
monolingual dictionary. The work here presented focuses on the extraction of
the semantic relations that best characterise the headword, that is, those of
synonymy, antonymy, hypernymy, and other relations marked by specific relators
and derivation. All nominal, verbal and adjectival entries were treated. Basque
uses morphological inflection to mark case, and therefore semantic relations
have to be inferred from suffixes rather than from prepositions. Our approach
combines a morphological analyser and surface syntax parsing (based on
Constraint Grammar), and has proven very successful for highly inflected
languages such as Basque. Both the effort to write the rules and the actual
processing time of the dictionary have been very low. At present we have
extracted 42,533 relations, leaving only 2,943 (9%) definitions without any
extracted relation. The error rate is extremely low, as only 2.2% of the
extracted relations are wrong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010026</id><created>2000-10-17</created><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Ansa</keyname><forenames>Olatz</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author><author><keyname>Martinez</keyname><forenames>David</forenames></author></authors><title>Enriching very large ontologies using the WWW</title><categories>cs.CL</categories><comments>6 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Procedings of the ECAI 2000 Workshop on Ontology Learning</journal-ref><abstract>  This paper explores the possibility to exploit text on the world wide web in
order to enrich the concepts in existing ontologies. First, a method to
retrieve documents from the WWW related to a concept is described. These
document collections are used 1) to construct topic signatures (lists of
topically related words) for each concept in WordNet, and 2) to build
hierarchical clusters of the concepts (the word senses) that lexicalize a given
word. The overall goal is to overcome two shortcomings of WordNet: the lack of
topical links among concepts, and the proliferation of senses. Topic signatures
are validated on a word sense disambiguation task with good results, which are
improved when the hierarchical clusters are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010027</id><created>2000-10-17</created><authors><author><keyname>Martinez</keyname><forenames>David</forenames></author><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author></authors><title>One Sense per Collocation and Genre/Topic Variations</title><categories>cs.CL</categories><comments>9 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the Joint SIGDAT Conference on Empirical Methods in
  Natural Language Processing and Very Large Corpora 2000</journal-ref><abstract>  This paper revisits the one sense per collocation hypothesis using
fine-grained sense distinctions and two different corpora. We show that the
hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported
earlier on 2-way ambiguities). We also show that one sense per collocation does
hold across corpora, but that collocations vary from one corpus to the other,
following genre and topic variations. This explains the low results when
performing word sense disambiguation across corpora. In fact, we demonstrate
that when two independent corpora share a related genre/topic, the word sense
disambiguation results would be better. Future work on word sense
disambiguation will have to take into account genre and topic as important
parameters on their models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010028</id><created>2000-10-19</created><authors><author><keyname>Charlier</keyname><forenames>Baudouin Le</forenames></author><author><keyname>Rossi</keyname><forenames>Sabina</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>Sequence-Based Abstract Interpretation of Prolog</title><categories>cs.LO cs.PL</categories><comments>62 pages. To appear in the journal &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>D.2; D.3; F.3.1; F.3.2</acm-class><abstract>  Many abstract interpretation frameworks and analyses for Prolog have been
proposed, which seek to extract information useful for program optimization.
Although motivated by practical considerations, notably making Prolog
competitive with imperative languages, such frameworks fail to capture some of
the control structures of existing implementations of the language.
  In this paper we propose a novel framework for the abstract interpretation of
Prolog which handles the depth-first search rule and the cut operator. It
relies on the notion of substitution sequence to model the result of the
execution of a goal. The framework consists of (i) a denotational concrete
semantics, (ii) a safe abstraction of the concrete semantics defined in terms
of a class of post-fixpoints, and (iii) a generic abstract interpretation
algorithm. We show that traditional abstract domains of substitutions may
easily be adapted to the new framework, and provide experimental evidence of
the effectiveness of our approach. We also show that previous work on
determinacy analysis, that was not expressible by existing abstract
interpretation frameworks, can be seen as an instance of our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010029</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010029</id><created>2000-10-20</created><updated>2001-01-18</updated><authors><author><keyname>Smaus</keyname><forenames>Jan-Georg</forenames></author><author><keyname>Fages</keyname><forenames>Francois</forenames></author><author><keyname>Deransart</keyname><forenames>Pierre</forenames></author></authors><title>Using Modes to Ensure Subject Reduction for Typed Logic Programs with
  Subtyping</title><categories>cs.LO</categories><comments>27 pages; Research Report of INRIA Rocquencourt, long version of
  paper in FSTTCS 2000 conference, New Delhi</comments><report-no>RR-4020</report-no><acm-class>D1.6; D3.3</acm-class><abstract>  We consider a general prescriptive type system with parametric polymorphism
and subtyping for logic programs. The property of subject reduction expresses
the consistency of the type system w.r.t. the execution model: if a program is
&quot;well-typed&quot;, then all derivations starting in a &quot;well-typed&quot; goal are again
&quot;well-typed&quot;. It is well-established that without subtyping, this property is
readily obtained for logic programs w.r.t. their standard (untyped) execution
model. Here we give syntactic conditions that ensure subject reduction also in
the presence of general subtyping relations between type constructors. The idea
is to consider logic programs with a fixed dataflow, given by modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010030</id><created>2000-10-23</created><authors><author><keyname>Kempe</keyname><forenames>Andre</forenames></author></authors><title>Reduction of Intermediate Alphabets in Finite-State Transducer Cascades</title><categories>cs.CL</categories><comments>9 pages, 7 figures, LaTeX (+ eps)</comments><acm-class>F.1.1; I.2.7</acm-class><journal-ref>Proc. TALN 2000, pp. 207-215, Lausanne, Switzerland. October 16-18</journal-ref><abstract>  This article describes an algorithm for reducing the intermediate alphabets
in cascades of finite-state transducers (FSTs). Although the method modifies
the component FSTs, there is no change in the overall relation described by the
whole cascade. No additional information or special algorithm, that could
decelerate the processing of input, is required at runtime. Two examples from
Natural Language Processing are used to illustrate the effect of the algorithm
on the sizes of the FSTs and their alphabets. With some FSTs the number of arcs
and symbols shrank considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010031</id><created>2000-10-24</created><authors><author><keyname>Akcoglu</keyname><forenames>Karhan</forenames></author><author><keyname>Aspnes</keyname><forenames>James</forenames></author><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Opportunity Cost Algorithms for Combinatorial Auctions</title><categories>cs.CE cs.DS</categories><report-no>DIMACS TR 2000-27</report-no><acm-class>F.2.0; J.4</acm-class><abstract>  Two general algorithms based on opportunity costs are given for approximating
a revenue-maximizing set of bids an auctioneer should accept, in a
combinatorial auction in which each bidder offers a price for some subset of
the available goods and the auctioneer can only accept non-intersecting bids.
Since this problem is difficult even to approximate in general, the algorithms
are most useful when the bids are restricted to be connected node subsets of an
underlying object graph that represents which objects are relevant to each
other. The approximation ratios of the algorithms depend on structural
properties of this graph and are small constants for many interesting families
of object graphs. The running times of the algorithms are linear in the size of
the bid graph, which describes the conflicts between bids. Extensions of the
algorithms allow for efficient processing of additional constraints, such as
budget constraints that associate bids with particular bidders and limit how
many bids from a particular bidder can be accepted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010032</id><created>2000-10-25</created><updated>2002-03-30</updated><authors><author><keyname>Brass</keyname><forenames>Stefan</forenames></author><author><keyname>Dix</keyname><forenames>Juergen</forenames></author><author><keyname>Przymusinski</keyname><forenames>Teodor C.</forenames></author></authors><title>Super Logic Programs</title><categories>cs.AI cs.LO</categories><comments>47 pages, revised version of the paper submitted 10/2000</comments><acm-class>F.4.1; I.2.3; I.2.4</acm-class><abstract>  The Autoepistemic Logic of Knowledge and Belief (AELB) is a powerful
nonmonotic formalism introduced by Teodor Przymusinski in 1994. In this paper,
we specialize it to a class of theories called `super logic programs'. We argue
that these programs form a natural generalization of standard logic programs.
In particular, they allow disjunctions and default negation of arbibrary
positive objective formulas.
  Our main results are two new and powerful characterizations of the static
semant ics of these programs, one syntactic, and one model-theoretic. The
syntactic fixed point characterization is much simpler than the fixed point
construction of the static semantics for arbitrary AELB theories. The
model-theoretic characterization via Kripke models allows one to construct
finite representations of the inherently infinite static expansions.
  Both characterizations can be used as the basis of algorithms for query
answering under the static semantics. We describe a query-answering interpreter
for super programs which we developed based on the model-theoretic
characterization and which is available on the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010033</id><created>2000-10-26</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Liberman</keyname><forenames>Mark</forenames></author></authors><title>A Formal Framework for Linguistic Annotation (revised version)</title><categories>cs.CL cs.DB cs.DS</categories><comments>29 pages, 20 figures, to appear in Speech Communication, An earlier
  version appeared as cs.CL/9903003</comments><acm-class>A.1; E.2; H.2.1; H.3.3; H.3.7; I.2.7</acm-class><abstract>  `Linguistic annotation' covers any descriptive or analytic notations applied
to raw language data. The basic data may be in the form of time functions -
audio, video and/or physiological recordings - or it may be textual. The added
notations may include transcriptions of all sorts (from phonetic features to
discourse structures), part-of-speech and sense tagging, syntactic analysis,
`named entity' identification, co-reference annotation, and so on. While there
are several ongoing efforts to provide formats and tools for such annotations
and to publish annotated linguistic databases, the lack of widely accepted
standards is becoming a critical problem. Proposed standards, to the extent
they exist, have focused on file formats. This paper focuses instead on the
logical structure of linguistic annotations. We survey a wide variety of
existing annotation formats and demonstrate a common conceptual core, the
annotation graph. This provides a formal framework for constructing,
maintaining and searching linguistic annotations, while remaining consistent
with many alternative data structures and file formats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010034</id><created>2000-10-27</created><authors><author><keyname>Verma</keyname><forenames>Rakesh M.</forenames></author></authors><title>Static Analysis Techniques for Equational Logic Programming</title><categories>cs.LO cs.PL</categories><comments>Appeared in 1st ACM SIGPLAN Workshop on Rule-based Programming (RULE
  2000)</comments><acm-class>F.3.2; D.3.2</acm-class><abstract>  An equational logic program is a set of directed equations or rules, which
are used to compute in the obvious way (by replacing equals with ``simpler''
equals). We present static analysis techniques for efficient equational logic
programming, some of which have been implemented in $LR^2$, a laboratory for
developing and evaluating fast, efficient, and practical rewriting techniques.
Two novel features of $LR^2$ are that non-left-linear rules are allowed in most
contexts and it has a tabling option based on the congruence-closure based
algorithm to compute normal forms. Although, the focus of this research is on
the tabling approach some of the techniques are applicable to the untabled
approach as well. Our presentation is in the context of $LR^2$, which is an
interpreter, but some of the techniques apply to compilation as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010035</id><created>2000-10-30</created><updated>2001-01-23</updated><authors><author><keyname>Ducasse</keyname><forenames>M.</forenames><affiliation>IRISA/INSA de Rennes</affiliation></author></authors><title>Proceedings of the Fourth International Workshop on Automated Debugging
  (AADEBUG 2000)</title><categories>cs.SE cs.PL</categories><comments>2 invited talks, 3 articles with demonstration description, 10
  articles, 3 demonstration descriptions, 5 poster abstracts</comments><acm-class>D.2.5</acm-class><abstract>  Over the past decades automated debugging has seen major achievements.
However, as debugging is by necessity attached to particular programming
paradigms, the results are scattered. The aims of the workshop are to gather
common themes and solutions across programming communities, and to
cross-fertilize ideas. AADEBUG 2000 in Munich follows AADEBUG'93 in Linkoeping,
Sweden; AADEBUG'95 in Saint Malo, France; AADEBUG'97 in Linkoeping, Sweden.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010036</id><created>2000-10-31</created><authors><author><keyname>Goles</keyname><forenames>Eric</forenames><affiliation>Univ. de Chile</affiliation></author><author><keyname>Morvan</keyname><forenames>Michel</forenames><affiliation>Univ. Paris 7 and IUF</affiliation></author><author><keyname>Phan</keyname><forenames>Ha Duong</forenames><affiliation>Univ. Paris 7</affiliation></author></authors><title>Lattice Structure and Convergence of a Game of Cards</title><categories>cs.DM</categories><comments>10 pages, 1 figures, submitted. See also:
  http://www.liafa.jussieu.fr/~phan/anglais/public.html</comments><acm-class>G.2.1; C.2.4</acm-class><abstract>  This paper is devoted to the study of the dynamics of a discrete system
related to some self stabilizing protocol on a ring of processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010037</id><created>2000-10-31</created><authors><author><keyname>Straccia</keyname><forenames>Umberto</forenames></author></authors><title>On the relationship between fuzzy logic and four-valued relevance logic</title><categories>cs.AI</categories><acm-class>F.4.1;I.2.3;I.2.4</acm-class><abstract>  In fuzzy propositional logic, to a proposition a partial truth in [0,1] is
assigned. It is well known that under certain circumstances, fuzzy logic
collapses to classical logic. In this paper, we will show that under dual
conditions, fuzzy logic collapses to four-valued (relevance) logic, where
propositions have truth-value true, false, unknown, or contradiction. As a
consequence, fuzzy entailment may be considered as ``in between'' four-valued
(relevance) entailment and classical entailment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010038</id><created>2000-10-31</created><authors><author><keyname>Jahier</keyname><forenames>Erwan</forenames><affiliation>IRISA/INSA de Rennes</affiliation></author></authors><title>Collecting Graphical Abstract Views of Mercury Program Executions</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  A program execution monitor is a program that collects and abstracts
information about program executions. The &quot;collect&quot; operator is a high level,
general purpose primitive which lets users implement their own monitors.
&quot;Collect&quot; is built on top of the Mercury trace. In previous work, we have
demonstrated how this operator can be used to efficiently collect various kinds
of statistics about Mercury program executions. In this article we further
demonstrate the expressive power and effectiveness of &quot;collect&quot; by providing
more monitor examples. In particular, we show how to implement monitors that
generate graphical abstractions of program executions such as proof trees,
control flow graphs and dynamic call graphs. We show how those abstractions can
be easily modified and adapted, since those monitors only require several
dozens of lines of code. Those abstractions are intended to serve as front-ends
of software visualization tools. Although &quot;collect&quot; is currently implemented on
top of the Mercury trace, none of its underlying concepts depend of Mercury and
it can be implemented on top of any tracer for any programming language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0010039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0010039</id><created>2000-10-31</created><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Computational Geometry Column 40</title><categories>cs.CG</categories><comments>3 pages; 4 figures</comments><acm-class>F.2.2; G.2.1</acm-class><abstract>  It has recently been established by Below, De Loera, and Richter-Gebert that
finding a minimum size (or even just a small) triangulation of a convex
polyhedron is NP-complete. Their 3SAT-reduction proof is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011001</id><created>2000-11-02</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>Utilizing the World Wide Web as an Encyclopedia: Extracting Term
  Descriptions from Semi-Structured Texts</title><categories>cs.CL</categories><comments>8 pages, 2 Postscript figures</comments><acm-class>I.2.7; H.3.3</acm-class><journal-ref>Proceedings of the 38th Annual Meeting of the Association for
  Computational Linguistics (ACL-2000), pp.488-495, Oct. 2000</journal-ref><abstract>  In this paper, we propose a method to extract descriptions of technical terms
from Web pages in order to utilize the World Wide Web as an encyclopedia. We
use linguistic patterns and HTML text structures to extract text fragments
containing term descriptions. We also use a language model to discard
extraneous descriptions, and a clustering method to summarize resultant
descriptions. We show the effectiveness of our method by way of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011002</id><created>2000-11-02</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>A Novelty-based Evaluation Method for Information Retrieval</title><categories>cs.CL</categories><comments>5 pages</comments><acm-class>H.3.3</acm-class><journal-ref>Proceedings of the 2nd International Conference on Language
  Resources and Evaluation (LREC-2000), pp.1637-1641, Jun. 2000</journal-ref><abstract>  In information retrieval research, precision and recall have long been used
to evaluate IR systems. However, given that a number of retrieval systems
resembling one another are already available to the public, it is valuable to
retrieve novel relevant documents, i.e., documents that cannot be retrieved by
those existing systems. In view of this problem, we propose an evaluation
method that favors systems retrieving as many novel documents as possible. We
also used our method to evaluate systems that participated in the IREX
workshop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011003</id><created>2000-11-02</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>Applying Machine Translation to Two-Stage Cross-Language Information
  Retrieval</title><categories>cs.CL</categories><comments>13 pages, 1 Postscript figure</comments><acm-class>I.2.7; H.3.3</acm-class><journal-ref>Proceedings of the 4th Conference of the Association for Machine
  Translation in the Americas (AMTA-2000), pp.13-24, Oct. 2000</journal-ref><abstract>  Cross-language information retrieval (CLIR), where queries and documents are
in different languages, needs a translation of queries and/or documents, so as
to standardize both of them into a common representation. For this purpose, the
use of machine translation is an effective approach. However, computational
cost is prohibitive in translating large-scale document collections. To resolve
this problem, we propose a two-stage CLIR method. First, we translate a given
query into the document language, and retrieve a limited number of foreign
documents. Second, we machine translate only those documents into the user
language, and re-rank them based on the translation result. We also show the
effectiveness of our method by way of experiments using Japanese queries and
English technical documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011004</id><created>2000-11-04</created><updated>2000-12-03</updated><authors><author><keyname>Mueller-Quade</keyname><forenames>J.</forenames></author><author><keyname>Imai</keyname><forenames>H.</forenames></author></authors><title>Anonymous Oblivious Transfer</title><categories>cs.CR</categories><comments>6 pages, some things clearified, especially that a broadcast channel
  is not necessary</comments><acm-class>C.2.0</acm-class><abstract>  In this short note we want to introduce {\em anonymous oblivious transfer} a
new cryptographic primitive which can be proven to be strictly more powerful
than oblivious transfer. We show that all functions can be robustly realized by
multi party protocols with {\em anonymous oblivious transfer}. No assumption
about possible collusions of cheaters or disruptors have to be made.
Furthermore we shortly discuss how to realize anonymous oblivious transfer with
oblivious broadcast or by quantum cryptography. The protocol of anonymous
oblivious transfer was inspired by a quantum protocol: the anonymous quantum
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011005</id><created>2000-11-06</created><authors><author><keyname>Ronsse</keyname><forenames>Michiel</forenames></author><author><keyname>De Bosschere</keyname><forenames>Koen</forenames></author></authors><title>Non-intrusive on-the-fly data race detection using execution replay</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AAdebug 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  This paper presents a practical solution for detecting data races in parallel
programs. The solution consists of a combination of execution replay (RecPlay)
with automatic on-the-fly data race detection. This combination enables us to
perform the data race detection on an unaltered execution (almost no probe
effect). Furthermore, the usage of multilevel bitmaps and snooped matrix clocks
limits the amount of memory used. As the record phase of RecPlay is highly
efficient, there is no need to switch it off, hereby eliminating the
possibility of Heisenbugs because tracing can be left on all the time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011006</id><created>2000-11-06</created><authors><author><keyname>Ronsse</keyname><forenames>Michiel</forenames></author><author><keyname>De Bosschere</keyname><forenames>Koen</forenames></author><author><keyname>de Kergommeaux</keyname><forenames>Jacques Chassin</forenames></author></authors><title>Execution replay and debugging</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADebug 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  As most parallel and distributed programs are internally non-deterministic --
consecutive runs with the same input might result in a different program flow
-- vanilla cyclic debugging techniques as such are useless. In order to use
cyclic debugging tools, we need a tool that records information about an
execution so that it can be replayed for debugging. Because recording
information interferes with the execution, we must limit the amount of
information and keep the processing of the information fast. This paper
contains a survey of existing execution replay techniques and tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011007</id><created>2000-11-06</created><authors><author><keyname>Sima'an</keyname><forenames>Khalil</forenames></author></authors><title>Tree-gram Parsing: Lexical Dependencies and Structural Relations</title><categories>cs.CL cs.AI cs.HC</categories><comments>8 pages. Appeared in Proceedings of the 38th Annual Meeting of the
  Association for Computational Linguistics (ACL'00), Hong Kong, China</comments><acm-class>I.2; K.3.2; J.5</acm-class><abstract>  This paper explores the kinds of probabilistic relations that are important
in syntactic disambiguation. It proposes that two widely used kinds of
relations, lexical dependencies and structural relations, have complementary
disambiguation capabilities. It presents a new model based on structural
relations, the Tree-gram model, and reports experiments showing that structural
relations should benefit from enrichment by lexical dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011008</id><created>2000-11-06</created><authors><author><keyname>Schmidt-Schau&#xdf;</keyname><forenames>Manfred</forenames></author><author><keyname>Huber</keyname><forenames>Michael</forenames></author></authors><title>A Lambda-Calculus with letrec, case, constructors and non-determinism</title><categories>cs.PL cs.AI cs.SC</categories><acm-class>F.4.1;D.3.2;I.2.2</acm-class><abstract>  A non-deterministic call-by-need lambda-calculus \calc with case,
constructors, letrec and a (non-deterministic) erratic choice, based on
rewriting rules is investigated. A standard reduction is defined as a variant
of left-most outermost reduction. The semantics is defined by contextual
equivalence of expressions instead of using $\alpha\beta(\eta)$-equivalence. It
is shown that several program transformations are correct, for example all
(deterministic) rules of the calculus, and in addition the rules for garbage
collection, removing indirections and unique copy.
  This shows that the combination of a context lemma and a meta-rewriting on
reductions using complete sets of commuting (forking, resp.) diagrams is a
useful and successful method for providing a semantics of a functional
programming language and proving correctness of program transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011009</id><created>2000-11-06</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Small Maximal Independent Sets and Faster Exact Graph Coloring</title><categories>cs.DS math.CO</categories><comments>8 pages</comments><acm-class>F.2.2</acm-class><journal-ref>J. Graph Algorithms &amp; Applications 7(2):131-140, 2003</journal-ref><abstract>  We show that, for any n-vertex graph G and integer parameter k, there are at
most 3^{4k-n}4^{n-3k} maximal independent sets I \subset G with |I| &lt;= k, and
that all such sets can be listed in time O(3^{4k-n} 4^{n-3k}). These bounds are
tight when n/4 &lt;= k &lt;= n/3. As a consequence, we show how to compute the exact
chromatic number of a graph in time O((4/3 + 3^{4/3}/4)^n) ~= 2.4150^n,
improving a previous O((1+3^{1/3})^n) ~= 2.4422^n algorithm of Lawler (1976).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011010</id><created>2000-11-06</created><authors><author><keyname>Parson</keyname><forenames>Dale</forenames></author><author><keyname>Schlieder</keyname><forenames>Bryan</forenames></author><author><keyname>Beatty</keyname><forenames>Paul</forenames></author></authors><title>Extension Language Automation of Embedded System Debugging</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADebug 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  Embedded systems contain several layers of target processing abstraction.
These layers include electronic circuit, binary machine code, mnemonic assembly
code, and high-level procedural and object-oriented abstractions. Physical and
temporal constraints and artifacts within physically embedded systems make it
impossible for software engineers to operate at a single layer of processor
abstraction. The Luxdbg embedded system debugger exposes these layers to
debugger users, and it adds an additional layer, the extension language layer,
that allows users to extend both the debugger and its target processor
capabilities. Tcl is Luxdbg's extension language. Luxdbg users can apply Tcl to
automate interactive debugging steps, to redirect and to interconnect target
processor input-output facilities, to schedule multiple processor execution, to
log and to react to target processing exceptions, and to automate target system
testing. Inclusion of an extension language like Tcl in a debugger promises
additional advantages for distributed debugging, where debuggers can pass
extension language expressions across computer networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011011</id><created>2000-11-07</created><authors><author><keyname>Berstel</keyname><forenames>Jean</forenames></author><author><keyname>Boasson</keyname><forenames>Luc</forenames></author></authors><title>Formal Properties of XML Grammars and Languages</title><categories>cs.DM cs.CL</categories><comments>24 pages</comments><acm-class>F.4.2;F.4.3;D.3.1</acm-class><abstract>  XML documents are described by a document type definition (DTD). An
XML-grammar is a formal grammar that captures the syntactic features of a DTD.
We investigate properties of this family of grammars. We show that every
XML-language basically has a unique XML-grammar. We give two characterizations
of languages generated by XML-grammars, one is set-theoretic, the other is by a
kind of saturation property. We investigate decidability problems and prove
that some properties that are undecidable for general context-free languages
become decidable for XML-languages. We also characterize those XML-grammars
that generate regular XML-languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011012</id><created>2000-11-07</created><updated>2005-11-07</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Causes and Explanations: A Structural-Model Approach, Part I: Causes</title><categories>cs.AI</categories><comments>Part II of the paper (on Explanation) is also on the arxiv.
  Previously the two parts were submitted as one paper. To appear in the
  British Journal for the Philosophy of Science</comments><acm-class>I.2.4</acm-class><abstract>  We propose a new definition of actual cause, using structural equations to
model counterfactuals. We show that the definition yields a plausible and
elegant account of causation that handles well examples which have caused
problems for other definitions and resolves major difficulties in the
traditional account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011013</id><created>2000-11-08</created><authors><author><keyname>Brass</keyname><forenames>Stefan</forenames></author><author><keyname>Dix</keyname><forenames>Juergen</forenames></author><author><keyname>Freitag</keyname><forenames>Burkhard</forenames></author><author><keyname>Zukowski</keyname><forenames>Ulrich</forenames></author></authors><title>Transformation-Based Bottom-Up Computation of the Well-Founded Model</title><categories>cs.LO</categories><comments>43 pages, 3 figures</comments><acm-class>I.2.3</acm-class><abstract>  We present a framework for expressing bottom-up algorithms to compute the
well-founded model of non-disjunctive logic programs. Our method is based on
the notion of conditional facts and elementary program transformations studied
by Brass and Dix for disjunctive programs. However, even if we restrict their
framework to nondisjunctive programs, their residual program can grow to
exponential size, whereas for function-free programs our program remainder is
always polynomial in the size of the extensional database (EDB).
  We show that particular orderings of our transformations (we call them
strategies) correspond to well-known computational methods like the alternating
fixpoint approach, the well-founded magic sets method and the magic alternating
fixpoint procedure. However, due to the confluence of our calculi, we come up
with computations of the well-founded model that are provably better than these
methods.
  In contrast to other approaches, our transformation method treats magic set
transformed programs correctly, i.e. it always computes a relevant part of the
well-founded model of the original program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011014</id><created>2000-11-09</created><authors><author><keyname>Liu</keyname><forenames>George Yong</forenames><affiliation>CMP Technology, Inc</affiliation></author><author><keyname>Zhang</keyname><forenames>Ray F.</forenames><affiliation>CMP Technology, Inc</affiliation></author><author><keyname>Hsu</keyname><forenames>Kelvin</forenames><affiliation>CMP Technology, Inc</affiliation></author><author><keyname>Camilletti</keyname><forenames>Lawrence</forenames><affiliation>Conexant Systems Inc</affiliation></author></authors><title>Chip-level CMP Modeling and Smart Dummy for HDP and Conformal CVD Films</title><categories>cs.CE</categories><comments>10 pages, 7 figures; for used software, see
  http://www.cmptechnology.com/</comments><acm-class>B.7.2</acm-class><journal-ref>Proceedings of CMPMIC 99, pp120-127</journal-ref><abstract>  Chip-level CMP modeling is investigated to obtain the post-CMP film profile
thickness across a die from its design layout file and a few film deposition
and CMP parameters. The work covers both HDP and conformal CVD film. The
experimental CMP results agree well with the modeled results. Different
algorithms for filling of dummy structure are compared. A smart algorithm for
dummy filling is presented, which achieves maximal pattern-density uniformity
and CMP planarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011015</id><created>2000-11-11</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lam</keyname><forenames>Tak-Wah</forenames></author><author><keyname>Sung</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Ting</keyname><forenames>Hing-Fung</forenames></author></authors><title>A Decomposition Theorem for Maximum Weight Bipartite Matchings</title><categories>cs.DS cs.DM</categories><comments>The journal version will appear in SIAM Journal on Computing. The
  conference version appeared in ESA 1999</comments><acm-class>E1; F2.2</acm-class><abstract>  Let G be a bipartite graph with positive integer weights on the edges and
without isolated nodes. Let n, N and W be the node count, the largest edge
weight and the total weight of G. Let k(x,y) be log(x)/log(x^2/y). We present a
new decomposition theorem for maximum weight bipartite matchings and use it to
design an O(sqrt(n)W/k(n,W/N))-time algorithm for computing a maximum weight
matching of G. This algorithm bridges a long-standing gap between the best
known time complexity of computing a maximum weight matching and that of
computing a maximum cardinality matching. Given G and a maximum weight matching
of G, we can further compute the weight of a maximum weight matching of G-{u}
for all nodes u in O(W) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011016</id><created>2000-11-12</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Tate</keyname><forenames>Stephen R.</forenames></author></authors><title>Designing Proxies for Stock Market Indices is Computationally Hard</title><categories>cs.CE cs.CC</categories><comments>An abstract appeared in the Proceedings of the 10th Annual ACM-SIAM
  Symposium on Discrete Algorithms, 1999</comments><acm-class>F.2.2;G.2.3;J.4</acm-class><abstract>  In this paper, we study the problem of designing proxies (or portfolios) for
various stock market indices based on historical data. We use four different
methods for computing market indices, all of which are formulas used in actual
stock market analysis. For each index, we consider three criteria for designing
the proxy: the proxy must either track the market index, outperform the market
index, or perform within a margin of error of the index while maintaining a low
volatility. In eleven of the twelve cases (all combinations of four indices
with three criteria except the problem of sacrificing return for less
volatility using the price-relative index) we show that the problem is NP-hard,
and hence most likely intractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011017</id><created>2000-11-13</created><authors><author><keyname>Schumann</keyname><forenames>Johann</forenames></author></authors><title>Automatic Debugging Support for UML Designs</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  Design of large software systems requires rigorous application of software
engineering methods covering all phases of the software process. Debugging
during the early design phases is extremely important, because late bug-fixes
are expensive.
  In this paper, we describe an approach which facilitates debugging of UML
requirements and designs. The Unified Modeling Language (UML) is a set of
notations for object-orient design of a software system. We have developed an
algorithm which translates requirement specifications in the form of annotated
sequence diagrams into structured statecharts. This algorithm detects conflicts
between sequence diagrams and inconsistencies in the domain knowledge. After
synthesizing statecharts from sequence diagrams, these statecharts usually are
subject to manual modification and refinement. By using the ``backward''
direction of our synthesis algorithm, we are able to map modifications made to
the statechart back into the requirements (sequence diagrams) and check for
conflicts there. Fed back to the user conflicts detected by our algorithm are
the basis for deductive-based debugging of requirements and domain theory in
very early development stages. Our approach allows to generate explanations on
why there is a conflict and which parts of the specifications are affected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011018</id><created>2000-11-14</created><authors><author><keyname>Chen</keyname><forenames>Gen-Huey</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lyuu</keyname><forenames>Yuh-Dauh</forenames></author><author><keyname>Wong</keyname><forenames>Hsing-Kuo</forenames></author></authors><title>Optimal Buy-and-Hold Strategies for Financial Markets with Bounded Daily
  Returns</title><categories>cs.CE cs.DS</categories><comments>The journal version will appear in SIAM Journal on Computing. A
  preliminary version appeared in Proceedings of the 31st Annual ACM Symposium
  on Theory of Computing, 1999, pages 119--128</comments><acm-class>F.2.2; I.1.2; J.4</acm-class><abstract>  In the context of investment analysis, we formulate an abstract online
computing problem called a planning game and develop general tools for solving
such a game. We then use the tools to investigate a practical buy-and-hold
trading problem faced by long-term investors in stocks. We obtain the unique
optimal static online algorithm for the problem and determine its exact
competitive ratio. We also compare this algorithm with the popular dollar
averaging strategy using actual market data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011019</id><created>2000-11-16</created><authors><author><keyname>Glasser</keyname><forenames>Christian</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author></authors><title>A Moment of Perfect Clarity II: Consequences of Sparse Sets Hard for NP
  with Respect to Weak Reductions</title><categories>cs.CC cs.DS</categories><comments>20 pages, 1 table</comments><report-no>URCS-TR-2000-738</report-no><acm-class>F.1.3; F.1.2</acm-class><abstract>  This paper discusses advances, due to the work of Cai, Naik, and Sivakumar
and Glasser, in the complexity class collapses that follow if NP has sparse
hard sets under reductions weaker than (full) truth-table reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011020</id><created>2000-11-16</created><updated>2000-11-30</updated><authors><author><keyname>Broeker</keyname><forenames>Norbert</forenames></author></authors><title>The Use of Instrumentation in Grammar Engineering</title><categories>cs.CL</categories><comments>7 pages, LaTeX2e, correction includes bibliography</comments><acm-class>D.2.5</acm-class><journal-ref>adapted from COLING2000, Saarbruecken/FRG, July31--Aug4 2000,
  pp.118-124</journal-ref><abstract>  This paper explores the usefulness of a technique from software engineering,
code instrumentation, for the development of large-scale natural language
grammars. Information about the usage of grammar rules in test and corpus
sentences is used to improve grammar and testsuite, as well as adapting a
grammar to a specific genre. Results show that less than half of a
large-coverage grammar for German is actually tested by two large testsuites,
and that 10--30% of testing time is redundant. This methodology applied can be
seen as a re-use of grammar writing knowledge for testsuite compilation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011021</id><created>2000-11-16</created><authors><author><keyname>Lencevicius</keyname><forenames>Raimondas</forenames></author></authors><title>On-the-fly Query-Based Debugging with Examples</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035 14
  pages, 6 figures</comments><acm-class>D.2.5</acm-class><abstract>  Program errors are hard to find because of the cause-effect gap between the
time when an error occurs and the time when the error becomes apparent to the
programmer. Although debugging techniques such as conditional and data
breakpoints help to find error causes in simple cases, they fail to effectively
bridge the cause-effect gap in many situations. Query-based debuggers offer
programmers an effective tool that provides instant error alert by continuously
checking inter-object relationships while the debugged program is running. To
enable the query-based debugger in the middle of program execution in a
portable way, we propose efficient Java class file instrumentation and discuss
alternative techniques. Although the on-the-fly debugger has a higher overhead
than a dynamic query-based debugger, it offers additional interactive power and
flexibility while maintaining complete portability. To speed up dynamic query
evaluation, our debugger implemented in portable Java uses a combination of
program instrumentation, load-time code generation, query optimization, and
incremental reevaluation. This paper discusses on-the-fly debugging and
demonstrates the query-based debugger application for debugging Java gas tank
applet as well as SPECjvm98 suite applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011022</id><created>2000-11-16</created><authors><author><keyname>Lencevicius</keyname><forenames>Raimondas</forenames></author><author><keyname>Ran</keyname><forenames>Alexander</forenames></author><author><keyname>Yairi</keyname><forenames>Rahav</forenames></author></authors><title>Apache web server execution tracing using Third Eye</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035 5
  pages, 2 figures</comments><acm-class>D.2.5</acm-class><abstract>  Testing of modern software systems that integrate many components developed
by different teams is a difficult task. Third Eye is a framework for tracing
and validating software systems using application domain events. We use formal
descriptions of the constraints between events to identify violations in
execution traces. Third Eye is a flexible and modular framework that can be
used in different products. We present the validation of the Apache Web Server
access policy implementation. The results indicate that our tool is a helpful
addition to software development infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011023</id><created>2000-11-16</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Qi</keyname><forenames>Junfeng</forenames></author><author><keyname>Tan</keyname><forenames>Lei</forenames></author></authors><title>Optimal Bidding Algorithms Against Cheating in Multiple-Object Auctions</title><categories>cs.CE cs.DS</categories><acm-class>F.2.2; G.2.1; J.4</acm-class><journal-ref>SIAM Journal on Computing, 28(3):955--969, 1999</journal-ref><abstract>  This paper studies some basic problems in a multiple-object auction model
using methodologies from theoretical computer science. We are especially
concerned with situations where an adversary bidder knows the bidding
algorithms of all the other bidders. In the two-bidder case, we derive an
optimal randomized bidding algorithm, by which the disadvantaged bidder can
procure at least half of the auction objects despite the adversary's a priori
knowledge of his algorithm. In the general $k$-bidder case, if the number of
objects is a multiple of $k$, an optimal randomized bidding algorithm is found.
If the $k-1$ disadvantaged bidders employ that same algorithm, each of them can
obtain at least $1/k$ of the objects regardless of the bidding algorithm the
adversary uses. These two algorithms are based on closed-form solutions to
certain multivariate probability distributions. In situations where a
closed-form solution cannot be obtained, we study a restricted class of bidding
algorithms as an approximation to desired optimal algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011024</id><created>2000-11-17</created><authors><author><keyname>Cohen</keyname><forenames>Sara</forenames></author><author><keyname>Nutt</keyname><forenames>Werner</forenames></author><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author></authors><title>Algorithms for Rewriting Aggregate Queries Using Views</title><categories>cs.DB</categories><comments>technical report CW 292 of Katholieke Universiteit Leuven (Short
  version in In Julius Stuller, Jaroslav Pokorn?, Bernhard Thalheim, Yoshifumi
  Masunaga (Eds.): Current Issues in Databases and Information Systems,
  East-European Conference on Advances in Databases and Information Systems
  Held Jointly with International Conference on Database Systems for Advanced
  Applications, ADBIS-DASFAA 2000, Prague, Czech Republic, September 5-8,
  2000.)</comments><acm-class>H.2.3</acm-class><abstract>  Queries involving aggregation are typical in database applications. One of
the main ideas to optimize the execution of an aggregate query is to reuse
results of previously answered queries. This leads to the problem of rewriting
aggregate queries using views. Due to a lack of theory, algorithms for this
problem were rather ad-hoc. They were sound, but were not proven to be
complete.
  Recently we have given syntactic characterizations for the equivalence of
aggregate queries and applied them to decide when there exist rewritings.
However, these decision procedures do not lend themselves immediately to an
implementation. In this paper, we present practical algorithms for rewriting
queries with $\COUNT$ and $\SUM$. Our algorithms are sound. They are also
complete for important cases. Our techniques can be used to improve well-known
procedures for rewriting non-aggregate queries. These procedures can then be
adapted to obtain algorithms for rewriting queries with $\MIN$ and $\MAX$. The
algorithms presented are a basis for realizing optimizers that rewrite queries
using views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011025</id><created>2000-11-17</created><authors><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author><author><keyname>De Schreye</keyname><forenames>Danny</forenames></author></authors><title>Termination analysis of logic programs using acceptability with general
  term orders</title><categories>cs.PL</categories><comments>technical report of K.U.Leuven</comments><acm-class>D.1.6; D.2.4</acm-class><abstract>  We present a new approach to termination analysis of logic programs. The
essence of the approach is that we make use of general term-orderings (instead
of level mappings), like it is done in transformational approaches to logic
program termination analysis, but that we apply these orderings directly to the
logic program and not to the term-rewrite system obtained through some
transformation. We define some variants of acceptability, based on general
term-orderings, and show how they are equivalent to LD-termination. We develop
a demand driven, constraint-based approach to verify these
acceptability-variants.
  The advantage of the approach over standard acceptability is that in some
cases, where complex level mappings are needed, fairly simple term-orderings
may be easily generated. The advantage over transformational approaches is that
it avoids the transformation step all together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011026</id><created>2000-11-20</created><updated>2003-08-30</updated><authors><author><keyname>Arkin</keyname><forenames>Esther M.</forenames></author><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Sethia</keyname><forenames>Saurabh</forenames></author><author><keyname>Skiena</keyname><forenames>Steven S.</forenames></author></authors><title>When Can You Fold a Map?</title><categories>cs.CG cs.DM</categories><comments>24 pages, 19 figures. Version 3 includes several improvements thanks
  to referees, including formal definitions of simple folds, more figures,
  table summarizing results, new open problems, and additional references</comments><acm-class>F.2.2; G.2.1</acm-class><abstract>  We explore the following problem: given a collection of creases on a piece of
paper, each assigned a folding direction of mountain or valley, is there a flat
folding by a sequence of simple folds? There are several models of simple
folds; the simplest one-layer simple fold rotates a portion of paper about a
crease in the paper by +-180 degrees. We first consider the analogous questions
in one dimension lower -- bending a segment into a flat object -- which lead to
interesting problems on strings. We develop efficient algorithms for the
recognition of simply foldable 1D crease patterns, and reconstruction of a
sequence of simple folds. Indeed, we prove that a 1D crease pattern is
flat-foldable by any means precisely if it is by a sequence of one-layer simple
folds.
  Next we explore simple foldability in two dimensions, and find a surprising
contrast: ``map'' folding and variants are polynomial, but slight
generalizations are NP-complete. Specifically, we develop a linear-time
algorithm for deciding foldability of an orthogonal crease pattern on a
rectangular piece of paper, and prove that it is (weakly) NP-complete to decide
foldability of (1) an orthogonal crease pattern on a orthogonal piece of paper,
(2) a crease pattern of axis-parallel and diagonal (45-degree) creases on a
square piece of paper, and (3) crease patterns without a mountain/valley
assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011027</id><created>2000-11-20</created><authors><author><keyname>Mateis</keyname><forenames>Cristinel</forenames></author><author><keyname>Stumptner</keyname><forenames>Markus</forenames></author><author><keyname>Wieland</keyname><forenames>Dominik</forenames></author><author><keyname>Wotawa</keyname><forenames>Franz</forenames></author></authors><title>Extended Abstract - Model-Based Debugging of Java Programs</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  Model-based reasoning is a central concept in current research into
intelligent diagnostic systems. It is based on the assumption that sources of
incorrect behavior in technical devices can be located and identified via the
existence of a model describing the basic properties of components of a certain
application domain. When actual data concerning the misbehavior of a system
composed from such components is available, a domain-independent diagnosis
engine can be used to infer which parts of the system contribute to the
observed behavior. This paper describes the application of the model-based
approach to the debugging of Java programs written in a subset of Java. We show
how a simple dependency model can be derived from a program, demonstrate the
use of the model for debugging and reducing the required user interactions,
give a comparison of the functional dependency model with program slicing, and
finally discuss some current research issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011028</id><created>2000-11-20</created><authors><author><keyname>Elworthy</keyname><forenames>David</forenames></author></authors><title>Retrieval from Captioned Image Databases Using Natural Language
  Processing</title><categories>cs.CL cs.IR</categories><comments>Proceedings of CIKM 2000</comments><acm-class>H.3.1; H.3.3</acm-class><abstract>  It might appear that natural language processing should improve the accuracy
of information retrieval systems, by making available a more detailed analysis
of queries and documents. Although past results appear to show that this is not
so, if the focus is shifted to short phrases rather than full documents, the
situation becomes somewhat different. The ANVIL system uses a natural language
technique to obtain high accuracy retrieval of images which have been annotated
with a descriptive textual caption. The natural language techniques also allow
additional contextual information to be derived from the relation between the
query and the caption, which can help users to understand the overall
collection of retrieval results. The techniques have been successfully used in
a information retrieval system which forms both a testbed for research and the
basis of a commercial system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011029</id><created>2000-11-21</created><authors><author><keyname>Ikezoe</keyname><forenames>Yohei</forenames></author><author><keyname>Sasaki</keyname><forenames>Akira</forenames></author><author><keyname>Ohshima</keyname><forenames>Yoshiki</forenames></author><author><keyname>Wakita</keyname><forenames>Ken</forenames></author><author><keyname>Sassa</keyname><forenames>Masataka</forenames></author></authors><title>Systematic Debugging of Attribute Grammars</title><categories>cs.SE</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  Although attribute grammars are commonly used for compiler construction,
little investigation has been conducted on debugging attribute grammars. The
paper proposes two types of systematic debugging methods, an algorithmic
debugging and slice-based debugging, both tailored for attribute grammars. By
means of query-based interaction with the developer, our debugging methods
effectively narrow the potential bug space in the attribute grammar description
and eventually identify the incorrect attribution rule. We have incorporated
this technology in our visual debugging tool called Aki.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011030</id><created>2000-11-21</created><authors><author><keyname>Pelov</keyname><forenames>Nikolay</forenames></author><author><keyname>De Mot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Logic Programming Approaches for Representing and Solving Constraint
  Satisfaction Problems: A Comparison</title><categories>cs.AI</categories><comments>15 pages, 3 eps-figures</comments><acm-class>I.2.3; I.2.4</acm-class><journal-ref>LPAR 2000, Lecture Notes in Artificial Intelligence, vol. 1955,
  Springer, 2000, pp. 225-239</journal-ref><abstract>  Many logic programming based approaches can be used to describe and solve
combinatorial search problems. On the one hand there is constraint logic
programming which computes a solution as an answer substitution to a query
containing the variables of the constraint satisfaction problem. On the other
hand there are systems based on stable model semantics, abductive systems, and
first order logic model generators which compute solutions as models of some
theory. This paper compares these different approaches from the point of view
of knowledge representation (how declarative are the programs) and from the
point of view of performance (how good are they at solving typical problems).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011031</id><created>2000-11-21</created><authors><author><keyname>Giglioli</keyname><forenames>N.</forenames><affiliation>Joint Research Centre, Ispra, Italy</affiliation></author><author><keyname>Saltelli</keyname><forenames>A.</forenames><affiliation>Joint Research Centre, Ispra, Italy</affiliation></author></authors><title>SimLab 1.1, Software for Sensitivity and Uncertainty Analysis, tool for
  sound modelling</title><categories>cs.DM</categories><comments>11 pages, pdf format, to be submitted to JACM</comments><acm-class>G.3</acm-class><abstract>  The aim of this paper is to present and describe SimLab 1.1 (Simulation
Laboratory for Uncertainty and Sensitivity Analysis) software designed for
Monte Carlo analysis that is based on performing multiple model evaluations
with probabilistically selected model input. The results of these evaluations
are used to determine both the uncertainty in model predictions and the input
variables that drive this uncertainty. This methodology is essential in
situations where a decision has to be taken based on the model results; typical
examples include risk and emergency management systems, financial analysis and
many others. It is also highly recommended as part of model validation, even
where the models are used for diagnostic purposes, as an element of sound model
building. SimLab allows an exploration of the space of possible alternative
model assumptions and structure on the prediction of the model, thereby testing
both the quality of the model and the robustness of the model based inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011032</id><created>2000-11-21</created><authors><author><keyname>Blockeel</keyname><forenames>Hendrik</forenames></author><author><keyname>De Raedt</keyname><forenames>Luc</forenames></author><author><keyname>Ramon</keyname><forenames>Jan</forenames></author></authors><title>Top-down induction of clustering trees</title><categories>cs.LG</categories><comments>9 pages, 3 figures</comments><acm-class>I.2.6</acm-class><journal-ref>Machine Learning, Proceedings of the 15th International Conference
  (J. Shavlik, ed.), Morgan Kaufmann, 1998, pp. 55-63</journal-ref><abstract>  An approach to clustering is presented that adapts the basic top-down
induction of decision trees method towards clustering. To this aim, it employs
the principles of instance based learning. The resulting methodology is
implemented in the TIC (Top down Induction of Clustering trees) system for
first order clustering. The TIC system employs the first order logical decision
tree representation of the inductive logic programming system Tilde. Various
experiments with TIC are presented, in both propositional and relational
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011033</id><created>2000-11-22</created><authors><author><keyname>Kosala</keyname><forenames>Raymond</forenames></author><author><keyname>Blockeel</keyname><forenames>Hendrik</forenames></author></authors><title>Web Mining Research: A Survey</title><categories>cs.LG cs.DB</categories><comments>15 pages</comments><acm-class>I.2.6; H.2.8</acm-class><journal-ref>ACM SIGKDD Explorations, 2(1):1-15, 2000</journal-ref><abstract>  With the huge amount of information available online, the World Wide Web is a
fertile area for data mining research. The Web mining research is at the cross
road of research from several research communities, such as database,
information retrieval, and within AI, especially the sub-areas of machine
learning and natural language processing. However, there is a lot of confusions
when comparing research efforts from different point of views. In this paper,
we survey the research in the area of Web mining, point out some confusions
regarded the usage of the term Web mining and suggest three Web mining
categories. Then we situate some of the research with respect to these three
categories. We also explore the connection between the Web mining categories
and the related agent paradigm. For the survey, we focus on representation
issues, on the process, on the learning algorithm, and on the application of
the recent works as the criteria. We conclude the paper with some research
issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011034</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011034</id><created>2000-11-22</created><authors><author><keyname>Verdoolaege</keyname><forenames>Sven</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author><author><keyname>Schelkens</keyname><forenames>Ness</forenames></author><author><keyname>De Schreye</keyname><forenames>Danny</forenames></author><author><keyname>Van Eynde</keyname><forenames>Frank</forenames></author></authors><title>Semantic interpretation of temporal information by abductive inference</title><categories>cs.CL</categories><comments>11 pages</comments><acm-class>I.2.4; I.2.7</acm-class><abstract>  Besides temporal information explicitly available in verbs and adjuncts, the
temporal interpretation of a text also depends on general world knowledge and
default assumptions. We will present a theory for describing the relation
between, on the one hand, verbs, their tenses and adjuncts and, on the other,
the eventualities and periods of time they represent and their relative
temporal locations.
  The theory is formulated in logic and is a practical implementation of the
concepts described in Ness Schelkens et al. We will show how an abductive
resolution procedure can be used on this representation to extract temporal
information from texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011035</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011035</id><created>2000-11-23</created><authors><author><keyname>Verdoolaege</keyname><forenames>Sven</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author><author><keyname>Van Eynde</keyname><forenames>Frank</forenames></author></authors><title>Abductive reasoning with temporal information</title><categories>cs.CL</categories><comments>16 pages</comments><acm-class>I.2.4; I.2.7</acm-class><abstract>  Texts in natural language contain a lot of temporal information, both
explicit and implicit. Verbs and temporal adjuncts carry most of the explicit
information, but for a full understanding general world knowledge and default
assumptions have to be taken into account. We will present a theory for
describing the relation between, on the one hand, verbs, their tenses and
adjuncts and, on the other, the eventualities and periods of time they
represent and their relative temporal locations, while allowing interaction
with general world knowledge.
  The theory is formulated in an extension of first order logic and is a
practical implementation of the concepts described in Van Eynde 2001 and
Schelkens et al. 2000. We will show how an abductive resolution procedure can
be used on this representation to extract temporal information from texts. The
theory presented here is an extension of that in Verdoolaege et al. 2000,
adapted to VanEynde 2001, with a simplified and extended analysis of adjuncts
and with more emphasis on how a model can be constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011036</id><created>2000-11-23</created><authors><author><keyname>Dershowitz</keyname><forenames>Nachum</forenames></author><author><keyname>Lindenstrauss</keyname><forenames>Naomi</forenames></author><author><keyname>Sagiv</keyname><forenames>Yehoshua</forenames></author><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author></authors><title>Automatic Termination Analysis of Programs Containing Arithmetic
  Predicates</title><categories>cs.PL</categories><comments>Appeared also in Electronic Notes in Computer Science vol. 30</comments><acm-class>D.1.6; D.2.4</acm-class><abstract>  For logic programs with arithmetic predicates, showing termination is not
easy, since the usual order for the integers is not well-founded. A new method,
easily incorporated in the TermiLog system for automatic termination analysis,
is presented for showing termination in this case.
  The method consists of the following steps: First, a finite abstract domain
for representing the range of integers is deduced automatically. Based on this
abstraction, abstract interpretation is applied to the program. The result is a
finite number of atoms abstracting answers to queries which are used to extend
the technique of query-mapping pairs. For each query-mapping pair that is
potentially non-terminating, a bounded (integer-valued) termination function is
guessed. If traversing the pair decreases the value of the termination
function, then termination is established. Simple functions often suffice for
each query-mapping pair, and that gives our approach an edge over the classical
approach of using a single termination function for all loops, which must
inevitably be more complicated and harder to guess automatically. It is worth
noting that the termination of McCarthy's 91 function can be shown
automatically using our method.
  In summary, the proposed approach is based on combining a finite abstraction
of the integers with the technique of the query-mapping pairs, and is
essentially capable of dividing a termination proof into several cases, such
that a simple termination function suffices for each case. Consequently, the
whole process of proving termination can be done automatically in the framework
of TermiLog and similar systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011037</id><created>2000-11-23</created><updated>2001-09-14</updated><authors><author><keyname>Aehlig</keyname><forenames>Klaus</forenames></author><author><keyname>Schwichtenberg</keyname><forenames>Helmut</forenames></author></authors><title>A syntactical analysis of non-size-increasing polynomial time
  computation</title><categories>cs.LO</categories><comments>20 pages (latex), revised submission (expanded proofs, extended
  references, new section on tree iteration)</comments><acm-class>F.4.1; F.2.2</acm-class><journal-ref>ACM Transactions on Computational Logic 3(3), 383-401 (2002)</journal-ref><abstract>  A syntactical proof is given that all functions definable in a certain affine
linear typed lambda-calculus with iteration in all types are polynomial time
computable. The proof provides explicit polynomial bounds that can easily be
calculated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011038</id><created>2000-11-23</created><authors><author><keyname>Csuros</keyname><forenames>Miklos</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Provably Fast and Accurate Recovery of Evolutionary Trees through
  Harmonic Greedy Triplets</title><categories>cs.DS cs.LG</categories><comments>The paper will appear in SIAM Journal on Computing</comments><acm-class>E.1; F.2.2; G.2.1; G.2.2; G.2.3; G.3; G.4; J.3</acm-class><abstract>  We give a greedy learning algorithm for reconstructing an evolutionary tree
based on a certain harmonic average on triplets of terminal taxa. After the
pairwise distances between terminal taxa are estimated from sequence data, the
algorithm runs in O(n^2) time using O(n) work space, where n is the number of
terminal taxa. These time and space complexities are optimal in the sense that
the size of an input distance matrix is n^2 and the size of an output tree is
n. Moreover, in the Jukes-Cantor model of evolution, the algorithm recovers the
correct tree topology with high probability using sample sequences of length
polynomial in (1) n, (2) the logarithm of the error probability, and (3) the
inverses of two small parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011039</id><created>2000-11-23</created><authors><author><keyname>Dezani-Ciancaglini</keyname><forenames>M.</forenames></author><author><keyname>Honsell</keyname><forenames>F.</forenames></author><author><keyname>Alessi</keyname><forenames>F.</forenames></author></authors><title>A Complete Characterization of Complete Intersection-Type Theories</title><categories>cs.LO</categories><comments>26 pages, no figure</comments><acm-class>F.4.1; F.3.3; F.3.2; D.1.1</acm-class><abstract>  We characterize those intersection-type theories which yield complete
intersection-type assignment systems for lambda-calculi, with respect to the
three canonical set-theoretical semantics for intersection-types: the inference
semantics, the simple semantics and the F-semantics. These semantics arise by
taking as interpretation of types subsets of applicative structures, as
interpretation of the intersection constructor set-theoretic inclusion, and by
taking the interpretation of the arrow constructor a' la Scott, with respect to
either any possible functionality set, or the largest one, or the least one.
  These results strengthen and generalize significantly all earlier results in
the literature, to our knowledge, in at least three respects. First of all the
inference semantics had not been considered before. Secondly, the
characterizations are all given just in terms of simple closure conditions on
the preorder relation on the types, rather than on the typing judgments
themselves. The task of checking the condition is made therefore considerably
more tractable. Lastly, we do not restrict attention just to lambda-models, but
to arbitrary applicative structures which admit an interpretation function.
Thus we allow also for the treatment of models of restricted lambda-calculi.
Nevertheless the characterizations we give can be tailored just to the case of
lambda-models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011040</id><created>2000-11-24</created><authors><author><keyname>Bod</keyname><forenames>Rens</forenames></author></authors><title>Do All Fragments Count?</title><categories>cs.CL</categories><comments>18 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Technical Report COMP-11-12</journal-ref><abstract>  We aim at finding the minimal set of fragments which achieves maximal parse
accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street
Journal treebank show that counts of almost arbitrary fragments within parse
trees are important, leading to improved parse accuracy over previous models
tested on this treebank. We isolate a number of dependency relations which
previous models neglect but which contribute to higher parse accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011041</id><created>2000-11-27</created><authors><author><keyname>Cohen</keyname><forenames>Sara</forenames></author><author><keyname>Kanza</keyname><forenames>Yaron</forenames></author><author><keyname>Kogan</keyname><forenames>Yakov</forenames></author><author><keyname>Nutt</keyname><forenames>Werner</forenames></author><author><keyname>Sagiv</keyname><forenames>Yehoshua</forenames></author><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author></authors><title>EquiX---A Search and Query Language for XML</title><categories>cs.DB</categories><comments>technical report of Hebrew University Jerusalem Israel</comments><acm-class>H.2.3</acm-class><abstract>  EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graphical abstract
syntax and a formal concrete syntax are presented for EquiX queries. In
addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011042</id><created>2000-11-27</created><authors><author><keyname>Turner</keyname><forenames>Hudson</forenames></author></authors><title>Order-consistent programs are cautiously monotonic</title><categories>cs.LO cs.AI</categories><comments>9 pages</comments><acm-class>F.4.1;I.2.3</acm-class><abstract>  Some normal logic programs under the answer set (stable model) semantics lack
the appealing property of &quot;cautious monotonicity.&quot; That is, augmenting a
program with one of its consequences may cause it to lose another of its
consequences. The syntactic condition of &quot;order-consistency&quot; was shown by Fages
to guarantee existence of an answer set. This note establishes that
order-consistent programs are not only consistent, but cautiously monotonic.
  From this it follows that they are also &quot;cumulative.&quot; That is, augmenting an
order-consistent with some of its consequences does not alter its consequences.
In fact, as we show, its answer sets remain unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011043</id><created>2000-11-28</created><authors><author><keyname>Cirstea</keyname><forenames>Horatiu</forenames></author></authors><title>Rewriting Calculus: Foundations and Applications</title><categories>cs.SC cs.LO cs.PL</categories><comments>PhD Thesis in French</comments><acm-class>I.1; D.1; D.3; F.4.0; F.4.1</acm-class><abstract>  This thesis is devoted to the study of a calculus that describes the
application of conditional rewriting rules and the obtained results at the same
level of representation. We introduce the rewriting calculus, also called the
rho-calculus, which generalizes the first order term rewriting and
lambda-calculus, and makes possible the representation of the non-determinism.
In our approach the abstraction operator as well as the application operator
are objects of calculus. The result of a reduction in the rewriting calculus is
either an empty set representing the application failure, or a singleton
representing a deterministic result, or a set having several elements
representing a not-deterministic choice of results.
  In this thesis we concentrate on the properties of the rewriting calculus
where a syntactic matching is used in order to bind the variables to their
current values. We define evaluation strategies ensuring the confluence of the
calculus and we show that these strategies become trivial for restrictions of
the general rewriting calculus to simpler calculi like the lambda-calculus. The
rewriting calculus is not terminating in the untyped case but the strong
normalization is obtained for the simply typed calculus.
  In the rewriting calculus extended with an operator allowing to test the
application failure we define terms representing innermost and outermost
normalizations with respect to a set of rewriting rules. By using these terms,
we obtain a natural and concise description of the conditional rewriting.
Finally, starting from the representation of the conditional rewriting rules,
we show how the rewriting calculus can be used to give a semantics to ELAN, a
language based on the application of rewriting rules controlled by strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011044</id><created>2000-11-29</created><authors><author><keyname>Blockeel</keyname><forenames>Hendrik</forenames><affiliation>Katholieke Universiteit Leuven, Dept. of Computer Science</affiliation></author><author><keyname>De Raedt</keyname><forenames>Luc</forenames><affiliation>Katholieke Universiteit Leuven, Dept. of Computer Science</affiliation></author><author><keyname>Jacobs</keyname><forenames>Nico</forenames><affiliation>Katholieke Universiteit Leuven, Dept. of Computer Science</affiliation></author><author><keyname>Demoen</keyname><forenames>Bart</forenames><affiliation>Katholieke Universiteit Leuven, Dept. of Computer Science</affiliation></author></authors><title>Scaling Up Inductive Logic Programming by Learning from Interpretations</title><categories>cs.LG</categories><comments>37 pages</comments><report-no>CW-297</report-no><acm-class>I.2.6 ; I.2.3</acm-class><journal-ref>Data Mining and Knowledge Discovery 3(1), pp. 59-93, 1999</journal-ref><abstract>  When comparing inductive logic programming (ILP) and attribute-value learning
techniques, there is a trade-off between expressive power and efficiency.
Inductive logic programming techniques are typically more expressive but also
less efficient. Therefore, the data sets handled by current inductive logic
programming systems are small according to general standards within the data
mining community. The main source of inefficiency lies in the assumption that
several examples may be related to each other, so they cannot be handled
independently.
  Within the learning from interpretations framework for inductive logic
programming this assumption is unnecessary, which allows to scale up existing
ILP algorithms. In this paper we explain this learning setting in the context
of relational databases. We relate the setting to propositional data mining and
to the classical ILP setting, and show that learning from interpretations
corresponds to learning from multiple relations and thus extends the
expressiveness of propositional learning, while maintaining its efficiency to a
large extent (which is not the case in the classical ILP setting).
  As a case study, we present two alternative implementations of the ILP system
Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which
loads all data in main memory, and Tilde-LDS, which loads the examples one by
one. We experimentally compare the implementations, showing Tilde-LDS can
handle large data sets (in the order of 100,000 examples or 100 MB) and indeed
scales up linearly in the number of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011045</id><created>2000-11-30</created><authors><author><keyname>Berger-Wolf</keyname><forenames>Tanya Y.</forenames></author><author><keyname>Reingold</keyname><forenames>Edward M.</forenames></author></authors><title>Index Assignment for Multichannel Communication under Failure</title><categories>cs.DS cs.DM</categories><comments>10 pages, 12 figures. First appeard in SODA 99, submitted to IEEE
  Transactions on Information Theory</comments><acm-class>H.1.1; F.2.2; G.2.2</acm-class><abstract>  We consider the problem of multiple description scalar quantizers and
describing the achievable rate-distortion tuples in that setting. We formulate
it as a combinatorial optimization problem of arranging numbers in a matrix to
minimize the maximum difference between the largest and the smallest number in
any row or column. We develop a technique for deriving lower bounds on the
distortion at given channel rates. The approach is constructive, thus allowing
an algorithm that gives a closely matching upper bound. For the case of two
communication channels with equal rates, the bounds coincide, thus giving the
precise lowest achievable distortion at fixed rates. The bounds are within a
small constant for higher number of channels. To the best of our knowledge,
this is the first result concerning systems with more than two communication
channels. The problem is also equivalent to the bandwidth minimization problem
of Hamming graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011046</id><created>2000-11-30</created><authors><author><keyname>Herman</keyname><forenames>Ted</forenames></author><author><keyname>Masuzawa</keyname><forenames>Toshimitsu</forenames></author></authors><title>Available Stabilizing Heaps</title><categories>cs.DC cs.DS</categories><comments>10 pages</comments><report-no>University of Iowa Department of Computer Science TR 00-03</report-no><acm-class>C.2.4; D.1.3; D.3.3; E.2; C.4</acm-class><abstract>  This paper describes a heap construction that supports insert and delete
operations in arbitrary (possibly illegitimate) states. After any sequence of
at most O(m) heap operations, the heap state is guarantee to be legitimate,
where m is the initial number of items in the heap. The response from each
operation is consistent with its effect on the data structure, even for
illegitimate states. The time complexity of each operation is O(lg K) where K
is the capacity of the data structure; when the heap's state is legitimate the
time complexity is O(lg n) for n equal to the number items in the heap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0011047</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0011047</id><created>2000-11-14</created><authors><author><keyname>Knuth</keyname><forenames>Donald E.</forenames></author></authors><title>Dancing links</title><categories>cs.DS</categories><comments>Abstract added by Greg Kuperberg</comments><report-no>Knuth migration 11/2004</report-no><journal-ref>Millenial Perspectives in Computer Science, 2000, 187--214</journal-ref><abstract>  The author presents two tricks to accelerate depth-first search algorithms
for a class of combinatorial puzzle problems, such as tiling a tray by a fixed
set of polyominoes. The first trick is to implement each assumption of the
search with reversible local operations on doubly linked lists. By this trick,
every step of the search affects the data incrementally.
  The second trick is to add a ghost square that represents the identity of
each polyomino. Thus puts the rule that each polyomino be used once on the same
footing as the rule that each square be covered once. The coding simplifies to
a more abstract form which is equivalent to 0-1 integer programming. More
significantly for the total computation time, the search can naturally switch
between placing a fixed polyomino or covering a fixed square at different
stages, according to a combined heuristic.
  Finally the author reports excellent performance for his algorithm for some
familiar puzzles. These include tiling a hexagon by 19 hexiamonds and the N
queens problem for N up to 18.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012001</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012001</id><created>2000-12-01</created><authors><author><keyname>Herman</keyname><forenames>Ted</forenames></author><author><keyname>Masuzawa</keyname><forenames>Toshimitsu</forenames></author></authors><title>Available and Stabilizing 2-3 Trees</title><categories>cs.DC cs.DS</categories><comments>22 pages, 5 figures</comments><report-no>University of Iowa Department of Computer Science TR 00-04</report-no><acm-class>D.4.5; E.1; C.4</acm-class><abstract>  Transient faults corrupt the content and organization of data structures. A
recovery technique dealing with such faults is stabilization, which guarantees,
following some number of operations on the data structure, that content of the
data structure is legitimate. Another notion of fault tolerance is
availability, which is the property that operations continue to be applied
during the period of recovery after a fault, and successful updates are not
lost while the data structure stabilizes to a legitimate state. The available,
stabilizing 2-3 tree supports find, insert, and delete operations, each with
O(lg n) complexity when the tree's state is legitimate and contains n items.
For an illegitimate state, these operations have O(lg K) complexity where K is
the maximum capacity of the tree. Within O(t) operations, the state of the tree
is guaranteed to be legitimate, where t is the number of nodes accessible via
some path from the tree's root at the initial state. This paper resolves, for
the first time, issues of dynamic allocation and pointer organization in a
stabilizing data structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012002</id><created>2000-12-02</created><authors><author><keyname>Karim</keyname><forenames>Md. Enamul</forenames><affiliation>University of Dhaka</affiliation></author><author><keyname>Mahmood</keyname><forenames>Abdun Naser</forenames><affiliation>University of Dhaka</affiliation></author></authors><title>Random Shuffling to Reduce Disorder in Adaptive Sorting Scheme</title><categories>cs.DS</categories><comments>7 pages, 2 tables</comments><acm-class>F.2.2</acm-class><abstract>  In this paper we present a random shuffling scheme to apply with adaptive
sorting algorithms. Adaptive sorting algorithms utilize the presortedness
present in a given sequence. We have probabilistically increased the amount of
presortedness present in a sequence by using a random shuffling technique that
requires little computation. Theoretical analysis suggests that the proposed
scheme can improve the performance of adaptive sorting. Experimental results
show that it significantly reduces the amount of disorder present in a given
sequence and improves the execution time of adaptive sorting algorithm as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012003</id><created>2000-12-11</created><authors><author><keyname>Snyder</keyname><forenames>Douglas M.</forenames></author></authors><title>Questions for a Materialist Philosophy Implying the Equivalence of
  Computers and Human Cognition</title><categories>cs.GL</categories><comments>20 pages</comments><report-no>0196</report-no><acm-class>A.0</acm-class><abstract>  Issues related to a materialist philosophy are explored as concerns the
implied equivalence of computers running software and human observers. One
issue explored concerns the measurement process in quantum mechanics. Another
issue explored concerns the nature of experience as revealed by the existence
of dreams. Some difficulties stemming from a materialist philosophy as regards
these issues are pointed out. For example, a gedankenexperiment involving what
has been called &quot;negative&quot; observation is discussed that illustrates the
difficulty with a materialist assumption in quantum mechanics. Based on an
exploration of these difficulties, specifications are outlined briefly that
would provide a means to demonstrate the equivalence of of computers running
software and human experience given a materialist assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012004</id><created>2000-12-11</created><authors><author><keyname>Ozcan</keyname><forenames>Fatma</forenames></author><author><keyname>Subrahmanian</keyname><forenames>VS</forenames></author><author><keyname>Dix</keyname><forenames>Juergen</forenames></author></authors><title>Improving Performance of heavily loaded agents</title><categories>cs.MA cs.AI</categories><comments>63 pages, 26 figures, 10 tables</comments><acm-class>I.2.12;I.2.3;D.2.12;H.2.4</acm-class><abstract>  With the increase in agent-based applications, there are now agent systems
that support \emph{concurrent} client accesses. The ability to process large
volumes of simultaneous requests is critical in many such applications. In such
a setting, the traditional approach of serving these requests one at a time via
queues (e.g. \textsf{FIFO} queues, priority queues) is insufficient.
Alternative models are essential to improve the performance of such
\emph{heavily loaded} agents. In this paper, we propose a set of
\emph{cost-based algorithms} to \emph{optimize} and \emph{merge} multiple
requests submitted to an agent. In order to merge a set of requests, one first
needs to identify commonalities among such requests. First, we provide an
\emph{application independent framework} within which an agent developer may
specify relationships (called \emph{invariants}) between requests. Second, we
provide two algorithms (and various accompanying heuristics) which allow an
agent to automatically rewrite requests so as to avoid redundant work---these
algorithms take invariants associated with the agent into account. Our
algorithms are independent of any specific agent framework. For an
implementation, we implemented both these algorithms on top of the \impact
agent development platform, and on top of a (non-\impact) geographic database
agent. Based on these implementations, we conducted experiments and show that
our algorithms are considerably more efficient than methods that use the $A^*$
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012005</id><created>2000-12-11</created><authors><author><keyname>Ferrand</keyname><forenames>Gerard</forenames></author><author><keyname>Lesaint</keyname><forenames>Willy</forenames></author><author><keyname>Tessier</keyname><forenames>Alexandre</forenames></author></authors><title>Value Withdrawal Explanation in CSP</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  This work is devoted to constraint solving motivated by the debugging of
constraint logic programs a la GNU-Prolog. The paper focuses only on the
constraints. In this framework, constraint solving amounts to domain reduction.
A computation is formalized by a chaotic iteration. The computed result is
described as a closure. This model is well suited to the design of debugging
notions and tools, for example failure explanations or error diagnosis. In this
paper we detail an application of the model to an explanation of a value
withdrawal in a domain. Some other works have already shown the interest of
such a notion of explanation not only for failure analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012006</id><created>2000-12-11</created><authors><author><keyname>Hood</keyname><forenames>Robert</forenames></author><author><keyname>Jost</keyname><forenames>Gabriele</forenames></author></authors><title>Support for Debugging Automatically Parallelized Programs</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  We describe a system that simplifies the process of debugging programs
produced by computer-aided parallelization tools. The system uses relative
debugging techniques to compare serial and parallel executions in order to show
where the computations begin to differ. If the original serial code is correct,
errors due to parallelization will be isolated by the comparison.
  One of the primary goals of the system is to minimize the effort required of
the user. To that end, the debugging system uses information produced by the
parallelization tool to drive the comparison process. In particular, the
debugging system relies on the parallelization tool to provide information
about where variables may have been modified and how arrays are distributed
across multiple processes. User effort is also reduced through the use of
dynamic instrumentation. This allows us to modify the program execution without
changing the way the user builds the executable.
  The use of dynamic instrumentation also permits us to compare the executions
in a fine-grained fashion and only involve the debugger when a difference has
been detected. This reduces the overhead of executing instrumentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012007</id><created>2000-12-13</created><updated>2001-01-05</updated><authors><author><keyname>Ajiro</keyname><forenames>Yasuhiro</forenames></author><author><keyname>Ueda</keyname><forenames>Kazunori</forenames></author></authors><title>Kima - an Automated Error Correction System for Concurrent Logic
  Programs</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  We have implemented Kima, an automated error correction system for concurrent
logic programs. Kima corrects near-misses such as wrong variable occurrences in
the absence of explicit declarations of program properties. Strong
moding/typing and constraint-based analysis are turning to play fundamental
roles in debugging concurrent logic programs as well as in establishing the
consistency of communication protocols and data types. Mode/type analysis of
Moded Flat GHC is a constraint satisfaction problem with many simple mode/type
constraints, and can be solved efficiently. We proposed a simple and efficient
technique which, given a non-well-moded/typed program, diagnoses the
``reasons'' of inconsistency by finding minimal inconsistent subsets of
mode/type constraints. Since each constraint keeps track of the symbol
occurrence in the program, a minimal subset also tells possible sources of
program errors. Kima realizes automated correction by replacing symbol
occurrences around the possible sources and recalculating modes and types of
the rewritten programs systematically. As long as bugs are near-misses, Kima
proposes a rather small number of alternatives that include an intended
program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012008</id><created>2000-12-13</created><authors><author><keyname>Dershowitz</keyname><forenames>Nachum</forenames></author><author><keyname>Lindenstrauss</keyname><forenames>Naomi</forenames></author><author><keyname>Sagiv</keyname><forenames>Yehoshua</forenames></author><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author></authors><title>A General Framework for Automatic Termination Analysis of Logic Programs</title><categories>cs.PL</categories><acm-class>D.1.6</acm-class><abstract>  This paper describes a general framework for automatic termination analysis
of logic programs, where we understand by ``termination'' the finitenes s of
the LD-tree constructed for the program and a given query. A general property
of mappings from a certain subset of the branches of an infinite LD-tree into a
finite set is proved. From this result several termination theorems are
derived, by using different finite sets. The first two are formulated for the
predicate dependency and atom dependency graphs. Then a general result for the
case of the query-mapping pairs relevant to a program is proved (cf.
\cite{Sagiv,Lindenstrauss:Sagiv}). The correctness of the {\em TermiLog} system
described in \cite{Lindenstrauss:Sagiv:Serebrenik} follows from it. In this
system it is not possible to prove termination for programs involving
arithmetic predicates, since the usual order for the integers is not
well-founded. A new method, which can be easily incorporated in {\em TermiLog}
or similar systems, is presented, which makes it possible to prove termination
for programs involving arithmetic predicates. It is based on combining a finite
abstraction of the integers with the technique of the query-mapping pairs, and
is essentially capable of dividing a termination proof into several cases, such
that a simple termination function suffices for each case. Finally several
possible extensions are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012009</id><created>2000-12-14</created><authors><author><keyname>Cleve</keyname><forenames>Holger</forenames></author><author><keyname>Zeller</keyname><forenames>Andreas</forenames></author></authors><title>Finding Failure Causes through Automated Testing</title><categories>cs.SE</categories><acm-class>D.2.5</acm-class><abstract>  A program fails. Under which circumstances does this failure occur? One
single algorithm, the delta debugging algorithm, suffices to determine these
failure-inducing circumstances. Delta debugging tests a program systematically
and automatically to isolate failure-inducing circumstances such as the program
input, changes to the program code, or executed statements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012010</id><created>2000-12-15</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>The Role of Commutativity in Constraint Propagation Algorithms</title><categories>cs.PF cs.AI</categories><comments>35 pages. To appear in ACM TOPLAS</comments><acm-class>D.3.3;I.1.2;I.1.3</acm-class><abstract>  Constraint propagation algorithms form an important part of most of the
constraint programming systems. We provide here a simple, yet very general
framework that allows us to explain several constraint propagation algorithms
in a systematic way. In this framework we proceed in two steps. First, we
introduce a generic iteration algorithm on partial orderings and prove its
correctness in an abstract setting. Then we instantiate this algorithm with
specific partial orderings and functions to obtain specific constraint
propagation algorithms.
  In particular, using the notions commutativity and semi-commutativity, we
show that the {\tt AC-3}, {\tt PC-2}, {\tt DAC} and {\tt DPC} algorithms for
achieving (directional) arc consistency and (directional) path consistency are
instances of a single generic algorithm. The work reported here extends and
simplifies that of Apt \citeyear{Apt99b}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012011</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012011</id><created>2000-12-16</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Towards a Universal Theory of Artificial Intelligence based on
  Algorithmic Probability and Sequential Decision Theory</title><categories>cs.AI cs.CC cs.IT cs.LG math.IT</categories><comments>8 two-column pages, latex2e, 1 figure, submitted to ijcai</comments><acm-class>I.2; I.2.3; I.2.6; I.2.8; F.1.3; F.2</acm-class><journal-ref>Lecture Notes in Artificial Intelligence (LNAI 2167), Proc. 12th
  Eurpean Conf. on Machine Learning, ECML (2001) 226--238</journal-ref><abstract>  Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown distribution. We unify both theories and give
strong arguments that the resulting universal AIXI model behaves optimal in any
computable environment. The major drawback of the AIXI model is that it is
uncomputable. To overcome this problem, we construct a modified algorithm
AIXI^tl, which is still superior to any other time t and space l bounded agent.
The computation time of AIXI^tl is of the order t x 2^l.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012012</identifier>
 <datestamp>2009-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012012</id><created>2000-12-16</created><authors><author><keyname>Kranzlmueller</keyname><forenames>Dieter</forenames><affiliation>GUP Linz, Joh. Kepler University Linz, Austria</affiliation></author><author><keyname>Schaubschlaeger</keyname><forenames>Christian</forenames><affiliation>GUP Linz, Joh. Kepler University Linz, Austria</affiliation></author><author><keyname>Volkert</keyname><forenames>Jens</forenames><affiliation>GUP Linz, Joh. Kepler University Linz, Austria</affiliation></author></authors><title>A brief overview of the MAD debugging activities</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035 (6
  pages, 2 figures)</comments><acm-class>D.2.5</acm-class><abstract>  Debugging parallel and distributed programs is a difficult activitiy due to
the multiplicity of sequential bugs, the existence of malign effects like race
conditions and deadlocks, and the huge amounts of data that have to be
processed. These problems are addressed by the Monitoring And Debugging
environment MAD, which offers debugging functionality based on a graphical
representation of a program's execution. The target applications of MAD are
parallel programs applying the standard Message-Passing Interface MPI, which is
used extensively in the high-performance computing domain. The highlights of
MAD are interactive inspection mechanisms including visualization of
distributed arrays, the possibility to graphically place breakpoints, a
mechanism for monitor overhead removal, and the evaluation of racing messages
occuring due to nondeterminism in the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012013</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012013</id><created>2000-12-17</created><updated>2014-09-08</updated><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>The Equity Tax and Shelter</title><categories>cs.CE</categories><comments>10 pages. Appendix modified</comments><acm-class>J.4</acm-class><journal-ref>Tax Notes 93(9):1203-1208 (11/26/2001)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taxes have major costs beyond the collected revenue: deadweight from
distorted incentives, compliance and enforcement costs, etc. A simple market
mechanism, the Equity Tax, avoids these problems for the trickiest cases:
corporate, dividend, and capital gains taxes.
  It exploits the ability of the share prices to reflect the expected true
annual return (as perceived by investors, not as defined by law) and works only
for publicly held corporations. Since going or staying public cannot be forced,
and for some constitutional reasons too, the conversion to equity tax must be a
voluntary contract. Repeated reconversions would be costly (all capital gains
are realized) and thus rare. The converts and their shareholders pay no income,
dividend, or capital gain taxes. Instead, they give the IRS, say, 2% of stock
per year to auction promptly. Debts are the lender's assets: its status, not
the debtor's, determines their equity-tax or income-tax treatment.
  The system looks too simple to be right. However, it does have no loopholes
(thus lowering the revenue-neutral tax rate), no compliance costs, requires
little regulation, and leaves all business decisions tax neutral. The total
capital the equity taxed sector absorbs is the only thing the tax could
possibly distort. The rates should match so as to minimize this distortion. The
equity tax enlarges the pre-tax profit since this is what the taxpayers
maximize, not a different after-tax net. The wealth shelter is paid for by
efficiency, not by lost tax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012014</id><created>2000-12-18</created><authors><author><keyname>Szilagyi</keyname><forenames>Gyongyi</forenames></author><author><keyname>Gyimothy</keyname><forenames>Tibor</forenames></author><author><keyname>Maluszynski</keyname><forenames>Jan</forenames></author></authors><title>Slicing of Constraint Logic Programs</title><categories>cs.SE</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  Slicing is a program analysis technique originally developed for imperative
languages. It facilitates understanding of data flow and debugging.
  This paper discusses slicing of Constraint Logic Programs. Constraint Logic
Programming (CLP) is an emerging software technology with a growing number of
applications. Data flow in constraint programs is not explicit, and for this
reason the concepts of slice and the slicing techniques of imperative languages
are not directly applicable.
  This paper formulates declarative notions of slice suitable for CLP. They
provide a basis for defining slicing techniques (both dynamic and static) based
on variable sharing. The techniques are further extended by using groundness
information.
  A prototype dynamic slicer of CLP programs implementing the presented ideas
is briefly described together with the results of some slicing experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012015</id><created>2000-12-20</created><updated>2001-01-18</updated><authors><author><keyname>Deransart</keyname><forenames>Pierre</forenames></author><author><keyname>Smaus</keyname><forenames>Jan-Georg</forenames></author></authors><title>Well-Typed Logic Programs Are not Wrong</title><categories>cs.LO</categories><comments>21 pages, 7 figures</comments><report-no>RR-4082</report-no><acm-class>D.1.6; D.3.3</acm-class><abstract>  We consider prescriptive type systems for logic programs (as in Goedel or
Mercury). In such systems, the typing is static, but it guarantees an
operational property: if a program is &quot;well-typed&quot;, then all derivations
starting in a &quot;well-typed&quot; query are again &quot;well-typed&quot;. This property has been
called subject reduction. We show that this property can also be phrased as a
property of the proof-theoretic semantics of logic programs, thus abstracting
from the usual operational (top-down) semantics. This proof-theoretic view
leads us to questioning a condition which is usually considered necessary for
subject reduction, namely the head condition. It states that the head of each
clause must have a type which is a variant (and not a proper instance) of the
declared type. We provide a more general condition, thus reestablishing a
certain symmetry between heads and body atoms. The condition ensures that in a
derivation, the types of two unified terms are themselves unifiable. We discuss
possible implications of this result. We also discuss the relationship between
the head condition and polymorphic recursion, a concept known in functional
programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012016</id><created>2000-12-20</created><authors><author><keyname>Diaz</keyname><forenames>Javier</forenames></author><author><keyname>Queiruga</keyname><forenames>Claudia</forenames></author><author><keyname>Claudia</keyname><forenames>Villar</forenames></author><author><keyname>Fava</keyname><forenames>Laura</forenames></author></authors><title>A Virtual Java Simulation Lab for Computer Science Students</title><categories>cs.OH</categories><comments>3 pages, WebNet 2000 (World Conference on the WWW and Internet) -
  AACE (Association for the Advancement of Computing Education)</comments><acm-class>H.5.1</acm-class><abstract>  The VJ-Lab is a project oriented to improve the students learning process of
Computer Science degree at the National University of La Plata. The VJ-Lab is a
Web application with Java based simulations. Java can be used to provide
simulation environments with simple pictorial interfaces that can help students
to understand the subject. There are many fields in which it is difficult to
give students a feel for the subject that they are learning. Computer based
simulations offer a fun and effective way to enable students to learn by doing.
Both, practicing skills and applying knowledge are both allowed in simulated
worlds. We will focus on the VJ-Lab project overview, the work in progress and
some Java based simulations running. They imitate the behavior of data network
protocol and data structure algorithms. These applets are produced by the
students of the 'Software Development Laboratory' course.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012017</id><created>2000-12-20</created><authors><author><keyname>Leung</keyname><forenames>Debbie W.</forenames></author></authors><title>Towards Robust Quantum Computation</title><categories>cs.CC quant-ph</categories><comments>243 pages, PhD Dissertation, Stanford University, July 2000</comments><acm-class>F.1.m</acm-class><abstract>  Quantum computation is a subject of much theoretical promise, but has not
been realized in large scale, despite the discovery of fault-tolerant
procedures to overcome decoherence. Part of the reason is that the
theoretically modest requirements still present daunting experimental
challenges. The goal of this Dissertation is to reduce various resources
required for robust quantum computation, focusing on quantum error correcting
codes and solution NMR quantum computation. A variety of techniques have been
developed, including high rate quantum codes for amplitude damping, relaxed
criteria for quantum error correction, systematic construction of
fault-tolerant gates, recipes for quantum process tomography, techniques in
bulk thermal state computation, and efficient decoupling techniques to
implement selective coupled logic gates. A detailed experimental study of a
quantum error correcting code in NMR is also presented. The Dissertation
clarifies and extends results previously reported in quant-ph/9610043,
quant-ph/9704002, quant-ph/9811068, quant-ph/9904100, quant-ph/9906112,
quant-ph/0002039. Additionally, a procedure for quantum process tomography
using maximally entangled states, and a review on NMR quantum computation are
included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012018</id><created>2000-12-20</created><authors><author><keyname>Harland</keyname><forenames>James</forenames></author><author><keyname>Pym</keyname><forenames>David</forenames></author></authors><title>Resource-distribution via Boolean constraints</title><categories>cs.LO</categories><comments>Submission to ACM Transactions on Computational Logic</comments><acm-class>F.4.1</acm-class><abstract>  We consider the problem of searching for proofs in sequential presentations
of logics with multiplicative (or intensional) connectives. Specifically, we
start with the multiplicative fragment of linear logic and extend, on the one
hand, to linear logic with its additives and, on the other, to the additives of
the logic of bunched implications, BI. We give an algebraic method for
calculating the distribution of the side-formulae in multiplicative rules which
allows the occurrence or non-occurrence of a formula on a branch of a proof to
be determined once sufficient information is available. Each formula in the
conclusion of such a rule is assigned a Boolean expression. As a search
proceeds, a set of Boolean constraint equations is generated. We show that a
solution to such a set of equations determines a proof corresponding to the
given search. We explain a range of strategies, from the lazy to the eager, for
solving sets of constraint equations. We indicate how to apply our methods
systematically to large family of relevant systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012019</id><created>2000-12-21</created><authors><author><keyname>Chou</keyname><forenames>Hongsong</forenames></author></authors><title>A Note on Power-Laws of Internet Topology</title><categories>cs.NI</categories><comments>16 pages, 3 figures</comments><acm-class>C.2.1</acm-class><abstract>  The three Power-Laws proposed by Faloutsos et al(1999) are important
discoveries among many recent works on finding hidden rules in the seemingly
chaotic Internet topology. In this note, we want to point out that the first
two laws discovered by Faloutsos et al(1999, hereafter, {\it Faloutsos' Power
Laws}) are in fact equivalent. That is, as long as any one of them is true, the
other can be derived from it, and {\it vice versa}. Although these two laws are
equivalent, they provide different ways to measure the exponents of their
corresponding power law relations. We also show that these two measures will
give equivalent results, but with different error bars. We argue that for nodes
of not very large out-degree($\leq 32$ in our simulation), the first Faloutsos'
Power Law is superior to the second one in giving a better estimate of the
exponent, while for nodes of very large out-degree($&gt; 32$) the power law
relation may not be present, at least for the relation between the frequency of
out-degree and node out-degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012020</id><created>2000-12-22</created><authors><author><keyname>Mendes</keyname><forenames>Daniele Quintella</forenames></author><author><keyname>de Carvalho</keyname><forenames>Luis Alfredo Vidal</forenames></author></authors><title>Creativity and Delusions: A Neurocomputational Approach</title><categories>cs.NE cs.AI</categories><comments>8 pages, 6 figures</comments><acm-class>I.5.1</acm-class><abstract>  Thinking is one of the most interesting mental processes. Its complexity is
sometimes simplified and its different manifestations are classified into
normal and abnormal, like the delusional and disorganized thought or the
creative one. The boundaries between these facets of thinking are fuzzy causing
difficulties in medical, academic, and philosophical discussions. Considering
the dopaminergic signal-to-noise neuronal modulation in the central nervous
system, and the existence of semantic maps in human brain, a self-organizing
neural network model was developed to unify the different thought processes
into a single neurocomputational substrate. Simulations were performed varying
the dopaminergic modulation and observing the different patterns that emerged
at the semantic map. Assuming that the thought process is the total pattern
elicited at the output layer of the neural network, the model shows how the
normal and abnormal thinking are generated and that there are no borders
between their different manifestations. Actually, a continuum of different
qualitative reasoning, ranging from delusion to disorganization of thought, and
passing through the normal and the creative thinking, seems to be more
plausible. The model is far from explaining the complexities of human thinking
but, at least, it seems to be a good metaphorical and unifying view of the many
facets of this phenomenon usually studied in separated settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012021</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012021</id><created>2000-12-22</created><authors><author><keyname>Gunther</keyname><forenames>Neil J.</forenames></author><author><keyname>Beretta</keyname><forenames>Giordano B.</forenames></author></authors><title>A Benchmark for Image Retrieval using Distributed Systems over the
  Internet: BIRDS-I</title><categories>cs.IR cs.MM</categories><comments>24 pages, To appear in the Proc. SPIE Internet Imaging Conference
  2001</comments><report-no>HPL-2000-162</report-no><acm-class>D.2.8;H.2.8;H.3.1;H.3.4;H.3.5</acm-class><doi>10.1117/12.411898</doi><abstract>  The performance of CBIR algorithms is usually measured on an isolated
workstation. In a real-world environment the algorithms would only constitute a
minor component among the many interacting components. The Internet
dramati-cally changes many of the usual assumptions about measuring CBIR
performance. Any CBIR benchmark should be designed from a networked systems
standpoint. These benchmarks typically introduce communication overhead because
the real systems they model are distributed applications. We present our
implementation of a client/server benchmark called BIRDS-I to measure image
retrieval performance over the Internet. It has been designed with the trend
toward the use of small personalized wireless systems in mind. Web-based CBIR
implies the use of heteroge-neous image sets, imposing certain constraints on
how the images are organized and the type of performance metrics applicable.
BIRDS-I only requires controlled human intervention for the compilation of the
image collection and none for the generation of ground truth in the measurement
of retrieval accuracy. Benchmark image collections need to be evolved
incrementally toward the storage of millions of images and that scaleup can
only be achieved through the use of computer-aided compilation. Finally, our
scoring metric introduces a tightly optimized image-ranking window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012022</id><created>2000-12-26</created><authors><author><keyname>Gunther</keyname><forenames>Neil J.</forenames></author></authors><title>Performance and Scalability Models for a Hypergrowth e-Commerce Web Site</title><categories>cs.PF cs.DC cs.SE</categories><comments>15 pages; To appear in the book entitled &quot;Performance Engineering -
  State of the Art and Current Trends,&quot; Lecture Notes in Computer Science,
  Springer-Verlag Heidelberg, 2001</comments><acm-class>C.4;D.2.8;D.4.8;H.3.5</acm-class><abstract>  The performance of successful Web-based e-commerce services has all the
allure of a roller-coaster ride: accelerated fiscal growth combined with the
ever-present danger of running out of server capacity. This chapter presents a
case study based on the author's own capacity planning engagement with one of
the hottest e-commerce Web sites in the world. Several spreadsheet techniques
are presented for forecasting both short-term and long-term trends in the
consumption of server capacity. Two new performance metrics are introduced for
site planning and procurement: the effective demand, and the doubling period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012023</id><created>2000-12-26</created><updated>2003-08-17</updated><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>The Tale of One-way Functions</title><categories>cs.CR cs.CC cs.GL</categories><comments>Small changes. 16 pages in English; 18 pages in Russian (KOI8
  encoding, translated by Alexander Shen)</comments><acm-class>F.1; F.0; G.3</acm-class><journal-ref>Problems of Information Transmission (= Problemy Peredachi
  Informatsii), 39(1):92-103, 2003</journal-ref><abstract>  The existence of one-way functions is arguably the most important problem in
computer theory. The article discusses and refines a number of concepts
relevant to this problem. For instance, it gives the first combinatorial
complete owf, i.e., a function which is one-way if any function is. There are
surprisingly many subtleties in basic definitions. Some of these subtleties are
discussed or hinted at in the literature and some are overlooked. Here, a
unified approach is attempted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0012024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0012024</id><created>2000-12-26</created><updated>2003-12-25</updated><authors><author><keyname>Considine</keyname><forenames>Jeffrey</forenames></author><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author><author><keyname>Metcalf</keyname><forenames>David</forenames></author></authors><title>Byzantine Agreement with Faulty Majority using Bounded Broadcast</title><categories>cs.DC</categories><comments>4 pages; round-up gap between bounds removed</comments><acm-class>F.1.2</acm-class><abstract>  Byzantine Agreement introduced in [Pease, Shostak, Lamport, 80] is a widely
used building block of reliable distributed protocols. It simulates broadcast
despite the presence of faulty parties within the network, traditionally using
only private unicast links. Under such conditions, Byzantine Agreement requires
more than 2/3 of the parties to be compliant. [Fitzi, Maurer, 00], constructed
a Byzantine Agreement protocol for any compliant majority based on an
additional primitive allowing transmission to any two parties simultaneously.
They proposed a problem of generalizing these results to wider channels and
fewer compliant parties. We prove that 2f &lt; kh condition is necessary and
sufficient for implementing broadcast with h compliant and f faulty parties
using k-cast channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101001</id><created>2001-01-03</created><authors><author><keyname>Mor&#xe9;</keyname><forenames>Jorge J.</forenames></author></authors><title>Automatic Differentiation Tools in Optimization Software</title><categories>cs.MS</categories><comments>11 pages</comments><report-no>ANL/MCS-P859-1100</report-no><acm-class>G.1.6</acm-class><abstract>  We discuss the role of automatic differentiation tools in optimization
software. We emphasize issues that are important to large-scale optimization
and that have proved useful in the installation of nonlinear solvers in the
NEOS Server. Our discussion centers on the computation of the gradient and
Hessian matrix for partially separable functions and shows that the gradient
and Hessian matrix can be computed with guaranteed bounds in time and memory
requirements
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101002</id><created>2001-01-03</created><authors><author><keyname>Murray</keyname><forenames>David J.</forenames><affiliation>Lehigh University</affiliation></author><author><keyname>Parson</keyname><forenames>Dale E.</forenames><affiliation>Lucent Technologies</affiliation></author></authors><title>Automated Debugging In Java Using OCL And JDI</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. See cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  Correctness constraints provide a foundation for automated debugging within
object-oriented systems. This paper discusses a new approach to incorporating
correctness constraints into Java development environments. Our approach uses
the Object Constraint Language (&quot;OCL&quot;) as a specification language and the Java
Debug Interface (&quot;JDI&quot;) as a verification API. OCL provides a standard language
for expressing object-oriented constraints that can integrate with Unified
Modeling Language (&quot;UML&quot;) software models. JDI provides a standard Java API
capable of supporting type-safe and side effect free runtime constraint
evaluation. The resulting correctness constraint mechanism: (1) entails no
programming language modifications; (2) requires neither access nor changes to
existing source code; and (3) works with standard off-the-shelf Java virtual
machines (&quot;VMs&quot;). A prototype correctness constraint auditor is presented to
demonstrate the utility of this mechanism for purposes of automated debugging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101003</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101003</id><created>2001-01-04</created><authors><author><keyname>Fontana</keyname><forenames>Federico</forenames></author><author><keyname>Rocchesso</keyname><forenames>Davide</forenames></author></authors><title>Signal-Theoretic Characterization of Waveguide Mesh Geometries for
  Models of Two--Dimensional Wave Propagation in Elastic Media</title><categories>cs.NA cs.SD</categories><comments>9 pages, 6 figures, 1 table, to appear on IEEE Transactions on Speech
  and Audio Processing, vol. 9, no. 2, february 2001</comments><acm-class>G.1.8; H.5.5</acm-class><abstract>  Waveguide Meshes are efficient and versatile models of wave propagation along
a multidimensional ideal medium. The choice of the mesh geometry affects both
the computational cost and the accuracy of simulations. In this paper, we focus
on 2D geometries and use multidimensional sampling theory to compare the
square, triangular, and hexagonal meshes in terms of sampling efficiency and
dispersion error under conditions of critical sampling. The analysis shows that
the triangular geometry exhibits the most desirable tradeoff between accuracy
and computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101004</id><created>2001-01-05</created><authors><author><keyname>Cheung</keyname><forenames>Kevin K. H.</forenames></author><author><keyname>Mosca</keyname><forenames>Michele</forenames></author></authors><title>Decomposing Finite Abelian Groups</title><categories>cs.DS quant-ph</categories><comments>6 pages</comments><acm-class>F.1</acm-class><abstract>  This paper describes a quantum algorithm for efficiently decomposing finite
Abelian groups. Such a decomposition is needed in order to apply the Abelian
hidden subgroup algorithm. Such a decomposition (assuming the Generalized
Riemann Hypothesis) also leads to an efficient algorithm for computing class
numbers (known to be at least as difficult as factoring).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101005</id><created>2001-01-11</created><authors><author><keyname>Smith</keyname><forenames>Raymond</forenames><affiliation>Lucent Technologies</affiliation></author><author><keyname>Korel</keyname><forenames>Bogdan</forenames><affiliation>Illinois Institute of Technology</affiliation></author></authors><title>Slicing Event Traces of Large Software Systems</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  Debugging of large software systems consisting of many processes accessing
shared resources is a very difficult task. Many commercial systems record
essential events during system execution for post-mortem analysis. However, the
event traces of large and long-running systems can be quite voluminous.
Analysis of such event traces to identify sources of incorrect behavior can be
very tedious, error-prone, and inefficient. In this paper, we propose a novel
technique of slicing event traces as a means of reducing the number of events
for analysis. This technique identifies events that may have influenced
observed incorrect system behavior. In order to recognize influencing events
several types of dependencies between events are identified. These dependencies
are determined automatically from an event trace. In order to improve the
precision of slicing we propose to use additional dependencies, referred to as
cause-effect dependencies, which can further reduce the size of sliced event
traces. Our initial experience has shown that this slicing technique can
significantly reduce the size of event traces for analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101006</id><created>2001-01-11</created><updated>2001-02-01</updated><authors><author><keyname>Bern</keyname><forenames>Marshall</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Optimal Moebius Transformations for Information Visualization and
  Meshing</title><categories>cs.CG</categories><comments>16 pages, 7 figures. Revised to include connection to brain
  flat-mapping</comments><acm-class>F.2.2; G.1.6</acm-class><abstract>  We give linear-time quasiconvex programming algorithms for finding a Moebius
transformation of a set of spheres in a unit ball or on the surface of a unit
sphere that maximizes the minimum size of a transformed sphere. We can also use
similar methods to maximize the minimum distance among a set of pairs of input
points. We apply these results to vertex separation and symmetry display in
spherical graph drawing, viewpoint selection in hyperbolic browsing, element
size control in conformal structured mesh generation, and brain flat mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101007</id><created>2001-01-11</created><authors><author><keyname>Auguston</keyname><forenames>Mikhail</forenames></author></authors><title>Assertion checker for the C programming language based on computations
  over event traces</title><categories>cs.SE cs.PL</categories><comments>Proceedings of AADEBUG 2000 Fourth International Workshop on
  Automated Debugging Munich, Germany, 28-30 August 2000</comments><acm-class>D.2.5</acm-class><abstract>  This paper suggests an approach to the development of software testing and
debugging automation tools based on precise program behavior models. The
program behavior model is defined as a set of events (event trace) with two
basic binary relations over events -- precedence and inclusion, and represents
the temporal relationship between actions. A language for the computations over
event traces is developed that provides a basis for assertion checking,
debugging queries, execution profiles, and performance measurements. The
approach is nondestructive, since assertion texts are separated from the target
program source code and can be maintained independently. Assertions can capture
the dynamic properties of a particular target program and can formalize the
general knowledge of typical bugs and debugging strategies. An event grammar
provides a sound basis for assertion language implementation via target program
automatic instrumentation. An implementation architecture and preliminary
experiments with a prototype assertion checker for the C programming language
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101008</identifier>
 <datestamp>2009-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101008</id><created>2001-01-12</created><authors><author><keyname>Zin</keyname><forenames>Abdullah Mohd</forenames><affiliation>Universiti Kebangsaan Malaysia, Bangi, Malaysia</affiliation></author><author><keyname>Aljunid</keyname><forenames>Syed Ahmad</forenames><affiliation>Universiti Kebangsaan Malaysia, Bangi, Malaysia</affiliation><affiliation>Universiti Teknologi MARA, Shah Alam, Malaysia</affiliation></author><author><keyname>Shukur</keyname><forenames>Zarina</forenames><affiliation>Universiti Kebangsaan Malaysia, Bangi, Malaysia</affiliation></author><author><keyname>Nordin</keyname><forenames>Mohd Jan</forenames><affiliation>Universiti Kebangsaan Malaysia, Bangi, Malaysia</affiliation></author></authors><title>A Knowledge-based Automated Debugger in Learning System</title><categories>cs.SE cs.PL</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5</acm-class><abstract>  Currently, programming instructors continually face the problem of helping to
debug students' programs. Although there currently exist a number of debuggers
and debugging tools in various platforms, most of these projects or products
are crafted through the needs of software maintenance, and not through the
perspective of teaching of programming. Moreover, most debuggers are too
general, meant for experts as well as not user-friendly. We propose a new
knowledge-based automated debugger to be used as a user-friendly tool by the
students to self-debug their own programs. Stereotyped code (cliche) and bugs
cliche will be stored as library of plans in the knowledge-base. Recognition of
correct code or bugs is based on pattern matching and constraint satisfaction.
Given a syntax error-free program and its specification, this debugger called
Adil (Automated Debugger in Learning system) will be able locate, pinpoint and
explain logical errors of programs. If there are no errors, it will be able to
explain the meaning of the program. Adil is based on the design of the
Conceiver, an automated program understanding system developed at Universiti
Kebangsaan Malaysia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101009</id><created>2001-01-12</created><authors><author><keyname>Navarro</keyname><forenames>Angel Herrranz-Nieva Juan Jose Moreno</forenames></author></authors><title>Generation of and Debugging with Logical Pre and Postconditions</title><categories>cs.PL cs.SE</categories><comments>In M. Ducasse (ed), proceedings of the Fourth International Workshop
  on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs.SE/0010035</comments><acm-class>D.2.5, F.3.1</acm-class><abstract>  This paper shows the debugging facilities provided by the SLAM system. The
SLAM system includes i) a specification language that integrates algebraic
specifications and model-based specifications using the object oriented model.
Class operations are defined by using rules each of them with logical pre and
postconditions but with a functional flavour. ii) A development environment
that, among other features, is able to generate readable code in a high level
object oriented language. iii) The generated code includes (part of) the pre
and postconditions as assertions, that can be automatically checked in the
debug mode execution of programs. We focus on this last aspect.
  The SLAM language is expressive enough to describe many useful properties and
these properties are translated into a Prolog program that is linked (via an
adequate interface) with the user program. The debugging execution of the
program interacts with the Prolog engine which is responsible for checking
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101010</id><created>2001-01-13</created><updated>2001-01-18</updated><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lam</keyname><forenames>Tak-Wah</forenames></author><author><keyname>Sung</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Ting</keyname><forenames>Hing-Fung</forenames></author></authors><title>An Even Faster and More Unifying Algorithm for Comparing Trees via
  Unbalanced Bipartite Matchings</title><categories>cs.CV cs.DS</categories><comments>To appear in Journal of Algorithms</comments><acm-class>F.2.2; I.5; J.3</acm-class><abstract>  A widely used method for determining the similarity of two labeled trees is
to compute a maximum agreement subtree of the two trees. Previous work on this
similarity measure is only concerned with the comparison of labeled trees of
two special kinds, namely, uniformly labeled trees (i.e., trees with all their
nodes labeled by the same symbol) and evolutionary trees (i.e., leaf-labeled
trees with distinct symbols for distinct leaves). This paper presents an
algorithm for comparing trees that are labeled in an arbitrary manner. In
addition to this generality, this algorithm is faster than the previous
algorithms.
  Another contribution of this paper is on maximum weight bipartite matchings.
We show how to speed up the best known matching algorithms when the input
graphs are node-unbalanced or weight-unbalanced. Based on these enhancements,
we obtain an efficient algorithm for a new matching problem called the
hierarchical bipartite matching problem, which is at the core of our maximum
agreement subtree algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101011</id><created>2001-01-14</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Multiple-Size Divide-and-Conquer Recurrences</title><categories>cs.GL cs.DS</categories><acm-class>F.2</acm-class><journal-ref>SIGACT News, 28(2):67--69, June 1997</journal-ref><abstract>  This short note reports a master theorem on tight asymptotic solutions to
divide-and-conquer recurrences with more than one recursive term: for example,
T(n) = 1/4 T(n/16) + 1/3 T(3n/5) + 4 T(n/100) + 10 T(n/300) + n^2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101012</id><created>2001-01-16</created><authors><author><keyname>Hildreth</keyname><forenames>Paul</forenames></author><author><keyname>Kimble</keyname><forenames>Chris</forenames></author><author><keyname>Wright</keyname><forenames>Peter</forenames></author></authors><title>Communities of Practice in the Distributed International Environment</title><categories>cs.HC cs.IR</categories><comments>Available from
  http://www-users.cs.york.ac.uk/~kimble/research/publics.html</comments><acm-class>H.5.3</acm-class><journal-ref>Journal of Knowledge Management, 4(1), March 2000, pp 27 - 37</journal-ref><abstract>  Modern commercial organisations are facing pressures which have caused them
to lose personnel. When they lose people, they also lose their knowledge.
Organisations also have to cope with the internationalisation of business
forcing collaboration and knowledge sharing across time and distance. Knowledge
Management (KM) claims to tackle these issues. This paper looks at an area
where KM does not offer sufficient support, that is, the sharing of knowledge
that is not easy to articulate.
  The focus in this paper is on Communities of Practice in commercial
organisations. We do this by exploring knowledge sharing in Lave and Wenger's
[1] theory of Communities of Practice and investigating how Communities of
Practice may translate to a distributed international environment. The paper
reports on two case studies that explore the functioning of Communities of
Practice across international boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101013</id><created>2001-01-16</created><authors><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Francois</forenames></author></authors><title>A Classification of Symbolic Transition Systems</title><categories>cs.LO</categories><comments>21 pages</comments><report-no>UCB-CSD99/1086</report-no><acm-class>F.3.1</acm-class><abstract>  We define five increasingly comprehensive classes of infinite-state systems,
called STS1--5, whose state spaces have finitary structure. For four of these
classes, we provide examples from hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101014</id><created>2001-01-17</created><authors><author><keyname>Lonc</keyname><forenames>Zbigniew</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>On the problem of computing the well-founded semantics</title><categories>cs.LO cs.AI cs.DS</categories><comments>19 pages, 4 figures, accepted for publication Theory and Practice of
  Logic Programming</comments><acm-class>I.2.3; F.2.2</acm-class><journal-ref>Theory and Practice of Logic Programming, 1(5), 591-609, 2001</journal-ref><abstract>  The well-founded semantics is one of the most widely studied and used
semantics of logic programs with negation. In the case of finite propositional
programs, it can be computed in polynomial time, more specifically, in
O(|At(P)|size(P)) steps, where size(P) denotes the total number of occurrences
of atoms in a logic program P. This bound is achieved by an algorithm
introduced by Van Gelder and known as the alternating-fixpoint algorithm.
Improving on the alternating-fixpoint algorithm turned out to be difficult. In
this paper we study extensions and modifications of the alternating-fixpoint
approach. We then restrict our attention to the class of programs whose rules
have no more than one positive occurrence of an atom in their bodies. For
programs in that class we propose a new implementation of the
alternating-fixpoint method in which false atoms are computed in a top-down
fashion. We show that our algorithm is faster than other known algorithms and
that for a wide class of programs it is linear and so, asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101015</id><created>2001-01-17</created><authors><author><keyname>Aspnes</keyname><forenames>James</forenames></author><author><keyname>Hartling</keyname><forenames>Julia</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Kim</keyname><forenames>Junhyong</forenames></author><author><keyname>Shah</keyname><forenames>Gauri</forenames></author></authors><title>Combinatorial Toolbox for Protein Sequence Design and Landscape Analysis
  in the Grand Canonical Model</title><categories>cs.CE cs.CC q-bio.BM</categories><acm-class>F.2; J.3</acm-class><abstract>  In modern biology, one of the most important research problems is to
understand how protein sequences fold into their native 3D structures. To
investigate this problem at a high level, one wishes to analyze the protein
landscapes, i.e., the structures of the space of all protein sequences and
their native 3D structures. Perhaps the most basic computational problem at
this level is to take a target 3D structure as input and design a fittest
protein sequence with respect to one or more fitness functions of the target 3D
structure. We develop a toolbox of combinatorial techniques for protein
landscape analysis in the Grand Canonical model of Sun, Brem, Chan, and Dill.
The toolbox is based on linear programming, network flow, and a linear-size
representation of all minimum cuts of a network. It not only substantially
expands the network flow technique for protein sequence design in Kleinberg's
seminal work but also is applicable to a considerably broader collection of
computational problems than those considered by Kleinberg. We have used this
toolbox to obtain a number of efficient algorithms and hardness results. We
have further used the algorithms to analyze 3D structures drawn from the
Protein Data Bank and have discovered some novel relationships between such
native 3D structures and the Grand Canonical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101016</id><created>2001-01-17</created><authors><author><keyname>Chen</keyname><forenames>Ting</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Tepel</keyname><forenames>Matthew</forenames></author><author><keyname>Rush</keyname><forenames>John</forenames></author><author><keyname>Church</keyname><forenames>George M.</forenames></author></authors><title>A Dynamic Programming Approach to De Novo Peptide Sequencing via Tandem
  Mass Spectrometry</title><categories>cs.CE cs.DS</categories><comments>A preliminary version appeared in Proceedings of the 11th Annual
  ACM-SIAM Symposium on Discrete Algorithms, pages 389--398, 2000</comments><acm-class>F.2; J.3</acm-class><abstract>  The tandem mass spectrometry fragments a large number of molecules of the
same peptide sequence into charged prefix and suffix subsequences, and then
measures mass/charge ratios of these ions. The de novo peptide sequencing
problem is to reconstruct the peptide sequence from a given tandem mass
spectral data of k ions. By implicitly transforming the spectral data into an
NC-spectrum graph G=(V,E) where |V|=2k+2, we can solve this problem in
O(|V|+|E|) time and O(|V|) space using dynamic programming. Our approach can be
further used to discover a modified amino acid in O(|V||E|) time and to analyze
data with other types of noise in O(|V||E|) time. Our algorithms have been
implemented and tested on actual experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101017</id><created>2001-01-19</created><authors><author><keyname>Ultes-Nitsche</keyname><forenames>Ulrich</forenames></author><author><keyname>Wolper</keyname><forenames>Pierre</forenames></author></authors><title>Checking Properties within Fairness and Behavior Abstractions</title><categories>cs.LO</categories><comments>22 pages</comments><acm-class>D.2.4; F.3.1</acm-class><abstract>  This paper is motivated by the fact that verifying liveness properties under
a fairness condition is often problematic, especially when abstraction is used.
It shows that using a more abstract notion than truth under fairness,
specifically the concept of a property being satisfied within fairness can lead
to interesting possibilities. Technically, it is first established that
deciding satisfaction within fairness is a PSPACE-complete problem and it is
shown that properties satisfied within fairness can always be satisfied by some
fair implementation. Thereafter, the interaction between behavior abstraction
and satisfaction within fairness is studied and it is proved that satisfaction
of properties within fairness can be verified on behavior abstractions, if the
abstraction homomorphism is weakly continuation-closed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101018</id><created>2001-01-19</created><authors><author><keyname>Benson</keyname><forenames>Steven J.</forenames></author><author><keyname>McInnes</keyname><forenames>Lois Curfman</forenames></author><author><keyname>Mor&#xe9;</keyname><forenames>Jorge J.</forenames></author></authors><title>GPCG: A Case Study in the Performance and Scalability of Optimization
  Algorithms</title><categories>cs.MS</categories><comments>title + 16 pages</comments><report-no>ANL/MCS-P768-0799</report-no><acm-class>G.1.6</acm-class><abstract>  GPCG is an algorithm within the Toolkit for Advanced Optimization (TAO) for
solving bound constrained, convex quadratic problems. Originally developed by
More' and Toraldo, this algorithm was designed for large-scale problems but had
been implemented only for a single processor. The TAO implementation is
available for a wide range of high-performance architecture, and has been
tested on up to 64 processors to solve problems with over 2.5 million
variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101019</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101019</id><created>2001-01-21</created><updated>2001-09-19</updated><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>General Loss Bounds for Universal Sequence Prediction</title><categories>cs.AI cs.LG math.ST stat.TH</categories><comments>8 two-column pages, LaTeX2e</comments><report-no>IDSIA-03-01</report-no><acm-class>I.2; I.2.6; I.2.8; F.1.3</acm-class><journal-ref>Proc. 18th Int. Conf. on Machine Learning ICML (2001) 210-217</journal-ref><abstract>  The Bayesian framework is ideally suited for induction problems. The
probability of observing $x_t$ at time $t$, given past observations
$x_1...x_{t-1}$ can be computed with Bayes' rule if the true distribution $\mu$
of the sequences $x_1x_2x_3...$ is known. The problem, however, is that in many
cases one does not even have a reasonable estimate of the true distribution. In
order to overcome this problem a universal distribution $\xi$ is defined as a
weighted sum of distributions $\mu_i\inM$, where $M$ is any countable set of
distributions including $\mu$. This is a generalization of Solomonoff
induction, in which $M$ is the set of all enumerable semi-measures. Systems
which predict $y_t$, given $x_1...x_{t-1}$ and which receive loss $l_{x_t y_t}$
if $x_t$ is the true next symbol of the sequence are considered. It is proven
that using the universal $\xi$ as a prior is nearly as good as using the
unknown true distribution $\mu$. Furthermore, games of chance, defined as a
sequence of bets, observations, and rewards are studied. The time needed to
reach the winning zone is bounded in terms of the relative entropy of $\mu$ and
$\xi$. Extensions to arbitrary alphabets, partial and delayed prediction, and
more active systems are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101020</id><created>2001-01-22</created><updated>2001-06-22</updated><authors><author><keyname>Mueller-Quade</keyname><forenames>J.</forenames><affiliation>Universitaet Karlsruhe</affiliation></author><author><keyname>Imai</keyname><forenames>H.</forenames><affiliation>U. of Tokyo</affiliation></author></authors><title>More Robust Multiparty Protocols with Oblivious Transfer</title><categories>cs.CR</categories><comments>13 pages, major revision</comments><acm-class>D.4.6</acm-class><abstract>  With oblivious transfer multiparty protocols become possible even in the
presence of a faulty majority. But all known protocols can be aborted by just
one disruptor.
  This paper presents more robust solutions for multiparty protocols with
oblivious transfer. This additional robustness against disruptors weakens the
security of the protocol and the guarantee that the result is correct. We can
observe a trade off between robustness against disruption and security and
correctness.
  We give an application to quantum multiparty protocols. These allow the
implementation of oblivious transfer and the protocols of this paper relative
to temporary assumptions, i.e., the security increases after the termination of
the protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101021</id><created>2001-01-22</created><authors><author><keyname>He</keyname><forenames>Xin</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>A Fast General Methodology for Information-Theoretically Optimal
  Encodings of Graphs</title><categories>cs.DS cs.GR</categories><acm-class>E.4; F.2.2</acm-class><journal-ref>SIAM Journal on Computing, 30(3):838--846, 2000</journal-ref><abstract>  We propose a fast methodology for encoding graphs with
information-theoretically minimum numbers of bits. Specifically, a graph with
property pi is called a pi-graph. If pi satisfies certain properties, then an
n-node m-edge pi-graph G can be encoded by a binary string X such that (1) G
and X can be obtained from each other in O(n log n) time, and (2) X has at most
beta(n)+o(beta(n)) bits for any continuous super-additive function beta(n) so
that there are at most 2^{beta(n)+o(beta(n))} distinct n-node pi-graphs. The
methodology is applicable to general classes of graphs; this paper focuses on
planar graphs. Examples of such pi include all conjunctions over the following
groups of properties: (1) G is a planar graph or a plane graph; (2) G is
directed or undirected; (3) G is triangulated, triconnected, biconnected,
merely connected, or not required to be connected; (4) the nodes of G are
labeled with labels from {1, ..., ell_1} for ell_1 &lt;= n; (5) the edges of G are
labeled with labels from {1, ..., ell_2} for ell_2 &lt;= m; and (6) each node
(respectively, edge) of G has at most ell_3 = O(1) self-loops (respectively,
ell_4 = O(1) multiple edges). Moreover, ell_3 and ell_4 are not required to be
O(1) for the cases of pi being a plane triangulation. These examples are novel
applications of small cycle separators of planar graphs and are the only
nontrivial classes of graphs, other than rooted trees, with known
polynomial-time information-theoretically optimal coding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101022</id><created>2001-01-23</created><authors><author><keyname>Bossi</keyname><forenames>Annalisa</forenames></author><author><keyname>Etalle</keyname><forenames>Sandro</forenames></author><author><keyname>Rossi</keyname><forenames>Sabina</forenames></author><author><keyname>Smaus</keyname><forenames>Jan-Georg</forenames></author></authors><title>Semantics and Termination of Simply-Moded Logic Programs with Dynamic
  Scheduling</title><categories>cs.LO cs.PL</categories><comments>25 pages, long version of paper with same title at ESOP 2001</comments><acm-class>D.1.3; D.1.6; F.3.2</acm-class><abstract>  In logic programming, dynamic scheduling refers to a situation where the
selection of the atom in each resolution (computation) step is determined at
runtime, as opposed to a fixed selection rule such as the left-to-right one of
Prolog. This has applications e.g. in parallel programming. A mechanism to
control dynamic scheduling is provided in existing languages in the form of
delay declarations.
  Input-consuming derivations were introduced to describe dynamic scheduling
while abstracting from the technical details. In this paper, we first formalise
the relationship between delay declarations and input-consuming derivations,
showing in many cases a one-to-one correspondence. Then, we define a
model-theoretic semantics for input-consuming derivations of simply-moded
programs. Finally, for this class of programs, we provide a necessary and
sufficient criterion for termination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101023</id><created>2001-01-23</created><authors><author><keyname>Bossi</keyname><forenames>Annalisa</forenames></author><author><keyname>Etalle</keyname><forenames>Sandro</forenames></author><author><keyname>Rossi</keyname><forenames>Sabina</forenames></author></authors><title>Properties of Input-Consuming Derivations</title><categories>cs.PL cs.LO</categories><comments>33 pages</comments><acm-class>D.1.6;D.3.1;F.3.2</acm-class><abstract>  We study the properties of input-consuming derivations of moded logic
programs. Input-consuming derivations can be used to model the behavior of
logic programs using dynamic scheduling and employing constructs such as delay
declarations.
  We consider the class of nicely-moded programs and queries. We show that for
these programs a weak version of the well-known switching lemma holds also for
input-consuming derivations. Furthermore, we show that, under suitable
conditions, there exists an algebraic characterization of termination of
input-consuming derivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101024</id><created>2001-01-23</created><updated>2001-01-24</updated><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Tate</keyname><forenames>Stephen R.</forenames></author></authors><title>On-Line Difference Maximization</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2; G.1.6; G.2.1; G.2.3; G.3</acm-class><journal-ref>SIAM Journal on Discrete Mathematics, 12(1):78-90, 1999</journal-ref><abstract>  In this paper we examine problems motivated by on-line financial problems and
stochastic games. In particular, we consider a sequence of entirely arbitrary
distinct values arriving in random order, and must devise strategies for
selecting low values followed by high values in such a way as to maximize the
expected gain in rank from low values to high values.
  First, we consider a scenario in which only one low value and one high value
may be selected. We give an optimal on-line algorithm for this scenario, and
analyze it to show that, surprisingly, the expected gain is n-O(1), and so
differs from the best possible off-line gain by only a constant additive term
(which is, in fact, fairly small -- at most 15).
  In a second scenario, we allow multiple nonoverlapping low/high selections,
where the total gain for our algorithm is the sum of the individual pair gains.
We also give an optimal on-line algorithm for this problem, where the expected
gain is n^2/8-\Theta(n\log n). An analysis shows that the optimal expected
off-line gain is n^2/6+\Theta(1), so the performance of our on-line algorithm
is within a factor of 3/4 of the best off-line strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101025</id><created>2001-01-23</created><authors><author><keyname>Zaffanella</keyname><forenames>Enea</forenames></author><author><keyname>Hill</keyname><forenames>Patricia M.</forenames></author><author><keyname>Bagnara</keyname><forenames>Roberto</forenames></author></authors><title>Decomposing Non-Redundant Sharing by Complementation</title><categories>cs.PL</categories><comments>To appear on Theory and Practice of Logic Programming. 30 pages, 4
  figures</comments><acm-class>F.3.2</acm-class><abstract>  Complementation, the inverse of the reduced product operation, is a technique
for systematically finding minimal decompositions of abstract domains. File'
and Ranzato advanced the state of the art by introducing a simple method for
computing a complement. As an application, they considered the extraction by
complementation of the pair-sharing domain PS from the Jacobs and Langen's
set-sharing domain SH. However, since the result of this operation was still
SH, they concluded that PS was too abstract for this. Here, we show that the
source of this result lies not with PS but with SH and, more precisely, with
the redundant information contained in SH with respect to ground-dependencies
and pair-sharing. In fact, a proper decomposition is obtained if the
non-redundant version of SH, PSD, is substituted for SH. To establish the
results for PSD, we define a general schema for subdomains of SH that includes
PSD and Def as special cases. This sheds new light on the structure of PSD and
exposes a natural though unexpected connection between Def and PSD. Moreover,
we substantiate the claim that complementation alone is not sufficient to
obtain truly minimal decompositions of domains. The right solution to this
problem is to first remove redundancies by computing the quotient of the domain
with respect to the observable behavior, and only then decompose it by
complementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101026</id><created>2001-01-24</created><authors><author><keyname>Gacs</keyname><forenames>Peter</forenames></author></authors><title>Deterministic computations whose history is independent of the order of
  asynchronous updating</title><categories>cs.DC cs.CC</categories><acm-class>F.1.2</acm-class><abstract>  Consider a network of processors (sites) in which each site x has a finite
set N(x) of neighbors. There is a transition function f that for each site x
computes the next state \xi(x) from the states in N(x). But these transitions
(updates) are applied in arbitrary order, one or many at a time. If the state
of site x at time t is \eta(x,t) then let us define the sequence \zeta(x,0),
\zeta(x,1), ... by taking the sequence \eta(x,0), \eta(x,1), ..., and deleting
repetitions. The function f is said to have invariant histories if the sequence
\zeta(x,i), (while it lasts, in case it is finite) depends only on the initial
configuration, not on the order of updates.
  This paper shows that though the invariant history property is typically
undecidable, there is a useful simple sufficient condition, called
commutativity: For any configuration, for any pair x,y of neighbors, if the
updating would change both \xi(x) and \xi(y) then the result of updating first
x and then y is the same as the result of doing this in the reverse order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101027</id><created>2001-01-25</created><authors><author><keyname>Warner</keyname><forenames>Simeon</forenames><affiliation>LANL</affiliation></author></authors><title>Open Archives Initiative protocol development and implementation at
  arXiv</title><categories>cs.DL</categories><comments>15 pages. Expanded version of talk presented at Open Archives
  Initiative Open Meeting in Washington, DC, USA on 23 January 2001</comments><acm-class>H.3.7</acm-class><abstract>  I outline the involvement of the Los Alamos e-print archive (arXiv) within
the Open Archives Initiative (OAI) and describe the implementation of the data
provider side of the OAI protocol v1.0. I highlight the ways in which we map
the existing structure of arXiv onto elements of the protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101028</id><created>2001-01-25</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Ma</keyname><forenames>Yuan</forenames></author><author><keyname>Sipser</keyname><forenames>Michael</forenames></author><author><keyname>Yin</keyname><forenames>Yiqun</forenames></author></authors><title>Optimal Constructions of Hybrid Algorithms</title><categories>cs.DM cs.DS</categories><acm-class>F.2.2</acm-class><journal-ref>Journal of Algorithms, 29:142--164, 1998</journal-ref><abstract>  We study on-line strategies for solving problems with hybrid algorithms.
There is a problem Q and w basic algorithms for solving Q. For some lambda &lt;=
w, we have a computer with lambda disjoint memory areas, each of which can be
used to run a basic algorithm and store its intermediate results. In the worst
case, only one basic algorithm can solve Q in finite time, and all the other
basic algorithms run forever without solving Q. To solve Q with a hybrid
algorithm constructed from the basic algorithms, we run a basic algorithm for
some time, then switch to another, and continue this process until Q is solved.
The goal is to solve Q in the least amount of time. Using competitive ratios to
measure the efficiency of a hybrid algorithm, we construct an optimal
deterministic hybrid algorithm and an efficient randomized hybrid algorithm.
This resolves an open question on searching with multiple robots posed by
Baeza-Yates, Culberson and Rawlins. We also prove that our randomized algorithm
is optimal for lambda = 1, settling a conjecture of Kao, Reif and Tate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101029</id><created>2001-01-26</created><authors><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author><author><keyname>Hurst</keyname><forenames>Amy</forenames></author><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author></authors><title>Tap Tips: Lightweight Discovery of Touchscreen Targets</title><categories>cs.HC</categories><acm-class>H.5.2; H.5.4; I.3.6</acm-class><journal-ref>Extended Abstracts, ACM SIGCHI Conf. on Human Factors in Computing
  Systems, Seattle, WA, March 2001, 237-238. ACM Press.</journal-ref><doi>10.1145/634067.634208</doi><abstract>  We describe tap tips, a technique for providing touch-screen target location
hints. Tap tips are lightweight in that they are non-modal, appear only when
needed, require a minimal number of user gestures, and do not add to the
standard touchscreen gesture vocabulary. We discuss our implementation of tap
tips in an electronic guidebook system and some usability test results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101030</id><created>2001-01-26</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Tree Contractions and Evolutionary Trees</title><categories>cs.CE cs.DS</categories><acm-class>F.2.2; J.3</acm-class><journal-ref>SIAM Journal on Computing, 27(6):1592--1616, December 1998</journal-ref><abstract>  An evolutionary tree is a rooted tree where each internal vertex has at least
two children and where the leaves are labeled with distinct symbols
representing species. Evolutionary trees are useful for modeling the
evolutionary history of species. An agreement subtree of two evolutionary trees
is an evolutionary tree which is also a topological subtree of the two given
trees. We give an algorithm to determine the largest possible number of leaves
in any agreement subtree of two trees T_1 and T_2 with n leaves each. If the
maximum degree d of these trees is bounded by a constant, the time complexity
is O(n log^2(n)) and is within a log(n) factor of optimal. For general d, this
algorithm runs in O(n d^2 log(d) log^2(n)) time or alternatively in O(n d
sqrt(d) log^3(n)) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101031</id><created>2001-01-26</created><updated>2001-01-26</updated><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lam</keyname><forenames>Tak-Wah</forenames></author><author><keyname>Sung</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Ting</keyname><forenames>Hing-Fung</forenames></author></authors><title>Cavity Matchings, Label Compressions, and Unrooted Evolutionary Trees</title><categories>cs.CE cs.DS</categories><acm-class>F.2.2; J.3</acm-class><journal-ref>SIAM Journal on Computing, 30(2):602--624, 2000</journal-ref><abstract>  We present an algorithm for computing a maximum agreement subtree of two
unrooted evolutionary trees. It takes O(n^{1.5} log n) time for trees with
unbounded degrees, matching the best known time complexity for the rooted case.
Our algorithm allows the input trees to be mixed trees, i.e., trees that may
contain directed and undirected edges at the same time. Our algorithm adopts a
recursive strategy exploiting a technique called label compression. The
backbone of this technique is an algorithm that computes the maximum weight
matchings over many subgraphs of a bipartite graph as fast as it takes to
compute a single matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101032</id><created>2001-01-26</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Total Protection of Analytic Invariant Information in Cross Tabulated
  Tables</title><categories>cs.CR cs.DM cs.DS</categories><acm-class>F.2.2; H.2.8; H.2.0</acm-class><journal-ref>SIAM Journal on Computing, 26(1):231--242, February 1997</journal-ref><abstract>  To protect sensitive information in a cross tabulated table, it is a common
practice to suppress some of the cells in the table. An analytic invariant is a
power series in terms of the suppressed cells that has a unique feasible value
and a convergence radius equal to +\infty. Intuitively, the information
contained in an invariant is not protected even though the values of the
suppressed cells are not disclosed. This paper gives an optimal linear-time
algorithm for testing whether there exist nontrivial analytic invariants in
terms of the suppressed cells in a given set of suppressed cells. This paper
also presents NP-completeness results and an almost linear-time algorithm for
the problem of suppressing the minimum number of cells in addition to the
sensitive ones so that the resulting table does not leak analytic invariant
information about a given set of suppressed cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101033</id><created>2001-01-26</created><authors><author><keyname>He</keyname><forenames>Xin</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>Linear-Time Succinct Encodings of Planar Graphs via Canonical Orderings</title><categories>cs.DS cs.GR</categories><acm-class>E.4; F.2.2</acm-class><journal-ref>SIAM Journal on Discrete Mathematics, 12(3):317--325, 1999</journal-ref><abstract>  Let G be an embedded planar undirected graph that has n vertices, m edges,
and f faces but has no self-loop or multiple edge. If G is triangulated, we can
encode it using {4/3}m-1 bits, improving on the best previous bound of about
1.53m bits. In case exponential time is acceptable, roughly 1.08m bits have
been known to suffice. If G is triconnected, we use at most
(2.5+2\log{3})\min\{n,f\}-7 bits, which is at most 2.835m bits and smaller than
the best previous bound of 3m bits. Both of our schemes take O(n) time for
encoding and decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101034</id><created>2001-01-26</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Data Security Equals Graph Connectivity</title><categories>cs.CR cs.DB cs.DS</categories><acm-class>F.2.2; H.2.0; H.2.8</acm-class><journal-ref>SIAM Journal on Discrete Mathematics, 9:87--100, 1996</journal-ref><abstract>  To protect sensitive information in a cross tabulated table, it is a common
practice to suppress some of the cells in the table. This paper investigates
four levels of data security of a two-dimensional table concerning the
effectiveness of this practice. These four levels of data security protect the
information contained in, respectively, individual cells, individual rows and
columns, several rows or columns as a whole, and a table as a whole. The paper
presents efficient algorithms and NP-completeness results for testing and
achieving these four levels of data security. All these complexity results are
obtained by means of fundamental equivalences between the four levels of data
security of a table and four types of connectivity of a graph constructed from
that table.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101035</id><created>2001-01-28</created><updated>2001-02-01</updated><authors><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author><author><keyname>Hurst</keyname><forenames>Amy</forenames></author><author><keyname>Szymanski</keyname><forenames>Margaret H.</forenames></author></authors><title>The Guidebook, the Friend, and the Room: Visitor Experience in a
  Historic House</title><categories>cs.HC</categories><acm-class>H.5.1; H.5.2; J.5</acm-class><journal-ref>Extended Abstracts, ACM SIGCHI Conf. on Human Factors in Computing
  Systems, Seattle, WA, March 2001, 273-274. ACM Press.</journal-ref><doi>10.1145/634067.634229</doi><abstract>  In this paper, we describe an electronic guidebook prototype and report on a
study of its use in a historic house. Supported by mechanisms in the guidebook,
visitors constructed experiences that had a high degree of interaction with
three entities: the guidebook, their companions, and the house and its
contents. For example, we found that most visitors played audio descriptions
played through speakers (rather than using headphones or reading textual
descriptions) to facilitate communication with their companions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0101036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0101036</id><created>2001-01-29</created><authors><author><keyname>Chater</keyname><forenames>Nick</forenames><affiliation>Univ. Warwick</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and Univ. Amsterdam</affiliation></author></authors><title>The Generalized Universal Law of Generalization</title><categories>cs.CV cs.AI math.PR physics.soc-ph</categories><comments>17 pages LaTeX, Submitted</comments><acm-class>J.4</acm-class><abstract>  It has been argued by Shepard that there is a robust psychological law that
relates the distance between a pair of items in psychological space and the
probability that they will be confused with each other. Specifically, the
probability of confusion is a negative exponential function of the distance
between the pair of items. In experimental contexts, distance is typically
defined in terms of a multidimensional Euclidean space-but this assumption
seems unlikely to hold for complex stimuli. We show that, nonetheless, the
Universal Law of Generalization can be derived in the more complex setting of
arbitrary stimuli, using a much more universal measure of distance. This
universal distance is defined as the length of the shortest program that
transforms the representations of the two items of interest into one another:
the algorithmic information distance. It is universal in the sense that it
minorizes every computable distance: it is the smallest computable distance. We
show that the universal law of generalization holds with probability going to
one-provided the confusion probabilities are computable. We also give a
mathematically more appealing form
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102001</id><created>2001-02-01</created><updated>2004-03-05</updated><authors><author><keyname>Dolan</keyname><forenames>Elizabeth D.</forenames></author><author><keyname>Mor&#xe9;</keyname><forenames>Jorge J.</forenames></author></authors><title>Benchmarking Optimization Software with Performance Profiles</title><categories>cs.MS</categories><comments>13 pages plus title and toc pages</comments><report-no>ANL/MCS-P861-1200</report-no><acm-class>G.4; G.1.6</acm-class><journal-ref>Math. Program., Ser. A 91: 201-213 (2002)</journal-ref><abstract>  We propose performance profiles-distribution functions for a performance
metric-as a tool for benchmarking and comparing optimization software. We show
that performance profiles combine the best features of other tools for
performance evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102002</id><created>2001-02-01</created><authors><author><keyname>Pierre</keyname><forenames>John M.</forenames></author></authors><title>On the Automated Classification of Web Sites</title><categories>cs.IR</categories><comments>12 pages, etendu.sty</comments><acm-class>H.3.3; I.5.2; H.5.4</acm-class><abstract>  In this paper we discuss several issues related to automated text
classification of web sites. We analyze the nature of web content and metadata
in relation to requirements for text features. We find that HTML metatags are a
good source of text features, but are not in wide use despite their role in
search engine rankings. We present an approach for targeted spidering including
metadata extraction and opportunistic crawling of specific semantic hyperlinks.
We describe a system for automatically classifying web sites into industry
categories and present performance results based on different combinations of
text features and training data. This system can serve as the basis for a
generalized framework for automated metadata creation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102003</id><created>2001-02-02</created><authors><author><keyname>Akcoglu</keyname><forenames>Karhan</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Raghavan</keyname><forenames>Shuba</forenames></author></authors><title>Fast Pricing of European Asian Options with Provable Accuracy:
  Single-stock and Basket Options</title><categories>cs.CE</categories><comments>22 pages</comments><acm-class>G.2.2; G.2.3; G.3</acm-class><abstract>  This paper develops three polynomial-time pricing techniques for European
Asian options with provably small errors, where the stock prices follow
binomial trees or trees of higher-degree. The first technique is the first
known Monte Carlo algorithm with analytical error bounds suitable for pricing
single-stock options with meaningful confidence and speed. The second technique
is a general recursive bucketing-based scheme that can use the
Aingworth-Motwani-Oldham aggregation algorithm, Monte-Carlo simulation and
possibly others as the base-case subroutine. This scheme enables robust
trade-offs between accuracy and time over subtrees of different sizes. For
long-term options or high frequency price averaging, it can price single-stock
options with smaller errors in less time than the base-case algorithms
themselves. The third technique combines Fast Fourier Transform with
bucketing-based schemes for pricing basket options. This technique takes
polynomial time in the number of days and the number of stocks, and does not
add any errors to those already incurred in the companion bucketing scheme.
This technique assumes that the price of each underlying stock moves
independently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102004</id><created>2001-02-06</created><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Computational Geometry Column 41</title><categories>cs.CG cs.DM</categories><comments>To appear in SIGACT News and in Internat. J. Comput. Geom. Appl</comments><acm-class>F.2.2; G.2.1</acm-class><abstract>  The recent result that n congruent balls in R^d have at most 4 distinct
geometric permutations is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102005</id><created>2001-02-07</created><updated>2001-02-08</updated><authors><author><keyname>Chuang</keyname><forenames>Richie Chih-Nan</forenames></author><author><keyname>Garg</keyname><forenames>Ashim</forenames></author><author><keyname>He</keyname><forenames>Xin</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>Compact Encodings of Planar Graphs via Canonical Orderings and Multiple
  Parentheses</title><categories>cs.DS cs.DM</categories><comments>24 pages; 3 figures; a preliminary version appeared in the
  Proceedings of ICALP'98, LNCS 1443, pp. 118-129. (The 2nd version contains
  some minor changes.)</comments><acm-class>F.2.2; G.2.2; E.4</acm-class><abstract>  Let G be a plane graph of n nodes, m edges, f faces, and no self-loop. G need
not be connected or simple (i.e., free of multiple edges). We give three sets
of coding schemes for G which all take O(m+n) time for encoding and decoding.
Our schemes employ new properties of canonical orderings for planar graphs and
new techniques of processing strings of multiple types of parentheses.
  For applications that need to determine in O(1) time the adjacency of two
nodes and the degree of a node, we use 2m+(5+1/k)n + o(m+n) bits for any
constant k &gt; 0 while the best previous bound by Munro and Raman is 2m+8n +
o(m+n). If G is triconnected or triangulated, our bit count decreases to 2m+3n
+ o(m+n) or 2m+2n + o(m+n), respectively. If G is simple, our bit count is
(5/3)m+(5+1/k)n + o(n) for any constant k &gt; 0. Thus, if a simple G is also
triconnected or triangulated, then 2m+2n + o(n) or 2m+n + o(n) bits suffice,
respectively.
  If only adjacency queries are supported, the bit counts for a general G and a
simple G become 2m+(14/3)n + o(m+n) and (4/3)m+5n + o(n), respectively.
  If we only need to reconstruct G from its code, a simple and triconnected G
uses roughly 2.38m + O(1) bits while the best previous bound by He, Kao, and Lu
is 2.84m.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102006</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102006</id><created>2001-02-07</created><updated>2002-07-13</updated><authors><author><keyname>Chiang</keyname><forenames>Yi-Ting</forenames></author><author><keyname>Lin</keyname><forenames>Ching-Chi</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>Orderly Spanning Trees with Applications</title><categories>cs.DS cs.DM</categories><comments>25 pages, 7 figures, A preliminary version appeared in Proceedings of
  the 12th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2001),
  Washington D.C., USA, January 7-9, 2001, pp. 506-515</comments><acm-class>F.2.2; G.2.2; E.4; E.1</acm-class><journal-ref>SIAM Journal on Computing 34(4): 924-945 (2005)</journal-ref><doi>10.1137/S0097539702411381</doi><abstract>  We introduce and study the {\em orderly spanning trees} of plane graphs. This
algorithmic tool generalizes {\em canonical orderings}, which exist only for
triconnected plane graphs. Although not every plane graph admits an orderly
spanning tree, we provide an algorithm to compute an {\em orderly pair} for any
connected planar graph $G$, consisting of a plane graph $H$ of $G$, and an
orderly spanning tree of $H$. We also present several applications of orderly
spanning trees: (1) a new constructive proof for Schnyder's Realizer Theorem,
(2) the first area-optimal 2-visibility drawing of $G$, and (3) the best known
encodings of $G$ with O(1)-time query support. All algorithms in this paper run
in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102007</id><created>2001-02-09</created><authors><author><keyname>Chen</keyname><forenames>Zhi-Zhong</forenames></author><author><keyname>He</keyname><forenames>Xin</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Common-Face Embeddings of Planar Graphs</title><categories>cs.DS cs.DM</categories><comments>A preliminary version appeared in the Proceedings of the 10th Annual
  ACM-SIAM Symposium on Discrete Algorithms, 1999, pp. 195-204</comments><acm-class>G.2.2</acm-class><abstract>  Given a planar graph G and a sequence C_1,...,C_q, where each C_i is a family
of vertex subsets of G, we wish to find a plane embedding of G, if any exists,
such that for each i in {1,...,q}, there is a face F_i in the embedding whose
boundary contains at least one vertex from each set in C_i. This problem has
applications to the recovery of topological information from geographical data
and the design of constrained layouts in VLSI. Let I be the input size, i.e.,
the total number of vertices and edges in G and the families C_i, counting
multiplicity. We show that this problem is NP-complete in general. We also show
that it is solvable in O(I log I) time for the special case where for each
input family C_i, each set in C_i induces a connected subgraph of the input
graph G. Note that the classical problem of simply finding a planar embedding
is a further special case of this case with q=0. Therefore, the processing of
the additional constraints C_1,...,C_q only incurs a logarithmic factor of
overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102008</id><created>2001-02-09</created><authors><author><keyname>Chen</keyname><forenames>Yuyu</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>Optimal Bid Sequences for Multiple-Object Auctions with Unequal Budgets</title><categories>cs.CE cs.DM cs.DS</categories><comments>A preliminary version appeared in In D. T. Lee and S. H. Teng,
  editors, Lecture Notes in Computer Science 1969: Proceedings of the 11th
  Annual International Symposium on Algorithms and Computation, pages 84--95,
  New York, NY, 2000. Springer-Verlag</comments><acm-class>F.2.2; G.2.1; J.4</acm-class><abstract>  In a multiple-object auction, every bidder tries to win as many objects as
possible with a bidding algorithm. This paper studies position-randomized
auctions, which form a special class of multiple-object auctions where a
bidding algorithm consists of an initial bid sequence and an algorithm for
randomly permuting the sequence. We are especially concerned with situations
where some bidders know the bidding algorithms of others. For the case of only
two bidders, we give an optimal bidding algorithm for the disadvantaged bidder.
Our result generalizes previous work by allowing the bidders to have unequal
budgets. One might naturally anticipate that the optimal expected numbers of
objects won by the bidders would be proportional to their budgets.
Surprisingly, this is not true. Our new algorithm runs in optimal O(n) time in
a straightforward manner. The case with more than two bidders is open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102009</id><created>2001-02-09</created><authors><author><keyname>Hsu</keyname><forenames>Tsan-sheng</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Optimal Augmentation for Bipartite Componentwise Biconnectivity in
  Linear Time</title><categories>cs.DS cs.DM</categories><comments>A preliminary version appeared in T. Asano, Y. Igarashi, H.
  Nagamochi, S. Miyano, and S. Suri, editors, Lecture Notes in Computer Science
  1178: Proceedings of the 7th Annual International Symposium on Algorithms and
  Computation, pages 213--222. Springer-Verlag, New York, NY, 1996</comments><acm-class>F.2.2; G.2.2</acm-class><abstract>  A graph is componentwise biconnected if every connected component either is
an isolated vertex or is biconnected. We present a linear-time algorithm for
the problem of adding the smallest number of edges to make a bipartite graph
componentwise biconnected while preserving its bipartiteness. This algorithm
has immediate applications for protecting sensitive information in statistical
tables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102010</id><created>2001-02-10</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Samet</keyname><forenames>Jared</forenames></author><author><keyname>Sung</keyname><forenames>Wing-Kin</forenames></author></authors><title>The Enhanced Double Digest Problem for DNA Physical Mapping</title><categories>cs.CE cs.DM cs.DS</categories><comments>A preliminary version appeared in M. Halldorsson, editor, Lecture
  Notes in Computer Science 1851: Proceedings of the 7th Scandinavian Workshop
  on Algorithm Theory, pages 383--392. Springer-Verlag, New York, NY, 2000</comments><acm-class>F.2.2; G.2.3; J.3</acm-class><abstract>  The double digest problem is a common NP-hard approach to constructing
physical maps of DNA sequences. This paper presents a new approach called the
enhanced double digest problem. Although this new problem is also NP-hard, it
can be solved in linear time in certain theoretically interesting cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102011</id><created>2001-02-15</created><authors><author><keyname>Rasmusson</keyname><forenames>Lars</forenames></author><author><keyname>Aurell</keyname><forenames>Erik</forenames></author></authors><title>A Price Dynamics in Bandwidth Markets for Point-to-point Connections</title><categories>cs.NI cond-mat.soft cs.MA</categories><comments>18 pages, 10 postscript figures</comments><acm-class>C.2.3; C.4</acm-class><abstract>  We simulate a network of N routers and M network users making concurrent
point-to-point connections by buying and selling router capacity from each
other. The resources need to be acquired in complete sets, but there is only
one spot market for each router. In order to describe the internal dynamics of
the market, we model the observed prices by N-dimensional Ito-processes.
Modeling using stochastic processes is novel in this context of describing
interactions between end-users in a system with shared resources, and allows a
standard set of mathematical tools to be applied. The derived models can also
be used to price contingent claims on network capacity and thus to price
complex network services such as quality of service levels, multicast, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102012</id><created>2001-02-16</created><authors><author><keyname>Philip</keyname><forenames>Ninan Sajeeth</forenames></author><author><keyname>Joseph</keyname><forenames>K. Babu</forenames></author></authors><title>Chaos for Stream Cipher</title><categories>cs.CR</categories><comments>8 pages 6 figures</comments><acm-class>A0</acm-class><journal-ref>In proceedings of ADCOM 2000, Tata McGraw Hill 2001</journal-ref><abstract>  This paper discusses mixing of chaotic systems as a dependable method for
secure communication. Distribution of the entropy function for steady state as
well as plaintext input sequences are analyzed. It is shown that the mixing of
chaotic sequences results in a sequence that does not have any state dependence
on the information encrypted by them. The generated output states of such a
cipher approach the theoretical maximum for both complexity measures and cycle
length. These features are then compared with some popular ciphers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102013</id><created>2001-02-19</created><updated>2003-06-10</updated><authors><author><keyname>Kobayashi</keyname><forenames>Hirotada</forenames></author><author><keyname>Matsumoto</keyname><forenames>Keiji</forenames></author></authors><title>Quantum Multi-Prover Interactive Proof Systems with Limited Prior
  Entanglement</title><categories>cs.CC quant-ph</categories><comments>LaTeX2e, 19 pages, 2 figures, title changed, some of the sections are
  fully revised, journal version in Journal of Computer and System Sciences</comments><acm-class>F.1.2;F.1.3</acm-class><journal-ref>Journal of Computer and System Sciences, 66(3):429--450, 2003</journal-ref><abstract>  This paper gives the first formal treatment of a quantum analogue of
multi-prover interactive proof systems. It is proved that the class of
languages having quantum multi-prover interactive proof systems is necessarily
contained in NEXP, under the assumption that provers are allowed to share at
most polynomially many prior-entangled qubits. This implies that, in
particular, if provers do not share any prior entanglement with each other, the
class of languages having quantum multi-prover interactive proof systems is
equal to NEXP. Related to these, it is shown that, in the case a prover does
not have his private qubits, the class of languages having quantum
single-prover interactive proof systems is also equal to NEXP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102014</id><created>2001-02-18</created><authors><author><keyname>Philip</keyname><forenames>Ninan Sajeeth</forenames></author><author><keyname>Joseph</keyname><forenames>K. Babu</forenames></author></authors><title>On the predictability of Rainfall in Kerala- An application of ABF
  Neural Network</title><categories>cs.NE cs.AI</categories><acm-class>A0</acm-class><abstract>  Rainfall in Kerala State, the southern part of Indian Peninsula in particular
is caused by the two monsoons and the two cyclones every year. In general,
climate and rainfall are highly nonlinear phenomena in nature giving rise to
what is known as the `butterfly effect'. We however attempt to train an ABF
neural network on the time series rainfall data and show for the first time
that in spite of the fluctuations resulting from the nonlinearity in the
system, the trends in the rainfall pattern in this corner of the globe have
remained unaffected over the past 87 years from 1893 to 1980. We also
successfully filter out the chaotic part of the system and illustrate that its
effects are marginal over long term predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102015</id><created>2001-02-20</created><authors><author><keyname>Visentin</keyname><forenames>Marco</forenames></author></authors><title>Non-convex cost functionals in boosting algorithms and methods for panel
  selection</title><categories>cs.NE cs.LG cs.NA</categories><acm-class>I.2.6;G.1.2;G.3;I.6.5</acm-class><abstract>  In this document we propose a new improvement for boosting techniques as
proposed in Friedman '99 by the use of non-convex cost functional. The idea is
to introduce a correlation term to better deal with forecasting of additive
time series. The problem is discussed in a theoretical way to prove the
existence of minimizing sequence, and in a numerical way to propose a new
&quot;ArgMin&quot; algorithm. The model has been used to perform the touristic presence
forecast for the winter season 1999/2000 in Trentino (italian Alps).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102016</id><created>2001-02-20</created><authors><author><keyname>No</keyname><forenames>Jaechun</forenames></author><author><keyname>Thakur</keyname><forenames>Rajeev</forenames></author><author><keyname>Kaushik</keyname><forenames>Dinesh</forenames></author><author><keyname>Freitag</keyname><forenames>Lori</forenames></author><author><keyname>Choudhary</keyname><forenames>Alok</forenames></author></authors><title>A Scientific Data Management System for Irregular Applications</title><categories>cs.DC</categories><comments>7 pages + title page</comments><report-no>ANL/MCS-P866-1000</report-no><acm-class>B.4; B.4.3</acm-class><abstract>  Many scientific applications are I/O intensive and generate or access large
data sets, spanning hundreds or thousands of &quot;files.&quot; Management, storage,
efficient access, and analysis of this data present an extremely challenging
task. We have developed a software system, called Scientific Data Manager
(SDM), that uses a combination of parallel file I/O and database support for
high-performance scientific data management. SDM provides a high-level API to
the user and internally, uses a parallel file system to store real data and a
database to store application-related metadata. In this paper, we describe how
we designed and implemented SDM to support irregular applications. SDM can
efficiently handle the reading and writing of data in an irregular mesh as well
as the distribution of index values. We describe the SDM user interface and how
we implemented it to achieve high performance. SDM makes extensive use of
MPI-IO's noncontiguous collective I/O functions. SDM also uses the concept of a
history file to optimize the cost of the index distribution using the metadata
stored in the database. We present performance results with two irregular
applications, a CFD code called FUN3D and a Rayleigh-Taylor instability code,
on the SGI Origin2000 at Argonne National Laboratory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102017</id><created>2001-02-21</created><authors><author><keyname>Butler</keyname><forenames>Ralph</forenames></author><author><keyname>Gropp</keyname><forenames>William</forenames></author><author><keyname>Lusk</keyname><forenames>Ewing</forenames></author></authors><title>Components and Interfaces of a Process Management System for Parallel
  Programs</title><categories>cs.DC</categories><comments>12 pages, Workshop on Clusters and Computational Grids for Scientific
  Computing, Sept. 24-27, 2000, Le Chateau de Faverges de la Tour, France</comments><report-no>ANL/MCS-P872-0201</report-no><acm-class>C.1.4</acm-class><abstract>  Parallel jobs are different from sequential jobs and require a different type
of process management. We present here a process management system for parallel
programs such as those written using MPI. A primary goal of the system, which
we call MPD (for multipurpose daemon), is to be scalable. By this we mean that
startup of interactive parallel jobs comprising thousands of processes is
quick, that signals can be quickly delivered to processes, and that stdin,
stdout, and stderr are managed intuitively. Our primary target is parallel
machines made up of clusters of SMPs, but the system is also useful in more
tightly integrated environments. We describe how MPD enables much faster
startup and better runtime management of parallel jobs. We show how close
control of stdio can support the easy implementation of a number of convenient
system utilities, even a parallel debugger. We describe a simple but general
interface that can be used to separate any process manager from a parallel
library, which we use to keep MPD separate from MPICH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102018</id><created>2001-02-21</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>An effective Procedure for Speeding up Algorithms</title><categories>cs.CC cs.AI cs.LG</categories><comments>10 LaTeX pages</comments><report-no>IDSIA-16-00</report-no><acm-class>F.2.3</acm-class><journal-ref>Workshop on Mathematical approaches to Biological Computation
  (MaBiC 2001) and Workshop on Algorithmic Information Theory (TAI 2001)</journal-ref><abstract>  The provably asymptotically fastest algorithm within a factor of 5 for
formally described problems will be constructed. The main idea is to enumerate
all programs provably equivalent to the original problem by enumerating all
proofs. The algorithm could be interpreted as a generalization and improvement
of Levin search, which is, within a multiplicative constant, the fastest
algorithm for inverting functions. Blum's speed-up theorem is avoided by taking
into account only programs for which a correctness proof exists. Furthermore,
it is shown that the fastest program that computes a certain function is also
one of the shortest programs provably computing this function. To quantify this
statement, the definition of Kolmogorov complexity is extended, and two new
natural measures for the complexity of a function are defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102019</id><created>2001-02-21</created><authors><author><keyname>Eisner</keyname><forenames>Jason</forenames><affiliation>Dept. of Computer Science / University of Rochester</affiliation></author></authors><title>Easy and Hard Constraint Ranking in OT: Algorithms and Complexity</title><categories>cs.CL cs.CC</categories><comments>12 pages, online proceedings version (small corrections and
  clarifications to printed version)</comments><acm-class>I.2.7; F.2.2</acm-class><journal-ref>Jason Eisner, Lauri Karttunen and Alain Theriault (eds.),
  Finite-State Phonology: Proceedings of the 5th Workshop of the ACL Special
  Interest Group in Computational Phonology (SIGPHON), pp. 22-33. Luxembourg,
  August 2000</journal-ref><abstract>  We consider the problem of ranking a set of OT constraints in a manner
consistent with data.
  We speed up Tesar and Smolensky's RCD algorithm to be linear on the number of
constraints. This finds a ranking so each attested form x_i beats or ties a
particular competitor y_i. We also generalize RCD so each x_i beats or ties all
possible competitors.
  Alas, this more realistic version of learning has no polynomial algorithm
unless P=NP! Indeed, not even generation does. So one cannot improve
qualitatively upon brute force:
  Merely checking that a single (given) ranking is consistent with given forms
is coNP-complete if the surface forms are fully observed and Delta_2^p-complete
if not. Indeed, OT generation is OptP-complete. As for ranking, determining
whether any consistent ranking exists is coNP-hard (but in Delta_2^p) if the
forms are fully observed, and Sigma_2^p-complete if not.
  Finally, we show that generation and ranking are easier in derivational
theories: in P, and NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102020</id><created>2001-02-22</created><authors><author><keyname>Belz</keyname><forenames>Anja</forenames><affiliation>CCSRC, SRI International</affiliation></author></authors><title>Multi-Syllable Phonotactic Modelling</title><categories>cs.CL</categories><comments>11 pages, 4 tables, 9 figures, workshop</comments><acm-class>I.2.7</acm-class><journal-ref>Jason Eisner, Lauri Karttunen and Alain Theriault (eds.),
  Finite-State Phonology: Proceedings of the 5th Workshop of the ACL Special
  Interest Group in Computational Phonology (SIGPHON), pp. 46-56. Luxembourg,
  August 2000</journal-ref><abstract>  This paper describes a novel approach to constructing phonotactic models. The
underlying theoretical approach to phonological description is the
multisyllable approach in which multiple syllable classes are defined that
reflect phonotactically idiosyncratic syllable subcategories. A new
finite-state formalism, OFS Modelling, is used as a tool for encoding,
automatically constructing and generalising phonotactic descriptions.
Language-independent prototype models are constructed which are instantiated on
the basis of data sets of phonological strings, and generalised with a
clustering algorithm. The resulting approach enables the automatic construction
of phonotactic models that encode arbitrarily close approximations of a
language's set of attested phonological forms. The approach is applied to the
construction of multi-syllable word-level phonotactic models for German,
English and Dutch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102021</id><created>2001-02-22</created><authors><author><keyname>Albro</keyname><forenames>Daniel</forenames><affiliation>UCLA</affiliation></author></authors><title>Taking Primitive Optimality Theory Beyond the Finite State</title><categories>cs.CL</categories><comments>11 pages, 5 figures, workshop</comments><acm-class>I.2.7</acm-class><journal-ref>Jason Eisner, Lauri Karttunen and Alain Theriault (eds.),
  Finite-State Phonology: Proceedings of the 5th Workshop of the ACL Special
  Interest Group in Computational Phonology (SIGPHON), pp. 57-67. Luxembourg,
  August 2000</journal-ref><abstract>  Primitive Optimality Theory (OTP) (Eisner, 1997a; Albro, 1998), a
computational model of Optimality Theory (Prince and Smolensky, 1993), employs
a finite state machine to represent the set of active candidates at each stage
of an Optimality Theoretic derivation, as well as weighted finite state
machines to represent the constraints themselves. For some purposes, however,
it would be convenient if the set of candidates were limited by some set of
criteria capable of being described only in a higher-level grammar formalism,
such as a Context Free Grammar, a Context Sensitive Grammar, or a Multiple
Context Free Grammar (Seki et al., 1991). Examples include reduplication and
phrasal stress models. Here we introduce a mechanism for OTP-like Optimality
Theory in which the constraints remain weighted finite state machines, but sets
of candidates are represented by higher-level grammars. In particular, we use
multiple context-free grammars to model reduplication in the manner of
Correspondence Theory (McCarthy and Prince, 1995), and develop an extended
version of the Earley Algorithm (Earley, 1970) to apply the constraints to a
reduplicating candidate set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102022</id><created>2001-02-22</created><updated>2001-02-22</updated><authors><author><keyname>Eisner</keyname><forenames>Jason</forenames><affiliation>University of Rochester</affiliation></author><author><keyname>Karttunen</keyname><forenames>Lauri</forenames><affiliation>Xerox Research Centre Europe</affiliation></author><author><keyname>Theriault</keyname><forenames>Alain</forenames><affiliation>Universite de Montreal</affiliation></author></authors><title>Finite-State Phonology: Proceedings of the 5th Workshop of the ACL
  Special Interest Group in Computational Phonology (SIGPHON)</title><categories>cs.CL</categories><comments>HTML page, Conference programme, short abstracts, links to papers,
  preface</comments><acm-class>I.2.7</acm-class><journal-ref>Jason Eisner, Lauri Karttunen and Alain Theriault (eds.),
  Finite-State Phonology: Proceedings of the 5th Workshop of the ACL Special
  Interest Group in Computational Phonology (SIGPHON). Luxembourg, August 2000</journal-ref><abstract>  Home page of the workshop proceedings, with pointers to the individually
archived papers. Includes front matter from the printed version of the
proceedings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102023</id><created>2001-02-23</created><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>Factored Notation for Interval I/O</title><categories>cs.NA</categories><report-no>DCS-264-IR</report-no><acm-class>B.4.m; G.1.0; G.4</acm-class><abstract>  This note addresses the input and output of intervals in the sense of
interval arithmetic and interval constraints. The most obvious, and so far most
widely used notation, for intervals has drawbacks that we remedy with a new
notation that we propose to call factored notation. It is more compact and
allows one to find a good trade-off between interval width and ease of reading.
We describe how such a trade-off can be based on the information yield (in the
sense of information theory) of the last decimal shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102024</id><created>2001-02-23</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Hempel</keyname><forenames>Harald</forenames></author></authors><title>P-Immune Sets with Holes Lack Self-Reducibility Properties</title><categories>cs.CC</categories><comments>11 pages</comments><report-no>UR-CS-TR-2001-742</report-no><acm-class>F.1.3; F.1.2; F.1.1</acm-class><abstract>  No P-immune set having exponential gaps is positive-Turing self-reducible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102025</id><created>2001-02-23</created><updated>2001-03-23</updated><authors><author><keyname>Bozzano</keyname><forenames>Marco</forenames></author><author><keyname>Delzanno</keyname><forenames>Giorgio</forenames></author><author><keyname>Martelli</keyname><forenames>Maurizio</forenames></author></authors><title>An Effective Fixpoint Semantics for Linear Logic Programs</title><categories>cs.PL</categories><comments>39 pages, 5 figures. To appear in Theory and Practice of Logic
  Programming</comments><acm-class>D.3.1;F.3.1;F.3.2</acm-class><abstract>  In this paper we investigate the theoretical foundation of a new bottom-up
semantics for linear logic programs, and more precisely for the fragment of
LinLog that consists of the language LO enriched with the constant 1. We use
constraints to symbolically and finitely represent possibly infinite
collections of provable goals. We define a fixpoint semantics based on a new
operator in the style of Tp working over constraints. An application of the
fixpoint operator can be computed algorithmically. As sufficient conditions for
termination, we show that the fixpoint computation is guaranteed to converge
for propositional LO. To our knowledge, this is the first attempt to define an
effective fixpoint semantics for linear logic programs. As an application of
our framework, we also present a formal investigation of the relations between
LO and Disjunctive Logic Programming. Using an approach based on abstract
interpretation, we show that DLP fixpoint semantics can be viewed as an
abstraction of our semantics for LO. We prove that the resulting abstraction is
correct and complete for an interesting class of LO programs encoding Petri
Nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102026</id><created>2001-02-24</created><authors><author><keyname>Kromer</keyname><forenames>Victor</forenames></author></authors><title>Mathematical Model of Word Length on the Basis of the Cebanov-Fucks
  Distribution with Uniform Parameter Distribution</title><categories>cs.CL</categories><comments>4 pages, 1 table, 2 figures.Submitted to Conference on Informatics
  and Telecommunications, to be held in Sibirian State University of
  Telecommunications (Novosibirsk, Russia) in April, 2001</comments><acm-class>I.2.7</acm-class><journal-ref>Kromer V.W. Matematiceskaja model' dliny slova na osnove
  raspredelenija Cebanova-Fuksa s ravnomernym raspredeleniem parametra //
  Informatika i problemy telekommunikacij: Mezdunarodnaja naucno-techniceskaja
  konferencija (SibGUTI, 26-27 aprelja 2001 g.) Materialy konferencii. -
  Novosibirsk: Isd-vo SibGUTI, 2001. - S. 74-75.</journal-ref><abstract>  The data on 13 typologically different languages have been processed using a
two-parameter word length model, based on 1-displaced uniform Poisson
distribution. Statistical dependencies of the 2nd parameter on the 1st one are
revealed for the German texts and genre of letters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102027</id><created>2001-02-25</created><updated>2001-12-30</updated><authors><author><keyname>Ferreira</keyname><forenames>Candida</forenames></author></authors><title>Gene Expression Programming: a New Adaptive Algorithm for Solving
  Problems</title><categories>cs.AI cs.NE</categories><comments>22 pages, 17 figures</comments><acm-class>I.2.2</acm-class><journal-ref>Complex Systems, 13(2): 87-129, 2001</journal-ref><abstract>  Gene expression programming, a genotype/phenotype genetic algorithm (linear
and ramified), is presented here for the first time as a new technique for the
creation of computer programs. Gene expression programming uses character
linear chromosomes composed of genes structurally organized in a head and a
tail. The chromosomes function as a genome and are subjected to modification by
means of mutation, transposition, root transposition, gene transposition, gene
recombination, and one- and two-point recombination. The chromosomes encode
expression trees which are the object of selection. The creation of these
separate entities (genome and expression tree) with distinct functions allows
the algorithm to perform with high efficiency that greatly surpasses existing
adaptive techniques. The suite of problems chosen to illustrate the power and
versatility of gene expression programming includes symbolic regression,
sequence induction with and without constant creation, block stacking, cellular
automata rules for the density-classification problem, and two problems of
boolean concept learning: the 11-multiplexer and the GP rule problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102028</id><created>2001-02-26</created><authors><author><keyname>Kimble</keyname><forenames>Chris</forenames></author><author><keyname>Hildreth</keyname><forenames>Paul</forenames></author><author><keyname>Wright</keyname><forenames>Peter</forenames></author></authors><title>Communities of Practice: Going Virtual</title><categories>cs.HC cs.CY</categories><comments>Draft version available from
  http://www-users.cs.york.ac.uk/~kimble/research/publics.html</comments><acm-class>H.5.3</acm-class><journal-ref>Chapter 13 in Knowledge Management and Business Model Innovation,
  Idea Group Publishing, Hershey (USA)/London (UK), 2001. pp 220 - 234</journal-ref><abstract>  With the current trends towards downsizing, outsourcing and globalisation,
modern organisations are reducing the numbers of people they employ. In
addition, organisations now have to cope with the increasing
internationalisation of business forcing collaboration and knowledge sharing
across time and distance simultaneously. There is a need for new ways of
thinking about how knowledge is shared in distributed groups. In this paper we
explore a relatively new approach to knowledge sharing using Lave and Wenger's
(1991) theory of Communities of Practice (CoPs). We investigate whether CoPs
might translate to a geographically distributed international environment
through a case study that explores the functioning of a CoP across national
boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102029</id><created>2001-02-26</created><authors><author><keyname>Kimble</keyname><forenames>Chris</forenames></author><author><keyname>McLoughlin</keyname><forenames>Kevin</forenames></author></authors><title>Computer based Information Systems and Managers' Work</title><categories>cs.CY cs.HC</categories><comments>Available from
  http://www-users.cs.york.ac.uk/~kimble/research/publics.html</comments><acm-class>K.4.3</acm-class><journal-ref>New Technology, Work and Employment, 10 (1), March, 1995. pp 56 -
  67</journal-ref><abstract>  This paper identifies three categories of model: the Technology Impact Model;
the Social Impact Model and the Integrationist Model, which imply different
views of the &quot;impact&quot; of Information Technology on work organisation. These
models are used to structure data from case studies conducted by the authors to
explore the implications of the use of computer-based information systems for
managers' work. The paper argues that the &quot;impact&quot; of information systems is
not a single stable and predictable outcome but a non-linear ongoing process
that changes and evolves over time. It also argues that the actions of
individuals and groups within an organisation are not wholly determined by
outside forces: people can and do react to, and shape, systems in different
ways. In this sense, the &quot;impact&quot; of computer-based information systems on
managers' work reflects decisions made by managers themselves about how the
technology is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0102030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0102030</id><created>2001-02-27</created><authors><author><keyname>Hill</keyname><forenames>Patricia M.</forenames></author><author><keyname>Bagnara</keyname><forenames>Roberto</forenames></author><author><keyname>Zaffanella</keyname><forenames>Enea</forenames></author></authors><title>Soundness, Idempotence and Commutativity of Set-Sharing</title><categories>cs.PL</categories><comments>48 pages</comments><acm-class>F.3.2</acm-class><abstract>  It is important that practical data-flow analyzers are backed by reliably
proven theoretical results. Abstract interpretation provides a sound
mathematical framework and necessary generic properties for an abstract domain
to be well-defined and sound with respect to the concrete semantics. In logic
programming, the abstract domain Sharing is a standard choice for sharing
analysis for both practical work and further theoretical study. In spite of
this, we found that there were no satisfactory proofs for the key properties of
commutativity and idempotence that are essential for Sharing to be well-defined
and that published statements of the soundness of Sharing assume the
occurs-check. This paper provides a generalization of the abstraction function
for Sharing that can be applied to any language, with or without the
occurs-check. Results for soundness, idempotence and commutativity for abstract
unification using this abstraction function are proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103001</id><created>2001-03-01</created><authors><author><keyname>Higuera</keyname><forenames>G. Mario A.</forenames></author><author><keyname>Sarria</keyname><forenames>Humberto</forenames></author><author><keyname>Fonseca</keyname><forenames>Diana</forenames></author><author><keyname>Idarraga</keyname><forenames>John</forenames></author></authors><title>Construction of an algorithm in parallel for the Fast Fourier Transform</title><categories>cs.DC cs.DS</categories><acm-class>D.1.3</acm-class><abstract>  It has been designed,built and executed a code for the Fast Fourier Transform
(FFT),compiled and executed in a cluster of 2^n computers under the operating
system MacOS and using the routines MacMPI. As practical application,the code
has been used to obtain the transformed from an astronomic imagen,to execute a
filter on its and with a transformed inverse to recover the image with the
variates given by the filter.The computers arrangement are installed in the
Observatorio Astronomico National in Colombia under the name OAN Cluster and in
this has been executed several applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103002</id><created>2001-03-01</created><authors><author><keyname>Gopych</keyname><forenames>Petro M.</forenames></author></authors><title>Quantitative Neural Network Model of the Tip-of-the-Tongue Phenomenon
  Based on Synthesized Memory-Psycholinguistic-Metacognitive Approach</title><categories>cs.CL cs.AI q-bio.NC q-bio.QM</categories><comments>Proceedings of The Second International Conference
  Internet-Education-Science-2000 (IES-2000): New Informational and Computer
  Technologies in Education and Science, held on October 10-12, 2000 in
  Vinnytsia, Ukraine, page 273</comments><acm-class>I.2.7</acm-class><abstract>  A new three-stage computer artificial neural network model of the
tip-of-the-tongue phenomenon is proposed. Each word's node is build from some
interconnected learned auto-associative two-layer neural networks each of which
represents separate word's semantic, lexical, or phonological components. The
model synthesizes memory, psycholinguistic, and metamemory approaches, bridges
speech errors and naming chronometry research traditions, and can explain
quantitatively many tip-of-the-tongue effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103003</id><created>2001-03-01</created><authors><author><keyname>Peshkin</keyname><forenames>Leonid</forenames></author><author><keyname>Meuleau</keyname><forenames>Nicolas</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie</forenames></author></authors><title>Learning Policies with External Memory</title><categories>cs.LG</categories><comments>8 pages</comments><acm-class>I.2.8;I.2.6;I.2.11;I.2;I.2.3</acm-class><journal-ref>In Bratko, I., and Dzeroski, S., eds., Machine Learning:
  Proceedings of the Sixteenth International Conference, pp. 307-314. Morgan
  Kaufmann, San Francisco, CA</journal-ref><abstract>  In order for an agent to perform well in partially observable domains, it is
usually necessary for actions to depend on the history of observations. In this
paper, we explore a {\it stigmergic} approach, in which the agent's actions
include the ability to set and clear bits in an external memory, and the
external memory is included as part of the input to the agent. In this case, we
need to learn a reactive policy in a highly non-Markovian domain. We explore
two algorithms: SARSA(\lambda), which has had empirical success in partially
observable domains, and VAPS, a new algorithm due to Baird and Moore, with
convergence guarantees in partially observable domains. We compare the
performance of these two algorithms on benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103004</id><created>2001-03-02</created><authors><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author><author><keyname>Smith</keyname><forenames>Ian E.</forenames></author><author><keyname>Thornton</keyname><forenames>James D.</forenames></author></authors><title>Rapid Application Evolution and Integration Through Document
  Metamorphosis</title><categories>cs.DB</categories><acm-class>H.2.3; D.2.11; K.6.3</acm-class><abstract>  The Harland document management system implements a data model in which
document (object) structure can be altered by mixin-style multiple inheritance
at any time. This kind of structural fluidity has long been supported by
knowledge-base management systems, but its use has primarily been in support of
reasoning and inference. In this paper, we report our experiences building and
supporting several non-trivial applications on top of this data model. Based on
these experiences, we argue that structural fluidity is convenient for
data-intensive applications other than knowledge-base management. Specifically,
we suggest that this flexible data model is a natural fit for the decoupled
programming methodology that arises naturally when using enterprise component
frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103005</id><created>2001-03-04</created><authors><author><keyname>Bisnovatyi</keyname><forenames>Ilia</forenames></author><author><keyname>O'Donnell</keyname><forenames>Michael J.</forenames></author></authors><title>Source-Filter Decomposition of Harmonic Sounds</title><categories>cs.SD</categories><comments>Preliminary results reported at DAFx-99, in &quot;Decomposition of Steady
  State Instrument Data into Excitation System and Formant Filter Components&quot;,
  http://www.tele.ntnu.no/akustikk/meetings/DAFx99/bisnovatyi.pdf 5 pages + 2
  appendices (6 graphs, 1 table)</comments><acm-class>H 5.5; G 1.0</acm-class><abstract>  This paper describes a method for decomposing steady-state instrument data
into excitation and formant filter components. The input data, taken from
several series of recordings of acoustical instruments is analyzed in the
frequency domain, and for each series a model is built, which most accurately
represents the data as a source-filter system. The source part is taken to be a
harmonic excitation system with frequency-invariant magnitudes, and the filter
part is considered to be responsible for all spectral inhomogenieties. This
method has been applied to the SHARC database of steady state instrument data
to create source-filter models for a large number of acoustical instruments.
Subsequent use of such models can have a wide variety of applications,
including improvements to wavetable and physical modeling synthesis, high
quality pitch shifting, and creation of &quot;hybrid&quot; instrument timbres.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103006</id><created>2001-03-05</created><authors><author><keyname>Bisnovatyi</keyname><forenames>Ilia</forenames></author></authors><title>Flexible Software Framework for Modal Synthesis</title><categories>cs.SD</categories><comments>Presented at DAFx00,
  http://profs.sci.univr.it/~dafx/DAFx-final-papers.html</comments><acm-class>H 5.5; I 6.3</acm-class><journal-ref>in Proceedings of COST-G6 Conference on Digital Audio Effects
  (DAFx-00), Dec 7-9, 2000, Verona, Italy</journal-ref><abstract>  Modal synthesis is an important area of physical modeling whose exploration
in the past has been held back by a large number of control parameters, the
scarcity of general-purpose design tools and the difficulty of obtaining the
computational power required for real-time synthesis. This paper presents an
overview of a flexible software framework facilitating the design and control
of instruments based on modal synthesis. The framework is designed as a
hierarchy of polymorphic synthesis objects, representing modal structures of
various complexity. As a method of generalizing all interactions among the
elements of a modal system, an abstract notion of {\it energy} is introduced,
and a set of energy transfer functions is provided. Such abstraction leads to a
design where the dynamics of interactions can be largely separated from the
specifics of particular modal structures, yielding an easily configurable and
expandable system. A real-time version of the framework has been implemented as
a set of C++ classes along with an integrating shell and a GUI, and is
currently being used to design and play modal instruments, as well as to survey
fundamental properties of various modal algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103007</id><created>2001-03-08</created><authors><author><keyname>Kromer</keyname><forenames>Victor</forenames></author></authors><title>Two-parameter Model of Word Length &quot;Language - Genre&quot;</title><categories>cs.CL</categories><comments>2 pages, 1 figure. In Russian</comments><acm-class>I.2.7</acm-class><abstract>  A two-parameter model of word length measured by the number of syllables
comprising it is proposed. The first parameter is dependent on language type,
the second one - on text genre and reflects the degree of completion of
synergetic processes of language optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103008</id><created>2001-03-08</created><updated>2002-02-07</updated><authors><author><keyname>Ma</keyname><forenames>Shilong</forenames></author><author><keyname>Sui</keyname><forenames>Yuefei</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>The Limits of Horn Logic Programs</title><categories>cs.LO cs.PL</categories><comments>11 pages, added new results. Welcome any comments to
  kexu@nlsde.buaa.edu.cn</comments><acm-class>D.1.6; F.3.2</acm-class><journal-ref>In P. J. Stuckey (Ed.): Proc. of 18th ICLP (short paper), LNCS
  2401, p. 467, Denmark, 2002.</journal-ref><abstract>  Given a sequence $\{\Pi_n\}$ of Horn logic programs, the limit $\Pi$ of
$\{\Pi_n\}$ is the set of the clauses such that every clause in $\Pi$ belongs
to almost every $\Pi_n$ and every clause in infinitely many $\Pi_n$'s belongs
to $\Pi$ also. The limit program $\Pi$ is still Horn but may be infinite. In
this paper, we consider if the least Herbrand model of the limit of a given
Horn logic program sequence $\{\Pi_n\}$ equals the limit of the least Herbrand
models of each logic program $\Pi_n$. It is proved that this property is not
true in general but holds if Horn logic programs satisfy an assumption which
can be syntactically checked and be satisfied by a class of Horn logic
programs. Thus, under this assumption we can approach the least Herbrand model
of the limit $\Pi$ by the sequence of the least Herbrand models of each finite
program $\Pi_n$. We also prove that if a finite Horn logic program satisfies
this assumption, then the least Herbrand model of this program is recursive.
Finally, by use of the concept of stability from dynamical systems, we prove
that this assumption is exactly a sufficient condition to guarantee the
stability of fixed points for Horn logic programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103009</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103009</id><created>2001-03-08</created><updated>2003-03-27</updated><authors><author><keyname>Bettelli</keyname><forenames>S.</forenames></author><author><keyname>Serafini</keyname><forenames>L.</forenames></author><author><keyname>Calarco</keyname><forenames>T.</forenames></author></authors><title>Toward an architecture for quantum programming</title><categories>cs.PL quant-ph</categories><comments>23 pages, 5 figures, A4paper. Final version accepted by EJPD (&quot;swap&quot;
  replaced by &quot;invert&quot; for Qops). Preliminary implementation available at:
  http://sra.itc.it/people/serafini/quantum-computing/qlang.html</comments><report-no>IRST technical report 0103-010</report-no><acm-class>D.3.1</acm-class><journal-ref>Eur. Phys. J. D, Vol. 25, No. 2, pp. 181-200 (2003)</journal-ref><doi>10.1140/epjd/e2003-00242-2</doi><abstract>  It is becoming increasingly clear that, if a useful device for quantum
computation will ever be built, it will be embodied by a classical computing
machine with control over a truly quantum subsystem, this apparatus performing
a mixture of classical and quantum computation.
  This paper investigates a possible approach to the problem of programming
such machines: a template high level quantum language is presented which
complements a generic general purpose classical language with a set of quantum
primitives. The underlying scheme involves a run-time environment which
calculates the byte-code for the quantum operations and pipes it to a quantum
device controller or to a simulator.
  This language can compactly express existing quantum algorithms and reduce
them to sequences of elementary operations; it also easily lends itself to
automatic, hardware independent, circuit simplification. A publicly available
preliminary implementation of the proposed ideas has been realized using the
C++ language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103010</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103010</id><created>2001-03-12</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Uchimoto</keyname><forenames>Kiyotaka</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Magical Number Seven Plus or Minus Two: Syntactic Structure Recognition
  in Japanese and English Sentences</title><categories>cs.CL</categories><comments>9 pages. Computation and Language. This paper is included in the book
  entitled by &quot;Computational Linguistics and Intelligent Text Processing,
  Second International Conference, CICLing 2001, Mexico City, February 2001
  Proceedings&quot;, Alexander Gelbukh (Ed.), Springer Publisher, ISSN 0302-9743,
  ISBN 3-540-41687-0</comments><acm-class>H.3.3; I.2.7</acm-class><journal-ref>CICLing'2001, Mexico City, February 2001</journal-ref><abstract>  George A. Miller said that human beings have only seven chunks in short-term
memory, plus or minus two. We counted the number of bunsetsus (phrases) whose
modifiees are undetermined in each step of an analysis of the dependency
structure of Japanese sentences, and which therefore must be stored in
short-term memory. The number was roughly less than nine, the upper bound of
seven plus or minus two. We also obtained similar results with English
sentences under the assumption that human beings recognize a series of words,
such as a noun phrase (NP), as a unit. This indicates that if we assume that
the human cognitive units in Japanese and English are bunsetsu and NP
respectively, analysis will support Miller's $7 \pm 2$ theory.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="94000" completeListSize="102538">1122234|95001</resumptionToken>
</ListRecords>
</OAI-PMH>
