<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:37:26Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|67001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2182</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2182</id><created>2014-09-20</created><authors><author><keyname>Niu</keyname><forenames>Zhihua</forenames></author><author><keyname>Chen</keyname><forenames>Zhixiong</forenames></author><author><keyname>Du</keyname><forenames>Xiaoni</forenames></author></authors><title>Linear complexity problems of level sequences of Euler quotients and
  their related binary sequences</title><categories>math.NT cs.CR</categories><comments>16 pages</comments><msc-class>94A55, 94A60, 65C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Euler quotient modulo an odd-prime power $p^r~(r&gt;1)$ can be uniquely
decomposed as a $p$-adic number of the form $$ \frac{u^{(p-1)p^{r-1}}
-1}{p^r}\equiv a_0(u)+a_1(u)p+\ldots+a_{r-1}(u)p^{r-1} \pmod {p^r},~
\gcd(u,p)=1, $$ where $0\le a_j(u)&lt;p$ for $0\le j\le r-1$ and we set all
$a_j(u)=0$ if $\gcd(u,p)&gt;1$. We firstly study certain arithmetic properties of
the level sequences $(a_j(u))_{u\ge 0}$ over $\mathbb{F}_p$ via introducing a
new quotient. Then we determine the exact values of linear complexity of
$(a_j(u))_{u\ge 0}$ and values of $k$-error linear complexity for binary
sequences defined by $(a_j(u))_{u\ge 0}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2188</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2188</id><created>2014-10-06</created><authors><author><keyname>Hu</keyname><forenames>Yuxin</forenames></author><author><keyname>Zhang</keyname><forenames>Luming</forenames></author></authors><title>An Aerial Image Recognition Framework using Discrimination and
  Redundancy Quality Measure</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aerial image categorization plays an indispensable role in remote sensing and
artificial intelligence. In this paper, we propose a new aerial image
categorization framework, focusing on organizing the local patches of each
aerial image into multiple discriminative subgraphs. The subgraphs reflect both
the geometric property and the color distribution of an aerial image. First,
each aerial image is decomposed into a collection of regions in terms of their
color intensities. Thereby region connected graph (RCG), which models the
connection between the spatial neighboring regions, is constructed to encode
the spatial context of an aerial image. Second, a subgraph mining technique is
adopted to discover the frequent structures in the RCGs constructed from the
training aerial images. Thereafter, a set of refined structures are selected
among the frequent ones toward being highly discriminative and low redundant.
Lastly, given a new aerial image, its sub-RCGs corresponding to the refined
structures are extracted. They are further quantized into a discriminative
vector for SVM classification. Thorough experimental results validate the
e?ectiveness of the proposed method. In addition, the visualized mined
subgraphs show that the discriminative topologies of each aerial image are
discovered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2190</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2190</id><created>2014-10-08</created><authors><author><keyname>Bapst</keyname><forenames>Victor</forenames></author><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Rassmann</keyname><forenames>Felicia</forenames></author></authors><title>A positive temperature phase transition in random hypergraph 2-coloring</title><categories>math.CO cs.DM math-ph math.MP math.PR</categories><msc-class>05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diluted mean-field models are graphical models in which the geometry of
interactions is determined by a sparse random graph or hypergraph. Based on a
non-rigorous but analytic approach called the &quot;cavity method&quot;, physicists have
predicted that in many diluted mean-field models a phase transition occurs as
the inverse temperature grows from $0$ to $\infty$. In this paper we establish
the existence and asymptotic location of this so-called condensation phase
transition in the random hypergraph $2$-coloring problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2191</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2191</id><created>2014-10-03</created><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author><author><keyname>Gao</keyname><forenames>Xin</forenames></author></authors><title>Learning manifold to regularize nonnegative matrix factorization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inthischapterwediscusshowtolearnanoptimalmanifoldpresentationto regularize
nonegative matrix factorization (NMF) for data representation problems.
NMF,whichtriestorepresentanonnegativedatamatrixasaproductoftwolowrank
nonnegative matrices, has been a popular method for data representation due to
its ability to explore the latent part-based structure of data. Recent study
shows that lots of data distributions have manifold structures, and we should
respect the manifold structure when the data are represented. Recently,
manifold regularized NMF used a nearest neighbor graph to regulate the learning
of factorization parameter matrices and has shown its advantage over
traditional NMF methods for data representation problems. However, how to
construct an optimal graph to present the manifold prop- erly remains a
difficultproblem due to the graph modelselection, noisy features, and nonlinear
distributed data. In this chapter, we introduce three effective methods to
solve these problems of graph construction for manifold regularized NMF.
Multiple graph learning is proposed to solve the problem of graph model
selection, adaptive graph learning via feature selection is proposed to solve
the problem of constructing a graph from noisy features, while multi-kernel
learning-based graph construction is used to solve the problem of learning a
graph from nonlinearly distributed data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2195</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2195</id><created>2014-10-08</created><authors><author><keyname>Alipour</keyname><forenames>Sharareh</forenames></author><author><keyname>Kalantari</keyname><forenames>Bahman</forenames></author><author><keyname>Homapour</keyname><forenames>Hamid</forenames></author></authors><title>Fast Approximation and Randomized Algorithms for Diameter</title><categories>cs.CG</categories><comments>13 pages, 6 figures, 3 tables</comments><msc-class>68Q25, 68U05, 65D18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider approximation of diameter of a set $S$ of $n$ points in dimension
$m$. E$\tilde{g}$ecio$\tilde{g}$lu and Kalantari \cite{kal} have shown that
given any $p \in S$, by computing its farthest in $S$, say $q$, and in turn the
farthest point of $q$, say $q'$, we have ${\rm diam}(S) \leq \sqrt{3} d(q,q')$.
Furthermore, iteratively replacing $p$ with an appropriately selected point on
the line segment $pq$, in at most $t \leq n$ additional iterations, the
constant bound factor is improved to $c_*=\sqrt{5-2\sqrt{3}} \approx 1.24$.
Here we prove when $m=2$, $t=1$. This suggests in practice a few iterations may
produce good solutions in any dimension. Here we also propose a randomized
version and present large scale computational results with these algorithm for
arbitrary $m$. The algorithms outperform many existing algorithms. On sets of
data as large as $1,000,000$ points, the proposed algorithms compute solutions
to within an absolute error of $10^{-4}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2196</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2196</id><created>2014-10-08</created><updated>2014-10-08</updated><authors><author><keyname>Zhang</keyname><forenames>June</forenames></author><author><keyname>Moura</keyname><forenames>Jos&#xe9; M. F.</forenames></author></authors><title>Role of Subgraphs in Epidemics over Finite-Size Networks under the
  Scaled SIS Process</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, we developed the scaled SIS process, which models the
dynamics of SIS epidemics over networks. With the scaled SIS process, we can
consider networks that are finite-sized and of arbitrary topology (i.e., we are
not restricted to specific classes of networks). We derived for the scaled SIS
process a closed-form expression for the time-asymptotic probability
distribution of the states of all the agents in the network. This closed-form
solution of the equilibrium distribution explicitly exhibits the underlying
network topology through its adjacency matrix. This paper determines which
network configuration is the most probable. We prove that, for a range of
epidemics parameters, this combinatorial problem leads to a submodular
optimization problem, which is exactly solvable in polynomial time. We relate
the most-probable configuration to the network structure, in particular, to the
existence of high density subgraphs. Depending on the epidemics parameters,
subset of agents may be more likely to be infected than others; these
more-vulnerable agents form subgraphs that are denser than the overall network.
We illustrate our results with a 193 node social network and the 4941 node
Western US power grid under different epidemics parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2202</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2202</id><created>2014-10-08</created><authors><author><keyname>Kalantari</keyname><forenames>Bahman</forenames></author><author><keyname>Lee</keyname><forenames>Eric</forenames></author></authors><title>Newton-Ellipsoid Method and its Polynomiography</title><categories>cs.NA</categories><comments>9 pages, 7 figures</comments><msc-class>65H04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new iterative root-finding method for complex polynomials,
dubbed {\it Newton-Ellipsoid} method. It is inspired by the Ellipsoid method, a
classical method in optimization, and a property of Newton's Method derived in
\cite{kalFTA}, according to which at each complex number a half-space can be
found containing a root. Newton-Ellipsoid method combines this property, bounds
on zeros, together with the plane-cutting properties of the Ellipsoid Method.
We present computational results for several examples, as well as corresponding
polynomiography. Polynomiography refers to algorithmic visualization of
root-finding. Newton's method is the first member of the infinite family of
iterations, the {\it basic family}. We also consider general versions of this
ellipsoid approach where Newton's method is replaced by a higher-order member
of the family such as Halley's method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2205</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2205</id><created>2014-09-10</created><authors><author><keyname>Nehemiah</keyname><forenames>Lawrence</forenames></author></authors><title>Towards EHR interoperability in Tanzania hospitals: Issues, Challenges
  and Opportunities</title><categories>cs.CY</categories><comments>8 pages, 3 tables</comments><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA) Vol.4, No.4, August 2014 (ISSN:2230 - 9616)</journal-ref><doi>10.5121/ijcsea.2014.4404</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This study aimed at identifying the issue, challenges and opportunities from
the health consumers in Tanzania towards interoperability of electronic health
records. Recognizing that we conducted a study to identify the challenges,
issues and opportunities towards health information exchange. The study was
conducted in three major cities of Tanzania. This was in order to come up with
a clear picture of how to implement some EHRs that will be trusted by health
consumers. The participants (n=240) were surveyed on computer usage, EHRs
knowledge, demographics, security and privacy issues. A total of 200 surveys
were completed and returned (83.3% response rate). 67.5% were women, 62.6% had
not heard of EHRs, 73% highly concerned about privacy and security of their
information. 75% believed that introduction of various security mechanisms will
make EHRs more secure. A number of chi-square tests (p&lt;0.05) showed there was a
strong relationship among the variable of age, computer use, EHRs knowledge and
concerns for privacy and security. The study showed that there was a small
difference of 8.5% between those who think EHRs are safer than paper records
and those who think otherwise. The general observation of the study was that in
order to make EHRs successful, then the issue of security, and health consumer
involvement were two key towards the road of successful EHRs in our hospitals
practices and that will make consumers more willing to allow their records to
be shared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2208</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2208</id><created>2014-09-22</created><authors><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author><author><keyname>Latiff</keyname><forenames>Muhammad Shafie Abd</forenames></author></authors><title>League Championship Algorithm Based Job Scheduling Scheme for
  Infrastructure as a Service Cloud</title><categories>cs.DC</categories><comments>6 pages, 3 figures, IGCESH2014</comments><journal-ref>5th International Graduate Conference on Engineering, Science and
  Humanities (IGCESH2014), 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  League Championship Algorithm (LCA) is a sports-inspired population based
algorithmic framework for global optimization over a continuous search space
first proposed by Ali Husseinzadeh Kashan in the year 2009. A common
characteristic between all population based optimization algorithms similar to
the LCA is that, they attemt to move a population of achievable solutions to
potential areas of the search space during optimization. In this paper, we
proposed a job scheduling algorithm based on the L CA optimization technique
for the infrastructure as a service (IaaS) cloud. Three other established
algorithms i.e. First Come First Served (FCFS), Last Job First (LJF) and Best
Effort First (BEF) were used to evaluate the performance of the proposed
algorithm. All four algorithms assumed to be non-preemptive. The parameters
used for this experiment are the average response time and the average
completion time. The results obtained shows that, LCA scheduling algorithm
perform moderately better than the other algorithms as the number of virtual
machines increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2209</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2209</id><created>2014-10-08</created><authors><author><keyname>Golovnev</keyname><forenames>Alexander</forenames></author><author><keyname>Kulikov</keyname><forenames>Alexander S.</forenames></author><author><keyname>Mihajlin</keyname><forenames>Ivan</forenames></author></authors><title>Families with infants: speeding up algorithms for NP-hard problems using
  FFT</title><categories>cs.DS</categories><comments>18 pages. arXiv admin note: substantial text overlap with
  arXiv:1311.2456</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assume that a group of people is going to an excursion and our task is to
seat them into buses with several constraints each saying that a pair of people
does not want to see each other in the same bus. This is a well-known coloring
problem and it can be solved in $O^*(2^n)$ time by the inclusion-exclusion
principle as shown by Bj\&quot;{o}rklund, Husfeldt, and Koivisto in 2009.Another
approach to solve this problem in $O^*(2^n)$ time is to use the fast Fourier
transform. A graph is $k$-colorable if and only if the $k$-th power of a
polynomial containing a monomial $\prod_{i=1}^n x_i^{[i \in I]}$ for each
independent set $I \subseteq [n]$ of the graph, contains the monomial
$x_1x_2... x_n$.
  Assume now that we have additional constraints: the group of people contains
several infants and these infants should be accompanied by their relatives in a
bus. We show that if the number of infants is linear then the problem can be
solved in $O^*((2-\varepsilon)^n)$ time. We use this approach to improve known
bounds for several NP-hard problems (the traveling salesman problem, the graph
coloring problem, the problem of counting perfect matchings) on graphs of
bounded average degree, as well as to simplify the proofs of several known
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2217</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2217</id><created>2014-10-08</created><authors><author><keyname>Acharya</keyname><forenames>Anurag</forenames></author><author><keyname>Verstak</keyname><forenames>Alex</forenames></author><author><keyname>Suzuki</keyname><forenames>Helder</forenames></author><author><keyname>Henderson</keyname><forenames>Sean</forenames></author><author><keyname>Iakhiaev</keyname><forenames>Mikhail</forenames></author><author><keyname>Lin</keyname><forenames>Cliff Chiung Yu</forenames></author><author><keyname>Shetty</keyname><forenames>Namit</forenames></author></authors><title>Rise of the Rest: The Growing Impact of Non-Elite Journals</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine the evolution of the impact of non-elite journals.
We attempt to answer two questions. First, what fraction of the top-cited
articles are published in non-elite journals and how has this changed over
time. Second, what fraction of the total citations are to non-elite journals
and how has this changed over time.
  We studied citations to articles published in 1995-2013. We computed the 10
most-cited journals and the 1000 most-cited articles each year for all 261
subject categories in Scholar Metrics. We marked the 10 most-cited journals in
a category as the elite journals for the category and the rest as non-elite.
  There are two conclusions from our study. First, the fraction of top-cited
articles published in non-elite journals increased steadily over 1995-2013.
While the elite journals still publish a substantial fraction of high-impact
articles, many more authors of well-regarded papers in diverse research fields
are choosing other venues.
  The number of top-1000 papers published in non-elite journals for the
representative subject category went from 149 in 1995 to 245 in 2013, a growth
of 64%. Looking at broad research areas, 4 out of 9 areas saw at least
one-third of the top-cited articles published in non-elite journals in 2013.
For 6 out of 9 areas, the fraction of top-cited papers published in non-elite
journals for the representative subject category grew by 45% or more.
  Second, now that finding and reading relevant articles in non-elite journals
is about as easy as finding and reading articles in elite journals, researchers
are increasingly building on and citing work published everywhere. Considering
citations to all articles, the percentage of citations to articles in non-elite
journals went from 27% in 1995 to 47% in 2013. Six out of nine broad areas had
at least 50% of citations going to articles published in non-elite journals in
2013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2231</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2231</id><created>2014-10-08</created><authors><author><keyname>Ballinger</keyname><forenames>Brad</forenames></author><author><keyname>Damian</keyname><forenames>Mirela</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Flatland</keyname><forenames>Robin</forenames></author><author><keyname>Ginepro</keyname><forenames>Jessica</forenames></author><author><keyname>Hull</keyname><forenames>Thomas</forenames></author></authors><title>Minimum Forcing Sets for Miura Folding Patterns</title><categories>cs.DS cs.DM math.CO</categories><comments>20 pages, 16 figures. To appear at the ACM/SIAM Symp. on Discrete
  Algorithms (SODA 2015)</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the study of forcing sets in mathematical origami. The origami
material folds flat along straight line segments called creases, each of which
is assigned a folding direction of mountain or valley. A subset $F$ of creases
is forcing if the global folding mountain/valley assignment can be deduced from
its restriction to $F$. In this paper we focus on one particular class of
foldable patterns called Miura-ori, which divide the plane into congruent
parallelograms using horizontal lines and zig-zag vertical lines. We develop
efficient algorithms for constructing a minimum forcing set of a Miura-ori map,
and for deciding whether a given set of creases is forcing or not. We also
provide tight bounds on the size of a forcing set, establishing that the
standard mountain-valley assignment for the Miura-ori is the one that requires
the most creases in its forcing sets. Additionally, given a partial
mountain/valley assignment to a subset of creases of a Miura-ori map, we
determine whether the assignment domain can be extended to a locally
flat-foldable pattern on all the creases. At the heart of our results is a
novel correspondence between flat-foldable Miura-ori maps and $3$-colorings of
grid graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2259</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2259</id><created>2014-09-14</created><authors><author><keyname>Prantl</keyname><forenames>Martin</forenames></author></authors><title>Image compression overview</title><categories>cs.GR cs.MM</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Compression plays a significant role in a data storage and a transmission. If
we speak about a generall data compression, it has to be a lossless one. It
means, we are able to recover the original data 1:1 from the compressed file.
Multimedia data (images, video, sound...), are a special case. In this area, we
can use something called a lossy compression. Our main goal is not to recover
data 1:1, but only keep them visually similar. This article is about an image
compression, so we will be interested only in image compression. For a human
eye, it is not a huge difference, if we recover RGB color with values
[150,140,138] instead of original [151,140,137]. The magnitude of a difference
determines the loss rate of the compression. The bigger difference usually
means a smaller file, but also worse image quality and noticable differences
from the original image. We want to cover compression techniques mainly from
the last decade. Many of them are variations of existing ones, only some of
them uses new principes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2265</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2265</id><created>2014-10-08</created><authors><author><keyname>Kaushik</keyname><forenames>Chetan</forenames></author><author><keyname>Mishra</keyname><forenames>Atul</forenames></author></authors><title>A Scalable, Lexicon Based Technique for Sentiment Analysis</title><categories>cs.IR cs.CL</categories><comments>9 pages 1 figure 2 tables</comments><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol.4, No.5, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid increase in the volume of sentiment rich social media on the web has
resulted in an increased interest among researchers regarding Sentimental
Analysis and opinion mining. However, with so much social media available on
the web, sentiment analysis is now considered as a big data task. Hence the
conventional sentiment analysis approaches fails to efficiently handle the vast
amount of sentiment data available now a days. The main focus of the research
was to find such a technique that can efficiently perform sentiment analysis on
big data sets. A technique that can categorize the text as positive, negative
and neutral in a fast and accurate manner. In the research, sentiment analysis
was performed on a large data set of tweets using Hadoop and the performance of
the technique was measured in form of speed and accuracy. The experimental
results shows that the technique exhibits very good efficiency in handling big
sentiment data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2266</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2266</id><created>2014-10-08</created><authors><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author><author><keyname>Nikishkin</keyname><forenames>Vladimir</forenames></author></authors><title>Testing Identity of Structured Distributions</title><categories>cs.DS cs.IT math.IT math.ST stat.TH</categories><comments>21 pages, to appear in SODA'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the question of identity testing for structured distributions. More
precisely, given samples from a {\em structured} distribution $q$ over $[n]$
and an explicit distribution $p$ over $[n]$, we wish to distinguish whether
$q=p$ versus $q$ is at least $\epsilon$-far from $p$, in $L_1$ distance. In
this work, we present a unified approach that yields new, simple testers, with
sample complexity that is information-theoretically optimal, for broad classes
of structured distributions, including $t$-flat distributions, $t$-modal
distributions, log-concave distributions, monotone hazard rate (MHR)
distributions, and mixtures thereof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2272</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2272</id><created>2014-10-08</created><authors><author><keyname>Clearwater</keyname><forenames>Adam</forenames></author><author><keyname>Puppe</keyname><forenames>Clemens</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>The single-crossing property on a tree</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the classical single-crossing property to single-crossing
property on trees and obtain new ways to construct Condorcet domains which are
sets of linear orders which possess the property that every profile composed
from those orders have transitive majority relation. We prove that for any tree
there exist profiles that are single-crossing on that tree; moreover, that tree
is minimal in this respect for at least one such profile. Finally, we provide a
polynomial-time algorithm to recognize whether or not a given profile is
single-crossing with respect to some tree. We also show that finding winners
for Chamberlin-Courant rule is polynomial for profiles that are single-crossing
on trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2291</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2291</id><created>2014-10-08</created><authors><author><keyname>Dupraz</keyname><forenames>Elsa</forenames></author><author><keyname>Declercq</keyname><forenames>David</forenames></author><author><keyname>Vasic</keyname><forenames>Bane</forenames></author><author><keyname>Savin</keyname><forenames>Valentin</forenames></author></authors><title>Analysis and Design of Finite Alphabet Iterative Decoders Robust to
  Faulty Hardware</title><categories>cs.IT math.IT</categories><comments>30 pages, submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of designing LDPC decoders robust to
transient errors introduced by a faulty hardware. We assume that the faulty
hardware introduces errors during the message passing updates and we propose a
general framework for the definition of the message update faulty functions.
Within this framework, we define symmetry conditions for the faulty functions,
and derive two simple error models used in the analysis. With this analysis, we
propose a new interpretation of the functional Density Evolution threshold
previously introduced, and show its limitations in case of highly unreliable
hardware. However, we show that under restricted decoder noise conditions, the
functional threshold can be used to predict the convergence behavior of FAIDs
under faulty hardware. In particular, we reveal the existence of robust and
non-robust FAIDs and propose a framework for the design of robust decoders. We
finally illustrate robust and non-robust decoders behaviors of finite length
codes using Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2295</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2295</id><created>2014-10-08</created><authors><author><keyname>Maftuleac</keyname><forenames>Daniela</forenames></author><author><keyname>Lee</keyname><forenames>Seoung Kyou</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Akash</keyname><forenames>Aditya Kumar</forenames></author><author><keyname>Lopez-Ortiz</keyname><forenames>Alejandro</forenames></author><author><keyname>McLurkin</keyname><forenames>James</forenames></author></authors><title>Local Policies for Efficiently Patrolling a Triangulated Region by a
  Robot Swarm</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and analyze methods for patrolling an environment with a
distributed swarm of robots. Our approach uses a physical data structure - a
distributed triangulation of the workspace. A large number of stationary
&quot;mapping&quot; robots cover and triangulate the environment and a smaller number of
mobile &quot;patrolling&quot; robots move amongst them. The focus of this work is to
develop, analyze, implement and compare local patrolling policies. We desire
strategies that achieve full coverage, but also produce good coverage frequency
and visitation times. Policies that provide theoretical guarantees for these
quantities have received some attention, but gaps have remained. We present: 1)
A summary of how to achieve coverage by building a triangulation of the
workspace, and the ensuing properties. 2) A description of simple local
policies (LRV, for Least Recently Visited and LFV, for Least Frequently
Visited) for achieving coverage by the patrolling robots. 3) New analytical
arguments why different versions of LRV may require worst case exponential time
between visits of triangles. 4) Analytical evidence that a local implementation
of LFV on the edges of the dual graph is possible in our scenario, and
immensely better in the worst case. 5) Experimental and simulation validation
for the practical usefulness of these policies, showing that even a small
number of weak robots with weak local information can greatly outperform a
single, powerful robots with full information and computational capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2320</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2320</id><created>2014-10-08</created><authors><author><keyname>Chambers</keyname><forenames>Erin Wolf</forenames></author><author><keyname>Vejdemo-Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Computing minimum area homologies</title><categories>cs.CG cs.GR math.AT</categories><comments>To appear in Computer Graphics Forum</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculating and categorizing the similarity of curves is a fundamental
problem which has generated much recent interest. However, to date there are no
implementations of these algorithms for curves on surfaces with provable
guarantees on the quality of the measure. In this paper, we present a
similarity measure for any two cycles that are homologous, where we calculate
the minimum area of any homology (or connected bounding chain) between the two
cycles. The minimum area homology exists for broader classes of cycles than
previous measures which are based on homotopy. It is also much easier to
compute than previously defined measures, yielding an efficient implementation
that is based on linear algebra tools. We demonstrate our algorithm on a range
of inputs, showing examples which highlight the feasibility of this similarity
measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2324</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2324</id><created>2014-10-08</created><authors><author><keyname>Sun</keyname><forenames>Jian</forenames></author><author><keyname>Zhong</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Zhou</keyname><forenames>Xuan</forenames></author><author><keyname>Fu</keyname><forenames>Xiaolong</forenames></author></authors><title>Recommendation Scheme Based on Converging Properties for Contents
  Broadcasting</title><categories>cs.MM cs.IR</categories><comments>6 pages. This work is present at 2015 International Workshop on
  Networking Issues in Multimedia Entertainment (NIME'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popular videos are often clicked by a mount of users in a short period. With
content recommendation, the popular contents could be broadcast to the
potential users in wireless network, to save huge transmitting resource. In
this paper, the contents propagation model is analyzed due to users' historical
behavior, location, and the converging properties in wireless data
transmission, with the users' communication log in the Chinese commercial
cellular network. And a recommendation scheme is proposed to achieve high
energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2326</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2326</id><created>2014-10-08</created><authors><author><keyname>Etezadi</keyname><forenames>Farrokh</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author><author><keyname>Trott</keyname><forenames>Mitchell</forenames></author></authors><title>Zero-Delay Sequential Transmission of Markov Sources Over Burst Erasure
  Channels</title><categories>cs.IT math.IT</categories><comments>31 pages, 17 figures</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 8, pp.
  4584--4613, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A setup involving zero-delay sequential transmission of a vector Markov
source over a burst erasure channel is studied. A sequence of source vectors is
compressed in a causal fashion at the encoder, and the resulting output is
transmitted over a burst erasure channel. The destination is required to
reconstruct each source vector with zero-delay, but those source sequences that
are observed either during the burst erasure, or in the interval of length $W$
following the burst erasure need not be reconstructed. The minimum achievable
compression rate is called the rate-recovery function. We assume that each
source vector is sampled i.i.d. across the spatial dimension and from a
stationary, first-order Markov process across the temporal dimension.
  For discrete sources the case of lossless recovery is considered, and upper
and lower bounds on the rate-recovery function are established. Both these
bounds can be expressed as the rate for predictive coding, plus a term that
decreases at least inversely with the recovery window length $W$. For
Gauss-Markov sources and a quadratic distortion measure, upper and lower bounds
on the minimum rate are established when $W=0$. These bounds are shown to
coincide in the high resolution limit. Finally another setup involving i.i.d.
Gaussian sources is studied and the rate-recovery function is completely
characterized in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2346</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2346</id><created>2014-10-08</created><updated>2015-06-10</updated><authors><author><keyname>Kumar</keyname><forenames>M. Ashok</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Minimization Problems Based on Relative $\alpha$-Entropy I: Forward
  Projection</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>24 pages; 4 figures; minor change in title; revised version. Accepted
  for publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimization problems with respect to a one-parameter family of generalized
relative entropies are studied. These relative entropies, which we term
relative $\alpha$-entropies (denoted $\mathscr{I}_{\alpha}$), arise as
redundancies under mismatched compression when cumulants of compressed lengths
are considered instead of expected compressed lengths. These parametric
relative entropies are a generalization of the usual relative entropy
(Kullback-Leibler divergence). Just like relative entropy, these relative
$\alpha$-entropies behave like squared Euclidean distance and satisfy the
Pythagorean property. Minimizers of these relative $\alpha$-entropies on closed
and convex sets are shown to exist. Such minimizations generalize the maximum
R\'{e}nyi or Tsallis entropy principle. The minimizing probability distribution
(termed forward $\mathscr{I}_{\alpha}$-projection) for a linear family is shown
to obey a power-law. Other results in connection with statistical inference,
namely subspace transitivity and iterated projections, are also established. In
a companion paper, a related minimization problem of interest in robust
statistics that leads to a reverse $\mathscr{I}_{\alpha}$-projection is
studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2360</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2360</id><created>2014-10-09</created><authors><author><keyname>Kaniezhil</keyname><forenames>R.</forenames></author><author><keyname>Chandrasekar</keyname><forenames>C.</forenames></author></authors><title>Improvement of Spectrum Sharing using Traffic pattern prediction</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1301.1134; and text overlap
  with arXiv:1201.1964, arXiv:cs/0609149 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on improving the spectrum sharing using NSU, FLS and
Traffic Pattern Prediction and also made comparison that traffic pattern
prediction provides a better way of improving the spectrum utilization and
avoids the spectrum scarcity. This helps to increase the number of active
users, ease of identification of optimal users to use the spectrum with
maximized coverage of the spectrum.. We experimentally evaluated the
effectiveness of our approach using NS2 simulator and showed that after
predicting the traffic, we can accommodate more number of users and avoiding
Interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2370</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2370</id><created>2014-10-09</created><updated>2015-01-07</updated><authors><author><keyname>Gupta</keyname><forenames>Shubham</forenames></author></authors><title>Synthesis of Sequential Reversible Circuits through Finite State Machine</title><categories>cs.ET</categories><comments>We request you to kindly withdraw this article from arXiv.org. This
  article has some crucial errors which can not be corrected. It means that it
  is not useful for public domain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible computing has attracted the attention of researchers due to its
low power consumption and less heat dissipation compared to conventional
computing. A number of reversible gates have been proposed by different
researchers and various combinational circuits based on reversible gates have
been developed. However the realization of sequential circuit in reversible
logic is still at premature stage. Sequential circuits were not available
because of feedback was not allowed in reversible circuit. However allowing
feedback in space (not in time), some sequential reversible gates and circuits
have been reported in the literature. In this dissertation, we have addressed
the problem from two sides. One side is to propose a low cost reversible gate
suitable for sequential building block i.e. T flip-flop and hence designing low
cost synchronous and asynchronous counters. Another side is to generate the
circuit from its behavioral description described in FSM form. Our propose
designs of reversible counters are significantly better in optimization
parameters such as gate counts, garbage outputs and constant inputs available
in literature. We have also proposed a procedure for obtaining reversible
circuit from behavioral description through FSM. A very few attempts have been
reported in the literature for the conversion FSM to reversible FSM.Because of
non-availability of generated sequential reversible circuit in literature, our
results cannot be compared with any other circuits. We expect that the
sequential reversible circuits will help in debugging the reversible circuits,
handling the ambiguous state of an FSM and generating the original input in
reverse direction by reversing the original output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2371</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2371</id><created>2014-10-09</created><authors><author><keyname>van Iersel</keyname><forenames>Leo</forenames></author><author><keyname>Kelk</keyname><forenames>Steven</forenames></author><author><keyname>Lekic</keyname><forenames>Nela</forenames></author><author><keyname>Linz</keyname><forenames>Simone</forenames></author></authors><title>Satisfying ternary permutation constraints by multiple linear orders or
  phylogenetic trees</title><categories>cs.CC q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A ternary permutation constraint satisfaction problem (CSP) is specified by a
subset Pi of the symmetric group S_3. An instance of such a problem consists of
a set of variables V and a set of constraints C, where each constraint is an
ordered triple of distinct elements from V. The goal is to construct a linear
order alpha on V such that, for each constraint (a,b,c) in C, the ordering of
a,b,c induced by alpha is in Pi. Excluding symmetries and trivial cases there
are 11 such problems, and their complexity is well known. Here we consider the
variant of the problem, denoted 2-Pi, where we are allowed to construct two
linear orders alpha and beta and each constraint needs to be satisfied by at
least one of the two. We give a full complexity classification of all 11 2-Pi
problems, observing that in the switch from one to two linear orders the
complexity landscape changes quite abruptly and that hardness proofs become
rather intricate. We then focus on one of the 11 problems in particular, which
is closely related to the '2-Caterpillar Compatibility' problem in the
phylogenetics literature. We show that this particular CSP remains hard on
three linear orders, and also in the biologically relevant case when we swap
three linear orders for three phylogenetic trees, yielding the '3-Tree
Compatibility' problem. Due to the biological relevance of this problem we also
give extremal results concerning the minimum number of trees required, in the
worst case, to satisfy a set of rooted triplet constraints on n leaf labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2373</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2373</id><created>2014-10-09</created><updated>2015-01-14</updated><authors><author><keyname>Pareek</keyname><forenames>Vishal</forenames></author></authors><title>A New Gate for Optimal Fault Tolerant &amp; Testable Reversible Sequential
  Circuit Design</title><categories>cs.ET</categories><comments>This paper has been withdrawn by the author. This article has some
  crucial errors which can not be corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With phenomenal growth of high speed and complex computing applications, the
design of low power and high speed logic circuits have created tremendous
interest. Conventional computing devices are based on irreversible logic and
further reduction in power consumption and/or increase in speed appears
non-promising. Reversible computing has emerged as a solution looking to the
power and speed requirements of future computing devices. In reversible
computing logic gates used are such that input can be generated by reversing
the operation from output. A number of reversible combinational circuits have
been developed but the growth of sequential circuits was not significant due to
feedback and fanout was not allowed. However, allowing feedback in space, a
very few sequential logic blocks i.e. flip-flops have been reported in
literature. In order to develop sequential circuits, flip-flops are used in
conventional circuits. Also good circuit design methods, optimized and fault
tolerant designs are also needed to build large, complex and reliable circuits
in conventional computing. Reversible flip-flops are the basic memory elements
that will be the building block of memory for reversible computing and quantum
computing devices. In this dissertation we plan to address above issues. First
we have proposed a Pareek gate suitable for low-cost flip-flops design and then
design methodology to develop flip-flops are illustrated. Further almost all
flip-flops and some example circuit have been developed and finally these
circuits have been converted into fault tolerant circuits by preserving their
parity and designs of offline as well as online testable circuits have been
proposed. In this dissertation work, we have also compared quantum cost as well
as other parameters with existing circuits and shown a significant improvement
in almost all parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2379</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2379</id><created>2014-10-09</created><updated>2014-10-09</updated><authors><author><keyname>Davies</keyname><forenames>Todd</forenames></author></authors><title>Digital Rights and Freedoms: A Framework for Surveying Users and
  Analyzing Policies</title><categories>cs.CY</categories><comments>6th International Conference on Social Informatics (SocInfo 2014),
  Barcelona, 10-13 November 2014, 16 pages, 3 tables</comments><acm-class>H.1.2; K.4.1; K.5.0; K.5.1; K.5.2</acm-class><doi>10.1007/978-3-319-13734-6_31</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Interest has been revived in the creation of a &quot;bill of rights&quot; for Internet
users. This paper analyzes users' rights into ten broad principles, as a basis
for assessing what users regard as important and for comparing different
multi-issue Internet policy proposals. Stability of the principles is
demonstrated in an experimental survey, which also shows that freedoms of users
to participate in the design and coding of platforms appear to be viewed as
inessential relative to other rights. An analysis of users' rights frameworks
that have emerged over the past twenty years similarly shows that such
proposals tend to leave out freedoms related to software platforms, as opposed
to user data or public networks. Evaluating policy frameworks in a comparative
analysis based on prior principles may help people to see what is missing and
what is important as the future of the Internet continues to be debated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2381</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2381</id><created>2014-10-09</created><authors><author><keyname>Farouk</keyname><forenames>R. M.</forenames></author><author><keyname>Badr</keyname><forenames>S.</forenames></author><author><keyname>Elahl</keyname><forenames>M. Sayed</forenames></author></authors><title>Recognition of cDNA microarray image Using Feedforward artificial neural
  network</title><categories>cs.CV cs.NE</categories><comments>17 pages, 7 figures and 23 References</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 5, No. 5, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complementary DNA (cDNA) sequence is considered to be the magic biometric
technique for personal identification. In this paper, we present a new method
for cDNA recognition based on the artificial neural network (ANN). Microarray
imaging is used for the concurrent identification of thousands of genes. We
have segmented the location of the spots in a cDNA microarray. Thus, a precise
localization and segmenting of a spot are essential to obtain a more accurate
intensity measurement, leading to a more precise expression measurement of a
gene. The segmented cDNA microarray image is resized and it is used as an input
for the proposed artificial neural network. For matching and recognition, we
have trained the artificial neural network. Recognition results are given for
the galleries of cDNA sequences . The numerical results show that, the proposed
matching technique is an effective in the cDNA sequences process. We also
compare our results with previous results and find out that, the proposed
technique is an effective matching performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2386</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2386</id><created>2014-10-09</created><updated>2015-04-16</updated><authors><author><keyname>Zhao</keyname><forenames>Qibin</forenames></author><author><keyname>Zhou</keyname><forenames>Guoxu</forenames></author><author><keyname>Zhang</keyname><forenames>Liqing</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author><author><keyname>Amari</keyname><forenames>Shun-ichi</forenames></author></authors><title>Bayesian Robust Tensor Factorization for Incomplete Multiway Data</title><categories>cs.CV cs.LG</categories><comments>in IEEE Transactions on Neural Networks and Learning Systems, 2015</comments><doi>10.1109/TNNLS.2015.2423694</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generative model for robust tensor factorization in the presence
of both missing data and outliers. The objective is to explicitly infer the
underlying low-CP-rank tensor capturing the global information and a sparse
tensor capturing the local information (also considered as outliers), thus
providing the robust predictive distribution over missing entries. The
low-CP-rank tensor is modeled by multilinear interactions between multiple
latent factors on which the column sparsity is enforced by a hierarchical
prior, while the sparse tensor is modeled by a hierarchical view of Student-$t$
distribution that associates an individual hyperparameter with each element
independently. For model learning, we develop an efficient closed-form
variational inference under a fully Bayesian treatment, which can effectively
prevent the overfitting problem and scales linearly with data size. In contrast
to existing related works, our method can perform model selection automatically
and implicitly without need of tuning parameters. More specifically, it can
discover the groundtruth of CP rank and automatically adapt the sparsity
inducing priors to various types of outliers. In addition, the tradeoff between
the low-rank approximation and the sparse representation can be optimized in
the sense of maximum model evidence. The extensive experiments and comparisons
with many state-of-the-art algorithms on both synthetic and real-world datasets
demonstrate the superiorities of our method from several perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2388</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2388</id><created>2014-10-09</created><authors><author><keyname>Sardroud</keyname><forenames>Asghar Asgharian</forenames></author><author><keyname>Bagheri</keyname><forenames>Alireza</forenames></author></authors><title>Unit-length embedding of cycles and paths on grid graphs</title><categories>cs.DM math.CO</categories><msc-class>68R10, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although there are very algorithms for embedding graphs on unbounded grids,
only few results on embedding or drawing graphs on restricted grids has been
published. In this work, we consider the problem of embedding paths and cycles
on grid graphs. We give the necessary and sufficient conditions for the
existence of cycles of given length $k$ and paths of given length $k$ between
two given vertices in $n$-vertex rectangular grid graphs and introduce two
algorithms with running times O$(k)$ and O$(k^2)$ for finding respectively such
cycles and paths. Also, we extend our results to $m\times n\times o$ 3D grids.
Our method for finding cycle of length $k$ in rectangular grid graphs also
introduces a linear-time algorithm for finding cycles of a given length $k$ in
hamiltonian solid grid graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2390</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2390</id><created>2014-10-09</created><updated>2014-10-13</updated><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Asymptotic Expansions for Gaussian Channels with Feedback under a Peak
  Power Constraint</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the asymptotic expansion for the size of block codes
defined for the additive white Gaussian noise (AWGN) channel with feedback
under the following setting: A peak power constraint is imposed on every
transmitted codeword, and the average error probability of decoding the
transmitted message is non-vanishing as the blocklength increases. It is
well-known that the presence of feedback does not increase the first-order
asymptotics (i.e., capacity) in the asymptotic expansion for the AWGN channel.
The main contribution of this paper is a self-contained proof of an upper bound
on the asymptotic expansion for the AWGN channel with feedback. Combined with
existing achievability results for the AWGN channel, our result implies that
the presence of feedback does not improve the second- and third-order
asymptotics. An auxiliary contribution is a proof of the strong converse for
the parallel Gaussian channels with feedback under a peak power constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2405</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2405</id><created>2014-10-09</created><updated>2015-10-12</updated><authors><author><keyname>Cameron</keyname><forenames>Peter J.</forenames></author><author><keyname>Dang</keyname><forenames>Anh N.</forenames></author><author><keyname>Riis</keyname><forenames>Soren</forenames></author></authors><title>Guessing Games on Triangle-free Graphs</title><categories>math.CO cs.IT math.IT</categories><comments>9 pages, submitted to Electronic Journal of Combinatoric</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The guessing game introduced by Riis is a variant of the &quot;guessing your own
hats&quot; game and can be played on any simple directed graph G on n vertices. For
each digraph G, it is proved that there exists a unique guessing number gn(G)
associated to the guessing game played on G. When we consider the directed edge
to be bidirected, in other words, the graph G is undirected, Christofides and
Markstrom introduced a method to bound the value of the guessing number from
below using the fractional clique number Kf(G). In particular they showed gn(G)
&gt;= |V(G)| - Kf(G). Moreover, it is pointed out that equality holds in this
bound if the underlying undirected graph G falls into one of the following
categories: perfect graphs, cycle graphs or their complement. In this paper, we
show that there are triangle-free graphs that have guessing numbers which do
not meet the fractional clique cover bound. In particular, the famous
triangle-free Higman-Sims graph has guessing number at least 77 and at most 78,
while the bound given by fractional clique cover is 50.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2415</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2415</id><created>2014-10-09</created><authors><author><keyname>Ignjatovi&#x107;</keyname><forenames>Jelena</forenames></author><author><keyname>&#x106;iri&#x107;</keyname><forenames>Miroslav</forenames></author><author><keyname>Jan&#x10d;i&#x107;</keyname><forenames>Zorana</forenames></author></authors><title>Weighted finite automata with output</title><categories>cs.FL</categories><comments>Preprint submitted to Soft Computing</comments><msc-class>68Q45, 68Q70, 68T37, 03E72</msc-class><acm-class>F.1.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove the equivalence of sequential, Mealy-type and
Moore-type weighted finite automata with output, with respect to various
semantics which are defined here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2418</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2418</id><created>2014-10-09</created><authors><author><keyname>Xu</keyname><forenames>Weiqiang</forenames></author><author><keyname>Tu</keyname><forenames>Jianchen</forenames></author><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author></authors><title>Cross-Layer Control for Worse Case Delay Guarantees in Heterogeneous
  Powered Wireless Sensor Network via Lyapunov Optimization</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The delay guarantee is a challenge in wireless sensor networks (WSNs), where
energy constraints must be considered. The coexistence of renewable energy and
electricity grid is expected as a promising energy supply manner for WSNs to
remain function for a potentially infinite lifetime. In this paper, we address
cross-layer control to guarantee worse case delay for Heterogeneous Powered
(HP) WSNs. We design a novel virtual delay queue structure, and apply the
Lyapunov optimization technique to develop cross-layer control algorithm only
requiring knowledge of the instantaneous system state, which provides efficient
throughput-utility, and guarantees bounded worst-case delay. We analyze the
performance of the proposed algorithm and verify the theoretic claims through
the simulation results. Compared to the existing work, the algorithm presented
in this paper achieves much higher optimal objective value with ultralow data
drop due to the proposed novel virtual queue structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2419</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2419</id><created>2014-10-09</created><authors><author><keyname>Elmahdy</keyname><forenames>Adel M.</forenames></author><author><keyname>El-Keyi</keyname><forenames>Amr</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author><author><keyname>Seddik</keyname><forenames>Karim G.</forenames></author></authors><title>On the Stable Throughput of Cooperative Cognitive Radio Networks with
  Finite Relaying Buffer</title><categories>cs.NI</categories><comments>5 pages, IEEE PIMRC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of cooperative communications in
cognitive radio systems where the secondary user has limited relaying room for
the overheard primary packets. More specifically, we characterize the stable
throughput region of a cognitive radio network with a finite relaying buffer at
the secondary user. Towards this objective, we formulate a constrained
optimization problem for maximizing the secondary user throughput while
guaranteeing the stability of the primary user queue. We consider a general
cooperation policy where the packet admission and queue selection
probabilities, at the secondary user, are both dependent on the state (length)
of the finite relaying buffer. Despite the sheer complexity of the optimization
problem, attributed to its non-convexity, we transform it to a linear program.
Our numerical results reveal a number of valuable insights, e.g., it is always
mutually beneficial to cooperate in delivering the primary packets in terms of
expanding the stable throughput region. In addition, the stable throughput
region of the system, compared to the case of infinite relaying queue capacity,
marginally shrinks for limited relaying queue capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2430</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2430</id><created>2014-10-09</created><authors><author><keyname>Deleforge</keyname><forenames>Antoine</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>Phase-Optimized K-SVD for Signal Extraction from Underdetermined
  Multichannel Sparse Mixtures</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel sparse representation for heavily underdetermined
multichannel sound mixtures, i.e., with much more sources than microphones. The
proposed approach operates in the complex Fourier domain, thus preserving
spatial characteristics carried by phase differences. We derive a
generalization of K-SVD which jointly estimates a dictionary capturing both
spectral and spatial features, a sparse activation matrix, and all
instantaneous source phases from a set of signal examples. The dictionary can
then be used to extract the learned signal from a new input mixture. The method
is applied to the challenging problem of ego-noise reduction for robot
audition. We demonstrate its superiority relative to conventional
dictionary-based techniques using recordings made in a real room.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2435</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2435</id><created>2014-10-09</created><authors><author><keyname>Liang</keyname><forenames>Min</forenames></author></authors><title>Quantum fully homomorphic encryption scheme based on universal quantum
  circuit</title><categories>quant-ph cs.CR</categories><comments>10 pages, 1 figure</comments><journal-ref>Quantum Information Processing, Volume 14, Issue 8 (2015), Page
  2749-2759</journal-ref><doi>10.1007/s11128-015-1034-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fully homomorphic encryption enables arbitrary computation on encrypted data
without decrypting the data. Here it is studied in the context of quantum
information processing. Based on universal quantum circuit, we present a
quantum fully homomorphic encryption (QFHE) scheme, which permits arbitrary
quantum transformation on an encrypted data. The QFHE scheme is proved to be
perfectly secure. In the scheme, the decryption key is different from the
encryption key, however, the encryption key cannot be public. Moreover, the
evaluate algorithm of the scheme is independent of the encryption key, so it is
very applicable in delegated quantum computing between two parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2437</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2437</id><created>2014-10-09</created><authors><author><keyname>Lazaridis</keyname><forenames>Lazaros</forenames></author><author><keyname>Papatsimouli</keyname><forenames>Maria</forenames></author><author><keyname>Fragulis</keyname><forenames>George F.</forenames></author></authors><title>S.A.T.E.P. : Synchronous-Asynchronous Tele-education Platform</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  S.A.T.E.P. means Synchronous Asynchronous Tele education Platform is a
software application for educational purposes, with a lot of parametrizing
features written entirely from scratch. It aims at the training and examination
of computer skills, a platform that can be adjusted to the needs of each
lesson. In the application the trainer and the administrator can define the
number of the lectures and upload files for each one of them. Furthermore, he
can insert, modify and delete questions which are used for evaluation tests but
also for the trainees examinations. The trainee can read and download the files
of each lesson and also test his knowledge on what he has studied through a
series of questions. A chat module where registered users as well as system
administrator can discuss and solve questions is also developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2442</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2442</id><created>2014-10-09</created><updated>2014-10-17</updated><authors><author><keyname>Schockaert</keyname><forenames>Steven</forenames></author><author><keyname>Li</keyname><forenames>Sanjiang</forenames></author></authors><title>Realizing RCC8 networks using convex regions</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RCC8 is a popular fragment of the region connection calculus, in which
qualitative spatial relations between regions, such as adjacency, overlap and
parthood, can be expressed. While RCC8 is essentially dimensionless, most
current applications are confined to reasoning about two-dimensional or
three-dimensional physical space. In this paper, however, we are mainly
interested in conceptual spaces, which typically are high-dimensional Euclidean
spaces in which the meaning of natural language concepts can be represented
using convex regions. The aim of this paper is to analyze how the restriction
to convex regions constrains the realizability of networks of RCC8 relations.
First, we identify all ways in which the set of RCC8 base relations can be
restricted to guarantee that consistent networks can be convexly realized in
respectively 1D, 2D, 3D, and 4D. Most surprisingly, we find that if the
relation 'partially overlaps' is disallowed, all consistent atomic RCC8
networks can be convexly realized in 4D. If instead refinements of the relation
'part of' are disallowed, all consistent atomic RCC8 relations can be convexly
realized in 3D. We furthermore show, among others, that any consistent RCC8
network with 2n+1 variables can be realized using convex regions in the
n-dimensional Euclidean space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2450</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2450</id><created>2014-10-09</created><authors><author><keyname>Boukenadil</keyname><forenames>Bahidja</forenames></author></authors><title>Importance of realistic mobility models for vanet network simulation</title><categories>cs.NI</categories><comments>8 pages, IJCNC 2014</comments><doi>10.5121/ijcnc.2014.6513</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the performance evaluation of a protocol for a vehicular ad hoc network,
the protocol should be tested under a realistic conditions including,
representative data traffic models, and realistic movements of the mobile nodes
which are the vehicles (i.e., a mobility model). This work is a comparative
study between two mobility models that are used in the simulations of vehicular
networks, i.e., MOVE (MObility model generator for VEhicular networks) and
CityMob, a mobility pattern generator for VANET. We describe several mobility
models for VANET simulations. In this paper we aim to show that the mobility
models can significantly affect the simulation results in VANET networks. The
results presented in this article prove the importance of choosing a suitable
real world scenario for performances studies of routing protocols in this kind
of network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2455</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2455</id><created>2014-10-09</created><updated>2016-02-04</updated><authors><author><keyname>Gouws</keyname><forenames>Stephan</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Corrado</keyname><forenames>Greg</forenames></author></authors><title>BilBOWA: Fast Bilingual Distributed Representations without Word
  Alignments</title><categories>stat.ML cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple
and computationally-efficient model for learning bilingual distributed
representations of words which can scale to large monolingual datasets and does
not require word-aligned parallel training data. Instead it trains directly on
monolingual data and extracts a bilingual signal from a smaller set of raw-text
sentence-aligned data. This is achieved using a novel sampled bag-of-words
cross-lingual objective, which is used to regularize two noise-contrastive
language models for efficient cross-lingual feature learning. We show that
bilingual embeddings learned using the proposed model outperform
state-of-the-art methods on a cross-lingual document classification task as
well as a lexical translation task on WMT11 data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2456</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2456</id><created>2014-10-09</created><authors><author><keyname>Podolskaya</keyname><forenames>Olga</forenames></author></authors><title>On Circuit Complexity of Parity and Majority Functions in Antichain
  Basis</title><categories>cs.CC</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the circuit complexity of boolean functions in a certain infinite
basis. The basis consists of all functions that take value $1$ on antichains
over the boolean cube. We prove that the circuit complexity of the parity
function and the majority function of $n$ variables in this basis is $\lfloor
\frac{n+1}{2} \rfloor$ and $\left\lfloor \frac{n}{2} \right \rfloor +1$
respectively. We show that the asymptotic of the maximum complexity of
$n$-variable boolean functions in this basis equals $n.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2457</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2457</id><created>2014-10-08</created><updated>2014-10-21</updated><authors><author><keyname>Ali</keyname><forenames>Anum</forenames></author><author><keyname>Al-Rabah</keyname><forenames>Abdullatif</forenames></author><author><keyname>Masood</keyname><forenames>Mudassir</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>Receiver-based Recovery of Clipped OFDM Signals for PAPR Reduction: A
  Bayesian Approach</title><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clipping is one of the simplest peak-to-average power ratio (PAPR) reduction
schemes for orthogonal frequency division multiplexing (OFDM). Deliberately
clipping the transmission signal degrades system performance, and clipping
mitigation is required at the receiver for information restoration. In this
work, we acknowledge the sparse nature of the clipping signal and propose a
low-complexity Bayesian clipping estimation scheme. The proposed scheme
utilizes a priori information about the sparsity rate and noise variance for
enhanced recovery. At the same time, the proposed scheme is robust against
inaccurate estimates of the clipping signal statistics. The undistorted phase
property of the clipped signal, as well as the clipping likelihood, is utilized
for enhanced reconstruction. Further, motivated by the nature of modern
OFDM-based communication systems, we extend our clipping reconstruction
approach to multiple antenna receivers, and multi-user OFDM. We also address
the problem of channel estimation from pilots contaminated by the clipping
distortion. Numerical findings are presented, that depict favourable results
for the proposed scheme compared to the established sparse reconstruction
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2463</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2463</id><created>2014-10-09</created><updated>2014-10-16</updated><authors><author><keyname>Kurz</keyname><forenames>Alexander</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Pattinson</keyname><forenames>Dirk</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Lutz</forenames></author></authors><title>Simplified Coalgebraic Trace Equivalence</title><categories>cs.LO</categories><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of concurrent and reactive systems is based to a large degree on
various notions of process equivalence, ranging, on the so-called
linear-time/branching-time spectrum, from fine-grained equivalences such as
strong bisimilarity to coarse-grained ones such as trace equivalence. The
theory of concurrent systems at large has benefited from developments in
coalgebra, which has enabled uniform definitions and results that provide a
common umbrella for seemingly disparate system types including
non-deterministic, weighted, probabilistic, and game-based systems. In
particular, there has been some success in identifying a generic coalgebraic
theory of bisimulation that matches known definitions in many concrete cases.
The situation is currently somewhat less settled regarding trace equivalence. A
number of coalgebraic approaches to trace equivalence have been proposed, none
of which however cover all cases of interest; notably, all these approaches
depend on explicit termination, which is not always imposed in standard
systems, e.g. LTS. Here, we discuss a joint generalization of these approaches
based on embedding functors modelling various aspects of the system, such as
transition and braching, into a global monad; this approach appears to cover
all cases considered previously and some additional ones, notably standard LTS
and probabilistic labelled transition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2470</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2470</id><created>2014-10-09</created><updated>2015-07-16</updated><authors><author><keyname>Upadhyay</keyname><forenames>Jalaj</forenames></author></authors><title>Randomness Efficient Fast-Johnson-Lindenstrauss Transform with
  Applications in Differential Privacy and Compressed Sensing</title><categories>cs.DS cs.CR</categories><comments>Corrected a mistake in the proof and few small typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Johnson-Lindenstrauss property ({\sf JLP}) of random matrices has immense
application in computer science ranging from compressed sensing, learning
theory, numerical linear algebra, to privacy. This paper explores the
properties and applications of a distribution of random matrices. Our
distribution satisfies {\sf JLP} with desirable properties like fast
matrix-vector multiplication, sparsity, and optimal subspace embedding. We can
sample a random matrix from this distribution using exactly $2n+n \log n$
random bits. We show that a random matrix picked from this distribution
preserves differential privacy under the condition that the input private
matrix satisfies certain spectral property. This improves the run-time of
various differentially private mechanisms like Blocki {\it et al.} (FOCS 2012)
and Upadhyay (ASIACRYPT 13). Our final construction has a bounded column
sparsity. Therefore, this answers an open problem stated in Blocki {\it et al.}
(FOCS 2012). Using the results of Baranuik {\it et al.} (Constructive
Approximation: 28(3)), our result implies a randomness efficient matrices that
satisfies the Restricted-Isometry Property of optimal order for small sparsity
with exactly linear random bits.
  We also show that other known distributions of sparse random matrices with
the {\sf JLP} does not preserves differential privacy; thereby, answering one
of the open problem posed by Blocki {\it et al.} (FOCS 2012). Extending on the
works of Kane and Nelson (JACM: 61(1)), we also give unified analysis of some
of the known Johnson-Lindenstrauss transform. We also present a self-contained
simplified proof of an inequality on quadratic form of Gaussian variables that
we use in all our proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2474</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2474</id><created>2014-10-09</created><authors><author><keyname>Ghazouani</keyname><forenames>Haythem</forenames></author></authors><title>Genetic Stereo Matching Algorithm with Fuzzy Fitness</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a genetic stereo matching algorithm with fuzzy evaluation
function. The proposed algorithm presents a new encoding scheme in which a
chromosome is represented by a disparity matrix. Evolution is controlled by a
fuzzy fitness function able to deal with noise and uncertain camera
measurements, and uses classical evolutionary operators. The result of the
algorithm is accurate dense disparity maps obtained in a reasonable
computational time suitable for real-time applications as shown in experimental
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2476</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2476</id><created>2014-10-09</created><authors><author><keyname>Barnett</keyname><forenames>George L.</forenames></author><author><keyname>Funke</keyname><forenames>Simon W.</forenames></author><author><keyname>Piggott</keyname><forenames>Matthew D.</forenames></author></authors><title>Hybrid global-local optimisation algorithms for the layout design of
  tidal turbine arrays</title><categories>math.OC cs.CE</categories><comments>36 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tidal stream power generation represents a promising source of renewable
energy. In order to extract an economically useful amount of power, tens to
hundreds of tidal turbines need to be placed within an array. The layout of
these turbines can have a significant impact on the power extracted and hence
on the viability of the site. Funke et al. formulated the question of the best
turbine layout as an optimisation problem constrained by the shallow water
equations and solved it using a local, gradient-based optimisation algorithm.
Given the local nature of this approach, the question arises of how optimal the
layouts actually are. This becomes particularly important for scenarios with
complex bathymetry and layout constraints, both of which typically introduce
locally optimal layouts. Optimisation algorithms which find the global optima
generally require orders of magnitude more iterations than local optimisation
algorithms and are thus infeasible in combination with an expensive flow model.
This paper presents an analytical wake model to act as an efficient proxy to
the shallow water model. Based upon this, a hybrid global-local two-stage
optimisation approach is presented in which turbine layouts are first optimised
with the analytical wake model via a global optimisation algorithm, and further
optimised with the shallow water model via a local gradient-based optimisation
algorithm. This procedure is applied to a number of idealised cases and a more
realistic case with complex bathymetry in the Pentland Firth, Scotland. It is
shown that in cases where bathymetry is considered, the two-stage optimisation
procedure is able to improve the power extracted from the array by as much as
25% compared to local optimisation for idealised scenarios and by as much as
12% for the more realistic Pentland Firth scenario whilst in many cases
reducing the overall computation time by approximately 35%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2479</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2479</id><created>2014-10-09</created><updated>2015-02-16</updated><authors><author><keyname>Schwarz</keyname><forenames>Andreas</forenames></author><author><keyname>Huemmer</keyname><forenames>Christian</forenames></author><author><keyname>Maas</keyname><forenames>Roland</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy
  and Reverberant Environments</title><categories>cs.CL cs.NE cs.SD stat.ML</categories><comments>accepted for ICASSP2015</comments><doi>10.1109/ICASSP.2015.7178798</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a spatial diffuseness feature for deep neural network (DNN)-based
automatic speech recognition to improve recognition accuracy in reverberant and
noisy environments. The feature is computed in real-time from multiple
microphone signals without requiring knowledge or estimation of the direction
of arrival, and represents the relative amount of diffuse noise in each time
and frequency bin. It is shown that using the diffuseness feature as an
additional input to a DNN-based acoustic model leads to a reduced word error
rate for the REVERB challenge corpus, both compared to logmelspec features
extracted from noisy signals, and features enhanced by spectral subtraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2480</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2480</id><created>2014-10-09</created><updated>2015-05-27</updated><authors><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Goldfeld</keyname><forenames>Jonathan</forenames></author><author><keyname>Puzis</keyname><forenames>Rami</forenames></author></authors><title>Efficient On-line Detection of Temporal Patterns</title><categories>cs.DS</categories><comments>withdrawn due to submission policy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying a temporal pattern of events is a fundamental task of on-line
(real-time) verification. We present efficient schemes for on-line monitoring
of events for identifying desired/undesired patterns of events. The schemes use
preprocessing to ensure that the number of comparisons during run-time is
minimized. In particular, the first comparison following the time point when an
execution sub-sequence cannot be further extended to satisfy the temporal
requirements, halts the process that monitors the sub-sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2488</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2488</id><created>2014-09-29</created><authors><author><keyname>Spratt</keyname><forenames>Emily L.</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Computational Beauty: Aesthetic Judgment at the Intersection of Art and
  Science</title><categories>cs.CV physics.hist-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In part one of the Critique of Judgment, Immanuel Kant wrote that &quot;the
judgment of taste...is not a cognitive judgment, and so not logical, but is
aesthetic.&quot;\cite{Kant} While the condition of aesthetic discernment has long
been the subject of philosophical discourse, the role of the arbiters of that
judgment has more often been assumed than questioned. The art historian,
critic, connoisseur, and curator have long held the esteemed position of the
aesthetic judge, their training, instinct, and eye part of the inimitable
subjective processes that Kant described as occurring upon artistic evaluation.
Although the concept of intangible knowledge in regard to aesthetic theory has
been much explored, little discussion has arisen in response to the development
of new types of artificial intelligence as a challenge to the seemingly
ineffable abilities of the human observer. This paper examines the developments
in the field of computer vision analysis of paintings from canonical movements
with the history of Western art and the reaction of art historians to the
application of this technology in the field. Through an investigation of the
ethical consequences of this innovative technology, the unquestioned authority
of the art expert is challenged and the subjective nature of aesthetic judgment
is brought to philosophical scrutiny once again.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2500</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2500</id><created>2014-10-09</created><updated>2015-06-04</updated><authors><author><keyname>Bax</keyname><forenames>Eric</forenames></author><author><keyname>Weng</keyname><forenames>Lingjie</forenames></author><author><keyname>Tian</keyname><forenames>Xu</forenames></author></authors><title>Validation of k-Nearest Neighbor Classifiers Using Inclusion and
  Exclusion</title><categories>cs.LG cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a series of PAC error bounds for $k$-nearest neighbors
classifiers, with O($n^{-\frac{r}{2r+1}}$) expected range in the difference
between error bound and actual error rate, for each integer $r&gt;0$, where $n$ is
the number of in-sample examples. The best previous expected bound range was
O($n^{-\frac{2}{5}}$). The result shows that $k$-nn classifiers, in spite of
their famously fractured decision boundaries, come arbitrarily close to having
Gaussian-style O($n^{-\frac{1}{2}}$) expected differences between PAC (probably
approximately correct) error bounds and actual expected out-of-sample error
rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2501</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2501</id><created>2014-10-09</created><authors><author><keyname>Casta&#xf1;eda</keyname><forenames>Armando</forenames></author><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>Unbeatable Consensus</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.6902</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The unbeatability of a consensus protocol, introduced by Halpern, Moses and
Waarts in 2001, is a stronger notion of optimality than the accepted notion of
early stopping protocols. Using a novel knowledge-based analysis, this paper
derives the first practical unbeatable consensus protocols in the literature,
for the standard synchronous message-passing model with crash failures. These
protocols strictly dominate the best known protocols for uniform and for
non-uniform consensus, in some case beating them by a large margin. The
analysis provides a new understanding of the logical structure of consensus,
and of the distinction between uniform and nonuniform consensus. Finally, the
first (early stopping and) unbeatable protocol that treats decision values
&quot;fairly&quot; is presented. All of these protocols have very concise descriptions,
and are shown to be efficiently implementable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2505</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2505</id><created>2014-10-09</created><updated>2015-12-30</updated><authors><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Recovery of Sparse Signals Using Multiple Orthogonal Least Squares</title><categories>stat.ME cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of recovering sparse signals from compressed linear
measurements. This problem, often referred to as sparse recovery or sparse
reconstruction, has generated a great deal of interest in recent years. To
recover the sparse signals, we propose a new method called multiple orthogonal
least squares (MOLS), which extends the well-known orthogonal least squares
(OLS) algorithm by allowing multiple $L$ indices to be chosen per iteration.
Owing to inclusion of multiple support indices in each selection, the MOLS
algorithm converges in much fewer iterations and improves the computational
efficiency over the conventional OLS algorithm. Theoretical analysis shows that
MOLS ($L &gt; 1$) performs exact recovery of all $K$-sparse signals within $K$
iterations if the measurement matrix satisfies the restricted isometry property
(RIP) with isometry constant $\delta_{LK} &lt; \frac{\sqrt{L}}{\sqrt{K} + 2
\sqrt{L}}.$ The recovery performance of MOLS in the noisy scenario is also
studied. It is shown that stable recovery of sparse signals can be achieved
with the MOLS algorithm when the signal-to-noise ratio (SNR) scales linearly
with the sparsity level of input signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2535</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2535</id><created>2014-10-09</created><authors><author><keyname>Houssineau</keyname><forenames>Jeremie</forenames></author><author><keyname>Clark</keyname><forenames>Daniel</forenames></author><author><keyname>Ivekovic</keyname><forenames>Spela</forenames></author><author><keyname>Lee</keyname><forenames>Chee Sing</forenames></author><author><keyname>Franco</keyname><forenames>Jose</forenames></author></authors><title>A unified approach for multi-object triangulation, tracking and camera
  calibration</title><categories>cs.CV stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object triangulation, 3-D object tracking, feature correspondence, and camera
calibration are key problems for estimation from camera networks. This paper
addresses these problems within a unified Bayesian framework for joint
multi-object tracking and sensor registration. Given that using standard
filtering approaches for state estimation from cameras is problematic, an
alternative parametrisation is exploited, called disparity space. The disparity
space-based approach for triangulation and object tracking is shown to be more
effective than non-linear versions of the Kalman filter and particle filtering
for non-rectified cameras. The approach for feature correspondence is based on
the Probability Hypothesis Density (PHD) filter, and hence inherits the ability
to update without explicit measurement association, to initiate new targets,
and to discriminate between target and clutter. The PHD filtering approach then
forms the basis of a camera calibration method from static or moving objects.
Results are shown on simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2553</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2553</id><created>2014-10-03</created><authors><author><keyname>Moussa</keyname><forenames>Bishoy</forenames></author><author><keyname>Mostafa</keyname><forenames>Mahmoud</forenames></author><author><keyname>El-Khouly</keyname><forenames>Mahmoud</forenames></author></authors><title>XML Schema-based Minification for Communication of Security Information
  and Event Management (SIEM) Systems in Cloud Environments</title><categories>cs.DC cs.CR cs.NI</categories><comments>XML, JSON, Minification, XML Schema, Cloud, Log, Communication,
  Compression, XMill, GZip, Code Generation, Code Readability, 9 pages, 12
  figures, 5 tables, Journal Article</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), 5(9), 2014</journal-ref><doi>10.14569/IJACSA.2014.050912</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  XML-based communication governs most of today's systems communication, due to
its capability of representing complex structural and hierarchical data.
However, XML document structure is considered a huge and bulky data that can be
reduced to minimize bandwidth usage, transmission time, and maximize
performance. This contributes to a more efficient and utilized resource usage.
In cloud environments, this affects the amount of money the consumer pays.
Several techniques are used to achieve this goal. This paper discusses these
techniques and proposes a new XML Schema-based Minification technique. The
proposed technique works on XML Structure reduction using minification. The
proposed technique provides a separation between the meaningful names and the
underlying minified names, which enhances software/code readability. This
technique is applied to Intrusion Detection Message Exchange Format (IDMEF)
messages, as part of Security Information and Event Management (SIEM) system
communication hosted on Microsoft Azure Cloud. Test results show message size
reduction ranging from 8.15% to 50.34% in the raw message, without using
time-consuming compression techniques. Adding GZip compression to the proposed
technique produces 66.1% shorter message size compared to original XML
messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2560</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2560</id><created>2014-10-07</created><authors><author><keyname>Wang</keyname><forenames>Kezhi</forenames></author><author><keyname>Chen</keyname><forenames>Yunfei</forenames></author><author><keyname>Chen</keyname><forenames>Jiming</forenames></author></authors><title>Novel energy detection using uniform noise distribution</title><categories>cs.IT cs.NI math.IT</categories><comments>17 pages, 7 figures. This paper has been submitted to Wireless
  Communications and Mobile Computing on 18-Oct-2013, and it is under second
  round of reviewing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy detection is widely used in cognitive radio due to its low complexity.
One fundamental challenge is that its performance degrades in the presence of
noise uncertainty, which inevitably occurs in practical implementations. In
this work, three novel detectors based on uniformly distributed noise
uncertainty as the worst-case scenario are proposed. Numerical results show
that the new detectors outperform the conventional energy detector with
considerable performance gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2561</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2561</id><created>2014-09-14</created><authors><author><keyname>Cao</keyname><forenames>Yun-He</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author></authors><title>IRCI Free MIMO OFDM SAR Using Circularly Shifted Zadoff-Chu Sequences</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><doi>10.1109/LGRS.2014.2385693</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cyclic prefix (CP) based MIMO-OFDM radar has been recently proposed for
distributed transmit antennas, where there is no inter-range-cell interference
(IRCI). It can collect full spatial diversity and each transmitter transmits
signals with the same frequency band, i.e., the range resolution is not
reduced. However, it needs to transmit multiple OFDM pulses consecutively to
obtain range profiles for a single swath, which may be too long in time for a
reasonable swath width. In this letter, we propose a CP based MIMO-OFDM
synthetic aperture radar (SAR) system, where each transmitter transmits only a
single OFDM pulse to obtain range profiles for a swath and has the same
frequency band, thus the range resolution is not reduced. It is IRCI free and
can collect the full spatial diversity if the transmit antennas are
distributed. Our main idea is to use circularly shifted Zadoff-Chu sequences as
the weighting coefficients in the OFDM pulses for different transmit antennas
and apply spatial filters with multiple receive antennas to divide the whole
swath into multiple subswaths, and then each subswath is reconstructed/imaged
using our proposed IRCI free range reconstruction method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2570</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2570</id><created>2014-10-09</created><updated>2014-12-16</updated><authors><author><keyname>Li</keyname><forenames>Zhang</forenames></author><author><keyname>Lin</keyname><forenames>Xiaojun</forenames></author><author><keyname>Peleato-Inarrea</keyname><forenames>Borja</forenames></author><author><keyname>Pollak</keyname><forenames>Ilya</forenames></author></authors><title>Optimal Monitoring and Mitigation of Systemic Risk in Financial Networks</title><categories>q-fin.RM cs.SI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of optimally allocating a cash injection into
a financial system in distress. Given a one-period borrower-lender network in
which all debts are due at the same time and have the same seniority, we
address the problem of allocating a fixed amount of cash among the nodes to
minimize the weighted sum of unpaid liabilities. Assuming all the loan amounts
and asset values are fixed and that there are no bankruptcy costs, we show that
this problem is equivalent to a linear program. We develop a duality-based
distributed algorithm to solve it which is useful for applications where it is
desirable to avoid centralized data gathering and computation. We also consider
the problem of minimizing the expectation of the weighted sum of unpaid
liabilities under the assumption that the net external asset holdings of all
institutions are stochastic. We show that this problem is a two-stage
stochastic linear program. To solve it, we develop two algorithms based on:
Benders decomposition algorithm and projected stochastic gradient descent. We
show that if the defaulting nodes never pay anything, the deterministic optimal
cash injection allocation problem is an NP-hard mixed-integer linear program.
However, modern optimization software enables the computation of very accurate
solutions to this problem on a personal computer in a few seconds for network
sizes comparable with the size of the US banking system. In addition, we
address the problem of allocating the cash injection amount so as to minimize
the number of nodes in default. For this problem, we develop two heuristic
algorithms: a reweighted l1 minimization algorithm and a greedy algorithm. We
illustrate these two algorithms using three synthetic network structures for
which the optimal solution can be calculated exactly. We also compare these two
algorithms on three types random networks which are more complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2592</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2592</id><created>2014-10-09</created><authors><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Belmega</keyname><forenames>E. Veronica</forenames></author></authors><title>Transmit without regrets: Online optimization in MIMO-OFDM cognitive
  radio systems</title><categories>cs.IT math.IT</categories><comments>25 pages, 3 figures, to appear in the IEEE Journal on Selected Areas
  in Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine cognitive radio systems that evolve dynamically
over time due to changing user and environmental conditions. To combine the
advantages of orthogonal frequency division multiplexing (OFDM) and
multiple-input, multiple-output (MIMO) technologies, we consider a MIMO-OFDM
cognitive radio network where wireless users with multiple antennas communicate
over several non-interfering frequency bands. As the network's primary users
(PUs) come and go in the system, the communication environment changes
constantly (and, in many cases, randomly). Accordingly, the network's
unlicensed, secondary users (SUs) must adapt their transmit profiles &quot;on the
fly&quot; in order to maximize their data rate in a rapidly evolving environment
over which they have no control. In this dynamic setting, static solution
concepts (such as Nash equilibrium) are no longer relevant, so we focus on
dynamic transmit policies that lead to no regret: specifically, we consider
policies that perform at least as well as (and typically outperform) even the
best fixed transmit profile in hindsight. Drawing on the method of matrix
exponential learning and online mirror descent techniques, we derive a
no-regret transmit policy for the system's SUs which relies only on local
channel state information (CSI). Using this method, the system's SUs are able
to track their individually evolving optimum transmit profiles remarkably well,
even under rapidly (and randomly) changing conditions. Importantly, the
proposed augmented exponential learning (AXL) policy leads to no regret even if
the SUs' channel measurements are subject to arbitrarily large observation
errors (the imperfect CSI case), thus ensuring the method's robustness in the
presence of uncertainties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2595</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2595</id><created>2014-10-08</created><authors><author><keyname>Sinclair</keyname><forenames>Alistair</forenames></author><author><keyname>Srivastava</keyname><forenames>Piyush</forenames></author><author><keyname>&#x160;tefankovi&#x10d;</keyname><forenames>Daniel</forenames></author><author><keyname>Yin</keyname><forenames>Yitong</forenames></author></authors><title>Spatial mixing and the connective constant: Optimal bounds</title><categories>cs.DS cs.DM math.PR</categories><comments>This paper supersedes arxiv:1308.1762, in which weaker versions of
  some of the results in this paper appeared. The current paper strengthens the
  main result of 1308.1762 (Theorem 1.3) to obtain an optimal setting of the
  parameters, and also adds new results for the monomer-dimer model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of deterministic approximate counting of matchings and
independent sets in graphs of bounded connective constant. More generally, we
consider the problem of evaluating the partition functions of the monomer-dimer
model (which is defined as a weighted sum over all matchings where each
matching is given a weight $\gamma^{|V| - 2 |M|}$ in terms of a fixed parameter
gamma called the monomer activity) and the hard core model (which is defined as
a weighted sum over all independent sets where an independent set I is given a
weight $\lambda^{|I|}$ in terms of a fixed parameter lambda called the vertex
activity). The connective constant is a natural measure of the average degree
of a graph which has been studied extensively in combinatorics and mathematical
physics, and can be bounded by a constant even for certain unbounded degree
graphs such as those sampled from the sparse Erd\H{o}s-R\'enyi model $G(n,
d/n)$.
  Our main technical contribution is to prove the best possible rates of decay
of correlations in the natural probability distributions induced by both the
hard core model and the monomer-dimer model in graphs with a given bound on the
connective constant. These results on decay of correlations are obtained using
a new framework based on the so-called message approach that has been
extensively used recently to prove such results for bounded degree graphs. We
then use these optimal decay of correlations results to obtain FPTASs for the
two problems on graphs of bounded connective constant.
  Our techniques also allow us to improve upon known bounds for decay of
correlations for the hard core model on various regular lattices, including
those obtained by Restrepo, Shin, Vigoda and Tetali (2011) for the special case
of Z^2 using sophisticated numerically intensive methods tailored to that
special case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2598</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2598</id><created>2014-10-09</created><authors><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Different perceptions of social dilemmas: Evolutionary multigames in
  structured populations</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>7 two-column pages, 5 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 90 (2014) 032813</journal-ref><doi>10.1103/PhysRevE.90.032813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the fact that the same social dilemma can be perceived
differently by different players, we here study evolutionary multigames in
structured populations. While the core game is the weak prisoner's dilemma, a
fraction of the population adopts either a positive or a negative value of the
sucker's payoff, thus playing either the traditional prisoner's dilemma or the
snowdrift game. We show that the higher the fraction of the population adopting
a different payoff matrix, the more the evolution of cooperation is promoted.
The microscopic mechanism responsible for this outcome is unique to structured
populations, and it is due to the payoff heterogeneity, which spontaneously
introduces strong cooperative leaders that give rise to an asymmetric strategy
imitation flow in favor of cooperation. We demonstrate that the reported
evolutionary outcomes are robust against variations of the interaction network,
and they also remain valid if players are allowed to vary which game they play
over time. These results corroborate existing evidence in favor of
heterogeneity-enhanced network reciprocity, and they reveal how different
perceptions of social dilemmas may contribute to their resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2609</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2609</id><created>2014-09-10</created><updated>2015-09-16</updated><authors><author><keyname>Bogale</keyname><forenames>Tadilo Endeshaw</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author><author><keyname>Haghighat</keyname><forenames>Afshin</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>On the Number of RF Chains and Phase Shifters, and Scheduling Design
  with Hybrid Analog-Digital Beamforming</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Wireless Communications (Minor Revision)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers hybrid beamforming (HB) for downlink multiuser massive
multiple input multiple output (MIMO) systems with frequency selective
channels. For this system, first we determine the required number of radio
frequency (RF) chains and phase shifters (PSs) such that the proposed HB
achieves the same performance as that of the digital beamforming (DB) which
utilizes $N$ (number of transmitter antennas) RF chains. We show that the
performance of the DB can be achieved with our HB just by utilizing $r_t$ RF
chains and $2r_t(N-r_t + 1)$ PSs, where $r_t \leq N$ is the rank of the
combined digital precoder matrices of all sub-carriers. Second, we provide a
simple and novel approach to reduce the number of PSs with only a negligible
performance degradation. Numerical results reveal that only $20-40$ PSs per RF
chain are sufficient for practically relevant parameter settings. Finally, for
the scenario where the deployed number of RF chains $(N_a)$ is less than $r_t$,
we propose a simple user scheduling algorithm to select the best set of users
in each sub-carrier. Simulation results validate theoretical expressions, and
demonstrate the superiority of the proposed HB design over the existing HB
designs in both flat fading and frequency selective channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2620</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2620</id><created>2014-10-09</created><authors><author><keyname>Shen</keyname><forenames>Wenlong</forenames></author><author><keyname>Hong</keyname><forenames>Weisheng</forenames></author><author><keyname>Cao</keyname><forenames>Xianghui</forenames></author><author><keyname>Yin</keyname><forenames>Bo</forenames></author><author><keyname>Shila</keyname><forenames>Devu Manikantan</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author></authors><title>Secure Key Establishment for Device-to-Device Communications</title><categories>cs.CR cs.NI</categories><comments>5 pages, 4 figures, accepted to IEEE Globecom</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid growth of smartphone and tablet users, Device-to-Device (D2D)
communications have become an attractive solution for enhancing the performance
of traditional cellular networks. However, relevant security issues involved in
D2D communications have not been addressed yet. In this paper, we investigate
the security requirements and challenges for D2D communications, and present a
secure and efficient key agreement protocol, which enables two mobile devices
to establish a shared secret key for D2D communications without prior
knowledge. Our approach is based on the Diffie-Hellman key agreement protocol
and commitment schemes. Compared to previous work, our proposed protocol
introduces less communication and computation overhead. We present the design
details and security analysis of the proposed protocol. We also integrate our
proposed protocol into the existing Wi-Fi Direct protocol, and implement it
using Android smartphones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2628</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2628</id><created>2014-10-09</created><updated>2014-10-17</updated><authors><author><keyname>King</keyname><forenames>Andrew D.</forenames></author><author><keyname>McGeoch</keyname><forenames>Catherine C.</forenames></author></authors><title>Algorithm engineering for a quantum annealing platform</title><categories>cs.DS cs.ET quant-ph</categories><comments>16 pages. V2: minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances bring within reach the viability of solving combinatorial
problems using a quantum annealing algorithm implemented on a purpose-built
platform that exploits quantum properties. However, the question of how to tune
the algorithm for most effective use in this framework is not well understood.
In this paper we describe some operational parameters that drive performance,
discuss approaches for mitigating sources of error, and present experimental
results from a D-Wave Two quantum annealing processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2632</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2632</id><created>2014-10-09</created><authors><author><keyname>Lillis</keyname><forenames>David</forenames></author><author><keyname>Collier</keyname><forenames>Rem W.</forenames></author><author><keyname>Jordan</keyname><forenames>Howell R.</forenames></author></authors><title>Evaluation of a Conversation Management Toolkit for Multi Agent
  Programming</title><categories>cs.MA</categories><comments>appears as Programming Multi-Agent Systems - 10th International
  Workshop, ProMAS 2012, Valencia, Spain, June 5, 2012, Revised Selected Papers</comments><doi>10.1007/978-3-642-38700-5_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Agent Conversation Reasoning Engine (ACRE) is intended to aid agent
developers to improve the management and reliability of agent communication. To
evaluate its effectiveness, a problem scenario was created that could be used
to compare code written with and without the use of ACRE by groups of test
subjects.
  This paper describes the requirements that the evaluation scenario was
intended to meet and how these motivated the design of the problem. Two
experiments were conducted with two separate sets of students and their
solutions were analysed using a combination of simple objective metrics and
subjective analysis. The analysis suggested that ACRE by default prevents some
common problems arising that would limit the reliability and extensibility of
conversation-handling code.
  As ACRE has to date been integrated only with the Agent Factory multi agent
framework, it was necessary to verify that the problems identified are not
unique to that platform. Thus a comparison was made with best practice
communication code written for the Jason platform, in order to demonstrate the
wider applicability of a system such as ACRE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2634</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2634</id><created>2014-10-09</created><authors><author><keyname>Lillis</keyname><forenames>David</forenames></author><author><keyname>Toolan</keyname><forenames>Fergus</forenames></author><author><keyname>Collier</keyname><forenames>Rem W.</forenames></author><author><keyname>Dunnion</keyname><forenames>John</forenames></author></authors><title>Extending Probabilistic Data Fusion Using Sliding Windows</title><categories>cs.IR</categories><journal-ref>Advances in Information Retrieval. Proceedings of the 30th
  European Conference on Information Retrieval Research (ECIR 2008), volume
  4956 of Lecture Notes in Computer Science, pages 358--369, Berlin, 2008.
  Springer Berlin Heidelberg</journal-ref><doi>10.1007/978-3-540-78646-7_33</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in the field of data fusion have seen a focus on
techniques that use training queries to estimate the probability that various
documents are relevant to a given query and use that information to assign
scores to those documents on which they are subsequently ranked. This paper
introduces SlideFuse, which builds on these techniques, introducing a sliding
window in order to compensate for situations where little relevance information
is available to aid in the estimation of probabilities.
  SlideFuse is shown to perform favourably in comparison with CombMNZ, ProbFuse
and SegFuse. CombMNZ is the standard baseline technique against which data
fusion algorithms are compared whereas ProbFuse and SegFuse represent the
state-of-the-art for probabilistic data fusion methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2640</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2640</id><created>2014-10-09</created><authors><author><keyname>Price</keyname><forenames>Eric</forenames></author></authors><title>Optimal Lower Bound for Itemset Frequency Indicator Sketches</title><categories>cs.DS</categories><comments>3 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Given a database, a common problem is to find the pairs or $k$-tuples of
items that frequently co-occur. One specific problem is to create a small space
&quot;sketch&quot; of the data that records which $k$-tuples appear in more than an
$\epsilon$ fraction of rows of the database.
  We improve the lower bound of Liberty, Mitzenmacher, and Thaler [LMT14],
showing that $\Omega(\frac{1}{\epsilon}d \log (\epsilon d))$ bits are necessary
even in the case of $k=2$. This matches the sampling upper bound for all
$\epsilon \geq 1/d^{.99}$, and (in the case of $k=2$) another trivial upper
bound for $\epsilon = 1/d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2645</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2645</id><created>2014-10-09</created><authors><author><keyname>Boral</keyname><forenames>Anudhyan</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author></authors><title>Multi-Party Set Reconciliation Using Characteristic Polynomials</title><categories>cs.DS</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the standard set reconciliation problem, there are two parties $A_1$ and
$A_2$, each respectively holding a set of elements $S_1$ and $S_2$. The goal is
for both parties to obtain the union $S_1 \cup S_2$. In many distributed
computing settings the sets may be large but the set difference
$|S_1-S_2|+|S_2-S_1|$ is small. In these cases one aims to achieve
reconciliation efficiently in terms of communication; ideally, the
communication should depend on the size of the set difference, and not on the
size of the sets.
  Recent work has considered generalizations of the reconciliation problem to
multi-party settings, using a framework based on a specific type of linear
sketch called an Invertible Bloom Lookup Table. Here, we consider multi-party
set reconciliation using the alternative framework of characteristic
polynomials, which have previously been used for efficient pairwise set
reconciliation protocols, and compare their performance with Invertible Bloom
Lookup Tables for these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2646</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2646</id><created>2014-10-09</created><authors><author><keyname>Bebah</keyname><forenames>Mohamed</forenames></author><author><keyname>Amine</keyname><forenames>Chennoufi</forenames></author><author><keyname>Azzeddine</keyname><forenames>Mazroui</forenames></author><author><keyname>Abdelhak</keyname><forenames>Lakhouaja</forenames></author></authors><title>Hybrid approaches for automatic vowelization of Arabic texts</title><categories>cs.CL</categories><comments>19 pages</comments><msc-class>68T50</msc-class><doi>10.5121/ijnlc.2014.3404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid approaches for automatic vowelization of Arabic texts are presented in
this article. The process is made up of two modules. In the first one, a
morphological analysis of the text words is performed using the open source
morphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed out
of context, are its different possible vowelizations. The integration of this
Analyzer in our vowelization system required the addition of a lexical database
containing the most frequent words in Arabic language. Using a statistical
approach based on two hidden Markov models (HMM), the second module aims to
eliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabic
words are the observed states and the vowelized words are the hidden states.
The observed states of the second HMM are identical to those of the first, but
the hidden states are the lists of possible diacritics of the word without its
Arabic letters. Our system uses Viterbi algorithm to select the optimal path
among the solutions proposed by Al Khalil Morpho Sys. Our approach opens an
important way to improve the performance of automatic vowelization of Arabic
texts for other uses in automatic natural language processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2652</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2652</id><created>2014-10-09</created><authors><author><keyname>Erdelyi</keyname><forenames>Gabor</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author></authors><title>More Natural Models of Electoral Control by Partition</title><categories>cs.GT cs.CC cs.MA</categories><acm-class>I.2.11; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Control&quot; studies attempts to set the outcome of elections through the
addition, deletion, or partition of voters or candidates. The set of benchmark
control types was largely set in the seminal 1992 paper by Bartholdi, Tovey,
and Trick that introduced control, and there now is a large literature studying
how many of the benchmark types various election systems are vulnerable to,
i.e., have polynomial-time attack algorithms for.
  However, although the longstanding benchmark models of addition and deletion
model relatively well the real-world settings that inspire them, the
longstanding benchmark models of partition model settings that are arguably
quite distant from those they seek to capture.
  In this paper, we introduce--and for some important cases analyze the
complexity of--new partition models that seek to better capture many real-world
partition settings. In particular, in many partition settings one wants the two
parts of the partition to be of (almost) equal size, or is partitioning into
more than two parts, or has groups of actors who must be placed in the same
part of the partition. Our hope is that having these new partition types will
allow studies of control attacks to include such models that more realistically
capture many settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2659</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2659</id><created>2014-10-09</created><updated>2014-10-14</updated><authors><author><keyname>Balado</keyname><forenames>F&#xe9;lix</forenames></author><author><keyname>Haughton</keyname><forenames>David</forenames></author></authors><title>Optimum Perfect Universal Steganography of Finite Memoryless Sources</title><categories>cs.IT math.IT</categories><comments>18 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A theoretical analysis and a practical solution to the fundamental problem of
optimum perfect universal steganography of finite memoryless sources with a
passive warden are provided. The theoretical analysis rests on the fact that
Slepian's Variant I permutation coding implements first-order perfect universal
steganography with maximum embedding rate. The practical solution underlines
the duality between perfect steganography with optimum embedding rate and
lossless source coding with optimum compression rate, since it is shown that
permutation coding can be implemented by means of adaptive arithmetic coding. A
distance constraint between host signal and information-carrying signal must be
observed in universal steganography, and thus an optimum tradeoff between
embedding rate and embedding distortion must be achieved. Partitioned
permutation coding is shown to be the solution to this problem. A practical
implementation of partitioned permutation coding is given that performs close
to an unattainable upper bound on the rate-distortion function of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2662</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2662</id><created>2014-10-09</created><authors><author><keyname>Mukherjee</keyname><forenames>Shreyasee</forenames></author><author><keyname>Su</keyname><forenames>Kai</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>K. K.</forenames></author><author><keyname>Raychaudhuri</keyname><forenames>Dipankar</forenames></author><author><keyname>Seskar</keyname><forenames>Ivan</forenames></author></authors><title>Evaluating Opportunistic Delivery of Large Content with TCP over WiFi in
  I2V Communication</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing interest in connected vehicles, it is useful to evaluate
the capability of delivering large content over a WiFi infrastructure to
vehicles. The throughput achieved over WiFi channels can be highly variable and
also rapidly degrades as the distance from the access point increases. While
this behavior is well understood at the data link layer, the interactions
across the various protocol layers (data link and up through the transport
layer) and the effect of mobility may reduce the amount of content transferred
to the vehicle, as it travels along the roadway.
  This paper examines the throughput achieved at the TCP layer over a carefully
designed outdoor WiFi environment and the interactions across the layers that
impact the performance achieved, as a function of the receiver mobility. The
experimental studies conducted reveal that impairments over the WiFi link
(frame loss, ARQ and increased delay) and the residual loss seen by TCP causes
a cascade of duplicate ACKs to be generated. This triggers large congestion
window reductions at the sender, leading to a drastic degradation of throughput
to the vehicular client. To ensure outdoor WiFi infrastructures have the
potential to sustain reasonable downlink throughput for drive-by vehicles, we
speculate that there is a need to adapt how WiFi and TCP (as well as mobility
protocols) function for such vehicular applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2663</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2663</id><created>2014-10-09</created><authors><author><keyname>Yger</keyname><forenames>Florian</forenames></author></authors><title>Challenge IEEE-ISBI/TCB : Application of Covariance matrices and wavelet
  marginals</title><categories>cs.CV</categories><comments>9 pages, 4 Figues, 2 Tables, Challenge IEEE-ISBI : Bone Texture
  Characterization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short memo aims at explaining our approach for the challenge IEEE-ISBI
on Bone Texture Characterization. In this work, we focus on the use of
covariance matrices and wavelet marginals in an SVM classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2670</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2670</id><created>2014-10-10</created><authors><author><keyname>Jesse</keyname><forenames>Forrest Fabian</forenames></author></authors><title>Entropy NAND: Early Functional Completeness in Entropy Networks</title><categories>cs.IT math.IT</categories><comments>12 pages, 3 figures</comments><msc-class>37A35, 81P15</msc-class><acm-class>F.1.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An observer increases in relative entropy as it receives information from
what it is observing. In a system of only an observer and the observed, an
increase in the relative entropy of the observer is a decrease in the relative
entropy of the observed. Linking together these directional entropy
disequilibriums we show that NAND and NOR functionality arise in such networks
at very low levels of complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2686</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2686</id><created>2014-10-10</created><updated>2015-03-11</updated><authors><author><keyname>&#xc7;atak</keyname><forenames>Ferhat &#xd6;zg&#xfc;r</forenames></author></authors><title>Polarization Measurement of High Dimensional Social Media Messages With
  Support Vector Machine Algorithm Using Mapreduce</title><categories>cs.LG cs.CL</categories><comments>12 pages, in Turkish</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose a new Support Vector Machine (SVM) training
algorithm based on distributed MapReduce technique. In literature, there are a
lots of research that shows us SVM has highest generalization property among
classification algorithms used in machine learning area. Also, SVM classifier
model is not affected by correlations of the features. But SVM uses quadratic
optimization techniques in its training phase. The SVM algorithm is formulated
as quadratic optimization problem. Quadratic optimization problem has $O(m^3)$
time and $O(m^2)$ space complexity, where m is the training set size. The
computation time of SVM training is quadratic in the number of training
instances. In this reason, SVM is not a suitable classification algorithm for
large scale dataset classification. To solve this training problem we developed
a new distributed MapReduce method developed. Accordingly, (i) SVM algorithm is
trained in distributed dataset individually; (ii) then merge all support
vectors of classifier model in every trained node; and (iii) iterate these two
steps until the classifier model converges to the optimal classifier function.
In the implementation phase, large scale social media dataset is presented in
TFxIDF matrix. The matrix is used for sentiment analysis to get polarization
value. Two and three class models are created for classification method.
Confusion matrices of each classification model are presented in tables. Social
media messages corpus consists of 108 public and 66 private universities
messages in Turkey. Twitter is used for source of corpus. Twitter user messages
are collected using Twitter Streaming API. Results are shown in graphics and
tables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2687</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2687</id><created>2014-10-10</created><authors><author><keyname>Le</keyname><forenames>Sy-Quoc</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author></authors><title>Second-Order Coding Rates for Conditional Rate-Distortion</title><categories>cs.IT math.IT</categories><comments>20 pages, 2 figures, second-order coding rates, finite blocklength,
  network information theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper characterizes the second-order coding rates for lossy source
coding with side information available at both the encoder and the decoder. We
first provide non-asymptotic bounds for this problem and then specialize the
non-asymptotic bounds for three different scenarios: discrete memoryless
sources, Gaussian sources, and Markov sources. We obtain the second-order
coding rates for these settings. It is interesting to observe that the
second-order coding rate for Gaussian source coding with Gaussian side
information available at both the encoder and the decoder is the same as that
for Gaussian source coding without side information. Furthermore, regardless of
the variance of the side information, the dispersion is $1/2$ nats squared per
source symbol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2697</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2697</id><created>2014-10-10</created><updated>2015-04-22</updated><authors><author><keyname>Aminfar</keyname><forenames>AmirHossein</forenames></author><author><keyname>Darve</keyname><forenames>Eric</forenames></author></authors><title>A Fast and Memory Efficient Sparse Solver with Applications to
  Finite-Element Matrices</title><categories>cs.NA math.NA</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we introduce a fast and memory efficient solver for sparse
matrices arising from the finite element discretization of elliptic partial
differential equations (PDEs). We use a fast direct (but approximate)
multifrontal solver as a preconditioner, and use an iterative solver to achieve
a desired accuracy. This approach combines the advantages of direct and
iterative schemes to arrive at a fast, robust and accurate solver. We will show
that this solver is faster ($\sim$ 2x) and more memory efficient ($\sim$ 2--3x)
than a conventional direct multifrontal solver. Furthermore, we will
demonstrate that the solver is both a faster and more effective preconditioner
than other preconditioners such as the incomplete LU preconditioner. Specific
speed-ups depend on the matrix size and improve as the size of the matrix
increases. The solver can be applied to both structured and unstructured meshes
in a similar manner. We build on our previous work and utilize the fact that
dense frontal and update matrices, in the multifrontal algorithm, can be
represented as hierarchically off-diagonal low-rank (HODLR) matrices. Using
this idea, we replace all large dense matrix operations in the multifrontal
elimination process with $O(N)$ HODLR operations to arrive at a faster and more
memory efficient solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2698</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2698</id><created>2014-10-10</created><authors><author><keyname>Gowanlock</keyname><forenames>Michael</forenames></author><author><keyname>Casanova</keyname><forenames>Henri</forenames></author></authors><title>Technical Report: Towards Efficient Indexing of Spatiotemporal
  Trajectories on the GPU for Distance Threshold Similarity Searches</title><categories>cs.DC cs.DB</categories><comments>30 pages, 18 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications in many domains require processing moving object trajectories.
In this work, we focus on a trajectory similarity search that finds all
trajectories within a given distance of a query trajectory over a time
interval, which we call the distance threshold similarity search. We develop
three indexing strategies with spatial, temporal and spatiotemporal selectivity
for the GPU that differ significantly from indexes suitable for the CPU, and
show the conditions under which each index achieves good performance.
Furthermore, we show that the GPU implementations outperform multithreaded CPU
implementations in a range of experimental scenarios, making the GPU an
attractive technology for processing moving object trajectories. We test our
implementations on two synthetic and one real-world dataset of a galaxy merger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2702</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2702</id><created>2014-10-10</created><authors><author><keyname>Yang</keyname><forenames>Minghui</forenames></author><author><keyname>Li</keyname><forenames>Jin</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author></authors><title>Generalized Hamming Weights of Irreducible Cyclic Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized Hamming weight (GHW) $d_r(C)$ of linear codes $C$ is a
natural generalization of the minimum Hamming distance $d(C)(=d_1(C))$ and has
become one of important research objects in coding theory since Wei's originary
work [23] in 1991. In this paper two general formulas on $d_r(C)$ for
irreducible cyclic codes are presented by using Gauss sums and the weight
hierarchy $\{d_1(C), d_2(C), \ldots, d_k(C)\}$ $(k=\dim C)$ are completely
determined for several cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2707</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2707</id><created>2014-10-10</created><authors><author><keyname>Caudullo</keyname><forenames>Giovanni</forenames></author></authors><title>Applying Geospatial Semantic Array Programming for a Reproducible Set of
  Bioclimatic Indices in Europe</title><categories>cs.CE physics.ao-ph</categories><comments>10 pages, 4 figures, 1 table, published in IEEE Earthzine 2014 Vol. 7
  Issue 2, 877975+ 2nd quarter theme. Geospatial Semantic Array Programming.
  Available: http://www.earthzine.org/?p=877975</comments><journal-ref>IEEE Earthzine, vol. 7, no. 2, pp. 877 975+, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bioclimate-driven regression analysis is a widely used approach for modelling
ecological niches and zonation. Although the bioclimatic complexity of the
European continent is high, a particular combination of 12 climatic and
topographic covariates was recently found able to reliably reproduce the
ecological zoning of the Food and Agriculture Organization of the United
Nations (FAO) for forest resources assessment at pan-European scale, generating
the first fuzzy similarity map of FAO ecozones in Europe. The reproducible
procedure followed to derive this collection of bioclimatic indices is now
presented. It required an integration of data-transformation modules (D-TM)
using geospatial tools such as Geographic Information System (GIS) software,
and array-based mathematical implementation such as semantic array programming
(SemAP). Base variables, intermediate and final covariates are described and
semantically defined by providing the workflow of D-TMs and the mathematical
formulation following the SemAP notation. Source layers to derive base
variables were extracted by exclusively relying on global-scale public open
geodata in order for the same set of bioclimatic covariates to be reproducible
in any region worldwide. In particular, two freely available datasets were
exploited for temperature and precipitation (WorldClim) and elevation (Global
Multi-resolution Terrain Elevation Data). The working extent covers the
European continent to the Urals with a resolution of 30 arc-second. The
proposed set of bioclimatic covariates will be made available as open data in
the European Forest Data Centre (EFDAC). The forthcoming complete set of D-TM
codelets will enable the 12 covariates to be easily reproduced and expanded
through free software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2721</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2721</id><created>2014-10-10</created><authors><author><keyname>Hamiez</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author></authors><title>A note on a sports league scheduling problem</title><categories>cs.DS</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sports league scheduling is a difficult task in the general case. In this
short note, we report two improvements to an existing enumerative search
algorithm for a NP-hard sports league scheduling problem known as &quot;prob026&quot; in
CSPLib. These improvements are based on additional rules to constraint and
accelerate the enumeration process. The proposed approach is able to find a
solution (schedule) for all prob026 instances for a number T of teams ranging
from 12 to 70, including several T values for which a solution is reported for
the first time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2722</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2722</id><created>2014-10-10</created><authors><author><keyname>Toscani</keyname><forenames>G.</forenames></author></authors><title>A concavity property for the reciprocal of Fisher information and its
  consequences on Costa's EPI</title><categories>cs.IT math-ph math.IT math.MP</categories><doi>10.1016/j.physa.2015.03.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the reciprocal of Fisher information of a log-concave
probability density $X$ in ${\bf{R}}^n$ is concave in $t$ with respect to the
addition of a Gaussian noise $Z_t = N(0, tI_n)$. As a byproduct of this result
we show that the third derivative of the entropy power of a log-concave
probability density $X$ in ${\bf{R}}^n$ is nonnegative in $t$ with respect to
the addition of a Gaussian noise $Z_t$. For log-concave densities this improves
the well-known Costa's concavity property of the entropy power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2724</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2724</id><created>2014-10-10</created><authors><author><keyname>Mota</keyname><forenames>Jo&#xe3;o F. C.</forenames></author><author><keyname>Deligiannis</keyname><forenames>Nikos</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Compressed Sensing With Side Information: Geometrical Interpretation and
  Performance Bounds</title><categories>cs.IT math.IT math.OC stat.ML</categories><comments>This paper, to be presented at GlobalSIP 2014, is a shorter version
  of http://arxiv.org/abs/1408.5250</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of Compressed Sensing (CS) with side information.
Namely, when reconstructing a target CS signal, we assume access to a similar
signal. This additional knowledge, the side information, is integrated into CS
via L1-L1 and L1-L2 minimization. We then provide lower bounds on the number of
measurements that these problems require for successful reconstruction of the
target signal. If the side information has good quality, the number of
measurements is significantly reduced via L1-L1 minimization, but not so much
via L1-L2 minimization. We provide geometrical interpretations and experimental
results illustrating our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2725</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2725</id><created>2014-10-10</created><authors><author><keyname>Pai</keyname><forenames>Srikanth</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>On the Bounds of Certain Maximal Linear Codes in a Projective Space</title><categories>cs.IT math.IT</categories><comments>10 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The set of all subspaces of $\mathbb{F}_q^n$ is denoted by $\mathbb{P}_q(n)$.
The subspace distance $d_S(X,Y) = \dim(X)+ \dim(Y) - 2\dim(X \cap Y)$ defined
on $\mathbb{P}_q(n)$ turns it into a natural coding space for error correction
in random network coding. A subset of $\mathbb{P}_q(n)$ is called a code and
the subspaces that belong to the code are called codewords. Motivated by
classical coding theory, a linear coding structure can be imposed on a subset
of $\mathbb{P}_q(n)$. Braun, Etzion and Vardy conjectured that the largest
cardinality of a linear code, that contains $\mathbb{F}_q^n$, is $2^n$. In this
paper, we prove this conjecture and characterize the maximal linear codes that
contain $\mathbb{F}_q^n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2736</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2736</id><created>2014-10-10</created><authors><author><keyname>Ehlers</keyname><forenames>Thorsten</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Mike</forenames></author></authors><title>Faster Sorting Networks for $17$, $19$ and $20$ Inputs</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new parallel sorting networks for $17$ to $20$ inputs. For $17,
19,$ and $20$ inputs these new networks are faster (i.e., they require less
computation steps) than the previously known best networks. Therefore, we
improve upon the known upper bounds for minimal depth sorting networks on $17,
19,$ and $20$ channels. The networks were obtained using a combination of
hand-crafted first layers and a SAT encoding of sorting networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2737</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2737</id><created>2014-10-10</created><updated>2014-10-23</updated><authors><author><keyname>Bachmeier</keyname><forenames>Georg</forenames></author><author><keyname>Luttenberger</keyname><forenames>Michael</forenames></author><author><keyname>Schlund</keyname><forenames>Maximilian</forenames></author></authors><title>Finite Automata for the Sub- and Superword Closure of CFLs:
  Descriptional and Computational Complexity</title><categories>cs.FL</categories><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We answer two open questions by (Gruber, Holzer, Kutrib, 2009) on the
state-complexity of representing sub- or superword closures of context-free
grammars (CFGs): (1) We prove a (tight) upper bound of $2^{\mathcal{O}(n)}$ on
the size of nondeterministic finite automata (NFAs) representing the subword
closure of a CFG of size $n$. (2) We present a family of CFGs for which the
minimal deterministic finite automata representing their subword closure
matches the upper-bound of $2^{2^{\mathcal{O}(n)}}$ following from (1).
Furthermore, we prove that the inequivalence problem for NFAs representing sub-
or superword-closed languages is only NP-complete as opposed to PSPACE-complete
for general NFAs. Finally, we extend our results into an approximation method
to attack inequivalence problems for CFGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2752</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2752</id><created>2014-10-10</created><updated>2014-10-13</updated><authors><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author><author><keyname>Schr&#xf6;cker</keyname><forenames>Hans-Peter</forenames></author></authors><title>Spatial Straight Line Linkages by Factorization of Motion Polynomials</title><categories>math.MG cs.RO math.RA</categories><comments>Corrected author name</comments><msc-class>70B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the recently introduced factorization of motion polynomials for
constructing overconstrained spatial linkages with a straight line trajectory.
Unlike previous examples, the end-effector motion is not translational and the
link graph is a cycle. In particular, we obtain a number of linkages with four
revolute and two prismatic joints and a remarkable linkage with seven revolute
joints one of whose joints performs a Darboux motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2757</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2757</id><created>2014-10-10</created><authors><author><keyname>Yang</keyname><forenames>Shenghao</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author><author><keyname>You</keyname><forenames>Lizhao</forenames></author><author><keyname>Chen</keyname><forenames>Yi</forenames></author></authors><title>Linearly-Coupled Fountain Codes</title><categories>cs.IT math.IT</categories><comments>37 pages, submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network-coded multiple access (NCMA) is a communication scheme for wireless
multiple-access networks where physical-layer network coding (PNC) is employed.
In NCMA, a user encodes and spreads its message into multiple packets. Time is
slotted and multiple users transmit packets (one packet each) simultaneously in
each timeslot. A sink node aims to decode the messages of all the users from
the sequence of receptions over successive timeslots. For each timeslot, the
NCMA receiver recovers multiple linear combinations of the packets transmitted
in that timeslot, forming a system of linear equations. Different systems of
linear equations are recovered in different timeslots. A message decoder then
recovers the original messages of all the users by jointly solving multiple
systems of linear equations obtained over different timeslots. We propose a
low-complexity digital fountain approach for this coding problem, where each
source node encodes its message into a sequence of packets using a fountain
code. The aforementioned systems of linear equations recovered by the NCMA
receiver effectively couple these fountain codes together. We refer to the
coupling of the fountain codes as a linearly-coupled (LC) fountain code. The
ordinary belief propagation (BP) decoding algorithm for conventional fountain
codes is not optimal for LC fountain codes. We propose a batched BP decoding
algorithm and analyze the convergence of the algorithm for general LC fountain
codes. We demonstrate how to optimize the degree distributions and show by
numerical results that the achievable rate region is nearly optimal. Our
approach significantly reduces the decoding complexity compared with the
previous NCMA schemes based on Reed-Solomon codes and random linear codes, and
hence has the potential to increase throughput and decrease delay in
computation-limited NCMA systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2759</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2759</id><created>2014-10-10</created><updated>2015-08-04</updated><authors><author><keyname>Hardin</keyname><forenames>Johanna</forenames></author><author><keyname>Sarkis</keyname><forenames>Ghassan</forenames></author><author><keyname>Urc</keyname><forenames>P. C.</forenames></author></authors><title>Network Analysis with the Enron Email Corpus</title><categories>stat.OT cs.SI</categories><comments>in Journal of Statistics Education, Volume 23, Number 2, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the Enron email corpus to study relationships in a network by applying
six different measures of centrality. Our results came out of an in-semester
undergraduate research seminar. The Enron corpus is well suited to statistical
analyses at all levels of undergraduate education. Through this note's focus on
centrality, students can explore the dependence of statistical models on
initial assumptions and the interplay between centrality measures and
hierarchical ranking, and they can use completed studies as springboards for
future research. The Enron corpus also presents opportunities for research into
many other areas of analysis, including social networks, clustering, and
natural language processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2770</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2770</id><created>2014-10-10</created><authors><author><keyname>Chen</keyname><forenames>Zheng</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Distributed SIR-Aware Opportunistic Access Control for D2D Underlaid
  Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 6 figures, to be presented at IEEE GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a distributed interference and channel-aware
opportunistic access control technique for D2D underlaid cellular networks, in
which each potential D2D link is active whenever its estimated
signal-to-interference ratio (SIR) is above a predetermined threshold so as to
maximize the D2D area spectral efficiency. The objective of our SIR-aware
opportunistic access scheme is to provide sufficient coverage probability and
to increase the aggregate rate of D2D links by harnessing interference caused
by dense underlaid D2D users using an adaptive decision activation threshold.
We determine the optimum D2D activation probability and threshold, building on
analytical expressions for the coverage probabilities and area spectral
efficiency of D2D links derived using stochastic geometry. Specifically, we
provide two expressions for the optimal SIR threshold, which can be applied in
a decentralized way on each D2D link, so as to maximize the D2D area spectral
efficiency derived using the unconditional and conditional D2D success
probability respectively. Simulation results in different network settings show
the performance gains of both SIR-aware threshold scheduling methods in terms
of D2D link coverage probability, area spectral efficiency, and average sum
rate compared to existing channel-aware access schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2771</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2771</id><created>2014-10-10</created><updated>2015-05-13</updated><authors><author><keyname>Barucca</keyname><forenames>Paolo</forenames></author><author><keyname>Rocchi</keyname><forenames>Jacopo</forenames></author><author><keyname>Marinari</keyname><forenames>Enzo</forenames></author><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>Federico</forenames></author></authors><title>Cross correlations of the American baby names</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>submitted for consideration to PNAS</comments><journal-ref>PNAS 112, 7943-7947 (2015)</journal-ref><doi>10.1073/pnas.1507143112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantitative description of cultural evolution is a challenging task. The
most difficult part of the problem is probably to find the appropriate
measurable quantities that can make more quantitative such evasive concepts as,
for example, dynamics of cultural movements, behavior patterns and traditions
of the people. A strategy to tackle this issue is to observe particular
features of human activities, i.e. cultural traits, such as names given to
newborns. We study the names of babies born in the United States of America
from 1910 to 2012. Our analysis shows that groups of different correlated
states naturally emerge in different epochs, and we are able to follow and
decrypt their evolution. While these groups of states are stable across many
decades, a sudden reorganization occurs in the last part of the twentieth
century. We think that this kind of quantitative analysis can be possibly
extended to other cultural traits: although databases covering more than one
century (as the one we used) are rare, the cultural evolution on shorter time
scales can be studied thanks to the fact that many human activities are usually
recorded in the present digital era.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2786</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2786</id><created>2014-10-10</created><authors><author><keyname>Qiao</keyname><forenames>Hanli</forenames></author></authors><title>New SVD based initialization strategy for Non-negative Matrix
  Factorization</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two problems need to be dealt with for Non-negative Matrix
Factorization (NMF): choose a suitable rank of the factorization and provide a
good initialization method for NMF algorithms. This paper aims to solve these
two problems using Singular Value Decomposition (SVD). At first we extract the
number of main components as the rank, actually this method is inspired from
[1, 2]. Second, we use the singular value and its vectors to initialize NMF
algorithm. In 2008, Boutsidis and Gollopoulos [3] provided the method titled
NNDSVD to enhance initialization of NMF algorithms. They extracted the positive
section and respective singular triplet information of the unit matrices
{C(j)}k j=1 which were obtained from singular vector pairs. This strategy aims
to use positive section to cope with negative elements of the singular vectors,
but in experiments we found that even replacing negative elements by their
absolute values could get better results than NNDSVD. Hence, we give another
method based SVD to fulfil initialization for NMF algorithms (SVD-NMF).
Numerical experiments on two face databases ORL and YALE [16, 17] show that our
method is better than NNDSVD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2792</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2792</id><created>2014-10-10</created><authors><author><keyname>Huang</keyname><forenames>Tiffany A.</forenames></author><author><keyname>Horowitz</keyname><forenames>Matanya B.</forenames></author><author><keyname>Burdick</keyname><forenames>Joel W.</forenames></author></authors><title>Convex Model Predictive Control for Vehicular Systems</title><categories>cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a method to perform Model Predictive Control (MPC)
over systems whose state is an element of $SO(n)$ for $n=2,3$. This is done
without charts or any local linearization, and instead is performed by
operating over the orbitope of rotation matrices. This results in a novel MPC
scheme without the drawbacks associated with conventional linearization
techniques. Instead, second order cone- or semidefinite-constraints on state
variables are the only requirement beyond those of a QP-scheme typical for MPC
of linear systems. Of particular emphasis is the application to aeronautical
and vehicular systems, wherein the method removes many of the transcendental
trigonometric terms associated with these systems' state space equations.
Furthermore, the method is shown to be compatible with many existing variants
of MPC, including obstacle avoidance via Mixed Integer Linear Programming
(MILP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2803</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2803</id><created>2014-10-10</created><updated>2015-03-03</updated><authors><author><keyname>Almeida</keyname><forenames>Paulo S&#xe9;rgio</forenames></author><author><keyname>Shoker</keyname><forenames>Ali</forenames></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames></author></authors><title>Efficient State-based CRDTs by Delta-Mutation</title><categories>cs.DC cs.DB cs.DS cs.PF</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CRDTs are distributed data types that make eventual consistency of a
distributed object possible and non ad-hoc. Specifically, state-based CRDTs
ensure convergence through disseminating the en- tire state, that may be large,
and merging it to other replicas; whereas operation-based CRDTs disseminate
operations (i.e., small states) assuming an exactly-once reliable dissemination
layer. We introduce Delta State Conflict-Free Replicated Datatypes
({\delta}-CRDT) that can achieve the best of both worlds: small messages with
an incremental nature, as in operation-based CRDTs, disseminated over
unreliable communication channels, as in traditional state-based CRDTs. This is
achieved by defining {\delta}-mutators to return a delta-state, typically with
a much smaller size than the full state, that is joined to both: local and
remote states. We introduce the {\delta}-CRDT framework, and we explain it
through establishing a correspondence to current state-based CRDTs. In
addition, we present an anti-entropy algorithm that ensures causal consistency,
and we introduce two {\delta}-CRDT specifications of well-known replicated
datatypes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2813</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2813</id><created>2014-10-10</created><authors><author><keyname>Greenberg</keyname><forenames>Michael</forenames></author></authors><title>Space-Efficient Manifest Contracts</title><categories>cs.PL</categories><comments>This is an extended version of a POPL'15 paper, with a great deal of
  material that does not appear in the conference paper: an exploration of the
  design space with two other space-efficient calculi and complete proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard algorithm for higher-order contract checking can lead to
unbounded space consumption and can destroy tail recursion, altering a
program's asymptotic space complexity. While space efficiency for gradual
types---contracts mediating untyped and typed code---is well studied, sound
space efficiency for manifest contracts---contracts that check stronger
properties than simple types, e.g., &quot;is a natural&quot; instead of &quot;is an
integer&quot;---remains an open problem.
  We show how to achieve sound space efficiency for manifest contracts with
strong predicate contracts. The essential trick is breaking the contract
checking down into coercions: structured, blame-annotated lists of checks. By
carefully preventing duplicate coercions from appearing, we can restore space
efficiency while keeping the same observable behavior.
  Along the way, we define a framework for space efficiency, traversing the
design space with three different space-efficient manifest calculi. We examine
the diverse correctness criteria for contract semantics; we conclude with a
coercion-based language whose contracts enjoy (galactically) bounded, sound
space consumption---they are observationally equivalent to the standard,
space-inefficient semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2828</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2828</id><created>2014-10-10</created><authors><author><keyname>Alonso</keyname><forenames>Omar</forenames></author><author><keyname>Kandylas</keyname><forenames>Vasilis</forenames></author></authors><title>A Study on Placement of Social Buttons in Web Pages</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosion of social media in the last few years, web pages nowadays
include different social network buttons where users can express if they
support or recommend content. Those social buttons are very visual and their
presentations, along with the counters, mark the importance of the social
network and the interest on the content. In this paper, we analyze the presence
of four types of social buttons (Facebook, Twitter, Google+1, and LinkedIn) in
a large collection of web pages that we tracked over a period of time. We
report on the distribution and counts along with some characteristics per
domain. Finally, we outline some research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2833</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2833</id><created>2014-10-10</created><authors><author><keyname>Kavanagh</keyname><forenames>Ryan</forenames></author><author><keyname>Madiot</keyname><forenames>Jean-Marie</forenames></author></authors><title>On Coupled Logical Bisimulation for the Lambda-Calculus</title><categories>cs.LO</categories><msc-class>03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study coupled logical bisimulation (CLB) to reason about contextual
equivalence in the lambda-calculus. CLB originates in a work by Dal Lago,
Sangiorgi and Alberti, as a tool to reason about a lambda-calculus with
probabilistic constructs. We adapt the original definition to the pure
lambda-calculus. We develop the metatheory of CLB in call-by-name and in
call-by-value, and draw comparisons with applicative bisimulation (due to
Abramsky) and logical bisimulation (due to Sangiorgi, Kobayashi and Sumii). We
also study enhancements of the bisimulation method for CLB by developing a
theory of up-to techniques for cases where the functional corresponding to
bisimulation is not necessarily monotone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2834</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2834</id><created>2014-10-10</created><authors><author><keyname>Junior</keyname><forenames>Ubiratam de Paula</forenames></author><author><keyname>Drummond</keyname><forenames>L&#xfa;cia M. A.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Daniel</forenames></author><author><keyname>Frota</keyname><forenames>Yuri</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author></authors><title>Handling Flash-Crowd Events to Improve the Performance of Web
  Applications</title><categories>cs.DC</categories><comments>Submitted to the 30th Symposium On Applied Computing (2015)</comments><journal-ref>Proceedings of the 30th ACM/SIGAPP Symposium on Applied Computing,
  769-774, 2015</journal-ref><doi>10.1145/2695664.2695839</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing can offer a set of computing resources according to users'
demand. It is suitable to be used to handle flash-crowd events in Web
applications due to its elasticity and on-demand characteristics. Thus, when
Web applications need more computing or storage capacity, they just instantiate
new resources. However, providers have to estimate the amount of resources to
instantiate to handle with the flash-crowd event. This estimation is far from
trivial since each cloud environment provides several kinds of heterogeneous
resources, each one with its own characteristics such as bandwidth, CPU, memory
and financial cost. In this paper, the Flash Crowd Handling Problem (FCHP) is
precisely defined and formulated as an integer programming problem. A new
algorithm for handling with a flash crowd named FCHP-ILS is also proposed. With
FCHP-ILS the Web applications can replicate contents in the already
instantiated resources and define the types and amount of resources to
instantiate in the cloud during a flash crowd. Our approach is evaluated
considering real flash crowd traces obtained from the related literature. We
also present a case study, based on a synthetic dataset representing
flash-crowd events in small scenarios aiming at the comparison of the proposed
approach against Amazon's Auto-Scale mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2838</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2838</id><created>2014-10-10</created><authors><author><keyname>Konukoglu</keyname><forenames>Ender</forenames></author><author><keyname>Ganz</keyname><forenames>Melanie</forenames></author></authors><title>Approximate False Positive Rate Control in Selection Frequency for
  Random Forest</title><categories>cs.LG stat.ME</categories><comments>26 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Forest has become one of the most popular tools for feature selection.
Its ability to deal with high-dimensional data makes this algorithm especially
useful for studies in neuroimaging and bioinformatics. Despite its popularity
and wide use, feature selection in Random Forest still lacks a crucial
ingredient: false positive rate control. To date there is no efficient,
principled and computationally light-weight solution to this shortcoming. As a
result, researchers using Random Forest for feature selection have to resort to
using heuristically set thresholds on feature rankings. This article builds an
approximate probabilistic model for the feature selection process in random
forest training, which allows us to compute an estimated false positive rate
for a given threshold on selection frequency. Hence, it presents a principled
way to determine thresholds for the selection of relevant features without any
additional computational load. Experimental analysis with synthetic data
demonstrates that the proposed approach can limit false positive rates on the
order of the desired values and keep false negative rates low. Results show
that this holds even in the presence of a complex correlation structure between
features. Its good statistical properties and light-weight computational needs
make this approach widely applicable to feature selection for a wide-range of
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2840</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2840</id><created>2014-10-10</created><updated>2015-07-02</updated><authors><author><keyname>Ji</keyname><forenames>Pengsheng</forenames></author><author><keyname>Jin</keyname><forenames>Jiashun</forenames></author></authors><title>Coauthorship and Citation Networks for Statisticians</title><categories>stat.AP cs.DL physics.soc-ph stat.ME</categories><msc-class>91C20, 62H30, 62P25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have collected and cleaned two network data sets: Coauthorship and
Citation networks for statisticians. The data sets are based on all research
papers published in four of the top journals in statistics from $2003$ to the
first half of $2012$. We analyze the data sets from many different
perspectives, focusing on (a) centrality, (b) community structures, and (c)
productivity, patterns and trends.
  For (a), we have identified the most prolific/collaborative/highly cited
authors. We have also identified a handful of &quot;hot&quot; papers, suggesting
&quot;Variable Selection&quot; as one of the &quot;hot&quot; areas.
  For (b), we have identified about $15$ meaningful communities or research
groups, including large-size ones such as &quot;Spatial Statistics&quot;, &quot;Large-Scale
Multiple Testing&quot;, &quot;Variable Selection&quot; as well as small-size ones such as
&quot;Dimensional Reduction&quot;, &quot;Objective Bayes&quot;, &quot;Quantile Regression&quot;, and
&quot;Theoretical Machine Learning&quot;.
  For (c), we find that over the 10-year period, both the average number of
papers per author and the fraction of self citations have been decreasing, but
the proportion of distant citations has been increasing. These suggest that the
statistics community has become increasingly more collaborative, competitive,
and globalized.
  Our findings shed light on research habits, trends, and topological patterns
of statisticians. The data sets provide a fertile ground for future researches
on or related to social networks of statisticians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2847</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2847</id><created>2014-10-10</created><updated>2015-06-11</updated><authors><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author><author><keyname>Nicholson</keyname><forenames>Patrick K.</forenames></author></authors><title>Encodings of Range Maximum-Sum Segment Queries and Applications</title><categories>cs.DS</categories><comments>19 pages + 2 page appendix, 4 figures. A shortened version of this
  paper will appear in CPM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an array A containing arbitrary (positive and negative) numbers, we
consider the problem of supporting range maximum-sum segment queries on A:
i.e., given an arbitrary range [i,j], return the subrange [i' ,j' ] \subseteq
[i,j] such that the sum of the numbers in A[i'..j'] is maximized. Chen and Chao
[Disc. App. Math. 2007] presented a data structure for this problem that
occupies {\Theta}(n) words, can be constructed in {\Theta}(n) time, and
supports queries in {\Theta}(1) time. Our first result is that if only the
indices [i',j'] are desired (rather than the maximum sum achieved in that
subrange), then it is possible to reduce the space to {\Theta}(n) bits,
regardless the numbers stored in A, while retaining the same construction and
query time. We also improve the best known space lower bound for any data
structure that supports range maximum-sum segment queries from n bits to
1.89113n - {\Theta}(lg n) bits, for sufficiently large values of n. Finally, we
provide a new application of this data structure which simplifies a previously
known linear time algorithm for finding k-covers: i.e., given an array A of n
numbers and a number k, find k disjoint subranges [i_1 ,j_1 ],...,[i_k ,j_k ],
such that the total sum of all the numbers in the subranges is maximized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2861</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2861</id><created>2014-10-10</created><authors><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Multiuser Joint Energy-Bandwidth Allocation with Energy Harvesting -
  Part I: Optimum Algorithm &amp; Multiple Point-to-Point Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop optimal energy-bandwidth allocation algorithms in
fading channels for multiple energy harvesting transmitters, each may
communicate with multiple receivers via orthogonal channels. We first assume
that the side information of both the channel states and the energy harvesting
states is known for $K$ time slots {\em a priori}, and the battery capacity and
the maximum transmission power in each time slot are bounded. The objective is
to maximize the weighted sum-rate of all transmitters over the $K$ time slots
by assigning the transmission power and bandwidth for each transmitter in each
slot. The problem is formulated as a convex optimization problem with ${\cal
O}(MK)$ constraints, where $M$ is the number of the receivers, making it hard
to solve with a generic convex solver. An iterative algorithm is proposed that
alternatively solves two subproblems in each iteration. The convergence and the
optimality of this algorithm are also shown. We then consider the special case
that each transmitter only communicates with one receiver and the objective is
to maximize the total throughput. We develop efficient algorithms for solving
the two subproblems and the optimal energy-bandwidth allocation can be obtained
with an overall complexity of ${\cal O}(MK^2)$. Moreover, a heuristic algorithm
is also proposed for energy-bandwidth allocation based on causal information of
channel and energy harvesting states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2862</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2862</id><created>2014-10-10</created><authors><author><keyname>Dowsley</keyname><forenames>Rafael</forenames></author><author><keyname>Nascimento</keyname><forenames>Anderson C. A.</forenames></author></authors><title>On the Oblivious Transfer Capacity of Generalized Erasure Channels
  against Malicious Adversaries</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noisy channels are a powerful resource for cryptography as they can be used
to obtain information-theoretically secure key agreement, commitment and
oblivious transfer protocols, among others. Oblivious transfer (OT) is a
fundamental primitive since it is complete for secure multi-party computation,
and the OT capacity characterizes how efficiently a channel can be used for
obtaining string oblivious transfer. Ahlswede and Csisz\'{a}r (\emph{ISIT'07})
presented upper and lower bounds on the OT capacity of generalized erasure
channels (GEC) against passive adversaries. In the case of GEC with erasure
probability at least 1/2, the upper and lower bounds match and therefore the OT
capacity was determined. It was later proved by Pinto et al. (\emph{IEEE Trans.
Inf. Theory 57(8)}) that in this case there is also a protocol against
malicious adversaries achieving the same lower bound, and hence the OT capacity
is identical for passive and malicious adversaries. In the case of GEC with
erasure probability smaller than 1/2, the known lower bound against passive
adversaries that was established by Ahlswede and Csisz\'{a}r does not match
their upper bound and it was unknown whether this OT rate could be achieved
against malicious adversaries as well. In this work we show that there is a
protocol against malicious adversaries achieving the same OT rate that was
obtained against passive adversaries.
  In order to obtain our results we introduce a novel use of interactive
hashing that is suitable for dealing with the case of low erasure probability
($p^* &lt;1/2$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2867</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2867</id><created>2014-10-10</created><authors><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Multiuser Joint Energy-Bandwidth Allocation with Energy Harvesting -
  Part II: Multiple Broadcast Channels &amp; Proportional Fairness</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the energy-bandwidth allocation for a network with
multiple broadcast channels, where the transmitters access the network
orthogonally on the assigned frequency band and each transmitter communicates
with multiple receivers orthogonally or non-orthogonally. We assume that the
energy harvesting state and channel gain of each transmitter can be predicted
for $K$ slots {\em a priori}. To maximize the weighted throughput, we formulate
an optimization problem with $O(MK)$ constraints, where $M$ is the number of
the receivers, and decompose it into the energy and bandwidth allocation
subproblems. In order to use the iterative algorithm proposed in [1] to solve
the problem, we propose efficient algorithms to solve the two subproblems, so
that the optimal energy-bandwidth allocation can be obtained with an overall
complexity of ${\cal O}(MK^2)$, even though the problem is non-convex when the
broadcast channel is non-orthogonal. For the orthogonal broadcast channel, we
further formulate a proportionally-fair (PF) throughput maximization problem
and derive the equivalence conditions such that the optimal solution can be
obtained by solving a weighted throughput maximization problem. Further, the
algorithm to obtain the proper weights is proposed. Simulation results show
that the proposed algorithm can make efficient use of the harvested energy and
the available bandwidth, and achieve significantly better performance than some
heuristic policies for energy and bandwidth allocation. Moreover, it is seen
that with energy-harvesting transmitters, non-orthogonal broadcast offers
limited gain over orthogonal broadcast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2871</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2871</id><created>2014-10-10</created><updated>2015-02-03</updated><authors><author><keyname>Raja</keyname><forenames>S. V. Kasmir</forenames></author><author><keyname>Rajitha</keyname><forenames>V.</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Meenakshi</forenames></author></authors><title>An Ontology for Comprehensive Tutoring of Euphonic Conjunctions of
  Sanskrit Grammar</title><categories>cs.CL</categories><journal-ref>European Journal of Scientific Research, ISSN 1450-216X /
  1450-202X, Vol. 124, No. 4, September 2014, pp 460-467</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Euphonic conjunctions (sandhis) form a very important aspect of Sanskrit
morphology and phonology. The traditional and modern methods of studying about
euphonic conjunctions in Sanskrit follow different methodologies. The former
involves a rigorous study of the Paninian system embodied in Panini's
Ashtadhyayi, while the latter usually involves the study of a few important
sandhi rules with the use of examples. The former is not suitable for
beginners, and the latter, not sufficient to gain a comprehensive understanding
of the operation of sandhi rules. This is so since there are not only numerous
sandhi rules and exceptions, but also complex precedence rules involved. The
need for a new ontology for sandhi-tutoring was hence felt. This work presents
a comprehensive ontology designed to enable a student-user to learn in stages
all about euphonic conjunctions and the relevant aphorisms of Sanskrit grammar
and to test and evaluate the progress of the student-user. The ontology forms
the basis of a multimedia sandhi tutor that was given to different categories
of users including Sanskrit scholars for extensive and rigorous testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2881</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2881</id><created>2014-10-10</created><updated>2014-11-23</updated><authors><author><keyname>Schieler</keyname><forenames>Curt</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>The Henchman Problem: Measuring Secrecy by the Minimum Distortion in a
  List</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. Updated version
  with citation typo fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new measure of information-theoretic secrecy based on
rate-distortion theory and study it in the context of the Shannon cipher
system. Whereas rate-distortion theory is traditionally concerned with a single
reconstruction sequence, in this work we suppose that an eavesdropper produces
a list of $2^{nR_{\sf L}}$ reconstruction sequences and measure secrecy by the
minimum distortion over the entire list. We show that this setting is
equivalent to one in which an eavesdropper must reconstruct a single sequence,
but also receives side information about the source sequence and public message
from a rate-limited henchman (a helper for an adversary). We characterize the
optimal tradeoff of secret key rate, list rate, and eavesdropper distortion.
The solution hinges on a problem of independent interest: lossy compression of
a codeword drawn uniformly from a random codebook. We also characterize the
solution to the lossy communication version of the problem in which distortion
is allowed at the legitimate receiver. The analysis in both settings is greatly
aided by a recent technique for proving source coding results with the use of a
likelihood encoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2882</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2882</id><created>2014-10-10</created><authors><author><keyname>Grilo</keyname><forenames>Alex B.</forenames></author><author><keyname>Kerenidis</keyname><forenames>Iordanis</forenames></author><author><keyname>Sikora</keyname><forenames>Jamie</forenames></author></authors><title>QMA with subset state witnesses</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class QMA plays a fundamental role in quantum complexity theory and it
has found surprising connections to condensed matter physics and in particular
in the study of the minimum energy of quantum systems. In this paper, we
further investigate the class QMA and its related class QCMA by asking what
makes quantum witnesses potentially more powerful than classical ones. We
provide a definition of a new class, SQMA, where we restrict the possible
quantum witnesses to the &quot;simpler&quot; subset states, i.e. a uniform superposition
over the elements of a subset of n-bit strings. Surprisingly, we prove that
this class is equal to QMA, hence providing a new characterisation of the class
QMA. We also prove the analogous result for QMA(2) and describe a new complete
problem for QMA and a stronger lower bound for the class QMA$_1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2887</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2887</id><created>2014-10-09</created><authors><author><keyname>Azim</keyname><forenames>Zubair Al</forenames></author><author><keyname>Fong</keyname><forenames>Xuanyao</forenames></author><author><keyname>Ostler</keyname><forenames>Thomas</forenames></author><author><keyname>Chantrell</keyname><forenames>Roy</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Laser Induced Magnetization Reversal for Detection in Optical
  Interconnects</title><categories>cond-mat.mtrl-sci cs.ET</categories><doi>10.1109/LED.2014.2364232</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical interconnect has emerged as the front-runner to replace electrical
interconnect especially for off-chip communication. However, a major drawback
with optical interconnects is the need for photodetectors and amplifiers at the
receiver, implemented usually by direct bandgap semiconductors and analog CMOS
circuits, leading to large energy consumption and slow operating time. In this
article, we propose a new optical interconnect architecture that uses a
magnetic tunnel junction (MTJ) at the receiver side that is switched by
femtosecond laser pulses. The state of the MTJ can be sensed using simple
digital CMOS latches, resulting in significant improvement in energy
consumption. Moreover, magnetization in the MTJ can be switched on the
picoseconds time-scale and our design can operate at a speed of 5 Gbits/sec for
a single link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2889</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2889</id><created>2014-10-10</created><authors><author><keyname>Rafique</keyname><forenames>Omar</forenames></author></authors><title>Area Versus Speed Trade-off Analysis of a WiMAX Deinterleaver Circuit
  Design</title><categories>cs.OH</categories><comments>four pages, two figures and six tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trade-off is one of the main design parameters in the field of electronic
circuit design. Whereas smaller electronics devices which use less hardware due
to techniques like hardware multiplexing or due to smaller devices created due
to techniques developed by nanotechnology and MEMS, are more appealing, a
trade-off between area, power and speed is inevitable. This paper analyses the
trade-off in the design of WiMAX deinterleaver. The main aim is to reduce the
hardware utilization in a deinterleaver but speed and power consumption are
important parameters which cannot be overlooked.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2901</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2901</id><created>2014-10-07</created><authors><author><keyname>Genet</keyname><forenames>Thomas</forenames></author></authors><title>Towards Static Analysis of Functional Programs using Tree Automata
  Completion</title><categories>cs.LO cs.FL cs.PL</categories><comments>Proceedings of WRLA'14. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first step of a wider research effort to apply tree
automata completion to the static analysis of functional programs. Tree
Automata Completion is a family of techniques for computing or approximating
the set of terms reachable by a rewriting relation. The completion algorithm we
focus on is parameterized by a set E of equations controlling the precision of
the approximation and influencing its termination. For completion to be used as
a static analysis, the first step is to guarantee its termination. In this
work, we thus give a sufficient condition on E and T(F) for completion
algorithm to always terminate. In the particular setting of functional
programs, this condition can be relaxed into a condition on E and T(C) (terms
built on the set of constructors) that is closer to what is done in the field
of static analysis, where abstractions are performed on data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2903</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2903</id><created>2014-10-10</created><updated>2014-10-31</updated><authors><author><keyname>Carlet</keyname><forenames>Claude</forenames></author><author><keyname>Gong</keyname><forenames>Guang</forenames></author><author><keyname>Tan</keyname><forenames>Yin</forenames></author></authors><title>Quadratic Zero-Difference Balanced Functions, APN Functions and Strongly
  Regular Graphs</title><categories>cs.IT math.CO math.IT</categories><msc-class>11T06, 11T71, 05E30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $F$ be a function from $\mathbb{F}_{p^n}$ to itself and $\delta$ a
positive integer. $F$ is called zero-difference $\delta$-balanced if the
equation $F(x+a)-F(x)=0$ has exactly $\delta$ solutions for all non-zero
$a\in\mathbb{F}_{p^n}$. As a particular case, all known quadratic planar
functions are zero-difference 1-balanced; and some quadratic APN functions over
$\mathbb{F}_{2^n}$ are zero-difference 2-balanced. In this paper, we study the
relationship between this notion and differential uniformity; we show that all
quadratic zero-difference $\delta$-balanced functions are differentially
$\delta$-uniform and we investigate in particular such functions with the form
$F=G(x^d)$, where $\gcd(d,p^n-1)=\delta +1$ and where the restriction of $G$ to
the set of all non-zero $(\delta +1)$-th powers in $\mathbb{F}_{p^n}$ is an
injection. We introduce new families of zero-difference $p^t$-balanced
functions. More interestingly, we show that the image set of such functions is
a regular partial difference set, and hence yields strongly regular graphs;
this generalizes the constructions of strongly regular graphs using planar
functions by Weng et al. Using recently discovered quadratic APN functions on
$\mathbb{F}_{2^8}$, we obtain $15$ new $(256, 85, 24, 30)$ negative Latin
square type strongly regular graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2904</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2904</id><created>2014-10-10</created><authors><author><keyname>Lou</keyname><forenames>Chung-Yu</forenames></author><author><keyname>Daneshrad</keyname><forenames>Babak</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Optimizing Pilot Length for a Go/No-Go Decision in Two-State Block
  Fading Channels with Feedback</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach where each user independently seeks to minimize the
amount of time that they occupy the channel. Essentially, we seek to minimize
the number of transmitted symbols required to communicate a packet assuming
variable-length coding with feedback. Users send a pilot sequence to estimate
the channel quality and decide whether to proceed with a transmission or wait
for the next opportunity. Thus a user may choose to leave the channel even
though it has already gained access, in order to increase the network
throughput and also save its own energy resources. This paper optimizes the
number of pilots and the channel identification threshold to minimize the total
number of transmitted symbols (including pilots) required to communicate the
packet. We prove a sufficient condition for the optimal pilot length and the
channel identification threshold. This optimal parameter pair is solved
numerically and the reduction in channel occupancy is shown for various channel
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2910</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2910</id><created>2014-10-10</created><authors><author><keyname>Clarke</keyname><forenames>Daoud</forenames></author></authors><title>Riesz Logic</title><categories>cs.LO cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Riesz Logic, whose models are abelian lattice ordered groups,
which generalise Riesz spaces (vector lattices), and show soundness and
completeness. Our motivation is to provide a logic for distributional semantics
of natural language, where words are typically represented as elements of a
vector space whose dimensions correspond to contexts in which words may occur.
This basis provides a lattice ordering on the space, and this ordering may be
interpreted as &quot;distributional entailment&quot;. Several axioms of Riesz Logic are
familiar from Basic Fuzzy Logic, and we show how the models of these two logics
may be related; Riesz Logic may thus be considered a new fuzzy logic. In
addition to applications in natural language processing, there is potential for
applying the theory to neuro-fuzzy systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2920</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2920</id><created>2014-10-10</created><authors><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Gal</keyname><forenames>Anna</forenames></author><author><keyname>Rawat</keyname><forenames>Ankit Singh</forenames></author><author><keyname>Song</keyname><forenames>Zhao</forenames></author></authors><title>Batch Codes through Dense Graphs without Short Cycles</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a large database of $n$ data items that need to be stored using $m$
servers. We study how to encode information so that a large number $k$ of read
requests can be performed in parallel while the rate remains constant (and
ideally approaches one). This problem is equivalent to the design of multiset
Batch Codes introduced by Ishai, Kushilevitz, Ostrovsky and Sahai [17].
  We give families of multiset batch codes with asymptotically optimal rates of
the form $1-1/\text{poly}(k)$ and a number of servers $m$ scaling polynomially
in the number of read requests $k$. An advantage of our batch code
constructions over most previously known multiset batch codes is explicit and
deterministic decoding algorithms and asymptotically optimal fault tolerance.
  Our main technical innovation is a graph-theoretic method of designing
multiset batch codes using dense bipartite graphs with no small cycles. We
modify prior graph constructions of dense, high-girth graphs to obtain our
batch code results. We achieve close to optimal tradeoffs between the
parameters for bipartite graph based batch codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2924</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2924</id><created>2014-10-10</created><authors><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author><author><keyname>Kwasinski</keyname><forenames>Andres</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Power allocation with stackelberg game in femtocell networks: a
  self-learning approach</title><categories>cs.NI cs.GT</categories><comments>2014 Eleventh Annual IEEE International Conference on Sensing,
  Communication, and Networking (SECON) (SECON 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the energy-efficient power allocation for a two-tier,
underlaid femtocell network. The behaviors of the Macrocell Base Station (MBS)
and the Femtocell Users (FUs) are modeled hierarchically as a Stackelberg game.
The MBS guarantees its own QoS requirement by charging the FUs individually
according to the cross-tier interference, and the FUs responds by controlling
the local transmit power non-cooperatively. Due to the limit of information
exchange in intra- and inter-tiers, a self-learning based strategy-updating
mechanism is proposed for each user to learn the equilibrium strategies. In the
same Stackelberg-game framework, two different scenarios based on the
continuous and discrete power profiles for the FUs are studied, respectively.
The self-learning schemes in the two scenarios are designed based on the local
best response. By studying the properties of the proposed game in the two
situations, the convergence property of the learning schemes is provided. The
simulation results are provided to support the theoretical finding in different
situations of the proposed game, and the efficiency of the learning schemes is
validated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2926</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2926</id><created>2014-10-10</created><updated>2016-01-03</updated><authors><author><keyname>Vincent-Lamarre</keyname><forenames>Philippe</forenames></author><author><keyname>Boivin</keyname><forenames>Jade</forenames></author><author><keyname>Gargouri</keyname><forenames>Yassine</forenames></author><author><keyname>Lariviere</keyname><forenames>Vincent</forenames></author><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author></authors><title>Estimating Open Access Mandate Effectiveness: The MELIBEA Score</title><categories>cs.DL</categories><comments>27 pages, 13 figures, 3 tables, 40 references, 7761 words</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MELIBEA is a Spanish database that uses a composite formula with eight
weighted conditions to estimate the effectiveness of Open Access mandates
(registered in ROARMAP). We analyzed 68 mandated institutions for publication
years 2011-2013 to determine how well the MELIBEA score and its individual
conditions predict what percentage of published articles indexed by Web of
Knowledge is deposited in each institution's OA repository, and when. We found
a small but significant positive correlation (0.18) between MELIBEA score and
deposit percentage. We also found that for three of the eight MELIBEA
conditions (deposit timing, internal use, and opt-outs), one value of each was
strongly associated with deposit percentage or deposit latency (immediate
deposit required, deposit required for performance evaluation, unconditional
opt-out allowed for the OA requirement but no opt-out for deposit requirement).
When we updated the initial values and weights of the MELIBEA formula for
mandate effectiveness to reflect the empirical association we had found, the
score's predictive power doubled (.36). There are not yet enough OA mandates to
test further mandate conditions that might contribute to mandate effectiveness,
but these findings already suggest that it would be useful for future mandates
to adopt these three conditions so as to maximize their effectiveness, and
thereby the growth of OA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2931</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2931</id><created>2014-10-10</created><updated>2015-11-17</updated><authors><author><keyname>Mallada</keyname><forenames>Enrique</forenames></author><author><keyname>Zhao</keyname><forenames>Changhong</forenames></author><author><keyname>Low</keyname><forenames>Steven H.</forenames></author></authors><title>Optimal load-side control for frequency regulation in smart grids</title><categories>math.OC cs.MA cs.SY</categories><comments>Under revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequency control rebalances supply and demand while maintaining the network
state within operational margins. It is implemented using fast ramping reserves
that are expensive and wasteful, and which are expected to grow with the
increasing penetration of renewables. The most promising solution to this
problem is the use of demand response, i.e. load participation in frequency
control. Yet it is still unclear how to efficiently integrate load
participation without introducing instabilities and violating operational
constraints.
  In this paper we present a comprehensive load-side frequency control
mechanism that can maintain the grid within operational constraints. In
particular, our controllers can rebalance supply and demand after disturbances,
restore the frequency to its nominal value and preserve inter-area power flows.
Furthermore, our controllers are distributed (unlike the currently implemented
frequency control), can allocate load updates optimally, and can maintain line
flows within thermal limits. We prove that such a distributed load-side control
is globally asymptotically stable and robust to unknown load parameters. We
illustrate its effectiveness through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2951</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2951</id><created>2014-10-11</created><authors><author><keyname>Han</keyname><forenames>Weiliang</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Ge</keyname><forenames>Jian-hua</forenames></author></authors><title>Cyclic Delay Transmission for Vector OFDM Systems</title><categories>cs.IT math.IT</categories><comments>26 pages, 8 figures, Transaction on Wireless Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single antenna vector OFDM (V-OFDM) system has been proposed and investigated
in the past. It contains the conventional OFDM and the single carrier frequency
domain equalizer (SC-FDE) as two special cases and is flexible to choose any
number of symbols in intersymbol interference (ISI) by choosing a proper vector
size. In this paper, we develop cyclic delay diversity (CDD) transmission for
V-OFDM when there are multiple transmit antennas (CDD-V-OFDM). Similar to
CDD-OFDM systems, CDD-V-OFDM can also collect both spatial and multipath
diversities. Since V-OFDM first converts a single input single output (SISO)
ISI channel to a multi-input and multi-output (MIMO) ISI channel of
order/length $K$ times less, where $K$ is the vector size, for a given
bandwidth, the CDD-V-OFDM can accommodate $K$ times more transmit antennas than
the CDD-OFDM does to collect all the spatial and multipath diversities. This
property will specially benefit a massive MIMO system. We show that with the
linear MMSE equalizer at each subcarrier, the CDD-V-OFDM achieves diversity
order $d_{\text{CDD-V-OFDM}}^{\text{MMSE}} = \min \{ \lfloor 2^{-R}K \rfloor,
N_t L \} +1$, where $R$ is the transmission rate, $N_t$ is the number of
transmit antennas, and $L$ is the ISI channel length between each transmit and
receive antenna pair. Simulations are presented to illustrate our theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2954</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2954</id><created>2014-10-11</created><authors><author><keyname>Luo</keyname><forenames>Biao</forenames></author><author><keyname>Liu</keyname><forenames>Derong</forenames></author><author><keyname>Huang</keyname><forenames>Tingwen</forenames></author></authors><title>Q-learning for Optimal Control of Continuous-time Systems</title><categories>cs.SY stat.ML</categories><comments>Submitted for Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, two Q-learning (QL) methods are proposed and their convergence
theories are established for addressing the model-free optimal control problem
of general nonlinear continuous-time systems. By introducing the Q-function for
continuous-time systems, policy iteration based QL (PIQL) and value iteration
based QL (VIQL) algorithms are proposed for learning the optimal control policy
from real system data rather than using mathematical system model. It is proved
that both PIQL and VIQL methods generate a nonincreasing Q-function sequence,
which converges to the optimal Q-function. For implementation of the QL
algorithms, the method of weighted residuals is applied to derived the
parameters update rule. The developed PIQL and VIQL algorithms are essentially
off-policy reinforcement learning approachs, where the system data can be
collected arbitrary and thus the exploration ability is increased. With the
data collected from the real system, the QL methods learn the optimal control
policy offline, and then the convergent control policy will be employed to real
system. The effectiveness of the developed QL algorithms are verified through
computer simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2959</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2959</id><created>2014-10-11</created><updated>2014-10-14</updated><authors><author><keyname>Javed</keyname><forenames>Mohammed</forenames></author><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>B. B.</forenames></author></authors><title>Direct Processing of Document Images in Compressed Domain</title><categories>cs.CV</categories><comments>2014 Fourth IDRBT Doctoral Colloquium, December 11-12, 2014
  Hyderabad, India</comments><doi>10.13140/2.1.2523.9042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid increase in the volume of Big data of this digital era, fax
documents, invoices, receipts, etc are traditionally subjected to compression
for the efficiency of data storage and transfer. However, in order to process
these documents, they need to undergo the stage of decompression which indents
additional computing resources. This limitation induces the motivation to
research on the possibility of directly processing of compressed images. In
this research paper, we summarize the research work carried out to perform
different operations straight from run-length compressed documents without
going through the stage of decompression. The different operations demonstrated
are feature extraction; text-line, word and character segmentation; document
block segmentation; and font size detection, all carried out in the compressed
version of the document. Feature extraction methods demonstrate how to extract
the conventionally defined features such as projection profile, run-histogram
and entropy, directly from the compressed document data. Document segmentation
involves the extraction of compressed segments of text-lines, words and
characters using the vertical and horizontal projection profile features.
Further an attempt is made to segment randomly a block of interest from the
compressed document and subsequently facilitate absolute and relative
characterization of the segmented block which finds real time applications in
automatic processing of Bank Cheques, Challans, etc, in compressed domain.
Finally an application to detect font size at text line level is also
investigated. All the proposed algorithms are validated experimentally with
sufficient data set of compressed documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2960</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2960</id><created>2014-10-11</created><updated>2014-10-21</updated><authors><author><keyname>Yan</keyname><forenames>Shihao</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author><author><keyname>Nevat</keyname><forenames>Ido</forenames></author><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author></authors><title>Location Spoofing Detection for VANETs by a Single Base Station in
  Rician Fading Channels</title><categories>cs.IT cs.CR math.IT</categories><comments>6 pages, 5 figures, Added further clarification on constraints
  imposed on the detection minimization strategy. Minor typos fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we examine the performance of a Location Spoofing Detection
System (LSDS) for vehicular networks in the realistic setting of Rician fading
channels. In the LSDS, an authorized Base Station (BS) equipped with multiple
antennas utilizes channel observations to identify a malicious vehicle, also
equipped with multiple antennas, that is spoofing its location. After deriving
the optimal transmit power and the optimal directional beamformer of a
potentially malicious vehicle, robust theoretical analysis and detailed
simulations are conducted in order to determine the impact of key system
parameters on the LSDS performance. Our analysis shows how LSDS performance
increases as the Rician K-factor of the channel between the BS and legitimate
vehicles increases, or as the number of antennas at the BS or legitimate
vehicle increases. We also obtain the counter-intuitive result that the
malicious vehicle's optimal number of antennas conditioned on its optimal
directional beamformer is equal to the legitimate vehicle's number of antennas.
The results we provide here are important for the verification of location
information reported in IEEE 1609.2 safety messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2963</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2963</id><created>2014-10-11</created><authors><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Xiao</keyname><forenames>Chengshan</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Linear Precoding for the MIMO Multiple Access Channel with Finite
  Alphabet Inputs and Statistical CSI</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Wireless Communications. arXiv admin
  note: substantial text overlap with arXiv:1401.5401</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the design of linear precoders for the
multiple-input multiple-output (MIMO) multiple access channel (MAC). We assume
that statistical channel state information (CSI) is available at the
transmitters and consider the problem under the practical finite alphabet input
assumption. First, we derive an asymptotic (in the large system limit)
expression for the weighted sum rate (WSR) of the MIMO MAC with finite alphabet
inputs and Weichselberger's MIMO channel model. Subsequently, we obtain the
optimal structures of the linear precoders of the users maximizing the
asymptotic WSR and an iterative algorithm for determining the precoders. We
show that the complexity of the proposed precoder design is significantly lower
than that of MIMO MAC precoders designed for finite alphabet inputs and
instantaneous CSI. Simulation results for finite alphabet signalling indicate
that the proposed precoder achieves significant performance gains over existing
precoder designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2980</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2980</id><created>2014-10-11</created><authors><author><keyname>Elango</keyname><forenames>B.</forenames></author><author><keyname>Rajendran</keyname><forenames>P.</forenames></author></authors><title>Whole counting vs. whole-normalized counting: A country level
  comparative study of internationally collaborated papers on Tribology</title><categories>cs.DL</categories><comments>Submitted to the Current Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this study is to compare the changing behavior of two counting
methods (whole counting and whole-normalized counting) and inflation rate at
country level research productivity and impact. For this, publication data on
tribology research published between 1998 and 2012 from SCOPUS has been used.
Only internationally collaborated papers are considered for comparison between
two counting methods. The result of correlation tests shows that there is
highly correlation in all the four indicators between the two counting methods.
However, the result of t-test shows that there is significant difference in the
three indicators (paper count, citation count and h-index) between the two
counting methods. This study concludes that whole-normalized counting
(fractional) is the better choice for publication and citations counting at the
country level assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2988</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2988</id><created>2014-10-11</created><authors><author><keyname>Sahoo</keyname><forenames>Jayakrushna</forenames></author><author><keyname>Das</keyname><forenames>Ashok Kumar</forenames></author><author><keyname>Goswami</keyname><forenames>A.</forenames></author></authors><title>An Algorithm for Mining High Utility Closed Itemsets and Generators</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Traditional association rule mining based on the support-confidence framework
provides the objective measure of the rules that are of interest to users.
However, it does not reflect the utility of the rules. To extract non-redundant
association rules in support-confidence framework frequent closed itemsets and
their generators play an important role. To extract non-redundant association
rules among high utility itemsets, high utility closed itemsets (HUCI) and
their generators should be extracted in order to apply traditional
support-confidence framework. However, no efficient method exists at present
for mining HUCIs with their generators. This paper addresses this issue. A
post-processing algorithm, called the HUCI-Miner, is proposed to mine HUCIs
with their generators. The proposed algorithm is implemented using both
synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2995</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2995</id><created>2014-10-11</created><updated>2015-02-12</updated><authors><author><keyname>Hu</keyname><forenames>Wuhua</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author><author><keyname>Harilal</keyname><forenames>Athul</forenames></author><author><keyname>Xiao</keyname><forenames>Gaoxi</forenames></author></authors><title>Network infection source identification under the SIRI model</title><categories>physics.soc-ph cs.SI math.DS</categories><comments>5 pages, 3 figures; to present in ICASSP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of identifying a single infection source in a network
under the susceptible-infected-recovered-infected (SIRI) model. We describe the
infection model via a state-space model, and utilizing a state propagation
approach, we derive an algorithm known as the heterogeneous infection spreading
source (HISS) estimator, to infer the infection source. The HISS estimator uses
the observations of node states at a particular time, where the elapsed time
from the start of the infection is unknown. It is able to incorporate side
information (if any) of the observed states of a subset of nodes at different
times, and of the prior probability of each infected or recovered node to be
the infection source. Simulation results suggest that the HISS estimator
outperforms the dynamic message pass- ing and Jordan center estimators over a
wide range of infection and reinfection rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2997</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2997</id><created>2014-10-11</created><authors><author><keyname>Razeghi</keyname><forenames>Behrooz</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author><author><keyname>Seyedin</keyname><forenames>Seyed Alireza</forenames></author></authors><title>Coverage Region Analysis for MIMO Amplify-and-Forward Relay Channel with
  the Source to Destination Link</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in Seventh International Symposium on
  Telecommunications (IST'2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study and analyze coverage region for half-duplex
multiple-input multiple-output (MIMO) relay channel with amplify-and-forward
(AF) strategy at the relay station. By assuming mixed Rayleigh and Rician
fading channels with two different relay station situations, we consider the
objective of maximizing coverage region for a given transmission rate and find
the optimal relay location in the sense of maximizing coverage region. using
Monte Carlo simulations, the coverage region and capacity bounds are shown for
different fading cases and different relay station locations. Finally, we
compare our results with previous ones obtained for decode-and-forward (DF)
MIMO relay channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3013</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3013</id><created>2014-10-11</created><authors><author><keyname>Kocak</keyname><forenames>Mustafa Anil</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Communicating Lists Over a Noisy Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to 52nd Annual Allerton Conference on Communication,
  Control, and Computing</comments><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers a communication scenario where the transmitter chooses a
list of size K from a total of M messages to send over a noisy communication
channel, the receiver generates a list of size L and communication is
considered successful if the intersection of the lists at two terminals has
cardinality greater than a threshold T. In traditional communication systems
K=L=T=1. The fundamental limits of this setup in terms of K, L, T and the
Shannon capacity of the channel between the terminals are examined.
Specifically, necessary and/or sufficient conditions for asymptotically error
free communication are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3015</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3015</id><created>2014-10-11</created><authors><author><keyname>Knychala</keyname><forenames>Piotr</forenames></author><author><keyname>Banaszak</keyname><forenames>Michal</forenames></author></authors><title>Heuristic Monte Carlo Method Applied to Cooperative Motion Algorithm for
  Binary Lattice Fluid</title><categories>cond-mat.stat-mech cs.CE</categories><comments>Accpeted in Journal of Computational Methods in Sciences and
  Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Cooperative Motion Algorithm is an e?cient lattice method to simulate
dense polymer systems and is often used with two di?erent criteria to generate
a Markov chain in the con?guration space. While the ?rst method is the
well-established Metropolis algorithm, the other one is an heuristic algorithm
which needs justi?cation. As an introductory step towards justi?cation for the
3D lattice polymers, we study a simple system which is the binary equimolar uid
on a 2D triangular lattice. Since all lattice sites are occupied only selected
type of motions are considered, such the vacancy movements, swapping
neighboring lattice sites (Kawasaki dynamics) and cooperative loops. We compare
both methods, calculating the energy as well as heat capacity as a function of
temperature. The critical temperature, which was determined using the Binder
cumulant, was the same for all methods with the simulation accuracy and in
agreement with the exact critical temperature for the Ising model on the 2D
triangular lattice. In order to achieve reliable results at low temperatures we
employ the parallel tempering algorithm which enables simultaneous simulations
of replicas of the system in a wide range of temperatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3016</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3016</id><created>2014-10-11</created><authors><author><keyname>Dziecielski</keyname><forenames>Michal</forenames></author><author><keyname>Lewandowski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Banaszak</keyname><forenames>Michal</forenames></author></authors><title>Phase Diagram of Diblock Copolymer Melt in Dimension d=5</title><categories>cond-mat.soft cs.CE</categories><journal-ref>Computational Methods in Science and Technology 17(1-2, 17-23
  (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the self-consistent field theory (SCFT) in spherical unit cells of
various dimensionalities, D, a phase diagram of a diblock, A-b-B, is calculated
in 5 dimensional space, d = 5. This is an extension of a previous work for d =
4. The phase diagram is parameterized by the chain composition, f, and
incompatibility between A and B , quantified by the product \c{hi} N. We
predict 5 stable nanophases: layers, cylinders, 3 D spherical cells, 4D
spherical cells, and 5D spherical cells. In the strong segregation limit, that
is for large \c{hi}N, the order-order transition compositions are determined by
the strong segregation theory (SST) in its simplest form. While the predictions
of the SST theory are close to the corresponding SCFT extrapolations for d=4,
the extrapolations for d=5 significantly differ from them. We find that the S5
nanophase is stable in a narrow strip between the ordered S4 nanophase and the
disordered phase. The calculated order-disorder transition lines depend weakly
on d, as expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3018</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3018</id><created>2014-10-11</created><authors><author><keyname>Nabiyev</keyname><forenames>Rifkat I.</forenames></author><author><keyname>Ziatdinov</keyname><forenames>Rushan</forenames></author></authors><title>A mathematical design and evaluation of Bernstein-Bezier curves' shape
  features using the laws of technical aesthetics</title><categories>cs.GR</categories><journal-ref>Mathematical Design &amp; Technical Aesthetics, 2014, Vol. 2, No. 1,
  pp. 6-13</journal-ref><doi>10.13187/md.2014.2.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present some notes on the definition of mathematical design as well as on
the methods of mathematical modeling which are used in the process of the
artistic design of the environment and its components. For the first time in
the field of geometric modeling, we perform an aesthetic analysis of planar
Bernstein-Bezier curves from the standpoint of the laws of technical
aesthetics. The shape features of the curve segments' geometry were evaluated
using the following criteria: conciseness-integrity, expressiveness,
proportional consistency, compositional balance, structural organization,
imagery, rationality, dynamism, scale, flexibility and harmony. In the
non-Russian literature, Bernstein-Bezier curves using a monotonic curvature
function (i.e., a class A Bezier curve) are considered to be fair (i.e.,
beautiful) curves, but their aesthetic analysis has never been performed. The
aesthetic analysis performed by the authors of this work means that this is no
longer the case. To confirm the conclusions of the authors' research, a survey
of the &quot;aesthetic appropriateness&quot; of certain Bernstein-Bezier curve segments
was conducted among 240 children, aged 14-17. The results of this survey have
shown themselves to be in full accordance with the authors' results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3022</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3022</id><created>2014-10-11</created><updated>2016-02-15</updated><authors><author><keyname>Fulek</keyname><forenames>Radoslav</forenames></author></authors><title>Toward the Hanani-Tutte Theorem for Clustered Graphs</title><categories>cs.CG math.CO</categories><comments>revised version, new Figure 15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The weak variant of Hanani-Tutte theorem says that a graph is planar, if it
can be drawn in the plane so that every pair of edges cross an even number of
times. Moreover, we can turn such a drawing into an embedding without changing
the order in which edges leave the vertices. We prove a generalization of the
weak Hanani-Tutte theorem that also easily implies the monotone variant of the
weak Hanani-Tutte theorem by Pach and T\'oth. Thus, our result can be thought
of as a common generalization of these two neat results. In other words, we
prove the weak Hanani-Tutte theorem for strip clustered graphs, whose clusters
are linearly ordered vertical strips in the plane and edges join only vertices
in the same cluster or in neighboring clusters with respect to this order. In
order to prove our main result we first obtain a forbidden substructure
characterization of embedded strip clustered planar graphs.
  The Hanani-Tutte theorem says that a graph is planar, if it can be drawn in
the plane so that every pair of edges not sharing a vertex cross an even number
of times. We prove the variant of Hanani-Tutte theorem for strip clustered
graphs if the underlying abstract graph is three connected or a tree. In the
case of trees our result implies that c-planarity for flat clustered graphs
with three clusters is solvable in a polynomial time if the underlying abstract
graph is a tree. The proof of the latter result combines our forbidden
substructure characterization of embedded strip clustered planar graphs with
Tucker's characterization of 0-1 matrices with consecutive ones property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3028</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3028</id><created>2014-10-11</created><authors><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Jiang</keyname><forenames>Jiamo</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Chonggang</forenames></author></authors><title>Heterogeneous Cloud Radio Access Networks: A New Perspective for
  Enhancing Spectral and Energy Efficiencies</title><categories>cs.NI</categories><comments>20 pages, 6 figures, to be published in IEEE Wireless Communications</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To mitigate the severe inter-tier interference and enhance limited
cooperative gains resulting from the constrained and non-ideal transmissions
between adjacent base stations in heterogeneous networks (HetNets),
heterogeneous cloud radio access networks (H-CRANs) are proposed as
cost-efficient potential solutions through incorporating the cloud computing
into HetNets. In this article, state-of-the-art research achievements and
challenges on H-CRANs are surveyed. In particular, we discuss issues of system
architectures, spectral and energy efficiency performances, and promising key
techniques. A great emphasis is given towards promising key techniques in
H-CRANs to improve both spectral and energy efficiencies, including cloud
computing based coordinated multi-point transmission and reception, large-scale
cooperative multiple antenna, cloud computing based cooperative radio resource
management, and cloud computing based self-organizing network in the cloud
converging scenarios. The major challenges and open issues in terms of
theoretical performance with stochastic geometry, fronthaul constrained
resource allocation, and standard development that may block the promotion of
H-CRANs are discussed as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3031</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3031</id><created>2014-10-11</created><updated>2015-10-02</updated><authors><author><keyname>Anshu</keyname><forenames>Anurag</forenames></author><author><keyname>Devabathini</keyname><forenames>Vamsi Krishna</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>Near optimal bounds on quantum communication complexity of single-shot
  quantum state redistribution</title><categories>quant-ph cs.IT math.IT</categories><comments>21 pages, 1 figure, version 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show near optimal bounds on the worst case quantum communication of
single-shot entanglement-assisted one-way quantum communication protocols for
the {\em quantum state redistribution} task and for the sub-tasks {\em quantum
state splitting} and {\em quantum state merging}. Our bounds are tighter than
previously known best bounds for the latter two sub-tasks.
  A key technical tool that we use is a {\em convex-split} lemma which may be
of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3033</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3033</id><created>2014-10-11</created><authors><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Cheung</keyname><forenames>Ho Yee</forenames></author><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Teng</keyname><forenames>Shanghua</forenames></author></authors><title>Signaling in Quasipolynomial time</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strategic interactions often take place in an environment rife with
uncertainty. As a result, the equilibrium of a game is intimately related to
the information available to its players. The \emph{signaling problem}
abstracts the task faced by an informed &quot;market maker&quot;, who must choose how to
reveal information in order to effect a desirable equilibrium.
  In this paper, we consider two fundamental signaling problems: one for
abstract normal form games, and the other for single item auctions. For the
former, we consider an abstract class of objective functions which includes the
social welfare and weighted combinations of players' utilities, and for the
latter we restrict our attention to the social welfare objective and to
signaling schemes which are constrained in the number of signals used. For both
problems, we design approximation algorithms for the signaling problem which
run in quasi-polynomial time under various conditions, extending and
complementing the results of various recent works on the topic.
  Underlying each of our results is a &quot;meshing scheme&quot; which effectively
overcomes the &quot;curse of dimensionality&quot; and discretizes the space of
&quot;essentially different&quot; posterior beliefs -- in the sense of inducing
&quot;essentially different&quot; equilibria. This is combined with an algorithm for
optimally assembling a signaling scheme as a convex combination of such
beliefs. For the normal form game setting, the meshing scheme leads to a convex
partition of the space of posterior beliefs and this assembly procedure is
reduced to a linear program, and in the auction setting the assembly procedure
is reduced to submodular function maximization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3041</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3041</id><created>2014-10-11</created><authors><author><keyname>Momani</keyname><forenames>Mohammad</forenames></author><author><keyname>Takruri</keyname><forenames>Maen</forenames></author><author><keyname>Al-Hmouz</keyname><forenames>Rami</forenames></author></authors><title>Risk Assessment Algorithm in Wireless Sensor Networks using Beta
  Distribution</title><categories>cs.CR cs.NI</categories><journal-ref>IJCNC, September 2014, Volume 6, number 5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Beta distribution as a novel technique to weight
direct and indirect trust and assessing the risk in wireless sensor networks.
This paper also reviews the trust factors, which play a major role in building
trust in wireless sensor networks and explains the dynamic aspects of trust.
This is an extension of a previous work done by the authors using a new
approach to assess risk. Simulation results related to the previous work and to
the new approach introduced in this paper are also presented for easy
comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3048</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3048</id><created>2014-10-12</created><authors><author><keyname>Cavallo</keyname><forenames>Ruggiero</forenames></author><author><keyname>Wilkens</keyname><forenames>Christopher A.</forenames></author></authors><title>GSP with General Independent Click-Through-Rates</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popular generalized second price (GSP) auction for sponsored search is
built upon a separable model of click-through-rates that decomposes the
likelihood of a click into the product of a &quot;slot effect&quot; and an &quot;advertiser
effect&quot; --- if the first slot is twice as good as the second for some bidder,
then it is twice as good for everyone. Though appealing in its simplicity, this
model is quite suspect in practice. A wide variety of factors including
externalities and budgets have been studied that can and do cause it to be
violated. In this paper we adopt a view of GSP as an iterated second price
auction (see, e.g., Milgrom 2010) and study how the most basic violation of
separability --- position dependent, arbitrary public click-through-rates that
do not decompose --- affects results from the foundational analysis of GSP
(Varian 2007, Edelman et al. 2007). For the two-slot setting we prove that for
arbitrary click-through-rates, for arbitrary bidder values, an efficient
pure-strategy equilibrium always exists; however, without separability there
always exist values such that the VCG outcome and payments cannot be realized
by any bids, in equilibrium or otherwise. The separability assumption is
therefore necessary in the two-slot case to match the payments of VCG but not
for efficiency. We moreover show that without separability, generic existence
of efficient equilibria is sensitive to the choice of tie-breaking rule, and
when there are more than two slots, no (bid-independent) tie-breaking rule
yields the positive result. In light of this we suggest alternative mechanisms
that trade the simplicity of GSP for better equilibrium properties when there
are three or more slots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3059</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3059</id><created>2014-10-12</created><updated>2014-12-12</updated><authors><author><keyname>Yang</keyname><forenames>Greg</forenames></author></authors><title>Computabilities of Validity and Satisfiability in Probability Logics
  over Finite and Countable Models</title><categories>cs.LO cs.LG math.LO math.PR</categories><comments>47 pages, 4 tables. Comments welcome. Fixed errors found by Rutger
  Kuyper</comments><msc-class>03B48 (Primary), 03D80, 68Q32 (Secondary)</msc-class><acm-class>F.4.1; F.4.1; I.2.3; I.2.6; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $\epsilon$-logic (which is called $\epsilon$E-logic in this paper) of
Kuyper and Terwijn is a variant of first order logic with the same syntax, in
which the models are equipped with probability measures and in which the
$\forall x$ quantifier is interpreted as &quot;there exists a set $A$ of measure
$\ge 1 - \epsilon$ such that for each $x \in A$, ....&quot; Previously, Kuyper and
Terwijn proved that the general satisfiability and validity problems for this
logic are, i) for rational $\epsilon \in (0, 1)$, respectively
$\Sigma^1_1$-complete and $\Pi^1_1$-hard, and ii) for $\epsilon = 0$,
respectively decidable and $\Sigma^0_1$-complete. The adjective &quot;general&quot; here
means &quot;uniformly over all languages.&quot;
  We extend these results in the scenario of finite models. In particular, we
show that the problems of satisfiability by and validity over finite models in
$\epsilon$E-logic are, i) for rational $\epsilon \in (0, 1)$, respectively
$\Sigma^0_1$- and $\Pi^0_1$-complete, and ii) for $\epsilon = 0$, respectively
decidable and $\Pi^0_1$-complete. Although partial results toward the countable
case are also achieved, the computability of $\epsilon$E-logic over countable
models still remains largely unsolved. In addition, most of the results, of
this paper and of Kuyper and Terwijn, do not apply to individual languages with
a finite number of unary predicates. Reducing this requirement continues to be
a major point of research.
  On the positive side, we derive the decidability of the corresponding
problems for monadic relational languages --- equality- and function-free
languages with finitely many unary and zero other predicates. This result holds
for all three of the unrestricted, the countable, and the finite model cases.
  Applications in computational learning theory, weighted graphs, and neural
networks are discussed in the context of these decidability and undecidability
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3060</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3060</id><created>2014-10-12</created><authors><author><keyname>Malas</keyname><forenames>Tareq</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Ltaief</keyname><forenames>Hatem</forenames></author><author><keyname>Stengel</keyname><forenames>Holger</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author><author><keyname>Keyes</keyname><forenames>David</forenames></author></authors><title>Multicore-optimized wavefront diamond blocking for optimizing stencil
  updates</title><categories>cs.DC</categories><doi>10.1137/140991133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of stencil-based algorithms in computational science has
focused attention on optimized parallel implementations for multilevel
cache-based processors. Temporal blocking schemes leverage the large bandwidth
and low latency of caches to accelerate stencil updates and approach
theoretical peak performance. A key ingredient is the reduction of data traffic
across slow data paths, especially the main memory interface. In this work we
combine the ideas of multi-core wavefront temporal blocking and diamond tiling
to arrive at stencil update schemes that show large reductions in memory
pressure compared to existing approaches. The resulting schemes show
performance advantages in bandwidth-starved situations, which are exacerbated
by the high bytes per lattice update case of variable coefficients. Our thread
groups concept provides a controllable trade-off between concurrency and memory
usage, shifting the pressure between the memory interface and the CPU. We
present performance results on a contemporary Intel processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3065</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3065</id><created>2014-10-12</created><updated>2015-05-11</updated><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Secure and Green SWIPT in Distributed Antenna Networks with Limited
  Backhaul Capacity</title><categories>cs.IT math.IT</categories><comments>accepted for publication, IEEE Transactions on Wireless
  Communications, May 10, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the resource allocation algorithm design for secure
information and renewable green energy transfer to mobile receivers in
distributed antenna communication systems. In particular, distributed remote
radio heads (RRHs/antennas) are connected to a central processor (CP) via
capacity-limited backhaul links to facilitate joint transmission. The RRHs and
the CP are equipped with renewable energy harvesters and share their energies
via a lossy micropower grid for improving the efficiency in conveying
information and green energy to mobile receivers via radio frequency (RF)
signals. The considered resource allocation algorithm design is formulated as a
mixed non-convex and combinatorial optimization problem taking into account the
limited backhaul capacity and the quality of service requirements for
simultaneous wireless information and power transfer (SWIPT). We aim at
minimizing the total network transmit power when only imperfect channel state
information of the wireless energy harvesting receivers, which have to be
powered by the wireless network, is available at the CP. In light of the
intractability of the problem, we reformulate it as an optimization problem
with binary selection, which facilitates the design of an iterative resource
allocation algorithm to solve the problem optimally using the generalized
Bender's decomposition (GBD). Furthermore, a suboptimal algorithm is proposed
to strike a balance between computational complexity and system performance.
Simulation results illustrate that the proposed GBD based algorithm obtains the
global optimal solution and the suboptimal algorithm achieves a
close-to-optimal performance. Besides, the distributed antenna network for
SWIPT with renewable energy sharing is shown to require a lower transmit power
compared to a traditional system with multiple co-located antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3068</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3068</id><created>2014-10-12</created><authors><author><keyname>Shi</keyname><forenames>Zhan</forenames></author><author><keyname>Nurdin</keyname><forenames>Hendra I.</forenames></author></authors><title>Effect of phase shifts on EPR entanglement generated on two propagating
  Gaussian fields via coherent feedback</title><categories>quant-ph cs.SY</categories><comments>19 pages, 6 figures. Condensed version of this paper to appear in
  Proceedings of the 53rd IEEE Conference on Decision and Control (CDC), Los
  Angeles, CA, Dec. 15-17, 2014. Continues upon the earlier work in
  http://dx.doi.org/10.1007/s11128-014-0845-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown that deploying two nondegenerate optical parametric
amplifiers (NOPAs) separately at two distant parties in a coherent feedback
loop generates stronger Einstein-Podolski-Rosen (EPR) entanglement between two
propagating continuous-mode output fields than a single NOPA under same pump
power, decay rate and transmission losses. The purpose of this paper is to
investigate the stability and EPR entanglement of a dual-NOPA coherent feedback
system under the effect of phase shifts in the transmission channel between two
distant parties. It is shown that, in the presence of phase shifts, EPR
entanglement worsens or can vanish, but can be improved to some extent in
certain scenarios by adding a phase shifter at each output with a certain value
of phase shift. In ideal cases, in the absence of transmission and
amplification losses, existence of EPR entanglement and whether the original
EPR entanglement can be recovered by the additional phase shifters are decided
by values of the phase shifts in the path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3073</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3073</id><created>2014-10-12</created><authors><author><keyname>Hong</keyname><forenames>Son Nguyen</forenames></author><author><keyname>Anh</keyname><forenames>Hao Nguyen</forenames></author><author><keyname>Trong</keyname><forenames>Thua Huynh</forenames></author></authors><title>A Study on Impacts of RTT Inaccuracy on Dynamic Bandwidth Allocation in
  PON and Solution</title><categories>cs.NI</categories><comments>10 pages, 4 figures, IJCNC</comments><msc-class>68M10</msc-class><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.5, September 2014 pp. 183-192</journal-ref><doi>10.5121/ijcnc.2014.6514</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The circle travelling delay between OLT (Optical Line Terminal) and ONU
(Optical Network Unit) is one of most important items in dynamic bandwidth
allocation (DBA) algorithms in PON, called RTT (Round Trip Time). The RTT is
taken into account when OLT assigns the start times for upstream bandwidth
grants. In most case, RTT is estimated before making bandwidth allocation
decisions in dynamic bandwidth allocation algorithms. If the estimated RTT is
incorrect, the bandwidth allocation decisions are not matched with bandwidth
requests of channels. Thus, performance of PON can get worse by deviation of
RTT. There are several reasons that cause the RTT to be varying, such as
processing delay, distance of OLT and ONU, changing in fiber refractive index
resulting from temperature drift, and degree of accuracy of RTT estimation
methods. In this paper, we evaluate the impacts of RTT inaccuracy on
performance of DBA and identify levels of collision and waste of bandwidth. By
this way, we propose a method to remedy the performance degradation encountered
by the situation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3080</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3080</id><created>2014-10-12</created><authors><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Llull</keyname><forenames>Patrick</forenames></author><author><keyname>Brady</keyname><forenames>David J.</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Tree-Structure Bayesian Compressive Sensing for Video</title><categories>cs.CV</categories><comments>5 pages, 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Bayesian compressive sensing framework is developed for video
reconstruction based on the color coded aperture compressive temporal imaging
(CACTI) system. By exploiting the three dimension (3D) tree structure of the
wavelet and Discrete Cosine Transformation (DCT) coefficients, a Bayesian
compressive sensing inversion algorithm is derived to reconstruct (up to 22)
color video frames from a single monochromatic compressive measurement. Both
simulated and real datasets are adopted to verify the performance of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3083</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3083</id><created>2014-10-12</created><authors><author><keyname>Moran</keyname><forenames>Bill</forenames></author><author><keyname>Cohen</keyname><forenames>Fred</forenames></author><author><keyname>Wang</keyname><forenames>Zengfu</forenames></author><author><keyname>Suvorova</keyname><forenames>Sofia</forenames></author><author><keyname>Cochran</keyname><forenames>Douglas</forenames></author><author><keyname>Taylor</keyname><forenames>Tom</forenames></author><author><keyname>Farrell</keyname><forenames>Peter</forenames></author><author><keyname>Howard</keyname><forenames>Stephen</forenames></author></authors><title>Bounds on Multiple Sensor Fusion</title><categories>cs.SY</categories><comments>23 pages</comments><acm-class>H.3.3; G.1.1; D.6.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of fusing measurements from multiple sensors, where
the sensing regions overlap and data are non-negative---possibly resulting from
a count of indistinguishable discrete entities. Because of overlaps, it is, in
general, impossible to fuse this information to arrive at an accurate estimate
of the overall amount or count of material present in the union of the sensing
regions. Here we study the range of overall values consistent with the data.
Posed as a linear programming problem, this leads to interesting questions
associated with the geometry of the sensor regions, specifically, the
arrangement of their non-empty intersections. We define a computational tool
called the fusion polytope and derive a condition for this to be in the
positive orthant thus simplifying calculations. We show that, in two
dimensions, inflated tiling schemes based on rectangular regions fail to
satisfy this condition, whereas inflated tiling schemes based on hexagons do.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3085</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3085</id><created>2014-10-12</created><authors><author><keyname>Susanto</keyname><forenames>Hengky</forenames></author><author><keyname>Kim</keyname><forenames>ByungGuk</forenames></author><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author></authors><title>QoE Support for Multi-Layered Multimedia Applications</title><categories>cs.NI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Congestion control protocol and bandwidth allocation problems are often
formulated into Network Utility Maximization (NUM) framework. Existing
solutions for NUM generally focus on single-layered applications. As
applications such as video streaming grow in importance and popularity,
addressing user utility function for these multi-layered multimedia
applications in NUM formulation becomes vital. In this paper, we propose a new
multi-layered user utility model that leverages on studies of human visual
perception and quality of experience (QoE) from the fields of computer graphics
and human computer interaction (HCI). Using this new utility model to
investigate network activities, we demonstrate that solving NUM with
multi-layered utility is intractable, and that rate allocation and network
pricing may oscillate due to user behavior specific to multi-layered
applications. To address this, we propose a new approach for admission control
to ensure quality of service (QoS) and quality of experience (QoE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3089</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3089</id><created>2014-10-12</created><authors><author><keyname>Drhima</keyname><forenames>Asmae</forenames></author><author><keyname>Najmeddine</keyname><forenames>Mustapha</forenames></author></authors><title>Duality for Modules and Applications to Decoding Linear Codes over
  Finite Commutative Rings</title><categories>cs.IT math.IT</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using linear functional-based duality of modules, we generalize the syndrome
decoding algorithm of linear codes over finite fields to those over finite
commutative rings. Moreover, If the ring is local the algorithm is simplified
by introducing the control matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3091</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3091</id><created>2014-10-12</created><authors><author><keyname>Bishnu</keyname><forenames>Arijit</forenames></author><author><keyname>Dutta</keyname><forenames>Kunal</forenames></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames></author><author><keyname>Paul</keyname><forenames>Subhabrata</forenames></author></authors><title>$(1,j)$-set problem in graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A subset $D \subseteq V $of a graph $G = (V, E)$ is a $(1, j)$-set if every
vertex $v \in V \setminus D$ is adjacent to at least $1$ but not more than $j$
vertices in D. The cardinality of a minimum $(1, j)$-set of $G$, denoted as
$\gamma_{(1,j)} (G)$, is called the $(1, j)$-domination number of $G$. Given a
graph $G = (V, E)$ and an integer $k$, the decision version of the $(1, j)$-set
problem is to decide whether $G$ has a $(1, j)$-set of cardinality at most $k$.
In this paper, we first obtain an upper bound on $\gamma_{(1,j)} (G)$ using
probabilistic methods, for bounded minimum and maximum degree graphs. Our bound
is constructive, by the randomized algorithm of Moser and Tardos [MT10], We
also show that the $(1, j)$- set problem is NP-complete for chordal graphs.
Finally, we design two algorithms for finding $\gamma_{(1,j)} (G)$ of a tree
and a split graph, for any fixed $j$, which answers an open question posed in
[CHHM13].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3097</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3097</id><created>2014-10-12</created><authors><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Magdy</keyname><forenames>Walid</forenames></author><author><keyname>Darwish</keyname><forenames>Kareem</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>Content and Network Dynamics Behind Egyptian Political Polarization on
  Twitter</title><categories>cs.SI physics.soc-ph</categories><comments>To appear in the Proceedings of the 18th Conference on
  Computer-Supported Cooperative Work and Social Computing CSCW (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is little doubt about whether social networks play a role in modern
protests. This agreement has triggered an entire research avenue, in which
social structure and content analysis have been central --but are typically
exploited separately.
  Here, we combine these two approaches to shed light on the opinion evolution
dynamics in Egypt during the summer of 2013 along two axes
(Islamist/Secularist, pro/anti-military intervention). We intend to find traces
of opinion changes in Egypt's population, paralleling those in the
international community --which oscillated from sympathetic to condemnatory as
civil clashes grew. We find little evidence of people &quot;switching&quot; sides, along
with clear changes in volume in both pro- and anti-military camps.
  Our work contributes new insights into the dynamics of large protest
movements, specially in the aftermath of the main events --rather unattended
previously. It questions the standard narrative concerning a simplistic mapping
between Secularist/pro-military and Islamist/anti-military. Finally, our
conclusions provide empirical validation to sociological models regarding the
behavior of individuals in conflictive contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3104</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3104</id><created>2014-10-12</created><authors><author><keyname>Sun</keyname><forenames>Hongyang</forenames></author><author><keyname>Stolf</keyname><forenames>Patricia</forenames></author><author><keyname>Pierson</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Da Costa</keyname><forenames>Georges</forenames></author></authors><title>Energy-Efficient and Thermal-Aware Resource Management for Heterogeneous
  Datacenters</title><categories>cs.DC</categories><doi>10.1016/j.suscom.2014.08.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose in this paper to study the energy-, thermal- and performance-aware
resource management in heterogeneous datacenters. Witnessing the continuous
development of heterogeneity in datacenters, we are confronted with their
different behaviors in terms of performance, power consumption and thermal
dissipation: Indeed, heterogeneity at server level lies both in the computing
infrastructure (computing power, electrical power consumption) and in the heat
removal systems (different enclosure, fans, thermal sinks). Also the physical
locations of the servers become important with heterogeneity since some servers
can (over)heat others. While many studies address independently these
parameters (most of the time performance and power or energy), we show in this
paper the necessity to tackle all these aspects for an optimal resource
management of the computing resources. This leads to improved energy usage in a
heterogeneous datacenter including the cooling of the computer rooms. We build
our approach on the concept of heat distribution matrix to handle the mutual
influence of the servers, in heterogeneous environments, which is novel in this
context. We propose a heuristic to solve the server placement problem and we
design a generic greedy framework for the online scheduling problem. We derive
several single-objective heuristics (for performance, energy, cooling) and a
novel fuzzy-based priority mechanism to handle their tradeoffs. Finally, we
show results using extensive simulations fed with actual measurements on
heterogeneous servers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3117</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3117</id><created>2014-10-12</created><authors><author><keyname>Chakraborty</keyname><forenames>Soumendu</forenames></author><author><keyname>Jalal</keyname><forenames>Anand Singh</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Charul</forenames></author></authors><title>An Efficient Bit Plane X-OR Algorithm for Irreversible Image
  Steganography</title><categories>cs.MM</categories><doi>10.1504/IJTMCC.2013.053263</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The science of hiding secret information in another message is known as
Steganography; hence the presence of secret information is concealed. It is the
method of hiding cognitive content in same or another media to avoid
recognition by the intruders. This paper introduces new method wherein
irreversible steganography is used to hide an image in the same medium so that
the secret data is masked. The secret image is known as payload and the carrier
is known as cover image. X-OR operation is used amongst mid level bit planes of
carrier image and high level bit planes of data image to generate new low level
bit planes of the stego image. Recovery process includes the X-ORing of low
level bit planes and mid level bit planes of the stego image. Based on the
result of the recovery, subsequent data image is generated. A RGB color image
is used as carrier and the data image is a grayscale image of dimensions less
than or equal to the dimensions of the carrier image. The proposed method
greatly increases the embedding capacity without significantly decreasing the
PSNR value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3120</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3120</id><created>2014-10-12</created><updated>2015-06-02</updated><authors><author><keyname>Gasnikov</keyname><forenames>Alexander</forenames></author><author><keyname>Dmitriev</keyname><forenames>Denis</forenames></author></authors><title>Efficient randomized algorithms for PageRank problem</title><categories>math.OC cs.IR cs.NA cs.SY</categories><comments>31 pages, in Russian</comments><journal-ref>Comp. Math. and Math. Phys. 2015. V. 55. no. 3. P.355-371</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper we compare well known numerical methods of finding PageRank
vector. We propose Markov Chain Monte Carlo method and obtain a new estimation
for this method. We also propose a new method for PageRank problem based on the
reduction of this problem to the matrix game. We solve this (sparse) matrix
game with randomized mirror descent. It should be mentioned that we used
non-standard randomization (in KL-projection) goes back to
Grigoriadis-Khachiayn (1995).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3122</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3122</id><created>2014-10-12</created><authors><author><keyname>Chakraborty</keyname><forenames>Soumendu</forenames></author><author><keyname>Jalal</keyname><forenames>Anand Singh</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Charul</forenames></author></authors><title>Secret Image Sharing Using Grayscale Payload Decomposition and
  Irreversible Image Steganography</title><categories>cs.MM</categories><journal-ref>J. of Info. Sec. and Appl. 18(4)(2013) 180-192</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To provide an added security level most of the existing reversible as well as
irreversible image steganography schemes emphasize on encrypting the secret
image (payload) before embedding it to the cover image. The complexity of
encryption for a large payload where the embedding algorithm itself is complex
may adversely affect the steganographic system. Schemes that can induce same
level of distortion, as any standard encryption technique with lower
computational complexity, can improve the performance of stego systems. In this
paper we propose a secure secret image sharing scheme, which bears minimal
computational complexity. The proposed scheme, as a replacement for encryption,
diversifies the payload into different matrices which are embedded into carrier
image (cover image) using bit X-OR operation. A payload is a grayscale image
which is divided into frequency matrix, error matrix, and sign matrix. The
frequency matrix is scaled down using a mapping algorithm to produce Down
Scaled Frequency (DSF) matrix. The DSF matrix, error matrix, and sign matrix
are then embedded in different cover images using bit X-OR operation between
the bit planes of the matrices and respective cover images. Analysis of the
proposed scheme shows that it effectively camouflages the payload with minimum
computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3125</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3125</id><created>2014-10-12</created><authors><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author><author><keyname>Mladenov</keyname><forenames>Martin</forenames></author><author><keyname>Tokmakov</keyname><forenames>Pavel</forenames></author></authors><title>Relational Linear Programs</title><categories>cs.AI cs.LO cs.PL math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose relational linear programming, a simple framework for combing
linear programs (LPs) and logic programs. A relational linear program (RLP) is
a declarative LP template defining the objective and the constraints through
the logical concepts of objects, relations, and quantified variables. This
allows one to express the LP objective and constraints relationally for a
varying number of individuals and relations among them without enumerating
them. Together with a logical knowledge base, effectively a logical program
consisting of logical facts and rules, it induces a ground LP. This ground LP
is solved using lifted linear programming. That is, symmetries within the
ground LP are employed to reduce its dimensionality, if possible, and the
reduced program is solved using any off-the-shelf LP solver. In contrast to
mainstream LP template languages like AMPL, which features a mixture of
declarative and imperative programming styles, RLP's relational nature allows a
more intuitive representation of optimization problems over relational domains.
We illustrate this empirically by experiments on approximate inference in
Markov logic networks using LP relaxations, on solving Markov decision
processes, and on collective inference using LP support vector machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3145</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3145</id><created>2014-10-12</created><authors><author><keyname>Hossain</keyname><forenames>Peter</forenames></author><author><keyname>Komisarczuk</keyname><forenames>Adaulfo</forenames></author><author><keyname>Pawetczak</keyname><forenames>Garin</forenames></author><author><keyname>Van Dijk</keyname><forenames>Sarah</forenames></author><author><keyname>Axelsen</keyname><forenames>Isabella</forenames></author></authors><title>Machine Learning Techniques in Cognitive Radio Networks</title><categories>cs.LG cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio is an intelligent radio that can be programmed and configured
dynamically to fully use the frequency resources that are not used by licensed
users. It defines the radio devices that are capable of learning and adapting
to their transmission to the external radio environment, which means it has
some kind of intelligence for monitoring the radio environment, learning the
environment and make smart decisions. In this paper, we are reviewing some
examples of the usage of machine learning techniques in cognitive radio
networks for implementing the intelligent radio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3147</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3147</id><created>2014-10-12</created><authors><author><keyname>Geneson</keyname><forenames>Jesse</forenames></author><author><keyname>Shen</keyname><forenames>Lilly</forenames></author></authors><title>Linear bounds on matrix extremal functions using visibility hypergraphs</title><categories>math.CO cs.DM</categories><comments>11 pages, 4 figures</comments><msc-class>05D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 0-1 matrix A contains a 0-1 matrix M if some submatrix of A can be
transformed into M by changing some ones to zeroes. If A does not contain M,
then A avoids M. Let ex(n,M) be the maximum number of ones in an n x n 0-1
matrix that avoids M, and let ex_k(m,M) be the maximum number of columns in a
0-1 matrix with m rows that avoids M and has at least k ones in every column. A
method for bounding ex(n,M) by using bounds on the maximum number of edges in
bar visibility graphs was introduced in (R. Fulek, Discrete Mathematics 309,
2009). By using a similar method with bar visibility hypergraphs, we obtain
linear bounds on the extremal functions of other forbidden 0-1 matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3154</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3154</id><created>2014-10-12</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Shakhar</forenames></author></authors><title>Epsilon-Nets for Halfspaces Revisited</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of $n$ points in $\mathbb{R}^3$, we show that, for any
$\varepsilon &gt;0$, there exists an $\varepsilon$-net of $P$ for halfspace
ranges, of size $O(1/\varepsilon)$. We give five proofs of this result, which
are arguably simpler than previous proofs \cite{msw-hnlls-90, cv-iaags-07,
pr-nepen-08}. We also consider several related variants of this result,
including the case of points and pseudo-disks in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3160</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3160</id><created>2014-10-12</created><authors><author><keyname>Garc&#xed;a-Recuero</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>Esteves</keyname><forenames>S&#xe9;rgio</forenames></author><author><keyname>Veiga</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>Quality-of-Data for Consistency Levels in Geo-replicated Cloud Data
  Stores</title><categories>cs.DC</categories><comments>IEEE CloudCom 2013, Bristol, UK</comments><doi>10.1109/CloudCom.2013.29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has recently emerged as a key technology to provide
individuals and companies with access to remote computing and storage
infrastructures. In order to achieve highly-available yet high-performing
services, cloud data stores rely on data replication. However, providing
replication brings with it the issue of consistency. Given that data are
replicated in multiple geographically distributed data centers, and to meet the
increasing requirements of distributed applications, many cloud data stores
adopt eventual consistency and therefore allow to run data intensive operations
under low latency. This comes at the cost of data staleness. In this paper, we
prioritize data replication based on a set of flexible data semantics that can
best suit all types of Big Data applications, avoiding overloading both network
and systems during large periods of disconnection or partitions in the network.
Therefore we integrated these data semantics into the core architecture of a
well-known NoSQL data store (e.g., HBase), which leverages a three-dimensional
vector-field model (regarding timeliness, number of pending updates and
divergence bounds) to provision data selectively in an on-demand fashion to
applications. This enhances the former consistency model by providing a number
of required levels of consistency to different applications such as, social
networks or e-commerce sites, where priority of updates also differ. In
addition, our implementation of the model into HBase allows updates to be
tagged and grouped atomically in logical batches, akin to transactions,
ensuring atomic changes and correctness of updates as they are propagated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3169</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3169</id><created>2014-10-12</created><authors><author><keyname>Bendich</keyname><forenames>Paul</forenames></author><author><keyname>Gasparovic</keyname><forenames>Ellen</forenames></author><author><keyname>Harer</keyname><forenames>John</forenames></author><author><keyname>Izmailov</keyname><forenames>Rauf</forenames></author><author><keyname>Ness</keyname><forenames>Linda</forenames></author></authors><title>Multi-Scale Local Shape Analysis and Feature Selection in Machine
  Learning Applications</title><categories>cs.CG cs.LG math.AT stat.ML</categories><comments>15 pages, 6 figures, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method called multi-scale local shape analysis, or MLSA, for
extracting features that describe the local structure of points within a
dataset. The method uses both geometric and topological features at multiple
levels of granularity to capture diverse types of local information for
subsequent machine learning algorithms operating on the dataset. Using
synthetic and real dataset examples, we demonstrate significant performance
improvement of classification algorithms constructed for these datasets with
correspondingly augmented features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3173</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3173</id><created>2014-10-12</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>Characteristic Length and Clustering</title><categories>cs.DM</categories><comments>20 pages 13 figures</comments><msc-class>05C35, 52Cxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore relations between various variational problems for graphs like
Euler characteristic chi(G), characteristic length mu(G), mean clustering
nu(G), inductive dimension iota(G), edge density epsilon(G), scale measure
sigma(G), Hilbert action eta(G) and spectral complexity xi(G). A new insight in
this note is that the local cluster coefficient C(x) in a finite simple graph
can be written as a relative characteristic length L(x) of the unit sphere S(x)
within the unit ball B(x) of a vertex. This relation L(x) = 2-C(x) will allow
to study clustering in more general metric spaces like Riemannian manifolds or
fractals. If eta is the average of scalar curvature s(x), a formula mu ~
1+log(epsilon)/log(eta) of Newman, Watts and Strogatz relates mu with the edge
density epsilon and average scalar curvature eta telling that large curvature
correlates with small characteristic length. Experiments show that the
statistical relation mu ~ log(1/nu) holds for random or deterministic
constructed networks, indicating that small clustering is often associated to
large characteristic lengths and lambda=mu/log(nu) can converge in some graph
limits of networks. Mean clustering nu, edge density epsilon and curvature
average eta therefore can relate with characteristic length mu on a statistical
level. We also discovered experimentally that inductive dimension iota and
cluster-length ratio lambda correlate strongly on Erdos-Renyi probability
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3191</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3191</id><created>2014-10-13</created><updated>2016-02-05</updated><authors><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Tamminen</keyname><forenames>Joose</forenames></author><author><keyname>Turunen</keyname><forenames>Matias</forenames></author><author><keyname>Huusari</keyname><forenames>Timo</forenames></author><author><keyname>Choi</keyname><forenames>Yang-Seok</forenames></author><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Talwar</keyname><forenames>Shilpa</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Full-Duplex Mobile Device - Pushing the Limits</title><categories>cs.IT math.IT</categories><comments>18 pages, submitted for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we focus on the challenges of transmitter-receiver isolation
in mobile full-duplex devices, building on shared-antenna based transceiver
architecture. Firstly, self-adaptive analog RF cancellation circuitry is
required, since the capability to track time-varying self-interference coupling
characteristics is of utmost importance in mobile devices. Furthermore, novel
adaptive nonlinear DSP methods are also required for final self-interference
suppression at digital baseband, since mobile-scale devices typically operate
under highly nonlinear low-cost RF components.
  In addition to describing above kind of advanced circuit and signal
processing solutions, comprehensive measurement results from a complete
demonstrator implementation are also provided, evidencing beyond 40 dB of
active RF cancellation over a 80 MHz waveform bandwidth with a highly nonlinear
transmitter power amplifier. Measured examples also demonstrate the good
self-healing characteristics of the developed control loop against fast changes
in the coupling channel. Furthermore, when complemented with nonlinear digital
cancellation processing, the residual self-interference level is pushed down to
the noise floor of the demonstration system, despite the harsh nonlinear nature
of the self-interference. These findings indicate that deploying the
full-duplex principle can indeed be feasible also in mobile devices, and thus
be one potential technology in, e.g., 5G and beyond radio systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3198</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3198</id><created>2014-10-13</created><authors><author><keyname>Lockwood</keyname><forenames>Svetlana</forenames></author><author><keyname>Krishnamoorthy</keyname><forenames>Bala</forenames></author></authors><title>Topological Features In Cancer Gene Expression Data</title><categories>q-bio.GN cs.CG math.AT q-bio.QM</categories><comments>12 pages, 9 figures, appears in proceedings of Pacific Symposium on
  Biocomputing 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for exploring cancer gene expression data based on
tools from algebraic topology. Our method selects a small relevant subset from
tens of thousands of genes while simultaneously identifying nontrivial higher
order topological features, i.e., holes, in the data. We first circumvent the
problem of high dimensionality by dualizing the data, i.e., by studying genes
as points in the sample space. Then we select a small subset of the genes as
landmarks to construct topological structures that capture persistent, i.e.,
topologically significant, features of the data set in its first homology
group. Furthermore, we demonstrate that many members of these loops have been
implicated for cancer biogenesis in scientific literature. We illustrate our
method on five different data sets belonging to brain, breast, leukemia, and
ovarian cancers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3199</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3199</id><created>2014-10-13</created><authors><author><keyname>Macktoobian</keyname><forenames>Matin</forenames></author><author><keyname>Jafari</keyname><forenames>Mohammad</forenames></author><author><keyname>Gh</keyname><forenames>Erfan Attarzadeh</forenames></author></authors><title>Applied Neural Cross-Correlation into the Curved Trajectory Detection
  Process for Braitenberg Vehicles</title><categories>cs.RO</categories><comments>3rd Basic &amp; Clinical Neuroscience Congress, Tehran, Iran</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Curved Trajectory Detection (CTD) process could be considered among
high-level planned capabilities for cognitive agents, has which been acquired
under aegis of embedded artificial spiking neuronal circuits. In this paper,
hard-wired implementation of the cross-correlation, as the most common
comparison-driven scheme for both natural and artificial bionic constructions
named Depth Detection Module(DDM), has been taken into account. It is
manifestation of efficient handling upon epileptic seizures due to application
of both excitatory and inhibitory connections within the circuit structure.
Presented traditional analytic approach of the cross-correlation computation
with regard to our neural mapping technique and the acquired traced precision
have been turned into account for coherent accomplishments of the
aforementioned design in perspective of the desired accuracy upon high-level
cognitive reactions. Furthermore, the proposed circuit could be fitted into the
scalable neuronal network of the CTD, properly. Simulated denouements have been
captured based on the computational model of PIONEER mobile robot to verify
characteristics of the module, in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3200</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3200</id><created>2014-10-13</created><authors><author><keyname>Gosebo</keyname><forenames>Ntjatji</forenames></author></authors><title>The Impact of e-Politician on the Adoption of e-Service: Perceptions
  from a sample of South African Municipal IT Heads</title><categories>cs.CY</categories><journal-ref>International Journal of Managing Public Sector Information and
  Communication Technologies (IJMPICT) Vol. 5, No. 3, pp 1-10, September 2014</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The purpose of this study is to establish whether the use of information
technology (IT) by elected municipal representatives, for constituency work,
emboldens the adoption of e-service in municipals of a developing country. The
research data was obtained through the completion of a questionnaire by a
sample of respondents who serve as authorities of IT in South African
municipals. The findings from both descriptive and inferential data analysis of
collected data confirm that the use of IT by elected municipal representatives
for constituency work impacts the adoption of e-service in municipals.
Furthermore, the use of IT by elected municipal representatives for
constituency work correlated with both e-service laws and e-service security.
This study contributes to a better understanding of choices needed when
planning for the adoption of e-service initiatives in municipals of developing
countries. Given that 87.2% of respondents are aware of a high access to
telephone mobile, a further research is needed to clarify why most elected
municipal representatives of a developing country choose not to exploit IT for
their constituency work, and similarly why municipals of a developing country
do not exploit IT to provide services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3214</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3214</id><created>2014-10-13</created><authors><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Secure Erasure Codes With Partial Decodability</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MDS property (aka the $k$-out-of-$n$ property) requires that if a file is
split into several symbols and subsequently encoded into $n$ coded symbols,
each being stored in one storage node of a distributed storage system (DSS),
then an user can recover the file by accessing any $k$ nodes. We study the
so-called $p$-decodable $\mu$-secure erasure coding scheme $(1 \leq p \leq k -
\mu, 0 \leq \mu &lt; k, p | (k-\mu))$, which satisfies the MDS property and the
following additional properties:
  (P1) strongly secure up to a threshold: an adversary which eavesdrops at most
$\mu$ storage nodes gains no information (in Shannon's sense) about the stored
file,
  (P2) partially decodable: a legitimate user can recover a subset of $p$ file
symbols by accessing some $\mu + p$ storage nodes.
  The scheme is perfectly $p$-decodable $\mu$-secure if it satisfies the
following additional property:
  (P3) weakly secure up to a threshold: an adversary which eavesdrops more than
$\mu$ but less than $\mu+p$ storage nodes cannot reconstruct any part of the
file.
  Most of the related work in the literature only focused on the case $p = k -
\mu$. In other words, no partial decodability is provided: an user cannot
retrieve any part of the file by accessing less than $k$ nodes.
  We provide an explicit construction of $p$-decodable $\mu$-secure coding
schemes over small fields for all $\mu$ and $p$. That construction also
produces perfectly $p$-decodable $\mu$-secure schemes over small fields when $p
= 1$ (for every $\mu$), and when $\mu = 0, 1$ (for every $p$). We establish
that perfect schemes exist over \emph{sufficiently large} fields for almost all
$\mu$ and $p$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3226</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3226</id><created>2014-10-13</created><authors><author><keyname>Garcia-Alfaro</keyname><forenames>Joaquin</forenames><affiliation>Institut Mines-T&#xe9;l&#xe9;com, T&#xe9;l&#xe9;com SudParis</affiliation></author><author><keyname>G&#xfc;r</keyname><forenames>G&#xfc;rkan</forenames><affiliation>Provus</affiliation></author></authors><title>Proceedings 2014 International Workshop on Advanced Intrusion Detection
  and Prevention</title><categories>cs.CR cs.NI cs.OS</categories><proxy>EPTCS</proxy><acm-class>Security</acm-class><journal-ref>EPTCS 165, 2014</journal-ref><doi>10.4204/EPTCS.165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 2014 International Advanced
Intrusion Detection and Prevention (AIDP'14) Workshop, held in Marrakesh,
Morocco, on the 5th of June 2014, in conjunction with the 29th IFIP TC-11 SEC
2014 International Conference. It includes a revised version of the papers
selected for presentation at the work- shop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3227</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3227</id><created>2014-10-13</created><authors><author><keyname>Keil</keyname><forenames>Matthias</forenames></author><author><keyname>Thiemann</keyname><forenames>Peter</forenames></author></authors><title>Symbolic Solving of Extended Regular Expression Inequalities</title><categories>cs.FL</categories><comments>Technical Report</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a new solution to the containment problem for extended
regular expressions that extends basic regular expressions with intersection
and complement operators and consider regular expressions on infinite alphabets
based on potentially infinite character sets. Standard approaches deciding the
containment do not take extended operators or character sets into account. The
algorithm avoids the translation to an expression-equivalent automaton and
provides a purely symbolic term rewriting systems for solving regular
expressions inequalities.
  We give a new symbolic decision procedure for the containment problem based
on Brzozowski's regular expression derivatives and Antimirov's rewriting
approach to check containment. We generalize Brzozowski's syntactic derivative
operator to two derivative operators that work with respect to (potentially
infinite) representable character sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3247</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3247</id><created>2014-10-13</created><authors><author><keyname>Bosek</keyname><forenames>Bart&#x142;omiej</forenames></author><author><keyname>Kierstead</keyname><forenames>Hal A.</forenames></author><author><keyname>Krawczyk</keyname><forenames>Tomasz</forenames></author><author><keyname>Matecki</keyname><forenames>Grzegorz</forenames></author><author><keyname>Smith</keyname><forenames>Matthew E.</forenames></author></authors><title>An Improved Subexponential Bound for On-line Chain Partitioning</title><categories>cs.DS math.CO</categories><comments>23 pages, 11 figures</comments><msc-class>68W27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bosek and Krawczyk exhibited an on-line algorithm for partitioning an on-line
poset of width $w$ into $w^{14\lg w}$ chains. They also observed that the
problem of on-line chain partitioning of general posets of width $w$ could be
reduced to First-Fit chain partitioning of $(2w^2 + 1)$-ladder-free posets of
width $w$, where an $m$-ladder is the transitive closure of the union of two
incomparable chains $x_1\le...\le x_m$, $y_1\le...\le y_m$ and the set of
comparabilities $\{x_1\le y_1,..., x_m\le y_m\}$. Here, we improve the
subexponential upper bound to $w^{6.5\lg w + O(1)}$ with a simplified proof,
exploiting the First-Fit algorithm on ladder-free posets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3248</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3248</id><created>2014-10-13</created><updated>2015-02-27</updated><authors><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author><author><keyname>Sen</keyname><forenames>Pranab</forenames></author><author><keyname>Warsi</keyname><forenames>Naqueeb</forenames></author></authors><title>One-shot Marton inner bound for classical-quantum broadcast channel</title><categories>cs.IT math.IT quant-ph</categories><comments>Corrected some typos and some other minor errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of communication over a classical-quantum broadcast
channel with one sender and two receivers. Generalizing the classical inner
bounds shown by Marton and the recent quantum asymptotic version shown by Savov
and Wilde, we obtain one-shot inner bounds in the quantum setting. Our bounds
are stated in terms of smooth min and max Renyi divergences. We obtain these
results using a different analysis of the random codebook argument and employ a
new one-shot classical mutual covering argument based on rejection sampling.
These results give a full justification of the claims of Savov and Wilde in the
classical-quantum asymptotic iid setting; the techniques also yield similar
bounds in the information spectrum setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3251</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3251</id><created>2014-10-13</created><authors><author><keyname>Mahia</keyname><forenames>Ram Niwash</forenames></author><author><keyname>Fulwani</keyname><forenames>Deepak</forenames></author><author><keyname>Singh</keyname><forenames>Mahaveer</forenames></author></authors><title>Characterization of Driver Nodes of Anti-Stable Networks</title><categories>cs.SY</categories><comments>6 Pages, 4 Figures, and 2 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A controllable network can be driven from any initial state to any desired
state using driver nodes. A set of driver nodes to control a network is not
unique. It is important to characterize these driver nodes and select the right
driver nodes. The work discusses theory and algorithms to select driver node
such that largest region of attraction can be obtained considering limited
capacity of driver node and with unstable eigenvalues of adjacency matrix. A
network which can be controllable using one driver node is considered.
Nonuniqueness of driver node poses a challenge to select right driver node when
multiple possibilities exist. The work addresses this issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3270</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3270</id><created>2014-10-13</created><authors><author><keyname>Ziegeldorf</keyname><forenames>Jan Henrik</forenames></author><author><keyname>Viol</keyname><forenames>Nicolai</forenames></author><author><keyname>Henze</keyname><forenames>Martin</forenames></author><author><keyname>Wehrle</keyname><forenames>Klaus</forenames></author></authors><title>POSTER: Privacy-preserving Indoor Localization</title><categories>cs.CR</categories><comments>Poster Session of the 7th ACM Conference on Security &amp; Privacy in
  Wireless and Mobile Networks (WiSec'14)</comments><doi>10.13140/2.1.2847.4886</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Upcoming WiFi-based localization systems for indoor environments face a
conflict of privacy interests: Server-side localization violates location
privacy of the users, while localization on the user's device forces the
localization provider to disclose the details of the system, e.g.,
sophisticated classification models. We show how Secure Two-Party Computation
can be used to reconcile privacy interests in a state-of-the-art localization
system. Our approach provides strong privacy guarantees for all involved
parties, while achieving room-level localization accuracy at reasonable
overheads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3277</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3277</id><created>2014-10-13</created><updated>2014-12-09</updated><authors><author><keyname>Hertling</keyname><forenames>Peter</forenames><affiliation>Universitaet der Bundeswehr Muenchen</affiliation></author><author><keyname>Spandl</keyname><forenames>Christoph</forenames><affiliation>Universitaet der Bundeswehr Muenchen</affiliation></author></authors><title>Computing a Solution of Feigenbaum's Functional Equation in Polynomial
  Time</title><categories>math.DS cs.CC cs.NA</categories><comments>CCA 2012, Cambridge, UK, 24-27 June 2012</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  9, 2014) lmcs:984</journal-ref><doi>10.2168/LMCS-10(4:7)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lanford has shown that Feigenbaum's functional equation has an analytic
solution. We show that this solution is a polynomial time computable function.
This implies in particular that the so-called first Feigenbaum constant is a
polynomial time computable real number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3302</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3302</id><created>2014-10-13</created><updated>2014-12-09</updated><authors><author><keyname>Manoussakis</keyname><forenames>George</forenames></author></authors><title>The clique problem on inductive $k$-independent graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is inductive $k$-independent if there exists and ordering of its
vertices $v_{1},...,v_{n}$ such that $\alpha(G[N(v_{i})\cap V_{i}])\leq k $
where $N(v_{i})$ is the neighborhood of $v_{i}$, $V_{i}=\{v_{i},...,v_{n}\}$
and $\alpha$ is the independence number. In this article, by answering to a
question of [Y.Ye, A.Borodin, Elimination graphs, ACM Trans. Algorithms 8 (2)
(2012) 14:1-14:23], we design a polynomial time approximation algorithm with
ratio {$\overline{\Delta} \slash log(log(\overline{ \Delta}) \slash k)$ for the
maximum clique and also show that the decision version of this problem is fixed
parameter tractable for this particular family of graphs with complexity
$O(1.2127^{(p+k-1)^{k}}n)$. Then we study a subclass of inductive
$k$-independent graphs, namely $k$-degenerate graphs. A graph is $k$-degenerate
if there exists an ordering of its vertices $v_{1},...,v_{n}$ such that
$|N(v_{i})\cap V_{i}|\leq k $. Our contribution is an algorithm computing a
maximum clique for this class of graphs in time $O(1.2127^{k}(n-k+1))$, thus
improving previous best results. We also prove some structural properties for
inductive $k$-independent graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3314</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3314</id><created>2014-10-13</created><authors><author><keyname>Neumann</keyname><forenames>Marion</forenames></author><author><keyname>Garnett</keyname><forenames>Roman</forenames></author><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author></authors><title>Propagation Kernels</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce propagation kernels, a general graph-kernel framework for
efficiently measuring the similarity of structured data. Propagation kernels
are based on monitoring how information spreads through a set of given graphs.
They leverage early-stage distributions from propagation schemes such as random
walks to capture structural information encoded in node labels, attributes, and
edge information. This has two benefits. First, off-the-shelf propagation
schemes can be used to naturally construct kernels for many graph types,
including labeled, partially labeled, unlabeled, directed, and attributed
graphs. Second, by leveraging existing efficient and informative propagation
schemes, propagation kernels can be considerably faster than state-of-the-art
approaches without sacrificing predictive performance. We will also show that
if the graphs at hand have a regular structure, for instance when modeling
image or video data, one can exploit this regularity to scale the kernel
computation to large databases of graphs with thousands of nodes. We support
our contributions by exhaustive experiments on a number of real-world graphs
from a variety of application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3322</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3322</id><created>2014-10-13</created><updated>2015-04-28</updated><authors><author><keyname>Emmerich</keyname><forenames>Paul</forenames></author><author><keyname>Gallenm&#xfc;ller</keyname><forenames>Sebastian</forenames></author><author><keyname>Raumer</keyname><forenames>Daniel</forenames></author><author><keyname>Wohlfart</keyname><forenames>Florian</forenames></author><author><keyname>Carle</keyname><forenames>Georg</forenames></author></authors><title>MoonGen: A Scriptable High-Speed Packet Generator</title><categories>cs.NI</categories><comments>Draft. Will be submitted to ACM IMC 2015. Section 5 will be expanded
  significantly and Section 8.2 rewritten before submitting it to the ACM IMC
  for peer review</comments><acm-class>C.4</acm-class><doi>10.1145/2815675.2815692</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present MoonGen, a flexible high-speed packet generator. It can saturate
10 GbE links with minimum sized packets using only a single CPU core by running
on top of the packet processing framework DPDK. Linear multi-core scaling
allows for even higher rates: We have tested MoonGen with up to 178.5 Mpps at
120 Gbit/s. We move the whole packet generation logic into user-controlled Lua
scripts to achieve the highest possible flexibility. In addition, we utilize
hardware features of Intel NICs that have not been used for packet generators
previously. A key feature is the measurement of latency with sub-microsecond
precision and accuracy by using hardware timestamping capabilities of modern
commodity NICs. We address timing issues with software-based packet generators
and apply methods to mitigate them with both hardware support on commodity NICs
and with a novel method to control the inter-packet gap in software. Features
that were previously only possible with hardware-based solutions are now
provided by MoonGen on commodity hardware. MoonGen is available as free
software under the MIT license at https://github.com/emmericp/MoonGen
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3334</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3334</id><created>2014-10-13</created><authors><author><keyname>Kravari</keyname><forenames>Kalliopi</forenames></author><author><keyname>Bassiliades</keyname><forenames>Nick</forenames></author></authors><title>DISARM: A Social Distributed Agent Reputation Model based on Defeasible
  Logic</title><categories>cs.MA</categories><comments>Paper under review. Keywords: Semantic Web, Intelligent Multi-agent
  Systems, Agent Reputation, Defeasible Reasoning</comments><acm-class>I.2.11; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent Agents act in open and thus risky environments, hence making the
appropriate decision about who to trust in order to interact with, could be a
challenging process. As intelligent agents are gradually enriched with Semantic
Web technology, acting on behalf of their users with limited or no human
intervention, their ability to perform assigned tasks is scrutinized. Hence,
trust and reputation models, based on interaction trust or witness reputation,
have been proposed, yet they often presuppose the use of a centralized
authority. Although such mechanisms are more popular, they are usually faced
with skepticism, since users may question the trustworthiness and the
robustness of a central authority. Distributed models, on the other hand, are
more complex but they provide personalized estimations based on each agent's
interests and preferences. To this end, this article proposes DISARM, a novel
distributed reputation model. DISARM deals MASs as social networks, enabling
agents to establish and maintain relationships, limiting the disadvantages of
the common distributed approaches. Additionally, it is based on defeasible
logic, modeling the way intelligent agents, like humans, draw reasonable
conclusions from incomplete and possibly conflicting (thus inconclusive)
information. Finally, we provide an evaluation that illustrates the usability
of the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3340</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3340</id><created>2014-10-09</created><authors><author><keyname>Parker</keyname><forenames>Joshua</forenames></author><author><keyname>Boedihardjo</keyname><forenames>Arnold</forenames></author></authors><title>Evidence of spatial embedding in the IPv4 router-level Internet network</title><categories>cs.NI cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much interest has been taken in understanding the global routing structure of
the Internet, both to model and protect the current structures and to modify
the structure to improve resilience. These studies rely on trace-routes and
algorithmic inference to resolve individual IP addresses into connected
routers, yielding a network of routers. Using WHOIS registries, parsing of DNS
registries, as well as simple latency-based triangulation, these routers can
often be geolocated to at least their country of origin, if not specific
regions. In this work, we use node subgraph summary statistics to present
evidence that the router-level (IPv4) network is spatially embedded, with the
similarity (or dissimilarity) of a node from it's neighbor strongly correlating
with the attributes of other routers residing in the same country or region. We
discuss these results in context of the recently proposed gravity models of the
Internet, as well as the potential application to geolocation inferrence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3341</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3341</id><created>2014-10-08</created><authors><author><keyname>Li</keyname><forenames>Haifang</forenames></author><author><keyname>Tian</keyname><forenames>Fei</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Qin</keyname><forenames>Tao</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>Generalization Analysis for Game-Theoretic Machine Learning</title><categories>cs.LG cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Internet applications like sponsored search, cautions need to be taken
when using machine learning to optimize their mechanisms (e.g., auction) since
self-interested agents in these applications may change their behaviors (and
thus the data distribution) in response to the mechanisms. To tackle this
problem, a framework called game-theoretic machine learning (GTML) was recently
proposed, which first learns a Markov behavior model to characterize agents'
behaviors, and then learns the optimal mechanism by simulating agents' behavior
changes in response to the mechanism. While GTML has demonstrated practical
success, its generalization analysis is challenging because the behavior data
are non-i.i.d. and dependent on the mechanism. To address this challenge,
first, we decompose the generalization error for GTML into the behavior
learning error and the mechanism learning error; second, for the behavior
learning error, we obtain novel non-asymptotic error bounds for both parametric
and non-parametric behavior learning methods; third, for the mechanism learning
error, we derive a uniform convergence bound based on a new concept called
nested covering number of the mechanism space and the generalization analysis
techniques developed for mixing sequences. To the best of our knowledge, this
is the first work on the generalization analysis of GTML, and we believe it has
general implications to the theoretical analysis of other complicated machine
learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3348</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3348</id><created>2014-10-13</created><authors><author><keyname>Razzaghi</keyname><forenames>Talayeh</forenames></author><author><keyname>Safro</keyname><forenames>Ilya</forenames></author></authors><title>Fast Multilevel Support Vector Machines</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving different types of optimization models (including parameters fitting)
for support vector machines on large-scale training data is often an expensive
computational task. This paper proposes a multilevel algorithmic framework that
scales efficiently to very large data sets. Instead of solving the whole
training set in one optimization process, the support vectors are obtained and
gradually refined at multiple levels of coarseness of the data. The proposed
framework includes: (a) construction of hierarchy of large-scale data coarse
representations, and (b) a local processing of updating the hyperplane
throughout this hierarchy. Our multilevel framework substantially improves the
computational time without loosing the quality of classifiers. The algorithms
are demonstrated for both regular and weighted support vector machines.
Experimental results are presented for balanced and imbalanced classification
problems. Quality improvement on several imbalanced data sets has been
observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3349</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3349</id><created>2014-10-13</created><updated>2015-03-10</updated><authors><author><keyname>Dang</keyname><forenames>Chinh</forenames></author><author><keyname>Radha</keyname><forenames>Hayder</forenames></author></authors><title>Single Image Super Resolution via Manifold Approximation</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image super-resolution remains an important research topic to overcome the
limitations of physical acquisition systems, and to support the development of
high resolution displays. Previous example-based super-resolution approaches
mainly focus on analyzing the co-occurrence properties of low resolution and
high-resolution patches. Recently, we proposed a novel single image
super-resolution approach based on linear manifold approximation of the
high-resolution image-patch space [1]. The image super-resolution problem is
then formulated as an optimization problem of searching for the best matched
high resolution patch in the manifold for a given low-resolution patch. We
developed a novel technique based on the l1 norm sparse graph to learn a set of
low dimensional affine spaces or tangent subspaces of the high-resolution patch
manifold. The optimization problem is then solved based on the learned set of
tangent subspaces. In this paper, we build on our recent work as follows.
First, we consider and analyze each tangent subspace as one point in a
Grassmann manifold, which helps to compute geodesic pairwise distances among
these tangent subspaces. Second, we develop a min-max algorithm to select an
optimal subset of tangent subspaces. This optimal subset reduces the
computational cost while still preserving the quality of the reconstructed
high-resolution image. Third, and to further achieve lower computational
complexity, we perform hierarchical clustering on the optimal subset based on
Grassmann manifold distances. Finally, we analytically prove the validity of
the proposed Grassmann-distance based clustering. A comparison of the obtained
results with other state-of-the-art methods clearly indicates the viability of
the new proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3351</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3351</id><created>2014-10-13</created><updated>2015-05-15</updated><authors><author><keyname>Ache</keyname><forenames>Antonio G.</forenames></author><author><keyname>Warren</keyname><forenames>Micah W.</forenames></author></authors><title>Coarse Ricci curvature with applications to manifold learning problem</title><categories>math.DG cs.LG math.MG stat.ML</categories><comments>28 pages</comments><msc-class>53</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a sample of $n$ points taken i.i.d from a submanifold of Euclidean
space. This defines a metric measure space. We show that there is an explicit
set of scales $t_{n}\rightarrow0$ such that a coarse Ricci curvature at scale
$t_{n}$ on this metric measure space converges almost surely to the coarse
Ricci curvature of the underlying manifold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3363</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3363</id><created>2014-10-13</created><updated>2014-11-04</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Translucent Players: Explaining Cooperative Behavior in Social Dilemmas</title><categories>cs.GT physics.soc-ph q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few decades, numerous experiments have shown that humans do not
always behave so as to maximize their material payoff. Cooperative behavior
when non-cooperation is a dominant strategy (with respect to the material
payoffs) is particularly puzzling. Here we propose a novel approach to explain
cooperation, assuming what Halpern and Pass (2013) call &quot;translucent players&quot;.
Typically, players are assumed to be &quot;opaque&quot;, in the sense that a deviation by
one player does not affect the strategies used by other players. But a player
may believe that if he switches from one strategy to another, the fact that he
chooses to switch may be visible to the other players. For example, if he
chooses to defect in Prisoner's Dilemma, the other player may sense his guilt.
We show that by assuming translucent players, we can recover many of the
regularities observed in human behavior in well-studied games such as
Prisoner's Dilemma, Traveler's Dilemma, Bertrand Competition, and the Public
Goods game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3375</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3375</id><created>2014-10-13</created><updated>2015-10-06</updated><authors><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author><author><keyname>Meeks</keyname><forenames>Kitty</forenames></author></authors><title>The parameterised complexity of counting even and odd induced subgraphs</title><categories>math.CO cs.CC cs.DM</categories><comments>Author final version, to appear in Combinatorica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of counting, in a given graph, the number of induced
k-vertex subgraphs which have an even number of edges, and also the
complementary problem of counting the k-vertex induced subgraphs having an odd
number of edges. We demonstrate that both problems are #W[1]-hard when
parameterised by k, in fact proving a somewhat stronger result about counting
subgraphs with a property that only holds for some subset of k-vertex subgraphs
which have an even (respectively odd) number of edges. On the other hand, we
show that the problems of counting even and odd k-vertex induced subgraphs both
admit an FPTRAS. These approximation schemes are based on a surprising
structural result, which exploits ideas from Ramsey theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3385</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3385</id><created>2014-10-13</created><authors><author><keyname>Baldan</keyname><forenames>Paolo</forenames></author><author><keyname>Bonchi</keyname><forenames>Filippo</forenames></author><author><keyname>Kerstan</keyname><forenames>Henning</forenames></author><author><keyname>K&#xf6;nig</keyname><forenames>Barbara</forenames></author></authors><title>Behavioral Metrics via Functor Lifting</title><categories>cs.LO</categories><comments>to be published in: Proceedings of FSTTCS 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study behavioral metrics in an abstract coalgebraic setting. Given a
coalgebra alpha: X -&gt; FX in Set, where the functor F specifies the branching
type, we define a framework for deriving pseudometrics on X which measure the
behavioral distance of states.
  A first crucial step is the lifting of the functor F on Set to a functor in
the category PMet of pseudometric spaces. We present two different approaches
which can be viewed as generalizations of the Kantorovich and Wasserstein
pseudometrics for probability measures. We show that the pseudometrics provided
by the two approaches coincide on several natural examples, but in general they
differ.
  Then a final coalgebra for F in Set can be endowed with a behavioral distance
resulting as the smallest solution of a fixed-point equation, yielding the
final coalgebra in PMet. The same technique, applied to an arbitrary coalgebra
alpha: X -&gt; FX in Set, provides the behavioral distance on X. Under some
constraints we can prove that two states are at distance 0 if and only if they
are behaviorally equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3386</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3386</id><created>2014-10-13</created><updated>2014-10-13</updated><authors><author><keyname>Acharya</keyname><forenames>Jayadev</forenames></author><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author></authors><title>Testing Poisson Binomial Distributions</title><categories>cs.DS cs.IT cs.LG math.IT</categories><comments>To appear in ACM-SIAM Symposium on Discrete Algorithms (SODA) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Poisson Binomial distribution over $n$ variables is the distribution of the
sum of $n$ independent Bernoullis. We provide a sample near-optimal algorithm
for testing whether a distribution $P$ supported on $\{0,...,n\}$ to which we
have sample access is a Poisson Binomial distribution, or far from all Poisson
Binomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$
to which we provide a matching lower bound. We note that our sample complexity
improves quadratically upon that of the naive &quot;learn followed by tolerant-test&quot;
approach, while instance optimal identity testing [VV14] is not applicable
since we are looking to simultaneously test against a whole family of
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3401</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3401</id><created>2014-10-10</created><authors><author><keyname>Felderer</keyname><forenames>Michael</forenames></author><author><keyname>Bjarnason</keyname><forenames>Elizabeth</forenames></author><author><keyname>Borg</keyname><forenames>Markus</forenames></author><author><keyname>Unterkalmsteiner</keyname><forenames>Michael</forenames></author><author><keyname>Morandini</keyname><forenames>Mirko</forenames></author><author><keyname>Staats</keyname><forenames>Matt</forenames></author></authors><title>Workshop Summary of the 1st International Workshop on Requirements and
  Testing (RET'14)</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of the RET workshop was to explore the interaction of
Requirements Engineering (RE) and Testing, i.e. RET, in research and industry,
and the challenges that result from this interaction. While much work has been
done in the respective fields of requirements engineering and testing, there
exists much more than can be done to understand the connection between the
processes of RE and of testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3408</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3408</id><created>2014-10-13</created><authors><author><keyname>Rajabi-Alni</keyname><forenames>Fatemeh</forenames></author><author><keyname>Bagheri</keyname><forenames>Alireza</forenames></author><author><keyname>Minaei-Bidgoli</keyname><forenames>Behrouz</forenames></author></authors><title>An O(n^3) time algorithm for the maximum weight b-matching problem on
  bipartite graphs</title><categories>cs.DS</categories><comments>10 pages, 1 figure, submitted. arXiv admin note: text overlap with
  arXiv:1303.4031</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A matching between two sets A and B assigns some elements of A to some
elements of B. Finding the similarity between two sets of elements by advantage
of the matching is widely used in computational biology. Frequently, the
capacities of the elements are limited. That is, the number of the elements
that can be matched to each element should not exceed a given number. We
describe the first O(n^3) time algorithm for matching two sets of elements with
limited capacities. We use bipartite graphs to model relationships between
pairs of objects. Let G = (A U B;E) denote an undirected bipartite graph with
real edge weights and node capacities b(v). The b-matching of G matches each
vertex v in A (B) to at least 1 and at most b(v) vertices in B (A). In this
paper, we present an O(n^3) time algorithm for finding the maximum weight
b-matching of G, where |A|+|B| = O(n). Our algorithm improves the previous best
time complexity of O(n^3 log n) for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3422</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3422</id><created>2014-10-13</created><updated>2015-06-08</updated><authors><author><keyname>Gulcu</keyname><forenames>Talha Cihad</forenames></author><author><keyname>Barg</keyname><forenames>Alexander</forenames></author></authors><title>Achieving Secrecy Capacity of the Wiretap Channel and Broadcast Channel
  with a Confidential Component</title><categories>cs.IT math.IT</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wiretap channel model is one of the first communication models with both
reliability and security constraints. Explicit capacity-achieving schemes for
various models of the wiretap channel have received considerable attention in
recent literature. In this paper we address the original version of this
problem, showing that capacity of the general (not necessarily degraded or
symmetric) wiretap channel under a &quot;strong secrecy constraint&quot; can be achieved
using an explicit scheme based on polar codes. We also extend our construction
to the case of broadcast channels with confidential messages defined by Csiszar
and Korner (1978), achieving the entire capacity region of this communication
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3426</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3426</id><created>2014-10-13</created><authors><author><keyname>Cavoretto</keyname><forenames>R.</forenames></author><author><keyname>De Rossi</keyname><forenames>A.</forenames></author><author><keyname>Qiao</keyname><forenames>H.</forenames></author><author><keyname>Quatember</keyname><forenames>B.</forenames></author><author><keyname>Recheis</keyname><forenames>W.</forenames></author><author><keyname>Mayr</keyname><forenames>M.</forenames></author></authors><title>Computing Topology Preservation of RBF Transformations for
  Landmark-Based Image Registration</title><categories>math.NA cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In image registration, a proper transformation should be topology preserving.
Especially for landmark-based image registration, if the displacement of one
landmark is larger enough than those of neighbourhood landmarks, topology
violation will be occurred. This paper aim to analyse the topology preservation
of some Radial Basis Functions (RBFs) which are used to model deformations in
image registration. Mat\'{e}rn functions are quite common in the statistic
literature (see, e.g. \cite{Matern86,Stein99}). In this paper, we use them to
solve the landmark-based image registration problem. We present the topology
preservation properties of RBFs in one landmark and four landmarks model
respectively. Numerical results of three kinds of Mat\'{e}rn transformations
are compared with results of Gaussian, Wendland's, and Wu's functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3438</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3438</id><created>2014-10-13</created><updated>2015-06-29</updated><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author><author><keyname>Ord&#xf3;&#xf1;ez</keyname><forenames>Alberto</forenames></author></authors><title>Efficient and Compact Representations of Prefix Codes</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the attention in statistical compression is given to the space used
by the compressed sequence, a problem completely solved with optimal prefix
codes. However, in many applications, the storage space used to represent the
prefix code itself can be an issue. In this paper we introduce and compare
several techniques to store prefix codes. Let $N$ be the sequence length and
$n$ be the alphabet size. Then a naive storage of an optimal prefix code uses
$O(n\log n)$ bits. Our first technique shows how to use $O(n\log\log(N/n))$
bits to store the optimal prefix code. Then we introduce an approximate
technique that, for any $0&lt;\epsilon&lt;1/2$, takes $O(n \log \log (1 / \epsilon))$
bits to store a prefix code with average codeword length within an additive
$\epsilon$ of the minimum. Finally, a second approximation takes, for any
constant $c &gt; 1$, $O(n^{1 / c} \log n)$ bits to store a prefix code with
average codeword length at most $c$ times the minimum. In all cases, our data
structures allow encoding and decoding of any symbol in $O(1)$ time. We
experimentally compare our new techniques with the state of the art, showing
that we achieve 6--8-fold space reductions, at the price of a slower encoding
(2.5--8 times slower) and decoding (12--24 times slower). The approximations
further reduce this space and improve the time significantly, up to recovering
the speed of classical implementations, for a moderate penalty in the average
code length. As a byproduct, we compare various heuristic, approximate, and
optimal algorithms to generate length-restricted codes, showing that the
optimal ones are clearly superior and practical enough to be implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3440</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3440</id><created>2014-10-10</created><authors><author><keyname>Abdurachmanov</keyname><forenames>David</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Eulisse</keyname><forenames>Giulio</forenames></author><author><keyname>Knight</keyname><forenames>Robert</forenames></author><author><keyname>Niemi</keyname><forenames>Tapio</forenames></author><author><keyname>Nurminen</keyname><forenames>Jukka K.</forenames></author><author><keyname>Nyback</keyname><forenames>Filip</forenames></author><author><keyname>Pestana</keyname><forenames>Goncalo</forenames></author><author><keyname>Ou</keyname><forenames>Zhonghong</forenames></author><author><keyname>Khan</keyname><forenames>Kashif</forenames></author></authors><title>Techniques and tools for measuring energy efficiency of scientific
  software applications</title><categories>cs.DC hep-ex physics.comp-ph</categories><comments>Submitted to proceedings of 16th International workshop on Advanced
  Computing and Analysis Techniques in physics research (ACAT 2014), Prague</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scale of scientific High Performance Computing (HPC) and High Throughput
Computing (HTC) has increased significantly in recent years, and is becoming
sensitive to total energy use and cost. Energy-efficiency has thus become an
important concern in scientific fields such as High Energy Physics (HEP). There
has been a growing interest in utilizing alternate architectures, such as low
power ARM processors, to replace traditional Intel x86 architectures.
Nevertheless, even though such solutions have been successfully used in mobile
applications with low I/O and memory demands, it is unclear if they are
suitable and more energy-efficient in the scientific computing environment.
Furthermore, there is a lack of tools and experience to derive and compare
power consumption between the architectures for various workloads, and
eventually to support software optimizations for energy efficiency. To that
end, we have performed several physical and software-based measurements of
workloads from HEP applications running on ARM and Intel architectures, and
compare their power consumption and performance. We leverage several profiling
tools (both in hardware and software) to extract different characteristics of
the power use. We report the results of these measurements and the experience
gained in developing a set of measurement techniques and profiling tools to
accurately assess the power consumption for scientific workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3441</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3441</id><created>2014-10-10</created><authors><author><keyname>Abdurachmanov</keyname><forenames>David</forenames></author><author><keyname>Bockelman</keyname><forenames>Brian</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Eulisse</keyname><forenames>Giulio</forenames></author><author><keyname>Knight</keyname><forenames>Robert</forenames></author><author><keyname>Muzaffar</keyname><forenames>Shahzad</forenames></author></authors><title>Heterogeneous High Throughput Scientific Computing with APM X-Gene and
  Intel Xeon Phi</title><categories>cs.DC hep-ex physics.comp-ph</categories><comments>Submitted to proceedings of 16th International workshop on Advanced
  Computing and Analysis Techniques in physics research (ACAT 2014), Prague</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electrical power requirements will be a constraint on the future growth of
Distributed High Throughput Computing (DHTC) as used by High Energy Physics.
Performance-per-watt is a critical metric for the evaluation of computer
architectures for cost- efficient computing. Additionally, future performance
growth will come from heterogeneous, many-core, and high computing density
platforms with specialized processors. In this paper, we examine the Intel Xeon
Phi Many Integrated Cores (MIC) co-processor and Applied Micro X-Gene ARMv8
64-bit low-power server system-on-a-chip (SoC) solutions for scientific
computing applications. We report our experience on software porting,
performance and energy efficiency and evaluate the potential for use of such
technologies in the context of distributed computing systems such as the
Worldwide LHC Computing Grid (WLCG).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3447</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3447</id><created>2014-10-13</created><authors><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author></authors><title>Optimal steering of a linear stochastic system to a final probability
  distribution, part II</title><categories>cs.SY math-ph math.MP</categories><comments>15 pages, 4 figures</comments><msc-class>93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimum energy steering of a linear stochastic
system to a final prescribed distribution over a finite horizon and to maintain
a stationary distribution over an infinite horizon. We present sufficient
conditions for optimality in terms of a system of dynamically coupled Riccati
equations in the finite horizon case and algebraic in the stationary case. We
then address the question of feasibility for both problems. For the
finite-horizon case, provided the system is controllable, we prove that without
any restriction on the directionality of the stochastic disturbance it is
always possible to steer the state to any arbitrary Gaussian distribution over
any specified finite time-interval. For the stationary infinite horizon case,
it is not always possible to maintain the state at an arbitrary Gaussian
distribution through constant state-feedback. It is shown that covariances of
admissible stationary Gaussian distributions are characterized by a certain
Lyapunov-like equation. We finally present an alternative to solving the system
of coupled Riccati equations, by expressing the optimal controls in the form of
solutions to (convex) semi-definite programs for both cases. We conclude with
an example to steer the state covariance of the distribution of inertial
particles to an admissible stationary Gaussian distribution over a finite
interval, to be maintained at that stationary distribution thereafter by
constant-gain state-feedback control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3450</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3450</id><created>2014-10-13</created><authors><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Data-Efficient Minimax Quickest Change Detection with Composite
  Post-Change Distribution</title><categories>math.ST cs.IT math.IT math.PR stat.TH</categories><comments>Submitted to IEEE Transactions on Info. Theory, Oct 2014. Preliminary
  version presented at ISIT 2014 at Honolulu, Hawaii</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of quickest change detection is studied, where there is an
additional constraint on the cost of observations used before the change point
and where the post-change distribution is composite. Minimax formulations are
proposed for this problem. It is assumed that the post-change family of
distributions has a member which is least favorable in some sense. An algorithm
is proposed in which on-off observation control is employed using the least
favorable distribution, and a generalized likelihood ratio based approach is
used for change detection. Under the additional condition that either the
post-change family of distributions is finite, or both the pre- and post-change
distributions belong to a one parameter exponential family, it is shown that
the proposed algorithm is asymptotically optimal, uniformly for all possible
post-change distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3460</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3460</id><created>2014-10-13</created><authors><author><keyname>Shen</keyname><forenames>Junhui</forenames></author><author><keyname>Zhu</keyname><forenames>Peiyan</forenames></author><author><keyname>Fan</keyname><forenames>Rui</forenames></author><author><keyname>Tan</keyname><forenames>Wei</forenames></author></authors><title>Sentiment Analysis based on User Tag for Traditional Chinese Medicine in
  Weibo</title><categories>cs.CL cs.SI</categories><comments>7 pages, 8 figures,3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the acceptance of Western culture and science, Traditional Chinese
Medicine (TCM) has become a controversial issue in China. So, it's important to
study the public's sentiment and opinion on TCM. The rapid development of
online social network, such as twitter, make it convenient and efficient to
sample hundreds of millions of people for the aforementioned sentiment study.
To the best of our knowledge, the present work is the first attempt that
applies sentiment analysis to the domain of TCM on Sina Weibo (a twitter-like
microblogging service in China). In our work, firstly we collect tweets topic
about TCM from Sina Weibo, and label the tweets as supporting TCM and opposing
TCM automatically based on user tag. Then, a support vector machine classifier
has been built to predict the sentiment of TCM tweets without labels. Finally,
we present a method to adjust the classifier result. The performance of
F-measure attained with our method is 97%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3461</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3461</id><created>2014-10-13</created><authors><author><keyname>Wray</keyname><forenames>K. Brad</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Philosophy of science viewed through the lense of &quot;References
  Publication Years spectrosopy&quot; (RPYS)</title><categories>cs.DL physics.hist-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the sub-field of philosophy of science using a new method
developed in information science, Referenced Publication Years Spectroscopy
(RPYS). RPYS allows us to identify peak years in citations in a field, which
promises to help scholars identify the key contributions to a field, and
revolutionary discoveries in a field. We discovered that philosophy of science,
a sub-field in the humanities, differs significantly from other fields examined
with this method. Books play a more important role in philosophy of science
than in the sciences. Further, Einstein's famous 1905 papers created a citation
peak in the philosophy of science literature. But rather than being a
contribution to the philosophy of science, their importance lies in the fact
that they are revolutionary contributions to physics with important
implications for philosophy of science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3462</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3462</id><created>2014-10-13</created><authors><author><keyname>Li</keyname><forenames>Xirong</forenames></author></authors><title>Tag Relevance Fusion for Social Image Retrieval</title><categories>cs.IR cs.CV</categories><acm-class>H.3.3</acm-class><doi>10.1007/s00530-014-0430-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the subjective nature of social tagging, measuring the relevance of
social tags with respect to the visual content is crucial for retrieving the
increasing amounts of social-networked images. Witnessing the limit of a single
measurement of tag relevance, we introduce in this paper tag relevance fusion
as an extension to methods for tag relevance estimation. We present a
systematic study, covering tag relevance fusion in early and late stages, and
in supervised and unsupervised settings. Experiments on a large present-day
benchmark set show that tag relevance fusion leads to better image retrieval.
Moreover, unsupervised tag relevance fusion is found to be practically as
effective as supervised tag relevance fusion, but without the need of any
training efforts. This finding suggests the potential of tag relevance fusion
for real-world deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3463</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3463</id><created>2014-10-13</created><authors><author><keyname>Tekumalla</keyname><forenames>Lavanya Sita</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Chiranjib</forenames></author></authors><title>Mining Block I/O Traces for Cache Preloading with Sparse Temporal
  Non-parametric Mixture of Multivariate Poisson</title><categories>cs.OS cs.LG cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing caching strategies, in the storage domain, though well suited to
exploit short range spatio-temporal patterns, are unable to leverage long-range
motifs for improving hitrates. Motivated by this, we investigate novel Bayesian
non-parametric modeling(BNP) techniques for count vectors, to capture long
range correlations for cache preloading, by mining Block I/O traces. Such
traces comprise of a sequence of memory accesses that can be aggregated into
high-dimensional sparse correlated count vector sequences.
  While there are several state of the art BNP algorithms for clustering and
their temporal extensions for prediction, there has been no work on exploring
these for correlated count vectors. Our first contribution addresses this gap
by proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and its
temporal extension(HMM-DP-MMVP) that captures the full covariance structure of
multivariate count data. However, modeling full covariance structure for count
vectors is computationally expensive, particularly for high dimensional data.
Hence, we exploit sparsity in our count vectors, and as our main contribution,
introduce the Sparse DP mixture of multivariate Poisson(Sparse-DP-MMVP),
generalizing our DP-MMVP mixture model, also leading to more efficient
inference. We then discuss a temporal extension to our model for cache
preloading.
  We take the first step towards mining historical data, to capture long range
patterns in storage traces for cache preloading. Experimentally, we show a
dramatic improvement in hitrates on benchmark traces and lay the groundwork for
further research in storage domain to reduce latencies using data mining
techniques to capture long range motifs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3469</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3469</id><created>2014-10-13</created><authors><author><keyname>Baldi</keyname><forenames>Pierre</forenames></author><author><keyname>Sadowski</keyname><forenames>Peter</forenames></author><author><keyname>Whiteson</keyname><forenames>Daniel</forenames></author></authors><title>Enhanced Higgs to $\tau^+\tau^-$ Searches with Deep Learning</title><categories>hep-ph cs.LG hep-ex</categories><comments>For submission to PRL</comments><journal-ref>Phys. Rev. Lett. 114, 111801 (2015)</journal-ref><doi>10.1103/PhysRevLett.114.111801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Higgs boson is thought to provide the interaction that imparts mass to
the fundamental fermions, but while measurements at the Large Hadron Collider
(LHC) are consistent with this hypothesis, current analysis techniques lack the
statistical power to cross the traditional 5$\sigma$ significance barrier
without more data. \emph{Deep learning} techniques have the potential to
increase the statistical power of this analysis by \emph{automatically}
learning complex, high-level data representations. In this work, deep neural
networks are used to detect the decay of the Higgs to a pair of tau leptons. A
Bayesian optimization algorithm is used to tune the network architecture and
training algorithm hyperparameters, resulting in a deep network of eight
non-linear processing layers that improves upon the performance of shallow
classifiers even without the use of features specifically engineered by
physicists for this application. The improvement in discovery significance is
equivalent to an increase in the accumulated dataset of 25\%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3481</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3481</id><created>2014-10-13</created><authors><author><keyname>Seaman</keyname><forenames>Rob</forenames></author></authors><title>Data engineering for archive evolution</title><categories>astro-ph.IM cs.DL</categories><comments>11 pages, this is a longer version of a poster paper submitted to the
  proceedings of ADASS XXIV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the moment astronomical observations are made the resulting data
products begin to grow stale. Even if perfect binary copies are preserved
through repeated timely migration to more robust storage media, data standards
evolve and new tools are created that require different kinds of data or
metadata. The expectations of the astronomical community change even if the
data do not. We discuss data engineering to mitigate the ensuing risks with
examples from a recent project to refactor seven million archival images to new
standards of nomenclature, metadata, format, and compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3506</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3506</id><created>2014-10-13</created><updated>2015-03-04</updated><authors><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author><author><keyname>Sinatra</keyname><forenames>Roberta</forenames></author></authors><title>Social Diffusion and Global Drift on Networks</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 3 figures; to appear in Phys. Rev. E</comments><doi>10.1103/PhysRevE.91.032809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a mathematical model of social diffusion on a symmetric weighted
network where individual nodes' states gradually assimilate to local social
norms made by their neighbors' average states. Unlike physical diffusion, this
process is not state conservational and thus the global state of the network
(i.e., sum of node states) will drift. The asymptotic average node state will
be the average of initial node states weighted by their strengths. Here we show
that, while the global state is not conserved in this process, the inner
product of strength and state vectors is conserved instead, and perfect
positive correlation between node states and local averages of their
self/neighbor strength ratios always results in upward (or at least neutral)
global drift. We also show that the strength assortativity negatively affects
the speed of homogenization. Based on these findings, we propose an adaptive
link weight adjustment method to achieve the highest upward global drift by
increasing the strength-state correlation. The effectiveness of the method was
confirmed through numerical simulations and implications for real-world social
applications are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3507</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3507</id><created>2014-10-13</created><updated>2015-02-11</updated><authors><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Ploumidis</keyname><forenames>Manolis</forenames></author><author><keyname>Traganitis</keyname><forenames>Apostolos</forenames></author></authors><title>Performance Evaluation of Flow Allocation with Successive Interference
  Cancelation for Random Access WMNs</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1406.6304</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we explore the performance gain that can be achieved at the
network level by employing successive interference cancelation (SIC) instead of
treating interference as noise for random access wireless mesh networks with
multi-packet reception capabilities. More precisely we explore both the
throughput and the delay of a distributed flow allocation scheme aimed at
maximizing average aggregate flow throughput while also providing bounded delay
combined with SIC. Simulation results derived from three simple topologies show
that the gain over treating interference as noise for this scheme can be up to
$15\%$ for an SINR threshold value equal to $0.5$. For SINR threshold values as
high as $2.0$ however, this gain is either insignificant or treating
interference as noise proves a better practice. The reason is that although SIC
improves the throughput on a specific link, it also increases the interference
imposed on neighboring receivers. We also show that the gain of applying SIC is
more profound in cases of a large degree of asymmetry among interfering links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3512</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3512</id><created>2014-10-13</created><authors><author><keyname>Eslami</keyname><forenames>Ali</forenames></author><author><keyname>Huang</keyname><forenames>Chuan</forenames></author><author><keyname>Zhang</keyname><forenames>Junshan</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Cascading Failures in Finite-Size Random Geometric Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of cascading failures in cyber-physical systems is drawing much
attention in lieu of different network models for a diverse range of
applications. While many analytic results have been reported for the case of
large networks, very few of them are readily applicable to finite-size
networks. This paper studies cascading failures in finite-size geometric
networks where the number of nodes is on the order of tens or hundreds as in
many real-life networks. First, the impact of the tolerance parameter on
network resiliency is investigated. We quantify the network reaction to initial
disturbances of different sizes by measuring the damage imposed on the network.
Lower and upper bounds on the number of failures are derived to characterize
such damages. Such finite-size analysis reveals the decisiveness and
criticality of taking action within the first few stages of failure propagation
in preventing a cascade. By studying the trend of the bounds as the number of
nodes increases, we observe a phase transition phenomenon in terms of the
tolerance parameter. The critical value of the tolerance parameter, known as
the threshold, is further derived. The findings of this paper, in particular,
shed light on how to choose the tolerance parameter appropriately such that a
cascade of failures could be avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3519</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3519</id><created>2014-10-13</created><updated>2016-01-10</updated><authors><author><keyname>Zhang</keyname><forenames>Qian-Ming</forenames></author><author><keyname>Xu</keyname><forenames>Xiao-Ke</forenames></author><author><keyname>Zhu</keyname><forenames>Yu-Xiao</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Measuring multiple evolution mechanisms of complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>21 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous concise models such as preferential attachment have been put forward
to reveal the evolution mechanisms of real-world networks, which show that
real-world networks are usually jointly driven by a hybrid mechanism of
multiplex features instead of a single pure mechanism. To get an accurate
simulation for real networks, some researchers proposed a few hybrid models of
mixing multiple evolution mechanisms. Nevertheless, how a hybrid mechanism of
multiplex features jointly influence the network evolution is not very clear.
In this study, we introduce two methods (link prediction and likelihood
analysis) to measure multiple evolution mechanisms of complex networks. Through
tremendous experiments on artificial networks, which can be controlled to
follow multiple mechanisms with different weights, we find the method based on
likelihood analysis performs much better and gives very accurate estimations.
At last, we apply this method to some real-world networks which are from
different domains (including technology networks and social networks) and
different countries (e.g., USA and China), to see how popularity and clustering
co-evolve. We find most of them are affected by both popularity and clustering,
but with quite different weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3522</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3522</id><created>2014-10-13</created><updated>2014-11-20</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Optimizing Multi-Cell Massive MIMO for Spectral Efficiency: How Many
  Users Should Be Scheduled?</title><categories>cs.IT math.IT</categories><comments>Published at IEEE Global Conference on Signal and Information
  Processing (GlobalSIP 2014), 5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO is a promising technique to increase the spectral efficiency of
cellular networks, by deploying antenna arrays with hundreds or thousands of
active elements at the base stations and performing coherent beamforming. A
common rule-of-thumb is that these systems should have an order of magnitude
more antennas, $N$, than scheduled users, $K$, because the users' channels are
then likely to be quasi-orthogonal. However, it has not been proved that this
rule-of-thumb actually maximizes the spectral efficiency. In this paper, we
analyze how the optimal number of scheduled users, $K^\star$, depends on $N$
and other system parameters. The value of $K^\star$ in the large-$N$ regime is
derived in closed form, while simulations are used to show what happens at
finite $N$, in different interference scenarios, and for different beamforming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3541</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3541</id><created>2014-10-13</created><updated>2015-07-07</updated><authors><author><keyname>Pershin</keyname><forenames>Yuriy V.</forenames></author><author><keyname>Traversa</keyname><forenames>Fabio L.</forenames></author><author><keyname>Di Ventra</keyname><forenames>Massimiliano</forenames></author></authors><title>Memcomputing with membrane memcapacitive systems</title><categories>cs.ET cond-mat.mes-hall cs.NE</categories><journal-ref>Nanotechnology, vol. 26, page 225201 (9 pp), year 2015</journal-ref><doi>10.1088/0957-4484/26/22/225201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show theoretically that networks of membrane memcapacitive systems --
capacitors with memory made out of membrane materials -- can be used to perform
a complete set of logic gates in a massively parallel way by simply changing
the external input amplitudes, but not the topology of the network. This
polymorphism is an important characteristic of memcomputing (computing with
memories) that closely reproduces one of the main features of the brain. A
practical realization of these membrane memcapacitive systems, using, e.g.,
graphene or other 2D materials, would be a step forward towards a solid-state
realization of memcomputing with passive devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3542</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3542</id><created>2014-10-13</created><updated>2015-12-28</updated><authors><author><keyname>Gad</keyname><forenames>Eyal En</forenames></author><author><keyname>Li</keyname><forenames>Yue</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Jiang</keyname><forenames>Anxiao</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Asymmetric Error Correction and Flash-Memory Rewriting using Polar Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. Partially
  presented at ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose efficient coding schemes for two communication settings: 1.
asymmetric channels, and 2. channels with an informed encoder. These settings
are important in non-volatile memories, as well as optical and broadcast
communication. The schemes are based on non-linear polar codes, and they build
on and improve recent work on these settings. In asymmetric channels, we tackle
the exponential storage requirement of previously known schemes, that resulted
from the use of large Boolean functions. We propose an improved scheme, that
achieves the capacity of asymmetric channels with polynomial computational
complexity and storage requirement.
  The proposed non-linear scheme is then generalized to the setting of channel
coding with an informed encoder, using a multicoding technique. We consider
specific instances of the scheme for flash memories, that incorporate
error-correction capabilities together with rewriting. Since the considered
codes are non-linear, they eliminate the requirement of previously known
schemes (called polar write-once-memory codes) for shared randomness between
the encoder and the decoder. Finally, we mention that the multicoding scheme is
also useful for broadcast communication in Marton's region, improving upon
previous schemes for this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3560</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3560</id><created>2014-10-13</created><updated>2015-05-28</updated><authors><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>Ahmed</keyname><forenames>Nesreen K.</forenames></author></authors><title>NetworkRepository: An Interactive Data Repository with Multi-scale
  Visual Analytics</title><categories>cs.DL cs.HC cs.SI</categories><comments>AAAI 2015 DT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Repository (NR) is the first interactive data repository with a
web-based platform for visual interactive analytics. Unlike other data
repositories (e.g., UCI ML Data Repository, and SNAP), the network data
repository (networkrepository.com) allows users to not only download, but to
interactively analyze and visualize such data using our web-based interactive
graph analytics platform. Users can in real-time analyze, visualize, compare,
and explore data along many different dimensions. The aim of NR is to make it
easy to discover key insights into the data extremely fast with little effort
while also providing a medium for users to share data, visualizations, and
insights. Other key factors that differentiate NR from the current data
repositories is the number of graph datasets, their size, and variety. While
other data repositories are static, they also lack a means for users to
collaboratively discuss a particular dataset, corrections, or challenges with
using the data for certain applications. In contrast, we have incorporated many
social and collaborative aspects into NR in hopes of further facilitating
scientific research (e.g., users can discuss each graph, post observations,
visualizations, etc.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3564</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3564</id><created>2014-10-13</created><authors><author><keyname>Kalantari</keyname><forenames>Bahman</forenames></author></authors><title>Randomized Triangle Algorithms for Convex Hull Membership</title><categories>cs.CG</categories><comments>8 pages, 3 figures</comments><msc-class>90C05, 68U05, 68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present randomized versions of the {\it triangle algorithm} introduced in
\cite{kal14}. The triangle algorithm tests membership of a distinguished point
$p \in \mathbb{R} ^m$ in the convex hull of a given set $S$ of $n$ points in
$\mathbb{R}^m$. Given any {\it iterate} $p' \in conv(S)$, it searches for a
{\it pivot}, a point $v \in S$ so that $d(p',v) \geq d(p,v)$. It replaces $p'$
with the point on the line segment $p'v$ closest to $p$ and repeats this
process. If a pivot does not exist, $p'$ certifies that $p \not \in conv(S)$.
Here we propose two random variations of the triangle algorithm that allow
relaxed steps so as to take more effective steps possible in subsequent
iterations. One is inspired by the {\it chaos game} known to result in the
Sierpinski triangle. The incentive is that randomized iterates together with a
property of Sierpinski triangle would result in effective pivots. Bounds on
their expected complexity coincides with those of the deterministic version
derived in \cite{kal14}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3576</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3576</id><created>2014-10-14</created><authors><author><keyname>Trad</keyname><forenames>Mohamad</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author><author><keyname>Rana</keyname><forenames>Rajib</forenames></author></authors><title>Guiding Ebola Patients to Suitable Health Facilities: An SMS-based
  Approach</title><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to utilize mobile phone technology as a vehicle for people to
report their symptoms and to receive immediate feedback about the health
services readily available, and for predicting spatial disease outbreak risk.
Once symptoms are extracted from the patients text message, they undergo
complex classification, pattern matching and prediction to recommend the
nearest suitable health service. The added benefit of this approach is that it
enables health care facilities to anticipate arrival of new potential Ebola
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3577</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3577</id><created>2014-10-14</created><authors><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author></authors><title>Stochastic Geometry Modeling and Analysis of Multi-Tier Millimeter Wave
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new mathematical framework to the analysis of millimeter
wave cellular networks is introduced. Its peculiarity lies in considering
realistic path-loss and blockage models, which are derived from recently
reported experimental data. The path-loss model accounts for different
distributions of line-of-sight and non-line-of-sight propagation conditions and
the blockage model includes an outage state that provides a better
representation of the outage possibilities of millimeter wave communications.
By modeling the locations of the base stations as points of a Poisson point
process and by relying on a noise-limited approximation for typical millimeter
wave network deployments, simple and exact integral as well as approximated and
closed-form formulas for computing the coverage probability and the average
rate are obtained. With the aid of Monte Carlo simulations, the noise-limited
approximation is shown to be sufficiently accurate for typical network
densities. The proposed mathematical framework is applicable to cell
association criteria based on the smallest path-loss and on the highest
received power. It accounts for beamforming alignment errors and for multi-tier
cellular network deployments. Numerical results confirm that sufficiently dense
millimeter wave cellular networks are capable of outperforming micro wave
cellular networks, both in terms of coverage probability and average rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3582</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3582</id><created>2014-10-14</created><authors><author><keyname>Mehmood</keyname><forenames>Abid</forenames></author><author><keyname>Jawawi</keyname><forenames>Dayang N. A.</forenames></author></authors><title>An exploratory study of the suitability of UML-based aspect modeling
  techniques with respect to their integration into Model-Driven Engineering
  context</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of aspect oriented modeling approaches with model-driven
engineering process achieved through their direct transformation to
aspect-oriented code is expected to enhance the software development from many
perspectives. However, since no aspect modeling technique has been adopted as
the standard while the code generation has to be fully dependent on the input
model, it becomes imperative to compare all ubiquitous techniques on the basis
of some appropriate criteria. This study aims to assess existing UML-based
aspect-oriented modeling techniques from the perspective of their suitability
with regards to integration into model-driven engineering process through
aspect-oriented code generation. We defined an evaluation framework and
employed it to evaluate 14 well-published, UML-based aspect-oriented modeling
approaches. Further, based on the comparison results, we selected 2 modeling
approaches, Reusable Aspect Models and Theme/UML, and proceeded to evaluate
them in a detailed way from specific perspectives of design and its mapping to
the implementation code. Results of the comparison of 14 approaches show that
majority of aspect modeling approaches lack from different perspectives, which
results in reducing their use in practice within the context of model-driven
engineering. The in-depth comparison of Reusable Aspect Models and Theme/UML
reveals some points equally shared by both approaches, and identifies some
areas where the former has advantage over the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3595</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3595</id><created>2014-10-14</created><authors><author><keyname>Takizawa</keyname><forenames>Masa-aki</forenames></author><author><keyname>Yukawa</keyname><forenames>Masahiro</forenames></author><author><keyname>Richard</keyname><forenames>Cedric</forenames></author></authors><title>A stochastic behavior analysis of stochastic restricted-gradient descent
  algorithm in reproducing kernel Hilbert spaces</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a stochastic behavior analysis of a kernel-based
stochastic restricted-gradient descent method. The restricted gradient gives a
steepest ascent direction within the so-called dictionary subspace. The
analysis provides the transient and steady state performance in the mean
squared error criterion. It also includes stability conditions in the mean and
mean-square sense. The present study is based on the analysis of the kernel
normalized least mean square (KNLMS) algorithm initially proposed by Chen et
al. Simulation results validate the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3596</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3596</id><created>2014-10-14</created><updated>2014-12-03</updated><authors><author><keyname>Yamanaka</keyname><forenames>Shogo</forenames></author><author><keyname>Ohzeki</keyname><forenames>Masayuki</forenames></author><author><keyname>Decelle</keyname><forenames>Aurelien</forenames></author></authors><title>Detection of cheating by decimation algorithm</title><categories>stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG</categories><comments>15 pages, 7 figures</comments><doi>10.7566/JPSJ.84.024801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We expand the item response theory to study the case of &quot;cheating students&quot;
for a set of exams, trying to detect them by applying a greedy algorithm of
inference. This extended model is closely related to the Boltzmann machine
learning. In this paper we aim to infer the correct biases and interactions of
our model by considering a relatively small number of sets of training data.
Nevertheless, the greedy algorithm that we employed in the present study
exhibits good performance with a few number of training data. The key point is
the sparseness of the interactions in our problem in the context of the
Boltzmann machine learning: the existence of cheating students is expected to
be very rare (possibly even in real world). We compare a standard approach to
infer the sparse interactions in the Boltzmann machine learning to our greedy
algorithm and we find the latter to be superior in several aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3617</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3617</id><created>2014-10-14</created><authors><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>eTutor: Online Learning for Personalized Education</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given recent advances in information technology and artificial intelligence,
web-based education systems have became complementary and, in some cases,
viable alternatives to traditional classroom teaching. The popularity of these
systems stems from their ability to make education available to a large
demographics (see MOOCs). However, existing systems do not take advantage of
the personalization which becomes possible when web-based education is offered:
they continue to be one-size-fits-all. In this paper, we aim to provide a first
systematic method for designing a personalized web-based education system.
Personalizing education is challenging: (i) students need to be provided
personalized teaching and training depending on their contexts (e.g. classes
already taken, methods of learning preferred, etc.), (ii) for each specific
context, the best teaching and training method (e.g type and order of teaching
materials to be shown) must be learned, (iii) teaching and training should be
adapted online, based on the scores/feedback (e.g. tests, quizzes, final exam,
likes/dislikes etc.) of the students. Our personalized online system, e-Tutor,
is able to address these challenges by learning how to adapt the teaching
methodology (in this case what sequence of teaching material to present to a
student) to maximize her performance in the final exam, while minimizing the
time spent by the students to learn the course (and possibly dropouts). We
illustrate the efficiency of the proposed method on a real-world eTutor
platform which is used for remedial training for a Digital Signal Processing
(DSP) course.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3628</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3628</id><created>2014-10-14</created><authors><author><keyname>Vr&#x161;ek</keyname><forenames>Jan</forenames></author><author><keyname>L&#xe1;vi&#x10d;ka</keyname><forenames>Miroslav</forenames></author></authors><title>Recognizing implicitly given rational canal surfaces</title><categories>cs.SC math.AG</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is still a challenging task of today to recognize the type of a given
algebraic surface which is described only by its implicit representation.
In~this paper we will investigate in more detail the case of canal surfaces
that are often used in geometric modelling, Computer-Aided Design and technical
practice (e.g. as blending surfaces smoothly joining two parts with circular
ends). It is known that if the squared medial axis transform is a rational
curve then so is also the corresponding surface. However, starting from a
polynomial it is not known how to decide if the corresponding algebraic surface
is rational canal surface or not. Our goal is to formulate a simple and
efficient algorithm whose input is a~polynomial with the coefficients from some
subfield of real numbers and the output is the answer whether the surface is a
rational canal surface. In the affirmative case we also compute a rational
parameterization of the squared medial axis transform which can be then used
for finding a rational parameterization of the implicitly given canal surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3632</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3632</id><created>2014-10-14</created><authors><author><keyname>Brim</keyname><forenames>L.</forenames></author><author><keyname>Niznan</keyname><forenames>J.</forenames></author><author><keyname>Safranek</keyname><forenames>D.</forenames></author></authors><title>Compact Representation of Photosynthesis Dynamics by Rule-based Models
  (Full Version)</title><categories>q-bio.MN cs.CE</categories><comments>SASB 2014 full paper</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Traditional mathematical models of photosynthesis are based on mass action
kinetics of light reactions. This approach requires the modeller to enumerate
all the possible state combinations of the modelled chemical species. This
leads to combinatorial explosion in the number of reactions although the
structure of the model could be expressed more compactly. We explore the use of
rule-based modelling, in particular, a simplified variant of Kappa, to
compactly capture and automatically reduce existing mathematical models of
photosynthesis. Finally, the reduction procedure is implemented in BioNetGen
language and demonstrated on several ODE models of photosynthesis processes.
This is an extended version of the paper published in proceedings of 5th
International Workshop on Static Analysis and Systems Biology (SASB) 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3637</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3637</id><created>2014-10-14</created><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>General Position Subsets and Independent Hyperplanes in d-Space</title><categories>math.CO cs.CG</categories><comments>8 pages</comments><msc-class>52C35, 52C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Erd\H{o}s asked what is the maximum number $\alpha(n)$ such that every set of
$n$ points in the plane with no four on a line contains $\alpha(n)$ points in
general position. We consider variants of this question for $d$-dimensional
point sets and generalize previously known bounds. In particular, we prove the
following two results for fixed $d$:
  - Every set $H$ of $n$ hyperplanes in $\mathbb{R}^d$ contains a subset
$S\subseteq H$ of size at least $c \left(n \log n\right)^{1/d}$, for some
constant $c=c(d)&gt;0$, such that no cell of the arrangement of $H$ is bounded by
hyperplanes of $S$ only.
  - Every set of $cq^d\log q$ points in $\mathbb{R}^d$, for some constant
$c=c(d)&gt;0$, contains a subset of $q$ cohyperplanar points or $q$ points in
general position.
  Two-dimensional versions of the above results were respectively proved by
Ackerman et al. [Electronic J. Combinatorics, 2014] and by Payne and Wood [SIAM
J. Discrete Math., 2013].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3656</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3656</id><created>2014-10-14</created><authors><author><keyname>Sahraei</keyname><forenames>Saeid</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Compute-and-Forward: Finding the Best Equation</title><categories>cs.IT math.IT</categories><comments>Paper presented at 52nd Allerton Conference, October 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compute-and-Forward is an emerging technique to deal with interference. It
allows the receiver to decode a suitably chosen integer linear combination of
the transmitted messages. The integer coefficients should be adapted to the
channel fading state. Optimizing these coefficients is a Shortest Lattice
Vector (SLV) problem. In general, the SLV problem is known to be prohibitively
complex. In this paper, we show that the particular SLV instance resulting from
the Compute-and-Forward problem can be solved in low polynomial complexity and
give an explicit deterministic algorithm that is guaranteed to find the optimal
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3673</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3673</id><created>2014-10-14</created><updated>2015-08-22</updated><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Zhou</keyname><forenames>Yiqing</forenames></author><author><keyname>Pan</keyname><forenames>Zhengang</forenames></author><author><keyname>Wang</keyname><forenames>Hualei</forenames></author></authors><title>Transceiver designs with matrix-version water-filling architecture under
  mixed power constraints</title><categories>cs.IT math.IT</categories><comments>13 Pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the multiple-input multiple-output (MIMO)
transceiver design under an interesting power model named mixed power
constraints. In the considered power model, several antenna subsets are
constrained by sum power constraints while the other antennas are subject to
per-antenna power constraints. This kind of transceiver designs includes both
the transceiver designs under sum power constraint and per-antenna power
constraint as its special cases. This kind of designs is of critical importance
for distributed antenna systems (DASs) with heterogeneous remote radio heads
(RRHs) such as cloud radio access networks (C-RANs). In our work, we try to
solve the optimization problem in an analytical way instead of using some
famous software packages e.g., CVX or SeDuMi. In our work, to strike tradeoffs
between performance and complexity, both iterative and non-iterative solutions
are proposed. Interestingly the non-iterative solution can be interpreted as a
matrix version water-filling solution extended from the well-known and
extensively studied vector version. Finally, simulation results demonstrate the
accuracy of our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3677</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3677</id><created>2014-10-14</created><authors><author><keyname>Poghosyan</keyname><forenames>Gevorg</forenames></author><author><keyname>Matta</keyname><forenames>Sanchit</forenames></author><author><keyname>Streit</keyname><forenames>Achim</forenames></author><author><keyname>Bejger</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Kr&#xf3;lak</keyname><forenames>Andrzej</forenames></author></authors><title>Architecture, implementation and parallelization of the software to
  search for periodic gravitational wave signals</title><categories>gr-qc astro-ph.HE cs.DC cs.PF cs.SE</categories><comments>11 pages, 9 figures. Submitted to Computer Physics Communications</comments><journal-ref>Computer Physics Communications volume 188 pages 168 - 176 (2015)</journal-ref><doi>10.1016/j.cpc.2014.10.025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parallelization, design and scalability of the \sky code to search for
periodic gravitational waves from rotating neutron stars is discussed. The code
is based on an efficient implementation of the F-statistic using the Fast
Fourier Transform algorithm. To perform an analysis of data from the advanced
LIGO and Virgo gravitational wave detectors' network, which will start
operating in 2015, hundreds of millions of CPU hours will be required - the
code utilizing the potential of massively parallel supercomputers is therefore
mandatory. We have parallelized the code using the Message Passing Interface
standard, implemented a mechanism for combining the searches at different
sky-positions and frequency bands into one extremely scalable program. The
parallel I/O interface is used to escape bottlenecks, when writing the
generated data into file system. This allowed to develop a highly scalable
computation code, which would enable the data analysis at large scales on
acceptable time scales. Benchmarking of the code on a Cray XE6 system was
performed to show efficiency of our parallelization concept and to demonstrate
scaling up to 50 thousand cores in parallel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3682</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3682</id><created>2014-10-14</created><authors><author><keyname>Chouvardas</keyname><forenames>Symeon</forenames></author><author><keyname>Mileounis</keyname><forenames>Gerasimos</forenames></author><author><keyname>Kalouptsidis</keyname><forenames>Nicholas</forenames></author><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames></author></authors><title>Greedy Sparsity-Promoting Algorithms for Distributed Learning</title><categories>cs.IT math.IT</categories><comments>Paper submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2393839</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the development of novel greedy techniques for
distributed learning under sparsity constraints. Greedy techniques have widely
been used in centralized systems due to their low computational requirements
and at the same time their relatively good performance in estimating sparse
parameter vectors/signals. The paper reports two new algorithms in the context
of sparsity--aware learning. In both cases, the goal is first to identify the
support set of the unknown signal and then to estimate the non--zero values
restricted to the active support set. First, an iterative greedy multi--step
procedure is developed, based on a neighborhood cooperation strategy, using
batch processing on the observed data. Next, an extension of the algorithm to
the online setting, based on the diffusion LMS rationale for adaptivity, is
derived. Theoretical analysis of the algorithms is provided, where it is shown
that the batch algorithm converges to the unknown vector if a Restricted
Isometry Property (RIP) holds. Moreover, the online version converges in the
mean to the solution vector under some general assumptions. Finally, the
proposed schemes are tested against recently developed sparsity--promoting
algorithms and their enhanced performance is verified via simulation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3688</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3688</id><created>2014-10-14</created><authors><author><keyname>Khammassi</keyname><forenames>Iyed</forenames></author><author><keyname>Elazouzi</keyname><forenames>Rachid</forenames></author><author><keyname>Haddad</keyname><forenames>Majed</forenames></author><author><keyname>Mabrouki</keyname><forenames>Issam</forenames></author></authors><title>A Game Theoretic Model for Network Virus Protection</title><categories>cs.GT cs.SI</categories><comments>Technical report, 8 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The network virus propagation is influenced by various factors, and some of
them are neglected in most of the existed models in the literature. In this
paper, we study the network virus propagation based on the the epidemiological
viewpoint. We assume that nodes can be equipped with protection against virus
and the security of a node depends not only on his protection strategy but also
by those chosen by other nodes in the network. A crucial aspect is whether
owners of device, e.g., either smartphones, machines or tablets, are willing to
be equipped to protect themselves or to take the risk to be contaminated in
order to avoid the payment for a new antivirus. We model the interaction
between nodes as a non-cooperative games where the node has two strategies:
either to update the antivirus or not. To this aim, we provide a full
characterization of the equilibria of the game and we investigate the impact of
the price of protection on the equilibrium as well as the efficiency of the
protection at equilibrium. Further we consider more realistic scenarios in
which the dynamic of sources that disseminate the virus, evolves as function of
the popularity of virus. In this work, the interest in the virus by sources
evolves under the Influence Linear Threshold (HILT) model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3691</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3691</id><created>2014-10-14</created><authors><author><keyname>Zhang</keyname><forenames>Tong</forenames></author><author><keyname>Tao</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Cui</keyname><forenames>Qimei</forenames></author></authors><title>Joint Multi-Cell Resource Allocation Using Pure Binary-Integer
  Programming for LTE Uplink</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted to IEEE Vehicular Technology Conference (VTC Spring), Seoul,
  Korea, May, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to high system capacity requirement, 3GPP Long Term Evolution (LTE) is
likely to adopt frequency reuse factor 1 at the cost of suffering severe
inter-cell interference (ICI). One of combating ICI strategies is network
cooperation of resource allocation (RA). For LTE uplink RA, requiring all the
subcarriers to be allocated adjacently complicates the RA problem greatly. This
paper investigates the joint multi-cell RA problem for LTE uplink. We model the
uplink RA and ICI mitigation problem using pure binary-integer programming
(BIP), with integrative consideration of all users' channel state information
(CSI). The advantage of the pure BIP model is that it can be solved by
branch-and-bound search (BBS) algorithm or other BIP solving algorithms, rather
than resorting to exhaustive search. The system-level simulation results show
that it yields 14.83% and 22.13% gains over single-cell optimal RA in average
spectrum efficiency and 5th percentile of user throughput, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3694</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3694</id><created>2014-10-11</created><authors><author><keyname>Hamadou</keyname><forenames>Sardaouna</forenames></author><author><keyname>Gherbi</keyname><forenames>Abdelouahed</forenames></author><author><keyname>Mullins</keyname><forenames>John</forenames></author><author><keyname>Beji</keyname><forenames>Sofiene</forenames></author></authors><title>A Time-Triggered Constraint-Based Calculus for Avionic Systems</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Integrated Modular Avionics (IMA) architec- ture and the Time-Triggered
Ethernet (TTEthernet) network have emerged as the key components of a typical
architecture model for recent civil aircrafts. We propose a real-time
constraint-based calculus targeted at the analysis of such concepts of avionic
embedded systems. We show our framework at work on the modelisation of both the
(IMA) architecture and the TTEthernet network, illustrating their behavior by
the well-known Flight Management System (FMS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3699</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3699</id><created>2014-10-14</created><authors><author><keyname>Ammanouil</keyname><forenames>Rita</forenames></author><author><keyname>Ferrari</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>A graph Laplacian regularization for hyperspectral data unmixing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a graph Laplacian regularization in the hyperspectral
unmixing formulation. The proposed regularization relies upon the construction
of a graph representation of the hyperspectral image. Each node in the graph
represents a pixel's spectrum, and edges connect spectrally and spatially
similar pixels. The proposed graph framework promotes smoothness in the
estimated abundance maps and collaborative estimation between homogeneous areas
of the image. The resulting convex optimization problem is solved using the
Alternating Direction Method of Multipliers (ADMM). A special attention is
given to the computational complexity of the algorithm, and Graph-cut methods
are proposed in order to reduce the computational burden. Finally, simulations
conducted on synthetic data illustrate the effectiveness of the graph Laplacian
regularization with respect to other classical regularizations for
hyperspectral unmixing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3705</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3705</id><created>2014-10-14</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Liu</keyname><forenames>Chen</forenames></author><author><keyname>Mao</keyname><forenames>Wenli</forenames></author></authors><title>Does A Paper Being Featured on The Cover of A Journal Guarantee More
  Attention and Greater Impact?</title><categories>cs.DL</categories><comments>9 pages, 3 figures, Scientometrics, 2014</comments><doi>10.1007/s11192-014-1456-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paper featured on the cover of a journal has more visibility in an issue
compared with other ordinary articles for both printed and electronic journal.
Does this kind of visibility guarantee more attention and greater impact of its
associated content than the non-cover papers? In this research, usage and
citation data of 60 issues of PLOS Biology from 2006 to 2010 are analyzed to
compare the attention and scholarly impact between cover and non-cover paper.
Our empirical study confirms that, in most cases, the group difference between
cover and non-cover paper is not significant for attention or impact. Cover
paper is not the best one, nor at the upper level in one issue considering the
attention or the citation impact. Having a paper featured on the cover of a
journal may be a source of pride to researchers, many institutions and
researchers would even release news about it. However, a paper being featured
on the cover of a journal doesn't guarantee more attention and greater impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3710</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3710</id><created>2014-10-14</created><authors><author><keyname>Kwak</keyname><forenames>Haewoon</forenames></author><author><keyname>An</keyname><forenames>Jisun</forenames></author></authors><title>Understanding News Geography and Major Determinants of Global News
  Coverage of Disasters</title><categories>cs.CY physics.soc-ph</categories><comments>Presented at Computation+Jounalism Symposium (C+J Symposium) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we reveal the structure of global news coverage of disasters
and its determinants by using a large-scale news coverage dataset collected by
the GDELT (Global Data on Events, Location, and Tone) project that monitors
news media in over 100 languages from the whole world. Significant variables in
our hierarchical (mixed-effect) regression model, such as the number of
population, the political stability, the damage, and more, are well aligned
with a series of previous research. Yet, strong regionalism we found in news
geography highlights the necessity of the comprehensive dataset for the study
of global news coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3711</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3711</id><created>2014-10-14</created><authors><author><keyname>Seo</keyname><forenames>Junyeong</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author><author><keyname>Lee</keyname><forenames>Gilwon</forenames></author><author><keyname>Kim</keyname><forenames>Donggun</forenames></author></authors><title>Training Beam Sequence Design for Millimeter-Wave MIMO Systems: A POMDP
  Framework</title><categories>cs.IT math.IT</categories><comments>This paper is the journal version of the previous conference version
  submitted to ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, adaptive training beam sequence design for efficient channel
estimation in large millimeter-wave(mmWave) multiple-input multiple-output
(MIMO) channels is considered. By exploiting the sparsity in large mmWave MIMO
channels and imposing a Markovian random walk assumption on the movement of the
receiver and reflection clusters, the adaptive training beam sequence design
and channel estimation problem is formulated as a partially observableMarkov
decision process (POMDP) problem that finds non-zero bins in a two-dimensional
grid. Under the proposed POMDP framework, optimal and suboptimal adaptive
training beam sequence design policies are derived. Furthermore, a very fast
suboptimal greedy algorithm is developed based on a newly proposed reduced
sufficient statistic to make the computational complexity of the proposed
algorithm low to a level for practical implementation. Numerical results are
provided to evaluate the performance of the proposed training beam design
method. Numerical results show that the proposed training beam sequence design
algorithms yield good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3712</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3712</id><created>2014-10-14</created><updated>2014-10-15</updated><authors><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames></author></authors><title>Process-aware web programming with Jolie</title><categories>cs.DC cs.NI cs.PL</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a programming language and runtime, which extends the Jolie
programming language, for the native modelling of process-aware web information
systems, i.e., web information systems based upon the execution of business
processes. Our main contribution is to offer a unifying approach for the
programming of distributed architectures on the web, which can capture web
servers, stateful process execution, and the composition of services via
mediation in a system. We discuss many examples around these aspects and show
how they can be captured using our approach, covering, e.g., static content
serving, multiparty sessions, and the evolution of web systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3723</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3723</id><created>2014-10-11</created><authors><author><keyname>Nabiyev</keyname><forenames>Rifkat I.</forenames></author><author><keyname>Nabiyev</keyname><forenames>Ilshat H.</forenames></author><author><keyname>Ziatdinov</keyname><forenames>Rushan</forenames></author></authors><title>The potential of mobile exhibition as a form of implementation for
  social transformation and educational expo-design in addressing public social
  problems</title><categories>cs.CY</categories><journal-ref>Mathematical Design &amp; Technical Aesthetics, 2014, Vol. 2, No. 1,
  pp. 14-35</journal-ref><doi>10.13187/md.2014.2.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article analyzes the humanistic basis of expo-design in conjunction with
the ideological problems of modern design and the causes of their occurrence.
It reveals the potential of mobile exhibition for social transformation in
terms of its structural and ideological aesthetic content. It substantiates the
effectiveness of expo-design expression's artistic and aesthetic means in
viewer personality moral education. It investigates the features of social
problems design presentation in the unity of the formative and design material
components features of mobile exhibition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3726</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3726</id><created>2014-10-14</created><authors><author><keyname>Lim</keyname><forenames>Chern Hong</forenames></author><author><keyname>Risnumawan</keyname><forenames>Anhar</forenames></author><author><keyname>Chan</keyname><forenames>Chee Seng</forenames></author></authors><title>Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene
  Understanding</title><categories>cs.CV cs.AI cs.IR</categories><comments>Accepted in IEEE Transactions on Fuzzy Systems</comments><journal-ref>IEEE Transactions on Fuzzy Systems, vol. 22(6), pp. 1541 - 1556,
  2014</journal-ref><doi>10.1109/TFUZZ.2014.2298233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ambiguity or uncertainty is a pervasive element of many real world decision
making processes. Variation in decisions is a norm in this situation when the
same problem is posed to different subjects. Psychological and metaphysical
research had proven that decision making by human is subjective. It is
influenced by many factors such as experience, age, background, etc. Scene
understanding is one of the computer vision problems that fall into this
category. Conventional methods relax this problem by assuming scene images are
mutually exclusive; and therefore, focus on developing different approaches to
perform the binary classification tasks. In this paper, we show that scene
images are non-mutually exclusive, and propose the Fuzzy Qualitative Rank
Classifier (FQRC) to tackle the aforementioned problems. The proposed FQRC
provides a ranking interpretation instead of binary decision. Evaluations in
term of qualitative and quantitative using large numbers and challenging public
scene datasets have shown the effectiveness of our proposed method in modeling
the non-mutually exclusive scene images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3728</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3728</id><created>2014-10-14</created><authors><author><keyname>Lin</keyname><forenames>Xingqin</forenames></author><author><keyname>Jiang</keyname><forenames>Libin</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Performance Analysis of Asynchronous Multicarrier Wireless Networks</title><categories>cs.IT math.IT</categories><comments>13 pages; 10 figures; submitted to IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a novel analytical framework for asynchronous wireless
networks deploying multicarrier transmission. Nodes in the network have
different notions of timing, so from the viewpoint of a typical receiver, the
received signals from different transmitters are asynchronous, leading to a
loss of orthogonality between subcarriers. We first develop a detailed
link-level analysis based on OFDM, based on which we propose a tractable
system-level signal-to-interference-plus-noise ratio (SINR) model for
asynchronous OFDM networks. The proposed model is used to analytically
characterize several important statistics in asynchronous networks with
spatially distributed transmitters, including (i) the number of decodable
transmitters, (ii) the decoding probability of the nearest transmitter, and
(iii) the system throughput. The system-level loss from lack of synchronization
is quantified, and to mitigate the loss, we compare and discuss four possible
solutions including extended cyclic prefix, advanced receiver timing, dynamic
receiver timing positioning, and semi-static receiver timing positioning with
multiple timing hypotheses. The model and results are general, and apply to ad
hoc networks, cellular systems, and neighbor discovery in device-to-device
(D2D) networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3735</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3735</id><created>2014-10-14</created><authors><author><keyname>Petcher</keyname><forenames>Adam</forenames></author><author><keyname>Morrisett</keyname><forenames>Greg</forenames></author></authors><title>The Foundational Cryptography Framework</title><categories>cs.PL cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Foundational Cryptography Framework (FCF) for developing and
checking complete proofs of security for cryptographic schemes within a proof
assistant. This is a general-purpose framework that is capable of modeling and
reasoning about a wide range of cryptographic schemes, security definitions,
and assumptions. Security is proven in the computational model, and the proof
provides concrete bounds as well as asymptotic conclusions. FCF provides a
language for probabilistic programs, a theory that is used to reason about
programs, and a library of tactics and definitions that are useful in proofs
about cryptography. The framework is designed to leverage fully the existing
theory and capabilities of the Coq proof assistant in order to reduce the
effort required to develop proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3744</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3744</id><created>2014-10-14</created><authors><author><keyname>Lim</keyname><forenames>Mei Kuan</forenames></author><author><keyname>Chan</keyname><forenames>Chee Seng</forenames></author><author><keyname>Monekosso</keyname><forenames>Dorothy</forenames></author><author><keyname>Remagnino</keyname><forenames>Paolo</forenames></author></authors><title>Refined Particle Swarm Intelligence Method for Abrupt Motion Tracking</title><categories>cs.CV cs.NE</categories><comments>Accepted in Information Sciences, new abrupt motion (MAMo) dataset is
  introduced</comments><journal-ref>Information Sciences, vol. 283, pp. 267-287, 2014</journal-ref><doi>10.1016/j.ins.2014.01.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional tracking solutions are not feasible in handling abrupt motion as
they are based on smooth motion assumption or an accurate motion model. Abrupt
motion is not subject to motion continuity and smoothness. To assuage this, we
deem tracking as an optimisation problem and propose a novel abrupt motion
tracker that based on swarm intelligence - the SwaTrack. Unlike existing
swarm-based filtering methods, we first of all introduce an optimised
swarm-based sampling strategy to tradeoff between the exploration and
exploitation of the search space in search for the optimal proposal
distribution. Secondly, we propose Dynamic Acceleration Parameters (DAP) allow
on the fly tuning of the best mean and variance of the distribution for
sampling. Such innovating idea of combining these strategies in an ingenious
way in the PSO framework to handle the abrupt motion, which so far no existing
works are found. Experimental results in both quantitative and qualitative had
shown the effectiveness of the proposed method in tracking abrupt motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3748</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3748</id><created>2014-10-14</created><authors><author><keyname>Hoo</keyname><forenames>Wai Lam</forenames></author><author><keyname>Chan</keyname><forenames>Chee Seng</forenames></author></authors><title>Zero-Shot Object Recognition System based on Topic Model</title><categories>cs.CV stat.ML</categories><comments>To appear in IEEE Transactions on Human-Machine Systems</comments><doi>10.1109/THMS.2014.2358649</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object recognition systems usually require fully complete manually labeled
training data to train the classifier. In this paper, we study the problem of
object recognition where the training samples are missing during the classifier
learning stage, a task also known as zero-shot learning. We propose a novel
zero-shot learning strategy that utilizes the topic model and hierarchical
class concept. Our proposed method advanced where cumbersome human annotation
stage (i.e. attribute-based classification) is eliminated. We achieve
comparable performance with state-of-the-art algorithms in four public
datasets: PubFig (67.09%), Cifar-100 (54.85%), Caltech-256 (52.14%), and
Animals with Attributes (49.65%) when unseen classes exist in the
classification task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3751</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3751</id><created>2014-10-14</created><authors><author><keyname>Tan</keyname><forenames>Wei Ren</forenames></author><author><keyname>Chan</keyname><forenames>Chee Seng</forenames></author><author><keyname>Yogarajah</keyname><forenames>Pratheepan</forenames></author><author><keyname>Condell</keyname><forenames>Joan</forenames></author></authors><title>A Fusion Approach for Efficient Human Skin Detection</title><categories>cs.CV stat.ML</categories><comments>Accepted in IEEE Transactions on Industrial Informatics, vol. 8(1),
  pp. 138-147, new skin detection + ground truth (Pratheepan) dataset</comments><journal-ref>IEEE Transactions on Industrial Informatics, vol. 8(1), pp.
  138-147, 2012</journal-ref><doi>10.1109/TII.2011.2172451</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A reliable human skin detection method that is adaptable to different human
skin colours and illu- mination conditions is essential for better human skin
segmentation. Even though different human skin colour detection solutions have
been successfully applied, they are prone to false skin detection and are not
able to cope with the variety of human skin colours across different ethnic.
Moreover, existing methods require high computational cost. In this paper, we
propose a novel human skin de- tection approach that combines a smoothed 2D
histogram and Gaussian model, for automatic human skin detection in colour
image(s). In our approach an eye detector is used to refine the skin model for
a specific person. The proposed approach reduces computational costs as no
training is required; and it improves the accuracy of skin detection despite
wide variation in ethnicity and illumination. To the best of our knowledge,
this is the first method to employ fusion strategy for this purpose.
Qualitative and quantitative results on three standard public datasets and a
comparison with state-of-the-art methods have shown the effectiveness and
robustness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3752</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3752</id><created>2014-10-14</created><authors><author><keyname>Hoo</keyname><forenames>Wai Lam</forenames></author><author><keyname>Kim</keyname><forenames>Tae-Kyun</forenames></author><author><keyname>Pei</keyname><forenames>Yuru</forenames></author><author><keyname>Chan</keyname><forenames>Chee Seng</forenames></author></authors><title>Enhanced Random Forest with Image/Patch-Level Learning for Image
  Understanding</title><categories>cs.CV stat.ML</categories><comments>Accepted in ICPR 2014 (Oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image understanding is an important research domain in the computer vision
due to its wide real-world applications. For an image understanding framework
that uses the Bag-of-Words model representation, the visual codebook is an
essential part. Random forest (RF) as a tree-structure discriminative codebook
has been a popular choice. However, the performance of the RF can be degraded
if the local patch labels are poorly assigned. In this paper, we tackle this
problem by a novel way to update the RF codebook learning for a more
discriminative codebook with the introduction of the soft class labels,
estimated from the pLSA model based on a feedback scheme. The feedback scheme
is performed on both the image and patch levels respectively, which is in
contrast to the state- of-the-art RF codebook learning that focused on either
image or patch level only. Experiments on 15-Scene and C-Pascal datasets had
shown the effectiveness of the proposed method in image understanding task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3756</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3756</id><created>2014-10-14</created><authors><author><keyname>Lim</keyname><forenames>Mei Kuan</forenames></author><author><keyname>Kok</keyname><forenames>Ven Jyn</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Chan</keyname><forenames>Chee Seng</forenames></author></authors><title>Crowd Saliency Detection via Global Similarity Structure</title><categories>cs.CV stat.ML</categories><comments>Accepted in ICPR 2014 (Oral). Mei Kuan Lim and Ven Jyn Kok share
  equal contributions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common for CCTV operators to overlook inter- esting events taking place
within the crowd due to large number of people in the crowded scene (i.e.
marathon, rally). Thus, there is a dire need to automate the detection of
salient crowd regions acquiring immediate attention for a more effective and
proactive surveillance. This paper proposes a novel framework to identify and
localize salient regions in a crowd scene, by transforming low-level features
extracted from crowd motion field into a global similarity structure. The
global similarity structure representation allows the discovery of the
intrinsic manifold of the motion dynamics, which could not be captured by the
low-level representation. Ranking is then performed on the global similarity
structure to identify a set of extrema. The proposed approach is unsupervised
so learning stage is eliminated. Experimental results on public datasets
demonstrates the effectiveness of exploiting such extrema in identifying
salient regions in various crowd scenarios that exhibit crowding, local
irregular motion, and unique motion areas such as sources and sinks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3764</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3764</id><created>2014-10-14</created><updated>2015-12-03</updated><authors><author><keyname>Kozik</keyname><forenames>Jakub</forenames></author><author><keyname>Matecki</keyname><forenames>Grzegorz</forenames></author></authors><title>A lazy approach to on-line bipartite matching</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach, called a lazy matching, to the problem of on-line
matching on bipartite graphs. Imagine that one side of a graph is given and the
vertices of the other side are arriving on-line. Originally, incoming vertex is
either irrevocably matched to an another element or stays forever unmatched. A
lazy algorithm is allowed to match a new vertex to a group of elements
(possibly empty) and afterwords, forced against next vertices, may give up
parts of the group. The restriction is that all the time each element is in at
most one group. We present an optimal lazy algorithm (deterministic) and prove
that its competitive ratio equals $1-\pi/\cosh(\frac{\sqrt{3}}{2}\pi)\approx
0.588$. The lazy approach allows us to break the barrier of $1/2$, which is the
best competitive ratio that can be guaranteed by any deterministic algorithm in
the classical on-line matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3770</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3770</id><created>2014-10-13</created><authors><author><keyname>Fijalkow</keyname><forenames>Nathana&#xeb;l</forenames><affiliation>LIAFA</affiliation></author></authors><title>What is known about the Value 1 Problem for Probabilistic Automata?</title><categories>cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The value 1 problem is a decision problem for probabilistic automata over
finite words: are there words accepted by the automaton with arbitrarily high
probability? Although undecidable, this problem attracted a lot of attention
over the last few years. The aim of this paper is to review and relate the
results pertaining to the value 1 problem. In particular, several algorithms
have been proposed to partially solve this problem. We show the relations
between them, leading to the following conclusion: the Markov Monoid Algorithm
is the most correct algorithm known to (partially) solve the value 1 problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3772</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3772</id><created>2014-10-12</created><authors><author><keyname>Jain</keyname><forenames>Rishabh</forenames></author><author><keyname>Gupta</keyname><forenames>Sakshi</forenames></author></authors><title>Optimizing the For loop: Comparison of For loop and micro For loop</title><categories>cs.PL</categories><comments>3 pages, 2 figures, 2 tables</comments><msc-class>68-02</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Looping is one of the fundamental logical instructions used for repeating a
block of code. It is used in programs across all programming languages.
Traditionally, in languages like C, the for loop is used extensively for
repeated execution of a block of code, due to its ease for use and simplified
representation. This paper proposes a new way of representing the for loop to
improve its runtime efficiency and compares the experimental statistics with
the traditional for loop representation. It is found that for small number of
iterations, the difference in computational time may not be considerable. But
given any large number of iterations, the difference is noticeable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3773</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3773</id><created>2014-10-13</created><authors><author><keyname>Li</keyname><forenames>Guozheng</forenames></author><author><keyname>Cao</keyname><forenames>Zining</forenames></author><author><keyname>Gao</keyname><forenames>Zheng</forenames></author></authors><title>Refinement Checking for Multirate Hybrid ZIA</title><categories>cs.LO cs.FL</categories><comments>11pages, 3figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hybrid system is a dynamical system with both discrete and continuous
components. In order to study the modeling and verification aspects of hybrid
system, in this paper we first introduce a specification approach combining
interface automata, initialized multirate hybrid automata and Z language, which
is named MZIA. Meanwhile we propose a refinement relation on MZIAs. Then we
give an algorithm for checking refinement relation between MZIAs with finite
domain and demonstrate the correctness of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3778</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3778</id><created>2014-10-14</created><authors><author><keyname>Lewandowski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Knychala</keyname><forenames>Piotr</forenames></author><author><keyname>Banaszak</keyname><forenames>Michal</forenames></author></authors><title>Parallel-Tempering Monte-Carlo Simulation with Feedback-Optimized
  Algorithm Applied to a Coil-to-Globule Transition of a Lattice Homopolymer</title><categories>cond-mat.stat-mech cs.CE</categories><journal-ref>Computational Methods in Science and Technology, 16(1), 29-35,
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a study of the parallel tempering (replica exchange) Monte Carlo
method, with special focus on the feedback-optimized parallel tempering
algorithm, used for generating an optimal set of simulation temperatures. This
method is applied to a lattice simulation of a homopolymer chain undergoing a
coil-to-globule transition upon cooling. We select the optimal number of
replicas for different chain lengths, N = 25, 50 and 75, using replica's
round-trip time in temperature space, in order to determine energy, specific
heat, and squared end-to-end distance of the hopolymer chain for the selected
temperatures. We also evaluate relative merits of this optimization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3786</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3786</id><created>2014-10-14</created><updated>2015-01-29</updated><authors><author><keyname>Harms</keyname><forenames>Andrew</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>Identification of Linear Time-Varying Systems Through Waveform Diversity</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Signal Processing;
  32 pages, 13 figures</comments><doi>10.1109/TSP.2015.2407319</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear, time-varying (LTV) systems composed of time shifts, frequency shifts,
and complex amplitude scalings are operators that act on continuous
finite-energy waveforms. This paper presents a novel, resource-efficient method
for identifying the parametric description of such systems, i.e., the time
shifts, frequency shifts, and scalings, from the sampled response to linear
frequency modulated (LFM) waveforms, with emphasis on the application to radar
processing. If the LTV operator is probed with a sufficiently diverse set of
LFM waveforms, then the system can be identified with high accuracy. In the
case of noiseless measurements, the identification is perfect, while in the
case of noisy measurements, the accuracy is inversely proportional to the noise
level. The use of parametric estimation techniques with recently proposed
denoising algorithms allows the estimation of the parameters with high
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3791</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3791</id><created>2014-10-14</created><authors><author><keyname>Al-Rfou</keyname><forenames>Rami</forenames></author><author><keyname>Kulkarni</keyname><forenames>Vivek</forenames></author><author><keyname>Perozzi</keyname><forenames>Bryan</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>POLYGLOT-NER: Massive Multilingual Named Entity Recognition</title><categories>cs.CL cs.LG</categories><comments>9 pages, 4 figures, 5 tables</comments><acm-class>I.2.7; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing diversity of languages used on the web introduces a new level
of complexity to Information Retrieval (IR) systems. We can no longer assume
that textual content is written in one language or even the same language
family. In this paper, we demonstrate how to build massive multilingual
annotators with minimal human expertise and intervention. We describe a system
that builds Named Entity Recognition (NER) annotators for 40 major languages
using Wikipedia and Freebase. Our approach does not require NER human annotated
datasets or language specific resources like treebanks, parallel corpora, and
orthographic rules. The novelty of approach lies therein - using only language
agnostic techniques, while achieving competitive performance.
  Our method learns distributed word representations (word embeddings) which
encode semantic and syntactic features of words in each language. Then, we
automatically generate datasets from Wikipedia link structure and Freebase
attributes. Finally, we apply two preprocessing stages (oversampling and exact
surface form matching) which do not require any linguistic expertise.
  Our evaluation is two fold: First, we demonstrate the system performance on
human annotated datasets. Second, for languages where no gold-standard
benchmarks are available, we propose a new method, distant evaluation, based on
statistical machine translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3812</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3812</id><created>2014-10-14</created><authors><author><keyname>Wei</keyname><forenames>Yi-Peng</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Polar Coding for the General Wiretap Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE ITW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-theoretic work for wiretap channels is mostly based on random
coding schemes. Designing practical coding schemes to achieve
information-theoretic security is an important problem. By applying the two
recently developed techniques for polar codes, we propose a polar coding scheme
to achieve the secrecy capacity of the general wiretap channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3820</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3820</id><created>2014-10-14</created><authors><author><keyname>Bodlaender</keyname><forenames>Hans L.</forenames></author><author><keyname>van Kreveld</keyname><forenames>Marc</forenames></author></authors><title>Google Scholar makes it Hard - the complexity of organizing one's
  publications</title><categories>cs.DL cs.CC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With Google Scholar, scientists can maintain their publications on personal
profile pages, while the citations to these works are automatically collected
and counted. Maintenance of publications is done manually by the researcher
herself, and involves deleting erroneous ones, merging ones that are the same
but which were not recognized as the same, adding forgotten co-authors, and
correcting titles of papers and venues. The publications are presented on pages
with 20 or 100 papers in the web page interface from 2012--2014. The interface
does not allow a scientist to merge two version of a paper if they appear on
different pages. This not only implies that a scientist who wants to merge
certain subsets of publications will sometimes be unable to do so, but also, we
show in this note that the decision problem to determine if it is possible to
merge given subsets of papers is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3831</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3831</id><created>2014-10-14</created><authors><author><keyname>Mehta</keyname><forenames>Pankaj</forenames></author><author><keyname>Schwab</keyname><forenames>David J.</forenames></author></authors><title>An exact mapping between the Variational Renormalization Group and Deep
  Learning</title><categories>stat.ML cond-mat.stat-mech cs.LG cs.NE</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning is a broad set of techniques that uses multiple layers of
representation to automatically learn relevant features directly from
structured data. Recently, such techniques have yielded record-breaking results
on a diverse set of difficult machine learning tasks in computer vision, speech
recognition, and natural language processing. Despite the enormous success of
deep learning, relatively little is understood theoretically about why these
techniques are so successful at feature learning and compression. Here, we show
that deep learning is intimately related to one of the most important and
successful techniques in theoretical physics, the renormalization group (RG).
RG is an iterative coarse-graining scheme that allows for the extraction of
relevant features (i.e. operators) as a physical system is examined at
different length scales. We construct an exact mapping from the variational
renormalization group, first introduced by Kadanoff, and deep learning
architectures based on Restricted Boltzmann Machines (RBMs). We illustrate
these ideas using the nearest-neighbor Ising Model in one and two-dimensions.
Our results suggests that deep learning algorithms may be employing a
generalized RG-like scheme to learn relevant features from data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3856</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3856</id><created>2014-10-14</created><updated>2014-11-25</updated><authors><author><keyname>Goyal</keyname><forenames>Aaradhna</forenames></author><author><keyname>Alshamrani</keyname><forenames>Ali</forenames></author><author><keyname>Nandakumar</keyname><forenames>Dhivyaa</forenames></author><author><keyname>Vanga</keyname><forenames>Dileep</forenames></author><author><keyname>Fingerman</keyname><forenames>Dmitriy</forenames></author><author><keyname>Gupta</keyname><forenames>Parul</forenames></author><author><keyname>Ray</keyname><forenames>Riya</forenames></author><author><keyname>Suryadevara</keyname><forenames>Srikanth</forenames></author></authors><title>Towards Refactoring DMARF and GIPSY OSS</title><categories>cs.SE cs.DC</categories><comments>Team 6, SOEN6471 Summer 2014; 67 pages</comments><proxy>Serguei Mokhov</proxy><acm-class>D.2; K.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present here an exploratory and investigatory study of the requirements,
design, and implementation of two opensource software systems: the Distributed
Modular Audio Recognition Framework (DMARF), and the General Intensional
Programming System (GIPSY). The inception, development, and evolution of the
two systems have overlapped and in terms of the involved developers, as well as
in their applications. DMARF is a platform independent collection of algorithms
for pattern recognition, identification and signal processing in audio and
natural language text samples, become a rich platform for the research
community in particular to use, test, and compare various algorithms in the
broad field of pattern recognition and machine learning. Intended as a platform
for intensional programming, GIPSY's inception was intended to push the field
of intensional programming further, overcoming limitations in the available
tools two decades ago. In this study, we present background research into the
two systems and elaborate on their motivations and the requirements that drove
and shaped their design and implementation. We subsequently elaborate in more
depth about various aspects their architectural design, including the
elucidation of some use cases, domain models, and the overall class diagram of
the major components. Moreover, we investigated existing design patterns in
both systems and provided a detailed view of the involved components in such
patterns. Furthermore, we delve deeper into the guts of both systems,
identifying code smells and suggesting possible refactorings. Patchsets of
implementations of selected refactorings have been collected into patchsets and
could be committed into future releases of the two systems, pending a review
and approval of the developers and maintainers of DMARF and GIPSY.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3862</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3862</id><created>2014-10-14</created><authors><author><keyname>Balhoff</keyname><forenames>James P.</forenames></author><author><keyname>Dececchi</keyname><forenames>T. Alexander</forenames></author><author><keyname>Mabee</keyname><forenames>Paula M.</forenames></author><author><keyname>Lapp</keyname><forenames>Hilmar</forenames></author></authors><title>Presence-absence reasoning for evolutionary phenotypes</title><categories>cs.AI q-bio.QM</categories><comments>4 pages. Peer-reviewed submission presented to the Bio-ontologies SIG
  Phenotype Day at ISMB 2014, Boston, Mass.
  http://phenoday2014.bio-lark.org/pdf/11.pdf</comments><journal-ref>James P. Balhoff, T. Alexander Dececchi, Paula M. Mabee, Hilmar
  Lapp. 2014. Presence-absence reasoning for evolutionary phenotypes. In
  proceedings of Phenotype Day of the Bio-ontologies SIG at ISMB 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Nearly invariably, phenotypes are reported in the scientific literature in
meticulous detail, utilizing the full expressivity of natural language. Often
it is particularly these detailed observations (facts) that are of interest,
and thus specific to the research questions that motivated observing and
reporting them. However, research aiming to synthesize or integrate phenotype
data across many studies or even fields is often faced with the need to
abstract from detailed observations so as to construct phenotypic concepts that
are common across many datasets rather than specific to a few. Yet,
observations or facts that would fall under such abstracted concepts are
typically not directly asserted by the original authors, usually because they
are &quot;obvious&quot; according to common domain knowledge, and thus asserting them
would be deemed redundant by anyone with sufficient domain knowledge. For
example, a phenotype describing the length of a manual digit for an organism
implicitly means that the organism must have had a hand, and thus a forelimb;
the presence or absence of a forelimb may have supporting data across a far
wider range of taxa than the length of a particular manual digit. Here we
describe how within the Phenoscape project we use a pipeline of OWL axiom
generation and reasoning steps to infer taxon-specific presence/absence of
anatomical entities from anatomical phenotypes. Although presence/absence is
all but one, and a seemingly simple way to abstract phenotypes across data
sources, it can nonetheless be powerful for linking genotype to phenotype, and
it is particularly relevant for constructing synthetic morphological
supermatrices for comparative analysis; in fact presence/absence is one of the
prevailing character observation types in published character matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3863</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3863</id><created>2014-10-14</created><authors><author><keyname>Del Prete</keyname><forenames>Andrea</forenames></author><author><keyname>Nori</keyname><forenames>Francesco</forenames></author><author><keyname>Metta</keyname><forenames>Giorgio</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author></authors><title>Prioritized motion-force control of constrained fully-actuated robots:
  &quot;Task Space Inverse Dynamics&quot;</title><categories>cs.RO</categories><comments>Pre-print submitted to &quot;Robotics and Autonomous Systems&quot;</comments><journal-ref>A. Del Prete, et al., Prioritized motion-force control of
  constrained fully-actuated robots: &quot;Task Space Inverse Dynamics&quot;, Robotics
  and Autonomous Systems (2014)</journal-ref><doi>10.1016/j.robot.2014.08.016</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a new framework for prioritized multi-task motion-force control of
fully-actuated robots. This work is established on a careful review and
comparison of the state of the art. Some control frameworks are not optimal,
that is they do not find the optimal solution for the secondary tasks. Other
frameworks are optimal, but they tackle the control problem at kinematic level,
hence they neglect the robot dynamics and they do not allow for force control.
Still other frameworks are optimal and consider force control, but they are
computationally less efficient than ours. Our final claim is that, for
fully-actuated robots, computing the operational-space inverse dynamics is
equivalent to computing the inverse kinematics (at acceleration level) and then
the joint-space inverse dynamics. Thanks to this fact, our control framework
can efficiently compute the optimal solution by decoupling kinematics and
dynamics of the robot. We take into account: motion and force control, soft and
rigid contacts, free and constrained robots. Tests in simulation validate our
control framework, comparing it with other state-of-the-art equivalent
frameworks and showing remarkable improvements in optimality and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3864</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3864</id><created>2014-10-14</created><updated>2014-10-16</updated><authors><author><keyname>Goswami</keyname><forenames>Debdipta</forenames></author><author><keyname>Saha</keyname><forenames>Chiranjib</forenames></author><author><keyname>Pal</keyname><forenames>Kunal</forenames></author><author><keyname>Das</keyname><forenames>Swagatam</forenames></author></authors><title>Multi-Agent Shape Formation and Tracking Inspired from a Social Foraging
  Dynamics</title><categories>cs.NE cs.RO</categories><msc-class>70F04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principle of Swarm Intelligence has recently found widespread application in
formation control and automated tracking by the automated multi-agent system.
This article proposes an elegant and effective method inspired by foraging
dynamics to produce geometric-patterns by the search agents. Starting from a
random initial orientation, it is investigated how the foraging dynamics can be
modified to achieve convergence of the agents on the desired pattern with
almost uniform density. Guided through the proposed dynamics, the agents can
also track a moving point by continuously circulating around the point. An
analytical treatment supported with computer simulation results is provided to
better understand the convergence behaviour of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3876</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3876</id><created>2014-10-14</created><authors><author><keyname>Cob&#xe2;rzan</keyname><forenames>Petru&#x163;</forenames></author></authors><title>A new upper bound for Achlioptas processes</title><categories>cs.DM math.CO</categories><comments>6 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider here on-line algorithms for Achlioptas processes. Given a
initially empty graph $G$ on $n$ vertices, a random process that at each step
selects independently and uniformly at random two edges from the set of
non-edges is launched. We must choose one of the two edges and add it to the
graph while discarding the other. The goal is to avoid the appearance of a
connected component spanning $\Omega(n)$ vertices (called a giant component)
for as many steps as possible.
  Bohman and Frieze proved in 2001 that on-line Achlioptas processes cannot
postpone the appearance of the giant for more that roughly $n$ steps whp. This
upper bound got even lower in 2003 when the two above mentioned authors and
Wormald proved that each on-line Achlioptas process creates a giant before step
$0.964446n$ whp.
  The purpose of this work is to determine a new upper bound. By using
essentially the same methods used by Bohman, Frieze and Wormald in 2003 and
some results of Spencer and Wormald on size algorithms we prove here that
Achlioptas processes cannot postpone the appearance of the giant for more than
$0.9455n$ steps whp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3877</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3877</id><created>2014-10-14</created><updated>2015-11-12</updated><authors><author><keyname>Bok</keyname><forenames>Jan</forenames></author><author><keyname>Hlad&#xed;k</keyname><forenames>Milan</forenames></author></authors><title>Selection-based Approach to Cooperative Interval Games</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative interval games are a generalized model of cooperative games in
which worth of every coalition corresponds to a closed interval representing
the possible outcomes of its cooperation. Selections are all possible outcomes
of the interval game with no additional uncertainty.
  We introduce new selection-based classes of interval games and prove their
characterization theorems and relations to existing classes based on the
interval weakly better operator. We show a new results regarding the core and
imputations and examine a problem of equality of two different versions of
core, which is the main stability solution of cooperative games.. Then we
introduce definition of strong imputation and strong core as a universal
solution concept of interval games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3886</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3886</id><created>2014-10-14</created><authors><author><keyname>Bhojanapalli</keyname><forenames>Srinadh</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>Tighter Low-rank Approximation via Sampling the Leveraged Element</title><categories>cs.DS cs.LG stat.ML</categories><comments>36 pages, 3 figures, Extended abstract to appear in the proceedings
  of ACM-SIAM Symposium on Discrete Algorithms (SODA15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a new randomized algorithm for computing a low-rank
approximation to a given matrix. Taking an approach different from existing
literature, our method first involves a specific biased sampling, with an
element being chosen based on the leverage scores of its row and column, and
then involves weighted alternating minimization over the factored form of the
intended low-rank matrix, to minimize error only on these samples. Our method
can leverage input sparsity, yet produce approximations in {\em spectral} (as
opposed to the weaker Frobenius) norm; this combines the best aspects of
otherwise disparate current results, but with a dependence on the condition
number $\kappa = \sigma_1/\sigma_r$. In particular we require $O(nnz(M) +
\frac{n\kappa^2 r^5}{\epsilon^2})$ computations to generate a rank-$r$
approximation to $M$ in spectral norm. In contrast, the best existing method
requires $O(nnz(M)+ \frac{nr^2}{\epsilon^4})$ time to compute an approximation
in Frobenius norm. Besides the tightness in spectral norm, we have a better
dependence on the error $\epsilon$. Our method is naturally and highly
parallelizable.
  Our new approach enables two extensions that are interesting on their own.
The first is a new method to directly compute a low-rank approximation (in
efficient factored form) to the product of two given matrices; it computes a
small random set of entries of the product, and then executes weighted
alternating minimization (as before) on these. The sampling strategy is
different because now we cannot access leverage scores of the product matrix
(but instead have to work with input matrices). The second extension is an
improved algorithm with smaller communication complexity for the distributed
PCA setting (where each server has small set of rows of the matrix, and want to
compute low rank approximation with small amount of communication with other
servers).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3889</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3889</id><created>2014-10-14</created><authors><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Wagner</keyname><forenames>Tal</forenames></author></authors><title>Cheeger-type approximation for sparsest $st$-cut</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the $st$-cut version the Sparsest-Cut problem, where the goal is
to find a cut of minimum sparsity among those separating two distinguished
vertices $s,t\in V$. Clearly, this problem is at least as hard as the usual
(non-$st$) version. Our main result is a polynomial-time algorithm for the
product-demands setting, that produces a cut of sparsity $O(\sqrt{\OPT})$,
where $\OPT$ denotes the optimum, and the total edge capacity and the total
demand are assumed (by normalization) to be $1$.
  Our result generalizes the recent work of Trevisan [arXiv, 2013] for the
non-$st$ version of the same problem (Sparsest-Cut with product demands), which
in turn generalizes the bound achieved by the discrete Cheeger inequality, a
cornerstone of Spectral Graph Theory that has numerous applications. Indeed,
Cheeger's inequality handles graph conductance, the special case of product
demands that are proportional to the vertex (capacitated) degrees. Along the
way, we obtain an $O(\log n)$-approximation, where $n=\card{V}$, for the
general-demands setting of Sparsest $st$-Cut.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3899</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3899</id><created>2014-10-14</created><updated>2016-01-19</updated><authors><author><keyname>Xu</keyname><forenames>Shaolun</forenames></author><author><keyname>Zhang</keyname><forenames>Liang</forenames></author><author><keyname>Yan</keyname><forenames>Zheng</forenames></author><author><keyname>Feng</keyname><forenames>Donghan</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Zhao</keyname><forenames>Xiaobo</forenames></author></authors><title>Optimal Scheduling of Electric Vehicles Charging in low-Voltage
  Distribution Systems</title><categories>cs.SY</categories><comments>Preparation of Final Manuscripts Accepted for Journal of Electrical
  Engineering &amp; Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncoordinated charging of large-scale electric vehicles (EVs) will have a
negative impact on the secure and economic operation of the power system,
especially at the distribution level. Given that the charging load of EVs can
be controlled to some extent, research on the optimal charging control of EVs
has been extensively carried out. In this paper, two possible smart charging
scenarios in China are studied: centralized optimal charging operated by an
aggregator and decentralized optimal charging managed by individual users.
Under the assumption that the aggregators and individual users only concern the
economic benefits, new load peaks will arise under time of use (TOU) pricing
which is extensively employed in China. To solve this problem, a simple
incentive mechanism is proposed for centralized optimal charging while a
rolling-update pricing scheme is devised for decentralized optimal charging.
The original optimal charging models are modified to account for the developed
schemes. Simulated tests corroborate the efficacy of optimal scheduling for
charging EVs in various scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3905</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3905</id><created>2014-10-14</created><authors><author><keyname>Lu</keyname><forenames>Xiankai</forenames></author><author><keyname>Fang</keyname><forenames>Zheng</forenames></author><author><keyname>Xu</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Haiting</forenames></author><author><keyname>Tuo</keyname><forenames>Hongya</forenames></author></authors><title>Efficient Image Categorization with Sparse Fisher Vector</title><categories>cs.CV</categories><comments>5pages,4 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In object recognition, Fisher vector (FV) representation is one of the
state-of-art image representations ways at the expense of dense, high
dimensional features and increased computation time. A simplification of FV is
attractive, so we propose Sparse Fisher vector (SFV). By incorporating locality
strategy, we can accelerate the Fisher coding step in image categorization
which is implemented from a collective of local descriptors. Combining with
pooling step, we explore the relationship between coding step and pooling step
to give a theoretical explanation about SFV. Experiments on benchmark datasets
have shown that SFV leads to a speedup of several-fold of magnitude compares
with FV, while maintaining the categorization performance. In addition, we
demonstrate how SFV preserves the consistence in representation of similar
local features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3910</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3910</id><created>2014-10-14</created><authors><author><keyname>Zhu</keyname><forenames>Wenya</forenames></author><author><keyname>Lu</keyname><forenames>Xiankai</forenames></author><author><keyname>Xu</keyname><forenames>Tao</forenames></author><author><keyname>Zhao</keyname><forenames>Ziyi</forenames></author></authors><title>High Order Structure Descriptors for Scene Images</title><categories>cs.CV</categories><comments>5 pages, 17 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Structure information is ubiquitous in natural scene images and it plays an
important role in scene representation. In this paper, third order structure
statistics (TOSS) and fourth order structure statistics (FOSS) are exploited to
encode higher order structure information. Afterwards, based on the radial and
normal slice of TOSS and FOSS, we propose the high order structure feature:
third order structure feature (TOSF) and fourth order structure feature (FOSF).
It is well known that scene images are well characterized by particular
arrangements of their local structures, we divide the scene image into the
non-overlapping sub-regions and compute the proposed higher order structural
features among them. Then a scene classification is performed by using SVM
classifier with these higher order structure features. The experimental results
show that higher order structure statistics can deliver image structure
information well and its spatial envelope has strong discriminative ability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3915</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3915</id><created>2014-10-14</created><authors><author><keyname>Shah</keyname><forenames>Neil</forenames></author><author><keyname>Beutel</keyname><forenames>Alex</forenames></author><author><keyname>Gallagher</keyname><forenames>Brian</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective</title><categories>cs.LG cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we detect suspicious users in large online networks? Online
popularity of a user or product (via follows, page-likes, etc.) can be
monetized on the premise of higher ad click-through rates or increased sales.
Web services and social networks which incentivize popularity thus suffer from
a major problem of fake connections from link fraudsters looking to make a
quick buck. Typical methods of catching this suspicious behavior use spectral
techniques to spot large groups of often blatantly fraudulent (but sometimes
honest) users. However, small-scale, stealthy attacks may go unnoticed due to
the nature of low-rank eigenanalysis used in practice.
  In this work, we take an adversarial approach to find and prove claims about
the weaknesses of modern, state-of-the-art spectral methods and propose fBox,
an algorithm designed to catch small-scale, stealth attacks that slip below the
radar. Our algorithm has the following desirable properties: (a) it has
theoretical underpinnings, (b) it is shown to be highly effective on real data
and (c) it is scalable (linear on the input size). We evaluate fBox on a large,
public 41.7 million node, 1.5 billion edge who-follows-whom social graph from
Twitter in 2010 and with high precision identify many suspicious accounts which
have persisted without suspension even to this day.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3916</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3916</id><created>2014-10-14</created><updated>2015-11-29</updated><authors><author><keyname>Weston</keyname><forenames>Jason</forenames></author><author><keyname>Chopra</keyname><forenames>Sumit</forenames></author><author><keyname>Bordes</keyname><forenames>Antoine</forenames></author></authors><title>Memory Networks</title><categories>cs.AI cs.CL stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We describe a new class of learning models called memory networks. Memory
networks reason with inference components combined with a long-term memory
component; they learn how to use these jointly. The long-term memory can be
read and written to, with the goal of using it for prediction. We investigate
these models in the context of question answering (QA) where the long-term
memory effectively acts as a (dynamic) knowledge base, and the output is a
textual response. We evaluate them on a large-scale QA task, and a smaller, but
more complex, toy task generated from a simulated world. In the latter, we show
the reasoning power of such models by chaining multiple supporting sentences to
answer questions that require understanding the intension of verbs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3929</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3929</id><created>2014-10-15</created><authors><author><keyname>Maya</keyname><forenames>Juan Augusto</forenames></author><author><keyname>Vega</keyname><forenames>Leonardo Rey</forenames></author><author><keyname>Galarza</keyname><forenames>Cecilia G.</forenames></author></authors><title>Distributed Detection of a Random Process over a Multiple Access Channel
  under Energy and Bandwidth Constraints</title><categories>stat.OT cs.IT math.IT stat.AP</categories><comments>This paper was submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a binary hypothesis testing problem built on a wireless sensor
network (WSN) for detecting a stationary random process distributed both in
space and time with circularly-symmetric complex Gaussian distribution under
the Neyman-Pearson framework. Using an analog scheme, the sensors transmit
different linear combinations of their measurements through a multiple access
channel (MAC) to reach the fusion center (FC), whose task is to decide whether
the process is present or not. Considering an energy constraint on each node
transmission and a limited amount of channel uses, we compute the miss error
exponent of the proposed scheme using Large Deviation Theory (LDT) and show
that the proposed strategy is asymptotically optimal (when the number of
sensors approaches to infinity) among linear orthogonal schemes. We also show
that the proposed scheme obtains significant energy saving in the low
signal-to-noise ratio regime, which is the typical scenario of WSNs. Finally, a
Monte Carlo simulation of a 2-dimensional process in space validates the
analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3932</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3932</id><created>2014-10-15</created><authors><author><keyname>Lim</keyname><forenames>Mei Kuan</forenames></author><author><keyname>Chan</keyname><forenames>Chee Seng</forenames></author><author><keyname>Monekosso</keyname><forenames>Dorothy</forenames></author><author><keyname>Remagnino</keyname><forenames>Paolo</forenames></author></authors><title>Detection of Salient Regions in Crowded Scenes</title><categories>cs.CV</categories><comments>Accepted in Electronics Letters Vol. 5, Issue 5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing number of cameras and a handful of human operators to monitor
the video inputs from hundreds of cameras leave the system ill equipped to
fulfil the task of detecting anomalies. Thus, there is a dire need to
automatically detect regions that require immediate attention for a more
effective and proactive surveillance. We propose a framework that utilises the
temporal variations in the flow field of a crowd scene to automatically detect
salient regions, while eliminating the need to have prior knowledge of the
scene or training. We deem the flow fields to be a dynamic system and adopt the
stability theory of dynamical systems, to determine the motion dynamics within
a given area. In the context of this work, salient regions refer to areas with
high motion dynamics, where points in a particular region are unstable.
Experimental results on public, crowd scenes have shown the effectiveness of
the proposed method in detecting salient regions which correspond to unstable
flow, occlusions, bottlenecks, entries and exits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3935</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3935</id><created>2014-10-15</created><authors><author><keyname>Sato</keyname><forenames>Taisuke</forenames></author><author><keyname>Kubota</keyname><forenames>Keiichi</forenames></author><author><keyname>Kameya</keyname><forenames>Yoshitaka</forenames></author></authors><title>A Logic-based Approach to Generatively Defined Discriminative Modeling</title><categories>cs.LG</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional random fields (CRFs) are usually specified by graphical models
but in this paper we propose to use probabilistic logic programs and specify
them generatively. Our intension is first to provide a unified approach to CRFs
for complex modeling through the use of a Turing complete language and second
to offer a convenient way of realizing generative-discriminative pairs in
machine learning to compare generative and discriminative models and choose the
best model. We implemented our approach as the D-PRISM language by modifying
PRISM, a logic-based probabilistic modeling language for generative modeling,
while exploiting its dynamic programming mechanism for efficient probability
computation. We tested D-PRISM with logistic regression, a linear-chain CRF and
a CRF-CFG and empirically confirmed their excellent discriminative performance
compared to their generative counterparts, i.e.\ naive Bayes, an HMM and a
PCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRF
versions of Bayesian network classifiers and probabilistic left-corner grammars
respectively and easily implementable in D-PRISM. We empirically showed that
they outperform their generative counterparts as expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3942</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3942</id><created>2014-10-15</created><authors><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author><author><keyname>Sachdeva</keyname><forenames>Niharika</forenames></author></authors><title>Privacy4ICTD in India: Exploring Perceptions, Attitudes and Awareness
  about ICT Use</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several ICT studies give anecdotal evidences showing privacy to be an area of
concern that can influence adoption of technology in the developing world.
However, in-depth understanding of end users' privacy attitudes and awareness
is largely unexplored in developing countries such as India. We conducted a
survey with 10,427 Indian citizens to bring forth various insights on privacy
expectations and perceptions of end users. Our study explores end-users'
privacy expectations on three ICT platforms - mobile phones, OSN (Online Social
Network), and government projects. Our results, though preliminary, show that
users disproportionately consider financial details as personal information in
comparison to medical records. Users heavily use mobile phones to store
personal details and show high trust in mobile service providers for protecting
the private data. However, users show concerns that mobile service provider may
allow improper access of their personal information to third parties and
government. We also find that female participants in the study were marginally
more conscious of their privacy than males. To the best of our knowledge, this
work presents the largest privacy study which benchmarks privacy perceptions
among Indian citizens. Understanding users' privacy perceptions can help
improve technology adoption and develop policies and laws for improving
technology experience and enabling development for a better life in India.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3943</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3943</id><created>2014-10-15</created><updated>2016-01-19</updated><authors><author><keyname>Herman</keyname><forenames>Ivo</forenames></author><author><keyname>Martinec</keyname><forenames>Dan</forenames></author><author><keyname>Hur&#xe1;k</keyname><forenames>Zden&#x11b;k</forenames></author><author><keyname>Sebek</keyname><forenames>Michael</forenames></author></authors><title>Scaling in bidirectional platoons with dynamic controllers and
  proportional asymmetry</title><categories>cs.SY</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider platoons composed of identical vehicles with an asymmetric
nearest-neighbor interaction. We restrict ourselves to intervehicular coupling
realized with dynamic arbitrary-order onboard controllers such that the
coupling to the immediately preceding vehicle is proportional to the coupling
to the immediately following vehicle. Each vehicle is modeled using a transfer
function and we impose no restriction on the order of the vehicle. The platoon
is described by a transfer function in a convenient product form. We
investigate how the H-infinity norm and the steady-state gain of the platoon
scale with the number of vehicles. We conclude that if the open-loop transfer
function of the vehicle contains two or more integrators and the Fiedler
eigenvalue of the graph Laplacian is uniformly bounded from below, the norm
scales exponentially with the growing distance in the graph. If there is just
one integrator in the open loop, we give a condition under which the norm of
the transfer function is bounded by its steady-state gain - the platoon is
string-stable. Moreover, we argue that in this case it is always possible to
design a controller the predecessor following strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3944</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3944</id><created>2014-10-15</created><updated>2015-02-24</updated><authors><author><keyname>Wang</keyname><forenames>Xiaohan</forenames></author><author><keyname>Liu</keyname><forenames>Pengfei</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Local-set-based Graph Signal Reconstruction</title><categories>cs.IT math.IT</categories><comments>28 pages, 9 figures, 6 tables, journal manuscript</comments><doi>10.1109/TSP.2015.2411217</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal processing on graph is attracting more and more attentions. For a
graph signal in the low-frequency subspace, the missing data associated with
unsampled vertices can be reconstructed through the sampled data by exploiting
the smoothness of the graph signal. In this paper, the concept of local set is
introduced and two local-set-based iterative methods are proposed to
reconstruct bandlimited graph signal from sampled data. In each iteration, one
of the proposed methods reweights the sampled residuals for different vertices,
while the other propagates the sampled residuals in their respective local
sets. These algorithms are built on frame theory and the concept of local sets,
based on which several frames and contraction operators are proposed. We then
prove that the reconstruction methods converge to the original signal under
certain conditions and demonstrate the new methods lead to a significantly
faster convergence compared with the baseline method. Furthermore, the
correspondence between graph signal sampling and time-domain irregular sampling
is analyzed comprehensively, which may be helpful to future works on graph
signals. Computer simulations are conducted. The experimental results
demonstrate the effectiveness of the reconstruction methods in various sampling
geometries, imprecise priori knowledge of cutoff frequency, and noisy
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3947</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3947</id><created>2014-10-15</created><authors><author><keyname>Liang</keyname><forenames>Le</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Dong</keyname><forenames>Xiaodai</forenames></author></authors><title>Low-Complexity Hybrid Precoding in Massive Multiuser MIMO Systems</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures, accepted by IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) is envisioned to offer
considerable capacity improvement, but at the cost of high complexity of the
hardware. In this paper, we propose a low-complexity hybrid precoding scheme to
approach the performance of the traditional baseband zero-forcing (ZF)
precoding (referred to as full-complexity ZF), which is considered a virtually
optimal linear precoding scheme in massive MIMO systems. The proposed hybrid
precoding scheme, named phased-ZF (PZF), essentially applies phase-only control
at the RF domain and then performs a low-dimensional baseband ZF precoding
based on the effective channel seen from baseband. Heavily quantized RF phase
control up to $2$ bits of precision is also considered and shown to incur very
limited degradation. The proposed scheme is simulated in both ideal Rayleigh
fading channels and sparsely scattered millimeter wave (mmWave) channels, both
achieving highly desirable performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3965</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3965</id><created>2014-10-15</created><authors><author><keyname>Cheong</keyname><forenames>Siotai</forenames></author><author><keyname>Fan</keyname><forenames>Pinyi</forenames></author></authors><title>Novel Degree Distribution Function for LT codes over Finite Field</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Luby Transform (LT) code over finite field is a recent research topic. In
order to find out the properties of LT codes over finite field, a novel degree
distribution function is proposed in this paper. The main thinking of our
developed distribution function is to improve the decoding success rate with
the same overhead, and still to keep the sparse property for the encoding
matrix. Numerical simulations are used to show the general performance of our
novel function. Various simulation results show that in the environment of LT
codes over finite field, our new degree distribution function performs much
better than the degree distribution functions proposed by Luby as the field
size increasing. In conclusion, our novel degree distribution function is more
suitable to be used in LT codes over finite field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3969</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3969</id><created>2014-10-15</created><authors><author><keyname>Michel</keyname><forenames>Lo&#xef;c</forenames></author></authors><title>Bernstein-based polynomial approach to study the stability of switched
  systems and formal verification using HOL Light</title><categories>cs.SY</categories><comments>11 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this preliminary work, we propose to use a polynomial approach in order to
study the stability of switched systems. The proposed strategy is based on the
Bernstein interpolation method that may transform a switched system into a
polynomial expression from which an associated &quot;simple&quot; Lyapunov function can
be eventually built. The HOL Light proof assistant allows verifying formally
the Lyapunov functions that are identified from the proposed switching
structure. Our approach is illustrated by numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3970</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3970</id><created>2014-10-15</created><authors><author><keyname>Ghazouani</keyname><forenames>Haythem</forenames></author></authors><title>Shape and Color Object Tracking for Real-Time Robotic Navigation</title><categories>cs.CV</categories><comments>in French</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a real-time approach for single-colored ball detection
and tracking. The approach consists of two main phases. In a first offline
calibration phase, the intrinsic parameters of the camera and the radial
distortion are estimated, and a classification of colors is learned from a
sample image of colored balls. The second phase consists of four main steps:
(1) color segmentation of the input image into several regions based on the
offline classification, (2) robust estimation of the circle parameters (3)
refinement of the circle parameters, and (4) ball tracking. The experimental
results showed that the approach presents a good compromise between suitability
for real-time navigation and robustness to occlusions, background congestion
and colors interference in the scene.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3977</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3977</id><created>2014-10-15</created><authors><author><keyname>Ho</keyname><forenames>Ting-Yu</forenames></author><author><keyname>Yeh</keyname><forenames>Yi-Nung</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author></authors><title>Multi-View 3D Video Multicast for Broadband IP Networks</title><categories>cs.MM</categories><comments>9 pages, 10 figures, IEEE ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the recent emergence of 3D-supported TVs, video service providers now
face an opportunity to provide high resolution multi-view 3D videos over IP
networks. One simple way to support efficient communications between a video
server and multiple clients is to deliver each desired view in a multicast
stream. Nevertheless, it is expected that significantly increased bandwidth
will be required to support the transmission of all views in multi-view 3D
videos. However, the recent emergence of a new video synthesis technique called
Depth-Image-Based Rendering (DIBR) suggests that multi-view 3D video does not
necessarily require the transmission of all views. Therefore, we formulate a
new problem, named Multi-view and Multicast Delivery Selection Problem (MMDS),
and design an algorithm, called MMDEA, to find the optimal solution. Simulation
results manifest that using DIBR can effectively reduce bandwidth consumption
by 35% compared to the original multicast delivery scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3978</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3978</id><created>2014-10-15</created><authors><author><keyname>Qiu</keyname><forenames>Harry J. F.</forenames></author><author><keyname>Ho</keyname><forenames>Ivan Wang-Hei</forenames></author><author><keyname>Tse</keyname><forenames>Chi K.</forenames></author><author><keyname>Xie</keyname><forenames>Yu</forenames></author></authors><title>Technical Report: A Methodology for Studying 802.11p VANET Broadcasting
  Performance with Practical Vehicle Distribution</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a Vehicular Ad-hoc Network (VANET), the performance of the communication
protocol is influenced heavily by the vehicular density dynamics. However, most
of the previous works on VANET performance modeling paid little attention to
vehicle distribution, or simply assumed homogeneous car distribution. It is
obvious that vehicles are distributed non-homogeneously along a road segment
due to traffic signals and speed limits at different portions of the road, as
well as vehicle interactions that are significant on busy streets. In light of
the inadequacy, we present in this paper an original methodology to study the
broadcasting performance of 802.11p VANETs with practical vehicle distribution
in urban environments. Firstly, we adopt the empirically verified stochastic
traffic models, which incorporates the effect of urban settings (such as
traffic lights and vehicle interactions) on car distribution and generates
practical vehicular density profiles. Corresponding 802.11p protocol and
performance models are then developed. When coupled with the traffic models,
they can predict broadcasting efficiency, delay, as well as throughput
performance of 802.11p VANETs based on the knowledge of car density at each
location on the road. Extensive simulation is conducted to verify the accuracy
of the developed mathematical models with the consideration of vehicle
interaction. In general, our results demonstrate the applicability of the
proposed methodology on modeling protocol performance in practical signalized
road networks, and shed insights into the design and development of future
communication protocols and networking functions for VANETs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3981</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3981</id><created>2014-10-15</created><authors><author><keyname>Hirsch</keyname><forenames>Robin</forenames></author><author><keyname>Jackson</keyname><forenames>Marcel</forenames></author><author><keyname>Mikul&#xe1;s</keyname><forenames>Szabolcs</forenames></author></authors><title>The algebra of functions with antidomain and range</title><categories>math.LO cs.LO</categories><msc-class>20M20, 03G15, 08A02</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give complete, finite quasiequational axiomatisations for algebras of
unary partial functions under the operations of composition, domain,
antidomain, range and intersection. This completes the extensive programme of
classifying algebras of unary partial functions under combinations of these
operations. We look at the complexity of the equational theories and provide a
nondeterministic polynomial upper bound. Finally we look at the problem of
finite representability and show that finite algebras can be represented as a
collection of unary functions over a finite base set provided that intersection
is not in the signature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3987</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3987</id><created>2014-10-15</created><updated>2015-07-08</updated><authors><author><keyname>Keshmiri</keyname><forenames>Soheil</forenames></author><author><keyname>Ahmed</keyname><forenames>Syeda Mariam</forenames></author><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Chew</keyname><forenames>Chee Meng</forenames></author><author><keyname>Pang</keyname><forenames>Chee Khiang</forenames></author></authors><title>Model-Free 3D Reconstruction of Weld Joint Using Laser Scanning</title><categories>cs.RO</categories><comments>Disapproval of funding organization</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This article presents a novel utilization of the concept of entropy in
information theory to model-free 3D reconstruction of weld joint in presence of
noise. We show that our formulation attains its global minimum at the upper
edge of this joint. This property significantly simplifies the extraction of
this welding joint. Furthermore, we present an approach to compute the volume
of this extracted space to facilitate the monitoring of the progress of the
welding task. Moreover, we provide a preliminary analysis of the effect of
variation of the noise on the extraction process of this space to realize the
impact of this noise on the computation of its area and volume.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3988</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3988</id><created>2014-10-15</created><updated>2014-10-16</updated><authors><author><keyname>Arjmandi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Aminian</keyname><forenames>Gholamali</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Kenari</keyname><forenames>Masoumeh Nasiri</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Capacity of Diffusion based Molecular Communication Networks over
  LTI-Poisson Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the capacity of a diffusion based molecular communication
network under the model of a Linear Time Invarient-Poisson (LTI-Poisson)
channel is studied. Introduced in the context of molecular communication, the
LTI-Poisson model is a natural extension of the conventional memoryless Poisson
channel to include memory. Exploiting prior art on linear ISI channels, a
computable finite-letter characterization of the capacity of single-hop
LTI-Poisson networks is provided. Then, the problem of finding more explicit
bounds on the capacity is examined, where lower and upper bounds for the point
to point case are provided. Furthermore, an approach for bounding mutual
information in the low SNR regime using the symmetrized KL divergence is
introduced and its applicability to Poisson channels is shown. To best of our
knowledge, the first non-trivial upper bound on the capacity of Poisson channel
with a maximum transmission constraint in the low SNR regime is found.
Numerical results show that the proposed upper bound is of the same order as
the capacity in the low SNR regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.3998</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.3998</id><created>2014-10-15</created><updated>2015-09-21</updated><authors><author><keyname>Moreno-Pozas</keyname><forenames>Laureano</forenames></author><author><keyname>Martos-Naya</keyname><forenames>Eduardo</forenames></author></authors><title>On Some Unifications Arising from the MIMO Rician Shadowed Model</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to an international for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the proposed Rician shadowed model for multi-antenna
communications allows for the unification of a wide set of models, both for
multiple-input multiple output (MIMO) and single-input single output (SISO)
communications. The MIMO Rayleigh and MIMO Rician can be deduced from the MIMO
Rician shadowed, and so their SISO counterparts. Other SISO models, besides the
Rician shadowed proposed by Abdi et. al., are included in the model, such as
the $\kappa$-$\mu$ defined by Yacoub, and its recent generalization, the
\mbox{$\kappa$-$\mu$} shadowed model. Moreover, the SISO \mbox{$\eta$-$\mu$}
and \mbox{Nakagami-$q$} models can be seen as particular cases of the MIMO
Rician shadowed. The literature already presents the probability density
function (pdf) of the Rician shadowed Gram channel matrix in terms of the
well-known gamma-Wishart distribution. We here derive its moment generating
function in a tractable form. Closed-form expressions for the cumulative
distribution function and the pdf of the maximum eigenvalue are also carried
out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4009</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4009</id><created>2014-10-15</created><authors><author><keyname>Eckles</keyname><forenames>Dean</forenames></author><author><keyname>Kaptein</keyname><forenames>Maurits</forenames></author></authors><title>Thompson sampling with the online bootstrap</title><categories>cs.LG stat.CO stat.ML</categories><comments>13 pages, 4 figures</comments><msc-class>68W27, 62L05</msc-class><acm-class>G.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thompson sampling provides a solution to bandit problems in which new
observations are allocated to arms with the posterior probability that an arm
is optimal. While sometimes easy to implement and asymptotically optimal,
Thompson sampling can be computationally demanding in large scale bandit
problems, and its performance is dependent on the model fit to the observed
data. We introduce bootstrap Thompson sampling (BTS), a heuristic method for
solving bandit problems which modifies Thompson sampling by replacing the
posterior distribution used in Thompson sampling by a bootstrap distribution.
We first explain BTS and show that the performance of BTS is competitive to
Thompson sampling in the well-studied Bernoulli bandit case. Subsequently, we
detail why BTS using the online bootstrap is more scalable than regular
Thompson sampling, and we show through simulation that BTS is more robust to a
misspecified error distribution. BTS is an appealing modification of Thompson
sampling, especially when samples from the posterior are otherwise not
available or are costly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4011</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4011</id><created>2014-10-15</created><authors><author><keyname>Ben-Amram</keyname><forenames>Amir M.</forenames></author><author><keyname>Pineles</keyname><forenames>Aviad</forenames></author></authors><title>Flowchart Programs, Regular Expressions, and Decidability of Polynomial
  Growth-Rate</title><categories>cs.PL cs.FL cs.LO</categories><comments>50 pages</comments><acm-class>D.2.4; F.2.0; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for inferring complexity properties for a class of
programs in the form of flowcharts annotated with loop information.
Specifically, our method can (soundly and completely) decide if computed values
are polynomially bounded as a function of the input; and similarly for the
running time. Such complexity properties are undecidable for a Turing-complete
programming language, and a common work-around in program analysis is to settle
for sound but incomplete solutions. In contrast, we consider a class of
programs that is Turing-incomplete, but strong enough to include several
challenges for this kind of analysis. For a related language that has
well-structured syntax, similar to Meyer and Ritchie's LOOP programs, the
problem has been previously proved to be decidable. The analysis relied on the
compositionally-defined structure of programs, hence the challenge in obtaining
similar results for flowchart programs with arbitrary control-flow graphs. Our
answer to the challenge is twofold: first, a definition of a class of
loop-annotated flowcharts, which is more general than the class of flowcharts
that directly represent structured programs; secondly, a technique to reuse the
ideas from the work on structured programs and apply them to such flowcharts.
The technique is inspired by the classic translation of non-deterministic
automata to regular expressions (which are compositional), but we obviate the
exponential cost of constructing such an expression, obtaining a
polynomial-time analysis. These ideas may well be applicable to other analysis
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4012</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4012</id><created>2014-10-15</created><authors><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Dey</keyname><forenames>S.</forenames></author><author><keyname>Mukherjee</keyname><forenames>K.</forenames></author><author><keyname>Jana</keyname><forenames>T. S.</forenames></author></authors><title>Online interpretation of numeric sign language using 2-d skeletal model</title><categories>cs.CV</categories><journal-ref>Proc. of Intl. Conf. on Communication Devices and Intelligent
  Systems (CODIS), pp. 570-573, Jan-2004, Kolkata</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gesturing is one of the natural modes of human communication. Signs produced
by gestures can have a basic meaning coupled with additional information that
is layered over the basic meaning of the sign. Sign language is an important
example of communicative gestures that are highly structured and well accepted
across the world as a communication medium for deaf and dumb. In this paper, an
online recognition scheme is proposed to interpret the standard numeric sign
language comprising of 10 basic hand symbols. A web camera is used to capture
the real time hand movements as input to the system. The basic meaning of the
hand gesture is extracted from the input data frame by analysing the shape of
the hand, considering its orientation, movement and location to be fixed. The
input hand shape is processed to identify the palm structure, fingertips and
their relative positions and the presence of the extended thumb. A
2-dimensional skeletal model is generated from the acquired shape information
to represent and subsequently interpret the basic meaning of the hand gesture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4013</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4013</id><created>2014-10-15</created><authors><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author></authors><title>A two-pass fuzzy-geno approach to pattern classification</title><categories>cs.CV</categories><journal-ref>Proc. of International Conference on Computer Processing of
  Bangla, pp. 130-134, Feb-2006, Dhaka</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work presents an extension of the fuzzy approach to 2-D shape recognition
[1] through refinement of initial or coarse classification decisions under a
two pass approach. In this approach, an unknown pattern is classified by
refining possible classification decisions obtained through coarse
classification of the same. To build a fuzzy model of a pattern class
horizontal and vertical fuzzy partitions on the sample images of the class are
optimized using genetic algorithm. To make coarse classification decisions
about an unknown pattern, the fuzzy representation of the pattern is compared
with models of all pattern classes through a specially designed similarity
measure. Coarse classification decisions are refined in the second pass to
obtain the final classification decision of the unknown pattern. To do so,
optimized horizontal and vertical fuzzy partitions are again created on certain
regions of the image frame, specific to each group of similar type of pattern
classes. It is observed through experiments that the technique improves the
overall recognition rate from 86.2%, in the first pass, to 90.4% after the
second pass, with 500 training samples of handwritten digits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4017</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4017</id><created>2014-10-15</created><authors><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Chakraborty</keyname><forenames>S.</forenames></author><author><keyname>Mukherjee</keyname><forenames>K.</forenames></author><author><keyname>Pandit</keyname><forenames>S. K.</forenames></author></authors><title>Online Tracking of Skin Colour Regions Against a Complex Background</title><categories>cs.CV</categories><journal-ref>Proc. of IEEE INDICON, pp. 184-186, Dec-2004, Kharagpur</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online tracking of human activity against a complex background is a
challenging task for many applications. In this paper, we have developed a
robust technique for localizing skin colour regions from unconstrained image
frames. A simple and fast segmentation algorithm is used to train a multiplayer
perceptron (MLP) for detection of skin colours. Stepper motors are synchronized
with the MLP to track the movement of the skin colour regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4019</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4019</id><created>2014-10-15</created><authors><author><keyname>Kole</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author></authors><title>An Automated Group Key Authentication System Using Secret Image Sharing
  Scheme</title><categories>cs.CR</categories><journal-ref>Proceedings of the International Conference on Recent trends in
  Information Systems (IRIS-06), pp. 98-106, Kovilpatti, Tamil Nadu, India,
  2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an open network environment, privacy of group communication and integrity
of the communication data are the two major issues related to secured
information exchange. The required level of security may be achieved by
authenticating a group key in the communication channel, where contribution
from each group member becomes a part of the overall group key. In the current
work, we have developed an authentication system through Central Administrative
Server (CAS) for automatic integration and validation of the group key. For
secured group communication, the CAS generates a secret alphanumeric group key
image. Using secret image sharing scheme, this group key image shares are
distributed among all the participating group members in the open network. Some
or all the secret shares may be merged to reconstruct the group key image at
CAS. A k-nearest neighbor classifier with 48 features to represent the images,
is used to validate the reconstructed image with the one stored in the CAS. 48
topological features are used to represent the reconstructed group key image.
We have achieved 99.1% classification accuracy for 26 printed English uppercase
characters and 10 numeric digits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4027</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4027</id><created>2014-10-15</created><authors><author><keyname>Ballarini</keyname><forenames>Paolo</forenames></author></authors><title>Analysing oscillatory trends of discrete-state stochastic processes
  through HASL statistical model checking</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of formal methods to the analysis of stochastic oscillators
has been at the focus of several research works in recent times. In this paper
we provide insights on the application of an expressive temporal logic
formalism, namely the Hybrid Automata Stochastic Logic (HASL), to that issue.
We show how one can take advantage of the expressive power of the HASL logic to
define and assess relevant characteristics of (stochastic) oscillators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4034</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4034</id><created>2014-10-15</created><updated>2015-12-18</updated><authors><author><keyname>Gonze</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author></authors><title>On the Synchronizing Probability Function and the Triple Rendezvous Time
  for Synchronizing Automata</title><categories>cs.FL</categories><comments>A preliminary version of the results has been presented at the
  conference LATA 2015. The current ArXiv version includes the most recent
  improvement on the triple rendezvous time upper bound as well as formal
  proofs of all the results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cerny's conjecture is a longstanding open problem in automata theory. We
study two different concepts, which allow to approach it from a new angle. The
first one is the triple rendezvous time, i.e., the length of the shortest word
mapping three states onto a single one. The second one is the synchronizing
probability function of an automaton, a recently introduced tool which
reinterprets the synchronizing phenomenon as a two-player game, and allows to
obtain optimal strategies through a Linear Program.
  Our contribution is twofold. First, by coupling two different novel
approaches based on the synchronizing probability function and properties of
linear programming, we obtain a new upper bound on the triple rendezvous time.
Second, by exhibiting a family of counterexamples, we disprove a conjecture on
the growth of the synchronizing probability function. We then suggest natural
follow-ups towards Cernys conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4044</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4044</id><created>2014-10-15</created><updated>2015-03-24</updated><authors><author><keyname>L&#xfc;ck</keyname><forenames>Martin</forenames></author><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Schindler</keyname><forenames>Irina</forenames></author></authors><title>Parameterized Complexity of CTL: A Generalization of Courcelle's Theorem</title><categories>cs.LO cs.CC</categories><comments>Conference version: &quot;L\&quot;uck, Meier, Schindler. Parameterized
  Complexity of CTL: A Generalization of Courcelle's Theorem. Language and
  Automata Theory and Applications - 9th International Conference, LATA 2015,
  Nice, France. Lecture Notes in Computer Science, Volume 8977, pp. 549-560,
  Springer&quot;</comments><msc-class>03B44</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an almost complete classification of the parameterized complexity
of all operator fragments of the satisfiability problem in computation tree
logic CTL. The investigated parameterization is the sum of temporal depth and
structural pathwidth. The classification shows a dichotomy between W[1]-hard
and fixed-parameter tractable fragments. The only real operator fragment which
is confirmed to be in FPT is the fragment containing solely AX. Also we prove a
generalization of Courcelle's theorem to infinite signatures which will be used
to proof the FPT-membership case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4049</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4049</id><created>2014-10-15</created><authors><author><keyname>Krajniak</keyname><forenames>Jakub</forenames></author><author><keyname>Banaszak</keyname><forenames>Michal</forenames></author></authors><title>Monte Carlo Study of Patchy Nanostructures Self-Assembled from a Single
  Multiblock Chain</title><categories>cond-mat.soft cs.CE</categories><journal-ref>Computational Methods in Science and Technology, 19(3) 137-143,
  2013</journal-ref><doi>10.12921/cmst.2013.19.03.137-143</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a lattice Monte Carlo simulation for a multiblock copolymer chain
of length N=240 and microarchitecture $(10-10)_{12}$.The simulation was
performed using the Monte Carlo method with the Metropolis algorithm. We
measured average energy, heat capacity, the mean squared radius of gyration,
and the histogram of cluster count distribution. Those quantities were
investigated as a function of temperature and incompatibility between segments,
quantified by parameter {\omega}. We determined the temperature of the
coil-globule transition and constructed the phase diagram exhibiting a variety
of patchy nanostructures. The presented results yield a qualitative agreement
with those of the off-lattice Monte Carlo method reported earlier, with a
significant exception for small incompatibilities,{\omega}, and low
temperatures, where 3-cluster patchy nanostructures are observed in contrast to
the 2-cluster structures observed for the off-lattice $(10-10)_{12}$ chain. We
attribute this difference to a considerable stiffness of lattice chains in
comparison to that of the off-lattice chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4054</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4054</id><created>2014-10-15</created><updated>2014-12-15</updated><authors><author><keyname>Rupp</keyname><forenames>Karl</forenames></author><author><keyname>Weinbub</keyname><forenames>Josef</forenames></author><author><keyname>J&#xfc;ngel</keyname><forenames>Ansgar</forenames></author><author><keyname>Grasser</keyname><forenames>Tibor</forenames></author></authors><title>Pipelined Iterative Solvers with Kernel Fusion for Graphics Processing
  Units</title><categories>cs.MS cs.DC cs.PF</categories><comments>24 pages, 9 figures, 1 table</comments><msc-class>65F10 (Secondary), 65F50, 65Y05 (Primary), 65Y10</msc-class><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the implementation of iterative solvers on discrete graphics
processing units and demonstrate the benefit of implementations using extensive
kernel fusion for pipelined formulations over conventional implementations of
classical formulations. The proposed implementations with both CUDA and OpenCL
are freely available in ViennaCL and are shown to be competitive with or even
superior to other solver packages for graphics processing units. Highest
performance gains are obtained for small to medium-sized systems, while our
implementations are on par with vendor-tuned implementations for very large
systems. Our results are especially beneficial for transient problems, where
many small to medium-sized systems instead of a single big system need to be
solved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4060</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4060</id><created>2014-10-15</created><authors><author><keyname>Dreesen</keyname><forenames>Philippe</forenames></author><author><keyname>Ishteva</keyname><forenames>Mariya</forenames></author><author><keyname>Schoukens</keyname><forenames>Johan</forenames></author></authors><title>Decoupling Multivariate Polynomials Using First-Order Information</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to decompose a set of multivariate real polynomials into
linear combinations of univariate polynomials in linear forms of the input
variables. The method proceeds by collecting the first-order information of the
polynomials in a set of operating points, which is captured by the Jacobian
matrix evaluated at the operating points. The polyadic canonical decomposition
of the three-way tensor of Jacobian matrices directly returns the unknown
linear relations, as well as the necessary information to reconstruct the
univariate polynomials. The conditions under which this decoupling procedure
works are discussed, and the method is illustrated on several numerical
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4062</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4062</id><created>2014-10-15</created><authors><author><keyname>Frandi</keyname><forenames>Emanuele</forenames></author><author><keyname>Nanculef</keyname><forenames>Ricardo</forenames></author><author><keyname>Suykens</keyname><forenames>Johan</forenames></author></authors><title>Complexity Issues and Randomization Strategies in Frank-Wolfe Algorithms
  for Machine Learning</title><categories>stat.ML cs.LG cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frank-Wolfe algorithms for convex minimization have recently gained
considerable attention from the Optimization and Machine Learning communities,
as their properties make them a suitable choice in a variety of applications.
However, as each iteration requires to optimize a linear model, a clever
implementation is crucial to make such algorithms viable on large-scale
datasets. For this purpose, approximation strategies based on a random sampling
have been proposed by several researchers. In this work, we perform an
experimental study on the effectiveness of these techniques, analyze possible
alternatives and provide some guidelines based on our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4065</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4065</id><created>2014-10-15</created><authors><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author></authors><title>Unified Analysis of Collapsible and Ordered Pushdown Automata via Term
  Rewriting</title><categories>cs.FL</categories><comments>in Proc. of FREC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model collapsible and ordered pushdown systems with term rewriting, by
encoding higher-order stacks and multiple stacks into trees. We show a uniform
inverse preservation of recognizability result for the resulting class of term
rewriting systems, which is obtained by extending the classic saturation-based
approach. This result subsumes and unifies similar analyses on collapsible and
ordered pushdown systems. Despite the rich literature on inverse preservation
of recognizability for term rewrite systems, our result does not seem to follow
from any previous study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4074</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4074</id><created>2014-10-14</created><updated>2015-04-30</updated><authors><author><keyname>R.</keyname><forenames>Sahasranand K.</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Distributed Nonparametric Sequential Spectrum Sensing under
  Electromagnetic Interference</title><categories>cs.IT math.IT</categories><comments>8 pages; 6 figures; Version 2 has the proofs for the theorems.
  Version 3 contains a new section on approximation analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nonparametric distributed sequential algorithm for quick detection of
spectral holes in a Cognitive Radio set up is proposed. Two or more local nodes
make decisions and inform the fusion centre (FC) over a reporting Multiple
Access Channel (MAC), which then makes the final decision. The local nodes use
energy detection and the FC uses mean detection in the presence of fading,
heavy-tailed electromagnetic interference (EMI) and outliers. The statistics of
the primary signal, channel gain or the EMI is not known. Different
nonparametric sequential algorithms are compared to choose appropriate
algorithms to be used at the local nodes and the FC. Modification of a recently
developed random walk test is selected for the local nodes for energy detection
as well as at the fusion centre for mean detection. It is shown via simulations
and analysis that the nonparametric distributed algorithm developed performs
well in the presence of fading, EMI and is robust to outliers. The algorithm is
iterative in nature making the computation and storage requirements minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4078</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4078</id><created>2014-10-15</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author></authors><title>Softwaretechnische Absicherung intelligenter Systeme im Fahrzeug</title><categories>cs.SE</categories><comments>14 pages, 1 figure in German</comments><journal-ref>Proceedings der 21. VDI/VW-Gemeinschaftstagung - Integrierte
  Sicherheit und Fahrerassistenzsysteme. 12.-13. Oktober 2006, Wolfsburg</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;This article describes software engineering techniques to be used in order
to ensure the necessary quality of intelligent and therefore massive
software-based systems in vehicles. Quality assurance for intelligent software
is achieved through a bundle of modern software engineering methods.
Architecture and design patterns for securing the software components are
supplemented by test concepts and frameworks for validation and checks of
robustness of the implementation. These patterns describe established and
therefore consolidated solutions for certain problems as for instance
reliability or efficient execution.
  --
  Dieser Artikel skizziert, welche Software-Entwurfstechniken heute zum Einsatz
kommen k\&quot;onnen, um intelligente, Software-lastige Systeme im Fahrzeug
abzusichern. Dabei spielt zun\&quot;achst das Qualit\&quot;atsmanagement durch
Software-technische Ma{\ss}nahmen eine zentrale Rolle. Architektur- und
Entwurfmuster f\&quot;ur die Software-technische Absicherung von Komponenten werden
erg\&quot;anzt um Test-Konzepte zur Validierung von Spezifikationen und der
Robustheit der Implementierung. Architekturen und Entwurfs-Muster beschreiben
erprobte und damit konsolidierte L\&quot;osungen f\&quot;ur bestimmte Problemklassen wie
etwa Zuverl\&quot;assigkeit oder effiziente Ausf\&quot;uhrung.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4080</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4080</id><created>2014-10-15</created><updated>2015-09-11</updated><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author></authors><title>Generalized Fibonacci and Lucas cubes arising from powers of paths and
  cycles</title><categories>cs.DM math.CO</categories><comments>19 pages. arXiv admin note: substantial text overlap with
  arXiv:1210.5561</comments><msc-class>11B39, 05C38</msc-class><journal-ref>Discrete Mathematics, Volume 339, Issue 1, 6 January 2016, Pages
  270-282, ISSN 0012-365X</journal-ref><doi>10.1016/j.disc.2015.08.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with some generalizations of Fibonacci and Lucas sequences,
arising from powers of paths and cycles, respectively.
  In the first part of the work we provide a formula for the number of edges of
the Hasse diagram of the independent sets of the h-th power of a path ordered
by inclusion. For h=1 such a diagram is called a Fibonacci cube, and for h&gt;1 we
obtain a generalization of the Fibonacci cube. Consequently, we derive a
generalized notion of Fibonacci sequence, called h-Fibonacci sequence. Then, we
show that the number of edges of a generalized Fibonacci cube is obtained by
convolution of an h-Fibonacci sequence with itself.
  In the second part we consider the case of cycles. We evaluate the number of
edges of the Hasse diagram of the independent sets of the hth power of a cycle
ordered by inclusion. For h=1 such a diagram is called Lucas cube, and for h&gt;1
we obtain a generalization of the Lucas cube. We derive then a generalized
version of the Lucas sequence, called h-Lucas sequence. Finally, we show that
the number of edges of a generalized Lucas cube is obtained by an appropriate
convolution of an h-Fibonacci sequence with an h-Lucas sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4082</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4082</id><created>2014-10-15</created><authors><author><keyname>Pree</keyname><forenames>Wolfgang</forenames></author><author><keyname>Fontoura</keyname><forenames>Marcus</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Product Line Annotations with UML-F</title><categories>cs.SE</categories><comments>10 pages, 9 figures</comments><journal-ref>Software Product Lines - Second International Conference, SPLC 2,
  San Diego. G.J. Chastek (ed.), LNCS 2379, Springer Verlag. 2002</journal-ref><doi>10.1007/3-540-45652-X_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unified Modeling Language (UML) community has started to define so-called
profiles in order to better suit the needs of specific domains or settings.
Product lines1 represent a special breed of systems they are extensible
semi-finished pieces of software. Completing the semi-finished software leads
to various software pieces, typically specific applications, which share the
same core. Though product lines have been developed for a wide range of
domains, they apply common construction principles. The intention of the UML-F
profile (for framework architectures) is the definition of a UML subset,
enriched with a few UML-compliant extensions, which allows the annotation of
such artifacts. This paper presents aspects of the profile with a focus on
patterns and exemplifies the profile's usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4085</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4085</id><created>2014-10-15</created><authors><author><keyname>de Luca</keyname><forenames>Aldo</forenames></author><author><keyname>De Luca</keyname><forenames>Alessandro</forenames></author></authors><title>Sturmian words and the Stern sequence</title><categories>cs.DM math.CO</categories><comments>35 pages</comments><msc-class>68R15, 11B37</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Central, standard, and Christoffel words are three strongly interrelated
classes of binary finite words which represent a finite counterpart of
characteristic Sturmian words. A natural arithmetization of the theory is
obtained by representing central and Christoffel words by irreducible fractions
labeling respectively two binary trees, the Raney (or Calkin-Wilf) tree and the
Stern-Brocot tree. The sequence of denominators of the fractions in Raney's
tree is the famous Stern diatomic numerical sequence. An interpretation of the
terms $s(n)$ of Stern's sequence as lengths of Christoffel words when $n$ is
odd, and as minimal periods of central words when $n$ is even, allows one to
interpret several results on Christoffel and central words in terms of Stern's
sequence and, conversely, to obtain a new insight in the combinatorics of
Christoffel and central words by using properties of Stern's sequence. One of
our main results is a non-commutative version of the &quot;alternating bit sets
theorem&quot; by Calkin and Wilf. We also study the length distribution of
Christoffel words corresponding to nodes of equal height in the tree, obtaining
some interesting bounds and inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4086</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4086</id><created>2014-10-15</created><updated>2015-05-20</updated><authors><author><keyname>Mulholland</keyname><forenames>Ian P.</forenames></author><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author><author><keyname>Flanagan</keyname><forenames>Mark F.</forenames></author></authors><title>Design of LDPC Code Ensembles with Fast Convergence Properties</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, Submitted to the 3rd International Black Sea
  Conference on Communications and Networking (IEEE BlackSeaCom 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of low-density parity-check (LDPC) code ensembles optimized for a
finite number of decoder iterations is investigated. Our approach employs EXIT
chart analysis and differential evolution to design such ensembles for the
binary erasure channel and additive white Gaussian noise channel. The error
rates of codes optimized for various numbers of decoder iterations are compared
and it is seen that in the cases considered, the best performance for a given
number of decoder iterations is achieved by codes which are optimized for this
particular number. The design of generalized LDPC (GLDPC) codes is also
considered, showing that these structures can offer better performance than
LDPC codes for low-iteration-number designs. Finally, it is illustrated that
LDPC codes which are optimized for a small number of iterations exhibit
significant deviations in terms of degree distribution and weight enumerators
with respect to LDPC codes returned by more conventional design tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4094</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4094</id><created>2014-10-15</created><authors><author><keyname>Paech</keyname><forenames>Barbara</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Towards Development of Correct Software using Views</title><categories>cs.SE</categories><comments>14 pages, 7 figures</comments><journal-ref>ROOS project report GR/K67311-2, Proceedings of BCS FACS/EROS ROOM
  Workshop, 1997, Andy Evans, Kevin Lano (eds)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at integrating heterogeneous documents used in pragmatic
software develpoment methods to describe views with a formal refinement based
software development process. Therefore we propose an integrated semantics of
heterogeneous documents based on a common system model and a set of syntactic
development steps with a well-defined semantics for document evolution. The use
of the development steps is demonstrated in a small example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4095</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4095</id><created>2014-10-15</created><authors><author><keyname>S&#x103;l&#x103;gean</keyname><forenames>Ana</forenames></author><author><keyname>Mandache-S&#x103;l&#x103;gean</keyname><forenames>Matei</forenames></author><author><keyname>Winter</keyname><forenames>Richard</forenames></author><author><keyname>Phan</keyname><forenames>Raphael C. -W.</forenames></author></authors><title>Higher Order Differentiation over Finite Fields with Applications to
  Generalising the Cube Attack</title><categories>cs.CR</categories><comments>submitted to a journal</comments><msc-class>94A60</msc-class><acm-class>E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher order differentiation was introduced in a cryptographic context by
Lai. Several attacks can be viewed in the context of higher order
differentiations, amongst them the cube attack and the AIDA attack. All of the
above have been developed for the binary case.
  We examine differentiation in larger fields, starting with the field $GF(p)$
of integers modulo a prime $p$. We prove a number of results on differentiating
polynomials over such fields and then apply these techniques to generalising
the cube attack to $GF(p)$. The crucial difference is that now the degree in
each variable can be higher than one, and our proposed attack will
differentiate several times with respect to each variable (unlike the classical
cube attack and its larger field version described by Dinur and Shamir, both of
which differentiate at most once with respect to each variable).
  Finally we describe differentiation over finite fields $GF(p^m)$ with $p^m$
elements and prove that it can be reduced to differentiation over $GF(p)$, so a
cube attack over $GF(p^m)$ would be equivalent to cube attacks over $GF(p)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4099</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4099</id><created>2014-10-15</created><authors><author><keyname>Duque</keyname><forenames>Frank</forenames></author><author><keyname>Hidalgo-Toscano</keyname><forenames>Carlos</forenames></author></authors><title>An upper bound on the k-modem illumination problem</title><categories>cs.CG</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variation on the classical polygon illumination problem was introduced in
[Aichholzer et. al. EuroCG'09]. In this variant light sources are replaced by
wireless devices called k-modems, which can penetrate a fixed number k, of
&quot;walls&quot;. A point in the interior of a polygon is &quot;illuminated&quot; by a k-modem if
the line segment joining them intersects at most k edges of the polygon. It is
easy to construct polygons of n vertices where the number of k-modems required
to illuminate all interior points is Omega(n/k). However, no non-trivial upper
bound is known. In this paper we prove that the number of k-modems required to
illuminate any polygon of n vertices is at most O(n/k). For the cases of
illuminating an orthogonal polygon or a set of disjoint orthogonal segments, we
give a tighter bound of 6n/k + 1. Moreover, we present an O(n log n) time
algorithm to achieve this bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4122</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4122</id><created>2014-10-14</created><authors><author><keyname>Broniec</keyname><forenames>Anna</forenames></author></authors><title>Analysis of EEG signal by Flicker Noise Spectroscopy: Identification of
  right/left hand movement imagination</title><categories>physics.med-ph cs.HC q-bio.NC</categories><comments>16 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flicker Noise Spectroscopy (FNS) has been used for the analysis of
electroencephalography (EEG) signal related to the movement imagination. The
analysis of sensorimotor rhythms in time-frequency maps reveals the
event-related desynchronization (ERD) and the post-movement event-related
synchronization (ERS), observed mainly in the contralateral hemisphere to the
hand moved for the motor imagery. The signal has been parameterized in
accordance with FNS method. The significant changes of the FNS parameters, at
the time when the subject imagines the movement, have been observed. The
analysis of these parameters allows to distinguish between imagination of right
and left hands movement. Our study shows that the flicker-noise spectroscopy
can be an alternative method of analyzing EEG signal related to the imagination
of movement in terms of a potential application in the brain-computer interface
(BCI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4126</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4126</id><created>2014-10-14</created><updated>2015-04-13</updated><authors><author><keyname>Huemer</keyname><forenames>Clemens</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author></authors><title>On the disks with diameters the sides of a convex 5-gon</title><categories>math.MG cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for any convex pentagon there are two disks, among the five
disks having a side of the pentagon as diameter and the midpoint of the side as
its center, that do not intersect. This shows that $K_5$ is never the
intersection graph of such five disks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4135</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4135</id><created>2014-10-15</created><authors><author><keyname>Case</keyname><forenames>Adam</forenames></author><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author></authors><title>Mutual Dimension</title><categories>cs.CC</categories><comments>This article is 29 pages and has been submitted to ACM Transactions
  on Computation Theory. A preliminary version of part of this material was
  reported at the 2013 Symposium on Theoretical Aspects of Computer Science in
  Kiel, Germany</comments><msc-class>68Q30</msc-class><acm-class>F.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the lower and upper mutual dimensions $mdim(x:y)$ and $Mdim(x:y)$
between any two points $x$ and $y$ in Euclidean space. Intuitively these are
the lower and upper densities of the algorithmic information shared by $x$ and
$y$. We show that these quantities satisfy the main desiderata for a
satisfactory measure of mutual algorithmic information. Our main theorem, the
data processing inequality for mutual dimension, says that, if $f:\mathbb{R}^m
\rightarrow \mathbb{R}^n$ is computable and Lipschitz, then the inequalities
$mdim(f(x):y) \leq mdim(x:y)$ and $Mdim(f(x):y) \leq Mdim(x:y)$ hold for all $x
\in \mathbb{R}^m$ and $y \in \mathbb{R}^t$. We use this inequality and related
inequalities that we prove in like fashion to establish conditions under which
various classes of computable functions on Euclidean space preserve or
otherwise transform mutual dimensions between points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4139</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4139</id><created>2014-10-15</created><authors><author><keyname>Haustein</keyname><forenames>Stefanie</forenames></author><author><keyname>Bowman</keyname><forenames>Timothy D.</forenames></author><author><keyname>Holmberg</keyname><forenames>Kim</forenames></author><author><keyname>Tsou</keyname><forenames>Andrew</forenames></author><author><keyname>Sugimoto</keyname><forenames>Cassidy R.</forenames></author><author><keyname>Larivi&#xe8;re</keyname><forenames>Vincent</forenames></author></authors><title>Tweets as impact indicators: Examining the implications of automated bot
  accounts on Twitter</title><categories>cs.DL</categories><comments>9 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This brief communication presents preliminary findings on automated Twitter
accounts distributing links to scientific papers deposited on the preprint
repository arXiv. It discusses the implication of the presence of such bots
from the perspective of social media metrics (altmetrics), where mentions of
scholarly documents on Twitter have been suggested as a means of measuring
impact that is both broader and timelier than citations. We present preliminary
findings that automated Twitter accounts create a considerable amount of tweets
to scientific papers and that they behave differently than common social bots,
which has critical implications for the use of raw tweet counts in research
evaluation and assessment. We discuss some definitions of Twitter cyborgs and
bots in scholarly communication and propose differentiating between different
levels of engagement from tweeting only bibliographic information to discussing
or commenting on the content of a paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4141</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4141</id><created>2014-10-15</created><authors><author><keyname>Sakib</keyname><forenames>Shadman</forenames></author><author><keyname>Haq</keyname><forenames>Rakibul</forenames></author><author><keyname>Wazed</keyname><forenames>Tariq</forenames></author></authors><title>Unified mobile public health care system (UMPHCS) for underdeveloped
  countries</title><categories>cs.CY</categories><comments>6 pages, 8 figures, 3 tables. Published in the proceedings of 3rd
  International Conference on Informatics, Electronics &amp; Vision, Dhaka,
  Bangladesh. Available in IEEExplore.
  http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6850801&amp;url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6850801</comments><journal-ref>Proceedings of 3rd International Conference on Informatics,
  Electronics &amp; Vision-2014, Dhaka, Bangladesh. Published in IEEExplore</journal-ref><doi>10.1109/ICIEV.2014.6850801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have proposed a new smartphone based system for health
care, monitoring and diagnosis, which is specially designed to efficiently
increase the public health care system in the distant, rural, unreached areas
of the underdeveloped and developing countries. In this all-in-one system, we
have digitized the health monitoring and diagnostic devices in a way so that
each device works as a minimum `plug and play' sensor module of the total
system, reducing the cost radically. Besides, the easy-to-use smartphone
application for operating the whole system reduces the necessity of skilled and
trained manpower, making it a perfect toolbox for the government health workers
in the unreached rural areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4145</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4145</id><created>2014-10-15</created><updated>2015-01-04</updated><authors><author><keyname>Sakib</keyname><forenames>Shadman</forenames></author><author><keyname>Chowdhury</keyname><forenames>Anik</forenames></author><author><keyname>Ahamed</keyname><forenames>Shekh Tanvir</forenames></author><author><keyname>Hasan</keyname><forenames>Syed Imam</forenames></author></authors><title>Maze solving Algorithm for line following robot and derivation of linear
  path distance from nonlinear path</title><categories>cs.RO</categories><comments>Published in the Proceedings of 16th International Conference on
  Computer and Information Technology, Khulna, Bangladesh</comments><doi>10.1109/ICCITechn.2014.6997314</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have discussed a unique general algorithm for exploring and
solving any kind of line maze with another simple one for simple mazes without
loops or loops having highest two branches none of which are inward. For the
general algorithm, we need a method to map the whole maze, which is required if
the maze is complex. The proposed maze mapping system is based on coordinate
system and after mapping the whole maze as a graph in standard 'Adjacency-list
representation' method, shortest path and shortest time path was extracted
using Dijkstra's algorithm. In order to find the coordinates of the turning
points and junctions, linear distance between the points are needed, for which
wheel encoder was used. However, due to non-linear movement of robot, the
directly measured distance from the encoder has some error and to remove this
error an idea is built up which ended by deriving equations that gives us
almost exact linear distance between two points from the reading of wheel
encoder of the robot moving in a non-linear path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4147</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4147</id><created>2014-10-06</created><authors><author><keyname>Amro</keyname><forenames>Belal</forenames></author></authors><title>Mobile Agent Systems, Recent Security Threats and Counter Measures</title><categories>cs.CR</categories><comments>international journal of computer science issues, march 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile agent technology offers a dramatically evolving computing paradigm in
which a program, in the form of a software agent, can suspend its execution on
a host computer, transfers itself to another agent-enabled host on the network,
and resumes execution on the new host. It is 1960's since mobile code has been
used in the form of remote job entry systems. Today's mobile agents can be
characterized in a number of ways ranging from simple distributed objects to
highly organized intelligent softwares. As a result of this rapid evolvement of
mobile agents, plenty of critical security issues has risen and plenty of work
is being done to address these problems. The aim is to provide trusted mobile
agent systems that can be easily deployed and widely adopted. In this paper, we
provide an overview of the most recent threats facing the designers of agent
platforms and the developers of agent-based applications. The paper also
identifies security objectives, and measures for countering the identified
threats and fulfilling those security objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4154</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4154</id><created>2014-10-15</created><updated>2015-09-23</updated><authors><author><keyname>Gupta</keyname><forenames>Anshul</forenames><affiliation>University of Liverpool</affiliation></author><author><keyname>Schewe</keyname><forenames>Sven</forenames><affiliation>University of Liverpool</affiliation></author><author><keyname>Wojtczak</keyname><forenames>Dominik</forenames><affiliation>University of Liverpool</affiliation></author></authors><title>Making the Best of Limited Memory in Multi-Player Discounted Sum Games</title><categories>cs.GT</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 16-30</journal-ref><doi>10.4204/EPTCS.193.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish the existence of optimal bounded memory strategy
profiles in multi-player discounted sum games. We introduce a non-deterministic
approach to compute optimal strategy profiles with bounded memory. Our approach
can be used to obtain optimal rewards in a setting where a powerful player
selects the strategies of all players for Nash and leader equilibria, where in
leader equilibria the Nash condition is waived for the strategy of this
powerful player. The resulting strategy profiles are optimal for this player
among all strategy profiles that respect the given memory bound, and the
related decision problem is NP-complete. We also provide simple examples, which
show that having more memory will improve the optimal strategy profile, and
that sufficient memory to obtain optimal strategy profiles cannot be inferred
from the structure of the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4155</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4155</id><created>2014-10-15</created><updated>2015-06-20</updated><authors><author><keyname>Joda</keyname><forenames>Roghayeh</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Access Policy Design for Cognitive Secondary Users under a Primary
  Type-I HARQ Process</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an underlay cognitive radio network that consists of an
arbitrary number of secondary users (SU) is considered, in which the primary
user (PU) employs Type-I Hybrid Automatic Repeat Request (HARQ). Exploiting the
redundancy in PU retransmissions, each SU receiver applies forward interference
cancelation to remove a successfully decoded PU message in the subsequent PU
retransmissions. The knowledge of the PU message state at the SU receivers and
the ACK/NACK message from the PU receiver are sent back to the transmitters.
With this approach and using a Constrained Markov Decision Process (CMDP) model
and Constrained Multi-agent MDP (CMMDP), centralized and decentralized optimum
access policies for SUs are proposed to maximize their average sum throughput
under a PU throughput constraint. In the decentralized case, the channel access
decision of each SU is unknown to the other SU. Numerical results demonstrate
the benefits of the proposed policies in terms of sum throughput of SUs. The
results also reveal that the centralized access policy design outperforms the
decentralized design especially when the PU can tolerate a low average long
term throughput. Finally, the difficulties in decentralized access policy
design with partial state information are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4156</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4156</id><created>2014-10-15</created><updated>2015-12-06</updated><authors><author><keyname>Afrati</keyname><forenames>Foto</forenames></author><author><keyname>Joglekar</keyname><forenames>Manas</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author><author><keyname>Salihoglu</keyname><forenames>Semih</forenames></author><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author></authors><title>GYM: A Multiround Join Algorithm In MapReduce</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiround algorithms are now commonly used in distributed data processing
systems, yet the extent to which algorithms can benefit from running more
rounds is not well understood. This paper answers this question for a spectrum
of rounds for the problem of computing the equijoin of $n$ relations.
Specifically, given any query $Q$ with width $\w$, {\em intersection width}
$\iw$, input size $\mathrm{IN}$, output size $\mathrm{OUT}$, and a cluster of
machines with $M$ memory available per machine, we show that:
  (1) $Q$ can be computed in $O(n)$ rounds with $O(n\frac{(\mathrm{IN}^{\w} +
\mathrm{OUT})^2}{M})$ communication cost.
  (2) $Q$ can be computed in $O(\log(n))$ rounds with
$O(n\frac{(\mathrm{IN}^{\max(\w, 3\iw)} + \mathrm{OUT})^2}{M})$ communication
cost. \end{itemize} Intersection width is a new notion of queries and
generalized hypertree decompositions (GHDs) of queries we introduce to capture
how connected the adjacent cyclic components of the GHDs are.
  We achieve our first result by introducing a distributed and generalized
version of Yannakakis's algorithm, called GYM. GYM takes as input any GHD of
$Q$ with width $\w$ and depth $d$, and computes $Q$ in $O(d + \log(n))$ rounds
and $O(n\frac{(\mathrm{IN}^{\w} + \mathrm{OUT})^2}{M})$ communication cost. We
achieve our second result by showing how to construct GHDs of $Q$ with width
$\max(\w, 3\iw)$ and depth $O(\log(n))$. We describe another technique to
construct GHDs with longer widths and shorter depths, demonstrating a spectrum
of tradeoffs one can make between communication and the number of rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4161</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4161</id><created>2014-10-15</created><updated>2015-09-22</updated><authors><author><keyname>Wu</keyname><forenames>Chenchen</forenames></author><author><keyname>Xu</keyname><forenames>Dachuan</forenames></author><author><keyname>Du</keyname><forenames>Donglei</forenames></author><author><keyname>Wang</keyname><forenames>Yishui</forenames></author></authors><title>An improved approximation algorithm for k-median problem using a new
  factor-revealing LP</title><categories>cs.DS</categories><comments>This paper has been withdrawn by the authors due to a critical error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-median problem is a well-known strongly NP-hard combinatorial
optimization problem of both theoretical and practical significance. The
previous best approximation ratio for this problem is 2.611+\epsilon (Bryka et
al. 2014) based on an (1, 1.95238219) bi-factor approximation algorithm for the
classical facility location problem (FLP). This work offers an improved
algorithm with an approximation ratio 2.592 +\epsilon based on a new (1,
1.93910094) bi-factor approximation algorithm for the FLP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4168</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4168</id><created>2014-10-15</created><authors><author><keyname>Devresse</keyname><forenames>Adrien</forenames></author><author><keyname>Furano</keyname><forenames>Fabrizio</forenames></author></authors><title>Efficient HTTP based I/O on very large datasets for high performance
  computing with the libdavix library</title><categories>cs.PF cs.DC</categories><comments>Presented at: Very large Data Bases (VLDB) 2014, Hangzhou</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Remote data access for data analysis in high performance computing is
commonly done with specialized data access protocols and storage systems. These
protocols are highly optimized for high throughput on very large datasets,
multi-streams, high availability, low latency and efficient parallel I/O. The
purpose of this paper is to describe how we have adapted a generic protocol,
the Hyper Text Transport Protocol (HTTP) to make it a competitive alternative
for high performance I/O and data analysis applications in a global computing
grid: the Worldwide LHC Computing Grid. In this work, we first analyze the
design differences between the HTTP protocol and the most common high
performance I/O protocols, pointing out the main performance weaknesses of
HTTP. Then, we describe in detail how we solved these issues. Our solutions
have been implemented in a toolkit called davix, available through several
recent Linux distributions. Finally, we describe the results of our benchmarks
where we compare the performance of davix against a HPC specific protocol for a
data analysis use case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4176</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4176</id><created>2014-10-15</created><authors><author><keyname>Bowman</keyname><forenames>Samuel R.</forenames></author><author><keyname>Potts</keyname><forenames>Christopher</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author></authors><title>Learning Distributed Word Representations for Natural Logic Reasoning</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural logic offers a powerful relational conception of meaning that is a
natural counterpart to distributed semantic representations, which have proven
valuable in a wide range of sophisticated language tasks. However, it remains
an open question whether it is possible to train distributed representations to
support the rich, diverse logical reasoning captured by natural logic. We
address this question using two neural network-based models for learning
embeddings: plain neural networks and neural tensor networks. Our experiments
evaluate the models' ability to learn the basic algebra of natural logic
relations from simulated data and from the WordNet noun graph. The overall
positive results are promising for the future of learned distributed
representations in the applied modeling of logical semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4180</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4180</id><created>2014-10-08</created><authors><author><keyname>Issac</keyname><forenames>B.</forenames></author><author><keyname>Hamid</keyname><forenames>K.</forenames></author><author><keyname>Tan</keyname><forenames>C. E.</forenames></author></authors><title>Wireless Mobility Management with Prediction, Delay Reduction and
  Resource Management in 802.11 Networks</title><categories>cs.NI</categories><comments>16 pages</comments><journal-ref>(2008). IAENG International Journal of Computer Science, 35(3),
  361-376</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 802.11 wireless infrastructure networks, as the mobile node moves from one
access point to another, the active connections will not be badly dropped if
the handoff is smooth and if there are sufficient resources reserved in the
target access point. In a 5x5 grid of access points, within a 6x6 grid of
regions, by location tracking and data mining, we predict the mobility pattern
of mobile node with good accuracy. The pre-scanning of mobile nodes, along with
pre-authenticating neighbouring access points and pre-reassociation is used to
reduce the scan delay, authentication delay and re-association delay
respectively. The model implements first stage reservation by using prediction
results and does second stage reservation based on the packet content type, so
that sufficient resources can be reserved when the mobile node does the handoff
to the next access point. The overall mobility management scheme thus reduces
the handoff delay. The performance simulations are done to verify the proposed
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4182</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4182</id><created>2014-10-08</created><authors><author><keyname>Modapothala</keyname><forenames>J. R.</forenames></author><author><keyname>Issac</keyname><forenames>B.</forenames></author></authors><title>Analysis of corporate environmental reports using statistical techniques
  and data mining</title><categories>cs.AI</categories><comments>8 pages</comments><journal-ref>Communications of the IBIMA, 10(6), 32-38. (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring the effectiveness of corporate environmental reports, it being
highly qualitative and less regulated, is often considered as a daunting task.
The task becomes more complex if comparisons are to be performed. This study is
undertaken to overcome the physical verification problems by implementing data
mining technique. It further explores on the effectiveness by performing
exploratory analysis and structural equation model to bring out the significant
linkages between the selected 10 variables. Samples of five hundred and thirty
nine reports across various countries are used from an international directory
to perform the statistical analysis like: One way ANOVA (Analysis of Variance),
MDA (Multivariate Discriminant Analysis) and SEM (Structural Equation
Modeling). The results indicate the significant differences among the various
types of industries in their environmental reporting, and the exploratory
factors like stakeholder, organization strategy and industrial oriented
factors, proved significant. The major accomplishment is that the findings
correlate with the conceptual frame work of GRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4187</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4187</id><created>2014-10-15</created><authors><author><keyname>Abbas</keyname><forenames>Taimoor</forenames><affiliation>Dept. of Electrical and Information Technology, Lund University, Lund, Sweden</affiliation></author><author><keyname>Nuckelt</keyname><forenames>J&#xf6;rg</forenames><affiliation>Nachrichtentechnik, Technische Universit&#xe4;t Braunschweig, Braunschweig, Germany</affiliation></author><author><keyname>K&#xfc;rner</keyname><forenames>Thomas</forenames><affiliation>Nachrichtentechnik, Technische Universit&#xe4;t Braunschweig, Braunschweig, Germany</affiliation></author><author><keyname>Zemen</keyname><forenames>Thomas</forenames><affiliation>Forschungszentrum Telekommunikation Wien</affiliation></author><author><keyname>Mecklenbr&#xe4;uker</keyname><forenames>Christoph</forenames><affiliation>Institut f&#xfc;r Nachrichtentechnik und Hochfrequenztechnik, Technische Universit&#xe4;t Wien, Vienna, Austria</affiliation></author><author><keyname>Tufvesson</keyname><forenames>Fredrik</forenames><affiliation>Dept. of Electrical and Information Technology, Lund University, Lund, Sweden</affiliation></author></authors><title>Simulation and Measurement Based Vehicle-to-Vehicle Channel
  Characterization: Accuracy and Constraint Analysis</title><categories>cs.NI</categories><comments>10 pages, 20 figures, Submitted for publication and is under review
  in IEEE Transactions on Antennas and Propagation, 2014</comments><journal-ref>IEEE Transactions on Antennas and Propagation, Vol. 63, Issue 7,
  2015</journal-ref><doi>10.1109/TAP.2015.2428280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a deterministic channel model for vehicle-to-vehicle (V2V)
communication, is compared against channel measurement data collected during a
V2V channel measurement campaign using a channel sounder. Channel metrics such
as channel gain, delay and Doppler spreads, eigenvalue decomposition and
antenna correlations are derived from the ray tracing (RT) simulations as well
as from the measurement data obtained from two different measurements in an
urban four-way intersection scenario. The channel metrics are compared
separately for line-of-sight (LOS) and non-LOS (NLOS) situation. Most power
contributions arise from the LOS component (if present) as well as from
multipaths with single bounce reflections. Measurement and simulation results
show a very good agreement in the presence of LOS, as most of the received
power is contributed from the LOS component. In NLOS, the difference is large
because the ray tracer is unable to capture some of the multi bounced
propagation paths that are present in the measurements. Despite the limitations
of the ray-based propagation model identified in this work, the model is
suitable to characterize the channel properties in a sufficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4207</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4207</id><created>2014-10-15</created><authors><author><keyname>Bazzoli</keyname><forenames>Enrico</forenames></author><author><keyname>Criscione</keyname><forenames>Claudio</forenames></author><author><keyname>Maggi</keyname><forenames>Federico</forenames></author><author><keyname>Zanero</keyname><forenames>Stefano</forenames></author></authors><title>XSS Peeker: A Systematic Analysis of Cross-site Scripting Vulnerability
  Scanners</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the first publication of the &quot;OWASP Top 10&quot; (2004), cross-site
scripting (XSS) vulnerabilities have always been among the top 5 web
application security bugs. Black-box vulnerability scanners are widely used in
the industry to reproduce (XSS) attacks automatically. In spite of the
technical sophistication and advancement, previous work showed that black-box
scanners miss a non-negligible portion of vulnerabilities, and report
non-existing, non-exploitable or uninteresting vulnerabilities. Unfortunately,
these results hold true even for XSS vulnerabilities, which are relatively
simple to trigger if compared, for instance, to logic flaws.
  Black-box scanners have not been studied in depth on this vertical: knowing
precisely how scanners try to detect XSS can provide useful insights to
understand their limitations, to design better detection methods. In this
paper, we present and discuss the results of a detailed and systematic study on
6 black-box web scanners (both proprietary and open source) that we conducted
in coordination with the respective vendors. To this end, we developed an
automated tool to (1) extract the payloads used by each scanner, (2) distill
the &quot;templates&quot; that have originated each payload, (3) evaluate them according
to quality indicators, and (4) perform a cross-scanner analysis. Unlike
previous work, our testbed application, which contains a large set of XSS
vulnerabilities, including DOM XSS, was gradually retrofitted to accomodate for
the payloads that triggered no vulnerabilities.
  Our analysis reveals a highly fragmented scenario. Scanners exhibit a wide
variety of distinct payloads, a non-uniform approach to fuzzing and mutating
the payloads, and a very diverse detection effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4209</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4209</id><created>2014-10-15</created><authors><author><keyname>Adar</keyname><forenames>Ron</forenames></author><author><keyname>Epstein</keyname><forenames>Leah</forenames></author></authors><title>Models for the k-metric dimension</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an undirected graph G=(V,E), a vertex x \in V separates vertices u and v
(where u,v \in V, u \neq v) if their distances to x are not equal. Given an
integer parameter k \geq 1, a set of vertices L \subseteq V is a feasible
solution if for every pair of distinct vertices, u,v, there are at least k
distinct vertices x_1,x_2,...,x_k \in L each separating u and v. Such a
feasible solution is called a &quot;landmark set&quot;, and the k-metric dimension of a
graph is the minimal cardinality of a landmark set for the parameter k. The
case k=1 is a classic problem, where in its weighted version, each vertex v has
a non-negative weight, and the goal is to find a landmark set with minimal
total weight. We generalize the problem for k \geq 2, introducing two models,
and we seek for solutions to both the weighted version and the unweighted
version of this more general problem. In the model of all-pairs (AP), k
separations are needed for every pair of distinct vertices of V, while in the
non-landmarks model (NL), such separations are required only for pairs of
distinct vertices in V \setminus L.
  We study the weighted and unweighted versions for both models (AP and NL),
for path graphs, complete graphs, complete bipartite graphs, and complete wheel
graphs, for all values of k \geq 2. We present algorithms for these cases, thus
demonstrating the difference between the two new models, and the differences
between the cases k=1 and k \geq 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4210</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4210</id><created>2014-10-15</created><authors><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of
  Convex Sets</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique
for simultaneously discovering group and within-group sparse patterns by using
a combination of the $\ell_1$ and $\ell_2$ norms. However, in large-scale
applications, the complexity of the regularizers entails great computational
challenges. In this paper, we propose a novel Two-Layer Feature REduction
method (TLFre) for SGL via a decomposition of its dual feasible set. The
two-layer reduction is able to quickly identify the inactive groups and the
inactive features, respectively, which are guaranteed to be absent from the
sparse representation and can be removed from the optimization. Existing
feature reduction methods are only applicable for sparse models with one
sparsity-inducing regularizer. To our best knowledge, TLFre is the first one
that is capable of dealing with multiple sparsity-inducing regularizers.
Moreover, TLFre has a very low computational cost and can be integrated with
any existing solvers. We also develop a screening method---called DPC
(DecomPosition of Convex set)---for the nonnegative Lasso problem. Experiments
on both synthetic and real data sets show that TLFre and DPC improve the
efficiency of SGL and nonnegative Lasso by several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4218</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4218</id><created>2014-10-15</created><updated>2015-02-20</updated><authors><author><keyname>Michelusi</keyname><forenames>Nicolo</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Optimal Adaptive Random Multiaccess in Energy Harvesting Wireless Sensor
  Networks</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communications</comments><doi>10.1109/TCOMM.2015.2402662</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensors can integrate rechargeable batteries and energy-harvesting
(EH) devices to enable long-term, autonomous operation, thus requiring
intelligent energy management to limit the adverse impact of energy outages.
This work considers a network of EH wireless sensors, which report packets with
a random utility value to a fusion center (FC) over a shared wireless channel.
Decentralized access schemes are designed, where each node performs a local
decision to transmit/discard a packet, based on an estimate of the packet's
utility, its own energy level, and the scenario state of the EH process, with
the objective to maximize the average long-term aggregate utility of the
packets received at the FC. Due to the non-convex structure of the problem, an
approximate optimization is developed by resorting to a mathematical artifice
based on a game theoretic formulation of the multiaccess scheme, where the
nodes do not behave strategically, but rather attempt to maximize a
\emph{common} network utility with respect to their own policy. The symmetric
Nash equilibrium (SNE) is characterized, where all nodes employ the same
policy; its uniqueness is proved, and it is shown to be a local maximum of the
original problem. An algorithm to compute the SNE is presented, and a heuristic
scheme is proposed, which is optimal for large battery capacity. It is shown
numerically that the SNE typically achieves near-optimal performance, within 3%
of the optimal policy, at a fraction of the complexity, and two operational
regimes of EH-networks are identified and analyzed: an energy-limited scenario,
where energy is scarce and the channel is under-utilized, and a network-limited
scenario, where energy is abundant and the shared wireless channel represents
the bottleneck of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4235</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4235</id><created>2014-10-15</created><authors><author><keyname>Dongol</keyname><forenames>Brijesh</forenames></author><author><keyname>Hayes</keyname><forenames>Ian J.</forenames></author><author><keyname>Struth</keyname><forenames>Georg</forenames></author></authors><title>Convolution, Separation and Concurrency</title><categories>cs.LO cs.FL</categories><comments>39 pages</comments><acm-class>F.4.0; F.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A notion of convolution is presented in the context of formal power series
together with lifting constructions characterising algebras of such series,
which usually are quantales. A number of examples underpin the universality of
these constructions, the most prominent ones being separation logics, where
convolution is separating conjunction in an assertion quantale; interval
logics, where convolution is the chop operation; and stream interval functions,
where convolution is used for analysing the trajectories of dynamical or
real-time systems. A Hoare logic is constructed in a generic fashion on the
power series quantale, which applies to each of these examples. In many cases,
commutative notions of convolution have natural interpretations as concurrency
operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4240</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4240</id><created>2014-10-15</created><authors><author><keyname>Umboh</keyname><forenames>Seeun</forenames></author></authors><title>Online Network Design Algorithms via Hierarchical Decompositions</title><categories>cs.DS</categories><comments>Accepted to SODA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new approach for online network design and obtain improved
competitive ratios for several problems. Our approach gives natural
deterministic algorithms and simple analyses. At the heart of our work is a
novel application of embeddings into hierarchically well-separated trees (HSTs)
to the analysis of online network design algorithms --- we charge the cost of
the algorithm to the cost of the optimal solution on any HST embedding of the
terminals. This analysis technique is widely applicable to many problems and
gives a unified framework for online network design.
  In a sense, our work brings together two of the main approaches to online
network design. The first uses greedy-like algorithms and analyzes them using
dual-fitting. The second uses tree embeddings and results in randomized $O(\log
n)$-competitive algorithms, where $n$ is the total number of vertices in the
graph. Our approach uses deterministic greedy-like algorithms but analyzes them
via HST embeddings of the terminals. Our proofs are simpler as we do not need
to carefully construct dual solutions and we get $O(\log k)$ competitive
ratios, where $k$ is the number of terminals.
  In this paper, we apply our approach to obtain deterministic $O(\log
k)$-competitive online algorithms for the following problems.
  - Steiner network with edge duplication. Previously, only a randomized
$O(\log n)$-competitive algorithm was known.
  - Rent-or-buy. Previously, only deterministic $O(\log^2 k)$-competitive and
randomized $O(\log k)$-competitive algorithms by Awerbuch, Azar and Bartal
(2004) were known.
  - Connected facility location. Previously, only a randomized $O(\log^2
k)$-competitive algorithm by San Felice, Williamson and Lee (2014) was known.
  - Prize-collecting Steiner forest. We match the competitive ratio first
achieved by Qian and Williamson (2011) and give a simpler analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4241</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4241</id><created>2014-10-15</created><authors><author><keyname>Ghazi</keyname><forenames>Badih</forenames></author><author><keyname>Lee</keyname><forenames>Euiwoong</forenames></author></authors><title>LP/SDP Hierarchy Lower Bounds for Decoding Random LDPC Codes</title><categories>cs.CC cs.IT math.IT</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random (dv,dc)-regular LDPC codes are well-known to achieve the Shannon
capacity of the binary symmetric channel (for sufficiently large dv and dc)
under exponential time decoding. However, polynomial time algorithms are only
known to correct a much smaller fraction of errors. One of the most powerful
polynomial-time algorithms with a formal analysis is the LP decoding algorithm
of Feldman et al. which is known to correct an Omega(1/dc) fraction of errors.
In this work, we show that fairly powerful extensions of LP decoding, based on
the Sherali-Adams and Lasserre hierarchies, fail to correct much more errors
than the basic LP-decoder. In particular, we show that:
  1) For any values of dv and dc, a linear number of rounds of the
Sherali-Adams LP hierarchy cannot correct more than an O(1/dc) fraction of
errors on a random (dv,dc)-regular LDPC code.
  2) For any value of dv and infinitely many values of dc, a linear number of
rounds of the Lasserre SDP hierarchy cannot correct more than an O(1/dc)
fraction of errors on a random (dv,dc)-regular LDPC code.
  Our proofs use a new stretching and collapsing technique that allows us to
leverage recent progress in the study of the limitations of LP/SDP hierarchies
for Maximum Constraint Satisfaction Problems (Max-CSPs). The problem then
reduces to the construction of special balanced pairwise independent
distributions for Sherali-Adams and special cosets of balanced pairwise
independent subgroups for Lasserre.
  Some of our techniques are more generally applicable to a large class of
Boolean CSPs called Min-Ones. In particular, for k-Hypergraph Vertex Cover, we
obtain an improved integrality gap of $k-1-\epsilon$ that holds after a
\emph{linear} number of rounds of the Lasserre hierarchy, for any k = q+1 with
q an arbitrary prime power. The best previous gap for a linear number of rounds
was equal to $2-\epsilon$ and due to Schoenebeck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4246</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4246</id><created>2014-10-15</created><updated>2015-04-24</updated><authors><author><keyname>Yi</keyname><forenames>Xinlei</forenames></author><author><keyname>Lu</keyname><forenames>Wenlian</forenames></author><author><keyname>Chen</keyname><forenames>Tianping</forenames></author></authors><title>Pull-Based Distributed Event-triggered Consensus for Multi-agent Systems
  with Directed Topologies</title><categories>math.OC cs.SY nlin.AO</categories><comments>arXiv admin note: text overlap with arXiv:1407.1377</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper mainly investigates consensus problem with pull-based
event-triggered feedback control. For each agent, the diffusion coupling
feedbacks are based on the states of its in-neighbors at its latest triggering
time and the next triggering time of this agent is determined by its
in-neighbors' information as well. The general directed topologies, including
irreducible and reducible cases, are investigated. The scenario of distributed
continuous monitoring is considered firstly, namely each agent can observe its
in-neighbors' continuous states. It is proved that if the network topology has
a spanning tree, then the event-triggered coupling strategy can realize
consensus for the multi-agent system. Then the results are extended to
discontinuous monitoring, i.e., self-triggered control, where each agent
computes its next triggering time in advance without having to observe the
system's states continuously. The effectiveness of the theoretical results are
illustrated by a numerical example finally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4249</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4249</id><created>2014-10-15</created><authors><author><keyname>Le</keyname><forenames>Long</forenames></author><author><keyname>Jones</keyname><forenames>Douglas L.</forenames></author></authors><title>Optimal Simultaneous Detection and Signal and Noise Power Estimation</title><categories>cs.IT math.IT</categories><comments>appears in 2014 IEEE International Symposium on Information Theory
  (ISIT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous detection and estimation is important in many engineering
applications. In particular, there are many applications where it is important
to perform signal detection and Signal-to-Noise-Ratio (SNR) estimation jointly.
Application of existing frameworks in the literature that handle simultaneous
detection and estimation is not straightforward for this class of application.
This paper therefore aims at bridging the gap between an existing framework,
specifically the work by Middleton et al., and the mentioned application class
by presenting a jointly optimal detector and signal and noise power estimators.
The detector and estimators are given for the Gaussian observation model with
appropriate conjugate priors on the signal and noise power. Simulation results
affirm the superior performance of the optimal solution compared to the
separate detection and estimation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4252</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4252</id><created>2014-10-15</created><updated>2015-06-23</updated><authors><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Cheung</keyname><forenames>Karen C.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Joint Channel Parameter Estimation via Diffusive Molecular Communication</title><categories>cs.IT math.IT</categories><comments>14 pages, 2 tables, 8 figures. To appear in IEEE Transactions on
  Molecular, Biological, and Multi-Scale Communications</comments><doi>10.1109/TMBMC.2015.2465511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design and analysis of diffusive molecular communication systems
generally requires knowledge of the environment's physical and chemical
properties. Furthermore, prospective applications might rely on the timely
detection of changes in the local system parameters. This paper studies the
local estimation of channel parameters for diffusive molecular communication
when a transmitter releases molecules that are observed by a receiver. The
Fisher information matrix of the joint parameter estimation problem is derived
so that the Cramer-Rao lower bound on the variance of locally unbiased
estimation can be found. The joint estimation problem can be reduced to the
estimation of any subset of the channel parameters. Maximum likelihood
estimation leads to closed-form solutions for some single-parameter estimation
problems and can otherwise be determined numerically. Peak-based estimators are
proposed for low-complexity estimation of a single unknown parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4256</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4256</id><created>2014-10-15</created><authors><author><keyname>Marzuoli</keyname><forenames>Aude</forenames></author><author><keyname>Boidot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author><author><keyname>van Erp</keyname><forenames>Paul B. C.</forenames></author><author><keyname>Ucko</keyname><forenames>Alexis</forenames></author><author><keyname>Bayen</keyname><forenames>Alexandre</forenames></author><author><keyname>Hansen</keyname><forenames>Mark</forenames></author></authors><title>Anatomy of a Crash</title><categories>cs.SY cs.MA physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transportation networks constitute a critical infrastructure enabling the
transfers of passengers and goods, with a significant impact on the economy at
different scales. Transportation modes, whether air, road or rail, are coupled
and interdependent. The frequent occurrence of perturbations on one or several
modes disrupts passengers' entire journeys, directly and through ripple
effects. The present paper provides a case report of the Asiana Crash in San
Francisco International Airport on July 6th 2013 and its repercussions on the
multimodal transportation network. It studies the resulting propagation of
disturbances on the transportation infrastructure in the United States. The
perturbation takes different forms and varies in scale and time frame :
cancellations and delays snowball in the airspace, highway traffic near the
airport is impacted by congestion in previously never congested locations, and
transit passenger demand exhibit unusual traffic peaks in between airports in
the Bay Area. This paper, through a case study, aims at stressing the
importance of further data-driven research on interdependent infrastructure
networks for increased resilience. The end goal is to form the basis for
optimization models behind providing more reliable passenger door-to-door
journeys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4258</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4258</id><created>2014-10-15</created><updated>2016-02-07</updated><authors><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author></authors><title>A Comprehensive Survey of Recent Advancements in Molecular Communication</title><categories>cs.ET</categories><comments>Accepted for publication in IEEE Communications Surveys &amp; Tutorials</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With much advancement in the field of nanotechnology, bioengineering and
synthetic biology over the past decade, microscales and nanoscales devices are
becoming a reality. Yet the problem of engineering a reliable communication
system between tiny devices is still an open problem. At the same time, despite
the prevalence of radio communication, there are still areas where traditional
electromagnetic waves find it difficult or expensive to reach. Points of
interest in industry, cities, and medical applications often lie in embedded
and entrenched areas, accessible only by ventricles at scales too small for
conventional radio waves and microwaves, or they are located in such a way that
directional high frequency systems are ineffective. Inspired by nature, one
solution to these problems is molecular communication (MC), where chemical
signals are used to transfer information. Although biologists have studied MC
for decades, it has only been researched for roughly 10 year from a
communication engineering lens. Significant number of papers have been
published to date, but owing to the need for interdisciplinary work, much of
the results are preliminary. In this paper, the recent advancements in the
field of MC engineering are highlighted. First, the biological, chemical, and
physical processes used by an MC system are discussed. This includes different
components of the MC transmitter and receiver, as well as the propagation and
transport mechanisms. Then, a comprehensive survey of some of the recent works
on MC through a communication engineering lens is provided. The paper ends with
a technology readiness analysis of MC and future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4262</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4262</id><created>2014-10-15</created><authors><author><keyname>Ickowicz</keyname><forenames>Adrien</forenames></author></authors><title>Approximate Bayesian algorithms for multiple target tracking with binary
  sensors</title><categories>cs.SY</categories><comments>19 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an approximate Bayesian computation approach to
perform a multiple target tracking within a binary sensor network. The nature
of the binary sensors (getting closer - moving away information) do not allow
the use of the classical tools (e.g. Kalman Filter, Particle Filer), because
the exact likelihood is intractable. To overcome this, we use the particular
feature of the likelihood-free algorithms to produce an efficient multiple
target tracking methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4266</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4266</id><created>2014-10-15</created><authors><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Manasse</keyname><forenames>Mark</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author></authors><title>Consistent Weighted Sampling Made Fast, Small, and Easy</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document sketching using Jaccard similarity has been a workable effective
technique in reducing near-duplicates in Web page and image search results, and
has also proven useful in file system synchronization, compression and learning
applications.
  Min-wise sampling can be used to derive an unbiased estimator for Jaccard
similarity and taking a few hundred independent consistent samples leads to
compact sketches which provide good estimates of pairwise-similarity.
Subsequent works extended this technique to weighted sets and show how to
produce samples with only a constant number of hash evaluations for any
element, independent of its weight. Another improvement by Li et al. shows how
to speedup sketch computations by computing many (near-)independent samples in
one shot. Unfortunately this latter improvement works only for the unweighted
case.
  In this paper we give a simple, fast and accurate procedure which reduces
weighted sets to unweighted sets with small impact on the Jaccard similarity.
This leads to compact sketches consisting of many (near-)independent weighted
samples which can be computed with just a small constant number of hash
function evaluations per weighted element. The size of the produced unweighted
set is furthermore a tunable parameter which enables us to run the unweighted
scheme of Li et al. in the regime where it is most efficient. Even when the
sets involved are unweighted, our approach gives a simple solution to the
densification problem that other works attempted to address.
  Unlike previously known schemes, ours does not result in an unbiased
estimator. However, we prove that the bias introduced by our reduction is
negligible and that the standard deviation is comparable to the unweighted
case. We also empirically evaluate our scheme and show that it gives
significant gains in computational efficiency, without any measurable loss in
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4273</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4273</id><created>2014-10-15</created><updated>2014-12-15</updated><authors><author><keyname>Anderson</keyname><forenames>David G.</forenames></author><author><keyname>Gu</keyname><forenames>Ming</forenames></author><author><keyname>Melgaard</keyname><forenames>Christopher</forenames></author></authors><title>An Efficient Algorithm for Unweighted Spectral Graph Sparsification</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral graph sparsification has emerged as a powerful tool in the analysis
of large-scale networks by reducing the overall number of edges, while
maintaining a comparable graph Laplacian matrix. In this paper, we present an
efficient algorithm for the construction of a new type of spectral sparsifier,
the unweighted spectral sparsifier. Given a general undirected and unweighted
graph $G = (V, E)$ and an integer $\ell &lt; |E|$ (the number of edges in $E$), we
compute an unweighted graph $H = (V, F)$ with $F \subset E$ and $|F| = \ell$
such that for every $x \in \mathbb{R}^{V}$ \[ {\displaystyle \frac{x^T L_G
x}{\kappa} \leq x^T L_H x \leq x^T L_G x,} \] where $L_G$ and $L_H$ are the
Laplacian matrices for $G$ and $H$, respectively, and $\kappa \geq 1$ is a
slowly-varying function of $|V|, |E|$ and $\ell$. This work addresses the open
question of the existence of unweighted graph sparsifiers for unweighted
graphs. Additionally, our algorithm can efficiently compute unweighted graph
sparsifiers for weighted graphs, leading to sparsified graphs that retain the
weights of the original graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4278</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4278</id><created>2014-10-15</created><updated>2016-02-02</updated><authors><author><keyname>Wen</keyname><forenames>Jinming</forenames></author><author><keyname>Zhou</keyname><forenames>Baojian</forenames></author><author><keyname>Mow</keyname><forenames>Wai Ho</forenames></author><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author></authors><title>An Efficient Algorithm for Optimally Solving a Shortest Vector Problem
  in Compute-and-Forward Protocol Design</title><categories>cs.IT math.IT</categories><comments>This work was presented in part at the IEEE International Conference
  on Communications (ICC 2015), London, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding the optimal coefficient vector that
maximizes the computation rate at a relay in the computer-and-forward scheme.
Based on the idea of sphere decoding, we propose a low-complexity method that
gives the maximum rate. First, we derive an efficient algorithm to compute the
Cholesky factorization by using the structure of the matrix, to transfer the
problem to a shortest vector problem (SVP). This algorithm gives the
closed-form expression of the Cholesky factor $\R$ with only $\bigO(n^2)$
flops. Based on this $\R$, we propose some conditions that can be checked by
$\bigO(n)$ flops, under which the optimal coefficient vector can be obtained
immediately without using any search algorithm. Second, by taking into account
some resultant useful properties of the optimal coefficient vector, we modify
the Schnorr-Euchner search algorithm to solve the SVP. Although the general SVP
is suspected to be NP-hard, by studying the properties of $\R$, we show that
the average cost of our new algorithm is estimated by $\bigO(n^{2.5})$ for
i.i.d. Gaussian channel entries. Numerical simulations show that our proposed
branch-and-bound method is much more efficient than the existing one that gives
the optimal solution. Besides, compared with the suboptimal methods, our method
offers the best performance at a cost lower than that of the LLL method and
similar to that of the quadratic programming relaxation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4281</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4281</id><created>2014-10-15</created><updated>2015-05-10</updated><authors><author><keyname>Li</keyname><forenames>Xiangang</forenames></author><author><keyname>Wu</keyname><forenames>Xihong</forenames></author></authors><title>Constructing Long Short-Term Memory based Deep Recurrent Neural Networks
  for Large Vocabulary Speech Recognition</title><categories>cs.CL cs.NE</categories><comments>submitted to ICASSP 2015 which does not perform blind reviews</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long short-term memory (LSTM) based acoustic modeling methods have recently
been shown to give state-of-the-art performance on some speech recognition
tasks. To achieve a further performance improvement, in this research, deep
extensions on LSTM are investigated considering that deep hierarchical model
has turned out to be more efficient than a shallow one. Motivated by previous
research on constructing deep recurrent neural networks (RNNs), alternative
deep LSTM architectures are proposed and empirically evaluated on a large
vocabulary conversational telephone speech recognition task. Meanwhile,
regarding to multi-GPU devices, the training process for LSTM networks is
introduced and discussed. Experimental results demonstrate that the deep LSTM
networks benefit from the depth and yield the state-of-the-art performance on
this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4290</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4290</id><created>2014-10-16</created><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Multi-Gigabits Millimetre Wave Wireless Communications for 5G: From
  Fixed Access to Cellular Networks</title><categories>cs.NI</categories><comments>25 pages, 4 figures, 2 tables, to appear in IEEE Communications
  Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the formidable growth of various booming wireless communication services
that require ever-increasing data throughputs, the conventional microwave band
below 10 GHz, which is currently used by almost all mobile communication
systems, is going to reach its saturation point within just a few years.
Therefore, the attention of radio system designers has been pushed towards
ever-higher segments of the frequency spectrum in a quest for capacity
increase. In this article, we investigate the feasibility, advantages and
challenges of future wireless communications over the E-band frequencies. We
start from a brief review of the history of E-band spectrum and its light
licensing policy as well as benefits/challenges. Then we introduce the
propagation characteristics of E-band signals, based on which some potential
fixed and mobile applications at the E-band are investigated. In particular, we
analyze the achievability of non-trivial multiplexing gain in fixed
point-to-point E-band links and propose an E-band mobile broadband (EMB) system
as a candidate for the next generation mobile communication networks. The
channelization and frame structure of the EMB system are discussed in details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4296</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4296</id><created>2014-10-16</created><updated>2014-10-20</updated><authors><author><keyname>Gramoli</keyname><forenames>Vincent</forenames></author><author><keyname>Jourjon</keyname><forenames>Guillaume</forenames></author><author><keyname>Mehani</keyname><forenames>Olivier</forenames></author></authors><title>Can SDN Mitigate Disasters?</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Datacenter networks and services are at risk in the face of disasters.
Existing fault-tolerant storage services cannot even achieve a nil recovery
point objective (RPO) as client-generated data may get lost before the
termination of their migration across geo-replicated datacenters. SDN has
proved instrumental in exploiting application-level information to optimise the
routing of information. In this paper, we propose Software Defined Edge (SDE)
or the implementation of SDN at the network edge to achieve nil RPO. We
illustrate our proposal with a fault-tolerant key-value store that
experimentally recovers from disaster within 30s. Although SDE is inherently
fault-tolerant and scalable, its deployment raises new challenges on the
partnership between ISPs and CDN providers. We conclude that failure detection
information at the SDN-level can effectively benefit applications to recover
from disaster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4303</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4303</id><created>2014-10-16</created><authors><author><keyname>Ellouze</keyname><forenames>Nourhene</forenames><affiliation>Communication Networks and Security Research Lab, University of Carthage, Tunisia</affiliation></author><author><keyname>Rekhis</keyname><forenames>Slim</forenames><affiliation>Communication Networks and Security Research Lab, University of Carthage, Tunisia</affiliation></author><author><keyname>Allouche</keyname><forenames>Mohamed</forenames><affiliation>Department of Forensic Medicine of Charles Nicolle, University of Tunis El Manar, Tunisia</affiliation></author><author><keyname>Boudriga</keyname><forenames>Noureddine</forenames><affiliation>Communication Networks and Security Research Lab, University of Carthage, Tunisia</affiliation></author></authors><title>Digital Investigation of Security Attacks on Cardiac Implantable Medical
  Devices</title><categories>cs.CR cs.LO</categories><comments>In Proceedings AIDP 2014, arXiv:1410.3226</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 165, 2014, pp. 15-30</journal-ref><doi>10.4204/EPTCS.165.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Cardiac Implantable Medical device (IMD) is a device, which is surgically
implanted into a patient's body, and wirelessly configured using an external
programmer by prescribing physicians and doctors. A set of lethal attacks
targeting these devices can be conducted due to the use of vulnerable wireless
communication and security protocols, and the lack of security protection
mechanisms deployed on IMDs. In this paper, we propose a system for postmortem
analysis of lethal attack scenarios targeting cardiac IMDs. Such a system
reconciles in the same framework conclusions derived by technical investigators
and deductions generated by pathologists. An inference system integrating a
library of medical rules is used to automatically infer potential medical
scenarios that could have led to the death of a patient. A Model Checking based
formal technique allowing the reconstruction of potential technical attack
scenarios on the IMD, starting from the collected evidence, is also proposed. A
correlation between the results obtained by the two techniques allows to prove
whether a potential attack scenario is the source of the patient's death.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4304</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4304</id><created>2014-10-16</created><authors><author><keyname>Chapman</keyname><forenames>Chris</forenames><affiliation>Royal Military College of Canada, Electrical and Computer Engineering Department, Kingston, Canada</affiliation></author><author><keyname>Knight</keyname><forenames>Scott</forenames><affiliation>Royal Military College of Canada, Electrical and Computer Engineering Department, Kingston, Canada</affiliation></author><author><keyname>Dean</keyname><forenames>Tom</forenames><affiliation>Queen's University, Electrical and Computer Engineering Department, Kingston, Canada</affiliation></author></authors><title>USBcat - Towards an Intrusion Surveillance Toolset</title><categories>cs.CR</categories><comments>In Proceedings AIDP 2014, arXiv:1410.3226</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 165, 2014, pp. 31-43</journal-ref><doi>10.4204/EPTCS.165.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper identifies an intrusion surveillance framework which provides an
analyst with the ability to investigate and monitor cyber-attacks in a covert
manner. Where cyber-attacks are perpetrated for the purposes of espionage the
ability to understand an adversary's techniques and objectives are an important
element in network and computer security. With the appropriate toolset,
security investigators would be permitted to perform both live and stealthy
counter-intelligence operations by observing the behaviour and communications
of the intruder. Subsequently a more complete picture of the attacker's
identity, objectives, capabilities, and infiltration could be formulated than
is possible with present technologies. This research focused on developing an
extensible framework to permit the covert investigation of malware.
Additionally, a Universal Serial Bus (USB) Mass Storage Device (MSD) based
covert channel was designed to enable remote command and control of the
framework. The work was validated through the design, implementation and
testing of a toolset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4305</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4305</id><created>2014-10-16</created><authors><author><keyname>Idrees</keyname><forenames>Muhammad Sabir</forenames><affiliation>Institut Mines-T&#xe9;l&#xe9;com, T&#xe9;l&#xe9;com Bretagne, France</affiliation></author><author><keyname>Roudier</keyname><forenames>Yves</forenames><affiliation>Institut EURECOM, France</affiliation></author><author><keyname>Apvrille</keyname><forenames>Ludovic</forenames><affiliation>Institut Mines-T&#xe9;l&#xe9;com, T&#xe9;l&#xe9;com ParisTech, France</affiliation></author></authors><title>Model the System from Adversary Viewpoint: Threats Identification and
  Modeling</title><categories>cs.CR cs.SE cs.SY</categories><comments>In Proceedings AIDP 2014, arXiv:1410.3226</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 165, 2014, pp. 45-58</journal-ref><doi>10.4204/EPTCS.165.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security attacks are hard to understand, often expressed with unfriendly and
limited details, making it difficult for security experts and for security
analysts to create intelligible security specifications. For instance, to
explain Why (attack objective), What (i.e., system assets, goals, etc.), and
How (attack method), adversary achieved his attack goals. We introduce in this
paper a security attack meta-model for our SysML-Sec framework, developed to
improve the threat identification and modeling through the explicit
representation of security concerns with knowledge representation techniques.
Our proposed meta-model enables the specification of these concerns through
ontological concepts which define the semantics of the security artifacts and
introduced using SysML-Sec diagrams. This meta-model also enables representing
the relationships that tie several such concepts together. This representation
is then used for reasoning about the knowledge introduced by system designers
as well as security experts through the graphical environment of the SysML-Sec
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4307</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4307</id><created>2014-10-16</created><updated>2015-09-02</updated><authors><author><keyname>Lalitha</keyname><forenames>Anusha</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand</forenames></author></authors><title>Social Learning and Distributed Hypothesis Testing</title><categories>math.ST cs.IT math.IT math.OC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a problem of distributed hypothesis testing and social
learning. Individual nodes in a network receive noisy local (private)
observations whose distribution is parameterized by a discrete parameter
(hypotheses). The conditional distributions are known locally at the nodes, but
the true parameter/hypothesis is not known. An update rule is analyzed in which
nodes first perform a Bayesian update of their belief (distribution estimate)
of the parameter based on their local observation, communicate these updates to
their neighbors, and then perform a &quot;non-Bayesian&quot; linear consensus using the
log-beliefs of their neighbors. In this paper we show that under mild
assumptions, the belief of any node in any incorrect hypothesis converges to
zero exponentially fast, and we characterize the exponential rate of learning
which is given in terms of the network structure and the divergences between
the observations' distributions. Our main result is the concentration property
established on the rate of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4312</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4312</id><created>2014-10-16</created><updated>2015-04-10</updated><authors><author><keyname>Yi</keyname><forenames>Wentan</forenames></author><author><keyname>Chen</keyname><forenames>Shaozhen</forenames></author></authors><title>Zero-Correlation Linear Cryptanalysis of Reduced-round MISTY1</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.6100</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MISTY1 algorithm, proposed by Matsui in FSE 1997, is a block cipher with
a 64-bit block size and a 128-bit key size. It was recommended by the European
NESSIE project and the CRYPTREC project, and became one RFC in 2002 and an ISO
standard in 2005, respectively. In this paper, we ?rst investigate the
properties of the FL linear function and identify 232 subkey- dependent
zero-correlation linear approximations over 5-round MISTY1 with 3 FL layers.
Fur- thermore, some observations on the FL, FO and FI functions are founded and
based upon those observations, we select 27 subkey-dependent zero-correlation
linear approximations and then, pro- pose the zero-correlation linear attacks
on 7-round MISTY1 with 4 FL layers. Besides, for the case without FL layers, 27
zero-correlation linear approximations over 5-round MISTY1 are employed to the
analysis of 7-round MISTY1. The zero-correlation linear attack on the 7-round
with 4 FL layers needs about 2^{119:5} encryptions with 2^{62.9} known
plaintexts and 2^61 memory bytes. For the attack on 7-round without FL layers,
the data complexity is about 2^{63.9} known plaintexts, the time complexity is
about 2^{81} encryptions and the memory requirements are about 2^{93} bytes.
Both have lower time complexity than previous attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4319</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4319</id><created>2014-10-16</created><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Achieving High Resolution for Super-resolution via Reweighted Atomic
  Norm Minimization</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, submitted to ICASSP 2015, Brisbane, Australia,
  April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The super-resolution theory developed recently by Cand\`{e}s and
Fernandes-Granda aims to recover fine details of a sparse frequency spectrum
from coarse scale information only. The theory was then extended to the cases
with compressive samples and/or multiple measurement vectors. However, the
existing atomic norm (or total variation norm) techniques succeed only if the
frequencies are sufficiently separated, prohibiting commonly known high
resolution. In this paper, a reweighted atomic-norm minimization (RAM) approach
is proposed which iteratively carries out atomic norm minimization (ANM) with a
sound reweighting strategy that enhances sparsity and resolution. It is
demonstrated analytically and via numerical simulations that the proposed
method achieves high resolution with application to DOA estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4330</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4330</id><created>2014-10-16</created><authors><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Ultra-Reliable Communication in 5G Wireless Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>To be presented at the 1st International Conference on 5G for
  Ubiquitous Connectivity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless 5G systems will not only be &quot;4G, but faster&quot;. One of the novel
features discussed in relation to 5G is Ultra-Reliable Communication (URC), an
operation mode not present in today's wireless systems. URC refers to provision
of certain level of communication service almost 100 % of the time. Example URC
applications include reliable cloud connectivity, critical connections for
industrial automation and reliable wireless coordination among vehicles. This
paper puts forward a systematic view on URC in 5G wireless systems. It starts
by analyzing the fundamental mechanisms that constitute a wireless connection
and concludes that one of the key steps towards enabling URC is revision of the
methods for encoding control information (metadata) and data. It introduces the
key concept of Reliable Service Composition, where a service is designed to
adapt its requirements to the level of reliability that can be attained. The
problem of URC is analyzed across two different dimensions. The first dimension
is the type of URC problem that is defined based on the time frame used to
measure the reliability of the packet transmission. Two types of URC problems
are identified: long-term URC (URC-L) and short-term URC (URC-S). The second
dimension is represented by the type of reliability impairment that can affect
the communication reliability in a given scenario. The main objective of this
paper is to create the context for defining and solving the new engineering
problems posed by URC in 5G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4341</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4341</id><created>2014-10-16</created><authors><author><keyname>Venkatesh</keyname><forenames>Manasij</forenames></author><author><keyname>Majjagi</keyname><forenames>Vikas</forenames></author><author><keyname>Vijayasenan</keyname><forenames>Deepu</forenames></author></authors><title>Implicit segmentation of Kannada characters in offline handwriting
  recognition using hidden Markov models</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for classification of handwritten Kannada characters
using Hidden Markov Models (HMMs). Kannada script is agglutinative, where
simple shapes are concatenated horizontally to form a character. This results
in a large number of characters making the task of classification difficult.
Character segmentation plays a significant role in reducing the number of
classes. Explicit segmentation techniques suffer when overlapping shapes are
present, which is common in the case of handwritten text. We use HMMs to take
advantage of the agglutinative nature of Kannada script, which allows us to
perform implicit segmentation of characters along with recognition. All the
experiments are performed on the Chars74k dataset that consists of 657
handwritten characters collected across multiple users. Gradient-based features
are extracted from individual characters and are used to train character HMMs.
The use of implicit segmentation technique at the character level resulted in
an improvement of around 10%. This system also outperformed an existing system
tested on the same dataset by around 16%. Analysis based on learning curves
showed that increasing the training data could result in better accuracy.
Accordingly, we collected additional data and obtained an improvement of 4%
with 6 additional samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4343</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4343</id><created>2014-10-16</created><updated>2014-10-23</updated><authors><author><keyname>Trivedi</keyname><forenames>Anupam</forenames></author><author><keyname>Pal</keyname><forenames>Kunal</forenames></author><author><keyname>Saha</keyname><forenames>Chiranjib</forenames></author><author><keyname>Srinivasan</keyname><forenames>Dipti</forenames></author></authors><title>Enhanced Multiobjective Evolutionary Algorithm based on Decomposition
  for Solving the Unit Commitment Problem</title><categories>cs.NE</categories><comments>This paper has been withdrawn by the author due to 1. inconsistency
  of vector representation in equation (12) and 2. algorithm 3 has not been
  included in this text which makes the description incomplete</comments><report-no>IEEE Transactions on Industrial Informatics - Manuscript ID
  TII-14-0921</report-no><msc-class>68T04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The unit commitment (UC) problem is a nonlinear, high-dimensional, highly
constrained, mixed-integer power system optimization problem and is generally
solved in the literature considering minimizing the system operation cost as
the only objective. However, due to increasing environmental concerns, the
recent attention has shifted to incorporating emission in the problem
formulation. In this paper, a multi-objective evolutionary algorithm based on
decomposition (MOEA/D) is proposed to solve the UC problem as a multi-objective
optimization problem considering minimizing cost and emission as the multiple
objec- tives. Since, UC problem is a mixed-integer optimization problem
consisting of binary UC variables and continuous power dispatch variables, a
novel hybridization strategy is proposed within the framework of MOEA/D such
that genetic algorithm (GA) evolves the binary variables while differential
evolution (DE) evolves the continuous variables. Further, a novel non-uniform
weight vector distribution strategy is proposed and a parallel island model
based on combination of MOEA/D with uniform and non-uniform weight vector
distribution strategy is implemented to enhance the performance of the
presented algorithm. Extensive case studies are presented on different test
systems and the effectiveness of the proposed hybridization strategy, the
non-uniform weight vector distribution strategy and parallel island model is
verified through stringent simulated results. Further, exhaustive benchmarking
against the algorithms proposed in the literature is presented to demonstrate
the superiority of the proposed algorithm in obtaining significantly better
converged and uniformly distributed trade-off solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4345</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4345</id><created>2014-10-16</created><updated>2014-12-03</updated><authors><author><keyname>Akeret</keyname><forenames>Joel</forenames><affiliation>ETH Zurich</affiliation></author><author><keyname>Gamper</keyname><forenames>Lukas</forenames><affiliation>ETH Zurich</affiliation></author><author><keyname>Amara</keyname><forenames>Adam</forenames><affiliation>ETH Zurich</affiliation></author><author><keyname>Refregier</keyname><forenames>Alexandre</forenames><affiliation>ETH Zurich</affiliation></author></authors><title>HOPE: A Python Just-In-Time compiler for astrophysical computations</title><categories>astro-ph.IM cs.MS cs.PL physics.comp-ph</categories><comments>Accepted for publication in Astronomy and Computing. 14 pages, 1
  figure. The code is available at http://hope.phys.ethz.ch</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Python programming language is becoming increasingly popular for
scientific applications due to its simplicity, versatility, and the broad range
of its libraries. A drawback of this dynamic language, however, is its low
runtime performance which limits its applicability for large simulations and
for the analysis of large data sets, as is common in astrophysics and
cosmology. While various frameworks have been developed to address this
limitation, most focus on covering the complete language set, and either force
the user to alter the code or are not able to reach the full speed of an
optimised native compiled language. In order to combine the ease of Python and
the speed of C++, we developed HOPE, a specialised Python just-in-time (JIT)
compiler designed for numerical astrophysical applications. HOPE focuses on a
subset of the language and is able to translate Python code into C++ while
performing numerical optimisation on mathematical expressions at runtime. To
enable the JIT compilation, the user only needs to add a decorator to the
function definition. We assess the performance of HOPE by performing a series
of benchmarks and compare its execution speed with that of plain Python, C++
and the other existing frameworks. We find that HOPE improves the performance
compared to plain Python by a factor of 2 to 120, achieves speeds comparable to
that of C++, and often exceeds the speed of the existing solutions. We discuss
the differences between HOPE and the other frameworks, as well as future
extensions of its capabilities. The fully documented HOPE package is available
at http://hope.phys.ethz.ch and is published under the GPLv3 license on PyPI
and GitHub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4353</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4353</id><created>2014-10-16</created><updated>2015-10-19</updated><authors><author><keyname>Escardo</keyname><forenames>Martin</forenames></author><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author></authors><title>The Herbrand Functional Interpretation of the Double Negation Shift</title><categories>cs.LO math.LO</categories><comments>18 pages</comments><msc-class>03E25, 03F10, 03F25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a generalisation of selection functions over an
arbitrary strong monad $T$, as functionals of type $J^T_R X = (X \to R) \to T
X$. It is assumed throughout that $R$ is a $T$-algebra. We show that $J^T_R$ is
also a strong monad, and that it embeds into the continuation monad $K_R X = (X
\to R) \to R$. We use this to derive that the explicitly controlled product of
$T$-selection functions is definable from the explicitly controlled product of
quantifiers, and hence from Spector's bar recursion. We then prove several
properties of this product in the special case when $T$ is the finite power set
monad ${\mathcal P}(\cdot)$. These are used to show that when $T X = {\mathcal
P}(X)$ the explicitly controlled product of $T$-selection functions calculates
a witness to the Herbrand functional interpretation of the double negation
shift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4355</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4355</id><created>2014-10-16</created><updated>2015-04-20</updated><authors><author><keyname>Bridges</keyname><forenames>Robert A.</forenames></author><author><keyname>Collins</keyname><forenames>John</forenames></author><author><keyname>Ferragut</keyname><forenames>Erik M.</forenames></author><author><keyname>Laska</keyname><forenames>Jason</forenames></author><author><keyname>Sullivan</keyname><forenames>Blair D.</forenames></author></authors><title>Multi-Level Anomaly Detection on Time-Varying Graph Data</title><categories>cs.SI cs.LG stat.ML</categories><comments>8 pages. Updated paper to address reviewer comments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a novel modeling and analysis framework for graph
sequences which addresses the challenge of detecting and contextualizing
anomalies in labelled, streaming graph data. We introduce a generalization of
the BTER model of Seshadhri et al. by adding flexibility to community
structure, and use this model to perform multi-scale graph anomaly detection.
Specifically, probability models describing coarse subgraphs are built by
aggregating probabilities at finer levels, and these closely related
hierarchical models simultaneously detect deviations from expectation. This
technique provides insight into a graph's structure and internal context that
may shed light on a detected event. Additionally, this multi-scale analysis
facilitates intuitive visualizations by allowing users to narrow focus from an
anomalous graph to particular subgraphs or nodes causing the anomaly.
  For evaluation, two hierarchical anomaly detectors are tested against a
baseline Gaussian method on a series of sampled graphs. We demonstrate that our
graph statistics-based approach outperforms both a distribution-based detector
and the baseline in a labeled setting with community structure, and it
accurately detects anomalies in synthetic and real-world datasets at the node,
subgraph, and graph levels. To illustrate the accessibility of information made
possible via this technique, the anomaly detector and an associated interactive
visualization tool are tested on NCAA football data, where teams and
conferences that moved within the league are identified with perfect recall,
and precision greater than 0.786.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4360</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4360</id><created>2014-10-16</created><authors><author><keyname>Song</keyname><forenames>Changick</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Park</keyname><forenames>Jaehyun</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>MIMO Broadcasting for Simultaneous Wireless Information and Power
  Transfer: Weighted MMSE Approaches</title><categories>cs.IT math.IT</categories><comments>6 pages, two-columned, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider simultaneous wireless information and power transfer (SWIPT) in
MIMO Broadcast networks where one energy harvesting (EH) user and one
information decoding (ID) user share the same time and frequency resource. In
contrast to previous SWIPT systems based on the information rate, this paper
addresses the problem in terms of the weighted minimum mean squared error
(WMMSE) criterion. First, we formulate the WMMSE-SWIPT problem which minimizes
the weighted sum-MSE of the message signal arrived at the ID user, while
satisfying the requirement on the energy that can be harvested from the signal
at the EH user. Then, we propose the optimal precoder structure of the problem
and identify the best possible MSE-energy tradeoff region through the
alternative update of the linear precoder at the transmitter with the linear
receiver at the ID user. From the derived solution, several interesting
observations are made compared to the conventional SWIPT designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4364</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4364</id><created>2014-10-16</created><authors><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author></authors><title>Unifying Functional Interpretations: Past and Future</title><categories>math.LO cs.LO</categories><comments>18 pages</comments><msc-class>03B47, 03F25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article surveys work done in the last six years on the unification of
various functional interpretations including G\&quot;odel's dialectica
interpretation, its Diller-Nahm variant, Kreisel modified realizability,
Stein's family of functional interpretations, functional interpretations &quot;with
truth&quot;, and bounded functional interpretations. Our goal in the present paper
is twofold: (1) to look back and single out the main lessons learnt so far, and
(2) to look forward and list several open questions and possible directions for
further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4370</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4370</id><created>2014-10-16</created><authors><author><keyname>Birkl</keyname><forenames>Christoph R.</forenames></author><author><keyname>Frost</keyname><forenames>Damien F.</forenames></author><author><keyname>Bizeray</keyname><forenames>Adrien M.</forenames></author><author><keyname>Richardson</keyname><forenames>Robert R.</forenames></author><author><keyname>Howey</keyname><forenames>David A.</forenames></author></authors><title>Modular converter system for low-cost off-grid energy storage using
  second life Li-ion batteries</title><categories>cs.SY</categories><comments>Presented at IEEE GHTC Oct 10-14, 2014, Silicon Valley</comments><doi>10.1109/GHTC.2014.6970281</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lithium ion batteries are promising for small off- grid energy storage
applications in developing countries because of their high energy density and
long life. However, costs are prohibitive. Instead, we consider 'used' Li-ion
batteries for this application, finding experimentally that many discarded
laptop cells, for example, still have good capacity and cycle life. In order to
make safe and optimal use of such cells, we present a modular power management
system using a separate power converter for every cell. This novel approach
allows individual batteries to be used to their full capacity. The power
converters operate in voltage droop control mode to provide easy charge
balancing and implement a battery management system to estimate the capacity of
each cell, as we demonstrate experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4373</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4373</id><created>2014-10-16</created><authors><author><keyname>Barjon</keyname><forenames>Matthieu</forenames></author><author><keyname>Casteigts</keyname><forenames>Arnaud</forenames></author><author><keyname>Chaumette</keyname><forenames>Serge</forenames></author><author><keyname>Johnen</keyname><forenames>Colette</forenames></author><author><keyname>Neggaz</keyname><forenames>Yessin M.</forenames></author></authors><title>Maintaining a Spanning Forest in Highly Dynamic Networks: The
  Synchronous Case</title><categories>cs.DC</categories><comments>A short version appeared in OPODIS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly dynamic networks are characterized by frequent changes in the
availability of communication links. Many of these networks are in general
partitioned into several components that keep splitting and merging
continuously and unpredictably. We present an algorithm that strives to
maintain a forest of spanning trees in such networks, without any kind of
assumption on the rate of changes. Our algorithm is the adaptation of a
coarse-grain interaction algorithm (Casteigts et al., 2013) to the synchronous
message passing model (for dynamic networks). While the high-level principles
of the coarse-grain variant are preserved, the new algorithm turns out to be
significantly more complex. In particular, it involves a new technique that
consists of maintaining a distributed permutation of the set of all nodes IDs
throughout the execution. The algorithm also inherits the properties of its
original variant: It relies on purely localized decisions, for which no global
information is ever collected at the nodes, and yet it maintains a number of
critical properties whatever the frequency and scale of the changes. In
particular, the network remains always covered by a spanning forest in which 1)
no cycle can ever appear, 2) every node belongs to a tree, and 3) after an
arbitrary number of edge disappearance, all maximal subtrees immediately
restore exactly one token (at their root). These properties are ensured
whatever the dynamics, even if it keeps going for an arbitrary long period of
time. Optimality is not the focus here, however the number of tree per
components -- the metric of interest here -- eventually converges to one if the
network stops changing (which is never expected to happen, though). The
algorithm correctness is proven and its behavior is tested through
experimentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4375</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4375</id><created>2014-10-16</created><authors><author><keyname>Jacob</keyname><forenames>S. M.</forenames></author><author><keyname>Issac</keyname><forenames>B.</forenames></author></authors><title>The mobile devices and its mobile learning usage analysis</title><categories>cs.CY</categories><comments>6 pages, appears as Jacob, S. M. &amp; Issac, B. (2008, March). Mobile
  Devices and its Mobile Learning Usage Analysis, ICICWS, Lecture Notes in
  Engineering and Computer Science, pp.782-787</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The usage of mobile devices for mobile learning is becoming increasingly
popular. There is a new brand of students in the universities now-a-days who
are easily connected to technology and innovative mobile devices. We attempt to
do an analysis on a survey done with university students on mobile device usage
for mobile learning purposes. This is to find the learning trends within the
student community so that some of these popular practices could be encouraged
to enhance learning among the student community. Both the quantitative and
qualitative approaches are adopted in the analysis. The results are discussed
and conclusions drawn in the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4377</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4377</id><created>2014-10-16</created><authors><author><keyname>Issac</keyname><forenames>B.</forenames></author><author><keyname>Hamid</keyname><forenames>K.</forenames></author><author><keyname>Tan</keyname><forenames>C. E.</forenames></author></authors><title>Hybrid Mobility Prediction of 802.11 Infrastructure Nodes by Location
  Tracking and Data Mining</title><categories>cs.NI</categories><comments>16 pages, appears in Journal of Information Technology in Asia
  (JITA), 3(1). arXiv admin note: text overlap with arXiv:1410.4180</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an IEEE 802.11 Infrastructure network, as the mobile node is moving from
one access point to another, the resource allocation and smooth hand off may be
a problem. If some reliable prediction is done on mobile nodes next move, then
resources can be allocated optimally as the mobile node moves around. This
would increase the performance throughput of wireless network. We plan to
investigate on a hybrid mobility prediction scheme that uses location tracking
and data mining to predict the future path of the mobile node. We also propose
a secure version of the same scheme. Through simulation and analysis, we
present the prediction accuracy of our proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4379</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4379</id><created>2014-10-16</created><authors><author><keyname>Jacob</keyname><forenames>S. M.</forenames></author><author><keyname>Issac</keyname><forenames>B.</forenames></author></authors><title>Mobile Learning Culture and Effects in Higher Education</title><categories>cs.CY</categories><comments>3 pages</comments><journal-ref>Jacob, S. M. &amp; Issac, B. (2007). Mobile Learning Culture and
  Effects in Higher Education, IEEE Multidisciplinary Engineering Education
  Magazine (MEEM), ISSN 1558-7908, EdSocSAC, IEEE Education Society, 2(2),
  19-21</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile learning through wireless enabled laptops (say, within a university
campus) can make use of the learning management system that is already
available through internet or intranet. Without restrictions within the four
walls of computer labs or library, students can now access the learning
resources anywhere in the campus where wireless access points or hotspots are
located. We briefly investigate on the mobile learning benefits and eventually
an analysis of the student perceptions on mobile learning is presented through
a survey, to validate the m-learning benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4381</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4381</id><created>2014-10-16</created><authors><author><keyname>Gajanovic</keyname><forenames>Borislav</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>ALICE: An Advanced Logic for Interactive Component Engineering</title><categories>cs.SE cs.LO</categories><comments>15 pages, 2 figures, 3 tables, Bernhard Beckert (Ed.) 4th
  International Verification Workshop (Verify'07). Proceedings, Bremen, July
  2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an overview of the verification framework ALICE in its
current version 0.7. It is based on the generic theorem prover Isabelle
[Pau03a]. Within ALICE a software or hardware component is specified as a
state-full black-box with directed communication channels. Components send and
receive asynchronous messages via these channels. The behavior of a component
is generally described as a relation on the observations in form of streams of
messages flowing over its input and output channels. Untimed and timed as well
as state-based, recursive, relational, equational, assumption/guarantee, and
functional styles of specification are supported. Hence, ALICE is well suited
for the formalization and verification of distributed systems modeled with this
stream-processing paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4391</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4391</id><created>2014-10-16</created><updated>2015-09-03</updated><authors><author><keyname>Bedo</keyname><forenames>Justin</forenames></author><author><keyname>Ong</keyname><forenames>Cheng Soon</forenames></author></authors><title>Multivariate Spearman's rho for aggregating ranks using copulas</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the problem of rank aggregation: given a set of ranked lists, we
want to form a consensus ranking. Furthermore, we consider the case of extreme
lists: i.e., only the rank of the best or worst elements are known. We impute
missing ranks by the average value and generalise Spearman's \rho to extreme
ranks. Our main contribution is the derivation of a non-parametric estimator
for rank aggregation based on multivariate extensions of Spearman's \rho, which
measures correlation between a set of ranked lists. Multivariate Spearman's
\rho is defined using copulas, and we show that the geometric mean of
normalised ranks maximises multivariate correlation. Motivated by this, we
propose a weighted geometric mean approach for learning to rank which has a
closed form least squares solution. When only the best or worst elements of a
ranked list are known, we impute the missing ranks by the average value,
allowing us to apply Spearman's \rho. Finally, we demonstrate good performance
on the rank aggregation benchmarks MQ2007 and MQ2008.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4393</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4393</id><created>2014-10-16</created><updated>2014-11-11</updated><authors><author><keyname>Herbon</keyname><forenames>Christopher</forenames></author></authors><title>The HAWKwood Database</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a database consisting of wood pile images, which can be used as a
benchmark to evaluate the performance of wood pile detection and surveying
algorithms. We distinguish six database cate- gories which can be used for
different types of algorithms. Images of real and synthetic scenes are
provided, which consist of 7655 images divided into 354 data sets. Depending on
the category the data sets either include ground truth data or forestry
specific measurements with which algorithms may be compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4395</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4395</id><created>2014-10-16</created><authors><author><keyname>Eikel</keyname><forenames>Martina</forenames></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author><author><keyname>Setzer</keyname><forenames>Alexander</forenames></author></authors><title>Minimum Linear Arrangement of Series-Parallel Graphs</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a factor $14D^2$ approximation algorithm for the minimum linear
arrangement problem on series-parallel graphs, where $D$ is the maximum degree
in the graph. Given a suitable decomposition of the graph, our algorithm runs
in time $O(|E|)$ and is very easy to implement. Its divide-and-conquer approach
allows for an effective parallelization. Note that a suitable decomposition can
also be computed in time $O(|E|\log{|E|})$ (or even $O(\log{|E|}\log^*{|E|})$
on an EREW PRAM using $O(|E|)$ processors).
  For the proof of the approximation ratio, we use a sophisticated charging
method that uses techniques similar to amortized analysis in advanced data
structures.
  On general graphs, the minimum linear arrangement problem is known to be
NP-hard. To the best of our knowledge, the minimum linear arrangement problem
on series-parallel graphs has not been studied before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4398</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4398</id><created>2014-10-16</created><authors><author><keyname>Issac</keyname><forenames>B.</forenames></author></authors><title>Secure ARP and Secure DHCP Protocols to Mitigate Security Attacks</title><categories>cs.CR</categories><comments>12 pages. available in Issac, B. (2009). Secure ARP and Secure DHCP
  Protocols to Mitigate Security Attacks, International Journal of Network
  Security (IJNS), ISSN 1816-353X (print), ISSN 1816-3548 (online), 8(1),
  102-113</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For network computers to communicate to one another, they need to know one
another's IP address and MAC address. Address Resolution Protocol (ARP) is
developed to find the Ethernet address that map to a specific IP address. The
source computer broadcasts the request for Ethernet address and eventually the
target computer replies. The IP to Ethernet address mapping would later be
stored in an ARP Cache for some time duration, after which the process is
repeated. Since ARP is susceptible to ARP poisoning attacks, we propose to make
it unicast, centralized and secure, along with a secure design of DHCP protocol
to mitigate MAC spoofing. The secure protocol designs are explained in detail.
Lastly we also discuss some performance issues to show how the proposed
protocols work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4399</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4399</id><created>2014-10-16</created><authors><author><keyname>Vanderhoydonc</keyname><forenames>Ynte</forenames></author><author><keyname>Vanroose</keyname><forenames>Wim</forenames></author></authors><title>Constrained Runs algorithm as a lifting operator for the Boltzmann
  equation</title><categories>cs.CE</categories><comments>submitted to SIAM MMS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lifting operators play an important role in starting a kinetic Boltzmann
model from given macroscopic information. The macroscopic variables need to be
mapped to the distribution functions, mesoscopic variables of the Boltzmann
model. A well-known numerical method for the initialization of Boltzmann models
is the Constrained Runs algorithm. This algorithm is used in literature for the
initialization of lattice Boltzmann models, special discretizations of the
Boltzmann equation. It is based on the attraction of the dynamics toward the
slow manifold and uses lattice Boltzmann steps to converge to the desired
dynamics on the slow manifold. We focus on applying the Constrained Runs
algorithm to map density, average flow velocity, and temperature, the
macroscopic variables, to distribution functions. Furthermore, we do not
consider only lattice Boltzmann models. We want to perform the algorithm for
different discretizations of the Boltzmann equation and consider a standard
finite volume discretization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4407</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4407</id><created>2014-10-16</created><updated>2014-10-26</updated><authors><author><keyname>Zhang</keyname><forenames>Tong</forenames></author><author><keyname>Xu</keyname><forenames>Xiaodong</forenames></author><author><keyname>Tao</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Cui</keyname><forenames>Qimei</forenames></author></authors><title>Optimal Allocation of Power Pool for Two-Cell Single-User Joint
  Transmission</title><categories>cs.IT math.IT</categories><comments>This paper has not finished</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint transmission is a kind of cooperative transmission technology, which
converts other cell interference into signal. In this letter, for single-user
joint transmission, we propose that a power pool can be created in order to
increase the system sum-rate or lower the power consumption. Then, we
investigate the sum-rate maximization problem under this framework. Since it is
impossible that subchannel gains for the two base stations are identical, we
can obtain a much more concise structure and reduce variables to the half of
original problem. And that the closed-form water-filling solution is unique and
global optimum, because it is a convex optimization problem. Compared to the
power allocation without pooling, the performance gain is verified by
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4408</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4408</id><created>2014-10-16</created><authors><author><keyname>Sukumar</keyname><forenames>Srikant</forenames></author><author><keyname>Akella</keyname><forenames>Maruthi R.</forenames></author></authors><title>Stabilizing Controllers for Multi-Input, Singular Control Gain Systems</title><categories>math.OC cs.SY</categories><comments>A version of this paper was presented at the IEEE Conference in
  Decision and Control, 2012 in Maui, Hawaii</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new methodology for design of a stabilizing control law
for multi-input linear systems with time-varying, singular gains on the
control. The results presented here assume the control gain to satisfy
persistence of excitation which is a necessary condition for existence of
stabilizing controllers in the presence of unstable drift. This work involves a
novel persistence filter construction and provides a significant extension to
the authors' previous result on stabilization of single-input linear systems
with time-varying singular gains. An application to underactuated spacecraft
stabilization is shown which illustrates the interesting features of the
time-varying control design in stabilization of nonlinear dynamical systems.
Finally, the development of an observer counterpart of these results is
presented in the presence of multiple-outputs subject to singular measurement
gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4410</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4410</id><created>2014-10-16</created><authors><author><keyname>Traversaro</keyname><forenames>Silvio</forenames></author><author><keyname>Del Prete</keyname><forenames>Andrea</forenames></author><author><keyname>Muradore</keyname><forenames>Riccardo</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author><author><keyname>Nori</keyname><forenames>Francesco</forenames></author></authors><title>Inertial Parameter Identification Including Friction and Motor Dynamics</title><categories>cs.RO</categories><comments>Pre-print of paper presented at Humanoid Robots, 13th IEEE-RAS
  International Conference on, Atlanta, Georgia, 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Identification of inertial parameters is fundamental for the implementation
of torque-based control in humanoids. At the same time, good models of friction
and actuator dynamics are critical for the low-level control of joint torques.
We propose a novel method to identify inertial, friction and motor parameters
in a single procedure. The identification exploits the measurements of the PWM
of the DC motors and a 6-axis force/torque sensor mounted inside the kinematic
chain. The partial least-square (PLS) method is used to perform the regression.
We identified the inertial, friction and motor parameters of the right arm of
the iCub humanoid robot. We verified that the identified model can accurately
predict the force/torque sensor measurements and the motor voltages. Moreover,
we compared the identified parameters against the CAD parameters, in the
prediction of the force/torque sensor measurements. Finally, we showed that the
estimated model can effectively detect external contacts, comparing it against
a tactile-based contact detection. The presented approach offers some
advantages with respect to other state-of-the-art methods, because of its
completeness (i.e. it identifies inertial, friction and motor parameters) and
simplicity (only one data collection, with no particular requirements).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4414</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4414</id><created>2014-10-16</created><authors><author><keyname>Del Prete</keyname><forenames>Andrea</forenames></author><author><keyname>Romano</keyname><forenames>Francesco</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author><author><keyname>Metta</keyname><forenames>Giorgio</forenames></author><author><keyname>Sandini</keyname><forenames>Giulio</forenames></author><author><keyname>Nori</keyname><forenames>Francesco</forenames></author></authors><title>Prioritized Optimal Control</title><categories>cs.RO</categories><comments>Pre-print of the paper presented at Robotics and Automation (ICRA),
  IEEE International Conference on, Hong Kong, China, 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a new technique to control highly redundant mechanical
systems, such as humanoid robots. We take inspiration from two approaches.
Prioritized control is a widespread multi-task technique in robotics and
animation: tasks have strict priorities and they are satisfied only as long as
they do not conflict with any higher-priority task. Optimal control instead
formulates an optimization problem whose solution is either a feedback control
policy or a feedforward trajectory of control inputs. We introduce strict
priorities in multi-task optimal control problems, as an alternative to
weighting task errors proportionally to their importance. This ensures the
respect of the specified priorities, while avoiding numerical conditioning
issues. We compared our approach with both prioritized control and optimal
control with tests on a simulated robot with 11 degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4416</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4416</id><created>2014-10-16</created><authors><author><keyname>Frielinghaus</keyname><forenames>Stefan Schulze</forenames></author><author><keyname>Petter</keyname><forenames>Michael</forenames></author><author><keyname>Seidl</keyname><forenames>Helmut</forenames></author></authors><title>Inter-procedural Two-Variable Herbrand Equalities</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that all valid Herbrand equalities can be inter-procedurally
inferred for programs where all assignments are taken into account whose
right-hand sides depend on at most one variable. The analysis is based on
procedure summaries representing the weakest pre-conditions for finitely many
generic post-conditions with template variables. In order to arrive at
effective representations for all occurring weakest pre-conditions, we show for
almost all values possibly computed at run-time, that they can be uniquely
factorized into tree patterns and a terminating ground term. Moreover, we
introduce an approximate notion of subsumption which is effectively decidable
and ensures that finite conjunctions of equalities may not grow infinitely.
Based on these technical results, we realize an effective fixpoint iteration to
infer all inter-procedurally valid Herbrand equalities for these programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4421</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4421</id><created>2014-10-16</created><updated>2015-05-17</updated><authors><author><keyname>Grammatico</keyname><forenames>Sergio</forenames></author><author><keyname>Parise</keyname><forenames>Francesca</forenames></author><author><keyname>Colombino</keyname><forenames>Marcello</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Decentralized Convergence to Nash Equilibria in Constrained
  Deterministic Mean Field Control</title><categories>cs.SY cs.GT math.OC</categories><comments>IEEE Trans. on Automatic Control (cond. accepted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers decentralized control and optimization methodologies for
large populations of systems, consisting of several agents with different
individual behaviors, constraints and interests, and affected by the aggregate
behavior of the overall population. For such large-scale systems, the theory of
aggregative and mean field games has been established and successfully applied
in various scientific disciplines. While the existing literature addresses the
case of unconstrained agents, we formulate deterministic mean field control
problems in the presence of heterogeneous convex constraints for the individual
agents, for instance arising from agents with linear dynamics subject to convex
state and control constraints. We propose several model-free feedback
iterations to compute in a decentralized fashion a mean field Nash equilibrium
in the limit of infinite population size. We apply our methods to the
constrained linear quadratic deterministic mean field control problem and to
the constrained mean field charging control problem for large populations of
plug-in electric vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4422</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4422</id><created>2014-10-11</created><authors><author><keyname>Sarma</keyname><forenames>Gopal</forenames></author></authors><title>Should we train scientific generalists?</title><categories>cs.CY physics.ed-ph</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I examine the topic of training scientific generalists. To focus the
discussion, I propose the creation of a new graduate program, analogous in
structure to existing MD/PhD programs, aimed at training a critical mass of
scientific researchers with substantial intellectual breadth. In addition to
completing the normal requirements for a PhD, students would undergo an
intense, several year training period designed to expose them to the core
vocabulary of multiple subjects at the graduate level. After providing some
historical and philosophical context for this proposal, I outline how such a
program could be implemented with little institutional overhead by existing
research universities. Finally, I discuss alternative possibilities for
training generalists by taking advantage of contemporary developments in online
learning and open science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4424</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4424</id><created>2014-10-16</created><authors><author><keyname>Lugo</keyname><forenames>Haydee</forenames></author><author><keyname>Miguel</keyname><forenames>Maxi San</forenames></author></authors><title>Learning and coordinating in a multilayer network</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a two layer network model for social coordination incorporating
two relevant ingredients: a) different networks of interaction to learn and to
obtain a payoff , and b) decision making processes based both on social and
strategic motivations. Two populations of agents are distributed in two layers
with intralayer learning processes and playing interlayer a coordination game.
We find that the skepticism about the wisdom of crowd and the local
connectivity are the driving forces to accomplish full coordination of the two
populations, while polarized coordinated layers are only possible for
all-to-all interactions. Local interactions also allow for full coordination in
the socially efficient Pareto-dominant strategy in spite of being the riskier
one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4426</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4426</id><created>2014-10-16</created><authors><author><keyname>Del Prete</keyname><forenames>Andrea</forenames></author><author><keyname>Mansard</keyname><forenames>Nicolas</forenames></author><author><keyname>Nori</keyname><forenames>Francesco</forenames></author><author><keyname>Metta</keyname><forenames>Giorgio</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author></authors><title>Partial Force Control of Constrained Floating-Base Robots</title><categories>cs.RO</categories><comments>Pre-print of paper presented at Intelligent Robots and Systems (IROS
  2014), IEEE International Conference on, Chicago, USA, 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Legged robots are typically in rigid contact with the environment at multiple
locations, which add a degree of complexity to their control. We present a
method to control the motion and a subset of the contact forces of a
floating-base robot. We derive a new formulation of the lexicographic
optimization problem typically arising in multitask motion/force control
frameworks. The structure of the constraints of the problem (i.e. the dynamics
of the robot) allows us to find a sparse analytical solution. This leads to an
equivalent optimization with reduced computational complexity, comparable to
inverse-dynamics based approaches. At the same time, our method preserves the
flexibility of optimization based control frameworks. Simulations were carried
out to achieve different multi-contact behaviors on a 23-degree-offreedom
humanoid robot, validating the presented approach. A comparison with another
state-of-the-art control technique with similar computational complexity shows
the benefits of our controller, which can eliminate force/torque
discontinuities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4428</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4428</id><created>2014-10-16</created><authors><author><keyname>Vanderhoydonc</keyname><forenames>Ynte</forenames></author><author><keyname>Vanroose</keyname><forenames>Wim</forenames></author></authors><title>Initialization of lattice Boltzmann models with the help of the
  numerical Chapman-Enskog expansion</title><categories>cs.CE</categories><comments>arXiv admin note: text overlap with arXiv:1108.4919</comments><journal-ref>Procedia Computer Science, vol 18, pp. 1036-1045 (2013)</journal-ref><doi>10.1016/j.procs.2013.05.269</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the applicability of the numerical Chapman-Enskog expansion as a
lifting operator for lattice Boltzmann models to map density and momentum to
distribution functions. In earlier work [Vanderhoydonc et al. Multiscale Model.
Simul. 10(3): 766-791, 2012] such an expansion was constructed in the context
of lifting only the zeroth order velocity moment, namely the density. A lifting
operator is necessary to convert information from the macroscopic to the
mesoscopic scale. This operator is used for the initialization of lattice
Boltzmann models. Given only density and momentum, the goal is to initialize
the distribution functions of lattice Boltzmann models. For this
initialization, the numerical Chapman-Enskog expansion is used in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4429</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4429</id><created>2014-10-16</created><authors><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author></authors><title>Weighted sampling of outer products</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note gives a simple analysis of the randomized approximation scheme for
matrix multiplication of Drineas et al (2006) with a particular sampling
distribution over outer products. The result follows from a matrix version of
Bernstein's inequality. To approximate the matrix product $AB^\top$ to spectral
norm error $\varepsilon\|A\|\|B\|$, it suffices to sample on the order of
$(\mathrm{sr}(A) \vee \mathrm{sr}(B)) \log(\mathrm{sr}(A) \wedge
\mathrm{sr}(B)) / \varepsilon^2$ outer products, where $\mathrm{sr}(M)$ is the
stable rank of a matrix $M$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4439</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4439</id><created>2014-10-16</created><authors><author><keyname>Dongol</keyname><forenames>Brijesh</forenames></author><author><keyname>Gomes</keyname><forenames>Victor B. F.</forenames></author><author><keyname>Struth</keyname><forenames>Georg</forenames></author></authors><title>Principles for Verification Tools: Separation Logic</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A principled approach to the design of program verification and con-
struction tools is applied to separation logic. The control flow is modelled by
power series with convolution as separating conjunction. A generic construction
lifts resource monoids to assertion and predicate transformer quantales. The
data flow is captured by concrete store/heap models. These are linked to the
separation algebra by soundness proofs. Verification conditions and
transformation laws are derived by equational reasoning within the predicate
transformer quantale. This separation of concerns makes an implementation in
the Isabelle/HOL proof as- sistant simple and highly automatic. The resulting
tool is correct by construction; it is explained on the classical linked list
reversal example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4441</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4441</id><created>2014-10-16</created><authors><author><keyname>Zarei</keyname><forenames>Ariyan</forenames></author></authors><title>Improve CAPTCHA's Security Using Gaussian Blur Filter</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing security for webservers against unwanted and automated
registrations has become a big concern. To prevent these kinds of false
registrations many websites use CAPTCHAs. Among all kinds of CAPTCHAs OCR-Based
or visual CAPTCHAs are very common. Actually visual CAPTCHA is an image
containing a sequence of characters. So far most of visual CAPTCHAs, in order
to resist against OCR programs, use some common implementations such as
wrapping the characters, random placement and rotations of characters, etc. In
this paper we applied Gaussian Blur filter, which is an image transformation,
to visual CAPTCHAs to reduce their readability by OCR programs. We concluded
that this technique made CAPTCHAs almost unreadable for OCR programs but, their
readability by human users still remained high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4444</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4444</id><created>2014-10-16</created><updated>2014-10-26</updated><authors><author><keyname>Zhang</keyname><forenames>Tong</forenames></author><author><keyname>Tao</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Xu</keyname><forenames>Xiaodong</forenames></author></authors><title>Power Allocation and Scheduling for Two-Cell Distributed Interference
  Subtraction System</title><categories>cs.IT cs.NI math.IT</categories><comments>This paper has not finished</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter studies the power allocation and scheduling problem in two-cell
distributed interference subtraction system. Through the interference is
cancelled at one receiver side and the analysis of objective function monotony,
our key result is the decoupling of power allocation and scheduling without
loss of optimality. Simulation exhibits the significant gains of our proposed
algorithm over open-loop power allocation with non-cooperation proportional
fair scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4445</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4445</id><created>2014-10-16</created><updated>2015-03-23</updated><authors><author><keyname>Stella</keyname><forenames>Massimo</forenames></author><author><keyname>Brede</keyname><forenames>Markus</forenames></author></authors><title>Patterns in the English Language: Phonological Networks, Percolation and
  Assembly Models</title><categories>cs.CL cond-mat.stat-mech</categories><comments>25 pages, 8 figures</comments><doi>10.1088/1742-5468/2015/05/P05006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a quantitative framework for the study of
phonological networks (PNs) for the English language by carrying out principled
comparisons to null models, either based on site percolation, randomization
techniques, or network growth models. In contrast to previous work, we mainly
focus on null models that reproduce lower order characteristics of the
empirical data. We find that artificial networks matching connectivity
properties of the English PN are exceedingly rare: this leads to the hypothesis
that the word repertoire might have been assembled over time by preferentially
introducing new words which are small modifications of old words. Our null
models are able to explain the &quot;power-law-like&quot; part of the degree
distributions and generally retrieve qualitative features of the PN such as
high clustering, high assortativity coefficient, and small-world
characteristics. However, the detailed comparison to expectations from null
models also points out significant differences, suggesting the presence of
additional constraints in word assembly. Key constraints we identify are the
avoidance of large degrees, the avoidance of triadic closure, and the avoidance
of large non-percolating clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4448</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4448</id><created>2014-10-15</created><updated>2015-01-01</updated><authors><author><keyname>Abdulla</keyname><forenames>Parosh Aziz</forenames></author><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author><author><keyname>Mayr</keyname><forenames>Richard</forenames></author><author><keyname>Sandberg</keyname><forenames>Sven</forenames></author></authors><title>Stochastic Parity Games on Lossy Channel Systems</title><categories>cs.LO</categories><comments>QEST'13 special issue, to appear in Logical Methods in Computer
  Science (LMCS-2014-964). arXiv admin note: substantial text overlap with
  arXiv:1305.5228</comments><proxy>Logical Methods In Computer Science</proxy><report-no>LMCS-2014-964</report-no><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (January
  5, 2015) lmcs:944</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm for solving stochastic parity games with almost-sure
winning conditions on {\it lossy channel systems}, under the constraint that
both players are restricted to finite-memory strategies. First, we describe a
general framework, where we consider the class of 2 1/2-player games with
almost-sure parity winning conditions on possibly infinite game graphs,
assuming that the game contains a {\it finite attractor}. An attractor is a set
of states (not necessarily absorbing) that is almost surely re-visited
regardless of the players' decisions. We present a scheme that characterizes
the set of winning states for each player. Then, we instantiate this scheme to
obtain an algorithm for {\it stochastic game lossy channel systems}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4449</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4449</id><created>2014-10-16</created><updated>2015-09-07</updated><authors><author><keyname>S&#xee;rbu</keyname><forenames>Alina</forenames></author><author><keyname>Babaoglu</keyname><forenames>Ozalp</forenames></author></authors><title>A Holistic Approach to Log Data Analysis in High-Performance Computing
  Systems: The Case of IBM Blue Gene/Q</title><categories>cs.DC</categories><comments>12 pages, 7 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity and cost of managing high-performance computing
infrastructures are on the rise. Automating management and repair through
predictive models to minimize human interventions is an attempt to increase
system availability and contain these costs. Building predictive models that
are accurate enough to be useful in automatic management cannot be based on
restricted log data from subsystems but requires a holistic approach to data
analysis from disparate sources. Here we provide a detailed multi-scale
characterization study based on four datasets reporting power consumption,
temperature, workload, and hardware/software events for an IBM Blue Gene/Q
installation. We show that the system runs a rich parallel workload, with low
correlation among its components in terms of temperature and power, but higher
correlation in terms of events. As expected, power and temperature correlate
strongly, while events display negative correlations with load and power. Power
and workload show moderate correlations, and only at the scale of components.
The aim of the study is a systematic, integrated characterization of the
computing infrastructure and discovery of correlation sources and levels to
serve as basis for future predictive modeling efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4453</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4453</id><created>2014-10-16</created><authors><author><keyname>Mivule</keyname><forenames>Kato</forenames></author><author><keyname>Harvey</keyname><forenames>Benjamin</forenames></author><author><keyname>Cobb</keyname><forenames>Crystal</forenames></author><author><keyname>Sayed</keyname><forenames>Hoda El</forenames></author></authors><title>A Review of CUDA, MapReduce, and Pthreads Parallel Computing Models</title><categories>cs.DC</categories><comments>10 Pages, 18 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of high performance computing (HPC) and graphics processing units
(GPU), present an enormous computation resource for Large data transactions
(big data) that require parallel processing for robust and prompt data
analysis. While a number of HPC frameworks have been proposed, parallel
programming models present a number of challenges, for instance, how to fully
utilize features in the different programming models to implement and manage
parallelism via multi-threading in both CPUs and GPUs. In this paper, we take
an overview of three parallel programming models, CUDA, MapReduce, and
Pthreads. The goal is to explore literature on the subject and provide a high
level view of the features presented in the programming models to assist high
performance users with a concise understanding of parallel programming concepts
and thus faster implementation of big data projects using high performance
computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4460</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4460</id><created>2014-10-16</created><updated>2015-01-26</updated><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Parizi</keyname><forenames>Mani Bastani</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>On Metric Sorting for Successive Cancellation List Decoding of Polar
  Codes</title><categories>cs.AR cs.IT math.IT</categories><comments>To be presented in 2015 IEEE International Symposium on Circuits and
  Systems (ISCAS'2015)</comments><doi>10.1109/ISCAS.2015.7169066</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the metric sorter unit of successive cancellation list decoders
for polar codes, which lies on the critical path in all current hardware
implementations of the decoder. We review existing metric sorter architectures
and we propose two new architectures that exploit the structure of the path
metrics in a log-likelihood ratio based formulation of successive cancellation
list decoding. Our synthesis results show that, for the list size of $L=32$,
our first proposed sorter is $14\%$ faster and $45\%$ smaller than existing
sorters, while for smaller list sizes, our second sorter has a higher delay in
return for up to $36\%$ reduction in the area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4461</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4461</id><created>2014-10-16</created><updated>2014-11-12</updated><authors><author><keyname>Ming</keyname><forenames>Xu</forenames></author><author><keyname>Yi-man</keyname><forenames>Du</forenames></author><author><keyname>Jian-ping</keyname><forenames>Wu</forenames></author><author><keyname>Yang</keyname><forenames>Zhou</forenames></author></authors><title>Map Matching based on Conditional Random Fields and Route Preference
  Mining for Uncertain Trajectories</title><categories>cs.NI cs.LG</categories><doi>10.1155/2015/717095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to improve offline map matching accuracy of low-sampling-rate GPS, a
map matching algorithm based on conditional random fields (CRF) and route
preference mining is proposed. In this algorithm, road offset distance and the
temporal-spatial relationship between the sampling points are used as features
of GPS trajectory in CRF model, which can utilize the advantages of integrating
the context information into features flexibly. When the sampling rate is too
low, it is difficult to guarantee the effectiveness using temporal-spatial
context modeled in CRF, and route preference of a driver is used as
replenishment to be superposed on the temporal-spatial transition features. The
experimental results show that this method can improve the accuracy of the
matching, especially in the case of low sampling rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4470</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4470</id><created>2014-10-16</created><updated>2014-10-17</updated><authors><author><keyname>Vemulapalli</keyname><forenames>Raviteja</forenames></author><author><keyname>Boda</keyname><forenames>Vinay Praneeth</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>MKL-RT: Multiple Kernel Learning for Ratio-trace Problems via Convex
  Optimization</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent past, automatic selection or combination of kernels (or
features) based on multiple kernel learning (MKL) approaches has been receiving
significant attention from various research communities. Though MKL has been
extensively studied in the context of support vector machines (SVM), it is
relatively less explored for ratio-trace problems. In this paper, we show that
MKL can be formulated as a convex optimization problem for a general class of
ratio-trace problems that encompasses many popular algorithms used in various
computer vision applications. We also provide an optimization procedure that is
guaranteed to converge to the global optimum of the proposed optimization
problem. We experimentally demonstrate that the proposed MKL approach, which we
refer to as MKL-RT, can be successfully used to select features for
discriminative dimensionality reduction and cross-modal retrieval. We also show
that the proposed convex MKL-RT approach performs better than the recently
proposed non-convex MKL-DR approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4477</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4477</id><created>2014-10-16</created><updated>2014-12-18</updated><authors><author><keyname>Khalili</keyname><forenames>Azam</forenames></author><author><keyname>Bazzi</keyname><forenames>Wael M.</forenames></author><author><keyname>Rastegarnia</keyname><forenames>Amir</forenames></author></authors><title>Analysis of incremental augmented affine projection algorithm for
  distributed estimation of complex signals</title><categories>cs.DC cs.SY</categories><comments>23 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of distributed estimation in an incremental
network when the measurements taken by the node follow a widely linear model.
The proposed algorithm which we refer to it as incremental augmented affine
projection algorithm (incAAPA) utilizes the full second order statistical
information in the complex domain. Moreover, it exploits spatio-temporal
diversity to improve the estimation performance. We derive steady-state
performance metric of the incAAPA in terms of the mean-square deviation (MSD).
We further derive sufficient conditions to ensure mean-square convergence. Our
analysis illustrate that the proposed algorithm is able to process both second
order circular (proper) and noncircular (improper) signals. The validity of the
theoretical results and the good performance of the proposed algorithm are
demonstrated by several computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4485</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4485</id><created>2014-10-16</created><updated>2014-11-05</updated><authors><author><keyname>Bautista</keyname><forenames>Miguel &#xc1;ngel</forenames></author><author><keyname>Hern&#xe1;ndez-Vela</keyname><forenames>Antonio</forenames></author><author><keyname>Escalera</keyname><forenames>Sergio</forenames></author><author><keyname>Igual</keyname><forenames>Laura</forenames></author><author><keyname>Pujol</keyname><forenames>Oriol</forenames></author><author><keyname>Moya</keyname><forenames>Josep</forenames></author><author><keyname>Violant</keyname><forenames>Ver&#xf3;nica</forenames></author><author><keyname>Anguera</keyname><forenames>Mar&#xed;a Teresa</forenames></author></authors><title>A Gesture Recognition System for Detecting Behavioral Patterns of ADHD</title><categories>cs.CV</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an application of gesture recognition using an extension of
Dynamic Time Warping (DTW) to recognize behavioural patterns of Attention
Deficit Hyperactivity Disorder (ADHD). We propose an extension of DTW using
one-class classifiers in order to be able to encode the variability of a
gesture category, and thus, perform an alignment between a gesture sample and a
gesture class. We model the set of gesture samples of a certain gesture
category using either GMMs or an approximation of Convex Hulls. Thus, we add a
theoretical contribution to classical warping path in DTW by including local
modeling of intra-class gesture variability. This methodology is applied in a
clinical context, detecting a group of ADHD behavioural patterns defined by
experts in psychology/psychiatry, to provide support to clinicians in the
diagnose procedure. The proposed methodology is tested on a novel multi-modal
dataset (RGB plus Depth) of ADHD children recordings with behavioural patterns.
We obtain satisfying results when compared to standard state-of-the-art
approaches in the DTW context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4488</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4488</id><created>2014-10-16</created><updated>2014-12-02</updated><authors><author><keyname>Godin</keyname><forenames>Thibault</forenames></author><author><keyname>Klimann</keyname><forenames>Ines</forenames></author><author><keyname>Picantin</keyname><forenames>Matthieu</forenames></author></authors><title>On Torsion-Free Semigroups Generated by Invertible Reversible Mealy
  Automata</title><categories>cs.FL math.GR</categories><comments>12 pages, 4 figures, LATA'15 : 9th International Conference on
  Language and Automata Theory and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the torsion problem for a class of automaton semigroups,
defined as semigroups of transformations induced by Mealy automata, aka
letter-by-letter transducers with the same input and output alphabet. The
torsion problem is undecidable for automaton semigroups in general, but is
known to be solvable within the well-studied class of (semi)groups generated by
invertible bounded Mealy automata. We focus on the somehow antipodal class of
invertible reversible Mealy automata and prove that for a wide subclass the
generated semigroup is torsion-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4500</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4500</id><created>2014-10-16</created><authors><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author></authors><title>On the Feasibility and Implications of Self-Contained Search Engines in
  the Browser</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  JavaScript engines inside modern browsers are capable of running
sophisticated multi-player games, rendering impressive 3D scenes, and
supporting complex, interactive visualizations. Can this processing power be
harnessed for information retrieval? This paper explores the feasibility of
building a JavaScript search engine that runs completely self-contained on the
client side within the browser---this includes building the inverted index,
gathering terms statistics for scoring, and performing query evaluation. The
design takes advantage of the IndexDB API, which is implemented by the LevelDB
key-value store inside Google's Chrome browser. Experiments show that although
the performance of the JavaScript prototype falls far short of the open-source
Lucene search engine, it is sufficiently responsive for interactive
applications. This feasibility demonstration opens the door to interesting
applications in offline and private search across multiple platforms as well as
hybrid split-execution architectures whereby clients and servers
collaboratively perform query evaluation. One possible future scenario is the
rise of an online search marketplace in which commercial search engine
companies and individual users participate as rational economic actors,
balancing privacy, resource usage, latency, and other factors based on
customizable utility profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4507</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4507</id><created>2014-10-15</created><authors><author><keyname>Isenberg</keyname><forenames>Tobias</forenames></author><author><keyname>Wehrheim</keyname><forenames>Heike</forenames></author></authors><title>Proof-Carrying Hardware via IC3</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proof-carrying hardware (PCH) is an approach to achieving safety of
dynamically reconfigurable hardware, transferring the idea of proof-carrying
code to the hardware domain. Current PCH approaches are, however, either
limited to combinational and bounded unfoldings of sequential circuits, or only
provide semi-automatic proof generation. We propose a new approach to PCH which
employs IC3 as proof generator, making automatic PCH applicable to sequential
circuits in their full generality. We demonstrate feasibility of our approach
by showing that proof validation is several orders of magnitude faster than
original proof generation while (most often) generating smaller proofs than
current PCHs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4509</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4509</id><created>2014-10-15</created><authors><author><keyname>Deshpande</keyname><forenames>Aakash</forenames></author><author><keyname>Herbreteau</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Srivathsan</keyname><forenames>B.</forenames></author><author><keyname>Tran</keyname><forenames>Thanh-Tung</forenames></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames></author></authors><title>Fast detection of cycles in timed automata</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new efficient algorithm for detecting if a cycle in a timed
automaton can be iterated infinitely often. Existing methods for this problem
have a complexity which is exponential in the number of clocks. Our method is
polynomial: it essentially does a logarithmic number of zone canonicalizations.
This method can be incorporated in algorithms for verifying B\&quot;uchi properties
on timed automata. We report on some experiments that show a significant
reduction in search space when our iteratability test is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4510</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4510</id><created>2014-10-16</created><updated>2014-11-21</updated><authors><author><keyname>Doshi-Velez</keyname><forenames>Finale</forenames></author><author><keyname>Wallace</keyname><forenames>Byron</forenames></author><author><keyname>Adams</keyname><forenames>Ryan</forenames></author></authors><title>Graph-Sparse LDA: A Topic Model with Structured Sparsity</title><categories>stat.ML cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Originally designed to model text, topic modeling has become a powerful tool
for uncovering latent structure in domains including medicine, finance, and
vision. The goals for the model vary depending on the application: in some
cases, the discovered topics may be used for prediction or some other
downstream task. In other cases, the content of the topic itself may be of
intrinsic scientific interest.
  Unfortunately, even using modern sparse techniques, the discovered topics are
often difficult to interpret due to the high dimensionality of the underlying
space. To improve topic interpretability, we introduce Graph-Sparse LDA, a
hierarchical topic model that leverages knowledge of relationships between
words (e.g., as encoded by an ontology). In our model, topics are summarized by
a few latent concept-words from the underlying graph that explain the observed
words. Graph-Sparse LDA recovers sparse, interpretable summaries on two
real-world biomedical datasets while matching state-of-the-art prediction
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4512</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4512</id><created>2014-10-14</created><updated>2015-03-27</updated><authors><author><keyname>Luttik</keyname><forenames>Bas</forenames></author><author><keyname>Yang</keyname><forenames>Fei</forenames></author></authors><title>Executable Behaviour and the $\pi$-Calculus</title><categories>cs.LO</categories><comments>19 pages,2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactive Turing machines extend classical Turing machines with a facility to
model observable interactive behaviour. We call a behaviour executable if, and
only if, it is behaviourally equivalent to the behaviour of a reactive Turing
machine. In this paper, we study the relationship between executable behaviour
and behaviour that can be specified in the $\pi$-calculus. We establish that
all executable behaviour can be specified in the $\pi$-calculus up to
divergence-preserving branching bisimilarity. The converse, however, is not
true due to (intended) limitations of the model of reactive Turing machines.
That is, the $\pi$-calculus allows the specification of behaviour that is not
executable up to divergence-preserving branching bisimilarity. Motivated by an
intuitive understanding of executability, we then consider a restriction on the
operational semantics of the $\pi$-calculus that does associate with every
$\pi$-term executable behaviour, at least up to the version of branching
bisimilarity that does not require the preservation of divergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4521</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4521</id><created>2014-10-16</created><authors><author><keyname>Maire</keyname><forenames>Michael</forenames></author><author><keyname>Yu</keyname><forenames>Stella X.</forenames></author><author><keyname>Perona</keyname><forenames>Pietro</forenames></author></authors><title>Reconstructive Sparse Code Transfer for Contour Detection and Semantic
  Labeling</title><categories>cs.CV</categories><comments>to appear in Asian Conference on Computer Vision (ACCV), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We frame the task of predicting a semantic labeling as a sparse
reconstruction procedure that applies a target-specific learned transfer
function to a generic deep sparse code representation of an image. This
strategy partitions training into two distinct stages. First, in an
unsupervised manner, we learn a set of generic dictionaries optimized for
sparse coding of image patches. We train a multilayer representation via
recursive sparse dictionary learning on pooled codes output by earlier layers.
Second, we encode all training images with the generic dictionaries and learn a
transfer function that optimizes reconstruction of patches extracted from
annotated ground-truth given the sparse codes of their corresponding image
patches. At test time, we encode a novel image using the generic dictionaries
and then reconstruct using the transfer function. The output reconstruction is
a semantic labeling of the test image.
  Applying this strategy to the task of contour detection, we demonstrate
performance competitive with state-of-the-art systems. Unlike almost all prior
work, our approach obviates the need for any form of hand-designed features or
filters. To illustrate general applicability, we also show initial results on
semantic part labeling of human faces.
  The effectiveness of our approach opens new avenues for research on deep
sparse representations. Our classifiers utilize this representation in a novel
manner. Rather than acting on nodes in the deepest layer, they attach to nodes
along a slice through multiple layers of the network in order to make
predictions about local patches. Our flexible combination of a generatively
learned sparse representation with discriminatively trained transfer
classifiers extends the notion of sparse reconstruction to encompass arbitrary
semantic labeling tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4535</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4535</id><created>2014-10-16</created><authors><author><keyname>Streif</keyname><forenames>Stefan</forenames></author><author><keyname>Karl</keyname><forenames>Matthias</forenames></author><author><keyname>Mesbah</keyname><forenames>Ali</forenames></author></authors><title>Stochastic Nonlinear Model Predictive Control with Efficient Sample
  Approximation of Chance Constraints</title><categories>math.OC cs.SY</categories><comments>Submitted to Journal of Process Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a stochastic model predictive control approach for
nonlinear systems subject to time-invariant probabilistic uncertainties in
model parameters and initial conditions. The stochastic optimal control problem
entails a cost function in terms of expected values and higher moments of the
states, and chance constraints that ensure probabilistic constraint
satisfaction. The generalized polynomial chaos framework is used to propagate
the time-invariant stochastic uncertainties through the nonlinear system
dynamics, and to efficiently sample from the probability densities of the
states to approximate the satisfaction probability of the chance constraints.
To increase computational efficiency by avoiding excessive sampling, a
statistical analysis is proposed to systematically determine a-priori the least
conservative constraint tightening required at a given sample size to guarantee
a desired feasibility probability of the sample-approximated chance constraint
optimization problem. In addition, a method is presented for sample-based
approximation of the analytic gradients of the chance constraints, which
increases the optimization efficiency significantly. The proposed stochastic
nonlinear model predictive control approach is applicable to a broad class of
nonlinear systems with the sufficient condition that each term is analytic with
respect to the states, and separable with respect to the inputs, states and
parameters. The closed-loop performance of the proposed approach is evaluated
using the Williams-Otto reactor with seven states, and ten uncertain parameters
and initial conditions. The results demonstrate the efficiency of the approach
for real-time stochastic model predictive control and its capability to
systematically account for probabilistic uncertainties in contrast to a
nonlinear model predictive control approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4536</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4536</id><created>2014-10-16</created><updated>2015-02-19</updated><authors><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author></authors><title>Numerical Optimization for Symmetric Tensor Decomposition</title><categories>math.NA cs.NA</categories><doi>10.1007/s10107-015-0895-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of decomposing a real-valued symmetric tensor as the
sum of outer products of real-valued vectors. Algebraic methods exist for
computing complex-valued decompositions of symmetric tensors, but here we focus
on real-valued decompositions, both unconstrained and nonnegative, for problems
with low-rank structure. We discuss when solutions exist and how to formulate
the mathematical program. Numerical results show the properties of the proposed
formulations (including one that ignores symmetry) on a set of test problems
and illustrate that these straightforward formulations can be effective even
though the problem is nonconvex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4537</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4537</id><created>2014-10-16</created><authors><author><keyname>Inukollu</keyname><forenames>Venkata N.</forenames></author><author><keyname>Keshamoni</keyname><forenames>Divya D.</forenames></author><author><keyname>Kang</keyname><forenames>Taeghyun</forenames></author><author><keyname>Inukollu</keyname><forenames>Manikanta</forenames></author></authors><title>Factors Influencing Quality of Mobile Apps:Role of Mobile App
  Development Life Cycle</title><categories>cs.SE</categories><comments>International journal of Software Engineering &amp; Applications (IJSEA),
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, The mobile application field has been receiving astronomical
attention from the past few years due to the growing number of mobile app
downloads and withal due to the revenues being engendered .With the surge in
the number of apps, the number of lamentable apps/failing apps has withal been
growing.Interesting mobile app statistics are included in this paper which
might avail the developers understand the concerns and merits of mobile
apps.The authors have made an effort to integrate all the crucial factors that
cause apps to fail which include negligence by the developers, technical
issues, inadequate marketing efforts, and high prospects of the
users/consumers.The paper provides suggestions to eschew failure of apps. As
per the various surveys, the number of lamentable/failing apps is growing
enormously, primarily because mobile app developers are not adopting a standard
development life cycle for the development of apps. In this paper, we have
developed a mobile application with the aid of traditional software development
life cycle phases (Requirements, Design, Develop, Test, and, Maintenance) and
we have used UML, M-UML, and mobile application development technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4546</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4546</id><created>2014-10-16</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Coevolutionary success-driven multigames</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>6 two-column pages, 7 figures; accepted for publication in
  Europhysics Letters</comments><journal-ref>EPL 108 (2014) 28004</journal-ref><doi>10.1209/0295-5075/108/28004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wealthy individuals may be less tempted to defect than those with
comparatively low payoffs. To take this into consideration, we introduce
coevolutionary success-driven multigames in structured populations. While the
core game is always the weak prisoner's dilemma, players whose payoffs from the
previous round exceed a threshold adopt only a minimally low temptation to
defect in the next round. Along with the strategies, the perceived strength of
the social dilemma thus coevolves with the success of each individual player.
We show that the lower the threshold for using the small temptation to defect,
the more the evolution of cooperation is promoted. Importantly, the promotion
of cooperation is not simply due to a lower average temptation to defect, but
rather due to a dynamically reversed direction of invasion along the interfaces
that separate cooperators and defectors on regular networks. Conversely, on
irregular networks, in the absence of clear invasion fronts, the promotion of
cooperation is due to intermediate-degree players. At sufficiently low
threshold values, these players accelerate the erosion of defectors and
significantly shorten the fixation time towards more cooperative stationary
states. Coevolutionary multigames could thus be the new frontier for the swift
resolution of social dilemmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4550</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4550</id><created>2014-10-16</created><authors><author><keyname>Orlitsky</keyname><forenames>A.</forenames></author><author><keyname>Santhanam</keyname><forenames>N.</forenames></author></authors><title>Universal compression of Gaussian sources with unknown parameters</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a collection of distributions over a countable support set, the worst
case universal compression formulation by Shtarkov attempts to assign a
universal distribution over the support set. The formulation aims to ensure
that the universal distribution does not underestimate the probability of any
element in the support set relative to distributions in the collection. When
the alphabet is uncountable and we have a collection $\cal P$ of Lebesgue
continuous measures instead, we ask if there is a corresponding universal
probability density function (pdf) that does not underestimate the value of the
density function at any point in the support relative to pdfs in $\cal P$.
  Analogous to the worst case redundancy of a collection of distributions over
a countable alphabet, we define the \textit{attenuation} of a class to be $A$
when the worst case optimal universal pdf at any point $x$ in the support is
always at least the value any pdf in the collection $\cal P$ assigns to $x$
divided by $A$. We analyze the attenuation of the worst optimal universal pdf
over length-$n$ samples generated \textit{i.i.d.} from a Gaussian distribution
whose mean can be anywhere between $-\alpha/2$ to $\alpha/2$ and variance
between $\sigma_m^2$ and $\sigma_M^2$. We show that this attenuation is finite,
grows with the number of samples as ${\cal O}(n)$, and also specify the
attentuation exactly without approximations. When only one parameter is allowed
to vary, we show that the attenuation grows as ${\cal O}(\sqrt{n})$, again
keeping in line with results from prior literature that fix the order of
magnitude as a factor of $\sqrt{n}$ per parameter. In addition, we also specify
the attenuation exactly without approximation when only the mean or only the
variance is allowed to vary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4573</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4573</id><created>2014-10-16</created><authors><author><keyname>Jayadeva</keyname></author><author><keyname>Chandra</keyname><forenames>Suresh</forenames></author><author><keyname>Sabharwal</keyname><forenames>Siddarth</forenames></author><author><keyname>Batra</keyname><forenames>Sanjit S.</forenames></author></authors><title>Learning a hyperplane regressor by minimizing an exact bound on the VC
  dimension</title><categories>cs.LG</categories><comments>see
  http://www.sciencedirect.com/science/article/pii/S0925231214010194 or
  arXiv:1408.2803 for background information</comments><msc-class>68T05, 68T10, 68Q32</msc-class><acm-class>I.5.1; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of a learning machine is measured by its Vapnik-Chervonenkis
dimension, and learning machines with a low VC dimension generalize better. It
is well known that the VC dimension of SVMs can be very large or unbounded,
even though they generally yield state-of-the-art learning performance. In this
paper, we show how to learn a hyperplane regressor by minimizing an exact, or
\boldmath{$\Theta$} bound on its VC dimension. The proposed approach, termed as
the Minimal Complexity Machine (MCM) Regressor, involves solving a simple
linear programming problem. Experimental results show, that on a number of
benchmark datasets, the proposed approach yields regressors with error rates
much less than those obtained with conventional SVM regresssors, while often
using fewer support vectors. On some benchmark datasets, the number of support
vectors is less than one tenth the number used by SVMs, indicating that the MCM
does indeed learn simpler representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4577</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4577</id><created>2014-10-16</created><authors><author><keyname>Lewandowski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Banaszak</keyname><forenames>Michal</forenames></author></authors><title>Intra-Globular Structures in Multiblock Copolymer Chains from a Monte
  Carlo Simulation</title><categories>cond-mat.soft cs.CE</categories><journal-ref>Physical Review E, 84, 011806, 2011</journal-ref><doi>10.1103/PhysRevE.84.011806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiblock copolymer chains in implicit nonselective solvents are studied by
Monte Carlo method which employs a parallel tempering algorithm. Chains
consisting of 120 $A$ and 120 $B$ monomers, arranged in three distinct
microarchitectures: $(10-10)_{12}$, $(6-6)_{20}$, and $(3-3)_{40}$, collapse to
globular states upon cooling, as expected. By varying both the reduced
temperature $T^*$ and compatibility between monomers $\omega$, numerous
intra-globular structures are obtained: diclusters (handshake, spiral, torus
with a core, etc.), triclusters, and $n$-clusters with $n&gt;3$ (lamellar and
other), which are reminiscent of the block copolymer nanophases for spherically
confined geometries. Phase diagrams for various chains in the $(T^*,
\omega)$-space are mapped. The structure factor $S(k)$, for a selected
microarchitecture and $\omega$, is calculated. Since $S(k)$ can be measured in
scattering experiments, it can be used to relate simulation results to an
experiment. Self-assembly in those systems is interpreted in term of
competition between minimization of the interfacial area separating different
types of monomers and minimization of contacts between chain and solvent.
Finally, the relevance of this model to the protein folding is addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4588</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4588</id><created>2014-10-16</created><authors><author><keyname>Clites</keyname><forenames>Dave</forenames></author><author><keyname>Orr</keyname><forenames>Richard</forenames></author><author><keyname>Rieser</keyname><forenames>Jack</forenames></author><author><keyname>Dellomo</keyname><forenames>Michael</forenames></author></authors><title>Providing Higher Throughput for a Single User with M-ary Orthogonal
  Walsh Codes</title><categories>cs.IT cs.NI math.IT</categories><comments>4 pages, 1 figure. Keywords: sonobuoy, Walsh codes, Code Division
  Multiple Access. This work was supported by the Naval Air Systems Command,
  PMA264, Air Anti-Submarine Warfare Systems Program Office, under contract
  W15P7T-14-C-A802</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable communication is a challenge in a very noisy RF channel further
corrupted by severe, multiple narrowband interference. Code Division Multiple
Access (CDMA) is a widely used method to both mitigate such interference and
support multiple in-band users. The ability to time-control multiple receptions
at a receiver permits use of deterministically orthogonal codes. Achieving full
capacity of the available frequency band depends, in part, on ensuring that all
signals in the band are from the agreed set of orthogonal signals. Each
unwanted signal consumes a fraction of the available capacity. In the extreme,
as more interferers are added the available bandwidth can support only one
user.
  We adapt CDMA techniques to a single user by assigning a constellation of M
symbols built around Walsh codes. This M-ary constellation encodes K bits of
data into one of the length N codewords, and gives preference to a
non-power-of-two Hadamard length, e.g., 12, 20, and higher multiples. The
tradeoff between the modified and conventional Walsh code designs is that one
user sends K bits per Walsh symbol interval as opposed to many users sending
one bit each per symbol. The throughput sacrifice is a consequence of the
severe external interference environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4598</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4598</id><created>2014-10-16</created><authors><author><keyname>Friebel</keyname><forenames>Adrian</forenames></author><author><keyname>Neitsch</keyname><forenames>Johannes</forenames></author><author><keyname>Johann</keyname><forenames>Tim</forenames></author><author><keyname>Hammad</keyname><forenames>Seddik</forenames></author><author><keyname>Hengstler</keyname><forenames>Jan G.</forenames></author><author><keyname>Drasdo</keyname><forenames>Dirk</forenames></author><author><keyname>Hoehme</keyname><forenames>Stefan</forenames></author></authors><title>TiQuant: Software for tissue analysis, quantification and surface
  reconstruction</title><categories>cs.CE cs.GR q-bio.TO</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: TiQuant is a modular software tool for efficient quantification
of biological tissues based on volume data obtained by biomedical image
modalities. It includes a number of versatile image and volume processing
chains tailored to the analysis of different tissue types which have been
experimentally verified. TiQuant implements a novel method for the
reconstruction of three-dimensional surfaces of biological systems, data that
often cannot be obtained experimentally but which is of utmost importance for
tissue modelling in systems biology. Availability: TiQuant is freely available
for non-commercial use at msysbio.com/tiquant. Windows, OSX and Linux are
supported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4599</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4599</id><created>2014-10-16</created><updated>2014-10-23</updated><authors><author><keyname>Pan</keyname><forenames>Erte</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Non-parametric Bayesian Learning with Deep Learning Structure and Its
  Applications in Wireless Networks</title><categories>cs.LG cs.NE cs.NI stat.ML</categories><comments>5 pages, 2 figures and 1 algorithm list</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an infinite hierarchical non-parametric Bayesian
model to extract the hidden factors over observed data, where the number of
hidden factors for each layer is unknown and can be potentially infinite.
Moreover, the number of layers can also be infinite. We construct the model
structure that allows continuous values for the hidden factors and weights,
which makes the model suitable for various applications. We use the
Metropolis-Hastings method to infer the model structure. Then the performance
of the algorithm is evaluated by the experiments. Simulation results show that
the model fits the underlying structure of simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4601</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4601</id><created>2014-10-16</created><authors><author><keyname>Wang</keyname><forenames>Zhuwei</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Liu</keyname><forenames>Lihan</forenames></author></authors><title>Stochastic Optimal Linear Control of Wireless Networked Control Systems
  with Delays and Packet Losses</title><categories>cs.SY</categories><comments>19 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the design of the optimal decentralized state-feedback
controllers is considered for a wireless sensor and actuator network (WSAN)
with stochastic network-induced delays and packet losses. In particular, taking
advantage of multiple controllers, we model the WSAN as a wireless networked
control system (NCS) with decentralized controllers, and then formulate the
stochastic optimal state-feedback control problem as a non-cooperative linear
quadratic (LQ) game. The optimal control law of each controller is obtained
that is a function of the current plant state and all past control signals. The
performance of the proposed stochastic optimal control algorithm is
investigated using both a genetic control system and a load frequency control
(LFC) system in power grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4603</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4603</id><created>2014-10-16</created><authors><author><keyname>Sulaiman</keyname><forenames>Hamzah Asyrani</forenames></author><author><keyname>Bade</keyname><forenames>Abdullah</forenames></author><author><keyname>Abdullah</keyname><forenames>Mohd Harun</forenames></author></authors><title>Efficient Distance Computation Algorithm between Nearly Intersected
  Objects Using Dynamic Pivot Point in Virtual Environment Application</title><categories>cs.GR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding nearly accurate distance between two or more nearly intersecting
three-dimensional (3D) objects is vital especially for collision determination
such as in virtual surgeon simulation and real-time car crash simulation.
Instead of performing broad phase collision detection, we need to check for
accuracy of detection by running narrow phase collision detection. One of the
important elements for narrow phase collision detection is to determine the
precise distance between two or more nearly intersecting objects or polygons in
order to prepare the area for potential colliding. Distance computation plays
important roles in determine the exact point of contact between two or more
nearly intersecting polygons where the preparation for collision detection is
determined at the earlier stage. In this paper, we describes our current works
of determining the distance between objects using dynamic pivot point that will
be used as reference point to reduce the complexity searching for potential
point of contacts. By using Axis-Aligned Bounding Box for each polygon, we
calculate a dynamic pivot point that will become our reference point to
determine the potential candidates for distance computation. The test our
finding distance will be simplified by using our method instead of performing
unneeded operations. Our method provides faster solution than the previous
method where it helps to determine the point of contact efficiently and faster
than the other method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4604</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4604</id><created>2014-10-16</created><authors><author><keyname>Machado</keyname><forenames>Marlos C.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Sriram</forenames></author><author><keyname>Bowling</keyname><forenames>Michael</forenames></author></authors><title>Domain-Independent Optimistic Initialization for Reinforcement Learning</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Reinforcement Learning (RL), it is common to use optimistic initialization
of value functions to encourage exploration. However, such an approach
generally depends on the domain, viz., the scale of the rewards must be known,
and the feature representation must have a constant norm. We present a simple
approach that performs optimistic initialization with less dependence on the
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4607</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4607</id><created>2014-10-16</created><updated>2015-02-05</updated><authors><author><keyname>Zhao</keyname><forenames>Jin-Hua</forenames></author><author><keyname>Habibulla</keyname><forenames>Yusupjan</forenames></author><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author></authors><title>Statistical Mechanics of the Minimum Dominating Set Problem</title><categories>physics.soc-ph cond-mat.dis-nn cs.CC</categories><comments>Extensively revised (final version to be published in Journal of
  Statistical Physics). 19 pages in total</comments><journal-ref>Journal of Statistical Physics 159: 1154--1174 (2015)</journal-ref><doi>10.1007/s10955-015-1220-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum dominating set problem has wide applications in network science
and related fields. It consists of assembling a node set of global minimum size
such that any node of the network is either in this set or is adjacent to at
least one node of this set. Although this is a difficult optimization problem
in general, we show it can be exactly solved by a generalized leaf-removal
process if the network contains no core. If the network has an extensive core,
we estimate the size of minimum dominating sets by a mean-field theory and
implement a belief-propagation algorithm to obtain near-optimal solutions. Our
algorithms also perform well on real-world network instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4612</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4612</id><created>2014-10-16</created><authors><author><keyname>Yamamoto</keyname><forenames>Hirosuke</forenames></author><author><keyname>Ueda</keyname><forenames>Masashi</forenames></author></authors><title>Identification Codes to Identify Multiple Objects</title><categories>cs.IT math.IT</categories><comments>14 pages, submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the case of ordinary identification coding, a code is devised to identify
a single object among $N$ objects. But, in this paper, we consider an
identification coding problem to identify $K$ objects at once among $N$ objects
in the both cases that $K$ objects are ranked or not ranked. By combining
Kurosawa-Yoshida scheme with Moulin-Koetter scheme, an efficient identification
coding scheme is proposed, which can attain high coding rate and error
exponents compared with the case that an ordinary identification code is used
$K$ times. Furthermore, the achievable triplet of rate and error exponents of
type I and type II decoding error probabilities are derived for the proposed
coding scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4613</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4613</id><created>2014-10-16</created><authors><author><keyname>Biel</keyname><forenames>Martin</forenames></author><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author></authors><title>SiMpLIfy: A Toolbox for Structured Model Reduction</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a toolbox for structured model reduction developed
for MATLAB. In addition to structured model reduction methods using balanced
realizations of the subsystems, we introduce a numerical algorithm for
structured model reduction using a subgradient optimization algorithm. We
briefly present the syntax for the toolbox and its features. Finally, we
demonstrate the applicability of various model reduction methods in the toolbox
on a structured mass-spring mechanical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4615</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4615</id><created>2014-10-16</created><updated>2015-02-19</updated><authors><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author></authors><title>Learning to Execute</title><categories>cs.NE cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are
widely used because they are expressive and are easy to train. Our interest
lies in empirically evaluating the expressiveness and the learnability of LSTMs
in the sequence-to-sequence regime by training them to evaluate short computer
programs, a domain that has traditionally been seen as too complex for neural
networks. We consider a simple class of programs that can be evaluated with a
single left-to-right pass using constant memory. Our main result is that LSTMs
can learn to map the character-level representations of such programs to their
correct outputs. Notably, it was necessary to use curriculum learning, and
while conventional curriculum learning proved ineffective, we developed a new
variant of curriculum learning that improved our networks' performance in all
experimental conditions. The improved curriculum had a dramatic impact on an
addition problem, making it possible to train an LSTM to add two 9-digit
numbers with 99% accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4616</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4616</id><created>2014-10-16</created><authors><author><keyname>Doran</keyname><forenames>Derek</forenames></author><author><keyname>Gokhale</keyname><forenames>Swapna</forenames></author><author><keyname>Dagnino</keyname><forenames>Aldo</forenames></author></authors><title>Accurate Local Estimation of Geo-Coordinates for Social Media Posts</title><categories>cs.IR</categories><comments>In Proceedings of the 26th International Conference on Software
  Engineering and Knowledge Engineering, pp. 642 - 647, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Associating geo-coordinates with the content of social media posts can
enhance many existing applications and services and enable a host of new ones.
Unfortunately, a majority of social media posts are not tagged with
geo-coordinates. Even when location data is available, it may be inaccurate,
very broad or sometimes fictitious. Contemporary location estimation approaches
based on analyzing the content of these posts can identify only broad areas
such as a city, which limits their usefulness. To address these shortcomings,
this paper proposes a methodology to narrowly estimate the geo-coordinates of
social media posts with high accuracy. The methodology relies solely on the
content of these posts and prior knowledge of the wide geographical region from
where the posts originate. An ensemble of language models, which are smoothed
over non-overlapping sub-regions of a wider region, lie at the heart of the
methodology. Experimental evaluation using a corpus of over half a million
tweets from New York City shows that the approach, on an average, estimates
locations of tweets to within just 2.15km of their actual positions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4617</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4617</id><created>2014-10-16</created><updated>2015-03-24</updated><authors><author><keyname>Guttman</keyname><forenames>Joshua D.</forenames></author><author><keyname>Rowe</keyname><forenames>Paul D.</forenames></author></authors><title>A Cut Principle for Information Flow</title><categories>cs.CR</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We view a distributed system as a graph of active locations with
unidirectional channels between them, through which they pass messages. In this
context, the graph structure of a system constrains the propagation of
information through it.
  Suppose a set of channels is a cut set between an information source and a
potential sink. We prove that, if there is no disclosure from the source to the
cut set, then there can be no disclosure to the sink. We introduce a new
formalization of partial disclosure, called *blur operators*, and show that the
same cut property is preserved for disclosure to within a blur operator. This
cut-blur property also implies a compositional principle, which ensures limited
disclosure for a class of systems that differ only beyond the cut.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4622</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4622</id><created>2014-10-16</created><authors><author><keyname>Dirafzoon</keyname><forenames>Alireza</forenames></author><author><keyname>Lobaton</keyname><forenames>Edgar</forenames></author></authors><title>Robust Topological Feature Extraction for Mapping of Environments using
  Bio-Inspired Sensor Networks</title><categories>cs.RO cs.SY math.AT stat.ML</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we exploit minimal sensing information gathered from
biologically inspired sensor networks to perform exploration and mapping in an
unknown environment. A probabilistic motion model of mobile sensing nodes,
inspired by motion characteristics of cockroaches, is utilized to extract weak
encounter information in order to build a topological representation of the
environment.
  Neighbor to neighbor interactions among the nodes are exploited to build
point clouds representing spatial features of the manifold characterizing the
environment based on the sampled data.
  To extract dominant features from sampled data, topological data analysis is
used to produce persistence intervals for features, to be used for topological
mapping. In order to improve robustness characteristics of the sampled data
with respect to outliers, density based subsampling algorithms are employed.
Moreover, a robust scale-invariant classification algorithm for persistence
diagrams is proposed to provide a quantitative representation of desired
features in the data. Furthermore, various strategies for defining encounter
metrics with different degrees of information regarding agents' motion are
suggested to enhance the precision of the estimation and classification
performance of the topological method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4624</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4624</id><created>2014-10-16</created><authors><author><keyname>Kim</keyname><forenames>Kiyeon</forenames></author><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Yang</keyname><forenames>Janghoon</forenames></author><author><keyname>Kim</keyname><forenames>Dong Ku</forenames></author></authors><title>The Feasibility of Interference Alignment for Reverse TDD Systems in
  MIMO Cellular Networks</title><categories>cs.IT math.IT</categories><comments>12 pages;submitted to IEEE Transactions on Signal Processing on
  20-Aug.-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The feasibility conditions of interference alignment (IA) are analyzed for
reverse TDD systems, i.e., one cell operates as downlink (DL) but the other
cell operates as uplink (UL). Under general multiple-input and multiple-output
(MIMO) antenna configurations, a necessary condition and a sufficient condition
for one-shot linear IA are established, i.e., linear IA without symbol or time
extension. In several example networks, optimal sum degrees of freedom (DoF) is
characterized by the derived necessary condition and sufficient condition. For
symmetric DoF within each cell, a sufficient condition is established in a more
compact expression, which yields the necessary and sufficient condition for a
class of symmetric DoF. An iterative construction of transmit and received
beamforming vectors is further proposed, which provides a specific beamforming
design satisfying one-shot IA. Simulation results demonstrate that the proposed
IA not only achieve lager DoF but also significantly improve the sum rate in
the practical signal-to-noise ratio (SNR) regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4627</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4627</id><created>2014-10-16</created><updated>2015-11-16</updated><authors><author><keyname>Vondrick</keyname><forenames>Carl</forenames></author><author><keyname>Pirsiavash</keyname><forenames>Hamed</forenames></author><author><keyname>Oliva</keyname><forenames>Aude</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Learning visual biases from human imagination</title><categories>cs.CV</categories><comments>To appear at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the human visual system can recognize many concepts under
challenging conditions, it still has some biases. In this paper, we investigate
whether we can extract these biases and transfer them into a machine
recognition system. We introduce a novel method that, inspired by well-known
tools in human psychophysics, estimates the biases that the human visual system
might use for recognition, but in computer vision feature spaces. Our
experiments are surprising, and suggest that classifiers from the human visual
system can be transferred into a machine with some success. Since these
classifiers seem to capture favorable biases in the human visual system, we
further present an SVM formulation that constrains the orientation of the SVM
hyperplane to agree with the bias from human visual system. Our results suggest
that transferring this human bias into machines may help object recognition
systems generalize across datasets and perform better when very little training
data is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4639</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4639</id><created>2014-10-17</created><updated>2015-07-21</updated><authors><author><keyname>McAdams</keyname><forenames>Darryl</forenames></author><author><keyname>Sterling</keyname><forenames>Jonathan</forenames></author></authors><title>Dependent Types for Pragmatics</title><categories>cs.CL</categories><comments>This version updates the paper for publication in LEUS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the use of dependent types for pragmatic phenomena such
as pronoun binding and presupposition resolution as a type-theoretic
alternative to formalisms such as Discourse Representation Theory and Dynamic
Semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4650</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4650</id><created>2014-10-17</created><updated>2015-06-07</updated><authors><author><keyname>Wang</keyname><forenames>Yilun</forenames></author><author><keyname>Zheng</keyname><forenames>Junjie</forenames></author><author><keyname>Zhang</keyname><forenames>Sheng</forenames></author><author><keyname>Duan</keyname><forenames>Xujun</forenames></author><author><keyname>Chen</keyname><forenames>Huafu</forenames></author></authors><title>Randomized Structural Sparsity via Constrained Block Subsampling for
  Improved Sensitivity of Discriminative Voxel Identification</title><categories>cs.CV stat.ML</categories><acm-class>G.3, I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider voxel selection for functional Magnetic Resonance
Imaging (fMRI) brain data with the aim of finding a more complete set of
probably correlated discriminative voxels, thus improving interpretation of the
discovered potential biomarkers. The main difficulty in doing this is an
extremely high dimensional voxel space and few training samples, resulting in
unreliable feature selection. In order to deal with the difficulty, stability
selection has received a great deal of attention lately, especially due to its
finite sample control of false discoveries and transparent principle for
choosing a proper amount of regularization. However, it fails to make explicit
use of the correlation property or structural information of these
discriminative features and leads to large false negative rates. In other
words, many relevant but probably correlated discriminative voxels are missed.
Thus, we propose a new variant on stability selection &quot;randomized structural
sparsity&quot;, which incorporates the idea of structural sparsity. Numerical
experiments demonstrate that our method can be superior in controlling for
false negatives while also keeping the control of false positives inherited
from stability selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4665</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4665</id><created>2014-10-17</created><authors><author><keyname>Sharma</keyname><forenames>Chayanika</forenames></author><author><keyname>Sibal</keyname><forenames>Ritu</forenames></author></authors><title>Applications of different metaheuristic techniques for finding optimal
  tst order during integration testing of object oriented systems and their
  comparative study</title><categories>cs.SE</categories><comments>19 pages</comments><doi>10.7321/jscse.v3n.12.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent past, a number of researchers have proposed genetic algorithm (GA)
based strategies for finding optimal test order while minimizing the stub
complexity during integration testing. Even though, metaheuristic algorithms
have a wide variety of use in various medium to large size optimization
problems [21], their application to solve the class integration test order
(CITO) problem [12] has not been investigated. In this research paper, we
propose to find a solution to CITO problem by the use of a GA based approach.
We have proposed a class dependency graph (CDG) to model dependencies namely,
association, aggregation, composition and inheritance between classes of
unified modeling language (UML) class diagram. In our approach, weights are
assigned to the edges connecting nodes of CDG and then these weights are used
to model the cost of stubbing. Finally, we compare and discuss the empirical
results of applying our approach with existing graph based and metaheuristic
techniques to the CITO problem and highlight the relative merits and demerits
of the various techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4672</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4672</id><created>2014-10-17</created><authors><author><keyname>Issac</keyname><forenames>B.</forenames></author><author><keyname>Chiong</keyname><forenames>R.</forenames></author><author><keyname>Jacob</keyname><forenames>S. M.</forenames></author></authors><title>Analysis of Phishing Attacks and Countermeasures</title><categories>cs.CR</categories><comments>8 pages</comments><journal-ref>Issac, B., Chiong, R. &amp; Jacob, S. M. (2006, June). Analysis of
  Phishing Attacks and Countermeasures. IBIMA, Bonn, Germany, ISBN
  0-9753393-5-4, pp.339-346</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the biggest problems with the Internet technology is the unwanted spam
emails. The well disguised phishing email comes in as part of the spam and
makes its entry into the inbox quite frequently nowadays. While phishing is
normally considered a consumer issue, the fraudulent tactics the phishers use
are now intimidating the corporate sector as well. In this paper, we analyze
the various aspects of phishing attacks and draw on some possible defenses as
countermeasures. We initially address the different forms of phishing attacks
in theory, and then look at some examples of attacks in practice, along with
their common defenses. We also highlight some recent statistical data on
phishing scam to project the seriousness of the problem. Finally, some specific
phishing countermeasures at both the user level and the organization level are
listed, and a multi-layered anti-phishing proposal is presented to round up our
studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4673</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4673</id><created>2014-10-17</created><authors><author><keyname>Liu</keyname><forenames>Weiyang</forenames></author><author><keyname>Yu</keyname><forenames>Zhiding</forenames></author><author><keyname>Lu</keyname><forenames>Lijia</forenames></author><author><keyname>Wen</keyname><forenames>Yandong</forenames></author><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Zou</keyname><forenames>Yuexian</forenames></author></authors><title>KCRC-LCD: Discriminative Kernel Collaborative Representation with
  Locality Constrained Dictionary for Visual Categorization</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the image classification problem via kernel collaborative
representation classification with locality constrained dictionary (KCRC-LCD).
Specifically, we propose a kernel collaborative representation classification
(KCRC) approach in which kernel method is used to improve the discrimination
ability of collaborative representation classification (CRC). We then measure
the similarities between the query and atoms in the global dictionary in order
to construct a locality constrained dictionary (LCD) for KCRC. In addition, we
discuss several similarity measure approaches in LCD and further present a
simple yet effective unified similarity measure whose superiority is validated
in experiments. There are several appealing aspects associated with LCD. First,
LCD can be nicely incorporated under the framework of KCRC. The LCD similarity
measure can be kernelized under KCRC, which theoretically links CRC and LCD
under the kernel method. Second, KCRC-LCD becomes more scalable to both the
training set size and the feature dimension. Example shows that KCRC is able to
perfectly classify data with certain distribution, while conventional CRC fails
completely. Comprehensive experiments on many public datasets also show that
KCRC-LCD is a robust discriminative classifier with both excellent performance
and good scalability, being comparable or outperforming many other
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4675</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4675</id><created>2014-10-17</created><authors><author><keyname>Jacob</keyname><forenames>S. M.</forenames></author><author><keyname>Issac</keyname><forenames>B.</forenames></author></authors><title>Formative Assessment and its E-learning Implementation</title><categories>cs.CY</categories><journal-ref>Jacob, S. M. &amp; Issac, B. (2005, December). Formative Assessment
  and its E-learning Implementation, ICE 2005, Singapore, pp.258-263</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Innovation in assessment is no more a choice in a tech-savvy instant age. The
purpose of this study was to get more insight into the implementation of
formative assessment through the e-learning tool called Black Board Learning
System in our University. The proposal is to implement a series of weekly or
fortnightly tests on the BB. These would have options to provide sufficient
feedback as a follow up to the attempt of students. Responses from
questionnaires were used to discover the important concerns in the perceptions
of students on the proposed concept of continuous assessment and the BB Online
test implementation. The results indicate that students support the idea mainly
because they find the disintegration into small topic assessments as useful,
coupled with the availability of immediate teacher feedback. What we intend is
a culture of success, backed by a belief that all pupils can achieve the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4677</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4677</id><created>2014-10-17</created><authors><author><keyname>Jacob</keyname><forenames>S. M.</forenames></author><author><keyname>Issac</keyname><forenames>B.</forenames></author></authors><title>Problem Based Learning and Implementations</title><categories>cs.CY</categories><journal-ref>ICE 2005, Singapore, pp.549-554</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an era where learning is considered a problem, we decided to go for
problems for the sake of learning! The purpose of this study was to throw light
on the issues involved in two forms of PBL viz., Case Study Based PBL and
Research Based PBL The influence of course and subject on the learning method
was investigated. Students perceptions and concerns were analyzed using
questionnaires. A slight variation was found among the different courses of
Engineering, Business and IT, as also among the sample subjects considered. It
is concluded that careful and systematic introduction to the case study based
learning can be progressively led to the Research based learning as the student
progresses from the first semester to the final semester of a graduate degree
course or from graduate degree course to post graduate degree course.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4688</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4688</id><created>2014-10-17</created><updated>2015-10-17</updated><authors><author><keyname>Abdelaziz</keyname><forenames>Ibrahim</forenames></author><author><keyname>Abdou</keyname><forenames>Sherif</forenames></author><author><keyname>Al-Barhamtoshy</keyname><forenames>Hassanin</forenames></author></authors><title>Large Vocabulary Arabic Online Handwriting Recognition System</title><categories>cs.CV</categories><comments>Preprint submitted to Pattern Analysis and Applications Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arabic handwriting is a consonantal and cursive writing. The analysis of
Arabic script is further complicated due to obligatory dots/strokes that are
placed above or below most letters and usually written delayed in order. Due to
ambiguities and diversities of writing styles, recognition systems are
generally based on a set of possible words called lexicon. When the lexicon is
small, recognition accuracy is more important as the recognition time is
minimal. On the other hand, recognition speed as well as the accuracy are both
critical when handling large lexicons. Arabic is rich in morphology and syntax
which makes its lexicon large. Therefore, a practical online handwriting
recognition system should be able to handle a large lexicon with reasonable
performance in terms of both accuracy and time. In this paper, we introduce a
fully-fledged Hidden Markov Model (HMM) based system for Arabic online
handwriting recognition that provides solutions for most of the difficulties
inherent in recognizing the Arabic script. A new preprocessing technique for
handling the delayed strokes is introduced. We use advanced modeling techniques
for building our recognition system from the training data to provide more
detailed representation for the differences between the writing units, minimize
the variances between writers in the training data and have a better
representation for the features space. System results are enhanced using an
additional post-processing step with a higher order language model and
cross-word HMM models. The system performance is evaluated using two different
databases covering small and large lexicons. Our system outperforms the
state-of-art systems for the small lexicon database. Furthermore, it shows
promising results (accuracy and time) when supporting large lexicon with the
possibility for adapting the models for specific writers to get even better
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4695</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4695</id><created>2014-10-17</created><authors><author><keyname>Issac</keyname><forenames>B.</forenames></author><author><keyname>Hamid</keyname><forenames>K.</forenames></author><author><keyname>Tan</keyname><forenames>C. E.</forenames></author></authors><title>QoS Survey in IPv6 and Queuing Methods</title><categories>cs.NI</categories><comments>6 pages</comments><journal-ref>Issac, B., Hamid, K. &amp; Tan, C.E (2006, June). QoS Survey in IPv6
  and Queuing Methods. ICOCI, Kuala Lumpur, Malaysia, ISBN 1-4244-0219-0,
  pp.1-6</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The routes in IP networks are determined by the IP destination address and
the routing tables in each router on the path to the destination. Hence all the
IP packets follow the same route until the route is changes due to congestion,
link failure or topology updates. IPv4 tried using Type of Service (TOS) field
in the IP header to classify traffic and that did not succeed as it was based
on fair self-classification of applications in comparison to the network
traffic of other applications. As multimedia applications were quite foreign at
the initial IPv4 stage, TOS field was not used uniformly. As there are
different existing Quality of Service (QoS) paradigms available, IPv6 QoS
approach was designed to be more flexible. The IPv6 protocol thus has
QoS-specific elements in Base header and Extension headers which can be used in
different ways to enhance multimedia application performance. In this paper, we
plan to survey these options and other QoS architectures and discuss their
strengths and weaknesses. Some basic simulation for various queuing schemes is
presented for comparison and a new queuing scheme prioritized WFQ with RR is
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4700</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4700</id><created>2014-10-17</created><authors><author><keyname>Issac</keyname><forenames>Biju</forenames></author><author><keyname>Hamid</keyname><forenames>Khairuddhin Ab</forenames></author><author><keyname>Tan</keyname><forenames>C. E.</forenames></author></authors><title>Analysis of Demand Driven Ad-hoc Routing Protocols on Performance and
  Mobility</title><categories>cs.NI</categories><comments>6 pages</comments><journal-ref>Issac, B., Hamid, K. &amp; Tan, C.E (2006, May). Analysis of Demand
  Driven Ad-hoc Routing Protocols on Performance and Mobility, IWTS, Shah Alam,
  Malaysia, ISBN 983-42747-3-4, pp.136-141</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad hoc networking (MANET) is a growing technology that can support the
operation of adaptive wireless networks. With the increased demand rate of
wireless applications it is useful to have more adaptive and self organizing
technologies that adapt to changes within a network region. In this paper we
initially present a brief listing of table driven ad hoc routing protocols and
eventually analyze in detail the behaviour of demand driven ad hoc routing
protocols like - Ad Hoc On Demand Distance Vector Routing (AODV), Dynamic
Source Routing (DSR), Temporally-Ordered Routing Algorithm (TORA) in terms of
throughput, traffic dropped, routing traffic and mobility. The output graphs
are eventually discussed, thus enabling us to understand the routing technology
better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4701</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4701</id><created>2014-10-17</created><updated>2015-01-26</updated><authors><author><keyname>Cording</keyname><forenames>Patrick Hagge</forenames></author></authors><title>Optimal Time Random Access to Grammar-Compressed Strings in Small Space</title><categories>cs.DS</categories><comments>Withdrawn because of errors in proofs. Fixed versions will be
  incorporated into a paper by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The random access problem for compressed strings is to build a data structure
that efficiently supports accessing the character in position $i$ of a string
given in compressed form. Given a grammar of size $n$ compressing a string of
size $N$, we present a data structure using $O(n\Delta \log_\Delta \frac N n
\log N)$ bits of space that supports accessing position $i$ in $O(\log_\Delta
N)$ time for $\Delta \leq \log^{O(1)} N$. The query time is optimal for
polynomially compressible strings, i.e., when $n=O(N^{1-\epsilon})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4713</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4713</id><created>2014-10-17</created><authors><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Chacra</keyname><forenames>David Abou</forenames></author><author><keyname>Nash</keyname><forenames>Rupert W.</forenames></author><author><keyname>Jaros</keyname><forenames>Jiri</forenames></author><author><keyname>Bernabeu</keyname><forenames>Miguel O.</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author></authors><title>Weighted decomposition in high-performance lattice-Boltzmann
  simulations: are some lattice sites more equal than others?</title><categories>cs.DC cond-mat.mes-hall</categories><comments>11 pages, 8 figures, 1 table, accepted for the EASC2014 conference</comments><msc-class>68W10, 68W40, 68U20, 68N30, 65Yxx</msc-class><acm-class>G.1.0; G.4; I.3.1; I.6.3; I.6.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obtaining a good load balance is a significant challenge in scaling up
lattice-Boltzmann simulations of realistic sparse problems to the exascale.
Here we analyze the effect of weighted decomposition on the performance of the
HemeLB lattice-Boltzmann simulation environment, when applied to sparse
domains. Prior to domain decomposition, we assign wall and in/outlet sites with
increased weights which reflect their increased computational cost. We combine
our weighted decomposition with a second optimization, which is to sort the
lattice sites according to a space filling curve. We tested these strategies on
a sparse bifurcation and very sparse aneurysm geometry, and find that using
weights reduces calculation load imbalance by up to 85%, although the overall
communication overhead is higher than some of our runs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4725</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4725</id><created>2014-10-17</created><updated>2015-04-28</updated><authors><author><keyname>Jahn</keyname><forenames>Thomas</forenames></author></authors><title>Geometric Algorithms for Minimal Enclosing Discs in Strictly Convex
  Normed Planes</title><categories>math.MG cs.CG</categories><comments>submitted to &quot;Contributions to Discrete Mathematics&quot;</comments><msc-class>90C25, 90B85, 46B20, 52A21</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the geometric background provided by Alonso, Martini, and Spirova on the
location of circumcenters of triangles in normed planes, we show the validity
of the Elzinga--Hearn algorithm and the Shamos--Hoey algorithm for solving the
minimal enclosing disk problem in strictly convex normed planes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4730</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4730</id><created>2014-10-17</created><authors><author><keyname>Hou</keyname><forenames>Junhui</forenames></author><author><keyname>Chau</keyname><forenames>Lap-Pui</forenames></author><author><keyname>Magnenat-Thalmann</keyname><forenames>Nadia</forenames></author><author><keyname>He</keyname><forenames>Ying</forenames></author></authors><title>Human Motion Capture Data Tailored Transform Coding</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human motion capture (mocap) is a widely used technique for digitalizing
human movements. With growing usage, compressing mocap data has received
increasing attention, since compact data size enables efficient storage and
transmission. Our analysis shows that mocap data have some unique
characteristics that distinguish themselves from images and videos. Therefore,
directly borrowing image or video compression techniques, such as discrete
cosine transform, does not work well. In this paper, we propose a novel
mocap-tailored transform coding algorithm that takes advantage of these
features. Our algorithm segments the input mocap sequences into clips, which
are represented in 2D matrices. Then it computes a set of data-dependent
orthogonal bases to transform the matrices to frequency domain, in which the
transform coefficients have significantly less dependency. Finally, the
compression is obtained by entropy coding of the quantized coefficients and the
bases. Our method has low computational cost and can be easily extended to
compress mocap databases. It also requires neither training nor complicated
parameter setting. Experimental results demonstrate that the proposed scheme
significantly outperforms state-of-the-art algorithms in terms of compression
performance and speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4744</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4744</id><created>2014-10-17</created><authors><author><keyname>Kone&#x10d;n&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>mS2GD: Mini-Batch Semi-Stochastic Gradient Descent in the Proximal
  Setting</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a mini-batching scheme for improving the theoretical complexity
and practical performance of semi-stochastic gradient descent applied to the
problem of minimizing a strongly convex composite function represented as the
sum of an average of a large number of smooth convex functions, and simple
nonsmooth convex function. Our method first performs a deterministic step
(computation of the gradient of the objective function at the starting point),
followed by a large number of stochastic steps. The process is repeated a few
times with the last iterate becoming the new starting point. The novelty of our
method is in introduction of mini-batching into the computation of stochastic
steps. In each step, instead of choosing a single function, we sample $b$
functions, compute their gradients, and compute the direction based on this. We
analyze the complexity of the method and show that the method benefits from two
speedup effects. First, we prove that as long as $b$ is below a certain
threshold, we can reach predefined accuracy with less overall work than without
mini-batching. Second, our mini-batching scheme admits a simple parallel
implementation, and hence is suitable for further acceleration by
parallelization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4754</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4754</id><created>2014-10-17</created><updated>2016-01-14</updated><authors><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author><author><keyname>Facchinei</keyname><forenames>Francisco</forenames></author><author><keyname>Lampariello</keyname><forenames>Lorenzo</forenames></author><author><keyname>Song</keyname><forenames>Peiran</forenames></author></authors><title>Parallel and Distributed Methods for Nonconvex Optimization-Part I:
  Theory</title><categories>cs.MA math.OC</categories><comments>Part of this work has been presented at IEEE ICASSP 2014; Part II is
  available as separate arrive submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this two-part paper, we propose a general algorithmic framework for the
minimization of a nonconvex smooth function subject to nonconvex smooth
constraints. The algorithm solves a sequence of (separable) strongly convex
problems and mantains feasibility at each iteration. Convergence to a
stationary solution of the original nonconvex optimization is established. Our
framework is very general and flexible; it unifies several existing Successive
Convex Approximation (SCA)-based algorithms such as (proximal) gradient or
Newton type methods, block coordinate (parallel) descent schemes, difference of
convex functions methods, and improves on their convergence properties. More
importantly, and differently from current SCA approaches, it naturally leads to
distributed and parallelizable implementations for a large class of nonconvex
problems.
  This Part I is devoted to the description of the framework in its generality.
In Part II we customize our general methods to several multi-agent optimization
problems, mainly in communications and networking; the result is a new class of
(distributed) algorithms that compare favorably to existing ad-hoc
(centralized) schemes (when they exist).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4760</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4760</id><created>2014-10-17</created><authors><author><keyname>Aralu</keyname><forenames>Ugonna</forenames></author></authors><title>Influence of Information Support System on ICT Use by distance learners
  in University of Lagos Nigeria</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The use of modern technology in Education is the key to an increased drive
for learning which shape learners critical and analytic competencies with
respect to disciplinary knowledge. Distance education (DE) is a system of
learning driven by computer linked to internet. The flexible nature of DE avail
students who are unable to attend full time education due to age, social or
religious barriers. However, in Nigeria, the University of Lagos distance
learning Institute has its shortfalls traced to poor student support system,
which affects the service delivery to students. The study examined Influence of
Information Support on ICT use by distance learners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4772</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4772</id><created>2014-10-17</created><updated>2016-02-08</updated><authors><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Luccio</keyname><forenames>Flaminia L.</forenames></author><author><keyname>Markou</keyname><forenames>Euripides</forenames></author></authors><title>Mobile Agents Rendezvous in spite of a Malicious Agent</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the problem of rendezvous, i.e., having multiple mobile agents
gather in a single node of the network. Unlike previous studies, we need to
achieve rendezvous in presence of a very powerful adversary, a malicious agent
that moves through the network and tries to block the honest agents and
prevents them from gathering. The malicious agent is assumed to be arbitrarily
fast, has full knowledge of the network and it cannot be exterminated by the
honest agents. On the other hand, the honest agents are assumed to be quite
weak: They are asynchronous and anonymous, they have only finite memory, they
have no prior knowledge of the network and they can communicate with the other
agents only when they meet at a node. Can the honest agents achieve rendezvous
starting from an arbitrary configuration in spite of the malicious agent? We
present some necessary conditions for solving rendezvous in spite of the
malicious agent in arbitrary networks. We then focus on the ring and mesh
topologies and provide algorithms to solve rendezvous. For ring networks, our
algorithms solve rendezvous in all feasible instances of the problem, while we
show that rendezvous is impossible for an even number of agents in unoriented
rings. For the oriented mesh networks, we prove that the problem can be solved
when the honest agents initially form a connected configuration without holes
if and only if they can see which are the occupied nodes within a two-hops
distance. To the best of our knowledge, this is the first attempt to study such
a powerful and mobile fault model, in the context of mobile agents. Our model
lies between the more powerful but static fault model of black holes (which can
even destroy the agents), and the less powerful but mobile fault model of
Byzantine agents (which can only imitate the honest agents but can neither harm
nor stop them).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4773</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4773</id><created>2014-10-17</created><updated>2014-11-03</updated><authors><author><keyname>Khalili</keyname><forenames>Shahrouz</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander M.</forenames></author></authors><title>Cloud Radio-Multistatic Radar: Joint Optimization of Code Vector and
  Backhaul Quantization</title><categories>math.ST cs.IT math.IT stat.CO stat.TH</categories><comments>To be published in IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2014.2363939</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multistatic radar set-up is considered in which distributed receive
antennas are connected to a Fusion Center (FC) via limited-capacity backhaul
links. Similar to cloud radio access networks in communications, the receive
antennas quantize the received baseband signal before transmitting it to the
FC. The problem of maximizing the detection performance at the FC jointly over
the code vector used by the transmitting antenna and over the statistics of the
noise introduced by backhaul quantization is investigated. Specifically,
adopting the information-theoretic criterion of the Bhattacharyya distance to
evaluate the detection performance at the FC and information-theoretic measures
of the quantization rate, the problem at hand is addressed via a Block
Coordinate Descent (BCD) method coupled with Majorization-Minimization (MM).
Numerical results demonstrate the advantages of the proposed joint optimization
approach over more conventional solutions that perform separate optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4774</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4774</id><created>2014-10-17</created><authors><author><keyname>Dorta-Gonzalez</keyname><forenames>Pablo</forenames></author><author><keyname>Dorta-Gonzalez</keyname><forenames>Maria Isabel</forenames></author><author><keyname>Santos-Penate</keyname><forenames>Dolores Rosa</forenames></author><author><keyname>Suarez-Vega</keyname><forenames>Rafael</forenames></author></authors><title>Research status and trends in Operations Research and Management Science
  (OR/MS) journals: A bibliometric analysis based on the Web of Science
  database 2001-2012</title><categories>cs.DL</categories><comments>21 pages, 4 figures and 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bibliometric analysis to evaluate global scientific production in the
subject category of Operations Research and Management Science (OR/MS) from
2001 to 2012 was applied. Data was based on the Web of Science (Science
Citation Index) database compiled by Thomson Reuters. The results showed that
the OR/MS research has significantly increased over the past twelve years. The
Bradford core journals in the category were identified. The researchers paid
great attention to networks, control, and simulation. Among the countries, USA
attained a dominant position in global research in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4777</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4777</id><created>2014-10-17</created><authors><author><keyname>Morris</keyname><forenames>Richard G.</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author></authors><title>A Hierarchical Multi-Output Nearest Neighbor Model for Multi-Output
  Dependence Learning</title><categories>stat.ML cs.LG</categories><comments>10 pages, 2 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-Output Dependence (MOD) learning is a generalization of standard
classification problems that allows for multiple outputs that are dependent on
each other. A primary issue that arises in the context of MOD learning is that
for any given input pattern there can be multiple correct output patterns. This
changes the learning task from function approximation to relation
approximation. Previous algorithms do not consider this problem, and thus
cannot be readily applied to MOD problems. To perform MOD learning, we
introduce the Hierarchical Multi-Output Nearest Neighbor model (HMONN) that
employs a basic learning model for each output and a modified nearest neighbor
approach to refine the initial results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4781</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4781</id><created>2014-10-15</created><authors><author><keyname>Bayat</keyname><forenames>F. Merrikh</forenames></author><author><keyname>Guo</keyname><forenames>X.</forenames></author><author><keyname>Ommani</keyname><forenames>H. A.</forenames></author><author><keyname>Do</keyname><forenames>N.</forenames></author><author><keyname>Likharev</keyname><forenames>K. K.</forenames></author><author><keyname>Strukov</keyname><forenames>D. B.</forenames></author></authors><title>Redesigning Commercial Floating-Gate Memory for Analog Computing
  Applications</title><categories>cs.ET cond-mat.other</categories><comments>4 pages, 6 figures</comments><acm-class>B.7.1; C.1.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have modified a commercial NOR flash memory array to enable high-precision
tuning of individual floating-gate cells for analog computing applications. The
modified array area per cell in a 180 nm process is about 1.5 um^2. While this
area is approximately twice the original cell size, it is still at least an
order of magnitude smaller than in the state-of-the-art analog circuit
implementations. The new memory cell arrays have been successfully tested, in
particular confirming that each cell may be automatically tuned, with ~1%
precision, to any desired subthreshold readout current value within an almost
three-orders-of-magnitude dynamic range, even using an unoptimized tuning
algorithm. Preliminary results for a four-quadrant vector-by-matrix multiplier,
implemented with the modified memory array gate-coupled with additional
peripheral floating-gate transistors, show highly linear transfer
characteristics over a broad range of input currents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4795</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4795</id><created>2014-10-17</created><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Rzeszutko</keyname><forenames>El&#x17c;bieta</forenames></author></authors><title>Security - a perpetual war: lessons from nature</title><categories>cs.CR</categories><comments>15 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For ages people have sought inspiration in nature. Biomimicry has been the
propelling power of such inventions, like Velcro tape or &quot;cat's eyes&quot; -
retroreflective road marking. At the same time, scientists have been developing
biologically inspired techniques: genetic algorithms, neural and sensor
networks, etc. Although at a first glance there is no direct inspiration behind
offensive and defensive techniques seen in the Internet and the patterns
present in nature, closer inspection reveals many analogies between these two
worlds. Botnets, DDoS (Distributed Denial of Service) attacks, IDS/IPSs
(Intrusion Detection/Prevention Systems), and others, all employ strategies
which very closely resemble actions undertaken by certain species of the
kingdoms of living things. The main conclusion of the analysis is that security
community should turn to nature in search of new offensive and defensive
techniques for virtual world security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4801</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4801</id><created>2014-10-17</created><updated>2015-11-24</updated><authors><author><keyname>Randour</keyname><forenames>Mickael</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Sankur</keyname><forenames>Ocan</forenames></author></authors><title>Percentile Queries in Multi-Dimensional Markov Decision Processes</title><categories>cs.LO</categories><comments>Extended version of CAV 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes (MDPs) with multi-dimensional weights are useful to
analyze systems with multiple objectives that may be conflicting and require
the analysis of trade-offs. We study the complexity of percentile queries in
such MDPs and give algorithms to synthesize strategies that enforce such
constraints. Given a multi-dimensional weighted MDP and a quantitative payoff
function $f$, thresholds $v_i$ (one per dimension), and probability thresholds
$\alpha_i$, we show how to compute a single strategy to enforce that for all
dimensions $i$, the probability of outcomes $\rho$ satisfying $f_i(\rho) \geq
v_i$ is at least $\alpha_i$. We consider classical quantitative payoffs from
the literature (sup, inf, lim sup, lim inf, mean-payoff, truncated sum,
discounted sum). Our work extends to the quantitative case the multi-objective
model checking problem studied by Etessami et al. in unweighted MDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4821</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4821</id><created>2014-10-17</created><authors><author><keyname>Udell</keyname><forenames>Madeleine</forenames></author><author><keyname>Mohan</keyname><forenames>Karanveer</forenames></author><author><keyname>Zeng</keyname><forenames>David</forenames></author><author><keyname>Hong</keyname><forenames>Jenny</forenames></author><author><keyname>Diamond</keyname><forenames>Steven</forenames></author><author><keyname>Boyd</keyname><forenames>Stephen</forenames></author></authors><title>Convex Optimization in Julia</title><categories>math.OC cs.MS stat.ML</categories><comments>To appear in Proceedings of the Workshop on High Performance
  Technical Computing in Dynamic Languages (HPTCDL) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes Convex, a convex optimization modeling framework in
Julia. Convex translates problems from a user-friendly functional language into
an abstract syntax tree describing the problem. This concise representation of
the global structure of the problem allows Convex to infer whether the problem
complies with the rules of disciplined convex programming (DCP), and to pass
the problem to a suitable solver. These operations are carried out in Julia
using multiple dispatch, which dramatically reduces the time required to verify
DCP compliance and to parse a problem into conic form. Convex then
automatically chooses an appropriate backend solver to solve the conic form
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4828</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4828</id><created>2014-10-17</created><authors><author><keyname>Yu</keyname><forenames>Yaoliang</forenames></author><author><keyname>Zhang</keyname><forenames>Xinhua</forenames></author><author><keyname>Schuurmans</keyname><forenames>Dale</forenames></author></authors><title>Generalized Conditional Gradient for Sparse Estimation</title><categories>math.OC cs.LG stat.ML</categories><comments>67 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured sparsity is an important modeling tool that expands the
applicability of convex formulations for data analysis, however it also creates
significant challenges for efficient algorithm design. In this paper we
investigate the generalized conditional gradient (GCG) algorithm for solving
structured sparse optimization problems---demonstrating that, with some
enhancements, it can provide a more efficient alternative to current state of
the art approaches. After providing a comprehensive overview of the convergence
properties of GCG, we develop efficient methods for evaluating polar operators,
a subroutine that is required in each GCG iteration. In particular, we show how
the polar operator can be efficiently evaluated in two important scenarios:
dictionary learning and structured sparse estimation. A further improvement is
achieved by interleaving GCG with fixed-rank local subspace optimization. A
series of experiments on matrix completion, multi-class classification,
multi-view dictionary learning and overlapping group lasso shows that the
proposed method can significantly reduce the training cost of current
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4831</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4831</id><created>2014-10-17</created><authors><author><keyname>Eliasi</keyname><forenames>Parisa A.</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author></authors><title>Low-Rank Spatial Channel Estimation for Millimeter Wave Cellular Systems</title><categories>cs.IT math.IT</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tremendous bandwidth available in the millimeter wave (mmW) frequencies
between 30 and 300 GHz have made these bands an attractive candidate for
next-generation cellular systems. However, reliable communication at these
frequencies depends extensively on beamforming with very high-dimensional
antenna arrays. Estimating the channel sufficiently accurately to perform
beamforming can thus be challenging both due to low coherence time and large
number of antennas. Also, the measurements used for channel estimation may need
to be made with analog beamforming where the receiver can &quot;look&quot; in only
direction at a time. This work presents a novel method for estimation of the
receive-side spatial covariance matrix of a channel from a sequence of power
measurements made at different angular directions. The method reduces the
spatial covariance estimation to a matrix completion optimization problem. To
reduce the number of measurements, the optimization can incorporate the
low-rank constraints in the channels that are typical in the mmW setting. The
optimization is convex and fast, iterative methods are presented to solving the
problem. Simulations are presented for both single and multi-path channels
using channel models derived from real measurements in New York City at 28 GHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4838</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4838</id><created>2014-10-17</created><authors><author><keyname>Sharma</keyname><forenames>Chayanika</forenames></author><author><keyname>Sabharwal</keyname><forenames>Sangeeta</forenames></author><author><keyname>Sibal</keyname><forenames>Ritu</forenames></author></authors><title>Applying Genetic Algorithm for Prioritization of Test Case Scenarios
  Derived from UML Diagrams</title><categories>cs.SE</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software testing involves identifying the test cases whichdiscover errors in
the program. However, exhaustive testing ofsoftware is very time consuming. In
this paper, a technique isproposed to prioritize test case scenarios by
identifying the critical path clusters using genetic algorithm. The test case
scenarios are derived from the UML activity diagram and state chart diagram.
The testing efficiency is optimized by applying the genetic algorithm on the
test data. The information flow metric is adopted in this work for calculating
the information flow complexity associated with each node of the activity
diagram and state chart diagram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4847</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4847</id><created>2014-09-30</created><authors><author><keyname>Maeno</keyname><forenames>Yoshiharu</forenames></author><author><keyname>Nishiguchi</keyname><forenames>Kenji</forenames></author><author><keyname>Morinaga</keyname><forenames>Satoshi</forenames></author><author><keyname>Matsushima</keyname><forenames>Hirokazu</forenames></author></authors><title>Impact of shadow banks on financial contagion</title><categories>q-fin.RM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An asset network systemic risk (ANWSER) model is presented to investigate the
impact of how shadow banks are intermingled in a financial system on the
severity of financial contagion. Particularly, the focus of this study is the
impact of the following three representative topologies of an interbank loan
network between shadow banks and regulated banks. (1) Random mixing network:
shadow banks and regulated banks are intermingled randomly. (2)
Asset-correlated mixing network: banks having bigger assets are a regulated
bank and other banks are shadow banks. (3) Layered mixing network: banks in a
shadow bank layer are connected to banks in a regulated bank layer with some
interbank loans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4849</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4849</id><created>2014-10-17</created><authors><author><keyname>Ren</keyname><forenames>Yihui</forenames></author><author><keyname>Ercsey-Ravasz</keyname><forenames>M&#xe1;ria</forenames></author><author><keyname>Wang</keyname><forenames>Pu</forenames></author><author><keyname>Gonz&#xe1;lez</keyname><forenames>Marta C.</forenames></author><author><keyname>Toroczkai</keyname><forenames>Zolt&#xe1;n</forenames></author></authors><title>Predicting commuter flows in spatial networks using a radiation model
  based on temporal ranges</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>24 pages, 6 figures. A first-principles based traffic prediction
  model</comments><doi>10.1038/ncomms6347</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding network flows such as commuter traffic in large transportation
networks is an ongoing challenge due to the complex nature of the
transportation infrastructure and of human mobility. Here we show a
first-principles based method for traffic prediction using a cost based
generalization of the radiation model for human mobility, coupled with a
cost-minimizing algorithm for efficient distribution of the mobility fluxes
through the network. Using US census and highway traffic data we show that
traffic can efficiently and accurately be computed from a range-limited,
network betweenness type calculation. The model based on travel time costs
captures the lognormal distribution of the traffic and attains a high Pearson
correlation coefficient (0.75) when compared to real traffic. Due to its
principled nature, this method can inform many applications related to human
mobility driven flows in spatial networks, ranging from transportation, through
urban planning to mitigation of the effects of catastrophic events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4863</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4863</id><created>2014-10-17</created><authors><author><keyname>Haralambous</keyname><forenames>Yannis</forenames></author><author><keyname>Elidrissi</keyname><forenames>Yassir</forenames></author><author><keyname>Lenca</keyname><forenames>Philippe</forenames></author></authors><title>Arabic Language Text Classification Using Dependency Syntax-Based
  Feature Selection</title><categories>cs.CL</categories><comments>10 pages, 4 figure, accepted at CITALA 2014 (http://www.citala.org/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of Arabic text classification combining various
techniques: (a) tfidf vs. dependency syntax, for feature selection and
weighting; (b) class association rules vs. support vector machines, for
classification. The Arabic text is used in two forms: rootified and lightly
stemmed. The results we obtain show that lightly stemmed text leads to better
performance than rootified text; that class association rules are better suited
for small feature sets obtained by dependency syntax constraints; and, finally,
that support vector machines are better suited for large feature sets based on
morphological feature selection criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4865</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4865</id><created>2014-10-17</created><updated>2015-09-15</updated><authors><author><keyname>Varshney</keyname><forenames>Kush R.</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author></authors><title>Olfactory Signal Processing</title><categories>cs.IT cs.MM math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Olfaction, the sense of smell, has received scant attention from a signal
processing perspective in comparison to audition and vision. In this paper, we
develop a signal processing paradigm for olfactory signals based on new
scientific discoveries including the psychophysics concept of olfactory white.
We describe a framework for predicting the perception of odorant compounds from
their physicochemical features and use the prediction as a foundation for
several downstream processing tasks. We detail formulations for odor
cancellation and food steganography, and provide real-world empirical examples
for the two tasks. We also discuss adaptive filtering and other olfactory
signal processing tasks at a high level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4868</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4868</id><created>2014-10-17</created><authors><author><keyname>Baker</keyname><forenames>Kathryn</forenames></author><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Dorr</keyname><forenames>Bonnie J.</forenames></author><author><keyname>Filardo</keyname><forenames>Nathaniel W.</forenames></author><author><keyname>Levin</keyname><forenames>Lori</forenames></author><author><keyname>Piatko</keyname><forenames>Christine</forenames></author></authors><title>A Modality Lexicon and its use in Automatic Tagging</title><categories>cs.CL</categories><comments>6 pages, 5 figures; appeared in Proceedings of the Seventh
  International Conference on Language Resources and Evaluation (LREC'10), May
  2010</comments><acm-class>I.2.7</acm-class><journal-ref>In Proceedings of the Seventh International Conference on Language
  Resources and Evaluation (LREC'10), pages 1402-1407, Valletta, Malta, May
  2010. European Language Resources Association</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes our resource-building results for an eight-week JHU
Human Language Technology Center of Excellence Summer Camp for Applied Language
Exploration (SCALE-2009) on Semantically-Informed Machine Translation.
Specifically, we describe the construction of a modality annotation scheme, a
modality lexicon, and two automated modality taggers that were built using the
lexicon and annotation scheme. Our annotation scheme is based on identifying
three components of modality: a trigger, a target and a holder. We describe how
our modality lexicon was produced semi-automatically, expanding from an initial
hand-selected list of modality trigger words and phrases. The resulting
expanded modality lexicon is being made publicly available. We demonstrate that
one tagger---a structure-based tagger---results in precision around 86%
(depending on genre) for tagging of a standard LDC data set. In a machine
translation application, using the structure-based tagger to annotate English
modalities on an English-Urdu training corpus improved the translation quality
score for Urdu by 0.3 Bleu points in the face of sparse training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4871</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4871</id><created>2014-10-17</created><updated>2015-04-09</updated><authors><author><keyname>Combrexelle</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Wendt</keyname><forenames>Herwig</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author><author><keyname>McLaughlin</keyname><forenames>Steve</forenames></author><author><keyname>Abry</keyname><forenames>Patrice</forenames></author></authors><title>Bayesian estimation of the multifractality parameter for image texture
  using a Whittle approximation</title><categories>physics.data-an cs.CV stat.ME</categories><journal-ref>IEEE T. Image Proces., vol. 24, no. 8, pp. 2540-2551, Aug. 2015</journal-ref><doi>10.1109/TIP.2015.2426021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texture characterization is a central element in many image processing
applications. Multifractal analysis is a useful signal and image processing
tool, yet, the accurate estimation of multifractal parameters for image texture
remains a challenge. This is due in the main to the fact that current
estimation procedures consist of performing linear regressions across frequency
scales of the two-dimensional (2D) dyadic wavelet transform, for which only a
few such scales are computable for images. The strongly non-Gaussian nature of
multifractal processes, combined with their complicated dependence structure,
makes it difficult to develop suitable models for parameter estimation. Here,
we propose a Bayesian procedure that addresses the difficulties in the
estimation of the multifractality parameter. The originality of the procedure
is threefold: The construction of a generic semi-parametric statistical model
for the logarithm of wavelet leaders; the formulation of Bayesian estimators
that are associated with this model and the set of parameter values admitted by
multifractal theory; the exploitation of a suitable Whittle approximation
within the Bayesian model which enables the otherwise infeasible evaluation of
the posterior distribution associated with the model. Performance is assessed
numerically for several 2D multifractal processes, for several image sizes and
a large range of process parameters. The procedure yields significant benefits
over current benchmark estimators in terms of estimation performance and
ability to discriminate between the two most commonly used classes of
multifractal process models. The gains in performance are particularly
pronounced for small image sizes, notably enabling for the first time the
analysis of image patches as small as 64x64 pixels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4876</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4876</id><created>2014-10-17</created><updated>2015-06-25</updated><authors><author><keyname>Dias</keyname><forenames>Elis&#xe2;ngela Silva</forenames></author><author><keyname>Castonguay</keyname><forenames>Diane</forenames></author><author><keyname>Longo</keyname><forenames>Humberto</forenames></author><author><keyname>Jradi</keyname><forenames>Walid Abdala Rfaei</forenames></author><author><keyname>Nascimento</keyname><forenames>Hugo A. D. do</forenames></author></authors><title>A GPU-based parallel algorithm for enumerating all chordless cycles in
  graphs</title><categories>cs.DC</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a finite undirected simple graph, a chordless cycle is an induced subgraph
which is a cycle. We propose a GPU parallel algorithm for enumerating all
chordless cycles of such a graph. The algorithm, implemented in OpenCL, is
based on a previous sequential algorithm developed by the current authors for
the same problem. It uses a more compact data structure for solution
representation which is suitable for the memory-size limitation of a GPU.
Moreover, for graphs with a sufficiently large amount of chordless cycles, the
algorithm presents a significant improvement in execution time that outperforms
the sequential method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4885</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4885</id><created>2014-10-17</created><authors><author><keyname>Hager</keyname><forenames>William W.</forenames></author><author><keyname>Hungerford</keyname><forenames>James T.</forenames></author><author><keyname>Safro</keyname><forenames>Ilya</forenames></author></authors><title>A Multilevel Bilinear Programming Algorithm For the Vertex Separator
  Problem</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Vertex Separator Problem for a graph is to find the smallest collection
of vertices whose removal breaks the graph into two disconnected subsets that
satisfy specified size constraints. In the paper 10.1016/j.ejor.2014.05.042,
the Vertex Separator Problem was formulated as a continuous
(non-concave/non-convex) bilinear quadratic program. In this paper, we develop
a more general continuous bilinear program which incorporates vertex weights,
and which applies to the coarse graphs that are generated in a multilevel
compression of the original Vertex Separator Problem. A Mountain Climbing
Algorithm is used to find a stationary point of the continuous bilinear
quadratic program, while second-order optimality conditions and perturbation
techniques are used to escape from either a stationary point or a local
maximizer. The algorithms for solving the continuous bilinear program are
employed during the solution and refinement phases in a multilevel scheme.
Computational results and comparisons demonstrate the advantage of the proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4896</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4896</id><created>2014-10-17</created><authors><author><keyname>Basciftci</keyname><forenames>Y. Ozan</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author></authors><title>Delay Optimal Secrecy in Two-Relay Network</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a two-relay network in which a source aims to communicate a
confidential message to a destination while keeping the message secret from the
relay nodes. In the first hop, the channels from the source to the relays are
assumed to be block-fading and the channel states change arbitrarily -possibly
non-stationary and non-ergodic- across blocks. When the relay feedback on the
states of the source-to-relay channels is available on the source with no
delay, we provide an encoding strategy to achieve the optimal delay. We next
consider the case in which there is one-block delayed relay feedback on the
states of the source-to-relay channels. We show that for a set of channel state
sequences, the optimal delay with one-block delayed feedback differs from the
optimal delay with no-delayed feedback at most one block.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4917</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4917</id><created>2014-10-18</created><updated>2014-10-25</updated><authors><author><keyname>Del Tedesco</keyname><forenames>Filippo</forenames></author><author><keyname>Sands</keyname><forenames>David</forenames></author><author><keyname>Russo</keyname><forenames>Alejandro</forenames></author></authors><title>Type-Directed Compilation for Fault-Tolerant Non-Interference</title><categories>cs.CR</categories><comments>Extended version, 77 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Environmental noise (e.g.heat, ionized particles, etc.) causes transient
faults in hardware, which lead to corruption of stored values. Mission-critical
devices require such faults to be mitigated by fault-tolerance --- a
combination of techniques that aim at preserving the functional behaviour of a
system despite the disruptive effects of transient faults. Fault-tolerance
typically has a high deployment cost -- special hardware might be required to
implement it -- and provides weak statistical guarantees. It is also based on
the assumption that faults are rare. In this paper, we consider scenarios where
security, rather than functional correctness, is the main asset to be
protected. Our contribution is twofold. Firstly, we develop a theory for
expressing confidentiality of data in the presence of transient faults. We show
that the natural probabilistic definition of security in the presence of faults
can be captured by a possibilistic definition. Furthermore, the possibilistic
definition is implied by a known bisimulation-based property, called Strong
Security. Secondly, we illustrate the utility of these results for a simple
RISC architecture for which only the code memory and program counter are
assumed fault-tolerant. We present a type-directed compilation scheme that
produces RISC code from a higher-level language for which Strong Security holds
--- i.e. well-typed programs compile to RISC code which is secure despite
transient faults. In contrast with fault-tolerance solutions, our technique
assumes relatively little special hardware, gives formal guarantees, and works
in the presence of an active attacker who aggressively targets parts of a
system and induces faults precisely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4919</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4919</id><created>2014-10-18</created><authors><author><keyname>Wang</keyname><forenames>PengFei</forenames></author><author><keyname>Li</keyname><forenames>JianPing</forenames></author></authors><title>On the relation between reliable computation time, float-point precision
  and the Lyapunov exponent in chaotic systems</title><categories>nlin.CD cs.NA</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relation among reliable computation time, Tc, float-point precision, K,
and the Lyapunov exponent, {\lambda}, is obtained as Tc= (lnB/{\lambda})K+C,
where B is the base of the float-point system and C is a constant dependent
only on the chaotic equation. The equation shows good agreement with numerical
experimental results, especially the scale factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4925</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4925</id><created>2014-10-18</created><authors><author><keyname>Kauranne</keyname><forenames>Tuomo</forenames></author></authors><title>Finitely unstable theories and computational complexity</title><categories>math.LO cs.CC cs.LO</categories><msc-class>03C13, 68Q15, 68Q19</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity class $NP$ can be logically characterized both through
existential second order logic $SO\exists$, as proven by Fagin, and through
simulating a Turing machine via the satisfiability problem of propositional
logic SAT, as proven by Cook. Both theorems involve encoding a Turing machine
by a formula in the corresponding logic and stating that a model of this
formula exists if and only if the Turing machine halts, i.e. the formula is
satisfiable iff the Turing machine accepts its input. Trakhtenbrot's theorem
does the same in first order logic $FO$. Such different orders of encoding are
possible because the set of all possible configurations of any Turing machine
up to any given finite time instant can be defined by a finite set of
propositional variables, or is locally represented by a model of fixed finite
size. In the current paper, we first encode such time-limited computations of a
deterministic Turing machine (DTM) in first order logic. We then take a closer
look at DTMs that solve SAT. When the length of the input string to such a DTM
that contains effectively encoded instances of SAT is parameterized by the
natural number $M$, we proceed to show that the corresponding $FO$ theory
$SAT_M$ has a lower bound on the size of its models that grows almost
exponentially with $M$. This lower bound on model size also translates into a
lower bound on the deterministic time complexity of SAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4928</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4928</id><created>2014-10-18</created><authors><author><keyname>Gharibi</keyname><forenames>Wajeb</forenames></author><author><keyname>Gharibi</keyname><forenames>Gharib</forenames></author></authors><title>Gharibi_FaceCard for Contacts and Easy Personal - Information Exchange</title><categories>cs.CY</categories><comments>2 pages. Proceedings of the 8th INDIACom; INDIACom-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss a new contact way for exchanging personal
information using mobile phones. The idea of this invention depends on
allocating a special code called Gharibi Code (GC) for each personal mobile and
creating a personal information file called Gharibi Face Card (GFC), which has
all specified personal data of the mobile phone user. When you request
someone's GC code, the other party's phone will send you the GFC of that
person. We think that this approach will facilitate the process of
communication and exchanging the specified personal data easily, especially
when acquaintance. Mobile number, phone and e-mail address, for example, will
be sent in a few seconds using a simple code that does not exceed six
characters long to transfer a huge amount of personal data through mobile
devices rather than using traditional business or visiting cards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4931</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4931</id><created>2014-10-18</created><authors><author><keyname>Kumar</keyname><forenames>M. Ashok</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Relative $\alpha$-Entropy Minimizers Subject to Linear Statistical
  Constraints</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>6 pages, 1 figure, submitted to National Conference on Communication
  (NCC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study minimization of a parametric family of relative entropies, termed
relative $\alpha$-entropies (denoted $\mathscr{I}_{\alpha}(P,Q)$). These arise
as redundancies under mismatched compression when cumulants of compressed
lengths are considered instead of expected compressed lengths. These parametric
relative entropies are a generalization of the usual relative entropy
(Kullback-Leibler divergence). Just like relative entropy, these relative
$\alpha$-entropies behave like squared Euclidean distance and satisfy the
Pythagorean property. Minimization of $\mathscr{I}_{\alpha}(P,Q)$ over the
first argument on a set of probability distributions that constitutes a linear
family is studied. Such a minimization generalizes the maximum R\'{e}nyi or
Tsallis entropy principle. The minimizing probability distribution (termed
$\mathscr{I}_{\alpha}$-projection) for a linear family is shown to have a
power-law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4950</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4950</id><created>2014-10-18</created><updated>2015-11-07</updated><authors><author><keyname>Nakagawa</keyname><forenames>Shota</forenames></author><author><keyname>Hasuo</keyname><forenames>Ichiro</forenames></author></authors><title>Near-Optimal Scheduling for LTL with Future Discounting</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the search problem for optimal schedulers for the linear temporal
logic (LTL) with future discounting. The logic, introduced by Almagor, Boker
and Kupferman, is a quantitative variant of LTL in which an event in the far
future has only discounted contribution to a truth value (that is a real number
in the unit interval [0, 1]). The precise problem we study---it naturally
arises e.g. in search for a scheduler that recovers from an internal error
state as soon as possible---is the following: given a Kripke frame, a formula
and a number in [0, 1] called a margin, find a path of the Kripke frame that is
optimal with respect to the formula up to the prescribed margin (a truly
optimal path may not exist). We present an algorithm for the problem; it works
even in the extended setting with propositional quality operators, a setting
where (threshold) model-checking is known to be undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4954</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4954</id><created>2014-10-18</created><authors><author><keyname>Mansour</keyname><forenames>Mohammad M.</forenames></author></authors><title>Pruned Bit-Reversal Permutations: Mathematical Characterization, Fast
  Algorithms and Architectures</title><categories>cs.IT math.IT</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mathematical characterization of serially-pruned permutations (SPPs)
employed in variable-length permuters and their associated fast pruning
algorithms and architectures are proposed. Permuters are used in many signal
processing systems for shuffling data and in communication systems as an
adjunct to coding for error correction. Typically only a small set of discrete
permuter lengths are supported. Serial pruning is a simple technique to alter
the length of a permutation to support a wider range of lengths, but results in
a serial processing bottleneck. In this paper, parallelizing SPPs is formulated
in terms of recursively computing sums involving integer floor and related
functions using integer operations, in a fashion analogous to evaluating
Dedekind sums. A mathematical treatment for bit-reversal permutations (BRPs) is
presented, and closed-form expressions for BRP statistics are derived. It is
shown that BRP sequences have weak correlation properties. A new statistic
called permutation inliers that characterizes the pruning gap of pruned
interleavers is proposed. Using this statistic, a recursive algorithm that
computes the minimum inliers count of a pruned BR interleaver (PBRI) in
logarithmic time complexity is presented. This algorithm enables parallelizing
a serial PBRI algorithm by any desired parallelism factor by computing the
pruning gap in lookahead rather than a serial fashion, resulting in significant
reduction in interleaving latency and memory overhead. Extensions to 2-D block
and stream interleavers, as well as applications to pruned fast Fourier
transforms and LTE turbo interleavers, are also presented. Moreover,
hardware-efficient architectures for the proposed algorithms are developed.
Simulation results demonstrate 3 to 4 orders of magnitude improvement in
interleaving time compared to existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4955</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4955</id><created>2014-10-18</created><updated>2015-10-27</updated><authors><author><keyname>Chen</keyname><forenames>Wenbin</forenames></author></authors><title>Settling the Randomized k-sever Conjecture on Some Special Metrics</title><categories>cs.DS</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  the LP formulation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we settle the randomized $k$-sever conjecture for the
following metric spaces: line, circle, Hierarchically well-separated tree
(HST). Specially, we show that there are $O(\log k)$-competitive randomized
$k$-sever algorithms for above metric spaces. For any general metric space with
$n$ points, we show that there is an $O( \log k \log n)$-competitive randomized
$k$-sever algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4956</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4956</id><created>2014-10-18</created><updated>2014-10-21</updated><authors><author><keyname>Insa</keyname><forenames>David</forenames></author><author><keyname>Silva</keyname><forenames>Josep</forenames></author></authors><title>Transforming while/do/for/foreach-Loops into Recursive Methods</title><categories>cs.PL</categories><comments>16 pages, 8 figures, 6 algorithms, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In software engineering, taking a good election between recursion and
iteration is essential because their efficiency and maintenance are different.
In fact, developers often need to transform iteration into recursion (e.g., in
debugging, to decompose the call graph into iterations); thus, it is quite
surprising that there does not exist a public transformation from loops to
recursion that handles all kinds of loops. This article describes a
transformation able to transform iterative loops into equivalent recursive
methods. The transformation is described for the programming language Java, but
it is general enough as to be adapted to many other languages that allow
iteration and recursion. We describe the changes needed to transform loops of
types while/do/for/foreach into recursion. Each kind of loop requires a
particular treatment that is described and exemplified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4963</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4963</id><created>2014-10-18</created><authors><author><keyname>Davoodi</keyname><forenames>Pooya</forenames></author><author><keyname>Raman</keyname><forenames>Rajeev</forenames></author><author><keyname>Satti</keyname><forenames>Srinivasa Rao</forenames></author></authors><title>On Succinct Representations of Binary Trees</title><categories>cs.DS</categories><comments>Journal version of part of COCOON 2012 paper</comments><acm-class>F.2.2; E.2; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We observe that a standard transformation between \emph{ordinal} trees
(arbitrary rooted trees with ordered children) and binary trees leads to
interesting succinct binary tree representations. There are four symmetric
versions of these transformations. Via these transformations we get four
succinct representations of $n$-node binary trees that use $2n + n/(\log
n)^{O(1)}$ bits and support (among other operations) navigation, inorder
numbering, one of pre- or post-order numbering, subtree size and lowest common
ancestor (LCA) queries. The ability to support inorder numbering is crucial for
the well-known range-minimum query (RMQ) problem on an array $A$ of $n$ ordered
values. While this functionality, and more, is also supported in $O(1)$ time
using $2n + o(n)$ bits by Davoodi et al.'s (\emph{Phil. Trans. Royal Soc. A}
\textbf{372} (2014)) extension of a representation by Farzan and Munro
(\emph{Algorithmica} \textbf{6} (2014)), their \emph{redundancy}, or the $o(n)$
term, is much larger, and their approach may not be suitable for practical
implementations.
  One of these transformations is related to the Zaks' sequence (S.~Zaks,
\emph{Theor. Comput. Sci.} \textbf{10} (1980)) for encoding binary trees, and
we thus provide the first succinct binary tree representation based on Zaks'
sequence. Another of these transformations is equivalent to Fischer and Heun's
(\emph{SIAM J. Comput.} \textbf{40} (2011)) \minheap\ structure for this
problem. Yet another variant allows an encoding of the Cartesian tree of $A$ to
be constructed from $A$ using only $O(\sqrt{n} \log n)$ bits of working space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4966</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4966</id><created>2014-10-18</created><authors><author><keyname>Lala</keyname><forenames>Chiraag</forenames></author><author><keyname>Cohen</keyname><forenames>Shay B.</forenames></author></authors><title>The Visualization of Change in Word Meaning over Time using Temporal
  Word Embeddings</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a visualization tool that can be used to view the change in
meaning of words over time. The tool makes use of existing (static) word
embedding datasets together with a timestamped $n$-gram corpus to create {\em
temporal} word embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4967</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4967</id><created>2014-10-18</created><authors><author><keyname>Kallergis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chimos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Stefanos</keyname><forenames>Vizikidis</forenames></author><author><keyname>Karvounidis</keyname><forenames>Theodoros</forenames></author><author><keyname>Douligeris</keyname><forenames>Christos</forenames></author></authors><title>Pirus: A Web-based File Hosting Service with Object Oriented Logic in
  Cloud Computing</title><categories>cs.SE cs.DC</categories><comments>6 pages, 3rd International Conference on Internet and Cloud Computing
  Technology (ICICCT2013), November 6-7 2013, Singapore</comments><msc-class>68U99</msc-class><acm-class>C.2.4; D.1.5; D.2.3; D.4.3; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new Web-based File Hosting Service with Object Oriented Logic
in Cloud Computing called Pirus was developed. The service will be used by the
academic community of the University of Piraeus giving users the ability to
remotely store and access their personal files with no security compromises. It
also offers the administrators the ability to manage users and roles. The
objective was to deliver a fully operational service, using state-of-the-art
programming techniques to enable scalability and future development of the
existing functionality. The use of technologies such as .NET Framework, C#
programming language, CSS and jQuery, MSSQL for database hosting and the
support of Virtualization and Cloud Computing will contribute significantly in
compatibility, code reuse, reliability and reduce of maintenance costs and
resources. The service was installed and tested in a controlled environment to
ascertain the required functionality and the offered reliability and safety
with complete success.
  The technologies used and supported, allow future work in upgrading and
extending the service. Changes and improvements, in hardware and software, in
order to convert the service to a SaaS (Software as a Service) Cloud
application is a logical step in order to efficiently offer the service to a
wider community. Improved and added functionality offered by further
development will leverage the user experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4970</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4970</id><created>2014-10-18</created><authors><author><keyname>Goussis</keyname><forenames>Lampros</forenames></author><author><keyname>Foukarakis</keyname><forenames>Ioannis E.</forenames></author><author><keyname>Kallergis</keyname><forenames>Dimitrios N.</forenames></author></authors><title>Aspects and challenges of mashup creator design</title><categories>cs.CY</categories><comments>5 pages, 14th Panhellenic Conference on Informatics (PCI2010),
  September 10-12 2010, Tripoli, Greece</comments><msc-class>68N19</msc-class><acm-class>H.3.4; I.2.2</acm-class><doi>10.1109/PCI.2010.22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of Web 2.0, an increasing number of web sites has started
offering their data over the web in standard formats and exposed their
functionality as APIs. A new type of applications has taken advantage of the
new data and services available by mixing them, in order to generate new
applications fast and efficiently, getting its name from its own architectural
style: mashups. A set of applications that aims to help a user create, deploy
and manage his mashups has also emerged, using various approaches. In this
paper we discuss the key factors that should be taken into consideration when
designing a mashup creator, along with the most important challenges that offer
a field for research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4972</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4972</id><created>2014-10-18</created><authors><author><keyname>Kallergis</keyname><forenames>Dimitrios N.</forenames></author><author><keyname>Foukarakis</keyname><forenames>Ioannis E.</forenames></author><author><keyname>Prezerakos</keyname><forenames>George N.</forenames></author></authors><title>Design issues for distributed mobile social networks</title><categories>cs.SI</categories><comments>6 pages, eRA 5th International Scientific Conference, September 15-18
  2010, Piraeus, Greece</comments><msc-class>68M10</msc-class><acm-class>C.1.3; C.2.1; K.6.1; H.3.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Social networks and their applications have become extremely popular during
the last years, mostly targeting users via the web. However, it has been
recently observed an interest to offer social network services to mobile users.
Telecom operators attempt to integrate existing social networks to their
systems or develop new ones, in order to offer new services to their
subscribers. Subsequently, emphasis is given to the user-context modeling, as
well as to the integration of sources that leads to the summarized collection
of information anchored to the user; such as its location or its mobile device
type, etc.
  In this paper we discuss the most important factors and challenges
encountered during the design of such a system on architectural, technological
and tool level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4977</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4977</id><created>2014-10-18</created><authors><author><keyname>Desai</keyname><forenames>Pratikkumar</forenames></author><author><keyname>Sheth</keyname><forenames>Amit</forenames></author><author><keyname>Anantharam</keyname><forenames>Pramod</forenames></author></authors><title>Semantic Gateway as a Service architecture for IoT Interoperability</title><categories>cs.NI cs.DC</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things (IoT) is set to occupy a substantial component of
future Internet. The IoT connects sensors and devices that record physical
observations to applications and services of the Internet. As a successor to
technologies such as RFID and Wireless Sensor Networks (WSN), the IoT has
stumbled into vertical silos of proprietary systems, providing little or no
interoperability with similar systems. As the IoT represents future state of
the Internet, an intelligent and scalable architecture is required to provide
connectivity between these silos, enabling discovery of physical sensors and
interpretation of messages between things. This paper proposes a gateway and
Semantic Web enabled IoT architecture to provide interoperability between
systems using established communication and data standards. The Semantic
Gateway as Service (SGS) allows translation between messaging protocols such as
XMPP, CoAP and MQTT via a multi-protocol proxy architecture. Utilization of
broadly accepted specifications such as W3C's Semantic Sensor Network (SSN)
ontology for semantic annotations of sensor data provide semantic
interoperability between messages and support semantic reasoning to obtain
higher-level actionable knowledge from low-level sensor data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4978</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4978</id><created>2014-10-18</created><authors><author><keyname>Haris</keyname><forenames>Muhammad</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Hui</keyname><forenames>Pan</forenames></author></authors><title>Privacy Leakage in Mobile Computing: Tools, Methods, and Characteristics</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of smartphones, tablets, sensors, and connected wearable devices
are rapidly increasing. Today, in many parts of the globe, the penetration of
mobile computers has overtaken the number of traditional personal computers.
This trend and the always-on nature of these devices have resulted in
increasing concerns over the intrusive nature of these devices and the privacy
risks that they impose on users or those associated with them. In this paper,
we survey the current state of the art on mobile computing research, focusing
on privacy risks and data leakage effects. We then discuss a number of methods,
recommendations, and ongoing research in limiting the privacy leakages and
associated risks by mobile computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4980</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4980</id><created>2014-10-18</created><updated>2014-12-14</updated><authors><author><keyname>Sch&#xf6;pp</keyname><forenames>Ulrich</forenames><affiliation>Ludwig-Maximilians-Universit&#xe4;t M&#xfc;nchen, Germany</affiliation></author></authors><title>On the Relation of Interaction Semantics to Continuations and
  Defunctionalization</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  16, 2014) lmcs:977</journal-ref><doi>10.2168/LMCS-10(4:10)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In game semantics and related approaches to programming language semantics,
programs are modelled by interaction dialogues. Such models have recently been
used in the design of new compilation methods, e.g. for hardware synthesis or
for programming with sublinear space. This paper relates such semantically
motivated non-standard compilation methods to more standard techniques in the
compilation of functional programming languages, namely continuation passing
and defunctionalization. We first show for the linear {\lambda}-calculus that
interpretation in a model of computation by interaction can be described as a
call-by-name CPS-translation followed by a defunctionalization procedure that
takes into account control-flow information. We then establish a relation
between these two compilation methods for the simply-typed {\lambda}-calculus
and end by considering recursion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4984</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4984</id><created>2014-10-18</created><authors><author><keyname>Dai</keyname><forenames>Zhenwen</forenames></author><author><keyname>Damianou</keyname><forenames>Andreas</forenames></author><author><keyname>Hensman</keyname><forenames>James</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil</forenames></author></authors><title>Gaussian Process Models with Parallelization and GPU acceleration</title><categories>cs.DC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present an extension of Gaussian process (GP) models with
sophisticated parallelization and GPU acceleration. The parallelization scheme
arises naturally from the modular computational structure w.r.t. datapoints in
the sparse Gaussian process formulation. Additionally, the computational
bottleneck is implemented with GPU acceleration for further speed up. Combining
both techniques allows applying Gaussian process models to millions of
datapoints. The efficiency of our algorithm is demonstrated with a synthetic
dataset. Its source code has been integrated into our popular software library
GPy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4985</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4985</id><created>2014-10-18</created><updated>2015-03-28</updated><authors><author><keyname>Tarapore</keyname><forenames>Danesh</forenames></author><author><keyname>Mouret</keyname><forenames>Jean-Baptiste</forenames></author></authors><title>Evolvability signatures of generative encodings: beyond standard
  performance benchmarks</title><categories>cs.NE</categories><comments>24 pages with 12 figures in the main text, and 4 supplementary
  figures. Accepted at Information Sciences journal (in press). Supplemental
  videos are available online at, see http://goo.gl/uyY1RX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary robotics is a promising approach to autonomously synthesize
machines with abilities that resemble those of animals, but the field suffers
from a lack of strong foundations. In particular, evolutionary systems are
currently assessed solely by the fitness score their evolved artifacts can
achieve for a specific task, whereas such fitness-based comparisons provide
limited insights about how the same system would evaluate on different tasks,
and its adaptive capabilities to respond to changes in fitness (e.g., from
damages to the machine, or in new situations). To counter these limitations, we
introduce the concept of &quot;evolvability signatures&quot;, which picture the
post-mutation statistical distribution of both behavior diversity (how
different are the robot behaviors after a mutation?) and fitness values (how
different is the fitness after a mutation?). We tested the relevance of this
concept by evolving controllers for hexapod robot locomotion using five
different genotype-to-phenotype mappings (direct encoding, generative encoding
of open-loop and closed-loop central pattern generators, generative encoding of
neural networks, and single-unit pattern generators (SUPG)). We observed a
predictive relationship between the evolvability signature of each encoding and
the number of generations required by hexapods to adapt from incurred damages.
Our study also reveals that, across the five investigated encodings, the SUPG
scheme achieved the best evolvability signature, and was always foremost in
recovering an effective gait following robot damages. Overall, our evolvability
signatures neatly complement existing task-performance benchmarks, and pave the
way for stronger foundations for research in evolutionary robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4986</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4986</id><created>2014-10-18</created><updated>2015-06-24</updated><authors><author><keyname>Emad</keyname><forenames>Amin</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Code Construction and Decoding Algorithms for Semi-Quantitative Group
  Testing with Nonuniform Thresholds</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a new group testing scheme, termed semi-quantitative group
testing, which may be viewed as a concatenation of an adder channel and a
discrete quantizer. Our focus is on non-uniform quantizers with arbitrary
thresholds. For the most general semi-quantitative group testing model, we
define three new families of sequences capturing the constraints on the code
design imposed by the choice of the thresholds. The sequences represent
extensions and generalizations of Bh and certain types of super-increasing and
lexicographically ordered sequences, and they lead to code structures amenable
for efficient recursive decoding. We describe the decoding methods and provide
an accompanying computational complexity and performance analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.4987</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.4987</id><created>2014-10-18</created><authors><author><keyname>Aamir</keyname><forenames>Muhammad</forenames></author></authors><title>Content-Priority based Interest Forwarding in Content Centric Networks</title><categories>cs.NI</categories><comments>06 pages, 06 figures, 01 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content Centric Networking (CCN) is a recent advancement in communication
networks where the current research is mainly focusing on routing &amp; cache
management strategies of CCN. Nonetheless, other perspectives such as network
level security and service quality are also of prime importance; areas which
have not been covered deeply so far. This paper introduces an interest
forwarding mechanism to process the requests of consumers at a CCN router.
Interest packets are forwarded with respect to the priorities of addressed
content while the priority level settings are done by content publishers during
an initialization phase using a collaborative mechanism of exchanging messages
to agree to the priority levels of all content according to the content-nature.
Interests with higher priority content are recorded in Pending Interest Table
(PIT) as well as forwarded to content publishers prior to those with lower
priority content. A simulation study is also conducted to show the
effectiveness of proposed scheme and we observe that the interests with higher
priority content are satisfied earlier than the interests with lower priority
content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5000</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5000</id><created>2014-10-18</created><updated>2015-02-17</updated><authors><author><keyname>Hamza</keyname><forenames>Jad</forenames></author></authors><title>On the complexity of Linearizability</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was shown in Alur et al. [1] that the problem of verifying finite
concurrent systems through Linearizability is in EXPSPACE. However, there was
still a complexity gap between the easy to obtain PSPACE lower bound and the
EXPSPACE upper bound. We show in this paper that Linearizability is
EXPSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5009</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5009</id><created>2014-10-18</created><updated>2015-06-11</updated><authors><author><keyname>Wang</keyname><forenames>Zhao</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Secure Degrees of Freedom of Wireless X Networks Using Artificial Noise
  Alignment</title><categories>cs.IT math.IT</categories><comments>14 pages, 3 figures, to appear in IEEE Trans. on Communications.
  Please note that the short version of this paper which is included in the
  Proceedings of ISIT-2015 contains an error. We have corrected it in this
  journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of transmitting confidential messages in $M \times K$ wireless X
networks is considered, in which each transmitter intends to send one
confidential message to every receiver. In particular, the secure degrees of
freedom (SDOF) of the considered network are studied based on an artificial
noise alignment (ANA) approach, which integrates interference alignment and
artificial noise transmission. At first, an SDOF upper bound is derived for the
$M \times K$ X network with confidential messages (XNCM) to be
$\frac{K(M-1)}{K+M-2}$. By proposing an ANA approach, it is shown that the SDOF
upper bound is tight when $K=2$ for the considered XNCM with time/frequency
varying channels. For $K \geq 3$, it is shown that SDOF of
$\frac{K(M-1)}{K+M-1}$ can be achieved, even when an external eavesdropper is
present. The key idea of the proposed scheme is to inject artificial noise into
the network, which can be aligned in the interference space at receivers for
confidentiality. Moreover, for the network with no channel state information at
transmitters, a blind ANA scheme is proposed to achieve SDOF of
$\frac{K(M-1)}{K+M-1}$ for $K,M \geq 2$, with reconfigurable antennas at
receivers. The proposed method provides a linear approach to secrecy coding and
interference alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5010</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5010</id><created>2014-10-18</created><updated>2015-01-17</updated><authors><author><keyname>Stengel</keyname><forenames>Holger</forenames></author><author><keyname>Treibig</keyname><forenames>Jan</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Quantifying performance bottlenecks of stencil computations using the
  Execution-Cache-Memory model</title><categories>cs.PF cs.DC</categories><comments>10 pages, 8 figures. Added Roofline comparison and other minor
  improvements</comments><doi>10.1145/2751205.2751240</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stencil algorithms on regular lattices appear in many fields of computational
science, and much effort has been put into optimized implementations. Such
activities are usually not guided by performance models that provide estimates
of expected speedup. Understanding the performance properties and bottlenecks
by performance modeling enables a clear view on promising optimization
opportunities. In this work we refine the recently developed
Execution-Cache-Memory (ECM) model and use it to quantify the performance
bottlenecks of stencil algorithms on a contemporary Intel processor. This
includes applying the model to arrive at single-core performance and
scalability predictions for typical corner case stencil loop kernels. Guided by
the ECM model we accurately quantify the significance of &quot;layer conditions,&quot;
which are required to estimate the data traffic through the memory hierarchy,
and study the impact of typical optimization approaches such as spatial
blocking, strength reduction, and temporal blocking for their expected
benefits. We also compare the ECM model to the widely known Roofline model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5016</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5016</id><created>2014-10-18</created><updated>2015-08-11</updated><authors><author><keyname>Roychowdhury</keyname><forenames>Jaijeet</forenames></author></authors><title>Boolean Computation Using Self-Sustaining Nonlinear Oscillators</title><categories>cs.ET</categories><comments>Added a section on energy-efficiency and speed using high-Q harmonic
  oscillators. Other minor updates</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-sustaining nonlinear oscillators of practically any type can function as
latches and registers if Boolean logic states are represented physically as the
phase of oscillatory signals. Combinational operations on such phase-encoded
logic signals can be implemented using arithmetic negation and addition
followed by amplitude limiting. With these, general-purpose Boolean computation
using a wide variety of natural and engineered oscillators becomes potentially
possible. Such phase-encoded logic shows promise for energy efficient
computing. It also has inherent noise immunity advantages over traditional
level-based logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5020</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5020</id><created>2014-10-18</created><authors><author><keyname>Dai</keyname><forenames>Binbin</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Sparse Beamforming and User-Centric Clustering for Downlink Cloud Radio
  Access Network</title><categories>cs.IT math.IT</categories><comments>14 pages, 9 figures, to appear in IEEE Access, Special Issue on
  Recent Advances in Cloud Radio Access Networks, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a downlink cloud radio access network (C-RAN) in which
all the base-stations (BSs) are connected to a central computing cloud via
digital backhaul links with finite capacities. Each user is associated with a
user-centric cluster of BSs; the central processor shares the user's data with
the BSs in the cluster, which then cooperatively serve the user through joint
beamforming. Under this setup, this paper investigates the user scheduling, BS
clustering and beamforming design problem from a network utility maximization
perspective. Differing from previous works, this paper explicitly considers the
per-BS backhaul capacity constraints. We formulate the network utility
maximization problem for the downlink C-RAN under two different models
depending on whether the BS clustering for each user is dynamic or static over
different user scheduling time slots. In the former case, the user-centric BS
cluster is dynamically optimized for each scheduled user along with the
beamforming vector in each time-frequency slot, while in the latter case the
user-centric BS cluster is fixed for each user and we jointly optimize the user
scheduling and the beamforming vector to account for the backhaul constraints.
In both cases, the nonconvex per-BS backhaul constraints are approximated using
the reweighted l1-norm technique. This approximation allows us to reformulate
the per-BS backhaul constraints into weighted per-BS power constraints and
solve the weighted sum rate maximization problem through a generalized weighted
minimum mean square error approach. This paper shows that the proposed dynamic
clustering algorithm can achieve significant performance gain over existing
naive clustering schemes. This paper also proposes two heuristic static
clustering schemes that can already achieve a substantial portion of the gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5021</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5021</id><created>2014-10-18</created><authors><author><keyname>Liu</keyname><forenames>Shuiyin</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Unshared Secret Key Cryptography</title><categories>cs.CR</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current security techniques can be implemented with either secret key
exchange or physical layer wiretap codes. In this work, we investigate an
alternative solution for MIMO wiretap channels. Inspired by the artificial
noise (AN) technique, we propose the unshared secret key (USK) cryptosystem,
where the AN is redesigned as a one-time pad secret key aligned within the null
space between transmitter and legitimate receiver. The proposed USK
cryptosystem is a new physical layer cryptographic scheme, obtained by
combining traditional network layer cryptography and physical layer security.
Unlike previously studied artificial noise techniques, rather than ensuring
non-zero secrecy capacity, the USK is valid for an infinite lattice input
alphabet and guarantees Shannon's ideal secrecy and perfect secrecy, without
the need of secret key exchange. We then show how ideal secrecy can be obtained
for finite lattice constellations with an arbitrarily small outage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5024</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5024</id><created>2014-10-18</created><authors><author><keyname>Jiang</keyname><forenames>Shuyang</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Block-Sparsity-Induced Adaptive Filter for Multi-Clustering System
  Identification</title><categories>cs.IT math.IT</categories><comments>29 pages, 11 figure, journal manuscript</comments><doi>10.1109/TSP.2015.2453133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to improve the performance of least mean square (LMS)-based adaptive
filtering for identifying block-sparse systems, a new adaptive algorithm called
block-sparse LMS (BS-LMS) is proposed in this paper. The basis of the proposed
algorithm is to insert a penalty of block-sparsity, which is a mixed \$l_{2,
0}\$ norm of adaptive tap-weights with equal group partition sizes, into the
cost function of traditional LMS algorithm. To describe a block-sparse system
response, we first propose a Markov-Gaussian model, which can generate a kind
of system responses of arbitrary average sparsity and arbitrary average block
length using given parameters. Then we present theoretical expressions of the
steady-state misadjustment and transient convergence behavior of BS-LMS with an
appropriate group partition size for white Gaussian input data. Based on the
above results, we theoretically demonstrate that BS-LMS has much better
convergence behavior than \$l_0\$-LMS with the same small level of
misadjustment. Finally, numerical experiments verify that all of the
theoretical analysis agrees well with simulation results in a large range of
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5028</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5028</id><created>2014-10-18</created><authors><author><keyname>Riechers</keyname><forenames>P. M.</forenames></author><author><keyname>Varn</keyname><forenames>D. P.</forenames></author><author><keyname>Crutchfield</keyname><forenames>J. P.</forenames></author></authors><title>Diffraction Patterns of Layered Close-packed Structures from Hidden
  Markov Models</title><categories>cond-mat.mtrl-sci cs.IT math.IT math.ST stat.TH</categories><comments>18 pages, 6 figures, 3 tables;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/dplcps.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recently derived analytical expressions for the pairwise (auto)correlation
functions (CFs) between modular layers (MLs) in close-packed structures (CPSs)
for the wide class of stacking processes describable as hidden Markov models
(HMMs) [Riechers \etal, (2014), Acta Crystallogr.~A, XX 000-000]. We now use
these results to calculate diffraction patterns (DPs) directly from HMMs,
discovering that the relationship between the HMMs and DPs is both simple and
fundamental in nature. We show that in the limit of large crystals, the DP is a
function of parameters that specify the HMM. We give three elementary but
important examples that demonstrate this result, deriving expressions for the
DP of CPSs stacked (i) independently, (ii) as infinite-Markov-order randomly
faulted 2H and 3C stacking structures over the entire range of growth and
deformation faulting probabilities, and (iii) as a HMM that models
Shockley-Frank stacking faults in 6H-SiC. While applied here to planar faulting
in CPSs, extending the methods and results to planar disorder in other layered
materials is straightforward. In this way, we effectively solve the broad
problem of calculating a DP---either analytically or numerically---for any
stacking structure---ordered or disordered---where the stacking process can be
expressed as a HMM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5029</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5029</id><created>2014-10-18</created><authors><author><keyname>Fern&#xe1;ndez</keyname><forenames>David C. Del Rey</forenames></author><author><keyname>Zingg</keyname><forenames>David W.</forenames></author></authors><title>Generalized Summation-by-Parts Operators for the Second Derivative with
  Variable Coefficients</title><categories>cs.NA math.NA</categories><comments>28 pages, 5 figures</comments><msc-class>65M06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The comprehensive generalization of summation-by-parts of Del Rey Fern\'andez
et al.\ (J. Comput. Phys., 266, 2014) is extended to approximations of second
derivatives with variable coefficients. This enables the construction of
second-derivative operators with one or more of the following characteristics:
i) non-repeating interior stencil, ii) nonuniform nodal distributions, and iii)
exclusion of one or both boundary nodes. Definitions are proposed that give
rise to generalized SBP operators that result in consistent, conservative, and
stable discretizations of PDEs with or without mixed derivatives. It is proven
that such operators can be constructed using a correction to the application of
the first-derivative operator twice that is the same as used for the
constant-coefficient operator. Moreover, for operators with a repeating
interior stencil, a decomposition is proposed that makes the application of
such operators particularly simple. A number of novel operators are
constructed, including operators on pseudo-spectral nodal distributions and
operators that have a repeating interior stencil, but unequal nodal spacing
near boundaries. The various operators are compared to the application of the
first-derivative operator twice in the context of the linear
convection-diffusion equation with constant and variable coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5037</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5037</id><created>2014-10-19</created><authors><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames></author></authors><title>Decidable Fragments of Logics Based on Team Semantics</title><categories>cs.LO cs.CC math.LO</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of variants of dependence logic defined by
generalized dependency atoms. Let FOC^2 denote two-variable logic with
counting, and let ESO(FOC^2) be the extension of FOC^2 with existential
second-order prenex quantification. We show that for any finite collection A of
atoms that are definable in ESO(FOC^2), the satisfiability problem of the
two-variable fragment of FO(A) is NEXPTIME-complete. We also study
satisfiability of sentences of FO(A) in the Bernays-Sch\&quot;onfinkel-Ramsey prefix
class. Our results show that, analogously to the first-order case, this problem
is decidable assuming the atoms in A are uniformly polynomial time computable
and closed under substructures. We establish inclusion in 2NEXPTIME. For fixed
arity vocabularies, we establish NEXPTIME-completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5038</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5038</id><created>2014-10-19</created><authors><author><keyname>Sano</keyname><forenames>Katsuhiko</forenames></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames></author></authors><title>Axiomatizing Propositional Dependence Logics</title><categories>cs.LO math.LO</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give sound and complete Hilbert-style axiomatizations for propositional
dependence logic (PD), modal dependence logic (MDL), and extended modal
dependence logic (EMDL) by extending existing axiomatizations for propositional
logic and modal logic. In addition, we give novel labeled tableau calculi for
PD, MDL, and EMDL. We prove soundness, completeness and termination for each of
the labeled calculi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5040</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5040</id><created>2014-10-19</created><authors><author><keyname>Wen</keyname><forenames>Jinming</forenames></author><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author></authors><title>Success probability of the Babai estimators for box-constrained integer
  linear models</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To estimate the box-constrained integer parameter vector $\hbx$ in a linear
model, a typical method is to solve a box-constrained integer least squares
(BILS) problem. However, due to its high complexity, the box-constrained Babai
integer point $\x^\sBB$ is commonly used as a suboptimal solution.
  First we derive formulas for the success probability $P^\sBB$ of $\x^\sBB$
and the success probability $P^\sOB$ of the ordinary Babai integer point
$\x^\sOB$ when $\hbx$ is uniformly distributed over the constraint box. Some
properties of $P^\sBB$ and $P^\sOB$ and the relationship between them are
studied.
  Then we investigate the effects of some column permutation strategies on
$\P^\sBB$. The V-BLAST and SQRD column permutation strategies are often used to
get better Babai integer points. The permutation strategy involved in the
well-known LLL lattice reduction, to be referred to as LLL-P, can also be used
for this purpose. On the one hand we show that under a condition LLL-P always
increases $P^\sBB$ and argue why both V-BLAST and SQRD often increase $P^\sBB$
under the same condition; and on the other hand we show that under an opposite
condition LLL-P always decreases $P^\sBB$ and argue why both V-BLAST and SQRD
often decrease $P^\sBB$ under the same condition. We also derive a column
permutation invariant bound on $P^\sBB$, which is an upper bound and a lower
bound under the two opposite conditions, respectively. Numerical results will
be given to demonstrate our findings.
  Finally we consider a conjecture concerning the ordinary Babai integer point
proposed by Ma et al. We first construct an example to show that the conjecture
does not hold in general, and then show that the conjecture does hold under
some conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5053</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5053</id><created>2014-10-19</created><updated>2015-03-26</updated><authors><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Gowers Norm, Function Limits, and Parameter Estimation</title><categories>cs.CC cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1212.3849, arXiv:1308.4108
  by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\{f_i:\mathbb{F}_p^i \to \{0,1\}\}$ be a sequence of functions, where
$p$ is a fixed prime and $\mathbb{F}_p$ is the finite field of order $p$. The
limit of the sequence can be syntactically defined using the notion of
ultralimit. Inspired by the Gowers norm, we introduce a metric over limits of
function sequences, and study properties of it. One application of this metric
is that it provides a characterization of affine-invariant parameters of
functions that are constant-query estimable. Using this characterization, we
show that the property of being a function of a constant number of low-degree
polynomials and a constant number of factored polynomials (of arbitrary
degrees) is constant-query testable if it is closed under blowing-up. Examples
of this property include the property of having a constant spectral norm and
degree-structural properties with rank conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5055</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5055</id><created>2014-10-19</created><authors><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Shen</keyname><forenames>Yanning</forenames></author><author><keyname>Li</keyname><forenames>Fuwei</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author></authors><title>Prior Support Knowledge-Aided Sparse Bayesian Learning with Partly
  Erroneous Support Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown both experimentally and theoretically that sparse signal
recovery can be significantly improved given that part of the signal's support
is known \emph{a priori}. In practice, however, such prior knowledge is usually
inaccurate and contains errors. Using such knowledge may result in severe
performance degradation or even recovery failure. In this paper, we study the
problem of sparse signal recovery when partial but partly erroneous prior
knowledge of the signal's support is available. Based on the conventional
sparse Bayesian learning framework, we propose a modified two-layer
Gaussian-inverse Gamma hierarchical prior model and, moreover, an improved
three-layer hierarchical prior model. The modified two-layer model employs an
individual parameter $b_i$ for each sparsity-controlling hyperparameter
$\alpha_i$, and has the ability to place non-sparsity-encouraging priors to
those coefficients that are believed in the support set. The three-layer
hierarchical model is built on the modified two-layer prior model, with a prior
placed on the parameters $\{b_i\}$ in the third layer. Such a model enables to
automatically learn the true support from partly erroneous information through
learning the values of the parameters $\{b_i\}$. Variational Bayesian
algorithms are developed based on the proposed hierarchical prior models.
Numerical results are provided to illustrate the performance of the proposed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5056</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5056</id><created>2014-10-19</created><updated>2015-10-21</updated><authors><author><keyname>Iosif</keyname><forenames>Radu</forenames></author><author><keyname>Rogalewicz</keyname><forenames>Adam</forenames></author><author><keyname>Vojnar</keyname><forenames>Tomas</forenames></author></authors><title>Abstraction Refinement for Trace Inclusion of Infinite State Systems</title><categories>cs.LO cs.FL</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A \emph{data automaton} is a finite automaton equipped with variables
(counters or registers) ranging over infinite data domains. A trace of a data
automaton is an alternating sequence of alphabet symbols and values taken by
the counters during an execution of the automaton. The problem addressed in
this paper is the inclusion between the sets of traces (data languages)
recognized by such automata. Since the problem is undecidable in general, we
give a semi-algorithm based on abstraction refinement, which is proved to be
sound and complete, but whose termination is not guaranteed. We have
implemented our technique in a~prototype tool and show promising results on
several non-trivial examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5058</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5058</id><created>2014-10-19</created><authors><author><keyname>Gilani</keyname><forenames>Syed Zulqarnain</forenames></author><author><keyname>Shafait</keyname><forenames>Faisal</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal</forenames></author></authors><title>Dense 3D Face Correspondence</title><categories>cs.CV</categories><comments>24 Pages, 12 Figures, 6 Tables and 3 Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that automatically establishes dense correspondences
between a large number of 3D faces. Starting from automatically detected sparse
correspondences on the convex hull of 3D faces, the algorithm triangulates
existing correspondences and expands them iteratively along the triangle edges.
New correspondences are established by matching keypoints on the geodesic
patches along the triangle edges and the process is repeated. After exhausting
keypoint matches, further correspondences are established by evolving level set
geodesic curves from the centroids of large triangles. A deformable model
(K3DM) is constructed from the dense corresponded faces and an algorithm is
proposed for morphing the K3DM to fit unseen faces. This algorithm iterates
between rigid alignment of an unseen face followed by regularized morphing of
the deformable model. We have extensively evaluated the proposed algorithms on
synthetic data and real 3D faces from the FRGCv2 and BU3DFE databases using
quantitative and qualitative benchmarks. Our algorithm achieved dense
correspondences with a mean localization error of 1.28mm on synthetic faces and
detected 18 anthropometric landmarks on unseen real faces from the FRGCv2
database with 3mm precision. Furthermore, our deformable model fitting
algorithm achieved 99.8% gender classification and 98.3% face recognition
accuracy on the FRGCv2 database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5062</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5062</id><created>2014-10-19</created><updated>2015-04-20</updated><authors><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>Mixing Color Coding-Related Techniques</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Narrow sieves, representative sets and divide-and-color are three
breakthrough color coding-related techniques, which led to the design of
extremely fast parameterized algorithms. We present a novel family of
strategies for applying mixtures of them. This includes: (a) a mix of
representative sets and narrow sieves; (b) a faster computation of
representative sets under certain separateness conditions, mixed with
divide-and-color and a new technique, &quot;balanced cutting&quot;; (c) two mixtures of
representative sets, iterative compression and a new technique, &quot;unbalanced
cutting&quot;. We demonstrate our strategies by obtaining, among other results,
significantly faster algorithms for $k$-Internal Out-Branching and Weighted
3-Set $k$-Packing, and a framework for speeding-up the previous best
deterministic algorithms for $k$-Path, $k$-Tree, $r$-Dimensional $k$-Matching,
Graph Motif and Partial Cover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5072</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5072</id><created>2014-10-19</created><authors><author><keyname>Pitsilis</keyname><forenames>Georgios</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Harnessing the power of Social Bookmarking for improving tag-based
  Recommendations</title><categories>cs.SI cs.IR</categories><comments>28 pages, 10 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social bookmarking and tagging has emerged a new era in user collaboration.
Collaborative Tagging allows users to annotate content of their liking, which
via the appropriate algorithms can render useful for the provision of product
recommendations. It is the case today for tag-based algorithms to work
complementary to rating-based recommendation mechanisms to predict the user
liking to various products. In this paper we propose an alternative algorithm
for computing personalized recommendations of products, that uses exclusively
the tags provided by the users. Our approach is based on the idea of using the
semantic similarity of the user-provided tags for clustering them into groups
of similar meaning. Afterwards, some measurable characteristics of users'
Annotation Competency are combined with other metrics, such as user similarity,
for computing predictions. The evaluation on data used from a real-world
collaborative tagging system, citeUlike, confirmed that our approach
outperforms the baseline Vector Space model, as well as other state of the art
algorithms, predicting the user liking more accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5076</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5076</id><created>2014-10-19</created><updated>2014-10-21</updated><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author><author><keyname>Pesavento</keyname><forenames>Marius</forenames></author></authors><title>A Parallel Stochastic Approximation Method for Nonconvex Multi-Agent
  Optimization Problems</title><categories>cs.MA math.OC</categories><comments>Part of this work has been presented at IEEE SPAWC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of minimizing the expected value of a (possibly
nonconvex) cost function parameterized by a random (vector) variable, when the
expectation cannot be computed accurately (e.g., because the statistics of the
random variables are unknown and/or the computational complexity is
prohibitive). Classical sample stochastic gradient methods for solving this
problem may empirically suffer from slow convergence. In this paper, we propose
for the first time a stochastic parallel Successive Convex Approximation-based
(best-response) algorithmic framework for general nonconvex stochastic
sum-utility optimization problems, which arise naturally in the design of
multi-agent systems. The proposed novel decomposition enables all users to
update their optimization variables in parallel by solving a sequence of
strongly convex subproblems, one for each user. Almost surely convergence to
stationary points is proved. We then customize our algorithmic framework to
solve the stochastic sum rate maximization problem over
Single-Input-Single-Output (SISO) frequency-selective interference channels,
multiple-input-multiple-output (MIMO) interference channels, and MIMO
multiple-access channels. Numerical results show that our algorithms are much
faster than state-of-the-art stochastic gradient schemes while achieving the
same (or better) sum-rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5077</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5077</id><created>2014-10-19</created><authors><author><keyname>Waites</keyname><forenames>William</forenames></author></authors><title>On the Provenance of Linked Data Statistics</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As the amount of linked data published on the web grows, attempts are being
made to describe and measure it. However even basic statistics about a graph,
such as its size, are difficult to express in a uniform and predictable way. In
order to be able to sensibly interpret a statistic it is necessary to know how
it was calculate. In this paper we survey the nature of the problem and outline
a strategy for addressing it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5078</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5078</id><created>2014-10-19</created><authors><author><keyname>Pareti</keyname><forenames>Paolo</forenames></author><author><keyname>Klein</keyname><forenames>Ewan</forenames></author></authors><title>Learning Vague Concepts for the Semantic Web</title><categories>cs.AI cs.CL</categories><comments>The 10th International Semantic Web Conference (ISWC 2011), Joint
  Workshop on Knowledge Evolution and Ontology Dynamics. Bonn, Germany (2011)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Ontologies can be a powerful tool for structuring knowledge, and they are
currently the subject of extensive research. Updating the contents of an
ontology or improving its interoperability with other ontologies is an
important but difficult process. In this paper, we focus on the presence of
vague concepts, which are pervasive in natural language, within the framework
of formal ontologies. We will adopt a framework in which vagueness is captured
via numerical restrictions that can be automatically adjusted. Since updating
vague concepts, either through ontology alignment or ontology evolution, can
lead to inconsistent sets of axioms, we define and implement a method to
detecting and repairing such inconsistencies in a local fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5083</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5083</id><created>2014-10-19</created><updated>2015-03-14</updated><authors><author><keyname>Paulson</keyname><forenames>Joel A.</forenames></author><author><keyname>Streif</keyname><forenames>Stefan</forenames></author><author><keyname>Mesbah</keyname><forenames>Ali</forenames></author></authors><title>Stability for Receding-horizon Stochastic Model Predictive Control</title><categories>cs.SY math.OC</categories><comments>American Control Conference (ACC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stochastic model predictive control (SMPC) approach is presented for
discrete-time linear systems with arbitrary time-invariant probabilistic
uncertainties and additive Gaussian process noise. Closed-loop stability of the
SMPC approach is established by appropriate selection of the cost function.
Polynomial chaos is used for uncertainty propagation through system dynamics.
The performance of the SMPC approach is demonstrated using the Van de Vusse
reactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5085</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5085</id><created>2014-10-19</created><authors><author><keyname>Ashraphijuo</keyname><forenames>Mehdi</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>On the Capacity Regions of Two-Way Diamond Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Inf. Th., Oct 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the capacity regions of two-way diamond channels. We
show that for a linear deterministic model the capacity of the diamond channel
in each direction can be simultaneously achieved for all values of channel
parameters, where the forward and backward channel parameters are not
necessarily the same. We divide the achievability scheme into three cases,
depending on the forward and backward channel parameters. For the first case,
we use a reverse amplify-and-forward strategy in the relays. For the second
case, we use four relay strategies based on the reverse amplify-and-forward
with some modifications in terms of replacement and repetition of some stream
levels. For the third case, we use two relay strategies based on performing two
rounds of repetitions in a relay. The proposed schemes for deterministic
channels are used to find the capacity regions within constant gaps for two
special cases of the Gaussian two-way diamond channel. First, for the general
Gaussian two-way relay channel with a simple coding scheme the smallest gap is
achieved compared to the prior works. Then, a special symmetric Gaussian
two-way diamond model is considered and the capacity region is achieved within
four bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5088</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5088</id><created>2014-10-19</created><authors><author><keyname>David</keyname><forenames>Cristina</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Lewis</keyname><forenames>Matt</forenames></author></authors><title>Propositional Reasoning about Safety and Termination of
  Heap-Manipulating Programs</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that it is possible to reason about the safety and
termination of programs handling potentially cyclic, singly-linked lists using
propositional reasoning even when the safety invariants and termination
arguments depend on constraints over the lengths of lists. For this purpose, we
propose the theory SLH of singly-linked lists with length, which is able to
capture non-trivial interactions between shape and arithmetic. When using the
theory of bit-vector arithmetic as a background, SLH is efficiently decidable
via a reduction to SAT. We show the utility of SLH for software verification by
using it to express safety invariants and termination arguments for programs
manipulating potentially cyclic, singly-linked lists with unrestricted,
unspecified sharing. We also provide an implementation of the decision
procedure and use it to check safety and termination proofs for several
heap-manipulating programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5089</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5089</id><created>2014-10-19</created><authors><author><keyname>David</keyname><forenames>Cristina</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Lewis</keyname><forenames>Matt</forenames></author></authors><title>Unrestricted Termination and Non-Termination Arguments for Bit-Vector
  Programs</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proving program termination is typically done by finding a well-founded
ranking function for the program states. Existing termination provers typically
find ranking functions using either linear algebra or templates. As such they
are often restricted to finding linear ranking functions over mathematical
integers. This class of functions is insufficient for proving termination of
many terminating programs, and furthermore a termination argument for a program
operating on mathematical integers does not always lead to a termination
argument for the same program operating on fixed-width machine integers. We
propose a termination analysis able to generate nonlinear, lexicographic
ranking functions and nonlinear recurrence sets that are correct for
fixed-width machine arithmetic and floating-point arithmetic Our technique is
based on a reduction from program \emph{termination} to second-order
\emph{satisfaction}. We provide formulations for termination and
non-termination in a fragment of second-order logic with restricted
quantification which is decidable over finite domains. The resulted technique
is a sound and complete analysis for the termination of finite-state programs
with fixed-width integers and IEEE floating-point arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5092</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5092</id><created>2014-10-19</created><updated>2015-06-03</updated><authors><author><keyname>Safdar</keyname><forenames>Muhammad</forenames></author><author><keyname>Luo</keyname><forenames>Ming Ronnier</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoyu</forenames></author></authors><title>Comparing CSI and PCA in Amalgamation with JPEG for Spectral Image
  Compression</title><categories>cs.MM</categories><comments>Published in Proceedings of AIC2015, Tokyo, Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuing our previous research on color image compression, we move towards
spectral image compression. This enormous amount of data needs more space to
store and more time to transmit. To manage this sheer amount of data,
researchers have investigated different techniques so that image quality can be
conserved and compressibility can be improved. The principle component analysis
(PCA) can be employed to reduce the dimensions of spectral images to achieve
high compressibility and performance. Due to processing complexity of PCA, a
simple interpolation technique called cubic spline interpolation (CSI) was
considered to reduce the dimensionality of spectral domain of spectral images.
The CSI and PCA were employed one by one in the spectral domain and were
amalgamated with the JPEG, which was employed in spatial domain. Three measures
including compression rate (CR), processing time (Tp) and color difference
CIEDE2000 were used for performance analysis. Test results showed that for a
fixed value of compression rate, CSI based algorithm performed poor in terms of
dE00, in comparison with PCA, but is still reliable because of small color
difference. On the other hand it has lower complexity and is computationally
much better as compared to PCA based algorithm, especially for spectral images
with large size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5102</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5102</id><created>2014-10-19</created><authors><author><keyname>Didona</keyname><forenames>Diego</forenames></author><author><keyname>Romano</keyname><forenames>Paolo</forenames></author></authors><title>On Bootstrapping Machine Learning Performance Predictors via Analytical
  Models</title><categories>cs.PF cs.LG</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance modeling typically relies on two antithetic methodologies: white
box models, which exploit knowledge on system's internals and capture its
dynamics using analytical approaches, and black box techniques, which infer
relations among the input and output variables of a system based on the
evidences gathered during an initial training phase. In this paper we
investigate a technique, which we name Bootstrapping, which aims at reconciling
these two methodologies and at compensating the cons of the one with the pros
of the other. We thoroughly analyze the design space of this gray box modeling
technique, and identify a number of algorithmic and parametric trade-offs which
we evaluate via two realistic case studies, a Key-Value Store and a Total Order
Broadcast service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5103</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5103</id><created>2014-10-19</created><authors><author><keyname>Crawford</keyname><forenames>Joseph</forenames></author><author><keyname>Milenkovi&#x107;</keyname><forenames>Tijana</forenames></author></authors><title>GREAT: GRaphlet Edge-based network AlignmenT</title><categories>q-bio.MN cs.CE</categories><comments>16 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network alignment aims to find regions of topological or functional
similarities between networks. In computational biology, it can be used to
transfer biological knowledge from a well-studied species to a poorly-studied
species between aligned network regions. Typically, existing network aligners
first compute similarities between nodes in different networks (via a node cost
function) and then aim to find a high-scoring alignment (node mapping between
the networks) with respect to &quot;node conservation&quot;, typically the total node
cost function over all aligned nodes. Only after an alignment is constructed,
the existing methods evaluate its quality with respect to an alternative
measure, such as &quot;edge conservation&quot;. Thus, we recently aimed to directly
optimize edge conservation while constructing an alignment, which improved
alignment quality. Here, we approach a novel idea of maximizing both node and
edge conservation, and we also approach this idea from a novel perspective, by
aligning optimally edges between networks first in order to improve node cost
function needed to then align well nodes between the networks. In the process,
unlike the existing measures of edge conservation that treat each conserved
edge the same, we favor conserved edges that are topologically similar over
conserved edges that are topologically dissimilar. We show that our novel
method, which we call GRaphlet Edge AlignmenT (GREAT), improves upon
state-of-the-art methods that aim to optimize node conservation only or edge
conservation only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5105</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5105</id><created>2014-10-19</created><authors><author><keyname>Guruganesh</keyname><forenames>Guru</forenames></author><author><keyname>Sanita</keyname><forenames>Laura</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Improved Region-Growing and Combinatorial Algorithms for $k$-Route Cut
  Problems</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2; G.1.6; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the {\em $k$-route} generalizations of various cut problems, the
most general of which is \emph{$k$-route multicut} ($k$-MC) problem, wherein we
have $r$ source-sink pairs and the goal is to delete a minimum-cost set of
edges to reduce the edge-connectivity of every source-sink pair to below $k$.
The $k$-route extensions of multiway cut ($k$-MWC), and the minimum $s$-$t$ cut
problem ($k$-$(s,t)$-cut), are similarly defined. We present various
approximation and hardness results for these $k$-route cut problems that
improve the state-of-the-art for these problems in several cases. (i) For {\em
$k$-route multiway cut}, we devise simple, but surprisingly effective,
combinatorial algorithms that yield bicriteria approximation guarantees that
markedly improve upon the previous-best guarantees. (ii) For {\em $k$-route
multicut}, we design algorithms that improve upon the previous-best
approximation factors by roughly an $O(\sqrt{\log r})$-factor, when $k=2$, and
for general $k$ and unit costs and any fixed violation of the connectivity
threshold $k$. The main technical innovation is the definition of a new,
powerful \emph{region growing} lemma that allows us to perform region-growing
in a recursive fashion even though the LP solution yields a {\em different
metric} for each source-sink pair. (iii) We complement these results by showing
that the {\em $k$-route $s$-$t$ cut} problem is at least as hard to approximate
as the {\em densest-$k$-subgraph} (DkS) problem on uniform hypergraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5107</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5107</id><created>2014-10-19</created><authors><author><keyname>Liu</keyname><forenames>Tang</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>The DoF of the Asymmetric MIMO Interference Channel with Square Direct
  Link Channel Matrices</title><categories>cs.IT math.IT</categories><comments>Presented at 52nd Allerton Conference, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the sum Degrees of Freedom (DoF) of $K$-user {\em
asymmetric} MIMO Interference Channel (IC) with square direct link channel
matrices, that is, the $u$-th transmitter and its intended receiver have
$M_u\in\mathbb{N}$ antennas each, where $M_u$ need not be the same for all
$u\in[1:K]$.
  Starting from a $3$-user example, it is shown that existing cooperation-based
outer bounds are insufficient to characterize the DoF. Moreover, it is shown
that two distinct operating regimes exist. With a {\it dominant} user, i.e., a
user that has more antennas than the other two users combined, %(say $M_1\geq
M_2+M_3$), it is DoF optimal to let that user transmit alone on the IC.
Otherwise, it is DoF optimal to {\em decompose} and operate the 3-user MIMO IC
as an $(M_1+ M_2+M_3)$-user SISO IC. This indicates that MIMO operations are
useless from a DoF perspective in systems without a dominant user.
  The main contribution of the paper is the derivation of a novel outer bound
for the general $K$-user case that is tight in the regime where a dominant user
is not present; this is done by generalizing the insights from the 3-user
example to an arbitrary number of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5111</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5111</id><created>2014-10-19</created><authors><author><keyname>Giraldo</keyname><forenames>Jairo</forenames></author><author><keyname>C&#xe1;rdenas</keyname><forenames>Alvaro</forenames></author><author><keyname>Quijano</keyname><forenames>Nicanor</forenames></author></authors><title>Attenuating the Impact of Integrity Attacks on Real-Time Pricing in
  Smart Grids</title><categories>cs.SY</categories><comments>15 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vulnerability of false data injection attacks on real-time electricity
pricing for the power grid market has been recently explored. Previous work has
focused on the impact caused by attackers that compromise pricing signals and
send false prices to a subset of consumers. In this paper we extend previous
work by considering a more powerful and general adversary model, a new analysis
method based on sensitivity functions, and by proposing several countermeasures
that can mitigate the negative impact of these attacks. Countermeasures include
adding a low-pass filter to the pricing signals, selecting the time interval
between price updates, selecting parameters of the controller, designing robust
control algorithms, and by detecting anomalies in the behavior of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5119</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5119</id><created>2014-10-19</created><authors><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Stability and Continuity of Centrality Measures in Weighted Graphs</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a formal definition of stability for node centrality
measures in weighted graphs. It is shown that the commonly used measures of
degree, closeness and eigenvector centrality are stable whereas betweenness
centrality is not. An alternative definition of the latter that preserves the
same centrality notion while satisfying the stability criteria is introduced.
Continuity is presented as a less stringent alternative to stability.
Betweenness centrality is shown to be not only unstable but discontinuous.
Numerical experiments in synthetic random networks and real-world data show
that, in practice, stability and continuity imply different levels of
robustness in the presence of noisy data. In particular, the stable betweenness
centrality is shown to exhibit resilience against noise that is absent in the
discontinuous and unstable standard betweenness centrality, while preserving a
similar notion of centrality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5126</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5126</id><created>2014-10-19</created><authors><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author></authors><title>Strongly Secure Quantum Ramp Secret Sharing Constructed from Algebraic
  Curves over Finite Fields</title><categories>quant-ph cs.CR cs.IT math.AG math.IT</categories><comments>svjour3.cls, 8 pages, no figure</comments><msc-class>81P94 (Primary) 94A62, 94B27 (Secondary)</msc-class><doi>10.1007/s11128-014-0863-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first construction of strongly secure quantum ramp secret sharing by
Zhang and Matsumoto had an undesirable feature that the dimension of quantum
shares must be larger than the number of shares. By using algebraic curves over
finite fields, we propose a new construction in which the number of shares can
become arbitrarily large for fixed dimension of shares.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5127</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5127</id><created>2014-10-13</created><authors><author><keyname>Elmannai</keyname><forenames>Wafa</forenames></author><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author><author><keyname>Elleithy</keyname><forenames>Khaled</forenames></author></authors><title>Simulation based Study of TCP Variants in Hybrid Network</title><categories>cs.NI</categories><comments>09 pages, 09 pages, In proceedings of international conference on
  2011 ASEE Northeast Section Conference, At University of Hartford,
  Connecticut, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission control protocol (TCP) was originally designed for fixed
networks to provide the reliability of the data delivery. The improvement of
TCP performance was also achieved with different types of networks with
introduction of new TCP variants. However, there are still many factors that
affect performance of TCP. Mobility is one of the major affects on TCP
performance in wireless networks and MANET (Mobile Ad Hoc Network). To
determine the best TCP variant from mobility point of view, we simulate some
TCP variants in real life scenario. This paper addresses the performance of TCP
variants such as TCP-Tahoe, TCP-Reno, TCP-New Reno, TCPVegas,TCP-SACK and
TCP-Westwood from mobility point of view.The scenarios presented in this paper
are supported by Zone routing Protocol (ZRP) with integration of random
waypoint mobility model in MANET area. The scenario shows the speed of walking
person to a vehicle and suited particularly for mountainous and deserted areas.
On the basis of simulation, we analyze Round trip time (RTT) fairness,
End-to-End delay, control overhead, number of broken links during the delivery
of data. Finally analyzed parameters help to find out the best TCP variant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5128</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5128</id><created>2014-10-13</created><authors><author><keyname>Alabass</keyname><forenames>Abeer</forenames></author><author><keyname>Elleithy</keyname><forenames>Khaled</forenames></author><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author></authors><title>Dynamic Cluster Head Node Election (DCHNE) Model over Wireless Sensor
  Networks (WSNs)</title><categories>cs.NI</categories><comments>5 pages, 2 figures, In proceedings of 29th International Conference
  on Computers and Their Applications (CATA 2014), Las Vegas, Nevada, USA,24-26
  March 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WSNs are becoming an appealing research area due to their several application
domains. The performance of WSNs depends on the topology of sensors and their
ability to adapt to changes in the network. Sensor nodes are often resource
constrained by their limited power, less communication distance capacity, and
restricted sensing capability. Therefore, they need to cooperate with each
other to accomplish a specific task. Thus, clustering enables sensor nodes to
communicate through the cluster head node for continuous communication process.
In this paper, we introduce a dynamic cluster head election mechanism. Each
node in the cluster calculates its residual energy value to determine its
candidacy to become the Cluster Head Node (CHN). With this mechanism, each
sensor node compares its residual energy level to other nodes in the same
cluster. Depending on the residual energy level the sensor node acts as the
next cluster head. Evaluation of the dynamic CHN election mechanism is
conducted using network simulator-2 (ns2). The simulation results demonstrate
that the proposed approach prolongs the network lifetime and balancing the
energy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5129</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5129</id><created>2014-10-13</created><authors><author><keyname>Elleithy</keyname><forenames>Khaled</forenames></author><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author></authors><title>Fostering of innovative usability testing to develop mobile application
  for mobile collaborative learning (MCL)</title><categories>cs.CY cs.HC</categories><comments>8 pages, 5 figures, ICGST-AIML Journal, Volume 12, Issue 1, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emergence of latest technologies has diverted the focus of people form
Computer-Supported Collaborative Learning (CSCL) to mobile supported
collaborative learning. MCL is highly demanded in educational organizations to
substantiate the pedagogical activities. Some of the MCL supportive
architectures including applications are introduced in several fields to
improve the activities of those organizations, but they need more concise
paradigm to support both types of collaboration: synchronous and asynchronous.
This paper introduces the new preusability testing method that provides
educational support and gives complete picture for developing new pedagogical
group application for MCL. The feature of application includes asynchronous and
synchronous collaboration support. To validate the features of application, we
conduct the post usability testing and heuristic evaluation, which helps in
collecting the empirical data to prove the effectiveness and suitability of
Group application. Further, application aims to improve learning impact and
educate people at anytime and anywhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5131</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5131</id><created>2014-10-19</created><updated>2015-07-13</updated><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>An Algebra of Reversible Computation</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.2960,
  arXiv:1312.0686, arXiv:1404.0665</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design an axiomatization for reversible computation called reversible ACP
(RACP). It has four extendible modules, basic reversible processes algebra
(BRPA), algebra of reversible communicating processes (ARCP), recursion and
abstraction. Just like process algebra ACP in classical computing, RACP can be
treated as an axiomatization foundation for reversible computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5137</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5137</id><created>2014-10-19</created><updated>2014-10-21</updated><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author></authors><title>On Iterative Hard Thresholding Methods for High-dimensional M-Estimation</title><categories>cs.LG stat.ML</categories><comments>20 pages, 3 figures, To appear in the proceedings of the 28th Annual
  Conference on Neural Information Processing Systems, NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of M-estimators in generalized linear regression models in high
dimensional settings requires risk minimization with hard $L_0$ constraints. Of
the known methods, the class of projected gradient descent (also known as
iterative hard thresholding (IHT)) methods is known to offer the fastest and
most scalable solutions. However, the current state-of-the-art is only able to
analyze these methods in extremely restrictive settings which do not hold in
high dimensional statistical models. In this work we bridge this gap by
providing the first analysis for IHT-style methods in the high dimensional
statistical setting. Our bounds are tight and match known minimax lower bounds.
Our results rely on a general analysis framework that enables us to analyze
several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in
the high dimensional regression setting. We also extend our analysis to a large
family of &quot;fully corrective methods&quot; that includes two-stage and partial
hard-thresholding algorithms. We show that our results hold for the problem of
sparse regression, as well as low-rank matrix recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5152</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5152</id><created>2014-10-20</created><authors><author><keyname>Borgs</keyname><forenames>Christian</forenames></author><author><keyname>Chayes</keyname><forenames>Jennifer</forenames></author><author><keyname>Marple</keyname><forenames>Adrian</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Fixed-Points of Social Choice: An Axiomatic Approach to Network
  Communities</title><categories>cs.SI cs.CC cs.DS cs.GT cs.IR</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first social choice theory approach to the question of what
constitutes a community in a social network. Inspired by the classic
preferences models in social choice theory, we start from an abstract social
network framework, called preference networks; these consist of a finite set of
members where each member has a total-ranking preference of all members in the
set.
  Within this framework, we develop two complementary approaches to
axiomatically study the formation and structures of communities. (1) We apply
social choice theory and define communities indirectly by postulating that they
are fixed points of a preference aggregation function obeying certain desirable
axioms. (2) We directly postulate desirable axioms for communities without
reference to preference aggregation, leading to eight natural community axioms.
  These approaches allow us to formulate and analyze community rules. We prove
a taxonomy theorem that provides a structural characterization of the family of
community rules that satisfies all eight axioms. The structure is actually
quite beautiful: these community rules form a bounded lattice under the natural
intersection and union operations. Our structural theorem is complemented with
a complexity result: while identifying a community by the most selective rule
of the lattice is in P, deciding if a subset is a community by the most
comprehensive rule of the lattice is coNP-complete. Our studies also shed light
on the limitations of defining community rules solely based on preference
aggregation: any aggregation function satisfying Arrow's IIA axiom, or based on
commonly used aggregation schemes like the Borda count or generalizations
thereof, lead to communities which violate at least one of our community
axioms. Finally, we give a polynomial-time rule consistent with seven axioms
and weakly satisfying the eighth axiom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5158</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5158</id><created>2014-10-20</created><authors><author><keyname>Peng</keyname><forenames>Dan</forenames></author><author><keyname>Han</keyname><forenames>Xiao-Pu</forenames></author><author><keyname>Wei</keyname><forenames>Zong-Wen</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Punctuated equilibrium dynamics in human communications</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A minimal model based on individual interactions is proposed to study the
non-Poisson statistical properties of human behavior: individuals in the system
interact with their neighbors, the probability of an individual acting
correlates to its activity, and all individuals involved in action will change
their activities randomly. The model creates rich non-Poisson spatial-temporal
properties in the activities of individuals, in agreement with the patterns of
human communication behaviors. Our findings provide insight into various human
activities, embracing a range of realistic social interacting systems,
particularly, intriguing bimodal phenomenons. This model bridges priority
queues and punctuated equilibrium, and our modeling and analysis is likely to
shed light on non-Poisson phenomena in many complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5165</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5165</id><created>2014-10-20</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author></authors><title>Sparsity Methods for Networked Control</title><categories>cs.SY math.OC</categories><comments>Submitted to IEICE SmartCom2014 Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this presentation, we introduce sparsity methods for networked control
systems and show the effectiveness of sparse control. In networked control,
efficient data transmission is important since transmission delay and error can
critically deteriorate the stability and performance. We will show that this
problem is solved by sparse control designed by recent sparse optimization
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5169</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5169</id><created>2014-10-20</created><authors><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Nathan</keyname><forenames>Vikram</forenames></author></authors><title>Hardness of Peeling with Stashes</title><categories>cs.CC</categories><comments>12 pages (including title/abstract), 5 PNG color figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of several algorithms and data structures can be framed as a
peeling process on a random hypergraph: vertices with degree less than k and
their adjacent edges are removed until no vertices of degree less than k are
left. Often the question is whether the remaining hypergraph, the k-core, is
empty or not. In some settings, it may be possible to remove either vertices or
edges from the hypergraph before peeling, at some cost. For example, in hashing
applications where keys correspond to edges and buckets to vertices, one might
use an additional side data structure, commonly referred to as a stash, to
separately handle some keys in order to avoid collisions. The natural question
in such cases is to find the minimum number of edges (or vertices) that need to
be stashed in order to realize an empty k-core. We show that both these
problems are NP-complete for all $k \geq 2$ on graphs and regular hypergraphs,
with the sole exception being that the edge variant of stashing is solvable in
polynomial time for $k = 2$ on standard (2-uniform) graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5185</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5185</id><created>2014-10-20</created><authors><author><keyname>Kwak</keyname><forenames>Haewoon</forenames></author><author><keyname>Blackburn</keyname><forenames>Jeremy</forenames></author></authors><title>Linguistic Analysis of Toxic Behavior in an Online Video Game</title><categories>cs.SI cs.CY</categories><comments>9 pages and 4 figures. Proc. of the 1st EGG (Exploration on Games and
  Gamers) workshop, 2014</comments><acm-class>K.4.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the linguistic components of toxic behavior by using
crowdsourced data from over 590 thousand cases of accused toxic players in a
popular match-based competition game, League of Legends. We perform a series of
linguistic analyses to gain a deeper understanding of the role communication
plays in the expression of toxic behavior. We characterize linguistic behavior
of toxic players and compare it with that of typical players in an online
competition game. We also find empirical support describing how a player
transitions from typical to toxic behavior. Our findings can be helpful to
automatically detect and warn players who may become toxic and thus insulate
potential victims from toxic playing in advance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5186</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5186</id><created>2014-10-20</created><updated>2015-02-27</updated><authors><author><keyname>Dorn</keyname><forenames>Britta</forenames></author><author><keyname>Kr&#xfc;ger</keyname><forenames>Dominikus</forenames></author></authors><title>On the Hardness of Bribery Variants in Voting with CP-Nets</title><categories>cs.GT cs.CC</categories><comments>improved readability (especially of Theorem 2 and some misleading
  formulations), more detailed introduction and conclusion, several typos fixed</comments><doi>10.1007/s10472-015-9469-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue previous work by Mattei et al. (Mattei, N., Pini, M., Rossi, F.,
Venable, K.: Bribery in voting with CP-nets. Ann. of Math. and Artif. Intell.
pp. 1--26 (2013)) in which they study the computational complexity of bribery
schemes when voters have conditional preferences that are modeled by CP-nets.
For most of the cases they considered, they could show that the bribery problem
is solvable in polynomial time. Some cases remained open---we solve two of them
and extend the previous results to the case that voters are weighted. Moreover,
we consider negative (weighted) bribery in CP-nets, when the briber is not
allowed to pay voters to vote for his preferred candidate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5187</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5187</id><created>2014-10-20</created><authors><author><keyname>Benammar</keyname><forenames>Meryem</forenames><affiliation>Shitz</affiliation></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>On the Compound Broadcast Channel: Multiple Description Coding and
  Interference Decoding</title><categories>cs.IT math.IT</categories><comments>77 pages, 8 figures, Submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the general two-user Compound Broadcast Channel (BC)
where an encoder wishes to transmit common and private messages to two
receivers while being oblivious to two possible channel realizations
controlling the communication. The focus is on the characterization of the
largest achievable rate region by resorting to more evolved encoding and
decoding techniques than the conventional coding for the standard BC. The role
of the decoder is first explored, and an achievable rate region is derived
based on the principle of &quot;Interference Decoding&quot; (ID) where each receiver
decodes its intended message and chooses to (non-uniquely) decode or not the
interfering message. This inner bound is shown to be capacity achieving for a
class of non-trivial compound BEC/BSC broadcast channels while the worst-case
of Marton's inner bound -based on &quot;Non Interference Decoding&quot; (NID)- fails to
achieve the capacity region. The role of the encoder is then studied, and an
achievable rate region is derived based on &quot;Multiple Description&quot; (MD) coding
where the encoder transmits a common as well as multiple dedicated private
descriptions to the many instances of the users channels. It turns out that MD
coding outperforms the single description scheme -Common Description (CD)
coding- for a class of compound Multiple Input Single Output Broadcast Channels
(MISO BC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5191</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5191</id><created>2014-10-20</created><updated>2015-01-25</updated><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Wahlstrom</keyname><forenames>Magnus</forenames></author></authors><title>Structural Parameterizations of the Mixed Chinese Postman Problem</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Mixed Chinese Postman Problem (MCPP), given a weighted mixed graph $G$
($G$ may have both edges and arcs), our aim is to find a minimum weight closed
walk traversing each edge and arc at least once. The MCPP parameterized by the
number of edges in $G$ or the number of arcs in $G$ is fixed-parameter
tractable as proved by van Bevern {\em et al.} (in press) and Gutin, Jones and
Sheng (ESA 2014), respectively. In this paper, we consider the unweighted
version of MCPP. Solving an open question of van Bevern {\em et al.} (in
press), we show that somewhat unexpectedly MCPP parameterized by the
(undirected) treewidth of $G$ is W[1]-hard. In fact, we prove that even the
MCPP parameterized by the pathwidth of $G$ is W[1]-hard. On the positive side,
we show that the unweighted version of MCPP parameterized by tree-depth is
fixed-parameter tractable. We are unaware of any natural graph parameters
between pathwidth and tree-depth and so our results provide a dichotomy of the
complexity of MCPP. Furthermore, we believe that MCPP is the first problem
known to be W[1]-hard with respect to treewidth but FPT with respect to
tree-depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5192</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5192</id><created>2014-10-20</created><updated>2014-12-19</updated><authors><author><keyname>Fang</keyname><forenames>Song</forenames></author></authors><title>Three laws of feedback systems: entropy rate never decreases,
  generalized Bode integral, absolute lower bound in variance minimization,
  Gaussianity-whiteness measure (joint Shannon-Wiener entropy),
  Gaussianing-whitening control, and beyond</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>This paper has been withdrawn by the author due to personal reasons</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at obtaining universal laws and absolute lower bounds of
feedback systems using information theory. The feedback system setup is that
with causal plants and causal controllers. Three laws (entropy rate never
decreases, generalized Bode integral, and absolute lower bound in variance
minimization) are obtained, which are in entropy domain, frequency domain, and
time domain, respectively. Those laws characterize the fundamental limitations
of such systems imposed by the feedback mechanism. Two new notions, negentropy
rate and Gaussianity-whiteness measure (joint Shannon-Wiener entropy), are
proposed to facilitate the analysis. Topics such as
whiteness-Gaussianity-variance decomposition, Gaussianing-whitening control
(the maximum Gaussianity-whiteness measure principle), whitening control
(spectrum/spectral flattening control), generalized Bode plot, and so on are
also discussed. The special case of linear time-invariant feedback systems is
considered in the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5197</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5197</id><created>2014-10-20</created><authors><author><keyname>Kartzow</keyname><forenames>Alexander</forenames></author></authors><title>The field of the Reals and the Random Graph are not Finite-Word
  Ordinal-Automatic</title><categories>cs.FL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recently, Schlicht and Stephan lifted the notion of automatic-structures to
the notion of (finite-word) ordinal-automatic structures. These are structures
whose domain and relations can be represented by automata reading finite words
whose shape is some fixed ordinal $\alpha$. We lift Delhomm\'e's
relative-growth-technique from the automatic and tree-automatic setting to the
ordinal-automatic setting. This result implies that the random graph is not
ordinal-automatic and infinite integral domains are not ordinal-automatic with
respect to ordinals below $\omega_1+\omega^\omega$ where $\omega_1$ is the
first uncountable ordinal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5200</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5200</id><created>2014-10-20</created><authors><author><keyname>&#x10c;&#xe1;p</keyname><forenames>Michal</forenames></author><author><keyname>Nov&#xe1;k</keyname><forenames>Peter</forenames></author><author><keyname>Kleiner</keyname><forenames>Alexander</forenames></author></authors><title>Finding Near-optimal Solutions in Multi-robot Path Planning</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We deal with the problem of planning collision-free trajectories for robots
operating in a shared space. Given the start and destination position for each
of the robots, the task is to find trajectories for all robots that reach their
destinations with minimum total cost such that the robots will not collide when
following the found trajectories. Our approach starts from individually optimal
trajectory for each robot, which are then penalized for being in collision with
other robots. The penalty is gradually increased and the individual
trajectories are iteratively replanned to account for the increased penalty
until a collision-free solution is found. Using extensive experimental
evaluation, we find that such a penalty method constructs trajectories with
near-optimal cost on the instances where the optimum is known and otherwise
with 4-10 % lower cost than the trajectories generated by prioritized planning
and up to 40 % cheaper than trajectories generated by local collision avoidance
techniques, such as ORCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5206</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5206</id><created>2014-10-20</created><authors><author><keyname>Hirn</keyname><forenames>Matthew</forenames></author><author><keyname>Widemann</keyname><forenames>David</forenames></author></authors><title>Frames for subspaces of $\mathbb{C}^N$</title><categories>cs.IT math.CA math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a theory of finite frames for subspaces of $\mathbb{C}^N$ . The
definition of a subspace frame is given and results analogous to those from
frame theory for $\mathbb{C}^N$ are proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5209</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5209</id><created>2014-10-20</created><updated>2015-07-11</updated><authors><author><keyname>Shin</keyname><forenames>Kijung</forenames></author><author><keyname>Kang</keyname><forenames>U.</forenames></author></authors><title>Distributed Methods for High-dimensional and Large-scale Tensor
  Factorization</title><categories>cs.NA cs.DB cs.IR</categories><journal-ref>Data Mining (ICDM), 2014 IEEE International Conference on, pp.
  989-994. IEEE, 2014</journal-ref><doi>10.1109/ICDM.2014.78</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a high-dimensional large-scale tensor, how can we decompose it into
latent factors? Can we process it on commodity computers with limited memory?
These questions are closely related to recommender systems, which have modeled
rating data not as a matrix but as a tensor to utilize contextual information
such as time and location. This increase in the dimension requires tensor
factorization methods scalable with both the dimension and size of a tensor. In
this paper, we propose two distributed tensor factorization methods, SALS and
CDTF. Both methods are scalable with all aspects of data, and they show an
interesting trade-off between convergence speed and memory requirements. SALS
updates a subset of the columns of a factor matrix at a time, and CDTF, a
special case of SALS, updates one column at a time. In our experiments, only
our methods factorize a 5-dimensional tensor with 1 billion observable entries,
10M mode length, and 1K rank, while all other state-of-the-art methods fail.
Moreover, our methods require several orders of magnitude less memory than our
competitors. We implement our methods on MapReduce with two widely-applicable
optimization techniques: local disk caching and greedy row assignment. They
speed up our methods up to 98.2X and also the competitors up to 5.9X.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5215</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5215</id><created>2014-10-20</created><authors><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames></author><author><keyname>Revenko</keyname><forenames>Artem</forenames></author></authors><title>Interactive Error Correction in Implicative Theories</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Errors in implicative theories coming from binary data are studied. First,
two classes of errors that may affect implicative theories are singled out. Two
approaches for finding errors of these classes are proposed, both of them based
on methods of Formal Concept Analysis. The first approach uses the cardinality
minimal (canonical or Duquenne-Guigues) implication base. The construction of
such a base is computationally intractable. Using an alternative approach one
checks possible errors on the fly in polynomial time via computing closures of
subsets of attributes. Both approaches are interactive, based on questions
about the validity of certain implications. Results of computer experiments are
presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5224</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5224</id><created>2014-10-20</created><updated>2014-11-14</updated><authors><author><keyname>Gordo</keyname><forenames>Albert</forenames></author></authors><title>Supervised mid-level features for word image representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of learning word image representations:
given the cropped image of a word, we are interested in finding a descriptive,
robust, and compact fixed-length representation. Machine learning techniques
can then be supplied with these representations to produce models useful for
word retrieval or recognition tasks. Although many works have focused on the
machine learning aspect once a global representation has been produced, little
work has been devoted to the construction of those base image representations:
most works use standard coding and aggregation techniques directly on top of
standard computer vision features such as SIFT or HOG.
  We propose to learn local mid-level features suitable for building word image
representations. These features are learnt by leveraging character bounding box
annotations on a small set of training images. However, contrary to other
approaches that use character bounding box information, our approach does not
rely on detecting the individual characters explicitly at testing time. Our
local mid-level features can then be aggregated to produce a global word image
signature. When pairing these features with the recent word attributes
framework of Almaz\'an et al., we obtain results comparable with or better than
the state-of-the-art on matching and recognition tasks using global descriptors
of only 96 dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5240</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5240</id><created>2014-10-20</created><authors><author><keyname>Mukherjee</keyname><forenames>Sudarshan</forenames></author><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author></authors><title>Energy-Spectral Efficiency Trade-off for a Massive SU-MIMO System with
  Transceiver Power Consumption</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE International Conference on Communications (ICC)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a single user (SU) massive MIMO system with multiple antennas at
the transmitter (base station) and a single antenna at the user terminal (UT).
Taking transceiver power consumption into consideration, for a given spectral
efficiency (SE) we maximize the energy efficiency (EE) as a function of the
number of base station (BS) antennas $M$, resulting in a closed-form expression
for the optimal SE-EE trade-off. It is observed that in contrast to the
classical SE-EE trade-off (which considers only the radiated power), with
transceiver power consumption taken into account, the EE increases with
increasing SE when SE is sufficiently small. Further, for a fixed SE we analyze
the impact of varying cell size (i.e., equivalently average channel gain $G_c$)
on the optimal EE. We show the interesting result that for sufficiently small
$G_c$, the optimal EE decreases as $\mathcal{O}(\sqrt{G_c})$ with decreasing
$G_c$. Our analysis also reveals that for sufficiently small SE (or large
$G_c$), the EE is insensitive to the power amplifier efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5242</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5242</id><created>2014-10-20</created><updated>2015-07-29</updated><authors><author><keyname>Kreutzer</keyname><forenames>Moritz</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author><author><keyname>Pieper</keyname><forenames>Andreas</forenames></author><author><keyname>Alvermann</keyname><forenames>Andreas</forenames></author><author><keyname>Fehske</keyname><forenames>Holger</forenames></author></authors><title>Performance Engineering of the Kernel Polynomial Method on Large-Scale
  CPU-GPU Systems</title><categories>cs.CE cond-mat.mes-hall cs.DC cs.PF physics.comp-ph</categories><comments>10 pages, 12 figures</comments><journal-ref>Proceedings of the 2015 IEEE International Parallel and
  Distributed Processing Symposium (IPDPS) 417-426</journal-ref><doi>10.1109/IPDPS.2015.76</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kernel Polynomial Method (KPM) is a well-established scheme in quantum
physics and quantum chemistry to determine the eigenvalue density and spectral
properties of large sparse matrices. In this work we demonstrate the high
optimization potential and feasibility of peta-scale heterogeneous CPU-GPU
implementations of the KPM. At the node level we show that it is possible to
decouple the sparse matrix problem posed by KPM from main memory bandwidth both
on CPU and GPU. To alleviate the effects of scattered data access we combine
loosely coupled outer iterations with tightly coupled block sparse matrix
multiple vector operations, which enables pure data streaming. All
optimizations are guided by a performance analysis and modelling process that
indicates how the computational bottlenecks change with each optimization step.
Finally we use the optimized node-level KPM with a hybrid-parallel framework to
perform large scale heterogeneous electronic structure calculations for novel
topological materials on a petascale-class Cray XC30 system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5257</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5257</id><created>2014-10-20</created><authors><author><keyname>Liu</keyname><forenames>Hui</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Tian</keyname><forenames>Xiaohua</forenames></author><author><keyname>Wang</keyname><forenames>Xinbing</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>On Content-centric Wireless Delivery Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>20 pages, 7 figures,accepted by IEEE Wireless
  Communications,Sept.2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The flux of social media and the convenience of mobile connectivity has
created a mobile data phenomenon that is expected to overwhelm the mobile
cellular networks in the foreseeable future. Despite the advent of 4G/LTE, the
growth rate of wireless data has far exceeded the capacity increase of the
mobile networks. A fundamentally new design paradigm is required to tackle the
ever-growing wireless data challenge.
  In this article, we investigate the problem of massive content delivery over
wireless networks and present a systematic view on content-centric network
design and its underlying challenges. Towards this end, we first review some of
the recent advancements in Information Centric Networking (ICN) which provides
the basis on how media contents can be labeled, distributed, and placed across
the networks. We then formulate the content delivery task into a content rate
maximization problem over a share wireless channel, which, contrasting the
conventional wisdom that attempts to increase the bit-rate of a unicast system,
maximizes the content delivery capability with a fixed amount of wireless
resources. This conceptually simple change enables us to exploit the &quot;content
diversity&quot; and the &quot;network diversity&quot; by leveraging the abundant computation
sources (through application-layer encoding, pushing and caching, etc.) within
the existing wireless networks. A network architecture that enables wireless
network crowdsourcing for content delivery is then described, followed by an
exemplary campus wireless network that encompasses the above concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5263</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5263</id><created>2014-10-20</created><updated>2015-02-20</updated><authors><author><keyname>Livi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Del Vescovo</keyname><forenames>Guido</forenames></author><author><keyname>Rizzi</keyname><forenames>Antonello</forenames></author><author><keyname>Mascioli</keyname><forenames>Fabio Massimo Frattale</forenames></author></authors><title>Building pattern recognition applications with the SPARE library</title><categories>cs.CV cs.MS</categories><comments>Home page: https://sourceforge.net/p/libspare/home/Spare/</comments><acm-class>D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the SPARE C++ library, an open source software tool
conceived to build pattern recognition and soft computing systems. The library
follows the requirement of the generality: most of the implemented algorithms
are able to process user-defined input data types transparently, such as
labeled graphs and sequences of objects, as well as standard numeric vectors.
Here we present a high-level picture of the SPARE library characteristics,
focusing instead on the specific practical possibility of constructing pattern
recognition systems for different input data types. In particular, as a proof
of concept, we discuss two application instances involving clustering of
real-valued multidimensional sequences and classification of labeled graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5288</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5288</id><created>2014-10-20</created><authors><author><keyname>De</keyname><forenames>Parthapratim</forenames></author></authors><title>Computationally -Efficient Algorithms for Multiuser Detection in Short
  Code Wideband CDMA (TDD) Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paper derives and analyzes a novel block Fast Fourier Transform (FFT) based
joint detection algorithm. The paper compares the performance and complexity of
the novel block-FFT based joint detector to that of the Cholesky based joint
detector and single user detection algorithms. The novel algorithm can operate
at chip rate sampling, as well as higher sampling rates, unlike existing
algorithms. For the performance/complexity analysis, the time division duplex
(TDD) mode of a wideband code division multiplex access (WCDMA) is considered.
The results indicate that the performance of the fast FFT based joint detector
is comparable to that of the Cholesky based joint detector, and much superior
to that of single user detection algorithms. On the other hand, the complexity
of the fast FFT based joint detector is significantly lower than that of the
Cholesky based joint detector and less than that of the single user detection
algorithms. For the Cholesky based joint detector, the approximate Cholesky
decomposition is applied. Moreover, the novel method can also be applied to any
generic multiple-input-multiple-output (MIMO) system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5290</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5290</id><created>2014-10-20</created><authors><author><keyname>Soul&#xe9;</keyname><forenames>Robert</forenames></author><author><keyname>Gedik</keyname><forenames>B&#xfc;gra</forenames></author></authors><title>Optimized Disk Layouts for Adaptive Storage of Interaction Graphs</title><categories>cs.DB</categories><comments>Universit\`a della Svizzera italiana Technical Report</comments><report-no>USI-INF-TR-2014-04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are living in an ever more connected world, where data recording the
interactions between people, software systems, and the physical world is
becoming increasingly prevalent. This data often takes the form of a temporally
evolving graph, where entities are the vertices and the interactions between
them are the edges. We call such graphs interaction graphs. Various application
domains, including telecommunications, transportation, and social media, depend
on analytics performed on interaction graphs. The ability to efficiently
support historical analysis over interaction graphs require effective solutions
for the problem of data layout on disk. This paper presents an adaptive disk
layout called the railway layout for optimizing disk block storage for
interaction graphs. The key idea is to divide blocks into one or more
sub-blocks, where each sub-block contains a subset of the attributes, but the
entire graph structure is replicated within each sub-block. This improves query
I/O, at the cost of increased storage overhead. We introduce optimal ILP
formulations for partitioning disk blocks into sub-blocks with overlapping and
non-overlapping attributes. Additionally, we present greedy heuristic
approaches that can scale better compared to the ILP alternatives, yet achieve
close to optimal query I/O. To demonstrate the benefits of the railway layout,
we provide an extensive experimental study comparing our approach to a few
baseline alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5297</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5297</id><created>2014-10-20</created><authors><author><keyname>Cavallo</keyname><forenames>Bren</forenames></author><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author></authors><title>A Polynomial Time Algorithm For The Conjugacy Decision and Search
  Problems in Free Abelian-by-Infinite Cyclic Groups</title><categories>math.GR cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a polynomial time algorithm that solves both the
conjugacy decision and search problems in free abelian-by-infinite cyclic
groups where the input is elements in normal form. We do this by adapting the
work of Bogopolski, Martino, Maslakova, and Ventura in
\cite{bogopolski2006conjugacy} and Bogopolski, Martino, and Ventura in
\cite{bogopolski2010orbit}, to free abelian-by-infinite cyclic groups, and in
certain cases apply a polynomial time algorithm for the orbit problem over
$\Z^n$ by Kannan and Lipton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5303</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5303</id><created>2014-10-20</created><updated>2015-08-20</updated><authors><author><keyname>Arrigo</keyname><forenames>Francesca</forenames></author><author><keyname>Benzi</keyname><forenames>Michele</forenames></author></authors><title>Updating and downdating techniques for optimizing network
  communicability</title><categories>cs.SI math.NA physics.soc-ph</categories><comments>20 pages, 9 pages Supplementary Material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total communicability of a network (or graph) is defined as the sum of
the entries in the exponential of the adjacency matrix of the network, possibly
normalized by the number of nodes. This quantity offers a good measure of how
easily information spreads across the network, and can be useful in the design
of networks having certain desirable properties. The total communicability can
be computed quickly even for large networks using techniques based on the
Lanczos algorithm.
  In this work we introduce some heuristics that can be used to add, delete, or
rewire a limited number of edges in a given sparse network so that the modified
network has a large total communicability. To this end, we introduce new edge
centrality measures which can be used to guide in the selection of edges to be
added or removed.
  Moreover, we show experimentally that the total communicability provides an
effective and easily computable measure of how &quot;well-connected&quot; a sparse
network is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5326</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5326</id><created>2014-10-20</created><updated>2015-07-15</updated><authors><author><keyname>Han</keyname><forenames>Shuangfeng</forenames></author><author><keyname>I</keyname><forenames>Chih-Lin</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Sun</keyname><forenames>Qi</forenames></author><author><keyname>Xu</keyname><forenames>Zhikun</forenames></author></authors><title>Full Duplex Networking: Mission Impossible?</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to there are some
  errors in this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile traffic is projected to increase 1000 times from 2010 to 2020. This
poses significant challenges on the 5th generation (5G) wireless communication
system design, including network structure, air interface, key transmission
schemes, multiple access, and duplexing schemes. In this paper, full duplex
networking issues are discussed, aiming to provide some insights on the design
and possible future deployment for 5G. Particularly, the interference scenarios
in full duplex are analyzed, followed by discussions on several candidate
interference mitigation approaches, interference proof frame structures,
transceiver structures for channel reciprocity recovery, and super full duplex
base station where each sector operates in time division duplex (TDD) mode. The
extension of TDD and frequency division duplex (FDD) to full duplex is also
examined. It is anticipated that with future standardization and deployment of
full duplex systems, TDD and FDD will be harmoniously integrated, supporting
all the existing half duplex mobile phones efficiently, and leading to a
substantially enhanced 5G system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5329</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5329</id><created>2014-10-16</created><updated>2015-05-18</updated><authors><author><keyname>Raschka</keyname><forenames>Sebastian</forenames></author></authors><title>Naive Bayes and Text Classification I - Introduction and Theory</title><categories>cs.LG</categories><comments>20 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Naive Bayes classifiers, a family of classifiers that are based on the
popular Bayes' probability theorem, are known for creating simple yet well
performing models, especially in the fields of document classification and
disease prediction. In this article, we will look at the main concepts of naive
Bayes classification in the context of document categorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5330</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5330</id><created>2014-10-16</created><authors><author><keyname>Raschka</keyname><forenames>Sebastian</forenames></author></authors><title>An Overview of General Performance Metrics of Binary Classifier Systems</title><categories>cs.LG</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document provides a brief overview of different metrics and terminology
that is used to measure the performance of binary classification systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5331</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5331</id><created>2014-10-20</created><authors><author><keyname>Shen</keyname><forenames>Wenqian</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Gao</keyname><forenames>Zhen</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Spatially correlated channel estimation based on block iterative support
  detection for large-scale MIMO</title><categories>cs.IT math.IT</categories><journal-ref>Electronics Letters, vol. 51, no. 7, pp. 587-588, Apr. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Downlink channel estimation with low pilot overhead is an important and
challenging problem in large-scale MIMO systems due to the substantially
increased MIMO channel dimension. In this letter, we propose a block iterative
support detection (block-ISD) based algorithm for downlink channel estimation
to reduce the pilot overhead, which is achieved by fully exploiting the block
sparsity inherent in the block-sparse equivalent channel derived from the
spatial correlations of MIMO channels. Furthermore, unlike conventional
compressive sensing (CS) algorithms that rely on prior knowledge of the
sparsity level, block-ISD relaxes this demanding requirement and is thus more
practically appealing. Simulation results demonstrate that block-ISD yields
better normalized mean square error (NMSE) performance than classical CS
algorithms, and achieve a reduction of 84% pilot overhead than conventional
channel estimation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5343</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5343</id><created>2014-10-20</created><updated>2015-09-11</updated><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author></authors><title>Classes of Delay-Independent Multimessage Multicast Networks with
  Zero-Delay Nodes</title><categories>cs.IT math.IT</categories><comments>32 pages. Submitted to IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 62, pp. 384-400,
  Jan, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a network, a node is said to incur a delay if its encoding of each
transmitted symbol involves only its received symbols obtained before the time
slot in which the transmitted symbol is sent (hence the transmitted symbol sent
in a time slot cannot depend on the received symbol obtained in the same time
slot). A node is said to incur no delay if its received symbol obtained in a
time slot is available for encoding its transmitted symbol sent in the same
time slot. Under the classical model, every node in the network incurs a delay.
In this paper, we investigate the multimessage multicast network (MMN) under a
generalized-delay model which allows some nodes to incur no delay. We obtain
the capacity regions for three classes of MMNs with zero-delay nodes, namely
the deterministic network dominated by product distribution, the MMN consisting
of independent DMCs and the wireless erasure network. In addition, we show that
for any MMN belonging to one of the above three classes, the set of achievable
rate tuples under the generalized-delay model and under the classical model are
the same, which implies that the set of achievable rate tuples for the MMN does
not depend on the delay amounts incurred by the nodes in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5349</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5349</id><created>2014-10-20</created><authors><author><keyname>Ishengoma</keyname><forenames>Fredrick R.</forenames></author><author><keyname>Mtaho</keyname><forenames>Adam B.</forenames></author></authors><title>3D Printing: Developing Countries Perspectives</title><categories>cs.CY</categories><comments>4 pages, International Journal of Computer Applications Volume
  104-Number 11, 2014</comments><doi>10.5120/18249-9329</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the past decade, 3D printing (3DP) has become popular due to availability
of low-cost 3D printers such as RepRap and Fab@Home; and better software, which
offers a broad range of manufacturing platform that enables users to create
customizable products. 3DP offers everybody with the power to convert a digital
design into a three dimensional physical object. While the application of 3DP
in developing countries is still at an early stage, the technology application
promises vast solutions to existing problems. This paper presents a critical
review of the current state of art of 3DP with a particular focus on developing
countries. Moreover, it discusses the challenges, opportunities and future
insights of 3DP in developing countries. This paper will serve as a basis for
discussion and further research on this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5352</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5352</id><created>2014-10-20</created><updated>2015-01-19</updated><authors><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Marusic</keyname><forenames>Ines</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Minimisation of Multiplicity Tree Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimising the number of states in a multiplicity
tree automaton over the field of rational numbers. We give a minimisation
algorithm that runs in polynomial time assuming unit-cost arithmetic. We also
show that a polynomial bound in the standard Turing model would require a
breakthrough in the complexity of polynomial identity testing by proving that
the latter problem is logspace equivalent to the decision version of
minimisation. The developed techniques also improve the state of the art in
multiplicity word automata: we give an NC algorithm for minimising multiplicity
word automata. Finally, we consider the minimal consistency problem: does there
exist an automaton with $n$ states that is consistent with a given finite
sample of weight-labelled words or trees? We show that this decision problem is
complete for the existential theory of the rationals, both for words and for
trees of a fixed alphabet rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5355</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5355</id><created>2014-10-20</created><updated>2014-12-09</updated><authors><author><keyname>Els&#xe4;sser</keyname><forenames>Robert</forenames></author><author><keyname>Kaaser</keyname><forenames>Dominik</forenames></author></authors><title>On the Influence of Graph Density on Randomized Gossiping</title><categories>cs.DS cs.DC</categories><comments>Full version of paper submitted to IPDPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information dissemination is a fundamental problem in parallel and
distributed computing. In its simplest variant, the broadcasting problem, a
message has to be spread among all nodes of a graph. A prominent communication
protocol for this problem is based on the random phone call model (Karp et al.,
FOCS 2000). In each step, every node opens a communication channel to a
randomly chosen neighbor for bi-directional communication.
  Motivated by replicated databases and peer-to-peer networks, Berenbrink et
al., ICALP 2010, considered the gossiping problem in the random phone call
model. There, each node starts with its own message and all messages have to be
disseminated to all nodes in the network. They showed that any $O(\log n)$-time
algorithm in complete graphs requires $\Omega(\log n)$ message transmissions
per node to complete gossiping, w.h.p, while for broadcasting the average
number of transmissions per node is $O(\log\log n)$.
  It is known that the $O(n\log\log n)$ bound on the number of transmissions
required for randomized broadcasting in complete graphs cannot be achieved in
sparse graphs even if they have best expansion and connectivity properties. In
this paper, we analyze whether a similar influence of the graph density also
holds w.r.t. the performance of gossiping. We study analytically and
empirically the communication overhead generated by randomized gossiping in
random graphs and consider simple modifications of the random phone call model
in these graphs. Our results indicate that, unlike in broadcasting, there is no
significant difference between the performance of randomized gossiping in
complete graphs and sparse random graphs. Furthermore, our simulations indicate
that by tuning the parameters of our algorithms, we can significantly reduce
the communication overhead compared to the traditional push-pull approach in
the graphs we consider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5358</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5358</id><created>2014-10-20</created><updated>2015-09-01</updated><authors><author><keyname>Cusano</keyname><forenames>Claudio</forenames></author><author><keyname>Napoletano</keyname><forenames>Paolo</forenames></author><author><keyname>Schettini</keyname><forenames>Raimondo</forenames></author></authors><title>Remote sensing image classification exploiting multiple kernel learning</title><categories>cs.CV</categories><comments>Accepted for publication on the IEEE Geoscience and Remote Sensing
  letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a strategy for land use classification which exploits Multiple
Kernel Learning (MKL) to automatically determine a suitable combination of a
set of features without requiring any heuristic knowledge about the
classification task. We present a novel procedure that allows MKL to achieve
good performance in the case of small training sets. Experimental results on
publicly available datasets demonstrate the feasibility of the proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5361</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5361</id><created>2014-10-20</created><authors><author><keyname>Tsukerman</keyname><forenames>Emmanuel</forenames></author></authors><title>Tropical Spectral Theory of Tensors</title><categories>math.CO cs.DM cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study tropical eigenpairs of tensors, a generalization of
the tropical spectral theory of matrices. We show the existence and uniqueness
of an eigenvalue. We associate to a tensor a directed hypergraph and define a
new type of cycle on a hypergraph, which we call an H-cycle. The eigenvalue of
a tensor turns out to be equal to the minimal normalized weighted length of
H-cycles of the associated hypergraph. We show that the eigenvalue can be
computed efficiently via a linear program. Finally, we suggest possible
directions of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5369</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5369</id><created>2014-10-20</created><updated>2016-02-18</updated><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author></authors><title>Proof Complexity Modulo the Polynomial Hierarchy: Understanding
  Alternation as a Source of Hardness</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and study a framework in which one can present alternation-based
lower bounds on proof length in proof systems for quantified Boolean formulas.
A key notion in this framework is that of proof system ensemble, which is
(essentially) a sequence of proof systems where, for each, proof checking can
be performed in the polynomial hierarchy. We introduce a proof system ensemble
called relaxing QU-res which is based on the established proof system
QU-resolution. Our main results include an exponential separation of the
tree-like and general versions of relaxing QU-res, and an exponential lower
bound for relaxing QU-res; these are analogs of classical results in
propositional proof complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5370</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5370</id><created>2014-10-20</created><updated>2015-01-15</updated><authors><author><keyname>Seidel</keyname><forenames>Eric L.</forenames></author><author><keyname>Vazou</keyname><forenames>Niki</forenames></author><author><keyname>Jhala</keyname><forenames>Ranjit</forenames></author></authors><title>Type Targeted Testing</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new technique called type targeted testing, which translates
precise refinement types into comprehensive test-suites. The key insight behind
our approach is that through the lens of SMT solvers, refinement types can also
be viewed as a high-level, declarative, test generation technique, wherein
types are converted to SMT queries whose models can be decoded into concrete
program inputs. Our approach enables the systematic and exhaustive testing of
implementations from high-level declarative specifications, and furthermore,
provides a gradual path from testing to full verification. We have implemented
our approach as a Haskell testing tool called TARGET, and present an evaluation
that shows how TARGET can be used to test a wide variety of properties and how
it compares against state-of-the-art testing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5373</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5373</id><created>2014-10-20</created><updated>2015-05-27</updated><authors><author><keyname>Emad</keyname><forenames>Amin</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Poisson Group Testing: A Probabilistic Model for Boolean Compressed
  Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel probabilistic group testing framework, termed Poisson
group testing, in which the number of defectives follows a right-truncated
Poisson distribution. The Poisson model has a number of new applications,
including dynamic testing with diminishing relative rates of defectives. We
consider both nonadaptive and semi-adaptive identification methods. For
nonadaptive methods, we derive a lower bound on the number of tests required to
identify the defectives with a probability of error that asymptotically
converges to zero; in addition, we propose test matrix constructions for which
the number of tests closely matches the lower bound. For semi-adaptive methods,
we describe a lower bound on the expected number of tests required to identify
the defectives with zero error probability. In addition, we propose a
stage-wise reconstruction algorithm for which the expected number of tests is
only a constant factor away from the lower bound. The methods rely only on an
estimate of the average number of defectives, rather than on the individual
probabilities of subjects being defective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5387</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5387</id><created>2014-10-20</created><updated>2015-02-23</updated><authors><author><keyname>Svorenova</keyname><forenames>Maria</forenames></author><author><keyname>Kretinsky</keyname><forenames>Jan</forenames></author><author><keyname>Chmelik</keyname><forenames>Martin</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Cerna</keyname><forenames>Ivana</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Temporal Logic Control for Stochastic Linear Systems using Abstraction
  Refinement of Probabilistic Games</title><categories>cs.SY</categories><comments>Technical report accompanying HSCC'15 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the set of initial states of a dynamical
system such that there exists a control strategy to ensure that the
trajectories satisfy a temporal logic specification with probability 1
(almost-surely). We focus on discrete-time, stochastic linear dynamics and
specifications given as formulas of the Generalized Reactivity(1) fragment of
Linear Temporal Logic over linear predicates in the states of the system. We
propose a solution based on iterative abstraction-refinement, and turn-based
2-player probabilistic games. While the theoretical guarantee of our algorithm
after any finite number of iterations is only a partial solution, we show that
if our algorithm terminates, then the result is the set of satisfying initial
states. Moreover, for any (partial) solution our algorithm synthesizes witness
control strategies to ensure almost-sure satisfaction of the temporal logic
specification. We demonstrate our approach on an illustrative case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5392</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5392</id><created>2014-10-20</created><authors><author><keyname>Cheng</keyname><forenames>Dehua</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling
  for Gaussian Graphical Models</title><categories>cs.DS cs.LG cs.NA math.NA stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a sampling problem basic to computational statistical inference,
we develop a nearly optimal algorithm for a fundamental problem in spectral
graph theory and numerical analysis. Given an $n\times n$ SDDM matrix ${\bf
\mathbf{M}}$, and a constant $-1 \leq p \leq 1$, our algorithm gives efficient
access to a sparse $n\times n$ linear operator $\tilde{\mathbf{C}}$ such that
$${\mathbf{M}}^{p} \approx \tilde{\mathbf{C}} \tilde{\mathbf{C}}^\top.$$ The
solution is based on factoring ${\bf \mathbf{M}}$ into a product of simple and
sparse matrices using squaring and spectral sparsification. For ${\mathbf{M}}$
with $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and
polylogarithmic depth on a parallel machine with $m$ processors. This gives the
first sampling algorithm that only requires nearly linear work and $n$ i.i.d.
random univariate Gaussian samples to generate i.i.d. random samples for
$n$-dimensional Gaussian random fields with SDDM precision matrices. For
sampling this natural subclass of Gaussian random fields, it is optimal in the
randomness and nearly optimal in the work and parallel complexity. In addition,
our sampling algorithm can be directly extended to Gaussian random fields with
SDD precision matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5401</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5401</id><created>2014-10-20</created><updated>2014-12-10</updated><authors><author><keyname>Graves</keyname><forenames>Alex</forenames></author><author><keyname>Wayne</keyname><forenames>Greg</forenames></author><author><keyname>Danihelka</keyname><forenames>Ivo</forenames></author></authors><title>Neural Turing Machines</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the capabilities of neural networks by coupling them to external
memory resources, which they can interact with by attentional processes. The
combined system is analogous to a Turing Machine or Von Neumann architecture
but is differentiable end-to-end, allowing it to be efficiently trained with
gradient descent. Preliminary results demonstrate that Neural Turing Machines
can infer simple algorithms such as copying, sorting, and associative recall
from input and output examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5410</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5410</id><created>2014-10-20</created><updated>2014-11-13</updated><authors><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner
  Product Search (MIPS)</title><categories>stat.ML cs.DS cs.IR cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1405.5869</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently it was shown that the problem of Maximum Inner Product Search (MIPS)
is efficient and it admits provably sub-linear hashing algorithms. Asymmetric
transformations before hashing were the key in solving MIPS which was otherwise
hard. In the prior work, the authors use asymmetric transformations which
convert the problem of approximate MIPS into the problem of approximate near
neighbor search which can be efficiently solved using hashing. In this work, we
provide a different transformation which converts the problem of approximate
MIPS into the problem of approximate cosine similarity search which can be
efficiently solved using signed random projections. Theoretical analysis show
that the new scheme is significantly better than the original scheme for MIPS.
Experimental evaluations strongly support the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5414</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5414</id><created>2014-10-17</created><updated>2015-06-19</updated><authors><author><keyname>Tsalaportas</keyname><forenames>Panagiotis G.</forenames></author><author><keyname>Kapinas</keyname><forenames>Vasileios M.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Solar Lab Notebook (SLN): An Ultra-Portable Web-Based System for
  Heliophysics and High-Security Labs</title><categories>cs.CY</categories><comments>10 pages, 7 figures, 1 table. Final content</comments><journal-ref>IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing, vol. 8, no. 8, pp. 4141-4150, August 2015</journal-ref><doi>10.1109/JSTARS.2015.2444332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Solar Lab Notebook (SLN), an electronic lab
notebook for improving the process of recording and sharing solar related
digital information in an organized manner. SLN is a pure web-based application
(available online: http://umbra.nascom.nasa.gov/sln) that runs client-side
only, employing a clean and very friendly graphical user interface design, and
thus providing a true cross-platform user experience. Furthermore, SLN
leverages unique technologies offered by modern web browsers, such as the
FileReader API, the Blob interface and Local Storage mechanism; it is coded
entirely using HTML5, CSS3, and JavaScript, and powered by the extremely well
documented XML file format. For high-security labs, it can be utilized as an
ultra-portable and secure digital notebook solution, since it is functionally
self-contained, and does not require any server-side process running on either
the local or a remote system. Finally, the W3C XML Schema language is used to
define a list of rules, namely a data standard, that an SLN file must conform
to in order to be valid. In this way, developers are able to implement their
own validation functions in their projects, or use one of the freely available
tools to check if a data file is properly structured. Similarly, scientific
groups at different labs can easily share information, being confident about
the integrity of the exchanged data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5415</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5415</id><created>2014-10-18</created><authors><author><keyname>Thomas</keyname><forenames>Antoine</forenames></author></authors><title>Rearrangement Problems with Duplicated Genomic Markers</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the dynamics of genome rearrangements is a major issue of
phylogenetics. Phylogenetics is the study of species evolution. A major goal of
the field is to establish evolutionary relationships within groups of species,
in order to infer the topology of an evolutionary tree formed by this group and
common ancestors to some of these species. In this context, having means to
evaluate relative evolutionary distances between species, or to infer common
ancestor genomes to a group of species would be of great help. This work, in
the vein of other studies from the past, aims at designing such means, here in
the particular case where genomes present multiple occurrencies of genes, which
makes things more complex. Several hypotheses accounting for the presence of
duplications were considered. Distances formulae as well as scenario computing
algorithms were established, along with their complexity proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5416</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5416</id><created>2014-10-19</created><authors><author><keyname>Mpasios</keyname><forenames>Michael</forenames></author><author><keyname>Kallergis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chimos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Karvounidis</keyname><forenames>Theodoros</forenames></author><author><keyname>Douligeris</keyname><forenames>Christos</forenames></author></authors><title>Software in e-Learning Architecture, Processes and Management</title><categories>cs.SE cs.CY</categories><comments>9 pages, 6th International Conference of Education, Research and
  Innovation (iCERI2013), November 18-20, Seville, Spain. ISBN:
  978-84-616-3847-5</comments><msc-class>68-06</msc-class><acm-class>D.2.9; I.2.5; K.3.1; K.3.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Our entire society is becoming more and more dependent on technology and
specifically on software. The integration of e-learning software systems into
our day by day life especially in e-learning applications generates
modifications upon the society and, at the same time, the society itself
changes the process of software development. This circle of continuous
determination engenders a highly dynamic environment. Lehman describes the
software development environment as being characterized by a high, necessary
and inevitable pressure for change. Changes are reflected in specific
uncertainties which impact the success and performance of the software project
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5420</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5420</id><created>2014-10-20</created><updated>2015-05-07</updated><authors><author><keyname>Brown</keyname><forenames>Russell A.</forenames></author></authors><title>Building a Balanced k-d Tree in O(kn log n) Time</title><categories>cs.DS</categories><comments>8 pages, 7 figures, published at
  http://jcgt.org/published/0004/01/03/</comments><journal-ref>Journal of Computer Graphics Techniques (JCGT), vol. 4, no. 1,
  50-68, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The original description of the k-d tree recognized that rebalancing
techniques, such as are used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is
necessary to find the median of the data for each recursive subdivision of
those data. The sort or selection that is used to find the median for each
subdivision strongly influences the computational complexity of building a k-d
tree. This paper discusses an alternative algorithm that builds a balanced k-d
tree by presorting the data in each of k dimensions prior to building the tree.
It then preserves the order of these k sorts during tree construction and
thereby avoids the requirement for any further sorting. Moreover, this
algorithm is amenable to parallel execution via multiple threads. Compared to
an algorithm that finds the median for each recursive subdivision, this
presorting algorithm has equivalent performance for four dimensions and better
performance for three or fewer dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5458</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5458</id><created>2014-10-20</created><authors><author><keyname>Sendra</keyname><forenames>J. Rafael</forenames></author><author><keyname>Sevilla</keyname><forenames>David</forenames></author><author><keyname>Villarino</keyname><forenames>Carlos</forenames></author></authors><title>Missing sets in rational parametrizations of surfaces of revolution</title><categories>math.AG cs.SC</categories><comments>13 pages, 8 jpg figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parametric representations do not cover, in general, the whole geometric
object that they parametrize. This can be a problem in practical applications.
In this paper we analyze the question for surfaces of revolution generated by
real rational profile curves, and we describe a simple small superset of the
real zone of the surface not covered by the parametrization. This superset
consists, in the worst case, of the union of a circle and the mirror curve of
the profile curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5467</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5467</id><created>2014-10-20</created><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Mamane</keyname><forenames>Lionel</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>Machine Learning of Coq Proof Guidance: First Experiments</title><categories>cs.LO cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report the results of the first experiments with learning proof
dependencies from the formalizations done with the Coq system. We explain the
process of obtaining the dependencies from the Coq proofs, the characterization
of formulas that is used for the learning, and the evaluation method. Various
machine learning methods are compared on a dataset of 5021 toplevel Coq proofs
coming from the CoRN repository. The best resulting method covers on average
75% of the needed proof dependencies among the first 100 predictions, which is
a comparable performance of such initial experiments on other large-theory
corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5473</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5473</id><created>2014-10-20</created><updated>2015-01-13</updated><authors><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Xu</keyname><forenames>Yi</forenames></author></authors><title>Feature Selection Based on Confidence Machine</title><categories>cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In machine learning and pattern recognition, feature selection has been a hot
topic in the literature. Unsupervised feature selection is challenging due to
the loss of labels which would supply the related information.How to define an
appropriate metric is the key for feature selection. We propose a filter method
for unsupervised feature selection which is based on the Confidence Machine.
Confidence Machine offers an estimation of confidence on a feature'reliability.
In this paper, we provide the math model of Confidence Machine in the context
of feature selection, which maximizes the relevance and minimizes the
redundancy of the selected feature. We compare our method against classic
feature selection methods Laplacian Score, Pearson Correlation and Principal
Component Analysis on benchmark data sets. The experimental results demonstrate
the efficiency and effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5476</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5476</id><created>2014-10-20</created><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author><author><keyname>Vyskocil</keyname><forenames>Jiri</forenames></author></authors><title>Certified Connection Tableaux Proofs for HOL Light and TPTP</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, the Metis prover based on ordered paramodulation and
model elimination has replaced the earlier built-in methods for general-purpose
proof automation in HOL4 and Isabelle/HOL. In the annual CASC competition, the
leanCoP system based on connection tableaux has however performed better than
Metis. In this paper we show how the leanCoP's core algorithm can be
implemented inside HOLLight. leanCoP's flagship feature, namely its
minimalistic core, results in a very simple proof system. This plays a crucial
role in extending the MESON proof reconstruction mechanism to connection
tableaux proofs, providing an implementation of leanCoP that certifies its
proofs. We discuss the differences between our direct implementation using an
explicit Prolog stack, to the continuation passing implementation of MESON
present in HOLLight and compare their performance on all core HOLLight goals.
The resulting prover can be also used as a general purpose TPTP prover. We
compare its performance against the resolution based Metis on TPTP and other
interesting datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5485</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5485</id><created>2014-10-20</created><updated>2014-11-28</updated><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>A stronger null hypothesis for crossing dependencies</title><categories>cs.CL cs.SI physics.soc-ph</categories><comments>typos corrected and English improved</comments><journal-ref>(2014). Europhysics Letters 108 (5), 58003</journal-ref><doi>10.1209/0295-5075/108/58003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The syntactic structure of a sentence can be modeled as a tree where vertices
are words and edges indicate syntactic dependencies between words. It is
well-known that those edges normally do not cross when drawn over the sentence.
Here a new null hypothesis for the number of edge crossings of a sentence is
presented. That null hypothesis takes into account the length of the pair of
edges that may cross and predicts the relative number of crossings in random
trees with a small error, suggesting that a ban of crossings or a principle of
minimization of crossings are not needed in general to explain the origins of
non-crossing dependencies. Our work paves the way for more powerful null
hypotheses to investigate the origins of non-crossing dependencies in nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5489</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5489</id><created>2014-10-20</created><authors><author><keyname>Chan</keyname><forenames>Terence H.</forenames></author><author><keyname>Ho</keyname><forenames>Siu-Wai</forenames></author><author><keyname>Yamamoto</keyname><forenames>Hirosuke</forenames></author></authors><title>Private Information Retrieval for Coded Storage</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Journal of Selected Topics in Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Private information retrieval scheme for coded data storage is considered in
this paper. We focus on the case where the size of each data record is large
and hence only the download cost (but not the upload cost for transmitting
retrieval queries) is of interest. We prove that the tradeoff between storage
cost and retrieval/download cost depends on the number of data records in the
system. We also propose a fairly general class of linear storage codes and
retrieval schemes and derive conditions under which our retrieval schemes are
error-free and private. Tradeoffs between the storage cost and retrieval costs
are also obtained. Finally, we consider special cases when the underlying
storage code is based on an MDS code. Using our proposed method, we show that a
randomly generated retrieval scheme is indeed very likely to be private and
error-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5491</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5491</id><created>2014-10-20</created><authors><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Callison-Burch</keyname><forenames>Chris</forenames></author></authors><title>Using Mechanical Turk to Build Machine Translation Evaluation Sets</title><categories>cs.CL cs.LG stat.ML</categories><comments>4 pages, 2 tables; appeared in Proceedings of the NAACL HLT 2010
  Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk,
  June 2010</comments><acm-class>I.2.7; I.2.6; I.5.1; I.5.4</acm-class><journal-ref>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech
  and Language Data with Amazon's Mechanical Turk, pages 208-211, Los Angeles,
  California, June 2010. Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building machine translation (MT) test sets is a relatively expensive task.
As MT becomes increasingly desired for more and more language pairs and more
and more domains, it becomes necessary to build test sets for each case. In
this paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT
test sets cheaply. We find that MTurk can be used to make test sets much
cheaper than professionally-produced test sets. More importantly, in
experiments with multiple MT systems, we find that the MTurk-produced test sets
yield essentially the same conclusions regarding system performance as the
professionally-produced test sets yield.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5499</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5499</id><created>2014-10-20</created><authors><author><keyname>Yan</keyname><forenames>Shihao</forenames></author><author><keyname>Nevat</keyname><forenames>Ido</forenames></author><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author></authors><title>Location Verification Systems Under Spatially Correlated Shadowing</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The verification of the location information utilized in wireless
communication networks is a subject of growing importance. In this work we
formally analyze, for the first time, the performance of a wireless Location
Verification System (LVS) under the realistic setting of spatially correlated
shadowing. Our analysis illustrates that anticipated levels of correlated
shadowing can lead to a dramatic performance improvement of a Received Signal
Strength (RSS)-based LVS. We also analyze the performance of an LVS that
utilizes Differential Received Signal Strength (DRSS), formally proving the
rather counter-intuitive result that a DRSS-based LVS has identical performance
to that of an RSS-based LVS, for all levels of correlated shadowing. Even more
surprisingly, the identical performance of RSS and DRSS-based LVSs is found to
hold even when the adversary does not optimize his true location. Only in the
case where the adversary does not optimize all variables under her control, do
we find the performance of an RSS-based LVS to be better than a DRSS-based LVS.
The results reported here are important for a wide range of emerging wireless
communication applications whose proper functioning depends on the authenticity
of the location information reported by a transceiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5509</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5509</id><created>2014-10-20</created><authors><author><keyname>Singh</keyname><forenames>Jaspreet</forenames></author><author><keyname>Ramakrishna</keyname><forenames>Sudhir</forenames></author></authors><title>On the feasibility of beamforming in millimeter wave communication
  systems with multiple antenna arrays</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of the millimeter (mm) wave spectrum for next generation (5G) mobile
communication has gained significant attention recently. The small carrier
wavelengths at mmwave frequencies enable synthesis of compact antenna arrays,
providing beamforming gains that compensate the increased propagation losses.
In this work, we investigate the feasibility of employing multiple antenna
arrays to obtain diversity/multiplexing gains in mmwave systems, where each of
the arrays is capable of beamforming independently. Considering a codebook
based beamforming system (e.g., to facilitate limited feedback), we observe
that the complexity of \emph{jointly} optimizing the beamforming directions
across the multiple arrays is highly prohibitive, even for very reasonable
system parameters. To overcome this bottleneck, we develop reduced complexity
algorithms for optimizing the choice of beamforming directions, premised on the
sparse multipath structure of the mmwave channel. Specifically, we reduce the
cardinality of the joint beamforming search space, by restricting attention to
a small set of dominant candidate directions. To obtain the set of dominant
directions, we develop two complementary approaches: (a) based on computation
of a novel spatial power metric; a detailed analysis of this metric shows that,
in the limit of large antenna arrays, the selected candidate directions
approach the channel's dominant angles of arrival and departure, and (b)
precise estimation of the channel's (long-term) dominant angles of arrival,
exploiting the correlations of the signals received across the different
receiver subarrays. Our methods enable a drastic reduction of the optimization
search space (a factor of 100 reduction), while delivering close to optimal
performance, thereby indicating the potential feasibility of achieving
diversity and multiplexing gains in mmwave systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5510</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5510</id><created>2014-10-20</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>R.</forenames></author></authors><title>Blind Adaptive MIMO Receivers for CDMA Systems with Space-Time
  Block-Codes and Low-Cost Algorithms</title><categories>cs.IT math.IT</categories><comments>10 pages, 4 figures, Signal Processing, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present low-complexity blind multi-input multi-output (MIMO)
adaptive linear multiuser receivers for direct sequence code division multiple
access (DS-CDMA) systems using multiple transmit antennas and space-time block
codes (STBC) in multipath channels. A space-time code-constrained constant
modulus (CCM) design criterion based on constrained optimization techniques and
low-complexity stochastic gradient (SG) adaptive algorithms are developed for
estimating the parameters of the space-time linear receivers. The receivers are
designed by exploiting the unique structure imposed by both spreading codes and
STBC. A blind space-time channel estimation scheme for STBC systems based on a
subspace approach is also proposed along with an efficient SG algorithm.
Simulation results for a downlink scenario assess the receiver structures and
algorithms and show that the proposed schemes achieve excellent performance,
outperforming existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5518</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5518</id><created>2014-10-20</created><updated>2015-06-08</updated><authors><author><keyname>Neyshabur</keyname><forenames>Behnam</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>On Symmetric and Asymmetric LSHs for Inner Product Search</title><categories>stat.ML cs.DS cs.IR cs.LG</categories><comments>11 pages, 3 figures, In Proceedings of The 32nd International
  Conference on Machine Learning (ICML)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing locality sensitive hashes (LSH) for
inner product similarity, and of the power of asymmetric hashes in this
context. Shrivastava and Li argue that there is no symmetric LSH for the
problem and propose an asymmetric LSH based on different mappings for query and
database points. However, we show there does exist a simple symmetric LSH that
enjoys stronger guarantees and better empirical performance than the asymmetric
LSH they suggest. We also show a variant of the settings where asymmetry is
in-fact needed, but there a different asymmetric LSH is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5524</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5524</id><created>2014-10-20</created><authors><author><keyname>Feng</keyname><forenames>Jie</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Yan</forenames></author></authors><title>Learning to Rank Binary Codes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary codes have been widely used in vision problems as a compact feature
representation to achieve both space and time advantages. Various methods have
been proposed to learn data-dependent hash functions which map a feature vector
to a binary code. However, considerable data information is inevitably lost
during the binarization step which also causes ambiguity in measuring sample
similarity using Hamming distance. Besides, the learned hash functions cannot
be changed after training, which makes them incapable of adapting to new data
outside the training data set. To address both issues, in this paper we propose
a flexible bitwise weight learning framework based on the binary codes obtained
by state-of-the-art hashing methods, and incorporate the learned weights into
the weighted Hamming distance computation. We then formulate the proposed
framework as a ranking problem and leverage the Ranking SVM model to offline
tackle the weight learning. The framework is further extended to an online mode
which updates the weights at each time new data comes, thereby making it
scalable to large and dynamic data sets. Extensive experimental results
demonstrate significant performance gains of using binary codes with bitwise
weighting in image retrieval tasks. It is appealing that the online weight
learning leads to comparable accuracy with its offline counterpart, which thus
makes our approach practical for realistic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5550</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5550</id><created>2014-10-21</created><updated>2015-06-10</updated><authors><author><keyname>Kumar</keyname><forenames>M. Ashok</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Minimization Problems Based on Relative $\alpha$-Entropy II: Reverse
  Projection</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><comments>20 pages; 3 figures; minor change in the title; revised manuscript.
  Accepted for publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In part I of this two-part work, certain minimization problems based on a
parametric family of relative entropies (denoted $\mathscr{I}_{\alpha}$) were
studied. Such minimizers were called forward
$\mathscr{I}_{\alpha}$-projections. Here, a complementary class of minimization
problems leading to the so-called reverse $\mathscr{I}_{\alpha}$-projections
are studied. Reverse $\mathscr{I}_{\alpha}$-projections, particularly on
log-convex or power-law families, are of interest in robust estimation problems
($\alpha &gt;1$) and in constrained compression settings ($\alpha &lt;1$).
Orthogonality of the power-law family with an associated linear family is first
established and is then exploited to turn a reverse
$\mathscr{I}_{\alpha}$-projection into a forward
$\mathscr{I}_{\alpha}$-projection. The transformed problem is a simpler
quasiconvex minimization subject to linear constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5555</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5555</id><created>2014-10-21</created><authors><author><keyname>Tikhomirov</keyname><forenames>Mikhail</forenames></author></authors><title>On computational complexity of length embeddability of graphs</title><categories>cs.CC cs.DM</categories><comments>12 pages, 1 figure</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is embeddable in $\mathbb{R}^d$ if vertices of $G$ can be
assigned with points of $\mathbb{R}^d$ in such a way that all pairs of adjacent
vertices are at the distance 1. We show that verifying embeddability of a given
graph in $\mathbb{R}^d$ is NP-hard in the case $d &gt; 2$ for all reasonable
notions of embeddability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5557</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5557</id><created>2014-10-21</created><authors><author><keyname>Rolf</keyname><forenames>Matthias</forenames></author><author><keyname>Asada</keyname><forenames>Minoru</forenames></author></authors><title>Where do goals come from? A Generic Approach to Autonomous Goal-System
  Development</title><categories>cs.LG cs.AI</categories><comments>Draft submitted to IEEE Transactions on Autonomous Mental Development
  (TAMD)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goals express agents' intentions and allow them to organize their behavior
based on low-dimensional abstractions of high-dimensional world states. How can
agents develop such goals autonomously? This paper proposes a detailed
conceptual and computational account to this longstanding problem. We argue to
consider goals as high-level abstractions of lower-level intention mechanisms
such as rewards and values, and point out that goals need to be considered
alongside with a detection of the own actions' effects. We propose Latent Goal
Analysis as a computational learning formulation thereof, and show
constructively that any reward or value function can by explained by goals and
such self-detection as latent mechanisms. We first show that learned goals
provide a highly effective dimensionality reduction in a practical
reinforcement learning problem. Then, we investigate a developmental scenario
in which entirely task-unspecific rewards induced by visual saliency lead to
self and goal representations that constitute goal-directed reaching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5561</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5561</id><created>2014-10-21</created><authors><author><keyname>Malas</keyname><forenames>Tareq</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Ltaief</keyname><forenames>Hatem</forenames></author><author><keyname>Keyes</keyname><forenames>David</forenames></author></authors><title>Towards energy efficiency and maximum computational intensity for
  stencil algorithms using wavefront diamond temporal blocking</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the impact of tunable parameters on computational intensity (i.e.,
inverse code balance) and energy consumption of multicore-optimized wavefront
diamond temporal blocking (MWD) applied to different stencil-based update
schemes. MWD combines the concepts of diamond tiling and multicore-aware
wavefront blocking in order to achieve lower cache size requirements than
standard single-core wavefront temporal blocking. We analyze the impact of the
cache block size on the theoretical and observed code balance, introduce loop
tiling in the leading dimension to widen the range of applicable diamond sizes,
and show performance results on a contemporary Intel CPU. The impact of code
balance on power dissipation on the CPU and in the DRAM is investigated and
shows that DRAM power is a decisive factor for energy consumption, which is
strongly influenced by the code balance. Furthermore we show that highest
performance does not necessarily lead to lowest energy even if the clock speed
is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5567</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5567</id><created>2014-10-21</created><updated>2015-05-30</updated><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Farley</keyname><forenames>Naomi</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Poettering</keyname><forenames>Bertram</forenames></author></authors><title>Cryptographic Enforcement of Information Flow Policies without Public
  Information</title><categories>cs.CR</categories><comments>To appear in Proceedings of ACNS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptographic access control has been studied for over 30 years and is now a
mature research topic. When symmetric cryptographic primitives are used, each
protected resource is encrypted and only authorized users should have access to
the encryption key. By treating the keys themselves as protected resources, it
is possible to develop schemes in which authorized keys are derived from the
keys explicitly assigned to the user's possession and publicly available
information. It has been generally assumed that each user would be assigned a
single key from which all other authorized keys would be derived. Recent work
has challenged this assumption by developing schemes that do not require public
information, the trade-off being that a user may require more than one key.
However, these new schemes, which require a chain partition of the partially
ordered set on which the access control policy is based, have some
disadvantages. In this paper we define the notion of a tree-based cryptographic
enforcement scheme, which, like chain-based schemes, requires no public
information. We establish that the strong security properties of chain-based
schemes are preserved by tree-based schemes, and provide an efficient
construction for deriving a tree-based enforcement scheme from a given policy
that minimizes the number of keys required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5568</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5568</id><created>2014-10-21</created><updated>2015-01-16</updated><authors><author><keyname>Sebastiani</keyname><forenames>Roberto</forenames></author><author><keyname>Trentin</keyname><forenames>Patrick</forenames></author></authors><title>Pushing the envelope of Optimization Modulo Theories with
  Linear-Arithmetic Cost Functions</title><categories>cs.LO</categories><comments>A slightly-shorter version of this paper is published at TACAS 2015
  conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade we have witnessed an impressive progress in the
expressiveness and efficiency of Satisfiability Modulo Theories (SMT) solving
techniques. This has brought previously-intractable problems at the reach of
state-of-the-art SMT solvers, in particular in the domain of SW and HW
verification. Many SMT-encodable problems of interest, however, require also
the capability of finding models that are optimal wrt. some cost functions. In
previous work, namely &quot;Optimization Modulo Theory with Linear Rational Cost
Functions -- OMT(LAR U T )&quot;, we have leveraged SMT solving to handle the
minimization of cost functions on linear arithmetic over the rationals, by
means of a combination of SMT and LP minimization techniques. In this paper we
push the envelope of our OMT approach along three directions: first, we extend
it to work also with linear arithmetic on the mixed integer/rational domain, by
means of a combination of SMT, LP and ILP minimization techniques; second, we
develop a multi-objective version of OMT, so that to handle many cost functions
simultaneously; third, we develop an incremental version of OMT, so that to
exploit the incrementality of some OMT-encodable problems. An empirical
evaluation performed on OMT-encoded verification problems demonstrates the
usefulness and efficiency of these extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5579</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5579</id><created>2014-10-21</created><updated>2015-01-28</updated><authors><author><keyname>Perotti</keyname><forenames>Alberto G.</forenames></author><author><keyname>Popovic</keyname><forenames>Branislav M.</forenames></author></authors><title>Non-Orthogonal Multiple Access for Degraded Broadcast Channels: RA-CEMA</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, WCNC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new non-orthogonal multiple access scheme performing simultaneous
transmission to multiple users characterized by different signal-to-noise
ratios is proposed. Different users are multiplexed by storing their codewords
into a multiplexing matrix according to properly designed patterns and then
mapping the columns of the matrix onto the symbols of a higher-order
constellation. At the receiver, an interference cancellation algorithm is
employed in order to achieve a higher spectral efficiency than orthogonal user
multiplexing. Rate-Adaptive Constellation Expansion Multiple Access (RA-CEMA)
is an alternative to conventional superposition coding as a solution for
transmission on the degraded broadcast channel. It combines the benefits of an
increased spectral efficiency with the advantages of reusing the coding and
modulation schemes already used in contemporary communication systems, thereby
facilitating its adoption in standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5585</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5585</id><created>2014-10-21</created><authors><author><keyname>Ahmadzadeh</keyname><forenames>Arman</forenames></author><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Analysis and Design of Multi-Hop Diffusion-Based Molecular Communication
  Networks</title><categories>cs.IT math.IT</categories><comments>14 pages, 3 tables, 9 figures, 1 algorithm. Submitted to IEEE Journal
  on Selected Areas in Communications (JSAC) on October 9, 2014. (Author's
  comment: Extended version of the conference paper arXiv:1404.5538)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multi-hop molecular communication network
consisting of one nanotransmitter, one nanoreceiver, and multiple
nanotransceivers acting as relays. We consider three different relaying schemes
to improve the range of diffusion-based molecular communication. In the first
scheme, different types of messenger molecules are utilized in each hop of the
multi-hop network. In the second and third scheme, we assume that two types of
molecules and one type of molecule are utilized in the network, respectively.
We identify self-interference, backward intersymbol interference
(backward-ISI), and forward-ISI as the performance-limiting effects for the
second and third relaying schemes. Furthermore, we consider two relaying modes
analogous to those used in wireless communication systems, namely full-duplex
and half-duplex relaying. We propose the adaptation of the decision threshold
as an effective mechanism to mitigate self-interference and backward-ISI at the
relay for full-duplex and half-duplex transmission. We derive closed-form
expressions for the expected end-to-end error probability of the network for
the three considered relaying schemes. Furthermore, we derive closed-form
expressions for the optimal number of molecules released by the nanotransmitter
and the optimal detection threshold of the nanoreceiver for minimization of the
expected error probability of each hop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5600</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5600</id><created>2014-10-21</created><authors><author><keyname>Parasuraman</keyname><forenames>Ramviyas</forenames></author></authors><title>Mobility Enhancement for Elderly</title><categories>cs.CV cs.RO</categories><comments>Masters thesis, Indian Institute of Technology Delhi</comments><report-no>2008JID2945</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loss of Mobility is a common handicap to senior citizens. It denies them the
ease of movement they would like to have like outdoor visits, movement in
hospitals, social outgoings, but more seriously in the day to day in-house
routine functions necessary for living etc. Trying to overcome this handicap by
means of servant or domestic help and simple wheel chairs is not only costly in
the long run, but forces the senior citizen to be at the mercy of sincerity of
domestic helps and also the consequent loss of dignity. In order to give a
dignified life, the mobility obtained must be at the complete discretion, will
and control of the senior citizen. This can be provided only by a reasonably
sophisticated and versatile wheel chair, giving enhanced ability of vision,
hearing through man-machine interface, and sensor aided navigation and control.
More often than not senior people have poor vision which makes it difficult for
them to maker visual judgement and so calls for the use of Artificial
Intelligence in visual image analysis and guided navigation systems.
  In this project, we deal with two important enhancement features for mobility
enhancement, Audio command and Vision aided obstacle detection and navigation.
We have implemented speech recognition algorithm using template of stored words
for identifying the voice command given by the user. This frees the user of an
agile hand to operate joystick or mouse control. Also, we have developed a new
appearance based obstacle detection system using stereo-vision cameras which
estimates the distance of nearest obstacle to the wheel chair and takes
necessary action. This helps user in making better judgement of route and
navigate obstacles. The main challenge in this project is how to navigate in an
unknown/unfamiliar environment by avoiding obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5602</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5602</id><created>2014-10-21</created><authors><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Kleinbort</keyname><forenames>Michal</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>Optimal randomized incremental construction for guaranteed logarithmic
  planar point location</title><categories>cs.CG</categories><comments>The article significantly extends the theoretical aspects of the work
  presented in http://arxiv.org/abs/1205.5434</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a planar map of $n$ segments in which we wish to efficiently locate
points, we present the first randomized incremental construction of the
well-known trapezoidal-map search-structure that only requires expected $O(n
\log n)$ preprocessing time while deterministically guaranteeing worst-case
linear storage space and worst-case logarithmic query time. This settles a long
standing open problem; the best previously known construction time of such a
structure, which is based on a directed acyclic graph, so-called the history
DAG, and with the above worst-case space and query-time guarantees, was
expected $O(n \log^2 n)$. The result is based on a deeper understanding of the
structure of the history DAG, its depth in relation to the length of its
longest search path, as well as its correspondence to the trapezoidal search
tree. Our results immediately extend to planar maps induced by finite
collections of pairwise interior disjoint well-behaved curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5604</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5604</id><created>2014-10-21</created><authors><author><keyname>Borges</keyname><forenames>Joaquim</forenames></author><author><keyname>Fern&#xe1;ndez-C&#xf3;rdoba</keyname><forenames>Cristina</forenames></author><author><keyname>Ten-Valls</keyname><forenames>Roger</forenames></author></authors><title>Z2-double cyclic codes</title><categories>cs.IT cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary linear code $C$ is a $\mathbb{Z}_2$-double cyclic code if the set of
coordinates can be partitioned into two subsets such that any cyclic shift of
the coordinates of both subsets leaves invariant the code. These codes can be
identified as submodules of the $\mathbb{Z}_2[x]$-module
$\mathbb{Z}_2[x]/(x^r-1)\times\mathbb{Z}_2[x]/(x^s-1).$ We determine the
structure of $\mathbb{Z}_2$-double cyclic codes giving the generator
polynomials of these codes. The related polynomial representation of
$\mathbb{Z}_2$-double cyclic codes and its duals, and the relations between the
polynomial generators of these codes are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5605</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5605</id><created>2014-10-21</created><updated>2015-04-27</updated><authors><author><keyname>Napoletano</keyname><forenames>Paolo</forenames></author><author><keyname>Boccignone</keyname><forenames>Giuseppe</forenames></author><author><keyname>Tisato</keyname><forenames>Francesco</forenames></author></authors><title>Attentive monitoring of multiple video streams driven by a Bayesian
  foraging strategy</title><categories>cs.CV</categories><comments>Accepted to IEEE Transactions on Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we shall consider the problem of deploying attention to subsets
of the video streams for collating the most relevant data and information of
interest related to a given task. We formalize this monitoring problem as a
foraging problem. We propose a probabilistic framework to model observer's
attentive behavior as the behavior of a forager. The forager, moment to moment,
focuses its attention on the most informative stream/camera, detects
interesting objects or activities, or switches to a more profitable stream. The
approach proposed here is suitable to be exploited for multi-stream video
summarization. Meanwhile, it can serve as a preliminary step for more
sophisticated video surveillance, e.g. activity and behavior analysis.
Experimental results achieved on the UCR Videoweb Activities Dataset, a
publicly available dataset, are presented to illustrate the utility of the
proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5607</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5607</id><created>2014-10-21</created><authors><author><keyname>Amir</keyname><forenames>Amihood</forenames></author><author><keyname>Kapah</keyname><forenames>Oren</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Rothschild</keyname><forenames>Amir</forenames></author></authors><title>Polynomials: a new tool for length reduction in binary discrete
  convolutions</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:0802.0017</comments><acm-class>F.2.1; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient handling of sparse data is a key challenge in Computer Science.
Binary convolutions, such as polynomial multiplication or the Walsh Transform
are a useful tool in many applications and are efficiently solved.
  In the last decade, several problems required efficient solution of sparse
binary convolutions. both randomized and deterministic algorithms were
developed for efficiently computing the sparse polynomial multiplication. The
key operation in all these algorithms was length reduction. The sparse data is
mapped into small vectors that preserve the convolution result. The reduction
method used to-date was the modulo function since it preserves location (of the
&quot;1&quot; bits) up to cyclic shift.
  To date there is no known efficient algorithm for computing the sparse Walsh
transform. Since the modulo function does not preserve the Walsh transform a
new method for length reduction is needed. In this paper we present such a new
method - polynomials. This method enables the development of an efficient
algorithm for computing the binary sparse Walsh transform. To our knowledge,
this is the first such algorithm. We also show that this method allows a faster
deterministic computation of sparse polynomial multiplication than currently
known in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5610</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5610</id><created>2014-10-21</created><authors><author><keyname>Scheler</keyname><forenames>Gabriele</forenames></author></authors><title>Universality of Power Law Coding for Principal Neurons</title><categories>q-bio.NC cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we document distributions for spike rates, synaptic weights and
neural gains for principal neurons in various tissues and under different
behavioral conditions. We find a remarkable consistency of a power-law,
specifically lognormal, distribution across observations from auditory or
visual cortex as well as midbrain nuclei, cerebellar Purkinje cells and
striatal medium spiny neurons. An exception is documented for fast-spiking
interneurons, as non-coding neurons, which seem to follow a normal
distribution. The difference between strongly recurrent and transfer
connectivity (cortex vs. striatum and cerebellum), or the level of activation
(low in cortex, high in Purkinje cells and midbrain nuclei) seems to be
irrelevant for these distributions. This has certain implications on neural
coding. In particular, logarithmic scale distribution of neuronal output
appears as a structural phenomenon that is always present in coding neurons. We
also report data for a lognormal distribution of synaptic strengths in cortex,
cerebellum and hippocampus and for intrinsic excitability in striatum, cortex
and cerebellum. We present a neural model for gain, weights and spike rates,
specifically matching the width of distributions. We discuss the data from the
perspective of a hierarchical coding scheme with few sparse or top-level
features and many additional distributed low-level features. Logarithmic-scale
coding may solve an access problem by combining a local modular structure with
few high frequency contact points. Computational models may need to incorporate
these observations as primary constraints. More data are needed to consolidate
the observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5614</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5614</id><created>2014-10-21</created><authors><author><keyname>Stavropoulos</keyname><forenames>Thanos G.</forenames></author><author><keyname>Andreadis</keyname><forenames>Stelios</forenames></author><author><keyname>Bassiliades</keyname><forenames>Nick</forenames></author><author><keyname>Vrakas</keyname><forenames>Dimitris</forenames></author><author><keyname>Vlahavas</keyname><forenames>Ioannis</forenames></author></authors><title>The Tomaco Hybrid Matching Framework for SAWSDL Semantic Web Services</title><categories>cs.SE cs.AI</categories><comments>Under review. Keywords: Web Services Discovery, Intelligent Web
  Services and Semantic Web, Internet reasoning services, Web-based services</comments><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims to resolve issues related to Web Service retrieval, also known
as Service Selection, Discovery or essentially Matching, in two directions.
Firstly, a novel matching algorithm for SAWSDL is introduced. The algorithm is
hybrid in nature, combining novel and known concepts, such as a logic-based
strategy and syntactic text-similarity measures on semantic annotations and
textual descriptions. A plugin for the S3 contest environment was developed, in
order to position Tomaco amongst state-of-the-art in an objective, reproducible
manner. Evaluation showed that Tomaco ranks high amongst state of the art,
especially for early recall levels. Secondly, this work introduces the Tomaco
web application, which aims to accelerate the wide-spread adoption of Semantic
Web Service technologies and algorithms while targeting the lack of
user-friendly applications in this field. Tomaco integrates a variety of
configurable matching algorithms proposed in this paper. It, finally, allows
discovery of both existing and user-contributed service collections and
ontologies, serving also as a service registry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5626</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5626</id><created>2014-10-21</created><authors><author><keyname>Caria</keyname><forenames>Marcel</forenames></author><author><keyname>Das</keyname><forenames>Tamal</forenames></author><author><keyname>Jukan</keyname><forenames>Admela</forenames></author><author><keyname>Hoffmann</keyname><forenames>Marco</forenames></author></authors><title>Divide and Conquer: Partitioning OSPF networks with SDN</title><categories>cs.NI</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Defined Networking (SDN) is an emerging network control paradigm
focused on logical centralization and programmability. At the same time,
distributed routing protocols, most notably OSPF and IS-IS, are still prevalent
in IP networks, as they provide shortest path routing, fast topological
convergence after network failures, and, perhaps most importantly, the
confidence based on decades of reliable operation. Therefore, a hybrid SDN/OSPF
operation remains a desirable proposition. In this paper, we propose a new
method of hybrid SDN/OSPF operation. Our method is different from other hybrid
approaches, as it uses SDN nodes to partition an OSPF domain into sub-domains
thereby achieving the traffic engineering capabilities comparable to full SDN
operation. We place SDN-enabled routers as sub-domain border nodes, while the
operation of the OSPF protocol continues unaffected. In this way, the SDN
controller can tune routing protocol updates for traffic engineering purposes
before they are flooded into sub-domains. While local routing inside
sub-domains remains stable at all times, inter-sub-domain routes can be
optimized by determining the routes in each traversed sub-domain. As the
majority of traffic in non-trivial topologies has to traverse multiple
sub-domains, our simulation results confirm that a few SDN nodes allow traffic
engineering up to a degree that renders full SDN deployment unnecessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5652</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5652</id><created>2014-10-21</created><authors><author><keyname>Varga</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Kir&#xe1;ly</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Abonyi</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Improvement of PSO algorithm by memory based gradient search -
  application in inventory management</title><categories>cs.NE</categories><comments>book chapter, 20 pages, 7 figures, 2 tables</comments><journal-ref>Swarm Intelligence and Bio-Inspired Computation: Theory and
  Applications, 1st Edition, pages 403-422. Elsevier, Oxford, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced inventory management in complex supply chains requires effective and
robust nonlinear optimization due to the stochastic nature of supply and demand
variations. Application of estimated gradients can boost up the convergence of
Particle Swarm Optimization (PSO) algorithm but classical gradient calculation
cannot be applied to stochastic and uncertain systems. In these situations
Monte-Carlo (MC) simulation can be applied to determine the gradient. We
developed a memory based algorithm where instead of generating and evaluating
new simulated samples the stored and shared former function evaluations of the
particles are sampled to estimate the gradients by local weighted least squares
regression. The performance of the resulted regional gradient-based PSO is
verified by several benchmark problems and in a complex application example
where optimal reorder points of a supply chain are determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5663</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5663</id><created>2014-10-21</created><authors><author><keyname>Egan</keyname><forenames>Malcolm</forenames></author><author><keyname>Deng</keyname><forenames>Yansha</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author></authors><title>Variance-Constrained Capacity of the Molecular Timing Channel with
  Synchronization Error</title><categories>cs.IT math.IT</categories><comments>Accepted for GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular communication is set to play an important role in the design of
complex biological and chemical systems. An important class of molecular
communication systems is based on the timing channel, where information is
encoded in the delay of the transmitted molecule---a synchronous approach. At
present, a widely used modeling assumption is the perfect synchronization
between the transmitter and the receiver. Unfortunately, this assumption is
unlikely to hold in most practical molecular systems. To remedy this, we
introduce a clock into the model---leading to the molecular timing channel with
synchronization error. To quantify the behavior of this new system, we derive
upper and lower bounds on the variance-constrained capacity, which we view as
the step between the mean-delay and the peak-delay constrained capacity. By
numerically evaluating our bounds, we obtain a key practical insight: the drift
velocity of the clock links does not need to be significantly larger than the
drift velocity of the information link, in order to achieve the
variance-constrained capacity with perfect synchronization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5665</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5665</id><created>2014-10-21</created><authors><author><keyname>Egan</keyname><forenames>Malcolm</forenames></author><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Deng</keyname><forenames>Yansha</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author></authors><title>The Unreasonable Effectiveness of Blood Pressure Measurement: Molecular
  Communication in Biological Systems</title><categories>cs.IT math.IT q-bio.MN</categories><comments>Submitted to IEEE ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arterial blood pressure is a key vital sign for the health of the human body.
As such, accurate and reproducible measurement techniques are necessary for
successful diagnosis. Blood pressure measurement is an example of molecular
communication in regulated biological systems. In general, communication in
regulated biological systems is difficult because the act of encoding
information about the state of the system can corrupt the message itself. In
this paper, we propose three strategies to cope with this problem to facilitate
reliable molecular communication links: communicate from the outskirts; build
it in; and leave a small footprint. Our strategies---inspired by communication
in natural biological systems---provide a classification to guide the design of
molecular communication mechanisms in synthetic biological systems. We
illustrate our classification using examples of the first two strategies in
natural systems. We then consider a molecular link within a model based on the
Michaelis-Menten kinetics. In particular, we compute the capacity of the link,
which reveals the potential of communicating using our leave a small footprint
strategy. This provides a way of identifying whether the molecular link can be
improved without affecting the function, and a guide to the design of synthetic
biological systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5684</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5684</id><created>2014-10-21</created><authors><author><keyname>Ognawala</keyname><forenames>Saahil</forenames></author><author><keyname>Bayer</keyname><forenames>Justin</forenames></author></authors><title>Regularizing Recurrent Networks - On Injected Noise and Norm-based
  Methods</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancements in parallel processing have lead to a surge in multilayer
perceptrons' (MLP) applications and deep learning in the past decades.
Recurrent Neural Networks (RNNs) give additional representational power to
feedforward MLPs by providing a way to treat sequential data. However, RNNs are
hard to train using conventional error backpropagation methods because of the
difficulty in relating inputs over many time-steps. Regularization approaches
from MLP sphere, like dropout and noisy weight training, have been
insufficiently applied and tested on simple RNNs. Moreover, solutions have been
proposed to improve convergence in RNNs but not enough to improve the long term
dependency remembering capabilities thereof.
  In this study, we aim to empirically evaluate the remembering and
generalization ability of RNNs on polyphonic musical datasets. The models are
trained with injected noise, random dropout, norm-based regularizers and their
respective performances compared to well-initialized plain RNNs and advanced
regularization methods like fast-dropout. We conclude with evidence that
training with noise does not improve performance as conjectured by a few works
in RNN optimization before ours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5689</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5689</id><created>2014-10-21</created><authors><author><keyname>Gao</keyname><forenames>Jingliang</forenames></author><author><keyname>Yang</keyname><forenames>Yanbo</forenames></author></authors><title>Quantum entropy-typical subspace and universal data compression</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantum entropy-typical subspace theory is specified. It is shown that
any mixed state with von Neumann entropy less than h can be preserved
approximately by the entropy-typical subspace with entropy= h. This result
implies an universal compression scheme for the case that the von Neumann
entropy of the source does not exceed h.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5694</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5694</id><created>2014-10-21</created><updated>2015-04-14</updated><authors><author><keyname>Vahdati</keyname><forenames>Sahar</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author></authors><title>OpenCourseWare Observatory -- Does the Quality of OpenCourseWare Live up
  to its Promise?</title><categories>cs.CY</categories><comments>A later version of this paper was presented in the proceedings of the
  Fifth International Conference on Learning Analytics And Knowledge(2015),
  pages 73-82. http://dl.acm.org/citation.cfm?id=2723605 On Zenodo:
  https://zenodo.org/deposit/21264/</comments><acm-class>K.3</acm-class><doi>10.1145/2723576.2723605</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A vast amount of OpenCourseWare (OCW) is meanwhile being published online to
make educational content accessible to larger audiences. The awareness of such
courses among users and the popularity of systems providing such courses are
increasing. However, from a subjective experience, OCW is frequently cursory,
outdated or non-reusable. In order to obtain a better understanding of the
quality of OCW, we assess the quality in terms of fitness for use. Based on
three OCW use case scenarios, we define a range of dimensions according to
which the quality of courses can be measured. From the definition of each
dimension a comprehensive list of quality metrics is derived. In order to
obtain a representative overview of the quality of OCW, we performed a quality
assessment on a set of 100 randomly selected courses obtained from 20 different
OCW repositories. Based on this assessment we identify crucial areas in which
OCW needs to improve in order to deliver up to its promises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5696</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5696</id><created>2014-10-21</created><authors><author><keyname>Sharma</keyname><forenames>Rajesh</forenames></author><author><keyname>Subramanian</keyname><forenames>Deepak</forenames></author><author><keyname>Srirama</keyname><forenames>Satish N.</forenames></author></authors><title>DAPriv: Decentralized architecture for preserving the privacy of medical
  data</title><categories>cs.CR cs.CY</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The digitization of the medical data has been a sensitive topic. In modern
times laws such as the HIPAA provide some guidelines for electronic
transactions in medical data to prevent attacks and fraudulent usage of private
information. In our paper, we explore an architecture that uses hybrid
computing with decentralized key management and show how it is suitable in
preventing a special form of re-identification attack that we name as the
re-assembly attack. This architecture would be able to use current
infrastructure from mobile phones to server certificates and cloud based
decentralized storage models in an efficient way to provide a reliable model
for communication of medical data. We encompass entities including patients,
doctors, insurance agents, emergency contacts, researchers, medical test
laboratories and technicians. This is a complete architecture that provides
patients with a good level of privacy, secure communication and more direct
control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5697</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5697</id><created>2014-10-21</created><authors><author><keyname>Xu</keyname><forenames>Weiqiang</forenames></author><author><keyname>Zhang</keyname><forenames>Yushu</forenames></author><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Dynamic Optimization For Heterogeneous Powered Wireless Multimedia
  Sensor Networks With Correlated Sources and Network Coding</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1410.1973</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The energy consumption in wireless multimedia sensor networks (WMSN) is much
greater than that in traditional wireless sensor networks. Thus, it is a huge
challenge to remain the perpetual operation for WMSN. In this paper, we propose
a new heterogeneous energy supply model for WMSN through the coexistence of
renewable energy and electricity grid. We address to cross-layer optimization
for the multiple multicast with distributed source coding and intra-session
network coding in heterogeneous powered wireless multimedia sensor networks
(HPWMSN) with correlated sources. The aim is to achieve the optimal reconstruct
distortion at sinks and the minimal cost of purchasing electricity from
electricity grid. Based on the Lyapunov drift-plus-penalty with perturbation
technique and dual decomposition technique, we propose a fully distributed
dynamic cross-layer algorithm, including multicast routing, source rate
control, network coding, session scheduling and energy management, only
requiring knowledge of the instantaneous system state. The explicit trade-off
between the optimization objective and queue backlog is theoretically proven.
Finally, the simulation results verify the theoretic claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5703</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5703</id><created>2014-10-17</created><authors><author><keyname>Velner</keyname><forenames>Yaron</forenames></author></authors><title>Robust Multidimensional Mean-Payoff Games are Undecidable</title><categories>cs.LO cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean-payoff games play a central role in quantitative synthesis and
verification. In a single-dimensional game a weight is assigned to every
transition and the objective of the protagonist is to assure a non-negative
limit-average weight. In the multidimensional setting, a weight vector is
assigned to every transition and the objective of the protagonist is to satisfy
a boolean condition over the limit-average weight of each dimension, e.g.,
$\LimAvg(x_1) \leq 0 \vee \LimAvg(x_2)\geq 0 \wedge \LimAvg(x_3) \geq 0$. We
recently proved that when one of the players is restricted to finite-memory
strategies then the decidability of determining the winner is inter-reducible
with Hilbert's Tenth problem over rationals (a fundamental long-standing open
problem). In this work we allow arbitrary (infinite-memory) strategies for both
players and we show that the problem is undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5716</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5716</id><created>2014-10-21</created><authors><author><keyname>Girnyk</keyname><forenames>Maksym A.</forenames></author><author><keyname>Vehkaper&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author></authors><title>Asymptotic Performance Analysis of a K-Hop Amplify-and-Forward Relay
  MIMO Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, September 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper studies the asymptotic performance of multi-hop
amplify-and-forward relay multiple-antenna communication channels. Each
multi-antenna terminal in the network amplifies the received signal, sent by a
source, and retransmits it upstream towards a destination. Achievable ergodic
rates under both jointly optimal detection and decoding and practical separate
decoding schemes for arbitrary signaling schemes, along with the average bit
error rate for various receiver structures are derived in the regime where the
number of antennas at each terminal grows large without a bound. To overcome
the difficulty of averaging over channel realizations we apply large-system
analysis based on the replica method from statistical physics. The validity of
the large-system analysis is further verified through Monte Carlo simulations
of realistic finite-sized systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5718</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5718</id><created>2014-10-21</created><authors><author><keyname>Khan</keyname><forenames>M. S. S.</forenames></author></authors><title>A Computer Virus Propagation Model Using Delay Differential Equations
  With Probabilistic Contagion And Immunity</title><categories>cs.SI cs.NI</categories><comments>International Journal of Computer Networks &amp; Communications (IJCNC)
  Vol.6, No.5, September 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SIR model is used extensively in the field of epidemiology, in
particular, for the analysis of communal diseases. One problem with SIR and
other existing models is that they are tailored to random or Erdos type
networks since they do not consider the varying probabilities of infection or
immunity per node. In this paper, we present the application and the simulation
results of the pSEIRS model that takes into account the probabilities, and is
thus suitable for more realistic scale free networks. In the pSEIRS model, the
death rate and the excess death rate are constant for infective nodes. Latent
and immune periods are assumed to be constant and the infection rate is assumed
to be proportional to I (t) N(t), where N (t) is the size of the total
population and I(t) is the size of the infected population. A node recovers
from an infection temporarily with a probability p and dies from the infection
with probability (1-p).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5738</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5738</id><created>2014-10-21</created><authors><author><keyname>Goswami</keyname><forenames>Debdipta</forenames></author><author><keyname>Hamann</keyname><forenames>Heiko</forenames></author></authors><title>Investigation of A Collective Decision Making System of Different
  Neighbourhood-Size Based on Hyper-Geometric Distribution</title><categories>cs.AI</categories><comments>9 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of collective decision making system has become the central part of
the Swarm- Intelligence Related research in recent years. The most challenging
task of modelling a collec- tive decision making system is to develop the
macroscopic stochastic equation from its microscopic model. In this report we
have investigated the behaviour of a collective decision making system with
specified microscopic rules that resemble the chemical reaction and used
different group size. Then we ventured to derive a generalized analytical model
of a collective-decision system using hyper-geometric distribution.
  Index Terms-swarm; collective decision making; noise; group size;
hyper-geometric distribution
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5764</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5764</id><created>2014-10-21</created><authors><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Lewis</keyname><forenames>Matt</forenames></author><author><keyname>Weissenbacher</keyname><forenames>Georg</forenames></author></authors><title>Proving Safety with Trace Automata and Bounded Model Checking</title><categories>cs.FL cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loop under-approximation is a technique that enriches C programs with
additional branches that represent the effect of a (limited) range of loop
iterations. While this technique can speed up the detection of bugs
significantly, it introduces redundant execution traces which may complicate
the verification of the program. This holds particularly true for verification
tools based on Bounded Model Checking, which incorporate simplistic heuristics
to determine whether all feasible iterations of a loop have been considered.
  We present a technique that uses \emph{trace automata} to eliminate redundant
executions after performing loop acceleration. The method reduces the diameter
of the program under analysis, which is in certain cases sufficient to allow a
safety proof using Bounded Model Checking. Our transformation is precise---it
does not introduce false positives, nor does it mask any errors. We have
implemented the analysis as a source-to-source transformation, and present
experimental results showing the applicability of the technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5770</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5770</id><created>2014-10-21</created><authors><author><keyname>Alfano</keyname><forenames>Giuseppa</forenames></author><author><keyname>Chiasserini</keyname><forenames>Carla-Fabiana</forenames></author><author><keyname>Nordio</keyname><forenames>Alessandro</forenames></author><author><keyname>Zhou</keyname><forenames>Siyuan</forenames></author></authors><title>Closed-form Output Statistics of MIMO Block-Fading Channels</title><categories>cs.IT math.IT</categories><comments>16 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The information that can be transmitted through a wireless channel, with
multiple-antenna equipped transmitter and receiver, is crucially influenced by
the channel behavior as well as by the structure of the input signal. We
characterize in closed form the probability density function (pdf) of the
output of MIMO block-fading channels, for an arbitrary SNR value. Our results
provide compact expressions for such output statistics, paving the way to a
more detailed analytical information-theoretic exploration of communications in
presence of block fading. The analysis is carried out assuming two different
structures for the input signal: the i.i.d. Gaussian distribution and a product
form that has been proved to be optimal for non-coherent communication, i.e.,
in absence of any channel state information. When the channel is fed by an
i.i.d. Gaussian input, we assume the Gramian of the channel matrix to be
unitarily invariant and derive the output statistics in both the noise-limited
and the interference-limited scenario, considering different fading
distributions. When the product-form input is adopted, we provide the
expressions of the output pdf as the relationship between the overall number of
antennas and the fading coherence length varies. We also highlight the relation
between our newly derived expressions and the results already available in the
literature, and, for some cases, we numerically compute the mutual information,
based on the proposed expression of the output statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5772</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5772</id><created>2014-10-20</created><updated>2014-12-09</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Marx</keyname><forenames>Werner</forenames></author></authors><title>Methods for the generation of normalized citation impact scores in
  bibliometrics: Which method best reflects the judgements of experts?</title><categories>cs.DL stat.OT</categories><comments>Accepted for publication in the Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluative bibliometrics compares the citation impact of researchers,
research groups and institutions with each other across time scales and
disciplines. Both factors - discipline and period - have an influence on the
citation count which is independent of the quality of the publication.
Normalizing the citation impact of papers for these two factors started in the
mid-1980s. Since then, a range of different methods have been presented for
producing normalized citation impact scores. The current study uses a data set
of over 50,000 records to test which of the methods so far presented correlate
better with the assessment of papers by peers. The peer assessments come from
F1000Prime - a post-publication peer review system of the biomedical
literature. Of the normalized indicators, the current study involves not only
cited-side indicators, such as the mean normalized citation score, but also
citing-side indicators. As the results show, the correlations of the indicators
with the peer assessments all turn out to be very similar. Since F1000 focuses
on biomedicine, it is important that the results of this study are validated by
other studies based on datasets from other disciplines or (ideally) based on
multi-disciplinary datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5775</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5775</id><created>2014-10-21</created><authors><author><keyname>Dieker</keyname><forenames>A. B.</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Stochastic billiards for sampling from the boundary of a convex set</title><categories>math.PR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic billiards can be used for approximate sampling from the boundary
of a bounded convex set through the Markov Chain Monte Carlo (MCMC) paradigm.
This paper studies how many steps of the underlying Markov chain are required
to get samples (approximately) from the uniform distribution on the boundary of
the set, for sets with an upper bound on the curvature of the boundary. Our
main theorem implies a polynomial-time algorithm for sampling from the boundary
of such sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5777</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5777</id><created>2014-10-18</created><authors><author><keyname>Josi</keyname><forenames>Ahmad</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Suryayusra</keyname></author></authors><title>Penerapan teknik web scraping pada mesin pencari artikel ilmiah</title><categories>cs.IR</categories><comments>6 pages, Jurnal Sistem Informasi (SISFO), vol. 5, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engines are a combination of hardware and computer software supplied
by a particular company through the website which has been determined. Search
engines collect information from the web through bots or web crawlers that
crawls the web periodically. The process of retrieval of information from
existing websites is called &quot;web scraping.&quot; Web scraping is a technique of
extracting information from websites. Web scraping is closely related to Web
indexing, as for how to develop a web scraping technique that is by first
studying the program makers HTML document from the website will be taken to the
information in the HTML tag flanking the aim is for information collected after
the program makers learn navigation techniques on the website information will
be taken to a web application mimicked the scraping that we will create. It
should also be noted that the implementation of this writing only scraping
involves a free search engine such as: portal garuda, Indonesian scientific
journal databases (ISJD), google scholar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5778</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5778</id><created>2014-10-21</created><updated>2015-03-26</updated><authors><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author><author><keyname>Gajser</keyname><forenames>David</forenames></author></authors><title>Simple PTAS's for families of graphs excluding a minor</title><categories>cs.DS cs.DM</categories><comments>To appear in Discrete Applied Mathematics</comments><msc-class>68W25, 68W40, 05C85, 05C83</msc-class><journal-ref>Discrete Applied Mathematics, 189, p. 41-48, 2015</journal-ref><doi>10.1016/j.dam.2015.03.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that very simple algorithms based on local search are polynomial-time
approximation schemes for Maximum Independent Set, Minimum Vertex Cover and
Minimum Dominating Set, when the input graphs have a fixed forbidden minor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5780</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5780</id><created>2014-10-03</created><authors><author><keyname>Robledo</keyname><forenames>Jesus</forenames></author><author><keyname>Leloux</keyname><forenames>Jonathan</forenames></author><author><keyname>Lorenzo</keyname><forenames>Eduardo</forenames></author></authors><title>3D simulation of complex shading affecting PV systems taking benefit
  from the power of graphics cards developed for the video game industry</title><categories>cs.OH</categories><comments>5 page, 9 figures, conference proceedings, 29th European Photovoltaic
  Solar Energy Conference and Exhibition, Amsterdam, 2014</comments><doi>10.13140/2.1.3722.1129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shading reduces the power output of a photovoltaic (PV) system. The design
engineering of PV systems requires modeling and evaluating shading losses. Some
PV systems are affected by complex shading scenes whose resulting PV energy
losses are very difficult to evaluate with current modeling tools. Several
specialized PV design and simulation software include the possibility to
evaluate shading losses. They generally possess a Graphical User Interface
(GUI) through which the user can draw a 3D shading scene, and then evaluate its
corresponding PV energy losses. The complexity of the objects that these tools
can handle is relatively limited. We have created a software solution, 3DPV,
which allows evaluating the energy losses induced by complex 3D scenes on PV
generators. The 3D objects can be imported from specialized 3D modeling
software or from a 3D object library. The shadows cast by this 3D scene on the
PV generator are then directly evaluated from the Graphics Processing Unit
(GPU). Thanks to the recent development of GPUs for the video game industry,
the shadows can be evaluated with a very high spatial resolution that reaches
well beyond the PV cell level, in very short calculation times. A PV simulation
model then translates the geometrical shading into PV energy output losses.
3DPV has been implemented using WebGL, which allows it to run directly from a
Web browser, without requiring any local installation from the user. This also
allows taken full benefits from the information already available from
Internet, such as the 3D object libraries. This contribution describes, step by
step, the method that allows 3DPV to evaluate the PV energy losses caused by
complex shading. We then illustrate the results of this methodology to several
application cases that are encountered in the world of PV systems design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5782</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5782</id><created>2014-10-20</created><updated>2015-03-23</updated><authors><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Sedwards</keyname><forenames>Sean</forenames></author><author><keyname>Traonouez</keyname><forenames>Louis-Marie</forenames></author></authors><title>Lightweight Monte Carlo Verification of Markov Decision Processes with
  Rewards</title><categories>cs.LO</categories><comments>16 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes are useful models of concurrency optimisation
problems, but are often intractable for exhaustive verification methods. Recent
work has introduced lightweight approximative techniques that sample directly
from scheduler space, bringing the prospect of scalable alternatives to
standard numerical model checking algorithms. The focus so far has been on
optimising the probability of a property, but many problems require
quantitative analysis of rewards. In this work we therefore present lightweight
statistical model checking algorithms to optimise the rewards of Markov
decision processes. We consider the standard definitions of rewards used in
model checking, introducing an auxiliary hypothesis test to accommodate
reachability rewards. We demonstrate the performance of our approach on a
number of standard case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5784</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5784</id><created>2014-10-18</created><authors><author><keyname>Hatua</keyname><forenames>Amartya</forenames></author></authors><title>Optimal Feature Selection from VMware ESXi 5.1 Feature Set</title><categories>cs.DC cs.LG</categories><comments>8 Pagee, http://airccse.org/journal/ijccms/current2014.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A study of VMware ESXi 5.1 server has been carried out to find the optimal
set of parameters which suggest usage of different resources of the server.
Feature selection algorithms have been used to extract the optimum set of
parameters of the data obtained from VMware ESXi 5.1 server using esxtop
command. Multiple virtual machines (VMs) are running in the mentioned server.
K-means algorithm is used for clustering the VMs. The goodness of each cluster
is determined by Davies Bouldin index and Dunn index respectively. The best
cluster is further identified by the determined indices. The features of the
best cluster are considered into a set of optimal parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5789</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5789</id><created>2014-10-17</created><authors><author><keyname>Aouadi</keyname><forenames>Mohamed H. E.</forenames></author><author><keyname>Toumi</keyname><forenames>Khalifa</forenames></author><author><keyname>Cavalli</keyname><forenames>Ana</forenames></author></authors><title>Testing Security Policies for Distributed Systems: Vehicular Networks as
  a Case Study</title><categories>cs.CR cs.SE</categories><comments>10 pages, 13 figures, published in IJCSI. ISSN (Print): 1694-0814 |
  ISSN (Online): 1694-0784 http://www.IJCSI.org</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 5, No 2, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasing complexity of distributed systems, security testing is
becoming increasingly critical in insuring reliability of such systems in
relation to their security requirements. . To challenge this issue, we rely in
this paper1 on model based active testing. In this paper we propose a framework
to specify security policies and test their implementation. Our framework makes
it possible to automatically generate test sequences, in order to validate the
conformance of a security policy. This framework contains several new methods
to ease the test case generation. To demonstrate the reliability of our
framework, we present a Vehicular Networks System as an ongoing case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5792</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5792</id><created>2014-10-21</created><authors><author><keyname>Bogomolov</keyname><forenames>Andrey</forenames></author><author><keyname>Lepri</keyname><forenames>Bruno</forenames></author><author><keyname>Pianesi</keyname><forenames>Fabio</forenames></author></authors><title>Generalized Compression Dictionary Distance as Universal Similarity
  Measure</title><categories>stat.ML cs.AI cs.CC cs.IT math.IT</categories><comments>2014 Conference on Big Data from Space (BiDS 14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new similarity measure based on information theoretic measures
which is superior than Normalized Compression Distance for clustering problems
and inherits the useful properties of conditional Kolmogorov complexity. We
show that Normalized Compression Dictionary Size and Normalized Compression
Dictionary Entropy are computationally more efficient, as the need to perform
the compression itself is eliminated. Also they scale linearly with exponential
vector size growth and are content independent. We show that normalized
compression dictionary distance is compressor independent, if limited to
lossless compressors, which gives space for optimizations and implementation
speed improvement for real-time and big data applications. The introduced
measure is applicable for machine learning tasks of parameter-free unsupervised
clustering, supervised learning such as classification and regression, feature
selection, and is applicable for big data problems with order of magnitude
speed increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5801</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5801</id><created>2014-10-20</created><authors><author><keyname>Bono</keyname><forenames>Valentina</forenames></author><author><keyname>Jamal</keyname><forenames>Wasifa</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Maharatna</keyname><forenames>Koushik</forenames></author></authors><title>Artifact reduction in multichannel pervasive EEG using hybrid WPT-ICA
  and WPT-EMD signal decomposition techniques</title><categories>physics.med-ph cs.LG stat.AP stat.ME</categories><comments>5 pages, 6 figures</comments><journal-ref>Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE
  International Conference on, pp. 5864 - 5868, May 2014</journal-ref><doi>10.1109/ICASSP.2014.6854728</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to reduce the muscle artifacts in multi-channel pervasive
Electroencephalogram (EEG) signals, we here propose and compare two hybrid
algorithms by combining the concept of wavelet packet transform (WPT),
empirical mode decomposition (EMD) and Independent Component Analysis (ICA).
The signal cleaning performances of WPT-EMD and WPT-ICA algorithms have been
compared using a signal-to-noise ratio (SNR)-like criterion for artifacts. The
algorithms have been tested on multiple trials of four different artifact cases
viz. eye-blinking and muscle artifacts including left and right hand movement
and head-shaking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5815</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5815</id><created>2014-10-21</created><authors><author><keyname>Majhi</keyname><forenames>Santosh Kumar</forenames></author><author><keyname>Bera</keyname><forenames>Padmalochan</forenames></author></authors><title>OHMF: A Query Based Optimal Healthcare Medication Framework</title><categories>cs.CY cs.IR cs.MA cs.NI</categories><journal-ref>International Journal of Information Processing, 8(3), 1-12, 2014
  ISSN : 0973-8215</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today cloud computing infrastructure is largely being deployed in healthcare
to access various healthcare services easily over the Internet on an as needed
basis. The main advantage of healthcare cloud is that it can be used as a tool
for patients, medical professionals and insurance providers, to query and
coordinate among medical departments, organizations and other healthcare
related hubs. Although healthcare cloud services can enable better medication
process with high responsiveness, but the privacy and other requirements of the
patients need to be ensured in the process. Patients medical data may be
required by the medical professionals, hospitals, diagnostic centers for
analysis and diagnosis. However, data privacy and service quality cannot be
compromised. In other words, there may exist various service providers
corresponding to a specific healthcare service. The main challenge is to find
the appropriate providers that comply best with patients requirement. In this
paper, we propose a query based optimal medication framework to support the
patients healthcare service accessibility comprehensively with considerable
response time. The framework accepts related healthcare queries in natural
language through a comprehensive user-interface and then processes the input
query through a first order logic based evaluation engine and finds all
possible services satisfying the requirements. First order logic is used for
modeling of user requirements and queries. The query evaluation engine is built
using zChaff, a Boolean logic satisfiability solver. The efficacy and usability
of the framework is evaluated with initial case studies on synthetic and real
life healthcare cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5816</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5816</id><created>2014-10-21</created><authors><author><keyname>Bogomolov</keyname><forenames>Andrey</forenames><affiliation>Sandy</affiliation></author><author><keyname>Lepri</keyname><forenames>Bruno</forenames><affiliation>Sandy</affiliation></author><author><keyname>Ferron</keyname><forenames>Michela</forenames><affiliation>Sandy</affiliation></author><author><keyname>Pianesi</keyname><forenames>Fabio</forenames><affiliation>Sandy</affiliation></author><author><keyname>Alex</keyname><affiliation>Sandy</affiliation></author><author><keyname>Pentland</keyname></author></authors><title>Daily Stress Recognition from Mobile Phone Data, Weather Conditions and
  Individual Traits</title><categories>cs.CY cs.LG physics.data-an stat.AP stat.ML</categories><comments>ACM Multimedia 2014, November 3-7, 2014, Orlando, Florida, USA</comments><doi>10.1145/2647868.2654933</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research has proven that stress reduces quality of life and causes many
diseases. For this reason, several researchers devised stress detection systems
based on physiological parameters. However, these systems require that
obtrusive sensors are continuously carried by the user. In our paper, we
propose an alternative approach providing evidence that daily stress can be
reliably recognized based on behavioral metrics, derived from the user's mobile
phone activity and from additional indicators, such as the weather conditions
(data pertaining to transitory properties of the environment) and the
personality traits (data concerning permanent dispositions of individuals). Our
multifactorial statistical model, which is person-independent, obtains the
accuracy score of 72.28% for a 2-class daily stress recognition problem. The
model is efficient to implement for most of multimedia applications due to
highly reduced low-dimensional feature space (32d). Moreover, we identify and
discuss the indicators which have strong predictive power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5845</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5845</id><created>2014-10-21</created><authors><author><keyname>Adcock</keyname><forenames>Aaron</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>O'Brien</keyname><forenames>Michael P.</forenames></author><author><keyname>Reidl</keyname><forenames>Felix</forenames></author><author><keyname>Villaamil</keyname><forenames>Fernando S&#xe1;nchez</forenames></author><author><keyname>Sullivan</keyname><forenames>Blair D.</forenames></author></authors><title>Zig-Zag Numberlink is NP-Complete</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When can $t$ terminal pairs in an $m \times n$ grid be connected by $t$
vertex-disjoint paths that cover all vertices of the grid? We prove that this
problem is NP-complete. Our hardness result can be compared to two previous
NP-hardness proofs: Lynch's 1975 proof without the ``cover all vertices''
constraint, and Kotsuma and Takenaga's 2010 proof when the paths are restricted
to have the fewest possible corners within their homotopy class. The latter
restriction is a common form of the famous Nikoli puzzle \emph{Numberlink}; our
problem is another common form of Numberlink, sometimes called \emph{Zig-Zag
Numberlink} and popularized by the smartphone app \emph{Flow Free}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5846</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5846</id><created>2014-10-21</created><updated>2015-03-18</updated><authors><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Cooperative Non-Orthogonal Multiple Access in 5G Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-orthogonal multiple access (NOMA) has recently received considerable
attention as a promising candidate for 5G systems. A key feature of NOMA is
that users with better channel conditions have prior information about the
messages of the other users. This prior knowledge is fully exploited in this
paper, where a cooperative NOMA scheme is proposed. Outage probability and
diversity order achieved by this cooperative NOMA scheme are analyzed, and an
approach based on user pairing is also proposed to reduce system complexity in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5850</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5850</id><created>2014-10-21</created><authors><author><keyname>D'Andreagiovanni</keyname><forenames>Fabio</forenames></author><author><keyname>Krolikowski</keyname><forenames>Jonatan</forenames></author><author><keyname>Pulaj</keyname><forenames>Jonad</forenames></author></authors><title>A Fast Hybrid Primal Heuristic for Multiband Robust Capacitated Network
  Design with Multiple Time Periods</title><categories>math.OC cs.DS cs.NE</categories><comments>Accepted for publication in Applied Soft Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the Robust Multiperiod Network Design Problem, a
generalization of the Capacitated Network Design Problem (CNDP) that, besides
establishing flow routing and network capacity installation as in a canonical
CNDP, also considers a planning horizon made up of multiple time periods and
protection against fluctuations in traffic volumes. As a remedy against traffic
volume uncertainty, we propose a Robust Optimization model based on Multiband
Robustness (B\&quot;using and D'Andreagiovanni, 2012), a refinement of classical
Gamma-Robustness by Bertsimas and Sim that uses a system of multiple deviation
bands. Since the resulting optimization problem may prove very challenging even
for instances of moderate size solved by a state-of-the-art optimization
solver, we propose a hybrid primal heuristic that combines a randomized fixing
strategy inspired by ant colony optimization, which exploits information coming
from linear relaxations of the problem, and an exact large neighbourhood
search. Computational experiments on a set of realistic instances from the
SNDlib show that our original heuristic can run fast and produce solutions of
extremely high quality associated with low optimality gaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5859</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5859</id><created>2014-10-21</created><updated>2015-02-04</updated><authors><author><keyname>Guha</keyname><forenames>Ramanathan</forenames></author></authors><title>Towards a Model Theory for Distributed Representations</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed representations (such as those based on embeddings) and discrete
representations (such as those based on logic) have complementary strengths. We
explore one possible approach to combining these two kinds of representations.
We present a model theory/semantics for first order logic based on vectors of
reals. We describe the model theory, discuss some interesting properties of
such a system and present a simple approach to query answering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5861</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5861</id><created>2014-10-21</created><authors><author><keyname>Xu</keyname><forenames>Ran</forenames></author><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Xiong</keyname><forenames>Caiming</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Corso</keyname><forenames>Jason J.</forenames></author></authors><title>Compositional Structure Learning for Action Understanding</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of the action understanding literature has predominately been
classification, how- ever, there are many applications demanding richer action
understanding such as mobile robotics and video search, with solutions to
classification, localization and detection. In this paper, we propose a
compositional model that leverages a new mid-level representation called
compositional trajectories and a locally articulated spatiotemporal deformable
parts model (LALSDPM) for fully action understanding. Our methods is
advantageous in capturing the variable structure of dynamic human activity over
a long range. First, the compositional trajectories capture long-ranging,
frequently co-occurring groups of trajectories in space time and represent them
in discriminative hierarchies, where human motion is largely separated from
camera motion; second, LASTDPM learns a structured model with multi-layer
deformable parts to capture multiple levels of articulated motion. We implement
our methods and demonstrate state of the art performance on all three problems:
action detection, localization, and recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5872</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5872</id><created>2014-10-21</created><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Pohl</keyname><forenames>Volker</forenames></author></authors><title>System Approximations and Generalized Measurements in Modern Sampling
  Theory</title><categories>cs.IT math.FA math.IT</categories><comments>This is a preprint of an invited chapter which appears in &quot;Sampling
  Theory - a Renaissance&quot;, Editor: G\&quot;otz Pfander, Springer-Birkh\&quot;auser, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies several aspects of signal reconstruction of sampled data
in spaces of bandlimited functions. In the first part, signal spaces are
characterized in which the classical sampling series uniformly converge, and we
investigate whether adaptive recovery algorithms can yield uniform convergence
in spaces where non-adaptive sampling series does not. In particular, it is
shown that the investigation of adaptive signal recovery algorithms needs
completely new analytic tools since the methods used for non-adaptive
reconstruction procedures, which are based on the celebrated Banach-Steinhaus
theorem, are not applicable in the adaptive case. The second part analyzes the
approximation of the output of stable linear time-invariant (LTI) systems based
on samples of the input signal, and where the input is assumed to belong to the
Paley-Wiener space of bandlimited functions with absolute integrable Fourier
transform. If the samples are acquired by point evaluations of the input signal
f, then there exist stable LTI systems H such that the approximation process
does not converge to the desired output H(f) even if the oversampling factor is
arbitrarily large. If one allows generalized measurements of the input signal,
then the output of every stable LTI system can be uniformly approximated in
terms of generalized measurements of the input signal. The last section studies
the situation where only the amplitudes of the signal samples are known. It is
shown that one can find specific measurement functionals such that signal
recovery of bandlimited signals from amplitude measurement is possible, with an
overall sampling rate of four times the Nyquist rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5877</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5877</id><created>2014-10-21</created><authors><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Callison-Burch</keyname><forenames>Chris</forenames></author></authors><title>Bucking the Trend: Large-Scale Cost-Focused Active Learning for
  Statistical Machine Translation</title><categories>cs.CL cs.LG stat.ML</categories><comments>11 pages, 14 figures; appeared in Proceedings of the 48th Annual
  Meeting of the Association for Computational Linguistics, July 2010</comments><acm-class>I.2.7; I.2.6; I.5.1; I.5.4</acm-class><journal-ref>In Proceedings of the 48th Annual Meeting of the Association for
  Computational Linguistics, pages 854-864, Uppsala, Sweden, July 2010.
  Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore how to improve machine translation systems by adding more
translation data in situations where we already have substantial resources. The
main challenge is how to buck the trend of diminishing returns that is commonly
encountered. We present an active learning-style data solicitation algorithm to
meet this challenge. We test it, gathering annotations via Amazon Mechanical
Turk, and find that we get an order of magnitude increase in performance rates
of improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5884</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5884</id><created>2014-10-21</created><authors><author><keyname>Li</keyname><forenames>Yujia</forenames></author><author><keyname>Zemel</keyname><forenames>Richard</forenames></author></authors><title>Mean-Field Networks</title><categories>cs.LG stat.ML</categories><comments>Published in ICML 2014 workshop on Learning Tractable Probabilistic
  Models</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The mean field algorithm is a widely used approximate inference algorithm for
graphical models whose exact inference is intractable. In each iteration of
mean field, the approximate marginals for each variable are updated by getting
information from the neighbors. This process can be equivalently converted into
a feedforward network, with each layer representing one iteration of mean field
and with tied weights on all layers. This conversion enables a few natural
extensions, e.g. untying the weights in the network. In this paper, we study
these mean field networks (MFNs), and use them as inference tools as well as
discriminative models. Preliminary experiment results show that MFNs can learn
to do inference very efficiently and perform significantly better than mean
field as discriminative models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5890</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5890</id><created>2014-10-21</created><authors><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Yan</keyname><forenames>Shi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Ergodic Capacity Analysis of Remote Radio Head Associations in Cloud
  Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, accepted by IEEE Wireless Communication Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing user to Remote Radio Head (RRH) association strategies in
cloud radio access networks (C-RANs) is critical for performance optimization.
In this letter, the single nearest and N--nearest RRH association strategies
are presented, and the corresponding impact on the ergodic capacity of C-RANs
is analyzed, where RRHs are distributed according to a stationary point
process. Closed-form expressions for the ergodic capacity of the proposed RRH
association strategies are derived. Simulation results demonstrate that the
derived uplink closed-form capacity expressions are accurate. Furthermore, the
analysis and simulation results show that the ergodic capacity gain is not
linear with either the RRH density or the number of antenna per RRH. The
ergodic capacity gain from the RRH density is larger than that from the number
of antennas per RRH,which indicates that the association number of the RRH
should not be bigger than 4 to balance the performance gain and the
implementation cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5894</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5894</id><created>2014-10-21</created><authors><author><keyname>Hadi</keyname><forenames>Raad Ahmed</forenames></author><author><keyname>Sulong</keyname><forenames>Ghazali</forenames></author><author><keyname>George</keyname><forenames>Loay Edwar</forenames></author></authors><title>Vehicle Detection and Tracking Techniques: A Concise Review</title><categories>cs.CV</categories><doi>10.5121/sipij.2013.5101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicle detection and tracking applications play an important role for
civilian and military applications such as in highway traffic surveillance
control, management and urban traffic planning. Vehicle detection process on
road are used for vehicle tracking, counts, average speed of each individual
vehicle, traffic analysis and vehicle categorizing objectives and may be
implemented under different environments changes. In this review, we present a
concise overview of image processing methods and analysis tools which used in
building these previous mentioned applications that involved developing traffic
surveillance systems. More precisely and in contrast with other reviews, we
classified the processing methods under three categories for more clarification
to explain the traffic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5904</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5904</id><created>2014-10-21</created><authors><author><keyname>Kailkhura</keyname><forenames>Bhavya</forenames></author><author><keyname>Brahma</keyname><forenames>Swastik</forenames></author><author><keyname>Dulek</keyname><forenames>Berkan</forenames></author><author><keyname>Han</keyname><forenames>Yunghsiang S</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Distributed Detection in Tree Networks: Byzantines and Mitigation
  Techniques</title><categories>cs.CR cs.DC math.OC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of distributed detection in tree networks in the
presence of Byzantines is considered. Closed form expressions for optimal
attacking strategies that minimize the miss detection error exponent at the
fusion center (FC) are obtained. We also look at the problem from the network
designer's (FC's) perspective. We study the problem of designing optimal
distributed detection parameters in a tree network in the presence of
Byzantines. Next, we model the strategic interaction between the FC and the
attacker as a Leader-Follower (Stackelberg) game. This formulation provides a
methodology for predicting attacker and defender (FC) equilibrium strategies,
which can be used to implement the optimal detector. Finally, a reputation
based scheme to identify Byzantines is proposed and its performance is
analytically evaluated. We also provide some numerical examples to gain
insights into the solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5907</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5907</id><created>2014-10-21</created><authors><author><keyname>Dernoncourt</keyname><forenames>Franck</forenames></author></authors><title>Replacing the computer mouse</title><categories>cs.HC</categories><acm-class>H.5.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In a few months the computer mouse will be half-a-century-old. It is known to
have many drawbacks, the main ones being: loss of productivity due to constant
switching between keyboard and mouse, and health issues such as RSI. Like the
keyboard, it is an unnatural human-computer interface. However the vast
majority of computer users still use computer mice nowadays.
  In this article, we explore computer mouse alternatives. Our research shows
that moving the mouse cursor can be done efficiently with camera-based head
tracking system such as the SmartNav device, and mouse clicks can be emulated
in many complementary ways. We believe that computer users can increase their
productivity and improve their long-term health by using these alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5919</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5919</id><created>2014-10-22</created><updated>2015-11-04</updated><authors><author><keyname>Xiao</keyname><forenames>Yonghui</forenames></author><author><keyname>Xiong</keyname><forenames>Li</forenames></author></authors><title>Protecting Locations with Differential Privacy under Temporal
  Correlations</title><categories>cs.DB cs.CR</categories><comments>Final version Nov-04-2015</comments><doi>10.1145/2810103.2813640</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concerns on location privacy frequently arise with the rapid development of
GPS enabled devices and location-based applications. While spatial
transformation techniques such as location perturbation or generalization have
been studied extensively, most techniques rely on syntactic privacy models
without rigorous privacy guarantee. Many of them only consider static scenarios
or perturb the location at single timestamps without considering temporal
correlations of a moving user's locations, and hence are vulnerable to various
inference attacks. While differential privacy has been accepted as a standard
for privacy protection, applying differential privacy in location based
applications presents new challenges, as the protection needs to be enforced on
the fly for a single user and needs to incorporate temporal correlations
between a user's locations.
  In this paper, we propose a systematic solution to preserve location privacy
with rigorous privacy guarantee. First, we propose a new definition,
&quot;$\delta$-location set&quot; based differential privacy, to account for the temporal
correlations in location data. Second, we show that the well known
$\ell_1$-norm sensitivity fails to capture the geometric sensitivity in
multidimensional space and propose a new notion, sensitivity hull, based on
which the error of differential privacy is bounded. Third, to obtain the
optimal utility we present a planar isotropic mechanism (PIM) for location
perturbation, which is the first mechanism achieving the lower bound of
differential privacy. Experiments on real-world datasets also demonstrate that
PIM significantly outperforms baseline approaches in data utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5920</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5920</id><created>2014-10-22</created><authors><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author><author><keyname>Munos</keyname><forenames>Remi</forenames></author></authors><title>Active Regression by Stratification</title><categories>stat.ML cs.LG</categories><journal-ref>Neural Information Processing Systems, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new active learning algorithm for parametric linear regression
with random design. We provide finite sample convergence guarantees for general
distributions in the misspecified model. This is the first active learner for
this setting that provably can improve over passive learning. Unlike other
learning settings (such as classification), in regression the passive learning
rate of $O(1/\epsilon)$ cannot in general be improved upon. Nonetheless, the
so-called `constant' in the rate of convergence, which is characterized by a
distribution-dependent risk, can be improved in many cases. For a given
distribution, achieving the optimal risk requires prior knowledge of the
distribution. Following the stratification technique advocated in Monte-Carlo
function integration, our active learner approaches the optimal risk using
piecewise constant approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5926</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5926</id><created>2014-10-22</created><authors><author><keyname>Jiang</keyname><forenames>Huaizu</forenames></author><author><keyname>Yuan</keyname><forenames>Zejian</forenames></author><author><keyname>Cheng</keyname><forenames>Ming-Ming</forenames></author><author><keyname>Gong</keyname><forenames>Yihong</forenames></author><author><keyname>Zheng</keyname><forenames>Nanning</forenames></author><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author></authors><title>Salient Object Detection: A Discriminative Regional Feature Integration
  Approach</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Salient object detection has been attracting a lot of interest, and recently
various heuristic computational models have been designed. In this paper, we
formulate saliency map computation as a regression problem. Our method, which
is based on multi-level image segmentation, utilizes the supervised learning
approach to map the regional feature vector to a saliency score. Saliency
scores across multiple levels are finally fused to produce the saliency map.
The contributions lie in two-fold. One is that we propose a discriminate
regional feature integration approach for salient object detection. Compared
with existing heuristic models, our proposed method is able to automatically
integrate high-dimensional regional saliency features and choose discriminative
ones. The other is that by investigating standard generic region properties as
well as two widely studied concepts for salient object detection, i.e.,
regional contrast and backgroundness, our approach significantly outperforms
state-of-the-art methods on six benchmark datasets. Meanwhile, we demonstrate
that our method runs as fast as most existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5932</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5932</id><created>2014-10-22</created><authors><author><keyname>Gao</keyname><forenames>Qian</forenames></author><author><keyname>Gong</keyname><forenames>Chen</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Xu</keyname><forenames>Zhengyuan</forenames></author><author><keyname>Hua</keyname><forenames>Yingbo</forenames></author></authors><title>Constellation Design for Multi-color Visible Light Communications</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel high dimensional constellation design
scheme for visible light communication (VLC) systems employing red/green/blue
light emitting diodes (RGB LEDs). It is in fact a generalized color shift
keying (CSK) scheme which does not suffer efficiency loss due to a constrained
sum intensity for all constellation symbols. Crucial lighting requirements are
included as optimization constraints. To control non-linear distortion, the
optical peak-to-average-power ratio (PAPR) of LEDs is individually constrained.
Fixing the average optical power, our scheme is able to achieve much lower
bit-error rate (BER) than conventional schems especially when illumination
color is more &quot;unbalanced&quot;. When cross-talks exist among the multiple optical
channels, we apply a singular value decomposition (SVD)-based pre-equalizer and
redesign the constellations, and such scheme is shown to outperform
post-equalized schemes based on zero-forcing or linear
minimum-mean-squared-error (LMMSE) principles. To further reduce system BER, a
binary switching algorithm (BSA) is employed the first time for labeling high
dimensional constellation. We thus obtains the optimal bits-to-symbols mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5944</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5944</id><created>2014-10-22</created><authors><author><keyname>Anouari</keyname><forenames>Tarik</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author></authors><title>An Improved UGS Scheduling with QoE Metrics in WiMAX Network</title><categories>cs.NI</categories><comments>6 pages, 8 figures</comments><journal-ref>(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 12, No. 9, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiMAX (Worldwide Interoperability for Microwave Access) technology has
emerged in response to the increasing demand for multimedia services in the
internet broadband networks. WiMAX standard has defined five different
scheduling services to meet the QoS (Quality of Service) requirement of
multimedia applications and this paper investigates one specific scheduling
service, i.e. UGS scheduling. In parallel, it was observed that in the
difference of the traditional quality assessment approaches, nowadays, current
researches are centered on the user perception of the quality, the existing
scheduling approaches take into account the QoS, mobility and many other
parameters, but do not consider the Quality of Experience (QoE). In order to
control the packet transmission rate so as to match with the minimum subjective
rate requirements of each user and therefore reduce packet loss and delays, an
efficient scheduling approach has been proposed in this paper. The solution has
been implemented and evaluated in the WiMAX simulation platform developed based
on NS-2. Simulation results show that by applying various levels of MOS (Mean
Opinion Score) the QoE provided to the users is enhanced in term of jitter,
packet loss rate, throughput and delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5952</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5952</id><created>2014-10-22</created><updated>2015-11-30</updated><authors><author><keyname>Ernestus</keyname><forenames>Maximilian</forenames></author><author><keyname>Friedrichs</keyname><forenames>Stephan</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Kokem&#xfc;ller</keyname><forenames>Jan</forenames></author><author><keyname>Kr&#xf6;ller</keyname><forenames>Alexander</forenames></author><author><keyname>Moeini</keyname><forenames>Mahdi</forenames></author><author><keyname>Schmidt</keyname><forenames>Christiane</forenames></author></authors><title>Algorithms for Art Gallery Illumination</title><categories>cs.CG</categories><comments>20 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Art Gallery Problem (AGP) is one of the classical problems in
computational geometry. It asks for the minimum number of guards required to
achieve visibility coverage of a given polygon. AGP is well-known to be NP-hard
even in restricted cases. In this paper, we consider the AGP with Fading
(AGPF): A polygonal region is to be illuminated with light sources such that
every point is illuminated with at least a global threshold. Light intensity
decreases over distance. We seek to minimize the total energy consumption.
Choosing fading exponents of zero, one and two are equivalent to AGP, laser
scanner applications, and natural light, respectively. We present complexity
results as well as a negative solvability result. Still, we propose two
practical algorithms for AGPF with fixed light positions (e.g., vertex guards)
independent of the fading exponent, which we demonstrate to work well in
practice. One is based on a discrete approximation, the other on nonlinear
programming by means of simplex partitioning strategies. The former approach
yields a fully polynomial-time approximation scheme for AGPF with fixed light
positions. The latter approach obtains better results in our experimental
evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5958</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5958</id><created>2014-10-22</created><updated>2014-11-26</updated><authors><author><keyname>Brandl</keyname><forenames>Florian</forenames></author></authors><title>An Application of Fixed-point Theory to Probabilistic Social Choice</title><categories>cs.GT cs.MA</categories><comments>This paper has been withdrawn due to a mistake in the proof of the
  main theorem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this note is to prove the existence of a randomized mechanism,
a social decision scheme (SDS), with desirable fairness, efficiency, and
strategyproofness properties unmatched by all known SDSs. In particular, we
disprove a conjecture by Aziz et al. (2013). Additionally, we obtain a strong
existence result for the domain of random assignment. Both, the notion of
efficiency and strategyproofness are based on stochastic dominance and have
been studied extensively for random assignment. The proof makes crucial use of
Brouwer's fixed-point theorem and is hence non-constructive. To the best of our
knowledge, this is the first application of a fixed-point theorem to show the
existence of a social choice function or mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5967</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5967</id><created>2014-10-22</created><authors><author><keyname>Janson</keyname><forenames>Svante</forenames></author><author><keyname>Viola</keyname><forenames>Alfredo</forenames></author></authors><title>A unified approach to linear probing hashing with buckets</title><categories>cs.DS</categories><comments>49 pages</comments><msc-class>60W40, 68P10, 68P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a unified analysis of linear probing hashing with a general bucket
size. We use both a combinatorial approach, giving exact formulas for
generating functions, and a probabilistic approach, giving simple derivations
of asymptotic results. Both approaches complement nicely, and give a good
insight in the relation between linear probing and random walks. A key
methodological contribution, at the core of Analytic Combinatorics, is the use
of the symbolic method (based on q-calculus) to directly derive the generating
functions to analyze.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5976</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5976</id><created>2014-10-22</created><authors><author><keyname>Luckeneder</keyname><forenames>Michael</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>Uncovering the Perfect Place: Optimising Workflow Engine Deployment in
  the Cloud</title><categories>cs.DC</categories><comments>Extended Abstract for the ACM International Symposium on
  High-Performance Parallel and Distributed Computing (HPDC 2013) Poster Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When orchestrating highly distributed and data-intensive Web service
workflows the geographical placement of the orchestration engine can greatly
affect the overall performance of a workflow. We present CloudForecast: a Web
service framework and analysis tool which, given a workflow specification,
computes the optimal Amazon EC2 Cloud region to automatically deploy the
orchestration engine and execute the workflow. We use geographical distance of
the workflow, network latency and HTTP round-trip time between Amazon Cloud
regions and the workflow nodes to find a ranking of Cloud regions. This overall
ranking predicts where the workflow orchestration engine should be deployed in
order to reduce overall execution time. Our experimental results show that our
proposed optimisation strategy, depending on the particular workflow, can speed
up execution time on average by 82.25% compared to local execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5977</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5977</id><created>2014-10-22</created><authors><author><keyname>Aubram</keyname><forenames>Daniel</forenames></author></authors><title>Optimization-based smoothing algorithm for triangle meshes over
  arbitrarily shaped domains</title><categories>cs.NA math.NA</categories><comments>27 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a node relocation algorithm based on nonlinear
optimization which delivers excellent results for both unstructured and
structured plane triangle meshes over convex as well as non-convex domains with
high curvature. The local optimization scheme is a damped Newton's method in
which the gradient and Hessian of the objective function are evaluated exactly.
The algorithm has been developed in order to continuously rezone the mesh in
arbitrary Lagrangian-Eulerian (ALE) methods for large deformation penetration
problems, but it is also suitable for initial mesh improvement. Numerical
examples highlight the capabilities of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5993</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5993</id><created>2014-10-22</created><authors><author><keyname>Schnoor</keyname><forenames>Henning</forenames></author></authors><title>The Relative Succinctness and Expressiveness of Modal Logics Can Be
  Arbitrarily Complex</title><categories>cs.LO</categories><comments>29 pages</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relative succinctness and expressiveness of modal logics, and
prove that these relationships can be as complex as any countable partial
order. For this, we use two uniform formalisms to define modal operators, and
obtain results on succinctness and expressiveness in these two settings. Our
proofs are based on formula size games introduced by Adler and Immerman and
bisimulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.5996</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.5996</id><created>2014-10-22</created><updated>2015-06-28</updated><authors><author><keyname>V'yugin</keyname><forenames>Vladimir</forenames></author></authors><title>Log-Optimal Portfolio Selection Using the Blackwell Approachability
  Theorem</title><categories>cs.AI q-fin.PM</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for constructing the log-optimal portfolio using the
well-calibrated forecasts of market values. Dawid's notion of calibration and
the Blackwell approachability theorem are used for computing well-calibrated
forecasts. We select a portfolio using this &quot;artificial&quot; probability
distribution of market values. Our portfolio performs asymptotically at least
as well as any stationary portfolio that redistributes the investment at each
round using a continuous function of side information. Unlike in classical
mathematical finance theory, no stochastic assumptions are made about market
values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6001</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6001</id><created>2014-10-22</created><authors><author><keyname>Zhang</keyname><forenames>Jingyuan</forenames></author><author><keyname>Shi</keyname><forenames>Xiaoxiao</forenames></author><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Shuai</keyname><forenames>Hong-Han</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Discovering Organizational Correlations from Twitter</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Organizational relationships are usually very complex in real life. It is
difficult or impossible to directly measure such correlations among different
organizations, because important information is usually not publicly available
(e.g., the correlations of terrorist organizations). Nowadays, an increasing
amount of organizational information can be posted online by individuals and
spread instantly through Twitter. Such information can be crucial for detecting
organizational correlations. In this paper, we study the problem of discovering
correlations among organizations from Twitter. Mining organizational
correlations is a very challenging task due to the following reasons: a) Data
in Twitter occurs as large volumes of mixed information. The most relevant
information about organizations is often buried. Thus, the organizational
correlations can be scattered in multiple places, represented by different
forms; b) Making use of information from Twitter collectively and judiciously
is difficult because of the multiple representations of organizational
correlations that are extracted. In order to address these issues, we propose
multi-CG (multiple Correlation Graphs based model), an unsupervised framework
that can learn a consensus of correlations among organizations based on
multiple representations extracted from Twitter, which is more accurate and
robust than correlations based on a single representation. Empirical study
shows that the consensus graph extracted from Twitter can capture the
organizational correlations effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6022</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6022</id><created>2014-10-22</created><authors><author><keyname>Peters</keyname><forenames>Thomas</forenames></author></authors><title>The physics of volume rendering</title><categories>astro-ph.IM cs.GR physics.ed-ph</categories><journal-ref>Eur. J. Phys. 35 (2014) 065028</journal-ref><doi>10.1088/0143-0807/35/6/065028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radiation transfer is an important topic in several physical disciplines,
probably most prominently in astrophysics. Computer scientists use radiation
transfer, among other things, for the visualisation of complex data sets with
direct volume rendering. In this note, I point out the connection between
physical radiation transfer and volume rendering, and I describe an
implementation of direct volume rendering in the astrophysical radiation
transfer code RADMC-3D. I show examples for the use of this module on
analytical models and simulation data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6026</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6026</id><created>2014-10-22</created><updated>2015-07-23</updated><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author></authors><title>A Survey on the Local Divisor Technique</title><categories>cs.FL cs.LO</categories><msc-class>68Q42, 68Q45, 68Q70, 20M3</msc-class><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local divisors allow a powerful induction scheme on the size of a monoid. We
survey this technique by giving several examples of this proof method. These
applications include linear temporal logic, rational expressions with Kleene
stars restricted to prefix codes with bounded synchronization delay,
Church-Rosser congruential languages, and Simon's Factorization Forest Theorem.
We also introduce the notion of localizable language class as a new abstract
concept which unifies some of the proofs for the results above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6028</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6028</id><created>2014-10-22</created><authors><author><keyname>Upadhya</keyname><forenames>Karthik</forenames></author><author><keyname>Seelamantula</keyname><forenames>Chandra Sekhar</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>A Risk Minimization Framework for Channel Estimation in OFDM Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of channel estimation for cyclic-prefix (CP)
Orthogonal Frequency Division Multiplexing (OFDM) systems. We model the channel
as a vector of unknown deterministic constants and hence, do not require prior
knowledge of the channel statistics. Since the mean-square error (MSE) is not
computable in practice, in such a scenario, we propose a novel technique using
Stein's lemma to obtain an unbiased estimate of the mean-square error, namely
the Stein's unbiased risk estimate (SURE). We obtain an estimate of the channel
from noisy observations using linear and nonlinear denoising functions, whose
parameters are chosen to minimize SURE. Based on computer simulations, we show
that using SURE-based channel estimate in equalization offers an improvement in
signal-to-noise ratio of around 2.25 dB over the maximum-likelihood channel
estimate, in practical channel scenarios, without assuming prior knowledge of
channel statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6030</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6030</id><created>2014-10-22</created><authors><author><keyname>Ishii</keyname><forenames>Toshimasa</forenames></author><author><keyname>Makino</keyname><forenames>Kazuhisa</forenames></author></authors><title>Posimodular Function Optimization</title><categories>cs.DS</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a posimodular function $f: 2^V \to \mathbb{R}$ on a finite set $V$, we
consider the problem of finding a nonempty subset $X$ of $V$ that minimizes
$f(X)$. Posimodular functions often arise in combinatorial optimization such as
undirected cut functions. In this paper, we show that any algorithm for the
problem requires $\Omega(2^{\frac{n}{7.54}})$ oracle calls to $f$, where
$n=|V|$. It contrasts to the fact that the submodular function minimization,
which is another generalization of cut functions, is polynomially solvable.
  When the range of a given posimodular function is restricted to be
$D=\{0,1,...,d\}$ for some nonnegative integer $d$, we show that
$\Omega(2^{\frac{d}{15.08}})$ oracle calls are necessary, while we propose an
$O(n^dT_f+n^{2d+1})$-time algorithm for the problem. Here, $T_f$ denotes the
time needed to evaluate the function value $f(X)$ for a given $X \subseteq V$.
  We also consider the problem of maximizing a given posimodular function. We
show that $\Omega(2^{n-1})$ oracle calls are necessary for solving the problem,
and that the problem has time complexity $\Theta(n^{d-1}T_f) $ when
$D=\{0,1,..., d\}$ is the range of $f$ for some constant $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6038</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6038</id><created>2014-10-22</created><updated>2015-04-13</updated><authors><author><keyname>Thomas</keyname><forenames>Anoop</forenames></author><author><keyname>R.</keyname><forenames>Kavitha</forenames></author><author><keyname>A.</keyname><forenames>Chandramouli</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Optimal Index Coding with Min-Max Probability of Error over Fading
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An index coding scheme in which the source (transmitter) transmits binary
symbols over a wireless fading channel is considered. Index codes with the
transmitter using minimum number of transmissions are known as optimal index
codes. Different optimal index codes give different performances in terms of
probability of error in a fading environment and this also varies from receiver
to receiver. In this paper we deal with optimal index codes which minimizes the
maximum probability of error among all the receivers. We identify a criterion
for optimal index codes that minimizes the maximum probability of error among
all the receivers. For a special class of index coding problems, we give an
algorithm to identify optimal index codes which minimize the maximum error
probability. We illustrate our techniques and claims with simulation results
leading to conclude that a careful choice among the optimal index codes will
give a considerable gain in fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6039</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6039</id><created>2014-10-22</created><authors><author><keyname>Sebastiani</keyname><forenames>Roberto</forenames></author><author><keyname>Tomasi</keyname><forenames>Silvia</forenames></author></authors><title>Optimization Modulo Theories with Linear Rational Costs</title><categories>cs.LO</categories><comments>Submitted on january 2014 to ACM Transactions on Computational Logic,
  currently under revision. arXiv admin note: text overlap with arXiv:1202.1409</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the contexts of automated reasoning (AR) and formal verification (FV),
important decision problems are effectively encoded into Satisfiability Modulo
Theories (SMT). In the last decade efficient SMT solvers have been developed
for several theories of practical interest (e.g., linear arithmetic, arrays,
bit-vectors). Surprisingly, little work has been done to extend SMT to deal
with optimization problems; in particular, we are not aware of any previous
work on SMT solvers able to produce solutions which minimize cost functions
over arithmetical variables. This is unfortunate, since some problems of
interest require this functionality.
  In the work described in this paper we start filling this gap. We present and
discuss two general procedures for leveraging SMT to handle the minimization of
linear rational cost functions, combining SMT with standard minimization
techniques. We have implemented the procedures within the MathSAT SMT solver.
Due to the absence of competitors in the AR, FV and SMT domains, we have
experimentally evaluated our implementation against state-of-the-art tools for
the domain of linear generalized disjunctive programming (LGDP), which is
closest in spirit to our domain, on sets of problems which have been previously
proposed as benchmarks for the latter tools. The results show that our tool is
very competitive with, and often outperforms, these tools on these problems,
clearly demonstrating the potential of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6044</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6044</id><created>2014-10-22</created><authors><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Sharma</keyname><forenames>Subodh</forenames></author><author><keyname>Wachter</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>AbPress: Flexing Partial-Order Reduction and Abstraction</title><categories>cs.LO</categories><comments>15 pages, 7 figures, under submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial-order reduction (POR) and lazy abstraction with interpolants are two
complementary techniques that have been successfully employed to make model
checking tools for concurrent programs effective. In this work, we present
AbPress - Abstraction-based Partial-order Reduction with Source-Sets - an
algorithm that fuses a recently proposed and powerful dynamic POR technique
based on source-sets and lazy abstraction to obtain an efficient software model
checker for multi-threaded programs. It trims the inter- leaving space by
taking the abstraction and source-sets into account. We amplify the
effectiveness of AbPress with a novel solution that summarizes the accesses to
shared variables over a collection of interleavings. We have implemented
AbPress in a tool that analyzes concurrent programs using lazy abstraction,
viz., Impara. Our evaluation on the effectiveness of the presented approach has
been encouraging. AbPress compares favorably to existing state-of-the-art tools
in the landscape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6063</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6063</id><created>2014-10-22</created><updated>2014-12-15</updated><authors><author><keyname>Mici&#x107;</keyname><forenames>Ivana</forenames></author><author><keyname>Jan&#x10d;i&#x107;</keyname><forenames>Zorana</forenames></author><author><keyname>Ignjatovi&#x107;</keyname><forenames>Jelena</forenames></author><author><keyname>&#x106;iri&#x107;</keyname><forenames>Miroslav</forenames></author></authors><title>Determinization of fuzzy automata by means of the degrees of language
  inclusion</title><categories>cs.FL</categories><comments>Submitted to IEEE Transactions on Fuzzy Systems. arXiv admin note:
  substantial text overlap with arXiv:1402.6510, arXiv:1311.5799</comments><msc-class>68Q45, 68Q70, 68T37, 03E72</msc-class><acm-class>F.1.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determinization of fuzzy finite automata is understood here as a procedure of
their conversion into equivalent crisp-deterministic fuzzy automata, which can
be viewed as being deterministic with possibly infinitely many states, but with
fuzzy sets of terminal states. Particularly significant determinization methods
are those that provide a minimal crisp-deterministic fuzzy automaton equivalent
to the original fuzzy finite automaton, called canonization methods. One
canonization method for fuzzy finite automata, the Brzozowski type
determinization, has been developed recently by Jan\v{c}i\'{c} and
\'{C}iri\'{c} in [10]. Here we provide another canonization method for a fuzzy
finite automaton $\cal A=(A,\sigma, \delta,\tau)$ over a complete residuated
lattice $\cal L$, based on the degrees of inclusion of the right fuzzy
languages associated with states of $\cal A$ into the left derivatives of the
fuzzy language recognized by $\cal A$. The proposed procedure terminates in a
finite number of steps whenever the membership values taken by $\delta $,
$\sigma $ and $\tau $ generate a finite subsemiring of the semiring reduct of
$\cal L$. This procedure is generally faster than the Brzozowski type
determinization, and if the basic operations in the residuated lattice $\cal L$
can be performed in constant time, it has the same computational time as all
other determinization procedures provided in [8], [11], [12].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6064</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6064</id><created>2014-10-22</created><updated>2016-01-13</updated><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author><author><keyname>Gupta</keyname><forenames>Ankit</forenames></author><author><keyname>Khammash</keyname><forenames>Mustafa</forenames></author></authors><title>Antithetic Integral Feedback ensures robust perfect adaptation in noisy
  biomolecular networks</title><categories>math.OC cs.SY math.PR q-bio.MN</categories><comments>20 pages, 5 figures</comments><doi>10.1016/j.cels.2016.01.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homeostasis is a running theme in biology. Often achieved through feedback
regulation strategies, homeostasis allows living cells to control their
internal environment as a means for surviving changing and unfavourable
environments. While many endogenous homeostatic motifs have been studied in
living cells, some other motifs may remain under-explored or even undiscovered.
At the same time, known regulatory motifs have been mostly analyzed at the
deterministic level, and the effect of noise on their regulatory function has
received low attention. Here we lay the foundation for a regulation theory at
the molecular level that explicitly takes into account the noisy nature of
biochemical reactions and provides novel tools for the analysis and design of
robust homeostatic circuits. Using these ideas, we propose a new regulation
motif, which we refer to as {\em antithetic integral feedback, and demonstrate
its effectiveness as a strategy for generically regulating a wide class of
reaction networks. By combining tools from probability and control theory, we
show that the proposed motif preserves the stability of the overall network,
steers the population of any regulated species to a desired set point, and
achieves robust perfect adaptation -- all with low prior knowledge of reaction
rates. Moreover, our proposed regulatory motif can be implemented using a very
small number of molecules and hence has a negligible metabolic load.
Strikingly, the regulatory motif exploits stochastic noise, leading to enhanced
regulation in scenarios where noise-free implementations result in
dysregulation. Finally, we discuss the possible manifestation of the proposed
antithetic integral feedback motif in endogenous biological circuits and its
realization in synthetic circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6072</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6072</id><created>2014-10-22</created><authors><author><keyname>Friedland</keyname><forenames>Shmuel</forenames></author><author><keyname>Lim</keyname><forenames>Lek-Heng</forenames></author></authors><title>Computational Complexity of Tensor Nuclear Norm</title><categories>cs.CC quant-ph</categories><comments>10 pages</comments><msc-class>15A69, 68Q15, 81-08</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main result of this paper shows that the weak membership problem in the
unit ball of a given norm is NP-hard if and only if the weak membership problem
in the unit ball of the dual norm is NP-hard. Equivalently, the approximation
of a given norm is polynomial time if and only if the approximation of the dual
norm is polynomial time. Using the NP-hardness of the approximation of the
spectral norm of tensors we show that the approximation of the nuclear norm is
NP-hard. We relate our results to bipartite separable states in quantum
mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6075</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6075</id><created>2014-10-22</created><authors><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Prabhu</keyname><forenames>Vinayak S.</forenames></author></authors><title>Computing the Skorokhod Distance between Polygonal Traces (Full Paper)</title><categories>cs.SY cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \emph{Skorokhod distance} is a natural metric on traces of continuous and
hybrid systems. For two traces, from $[0,T]$ to values in a metric space $O$,
it measures the best match between the traces when allowed continuous bijective
timing distortions. Formally, it computes the infimum, over all timing
distortions, of the maximum of two components: the first component quantifies
the {\em timing discrepancy} of the timing distortion, and the second
quantifies the mismatch (in the metric space $O$) of the values under the
timing distortion. Skorokhod distances appear in various fundamental hybrid
systems analysis concerns: from definitions of hybrid systems semantics and
notions of equivalence, to practical problems such as checking the closeness of
models or the quality of simulations. Despite its popularity and extensive
theoretical use, the \emph{computation} problem for the Skorokhod distance
between two finite sampled-time hybrid traces has remained open.
  We address in this work the problem of computing the Skorokhod distance
between two polygonal traces (these traces arise when sampled-time traces are
completed by linear interpolation between sample points). We provide the first
algorithm to compute the exact Skorokhod distance when trace values are in
$\reals^n$ for the $L_1$, $L_2$, and $L_{\infty}$ norms. Our algorithm, based
on a reduction to Frechet distances, is fully polynomial-time, and incorporates
novel polynomial-time procedures for a set of geometric primitives in
$\reals^n$ over the three norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6079</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6079</id><created>2014-10-22</created><updated>2015-01-07</updated><authors><author><keyname>Biryukov</keyname><forenames>Alex</forenames></author><author><keyname>Pustogarov</keyname><forenames>Ivan</forenames></author></authors><title>Bitcoin over Tor isn't a good idea</title><categories>cs.CR</categories><comments>11 pages, 4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is a decentralized P2P digital currency in which coins are generated
by a distributed set of miners and transaction are broadcasted via a
peer-to-peer network. While Bitcoin provides some level of anonymity (or rather
pseudonymity) by encouraging the users to have any number of random-looking
Bitcoin addresses, recent research shows that this level of anonymity is rather
low. This encourages users to connect to the Bitcoin network through
anonymizers like Tor and motivates development of default Tor functionality for
popular mobile SPV clients. In this paper we show that combining Tor and
Bitcoin creates an attack vector for the deterministic and stealthy
man-in-the-middle attacks. A low-resource attacker can gain full control of
information flows between all users who chose to use Bitcoin over Tor. In
particular the attacker can link together user's transactions regardless of
pseudonyms used, control which Bitcoin blocks and transactions are relayed to
the user and can \ delay or discard user's transactions and blocks. In
collusion with a powerful miner double-spending attacks become possible and a
totally virtual Bitcoin reality can be created for such set of users. Moreover,
we show how an attacker can fingerprint users and then recognize them and learn
their IP address when they decide to connect to the Bitcoin network directly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6093</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6093</id><created>2014-10-22</created><authors><author><keyname>Gunay</keyname><forenames>Osman</forenames></author><author><keyname>Akbas</keyname><forenames>Cem Emre</forenames></author><author><keyname>Cetin</keyname><forenames>A. Enis</forenames></author></authors><title>Cosine Similarity Measure According to a Convex Cost Function</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a new vector similarity measure associated with a
convex cost function. Given two vectors, we determine the surface normals of
the convex function at the vectors. The angle between the two surface normals
is the similarity measure. Convex cost function can be the negative entropy
function, total variation (TV) function and filtered variation function. The
convex cost function need not be differentiable everywhere. In general, we need
to compute the gradient of the cost function to compute the surface normals. If
the gradient does not exist at a given vector, it is possible to use the
subgradients and the normal producing the smallest angle between the two
vectors is used to compute the similarity measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6094</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6094</id><created>2014-10-22</created><authors><author><keyname>Blanco-Chac&#xf3;n</keyname><forenames>Iv&#xe1;n</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Alsina</keyname><forenames>Montserrat</forenames></author><author><keyname>Rem&#xf3;n</keyname><forenames>Dion&#xed;s</forenames></author></authors><title>Fuchsian codes with arbitrarily high code rate</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, so-called Fuchsian codes have been proposed in [I. Blanco-Chac\'on
et al., &quot;Nonuniform Fuchsian codes for noisy channels&quot;, J. of the Franklin
Institute 2014] for communication over channels subject to additive white
Gaussian noise (AWGN). The two main advantages of Fuchsian codes are their
ability to compress information, i.e., high code rate, and their logarithmic
decoding complexity. In this paper, we improve the first property further by
constructing Fuchsian codes with arbitrarily high code rates while maintaining
logarithmic decoding complexity. Namely, in the case of Fuchsian groups derived
from quaternion algebras over totally real fields we obtain a code rate that is
proportional to the degree of the base field. In particular, we consider
arithmetic Fuchsian groups of signature (1;e) to construct explicit codes
having code rate six, meaning that we can transmit six independent integers
during one channel use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6095</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6095</id><created>2014-10-22</created><authors><author><keyname>Kekatos</keyname><forenames>Vassilis</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author><author><keyname>Baldick</keyname><forenames>Ross</forenames></author></authors><title>Online Energy Price Matrix Factorization for Power Grid Topology
  Tracking</title><categories>stat.ML cs.LG math.OC stat.AP</categories><comments>Submitted to the IEEE Trans. on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grid security and open markets are two major smart grid goals. Transparency
of market data facilitates a competitive and efficient energy environment, yet
it may also reveal critical physical system information. Recovering the grid
topology based solely on publicly available market data is explored here.
Real-time energy prices are calculated as the Lagrange multipliers of
network-constrained economic dispatch; that is, via a linear program (LP)
typically solved every 5 minutes. Granted the grid Laplacian is a parameter of
this LP, one could infer such a topology-revealing matrix upon observing
successive LP dual outcomes. The matrix of spatio-temporal prices is first
shown to factor as the product of the inverse Laplacian times a sparse matrix.
Leveraging results from sparse matrix decompositions, topology recovery schemes
with complementary strengths are subsequently formulated. Solvers scalable to
high-dimensional and streaming market data are devised. Numerical validation
using real load data on the IEEE 30-bus grid provide useful input for current
and future market designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6097</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6097</id><created>2014-10-22</created><updated>2014-11-03</updated><authors><author><keyname>D'Angeli</keyname><forenames>Daniele</forenames></author><author><keyname>Rodaro</keyname><forenames>Emanuele</forenames></author></authors><title>Freeness of automata groups vs boundary dynamics</title><categories>math.GR cs.FL</categories><msc-class>20E08, 20F69, 20F65, 37E25, 68Q70, 05C63</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the boundary dynamics of the (semi)group generated by the
enriched dual transducer characterizes the algebraic property of being free for
an automaton group. We specialize this result to the class of bireversible
transducers and we show that the property of being not free is equivalent to
have a finite Schreier graph in the boundary of the enriched dual pointed on
some essentially non-trivial point. From these results we derive some
consequences from the dynamical, algorithmic and algebraic point of view. In
the last part of the paper we address the problem of finding examples of
non-bireversible transducers defining free groups, we show examples of
transducers with sink accessible from every state which generate free groups,
and, in general, we link this problem to the nonexistence of certain words with
interesting combinatorial and geometrical properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6108</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6108</id><created>2014-10-22</created><updated>2015-02-16</updated><authors><author><keyname>Hamon</keyname><forenames>Ronan</forenames></author><author><keyname>Borgnat</keyname><forenames>Pierre</forenames></author><author><keyname>Flandrin</keyname><forenames>Patrick</forenames></author><author><keyname>Robardet</keyname><forenames>C&#xe9;line</forenames></author></authors><title>Discovering the structure of complex networks by minimizing cyclic
  bandwidth sum</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Getting a labeling of vertices close to the structure of the graph has been
proved to be of interest in many applications e.g., to follow smooth signals
indexed by the vertices of the network. This question can be related to a graph
labeling problem known as the cyclic bandwidth sum problem. It consists in
finding a labeling of the vertices of an undirected and unweighted graph with
distinct integers such that the sum of (cyclic) difference of labels of
adjacent vertices is minimized. Although theoretical results exist that give
optimal value of cyclic bandwidth sum for standard graphs, there are neither
results in the general case, nor explicit methods to reach this optimal result.
In addition to this lack of theoretical knowledge, only a few methods have been
proposed to approximately solve this problem. In this paper, we introduce a new
heuristic to find an approximate solution for the cyclic bandwidth sum problem,
by following the structure of the graph. The heuristic is a two-step algorithm:
the first step consists of traversing the graph to find a set of paths which
follow the structure of the graph, using a similarity criterion based on the
Jaccard index to jump from one vertex to the next one. The second step is the
merging of all obtained paths, based on a greedy approach that extends a
partial solution by inserting a new path at the position that minimizes the
cyclic bandwidth sum. The effectiveness of the proposed heuristic, both in
terms of performance and time execution, is shown through experiments on graphs
whose optimal value of CBS is known as well as on real-world networks, where
the consistence between labeling and topology is highlighted. An extension to
weighted graphs is also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6118</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6118</id><created>2014-10-22</created><updated>2014-10-27</updated><authors><author><keyname>Serra</keyname><forenames>Edoardo</forenames></author><author><keyname>Spezzano</keyname><forenames>Francesca</forenames></author><author><keyname>Subrahmanian</keyname><forenames>V. S.</forenames></author></authors><title>ChoiceGAPs: Competitive Diffusion as a Massive Multi-Player Game in
  Social Networks</title><categories>cs.LO cs.SI</categories><comments>The paper is currently submitted to a journal suggesting parallel
  submission to the CoRR repository</comments><acm-class>I.2.4; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of modeling competitive diffusion in real world
social networks via the notion of ChoiceGAPs which combine choice logic
programs due to Sacca` and Zaniolo and Generalized Annotated Programs due to
Kifer and Subrahmanian. We assume that each vertex in a social network is a
player in a multi-player game (with a huge number of players) - the choice part
of the ChoiceGAPs describe utilities of players for acting in various ways
based on utilities of their neighbors in those and other situations. We define
multi-player Nash equilibrium for such programs - but because they require some
conditions that are hard to satisfy in the real world, we introduce a new
model-theoretic concept of strong equilibrium. We show that stable equilibria
can capture all Nash equilibria. We prove a host of complexity (intractability)
results for checking existence of strong equilibria (as well as related
counting complexity results), together with algorithms to find them. We then
identify a class of ChoiceGAPs for which stable equilibria can be polynomially
computed. We develop algorithms for computing these equilibria under various
restrictions. We come up with the important concept of an estimation query
which can compute quantities w.r.t. a given strong equilibrium, and approximate
ranges of values (answers) across the space of strong equilibria. Even though
we show that computing range answers to estimation queries exactly is
intractable, we are able to identify classes of estimation queries that can be
answered in polynomial time. We report on experiments we conducted with a
real-world FaceBook data set surrounding the 2013 Italian election showing that
our algorithms have good predictive accuracy with an Area Under a ROC Curve
that, on average, is over 0.76.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6121</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6121</id><created>2014-10-22</created><updated>2014-12-09</updated><authors><author><keyname>Freericks</keyname><forenames>J. K.</forenames></author><author><keyname>Nikolic</keyname><forenames>B. K.</forenames></author><author><keyname>Frieder</keyname><forenames>O.</forenames></author></authors><title>The Nonequilibrium Many-Body Problem as a paradigm for extreme data
  science</title><categories>cond-mat.str-el cond-mat.stat-mech cs.CC cs.CE math-ph math.MP</categories><comments>33 pages, 7 figures, invited review for Int. J. Mod. Phys. B;
  published version with additional references</comments><journal-ref>Int J. Mod. Phys. B 28, 1430021 (2014)</journal-ref><doi>10.1142/S0217979214300217</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating big data pervades much of physics. But some problems, which we
call extreme data problems, are too large to be treated within big data
science. The nonequilibrium quantum many-body problem on a lattice is just such
a problem, where the Hilbert space grows exponentially with system size and
rapidly becomes too large to fit on any computer (and can be effectively
thought of as an infinite-sized data set). Nevertheless, much progress has been
made with computational methods on this problem, which serve as a paradigm for
how one can approach and attack extreme data problems. In addition, viewing
these physics problems from a computer-science perspective leads to new
approaches that can be tried to solve them more accurately and for longer
times. We review a number of these different ideas here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6122</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6122</id><created>2014-10-22</created><updated>2015-08-06</updated><authors><author><keyname>Dell'Amico</keyname><forenames>Matteo</forenames></author><author><keyname>Carra</keyname><forenames>Damiano</forenames></author><author><keyname>Michiardi</keyname><forenames>Pietro</forenames></author></authors><title>PSBS: Practical Size-Based Scheduling</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1403.5996</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Size-based schedulers have very desirable performance properties: optimal or
near-optimal response time can be coupled with strong fairness guarantees.
Despite this, such systems are very rarely implemented in practical settings,
because they require knowing a priori the amount of work needed to complete
jobs: this assumption is very difficult to satisfy in concrete systems. It is
definitely more likely to inform the system with an estimate of the job sizes,
but existing studies point to somewhat pessimistic results if existing
scheduler policies are used based on imprecise job size estimations. We take
the goal of designing scheduling policies that are explicitly designed to deal
with inexact job sizes: first, we show that existing size-based schedulers can
have bad performance with inexact job size information when job sizes are
heavily skewed; we show that this issue, and the pessimistic results shown in
the literature, are due to problematic behavior when large jobs are
underestimated. Once the problem is identified, it is possible to amend
existing size-based schedulers to solve the issue. We generalize FSP -- a fair
and efficient size-based scheduling policy -- in order to solve the problem
highlighted above; in addition, our solution deals with different job weights
(that can be assigned to a job independently from its size). We provide an
efficient implementation of the resulting protocol, which we call Practical
Size-Based Scheduler (PSBS). Through simulations evaluated on synthetic and
real workloads, we show that PSBS has near-optimal performance in a large
variety of cases with inaccurate size information, that it performs fairly and
it handles correctly job weights. We believe that this work shows that PSBS is
indeed pratical, and we maintain that it could inspire the design of schedulers
in a wide array of real-world use cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6126</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6126</id><created>2014-10-22</created><authors><author><keyname>Ros</keyname><forenames>German</forenames></author><author><keyname>Alvarez</keyname><forenames>Jose</forenames></author><author><keyname>Guerrero</keyname><forenames>Julio</forenames></author></authors><title>Motion Estimation via Robust Decomposition with Constrained Rank</title><categories>cs.CV</categories><comments>Submitted to IEEE TIP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the problem of outlier detection for robust motion
estimation by using modern sparse-low-rank decompositions, i.e., Robust
PCA-like methods, to impose global rank constraints. Robust decompositions have
shown to be good at splitting a corrupted matrix into an uncorrupted low-rank
matrix and a sparse matrix, containing outliers. However, this process only
works when matrices have relatively low rank with respect to their ambient
space, a property not met in motion estimation problems. As a solution, we
propose to exploit the partial information present in the decomposition to
decide which matches are outliers. We provide evidences showing that even when
it is not possible to recover an uncorrupted low-rank matrix, the resulting
information can be exploited for outlier detection. To this end we propose the
Robust Decomposition with Constrained Rank (RD-CR), a proximal gradient based
method that enforces the rank constraints inherent to motion estimation. We
also present a general framework to perform robust estimation for stereo Visual
Odometry, based on our RD-CR and a simple but effective compressed optimization
method that achieves high performance. Our evaluation on synthetic data and on
the KITTI dataset demonstrates the applicability of our approach in complex
scenarios and it yields state-of-the-art performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6142</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6142</id><created>2014-10-22</created><updated>2014-12-21</updated><authors><author><keyname>Riedl</keyname><forenames>Mark O.</forenames></author></authors><title>The Lovelace 2.0 Test of Artificial Creativity and Intelligence</title><categories>cs.AI</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Observing that the creation of certain types of artistic artifacts
necessitate intelligence, we present the Lovelace 2.0 Test of creativity as an
alternative to the Turing Test as a means of determining whether an agent is
intelligent. The Lovelace 2.0 Test builds off prior tests of creativity and
additionally provides a means of directly comparing the relative intelligence
of different agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6146</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6146</id><created>2014-10-22</created><authors><author><keyname>Van Do</keyname><forenames>Tien</forenames><affiliation>Budapest University of Technology and Economics</affiliation></author><author><keyname>Vu</keyname><forenames>Binh T.</forenames><affiliation>Budapest University of Technology and Economics</affiliation></author><author><keyname>Do</keyname><forenames>Nam H.</forenames><affiliation>Budapest University of Technology and Economics</affiliation></author><author><keyname>Farkas</keyname><forenames>L&#xf3;r&#xe1;nt</forenames><affiliation>Nokia</affiliation></author><author><keyname>Rotter</keyname><forenames>Csaba</forenames><affiliation>Nokia</affiliation></author><author><keyname>Tarj&#xe1;nyi</keyname><forenames>Tam&#xe1;s</forenames><affiliation>Nokia</affiliation></author></authors><title>Building Block Components to Control a Data Rate in the Apache Hadoop
  Compute Platform</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Resource management is one of the most indispensable components of
cluster-level infrastructure layers. Users of such systems should be able to
specify their job requirements as a configuration parameter (CPU, RAM, disk
I/O, network I/O) and have the scheduler translate those into an appropriate
reservation and allocation of resources. YARN is an emerging resource
management in the Hadoop ecosystem, which supports only RAM and CPU reservation
at present.
  In this paper, we propose a solution that takes into account the operation of
the Hadoop Distributed File System to control the data rate of applications in
the framework of a Hadoop compute platform. We utilize the property that a data
pipe between a container and a DataNode consists of a disk I/O subpipe and a
TCP/IP subpipe. We have implemented building block software components to
control the data rate of data pipes between containers and DataNodes and
provide a proof-of-concept with measurement results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6153</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6153</id><created>2014-10-22</created><updated>2015-05-12</updated><authors><author><keyname>Ivorra</keyname><forenames>Benjamin</forenames></author><author><keyname>Ngom</keyname><forenames>Di&#xe8;ne</forenames></author><author><keyname>Ramos</keyname><forenames>&#xc1;ngel Manuel</forenames></author></authors><title>Be-CoDiS: A mathematical model to predict the risk of human diseases
  spread between countries. Validation and application to the 2014-15 Ebola
  Virus Disease epidemic</title><categories>q-bio.PE cs.CE math.DS physics.soc-ph</categories><comments>34 pages; Version 5; Work in Progress</comments><msc-class>97M10, 37N25, 92D25, 97M60</msc-class><acm-class>G.1.7; I.6.4; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ebola virus disease is a lethal human and primate disease that currently
requires a particular attention from the international health authorities due
to important outbreaks in some Western African countries and isolated cases in
the United Kingdom, the USA and Spain. Regarding the emergency of this
situation, there is a need of development of decision tools, such as
mathematical models, to assist the authorities to focus their efforts in
important factors to eradicate Ebola. In this work, we propose a novel
deterministic spatial-temporal model, called Be-CoDiS (Between-Countries
Disease Spread), to study the evolution of human diseases within and between
countries. The main interesting characteristics of Be-CoDiS are the
consideration of the movement of people between countries, the control measure
effects and the use of time dependent coefficients adapted to each country.
First, we focus on the mathematical formulation of each component of the model
and explain how its parameters and inputs are obtained. Then, in order to
validate our approach, we consider two numerical experiments regarding the
2014-15 Ebola epidemic. The first one studies the ability of the model in
predicting the EVD evolution between countries starting from the index cases in
Guinea in December 2013. The second one consists of forecasting the evolution
of the epidemic by using some recent data. The results obtained with Be-CoDiS
are compared to real data and other models outputs found in the literature.
Finally, a brief parameter sensitivity analysis is done. A free Matlab version
of Be-CoDiS is available at: http://www.mat.ucm.es/momat/software.htm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6154</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6154</id><created>2014-10-22</created><authors><author><keyname>Anouari</keyname><forenames>Tarik</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author></authors><title>A QoE-Based Scheduling Algorithm for UGS Service Class in WiMAX Network</title><categories>cs.NI</categories><comments>5 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:1410.5944</comments><journal-ref>International Journal of Soft Computing and Engineering
  (IJSCE)ISSN: 2231-2307, Volume-4, Issue-1, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To satisfy the increasing demand for multimedia services in broadband
Internet networks, the WiMAX (Worldwide Interoperability for Microwave Acces)
technology has emerged as an alternative to the wired broadband access
solutions. It provides an Internet connection to broadband coverage area of
several kilometers in radius by ensuring a satisfactory quality of service
(QoS), it's an adequate response to some rural or inaccessible areas. Unlike
DSL (Digital Subscriber Line) or other wired technology, WiMAX uses radio waves
and can provide point-to-multipoint (PMP) and point-to-point (P2P) modes. In
parallel, it's observed that in the opposite of the traditional quality
evaluation approaches, nowadays, current researches focus on the user perceived
quality, the existing scheduling algorithms take into account the QoS and many
other parameters, but not the Quality of Experience (QoE). In this paper, we
present a QoE-based scheduling solution in WiMAX network in order to make the
scheduling of the UGS connections based on the use of QoE metrics. Indeed, the
proposed solution allows controlling the packet transmission rate so as to
match with the minimum subjective rate requirements of each user. Simulation
results show that by applying various levels of mean opinion score (MOS) the
QoE provided to the users is improved in term of throughput, jitter, packet
loss rate and delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6220</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6220</id><created>2014-10-22</created><authors><author><keyname>Nayebi</keyname><forenames>Aran</forenames></author><author><keyname>Williams</keyname><forenames>Virginia Vassilevska</forenames></author></authors><title>Quantum algorithms for shortest paths problems in structured instances</title><categories>quant-ph cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the quantum time complexity of the all pairs shortest paths
(APSP) problem and some of its variants. The trivial classical algorithm for
APSP and most all pairs path problems runs in $O(n^3)$ time, while the trivial
algorithm in the quantum setting runs in $\tilde{O}(n^{2.5})$ time, using
Grover search. A major open problem in classical algorithms is to obtain a
truly subcubic time algorithm for APSP, i.e. an algorithm running in
$O(n^{3-\varepsilon})$ time for constant $\varepsilon&gt;0$. To approach this
problem, many truly subcubic time classical algorithms have been devised for
APSP and its variants for structured inputs. Some examples of such problems are
APSP in geometrically weighted graphs, graphs with small integer edge weights
or a small number of weights incident to each vertex, and the all pairs
earliest arrivals problem. In this paper we revisit these problems in the
quantum setting and obtain the first nontrivial (i.e. $O(n^{2.5-\varepsilon})$
time) quantum algorithms for the problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6242</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6242</id><created>2014-10-23</created><authors><author><keyname>Wang</keyname><forenames>Pengfei</forenames></author></authors><title>Forward period analysis and the long term simulation of a periodic
  Hamiltonian system</title><categories>math.NA cs.NA nlin.CD</categories><comments>3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The period of a Morse oscillator and mathematical pendulum system are
obtained, accurate to 100 significant digits, by forward period analysis (FPA).
From these results, the long-term [0, 10^60] (time unit) solutions, which
overlap from the Planck time to the age of the universe, are computed reliably
and quickly with a parallel multiple-precision Taylor series (PMT) scheme. The
application of FPA to periodic systems can reduce the computation loops of
long-term reliable simulation from O(t^(1+1/M)) to O(lnt+t/h0) where T is the
period, M the order and h0 a constant step-size. This scheme provides a way to
generate reference solutions to test other schemes' long-term simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6264</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6264</id><created>2014-10-23</created><authors><author><keyname>Perina</keyname><forenames>Alessandro</forenames></author><author><keyname>Jojic</keyname><forenames>Nebojsa</forenames></author></authors><title>Capturing spatial interdependence in image features: the counting grid,
  an epitomic representation for bags of features</title><categories>cs.CV stat.ML</categories><comments>The counting grid code is available at www.alessandroperina.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent scene recognition research images or large image regions are often
represented as disorganized &quot;bags&quot; of features which can then be analyzed using
models originally developed to capture co-variation of word counts in text.
However, image feature counts are likely to be constrained in different ways
than word counts in text. For example, as a camera pans upwards from a building
entrance over its first few floors and then further up into the sky Fig. 1,
some feature counts in the image drop while others rise -- only to drop again
giving way to features found more often at higher elevations. The space of all
possible feature count combinations is constrained both by the properties of
the larger scene and the size and the location of the window into it. To
capture such variation, in this paper we propose the use of the counting grid
model. This generative model is based on a grid of feature counts, considerably
larger than any of the modeled images, and considerably smaller than the real
estate needed to tile the images next to each other tightly. Each modeled image
is assumed to have a representative window in the grid in which the feature
counts mimic the feature distribution in the image. We provide a learning
procedure that jointly maps all images in the training set to the counting grid
and estimates the appropriate local counts in it. Experimentally, we
demonstrate that the resulting representation captures the space of feature
count combinations more accurately than the traditional models, not only when
the input images come from a panning camera, but even when modeling images of
different scenes from the same category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6268</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6268</id><created>2014-10-23</created><updated>2015-01-31</updated><authors><author><keyname>Dongol</keyname><forenames>Brijesh</forenames></author><author><keyname>Derrick</keyname><forenames>John</forenames></author></authors><title>Verifying linearizability: A comparative survey</title><categories>cs.LO</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linearizability has become the key correctness criterion for concurrent data
structures, ensuring that histories of the concurrent object under
consideration are consistent, where consistency is judged with respect to a
sequential history of a corresponding abstract data structure. Linearizability
allows any order of concurrent (i.e., overlapping) calls to operations to be
picked, but requires the real-time order of non-overlapping to be preserved.
Over the years numerous techniques for verifying linearizability have been
developed, using a variety of formal foundations such as refinement, shape
analysis, reduction, etc. However, as the underlying framework, nomenclature
and terminology for each method differs, it has become difficult for
practitioners to judge the differences between each approach, and hence, judge
the methodology most appropriate for the data structure at hand. We compare the
major of methods used to verify linearizability, describe the main contribution
of each method, and compare their advantages and limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6277</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6277</id><created>2014-10-23</created><authors><author><keyname>Banisch</keyname><forenames>Sven</forenames></author></authors><title>The Probabilistic Structure of Discrete Agent-Based Models</title><categories>cs.MA cs.CY nlin.AO physics.comp-ph physics.soc-ph</categories><journal-ref>Discontinuity, Nonlinearity, and Complexity, 3:281--292, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a formalization of agent-based models (ABMs) as random
walks on regular graphs and relates the symmetry group of those graphs to a
coarse-graining of the ABM that is still Markovian. An ABM in which $N$ agents
can be in $\delta$ different states leads to a Markov chain with $\delta^N$
states. In ABMs with a sequential update scheme by which one agent is chosen to
update its state at a time, transitions are only allowed between system
configurations that differ with respect to a single agent. This characterizes
ABMs as random walks on regular graphs. The non-trivial automorphisms of those
graphs make visible the dynamical symmetries that an ABM gives rise to because
sets of micro configurations can be interchanged without changing the
probability structure of the random walk. This allows for a systematic
loss-less reduction of the state space of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6289</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6289</id><created>2014-10-23</created><updated>2015-03-02</updated><authors><author><keyname>Dorn</keyname><forenames>Sebastian</forenames></author><author><keyname>En&#xdf;lin</keyname><forenames>Torsten A.</forenames></author><author><keyname>Greiner</keyname><forenames>Maksim</forenames></author><author><keyname>Selig</keyname><forenames>Marco</forenames></author><author><keyname>Boehm</keyname><forenames>Vanessa</forenames></author></authors><title>Signal inference with unknown response: Calibration-uncertainty
  renormalized estimator</title><categories>physics.data-an astro-ph.IM cs.IT math.IT stat.ML</categories><journal-ref>PhysRevE 91, 013311 (2015)</journal-ref><doi>10.1103/PhysRevE.91.013311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The calibration of a measurement device is crucial for every scientific
experiment, where a signal has to be inferred from data. We present CURE, the
calibration uncertainty renormalized estimator, to reconstruct a signal and
simultaneously the instrument's calibration from the same data without knowing
the exact calibration, but its covariance structure. The idea of CURE,
developed in the framework of information field theory, is starting with an
assumed calibration to successively include more and more portions of
calibration uncertainty into the signal inference equations and to absorb the
resulting corrections into renormalized signal (and calibration) solutions.
Thereby, the signal inference and calibration problem turns into solving a
single system of ordinary differential equations and can be identified with
common resummation techniques used in field theories. We verify CURE by
applying it to a simplistic toy example and compare it against existent
self-calibration schemes, Wiener filter solutions, and Markov Chain Monte Carlo
sampling. We conclude that the method is able to keep up in accuracy with the
best self-calibration methods and serves as a non-iterative alternative to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6295</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6295</id><created>2014-10-23</created><authors><author><keyname>Beck</keyname><forenames>Martin</forenames></author></authors><title>Enhanced TKIP Michael Attacks</title><categories>cs.CR</categories><comments>10 pages, 7 figures. Report was written in 2010, referenced several
  times in relevant literature</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents new attacks against TKIP within IEEE 802.11 based
networks. Using the known Beck-Tews attack, we define schemas to con- tinuously
generate new keystreams, which allow more and longer arbitrary packets to be
injected into the network. We further describe an attack against the Michael
message integrity code, that allows an attacker to concatenate a known with an
unknown valid TKIP packet such that the unknown MIC at the end is still valid
for the new entire packet. Based on this, a schema to decrypt all traffic that
flows towards the client is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6298</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6298</id><created>2014-10-23</created><authors><author><keyname>De Benedetti</keyname><forenames>Erika</forenames></author><author><keyname>Della Rocca</keyname><forenames>Simona Ronchi</forenames></author></authors><title>A type assignment for lambda-calculus complete both for FPTIME and
  strong normalization</title><categories>cs.LO cs.CC</categories><comments>31 pages</comments><msc-class>03D15</msc-class><acm-class>F.4; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the aims of Implicit Computational Complexity is the design of
programming languages with bounded computational complexity; indeed,
guaranteeing and certifying a limited resources usage is of central importance
for various aspects of computer science. One of the more promising approaches
to this aim is based on the use of lambda-calculus as paradigmatic programming
language and the design of type assignment systems for lambda-terms, where
types guarantee both the functional correctness and the complexity bound. Here
we propose a system of stratified types, inspired by intersection types, where
intersection is a non-associative operator. The system, called STR, is correct
and complete for polynomial time computations; moreover, all the strongly
normalizing terms are typed in it, thus increasing the typing power with
respect to the previous proposals. Moreover, STR enjoys a stronger expressivity
with respect to the previous system STA, since it allows to type a restricted
version of iteration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6313</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6313</id><created>2014-10-23</created><updated>2015-11-05</updated><authors><author><keyname>Li</keyname><forenames>Junhua</forenames></author><author><keyname>Li</keyname><forenames>Chao</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Canonical Polyadic Decomposition with Auxiliary Information for Brain
  Computer Interface</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physiological signals are often organized in the form of multiple dimensions
(e.g., channel, time, task, and 3D voxel), so it is better to preserve original
organization structure when processing. Unlike vector-based methods that
destroy data structure, Canonical Polyadic Decomposition (CPD) aims to process
physiological signals in the form of multi-way array, which considers
relationships between dimensions and preserves structure information contained
by the physiological signal. Nowadays, CPD is utilized as an unsupervised
method for feature extraction in a classification problem. After that, a
classifier, such as support vector machine, is required to classify those
features. In this manner, classification task is achieved in two isolated
steps. We proposed supervised Canonical Polyadic Decomposition by directly
incorporating auxiliary label information during decomposition, by which a
classification task can be achieved without an extra step of classifier
training. The proposed method merges the decomposition and classifier learning
together, so it reduces procedure of classification task compared with that of
respective decomposition and classification. In order to evaluate the
performance of the proposed method, three different kinds of signals, synthetic
signal, EEG signal, and MEG signal, were used. The results based on evaluations
of synthetic and real signals demonstrated that the proposed method is
effective and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6333</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6333</id><created>2014-10-23</created><updated>2015-04-13</updated><authors><author><keyname>van Gennip</keyname><forenames>Yves</forenames></author><author><keyname>Athavale</keyname><forenames>Prashant</forenames></author><author><keyname>Gilles</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Choksi</keyname><forenames>Rustum</forenames></author></authors><title>A Regularization Approach to Blind Deblurring and Denoising of QR
  Barcodes</title><categories>cs.CV math.NA</categories><comments>14 pages, 19 figures (with a total of 57 subfigures), 1 table</comments><msc-class>68U10, 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  QR bar codes are prototypical images for which part of the image is a priori
known (required patterns). Open source bar code readers, such as ZBar, are
readily available. We exploit both these facts to provide and assess purely
regularization-based methods for blind deblurring of QR bar codes in the
presence of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6335</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6335</id><created>2014-10-23</created><authors><author><keyname>Hron</keyname><forenames>Pavel</forenames></author><author><keyname>Jost</keyname><forenames>Daniel</forenames></author><author><keyname>Bastian</keyname><forenames>Peter</forenames></author><author><keyname>Gallert</keyname><forenames>Claudia</forenames></author><author><keyname>Winter</keyname><forenames>Josef</forenames></author><author><keyname>Ippisch</keyname><forenames>Olaf</forenames></author></authors><title>Application of reactive transport modelling to growth and transport of
  microorganisms in the capillary fringe</title><categories>cs.CE physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multicomponent multiphase reactive transport simulator has been developed
to facilitate the investigation of a large variety of phenomena in porous media
including component transport, diffusion, microbiological growth and decay,
cell attachment and detachment and phase exchange. The coupled problem is
solved using operator splitting. This approach allows a flexible adaptation of
the solution strategy to the concrete problem.
  Moreover, the individual submodels were optimised to be able to describe
behaviour of Escherichia coli (HB101 K12 pGLO) in the capillary fringe in the
presence or absence of dissolved organic carbon and oxygen under steady-state
and flow conditions. Steady-state and flow through experiments in a Hele-Shaw
cell, filled with quartz sand, were conducted to study eutrophic bacterial
growth and transport in both saturated and unsaturated porous media. As E. coli
cells can form the green fluorescent protein (GFP), the cell densities,
calculated by evaluation of measured fluorescence intensities (in situ
detection) were compared with the cell densities computed by numerical
simulation. The comparison showed the laboratory experiments can be well
described by our mathematical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6339</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6339</id><created>2014-10-23</created><updated>2015-01-13</updated><authors><author><keyname>Ernvall</keyname><forenames>Toni</forenames></author><author><keyname>Westerb&#xe4;ck</keyname><forenames>Thomas</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Freij</keyname><forenames>Ragnar</forenames></author></authors><title>Constructions and Properties of Linear Locally Repairable Codes</title><categories>cs.IT math.IT</categories><comments>32 pages. Second code construction in Section V is corrected in this
  version. Also, some typos are corrected. The results remain the same.
  Submitted to IEEE Transactions on Information Theory. This is extended,
  generalized, and clarified version of arXiv:1408.0180</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, locally repairable codes with all-symbol locality are studied.
Methods to modify already existing codes are presented. Also, it is shown that
with high probability, a random matrix with a few extra columns guaranteeing
the locality property, is a generator matrix for a locally repairable code with
a good minimum distance. The proof of this also gives a constructive method to
find locally repairable codes. Constructions are given of three infinite
classes of optimal vector-linear locally repairable codes over an alphabet of
small size, not depending on the size of the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6361</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6361</id><created>2014-10-23</created><updated>2015-08-17</updated><authors><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author><author><keyname>Powell</keyname><forenames>Thomas</forenames></author></authors><title>Spector bar recursion over finite partial functions</title><categories>cs.LO math.LO</categories><comments>28 pages</comments><msc-class>03D65, 03F03, 03F10, 03F25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new, demand-driven variant of Spector's bar recursion in the
spirit of the Berardi-Bezem-Coquand functional. The recursion takes place over
finite partial functions $u$, where the control parameter $\varphi$, used in
Spector's bar recursion to terminate the computation at sequences $s$
satisfying $\varphi(\hat{s})&lt;|s|$, now acts as a guide for deciding exactly
where to make bar recursive updates, terminating the computation whenever
$\varphi(\hat{u})\in\mbox{dom}(u)$. We begin by exploring theoretical aspects
of this new form of recursion, then in the main part of the paper we show that
demand-driven bar recursion can be directly used to give an alternative
functional interpretation of classical countable choice. We provide a short
case study as an illustration, in which we extract a new bar recursive program
from the proof that there is no injection from $\mathbb{N}\to\mathbb{N}$ to
$\mathbb{N}$, and compare this to the program that would be obtained using
Spector's original variant. We conclude by formally establishing that our new
bar recursor is primitive recursively equivalent to the original Spector bar
recursion, and thus defines the same class of functionals when added to
G\&quot;odel's system $\sf T$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6375</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6375</id><created>2014-10-23</created><updated>2016-01-12</updated><authors><author><keyname>DeCastro</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Ehlers</keyname><forenames>Ruediger</forenames></author><author><keyname>Rungger</keyname><forenames>Matthias</forenames></author><author><keyname>Balkan</keyname><forenames>Ayca</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author><author><keyname>Kress-Gazit</keyname><forenames>Hadas</forenames></author></authors><title>Dynamics-Based Reactive Synthesis and Automated Revisions for High-Level
  Robot Control</title><categories>cs.RO</categories><comments>25 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to address issues where formal specifications cannot
be realized on a given dynamical system subjected to a changing environment.
Such failures occur whenever the dynamics of the system restrict the robot in
such a way that the environment may prevent the robot from progressing safely
to its goals. We provide a framework that automatically synthesizes revisions
to such specifications that restrict the assumed behaviors of the environment
and the behaviors of the system. We provide a means for explaining such
modifications to the user in a concise, easy-to-understand manner. Integral to
the framework is a new algorithm for synthesizing controllers for reactive
specifications that include a discrete representation of the robot's dynamics.
The new approach is demonstrated with a complex task implemented using a
unicycle model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6382</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6382</id><created>2014-10-23</created><authors><author><keyname>Kukliansky</keyname><forenames>Doron</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Attribute Efficient Linear Regression with Data-Dependent Sampling</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze a budgeted learning setting, in which the learner
can only choose and observe a small subset of the attributes of each training
example. We develop efficient algorithms for ridge and lasso linear regression,
which utilize the geometry of the data by a novel data-dependent sampling
scheme. When the learner has prior knowledge on the second moments of the
attributes, the optimal sampling probabilities can be calculated precisely, and
result in data-dependent improvements factors for the excess risk over the
state-of-the-art that may be as large as $O(\sqrt{d})$, where $d$ is the
problem's dimension. Moreover, under reasonable assumptions our algorithms can
use less attributes than full-information algorithms, which is the main concern
in budgeted learning settings. To the best of our knowledge, these are the
first algorithms able to do so in our setting. Where no such prior knowledge is
available, we develop a simple estimation technique that given a sufficient
amount of training examples, achieves similar improvements. We complement our
theoretical analysis with experiments on several data sets which support our
claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6387</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6387</id><created>2014-10-23</created><authors><author><keyname>Arjevani</keyname><forenames>Yossi</forenames></author></authors><title>On Lower and Upper Bounds in Smooth Strongly Convex Optimization - A
  Unified Approach via Linear Iterative Methods</title><categories>math.OC cs.LG</categories><comments>A related paper co-authored with Shai Shalev-Shwartz and Ohad Shamir
  is to be published soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis we develop a novel framework to study smooth and strongly
convex optimization algorithms, both deterministic and stochastic. Focusing on
quadratic functions we are able to examine optimization algorithms as a
recursive application of linear operators. This, in turn, reveals a powerful
connection between a class of optimization algorithms and the analytic theory
of polynomials whereby new lower and upper bounds are derived. In particular,
we present a new and natural derivation of Nesterov's well-known Accelerated
Gradient Descent method by employing simple 'economic' polynomials. This rather
natural interpretation of AGD contrasts with earlier ones which lacked a
simple, yet solid, motivation. Lastly, whereas existing lower bounds are only
valid when the dimensionality scales with the number of iterations, our lower
bound holds in the natural regime where the dimensionality is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6396</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6396</id><created>2014-10-23</created><updated>2014-10-27</updated><authors><author><keyname>De Biasi</keyname><forenames>Marzio</forenames></author></authors><title>Permutation Reconstruction from Differences</title><categories>cs.CC</categories><comments>22 pages, appears in The Electronic Journal of Combinatorics 21(4)
  (2014); #P4.3</comments><msc-class>68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the problem of reconstructing a permutation
$\pi_1,\dotsc,\pi_n$ of the integers $[1\dotso n]$ given the absolute
differences $|\pi_{i+1}-\pi_i|$, $i = 1,\dotsc,n-1$ is NP-complete. As an
intermediate step we first prove the NP-completeness of the decision version of
a new puzzle game that we call Crazy Frog Puzzle. The permutation
reconstruction from differences is one of the simplest combinatorial problems
that have been proved to be computationally intractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6397</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6397</id><created>2014-10-23</created><authors><author><keyname>Mehrabian</keyname><forenames>Abbas</forenames></author></authors><title>Justifying the small-world phenomenon via random recursive trees</title><categories>cs.DM cs.SI math.CO</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new technique for proving logarithmic upper bounds for diameters
of evolving random graph models, which is based on defining a coupling between
random graphs and variants of random recursive trees. The advantage of the
technique is three-fold: it is quite simple and provides short proofs, it is
applicable to a broad variety of models including those incorporating
preferential attachment, and it provides bounds with small constants. We
illustrate this by proving, for the first time, logarithmic upper bounds for
the diameters of the following well known models: the forest fire model, the
copying model, the PageRank-based selection model, the Aiello-Chung-Lu models,
the generalized linear preference model, directed scale-free graphs, the
Cooper-Frieze model, and random unordered increasing $k$-trees. Our results
shed light on why the small-world phenomenon is observed in so many real-world
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6400</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6400</id><created>2014-10-23</created><authors><author><keyname>Fountoulakis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Friedrich</keyname><forenames>Tobias</forenames></author><author><keyname>Hermelin</keyname><forenames>Danny</forenames></author></authors><title>On the Average-case Complexity of Parameterized Clique</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-Clique problem is a fundamental combinatorial problem that plays a
prominent role in classical as well as in parameterized complexity theory. It
is among the most well-known NP-complete and W[1]-complete problems. Moreover,
its average-case complexity analysis has created a long thread of research
already since the 1970s. Here, we continue this line of research by studying
the dependence of the average-case complexity of the k-Clique problem on the
parameter k. To this end, we define two natural parameterized analogs of
efficient average-case algorithms. We then show that k-Clique admits both
analogues for Erd\H{o}s-R\'{e}nyi random graphs of arbitrary density. We also
show that k-Clique is unlikely to admit neither of these analogs for some
specific computable input distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6413</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6413</id><created>2014-10-23</created><authors><author><keyname>Bochkarev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Maslennikova</keyname><forenames>Yulia S.</forenames></author></authors><title>Initialization of multilayer forecasting artifical neural networks</title><categories>cs.NE stat.ME</categories><comments>9 pages, 3 figures</comments><msc-class>62M45, 62M10, 68T05</msc-class><acm-class>I.5.1</acm-class><journal-ref>Uchenye Zapiski Kazanskogo Universiteta. Seriya
  Fiziko-Matematicheskie Nauki, 2010, vol. 152, no. 1, pp. 7-14. (In Russian)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new method was developed for initialising artificial neural
networks predicting dynamics of time series. Initial weighting coefficients
were determined for neurons analogously to the case of a linear prediction
filter. Moreover, to improve the accuracy of the initialization method for a
multilayer neural network, some variants of decomposition of the transformation
matrix corresponding to the linear prediction filter were suggested. The
efficiency of the proposed neural network prediction method by forecasting
solutions of the Lorentz chaotic system is shown in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6414</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6414</id><created>2014-10-21</created><authors><author><keyname>Shang</keyname><forenames>Jingbo</forenames></author><author><keyname>Chen</keyname><forenames>Tianqi</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author></authors><title>A Parallel and Efficient Algorithm for Learning to Match</title><categories>cs.LG cs.AI</categories><comments>10 pages, short version was published in ICDM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many tasks in data mining and related fields can be formalized as matching
between objects in two heterogeneous domains, including collaborative
filtering, link prediction, image tagging, and web search. Machine learning
techniques, referred to as learning-to-match in this paper, have been
successfully applied to the problems. Among them, a class of state-of-the-art
methods, named feature-based matrix factorization, formalize the task as an
extension to matrix factorization by incorporating auxiliary features into the
model. Unfortunately, making those algorithms scale to real world problems is
challenging, and simple parallelization strategies fail due to the complex
cross talking patterns between sub-tasks. In this paper, we tackle this
challenge with a novel parallel and efficient algorithm for feature-based
matrix factorization. Our algorithm, based on coordinate descent, can easily
handle hundreds of millions of instances and features on a single machine. The
key recipe of this algorithm is an iterative relaxation of the objective to
facilitate parallel updates of parameters, with guaranteed convergence on
minimizing the original objective function. Experimental results demonstrate
that the proposed method is effective on a wide range of matching problems,
with efficiency significantly improved upon the baselines while accuracy
retained unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6433</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6433</id><created>2014-10-23</created><updated>2016-02-25</updated><authors><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Uzna&#x144;ski</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Tight tradeoffs for approximating palindromes in streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider computing the longest palindrome in a text of length $n$ in the
streaming model, where the characters arrive one-by-one, and we do not have
random access to the input. While computing the answer exactly using sublinear
memory is not possible in such a setting, one can still hope for a good
approximation guarantee.
  We focus on the two most natural variants, where we aim for either additive
or multiplicative approximation of the length of the longest palindrome. We
first show that there is no point in considering Las Vegas algorithms in such a
setting, as they cannot achieve sublinear space complexity. For Monte Carlo
algorithms, we provide a lowerbound of $\Omega(\frac{n}{E})$ bits for
approximating the answer with additive error $E$, and $\Omega(\frac{\log
n}{\log(1+\varepsilon)})$ bits for approximating the answer with multiplicative
error $(1+\varepsilon)$ for the binary alphabet. Then, we construct a generic
Monte Carlo algorithm, which by choosing the parameters appropriately achieves
space complexity matching up to a logarithmic factor for both variants. This
substantially improves the previous results by Berenbrink et al. (STACS 2014)
and essentially settles the space complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6447</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6447</id><created>2014-10-23</created><authors><author><keyname>Zhao</keyname><forenames>Ji</forenames></author><author><keyname>Meng</keyname><forenames>Deyu</forenames></author><author><keyname>Ma</keyname><forenames>Jiayi</forenames></author></authors><title>Density-Based Region Search with Arbitrary Shape for Object Localization</title><categories>cs.CV</categories><doi>10.1049/iet-cvi.2014.0442</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Region search is widely used for object localization. Typically, the region
search methods project the score of a classifier into an image plane, and then
search the region with the maximal score. The recently proposed region search
methods, such as efficient subwindow search and efficient region search, %which
localize objects from the score distribution on an image are much more
efficient than sliding window search. However, for some classifiers and tasks,
the projected scores are nearly all positive, and hence maximizing the score of
a region results in localizing nearly the entire images as objects, which is
meaningless.
  In this paper, we observe that the large scores are mainly concentrated on or
around objects. Based on this observation, we propose a method, named level set
maximum-weight connected subgraph (LS-MWCS), which localizes objects with
arbitrary shapes by searching regions with the densest score rather than the
maximal score. The region density can be controlled by a parameter flexibly.
And we prove an important property of the proposed LS-MWCS, which guarantees
that the region with the densest score can be searched. Moreover, the LS-MWCS
can be efficiently optimized by belief propagation. The method is evaluated on
the problem of weakly-supervised object localization, and the quantitative
results demonstrate the superiorities of our LS-MWCS compared to other
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6449</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6449</id><created>2014-10-23</created><updated>2014-11-06</updated><authors><author><keyname>Chen</keyname><forenames>Jiahao</forenames></author><author><keyname>Edelman</keyname><forenames>Alan</forenames></author></authors><title>Parallel Prefix Polymorphism Permits Parallelization, Presentation &amp;
  Proof</title><categories>cs.PL</categories><comments>10 pages, 3 figures. Proceedings of HPTCDL, the 1st Workshop on High
  Performance Technical Computing in Dynamic Languages, November 17, 2014, New
  Orleans, Louisiana, USA. Supporting Information available at
  http://jiahao.github.io/parallel-prefix</comments><acm-class>D.1.3; D.3.2; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polymorphism in programming languages enables code reuse. Here, we show that
polymorphism has broad applicability far beyond computations for technical
computing: parallelism in distributed computing, presentation of visualizations
of runtime data flow, and proofs for formal verification of correctness. The
ability to reuse a single codebase for all these purposes provides new ways to
understand and verify parallel programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6455</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6455</id><created>2014-10-23</created><authors><author><keyname>Kong</keyname><forenames>Yong</forenames></author></authors><title>Btrim: A fast, lightweight adapter and quality trimming program for
  next-generation sequencing technologies</title><categories>q-bio.GN cs.CE cs.DS</categories><comments>8 pages, 1 figure</comments><journal-ref>Genomics, 98, 152-153 (2001)</journal-ref><doi>10.1016/j.ygeno.2011.05.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Btrim is a fast and lightweight software to trim adapters and low quality
regions in reads from ultra high-throughput next-generation sequencing
machines. It also can reliably identify barcodes and assign the reads to the
original samples. Based on a modified Myers's bit-vector dynamic programming
algorithm, Btrim can handle indels in adapters and barcodes. It removes low
quality regions and trims off adapters at both or either end of the reads. A
typical trimming of 30M reads with two sets of adapter pairs can be done in
about a minute with a small memory footprint. Btrim is a versatile stand-alone
tool that can be used as the first step in virtually all next-generation
sequence analysis pipelines. The program is available at
\url{http://graphics.med.yale.edu/trim/}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6457</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6457</id><created>2014-10-23</created><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Moreira</keyname><forenames>Joel</forenames></author></authors><title>A conditional construction of restricted isometries</title><categories>math.FA cs.IT math.IT math.NT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the restricted isometry property of a matrix that is built from the
discrete Fourier transform matrix by collecting rows indexed by quadratic
residues. We find an $\epsilon&gt;0$ such that, conditioned on a folklore
conjecture in number theory, this matrix satisfies the restricted isometry
property with sparsity parameter $K=\Omega(M^{1/2+\epsilon})$, where $M$ is the
number of rows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6466</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6466</id><created>2014-10-23</created><updated>2015-02-16</updated><authors><author><keyname>Cheng</keyname><forenames>Dehua</forenames></author><author><keyname>He</keyname><forenames>Xinran</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author></authors><title>Model Selection for Topic Models via Spectral Decomposition</title><categories>stat.ML cs.IR cs.LG stat.CO</categories><comments>accepted in AISTATS 2015</comments><msc-class>62H30</msc-class><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic models have achieved significant successes in analyzing large-scale
text corpus. In practical applications, we are always confronted with the
challenge of model selection, i.e., how to appropriately set the number of
topics. Following recent advances in topic model inference via tensor
decomposition, we make a first attempt to provide theoretical analysis on model
selection in latent Dirichlet allocation. Under mild conditions, we derive the
upper bound and lower bound on the number of topics given a text collection of
finite size. Experimental results demonstrate that our bounds are accurate and
tight. Furthermore, using Gaussian mixture model as an example, we show that
our methodology can be easily generalized to model selection analysis for other
latent models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6472</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6472</id><created>2014-10-23</created><authors><author><keyname>Mousse</keyname><forenames>Mika&#xeb;l A.</forenames></author><author><keyname>Ezin</keyname><forenames>Eug&#xe8;ne C.</forenames></author><author><keyname>Motamed</keyname><forenames>Cina</forenames></author></authors><title>Foreground-Background Segmentation Based on Codebook and Edge Detector</title><categories>cs.CV</categories><comments>to appear in the 10th International Conference on Signal Image
  Technology &amp; Internet Based Systems, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background modeling techniques are used for moving object detection in video.
Many algorithms exist in the field of object detection with different purposes.
In this paper, we propose an improvement of moving object detection based on
codebook segmentation. We associate the original codebook algorithm with an
edge detection algorithm. Our goal is to prove the efficiency of using an edge
detection algorithm with a background modeling algorithm. Throughout our study,
we compared the quality of the moving object detection when codebook
segmentation algorithm is associated with some standard edge detectors. In each
case, we use frame-based metrics for the evaluation of the detection. The
different results are presented and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6500</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6500</id><created>2014-10-22</created><authors><author><keyname>Anouari</keyname><forenames>Tarik</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author></authors><title>QoE-Based Scheduling Algorithm in WiMAX Network using Manhattan Grid
  Mobility Model</title><categories>cs.NI</categories><comments>6 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:1410.6154, arXiv:1410.5944</comments><journal-ref>The World of Computer Science and Information Technology Journal
  (WSCIT). 2014, Volume 4, Issue 10. pp. 133.138</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiMAX (acronym for Worldwide Interoperability for Microwave Access) is a
family of technical standards based on IEEE 802.16 standard that defines the
high speed connection through radio waves unlike DSL (Digital Subscriber Line)
or other wired technology. It can provide coverage to remote rural areas of
several kilometers in radius, it's an adequate response to some rural or
inaccessible areas. WiMAX can provide point-to-point (P2P) and
point-to-multipoint (PMP) modes. In parallel, it was observed that, unlike the
traditional assessment methods for quality, nowadays, current research focuses
on the user perceived quality, the existing scheduling approaches take into
account the quality of service (QoS) and many technical parameters, but does
not take into account the quality of experience (QoE). In this paper, we
present a scheduling algorithm to provide QoE in WiMAX network under Manhattan
Mobility. A new approach is proposed, particularly for the Best Effort (BE)
service class WiMAX, in this approach, if a packet loss occurs on a link
connection, the system then reduces the transmission rate of this connection to
obtain its minimum allowable transmission rate. The NS-2 simulation results
show that the QoE provided to users is enhanced in terms of throughput, jitter,
packet loss rate and delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6502</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6502</id><created>2014-10-05</created><authors><author><keyname>Bhambri</keyname><forenames>Satish</forenames></author></authors><title>Quantum Clouds: A future perspective</title><categories>cs.DC</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computing and cloud computing are two giants for futuristic
computing. Both technologies complement each other. Quantum clouds, therefore,
is deploying the resources of quantum computation in a cloud environment to
provide solution to the challenges and problems faced by present model of
classical cloud computation. State of the art challenges faced by the cloud
such as VM migration, data security, traffic management can be addressed by the
quantum principles. But the merging of these two technologies have challenges
of their own which need to be addressed before moving forward. What are those
challenges and how does a quantum computer solve the cloud problems? The
relation among quantum parallelism, superposition and flash crowd effect;
Laundauer's principle and energy management; photon polarization principle and
data security; these fascinating queries are addressed in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6505</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6505</id><created>2014-10-23</created><authors><author><keyname>Haykazyan</keyname><forenames>Levon</forenames></author></authors><title>Decidability of the Clark's Completion Semantics for Monadic Programs
  and Queries</title><categories>cs.LO</categories><journal-ref>Theory and Practice of Logic Programming, 15 (3): 402-412, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many different semantics for general logic programs (i.e. programs
that use negation in the bodies of clauses). Most of these semantics are Turing
complete (in a sense that can be made precise), implying that they are
undecidable. To obtain decidability one needs to put additional restrictions on
programs and queries. In logic programming it is natural to put restrictions on
the underlying first-order language. In this note we show the decidability of
the Clark's completion semantics for monadic general programs and queries.
  To appear in Theory and Practice of Logic Programming (TPLP)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6513</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6513</id><created>2014-10-23</created><updated>2015-03-12</updated><authors><author><keyname>Gu</keyname><forenames>Yunan</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Matching Theory for Future Wireless Networks: Fundamentals and
  Applications</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of novel wireless networking paradigms such as small cell and
cognitive radio networks has forever transformed the way in which wireless
systems are operated. In particular, the need for self-organizing solutions to
manage the scarce spectral resources has become a prevalent theme in many
emerging wireless systems. In this paper, the first comprehensive tutorial on
the use of matching theory, a Nobelprize winning framework, for resource
management in wireless networks is developed. To cater for the unique features
of emerging wireless networks, a novel, wireless-oriented classification of
matching theory is proposed. Then, the key solution concepts and algorithmic
implementations of this framework are exposed. Then, the developed concepts are
applied in three important wireless networking areas in order to demonstrate
the usefulness of this analytical tool. Results show how matching theory can
effectively improve the performance of resource allocation in all three
applications discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6516</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6516</id><created>2014-10-23</created><authors><author><keyname>Rahwan</keyname><forenames>Talal</forenames></author><author><keyname>Michalak</keyname><forenames>Tomasz P.</forenames></author></authors><title>Coalition Structure Generation on Graphs</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two fundamental algorithm-design paradigms are Tree Search and Dynamic
Programming. The techniques used therein have been shown to complement one
another when solving the complete set partitioning problem, also known as the
coalition structure generation problem [5]. Inspired by this observation, we
develop in this paper an algorithm to solve the coalition structure generation
problem on graphs, where the goal is to identifying an optimal partition of a
graph into connected subgraphs. More specifically, we develop a new depth-first
search algorithm, and combine it with an existing dynamic programming algorithm
due to Vinyals et al. [9]. The resulting hybrid algorithm is empirically shown
to significantly outperform both its constituent parts when the
subset-evaluation function happens to have certain intuitive properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6519</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6519</id><created>2014-10-23</created><authors><author><keyname>Tolpin</keyname><forenames>David</forenames></author></authors><title>Justifying and Improving Meta-Agent Conflict-Based Search</title><categories>cs.AI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Meta-Agent Conflict-Based Search~(MA-CBS) is a recently proposed
algorithm for the multi-agent path finding problem. The algorithm is an
extension of Conflict-Based Search~(CBS), which automatically merges
conflicting agents into meta-agents if the number of conflicts exceeds a
certain threshold. However, the decision to merge agents is made according to
an empirically chosen fixed threshold on the number of conflicts. The best
threshold depends both on the domain and on the number of agents, and the
nature of the dependence is not clearly understood.
  We suggest a justification for the use of a fixed threshold on the number of
conflicts based on the analysis of a model problem. Following the suggested
justification, we introduce new decision policies for the MA-CBS algorithm,
which considerably improve the algorithm's performance. The improved variants
of the algorithm are evaluated on several sets of problems, chosen to underline
different aspects of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6520</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6520</id><created>2014-10-23</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Ku</keyname><forenames>Jason S.</forenames></author></authors><title>Filling a Hole in a Crease Pattern: Isometric Mapping from Prescribed
  Boundary Folding</title><categories>cs.CG</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a sheet of paper and a prescribed folding of its boundary, is there a
way to fold the paper's interior without stretching so that the boundary lines
up with the prescribed boundary folding? For polygonal boundaries
nonexpansively folded at finitely many points, we prove that a consistent
isometric mapping of the polygon interior always exists and is computable in
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6532</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6532</id><created>2014-10-23</created><authors><author><keyname>Zhang</keyname><forenames>Ziming</forenames></author><author><keyname>Chen</keyname><forenames>Yuting</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>A Novel Visual Word Co-occurrence Model for Person Re-identification</title><categories>cs.CV</categories><comments>Accepted at ECCV Workshop on Visual Surveillance and
  Re-Identification, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person re-identification aims to maintain the identity of an individual in
diverse locations through different non-overlapping camera views. The problem
is fundamentally challenging due to appearance variations resulting from
differing poses, illumination and configurations of camera views. To deal with
these difficulties, we propose a novel visual word co-occurrence model. We
first map each pixel of an image to a visual word using a codebook, which is
learned in an unsupervised manner. The appearance transformation between camera
views is encoded by a co-occurrence matrix of visual word joint distributions
in probe and gallery images. Our appearance model naturally accounts for
spatial similarities and variations caused by pose, illumination &amp;
configuration change across camera views. Linear SVMs are then trained as
classifiers using these co-occurrence descriptors. On the VIPeR and CUHK Campus
benchmark datasets, our method achieves 83.86% and 85.49% at rank-15 on the
Cumulative Match Characteristic (CMC) curves, and beats the state-of-the-art
results by 10.44% and 22.27%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6533</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6533</id><created>2014-10-23</created><authors><author><keyname>Patterson</keyname><forenames>Stacy</forenames></author></authors><title>In-Network Leader Selection for Acyclic Graphs</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of leader selection in leader-follower multi-agent
systems that are subject to stochastic disturbances. This problem arises in
applications such as vehicle formation control, distributed clock
synchronization, and distributed localization in sensor networks. We pose a new
leader selection problem called the in-network leader selection problem.
Initially, an arbitrary node is selected to be a leader, and in all consequent
steps the network must have exactly one leader. The agents must collaborate to
find the leader that minimizes the variance of the deviation from the desired
trajectory, and they must do so within the network using only communication
between neighbors. To develop a solution for this problem, we first show a
connection between the leader selection problem and a class of discrete
facility location problems. We then leverage a previously proposed
self-stabilizing facility location algorithm to develop a self-stabilizing
in-network leader selection algorithm for acyclic graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6558</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6558</id><created>2014-10-23</created><updated>2015-03-23</updated><authors><author><keyname>Giryes</keyname><forenames>Raja</forenames></author></authors><title>Sampling in the Analysis Transform Domain</title><categories>cs.IT math.IT math.NA stat.ME</categories><comments>13 Pages, 2 figures</comments><msc-class>94A20, 94A12, 62H12</msc-class><acm-class>I.4.4; I.4.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many signal and image processing applications have benefited remarkably from
the fact that the underlying signals reside in a low dimensional subspace. One
of the main models for such a low dimensionality is the sparsity one. Within
this framework there are two main options for the sparse modeling: the
synthesis and the analysis ones, where the first is considered the standard
paradigm for which much more research has been dedicated. In it the signals are
assumed to have a sparse representation under a given dictionary. On the other
hand, in the analysis approach the sparsity is measured in the coefficients of
the signal after applying a certain transformation, the analysis dictionary, on
it. Though several algorithms with some theory have been developed for this
framework, they are outnumbered by the ones proposed for the synthesis
methodology.
  Given that the analysis dictionary is either a frame or the two dimensional
finite difference operator, we propose a new sampling scheme for signals from
the analysis model that allows to recover them from their samples using any
existing algorithm from the synthesis model. The advantage of this new sampling
strategy is that it makes the existing synthesis methods with their theory also
available for signals from the analysis framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6569</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6569</id><created>2014-10-24</created><updated>2015-10-07</updated><authors><author><keyname>Natarajan</keyname><forenames>Lakshmi</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Lattice Index Coding</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE Transactions on Information
  Theory. Two-column format, 21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The index coding problem involves a sender with K messages to be transmitted
across a broadcast channel, and a set of receivers each of which demands a
subset of the K messages while having prior knowledge of a different subset as
side information. We consider the specific case of noisy index coding where the
broadcast channel is Gaussian and every receiver demands all the messages from
the source. Instances of this communication problem arise in wireless relay
networks, sensor networks, and retransmissions in broadcast channels. We
construct 'lattice index codes' for this channel by encoding the K messages
individually using K modulo lattice constellations and transmitting their sum
modulo a coarse lattice. We introduce a design metric called 'side information
gain' that measures the advantage of a code in utilizing the side information
at the receivers, and hence its goodness as an index code. Based on the Chinese
remainder theorem, we then construct lattice index codes with large side
information gains using lattices over the following principal ideal domains:
rational integers, Gaussian integers, Eisenstein integers, and the Hurwitz
quaternions. Among all lattice index codes constructed using any densest
lattice of a given dimension, our codes achieve the maximum side information
gain. Finally, using an example, we illustrate how the proposed lattice index
codes can benefit Gaussian broadcast channels with more general message
demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6572</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6572</id><created>2014-10-24</created><updated>2015-12-09</updated><authors><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Rombach</keyname><forenames>Puck</forenames></author><author><keyname>Lee</keyname><forenames>Sang Hoon</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Detection of Core-Periphery Structure in Networks Using Spectral Methods
  and Geodesic Paths</title><categories>cs.DM cond-mat.dis-nn cs.SI math.CO physics.soc-ph</categories><comments>30 pages, 16 figures, 4 tables, 7 algorithm floats</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce several novel and computationally efficient methods for
detecting &quot;core--periphery structure&quot; in networks. Core--periphery structure is
a type of mesoscale structure that includes densely-connected core vertices and
sparsely-connected peripheral vertices. Core vertices tend to be well-connected
both among themselves and to peripheral vertices, which tend not to be
well-connected to other vertices. Our first method, which is based on
transportation in networks, aggregates information from many geodesic paths in
a network and yields a score for each vertex that reflects the likelihood that
vertex is a core vertex. Our second method is based on a low-rank approximation
of a network's adjacency matrix, which can often be expressed as a
tensor-product matrix. Our third approach uses the bottom eigenvector of the
random-walk Laplacian to infer a coreness score and a classification into core
and peripheral vertices. Additionally, we design an objective function to (1)
help classify vertices into core or peripheral vertices and (2) provide a
goodness-of-fit criterion for classifications into core versus peripheral
vertices. To examine the performance of our methods, we apply our algorithms to
both synthetically-generated networks and a variety of real-world data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6582</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6582</id><created>2014-10-24</created><authors><author><keyname>Zhang</keyname><forenames>Lan</forenames></author><author><keyname>Liu</keyname><forenames>Kebin</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Feng</keyname><forenames>Puchun</forenames></author><author><keyname>Liu</keyname><forenames>Cihang</forenames></author><author><keyname>Liu</keyname><forenames>Yunhao</forenames></author></authors><title>Enable Portrait Privacy Protection in Photo Capturing and Sharing</title><categories>cs.CR</categories><comments>9 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wide adoption of wearable smart devices with onboard cameras greatly
increases people's concern on privacy infringement. Here we explore the
possibility of easing persons from photos captured by smart devices according
to their privacy protection requirements. To make this work, we need to address
two challenges: 1) how to let users explicitly express their privacy protection
intention, and 2) how to associate the privacy requirements with persons in
captured photos accurately and efficiently. Furthermore, the association
process itself should not cause portrait information leakage and should be
accomplished in a privacy-preserving way. In this work, we design, develop, and
evaluate a protocol, that enables a user to flexibly express her privacy
requirement and empowers the photo service provider (or image taker) to exert
the privacy protection policy.Leveraging the visual distinguishability of
people in the field-of-view and the dimension-order-independent property of
vector similarity measurement, we achieves high accuracy and low overhead.
  We implement a prototype system, and our evaluation results on both the
trace-driven and real-life experiments confirm the feasibility and efficiency
of our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6589</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6589</id><created>2014-10-24</created><authors><author><keyname>Zhang</keyname><forenames>Lan</forenames></author><author><keyname>Jung</keyname><forenames>Taeho</forenames></author><author><keyname>Liu</keyname><forenames>Cihang</forenames></author><author><keyname>Ding</keyname><forenames>Xuan</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Liu</keyname><forenames>Yunhao</forenames></author></authors><title>Outsource Photo Sharing and Searching for Mobile Devices With Privacy
  Protection</title><categories>cs.CR</categories><comments>10 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the proliferation of mobile devices, cloud-based photo sharing and
searching services are becoming common due to the mobile devices' resource
constrains. Meanwhile, there is also increasing concern about privacy in
photos. In this work, we present a framework \ourprotocolNSP, which enables
cloud servers to provide privacy-preserving photo sharing and search as a
service to mobile device users. Privacy-seeking users can share their photos
via our framework to allow only their authorized friends to browse and search
their photos using resource-bounded mobile devices. This is achieved by our
carefully designed architecture and novel outsourced privacy-preserving
computation protocols, through which no information about the outsourced photos
or even the search contents (including the results) would be revealed to the
cloud servers. Our framework is compatible with most of the existing image
search technologies, and it requires few changes to the existing cloud systems.
The evaluation of our prototype system with 31,772 real-life images shows the
communication and computation efficiency of our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6592</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6592</id><created>2014-10-24</created><authors><author><keyname>Gupta</keyname><forenames>Ankur</forenames></author><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author></authors><title>Hiding Sound in Image by K-LSB Mutation</title><categories>cs.MM cs.CR</categories><comments>appears in ISCBI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a novel approach to hide sound files in a digital image is
proposed and implemented such that it becomes difficult to conclude about the
existence of the hidden data inside the image. In this approach, we utilize the
rightmost k-LSB of pixels in an image to embed MP3 sound bits into a pixel. The
pixels are so chosen that the distortion in image would be minimized due to
embedding. This requires comparing all the possible permutations of pixel
values, which may would lead to exponential time computation. To speed up this,
Cuckoo Search (CS) could be used to find the most optimal solution. The
advantage of using proposed CS is that it is easy to implement and is very
effective at converging in relatively less iterations/generations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6593</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6593</id><created>2014-10-24</created><authors><author><keyname>Zhang</keyname><forenames>Lan</forenames></author><author><keyname>Jung</keyname><forenames>Taeho</forenames></author><author><keyname>Feng</keyname><forenames>Puchun</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Liu</keyname><forenames>Yunhao</forenames></author></authors><title>Cloud-based Privacy Preserving Image Storage, Sharing and Search</title><categories>cs.CR</categories><comments>15 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-resolution cameras produce huge volume of high quality images everyday.
It is extremely challenging to store, share and especially search those huge
images, for which increasing number of cloud services are presented to support
such functionalities. However, images tend to contain rich sensitive
information (\eg, people, location and event), and people's privacy concerns
hinder their readily participation into the services provided by untrusted
third parties. In this work, we introduce PIC: a Privacy-preserving large-scale
Image search system on Cloud. Our system enables efficient yet secure
content-based image search with fine-grained access control, and it also
provides privacy-preserving image storage and sharing among users. Users can
specify who can/cannot search on their images when using the system, and they
can search on others' images if they satisfy the condition specified by the
image owners. Majority of the computationally intensive jobs are outsourced to
the cloud side, and users only need to submit the query and receive the result
throughout the entire image search. Specially, to deal with massive images, we
design our system suitable for distributed and parallel computation and
introduce several optimizations to further expedite the search process. We
implement a prototype of PIC including both cloud side and client side. The
cloud side is a cluster of computers with distributed file system (Hadoop HDFS)
and MapReduce architecture (Hadoop MapReduce). The client side is built for
both Windows OS laptops and Android phones. We evaluate the prototype system
with large sets of real-life photos. Our security analysis and evaluation
results show that PIC successfully protect the image privacy at a low cost of
computation and communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6604</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6604</id><created>2014-10-24</created><authors><author><keyname>Wang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Peng</keyname><forenames>Peichao</forenames></author><author><keyname>Dunson</keyname><forenames>David</forenames></author></authors><title>Median Selection Subset Aggregation for Parallel Inference</title><categories>stat.ML cs.DC stat.CO stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For massive data sets, efficient computation commonly relies on distributed
algorithms that store and process subsets of the data on different machines,
minimizing communication costs. Our focus is on regression and classification
problems involving many features. A variety of distributed algorithms have been
proposed in this context, but challenges arise in defining an algorithm with
low communication, theoretical guarantees and excellent practical performance
in general settings. We propose a MEdian Selection Subset AGgregation Estimator
(message) algorithm, which attempts to solve these problems. The algorithm
applies feature selection in parallel for each subset using Lasso or another
method, calculates the `median' feature inclusion index, estimates coefficients
for the selected features in parallel for each subset, and then averages these
estimates. The algorithm is simple, involves very minimal communication, scales
efficiently in both sample and feature size, and has theoretical guarantees. In
particular, we show model selection consistency and coefficient estimation
efficiency. Extensive experiments show excellent performance in variable
selection, estimation, prediction, and computation time relative to usual
competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6609</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6609</id><created>2014-10-24</created><authors><author><keyname>Bartuschat</keyname><forenames>Dominik</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author></authors><title>Parallel Multiphysics Simulations of Charged Particles in Microfluidic
  Flows</title><categories>cs.CE physics.comp-ph</categories><comments>Submitted to Journal of Computational Science (Elsevier)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article describes parallel multiphysics simulations of charged particles
in microfluidic flows with the waLBerla framework. To this end, three physical
effects are coupled: rigid body dynamics, fluid flow modelled by a lattice
Boltzmann algorithm, and electric potentials represented by a finite volume
discretisation. For solving the finite volume discretisation for the
electrostatic forces, a cell-centered multigrid algorithm is developed that
conforms to the lattice Boltzmann meshes and the parallel communication
structure of waLBerla. The new functionality is validated with suitable
benchmark scenarios. Additionally, the parallel scaling and the numerical
efficiency of the algorithms are analysed on an advanced supercomputer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6621</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6621</id><created>2014-10-24</created><authors><author><keyname>Kavin</keyname><forenames>M.</forenames></author><author><keyname>Keerthana</keyname><forenames>K.</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author><author><keyname>S</keyname><forenames>Sangeetha.</forenames></author><author><keyname>Vinothini</keyname><forenames>R.</forenames></author></authors><title>Some Combinatorial Problems on Halin Graphs</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $T$ be a tree with no degree 2 vertices and $L(T)=\{l_1,\ldots,l_r\}, r
\geq 2$ denote the set of leaves in $T$. An Halin graph $G$ is a graph obtained
from $T$ such that $V(G)=V(T)$ and $E(G)=E(T) \cup \{\{l_i,l_{i+1}\} ~|~ 1 \leq
i \leq r-1\} \cup \{l_1,l_r\}$. In this paper, we investigate combinatorial
problems such as, testing whether a given graph is Halin or not, chromatic
bounds, an algorithm to color Halin graphs with the minimum number of colors.
Further, we present polynomial-time algorithms for testing and coloring
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6625</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6625</id><created>2014-10-24</created><updated>2015-04-20</updated><authors><author><keyname>Vilone</keyname><forenames>Daniele</forenames></author><author><keyname>Giardini</keyname><forenames>Francesca</forenames></author><author><keyname>Paolucci</keyname><forenames>Mario</forenames></author></authors><title>Partner selection supports reputation-based cooperation in a Public
  Goods Game</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 10 figures. In press for Springer Ed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dyadic models of indirect reciprocity, the receivers' history of giving
has a significant impact on the donor's decision. When the interaction involves
more than two agents things become more complicated, and in large groups
cooperation can hardly emerge. In this work we use a Public Goods Game to
investigate whether publicly available reputation scores may support the
evolution of cooperation and whether this is affected by the kind of network
structure adopted. Moreover, if agents interact on a bipartite graph with
partner selection cooperation can thrive in large groups and in a small amount
of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6627</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6627</id><created>2014-10-24</created><authors><author><keyname>Madue&#xf1;o</keyname><forenames>Germ&#xe1;n Corrales</forenames></author><author><keyname>Stefanovi&#x107;</keyname><forenames>&#x10c;edomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Reengineering GSM/GPRS Towards a Dedicated Network for Massive Smart
  Metering</title><categories>cs.IT cs.NI math.IT</categories><comments>SmartGridComm 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GSM is a synonym for a major success in wireless technology, achieving
widespread use and high technology ma- turity. However, its future is
questionable, as many stakeholders indicate that the GSM spectrum should be
refarmed for LTE. On the other hand, the advent of smart grid and the ubiquity
of smart meters will require reliable, long-lived wide area connections. This
motivates to investigate the potential of GSM to be evolved into a dedicated
network for smart metering. We introduce simple mechanisms to reengineer the
access control in GSM. The result is a system that offers excellent support for
smart metering, as well as the other massive machine-to-machine traffic
patterns that are envisioned in 3GPP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6628</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6628</id><created>2014-10-24</created><authors><author><keyname>Madue&#xf1;o</keyname><forenames>Germ&#xe1;n Corrales</forenames></author><author><keyname>Stefanovi&#x107;</keyname><forenames>&#x10c;edomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Efficient LTE Access with Collision Resolution for Massive M2M
  Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>Globecom 2014 Workshop - Ultra-Low Latency and Ultra-High Reliability
  in Wireless Communications (ULTRA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LTE random access procedure performs satisfactorily in case of asynchronous,
uncorrelated traffic arrivals. However, when the arrivals are correlated and
arrive synchronously, the performance of the random access channel (RACH) is
drastically reduced, causing a large number of devices to experience outage. In
this work we propose a LTE RACH scheme tailored for delay-sensitive M2M
services with synchronous traffic arrivals. The key idea is, upon detection of
a RACH overload, to apply a collision resolution algorithm based on splitting
trees. The solution is implemented on top of the existing LTE RACH mechanism,
requiring only minor modifications of the protocol operation and not incurring
any changes to the physical layer. The results are very promising,
outperforming the related solutions by a wide margin. As an illustration, the
proposed scheme can resolve 30k devices with an average of 5 preamble
transmissions and delay of 1.2 seconds, under a realistic probability of
transmissions error both in the downlink and in the uplink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6629</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6629</id><created>2014-10-24</created><authors><author><keyname>Stringhini</keyname><forenames>Gianluca</forenames></author><author><keyname>Thonnard</keyname><forenames>Olivier</forenames></author></authors><title>That Ain't You: Detecting Spearphishing Emails Before They Are Sent</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the ways in which attackers try to steal sensitive information from
corporations is by sending spearphishing emails. This type of emails typically
appear to be sent by one of the victim's coworkers, but have instead been
crafted by an attacker. A particularly insidious type of spearphishing emails
are the ones that do not only claim to come from a trusted party, but were
actually sent from that party's legitimate email account that was compromised
in the first place. In this paper, we propose a radical change of focus in the
techniques used for detecting such malicious emails: instead of looking for
particular features that are indicative of attack emails, we look for possible
indicators of impersonation of the legitimate owners. We present
IdentityMailer, a system that validates the authorship of emails by learning
the typical email-sending behavior of users over time, and comparing any
subsequent email sent from their accounts against this model. Our experiments
on real world e-mail datasets demonstrate that our system can effectively block
advanced email attacks sent from genuine email accounts, which traditional
protection systems are unable to detect. Moreover, we show that it is resilient
to an attacker willing to evade the system. To the best of our knowledge,
IdentityMailer is the first system able to identify spearphishing emails that
are sent from within an organization, by a skilled attacker having access to a
compromised email account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6641</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6641</id><created>2014-10-24</created><updated>2015-08-18</updated><authors><author><keyname>Swoboda</keyname><forenames>Paul</forenames></author><author><keyname>Shekhovtsov</keyname><forenames>Alexander</forenames></author><author><keyname>Kappes</keyname><forenames>J&#xf6;rg Hendrik</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Savchynskyy</keyname><forenames>Bogdan</forenames></author></authors><title>Partial Optimality by Pruning for MAP-Inference with General Graphical
  Models</title><categories>cs.AI</categories><comments>16 pages, 4 tables and 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the energy minimization problem for undirected graphical models,
also known as MAP-inference problem for Markov random fields which is NP-hard
in general. We propose a novel polynomial time algorithm to obtain a part of
its optimal non-relaxed integral solution. Our algorithm is initialized with
variables taking integral values in the solution of a convex relaxation of the
MAP-inference problem and iteratively prunes those, which do not satisfy our
criterion for partial optimality. We show that our pruning strategy is in a
certain sense theoretically optimal. Also empirically our method outperforms
previous approaches in terms of the number of persistently labelled variables.
The method is very general, as it is applicable to models with arbitrary
factors of an arbitrary order and can employ any solver for the considered
relaxed problem. Our method's runtime is determined by the runtime of the
convex relaxation solver for the MAP-inference problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6648</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6648</id><created>2014-10-24</created><updated>2015-07-13</updated><authors><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Julian-Steffen</forenames></author><author><keyname>Schnoor</keyname><forenames>Henning</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>A Van Benthem Theorem for Modal Team Semantics</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The famous van Benthem theorem states that modal logic corresponds exactly to
the fragment of first-order logic that is invariant under bisimulation. In this
article we prove an exact analogue of this theorem in the framework of modal
dependence logic MDL and team semantics. We show that modal team logic MTL,
extending MDL by classical negation, captures exactly the FO-definable
bisimulation invariant properties of Kripke structures and teams. We also
compare the expressive power of MTL to most of the variants and extensions of
MDL recently studied in the area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6651</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6651</id><created>2014-10-24</created><authors><author><keyname>Martinez</keyname><forenames>Matias</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>ASTOR: Evolutionary Automatic Software Repair for Java</title><categories>cs.SE</categories><report-no>hal-01075976</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: During last years, many automatic software repair approaches have
been presented by the software engineering research community. According to the
corresponding papers, these approaches are able to repair real defects from
open source projects. Problematic: Some previous publications in the automatic
repair field do not provide the implementation of theirs approaches.
Consequently, it is not possible for the research community to re-execute the
original evaluation, to set up new evaluations (for example, to evaluate the
performance against new defects) or to compare approaches against each others.
Solution: We propose a publicly available automatic software repair tool called
Astor. It implements three state-of-the-art automatic software repair
approaches in the context of Java programs (including GenProg and a subset of
PAR's templates). The source code of Astor is licensed under the GNU General
Public Licence (GPL v2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6652</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6652</id><created>2014-10-24</created><authors><author><keyname>Shamkanov</keyname><forenames>Daniyar</forenames></author></authors><title>Nested Sequents for Provability Logic GLP</title><categories>math.LO cs.LO</categories><msc-class>03F05, 03F45</msc-class><journal-ref>Logic Journal of the IGPL, 23:5 (2015), 789-815</journal-ref><doi>10.1093/jigpal/jzv029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a proof system for the provability logic GLP in the formalism of
nested sequents and prove the cut elimination theorem for it. As an
application, we obtain the reduction of GLP to its important fragment called J
syntactically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6656</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6656</id><created>2014-10-24</created><authors><author><keyname>Boehm</keyname><forenames>Benedikt</forenames></author></authors><title>StegExpose - A Tool for Detecting LSB Steganography</title><categories>cs.MM cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganalysis tools play an important part in saving time and providing new
angles of attack for forensic analysts. StegExpose is a solution designed for
use in the real world, and is able to analyse images for LSB steganography in
bulk using proven attacks in a time efficient manner. When steganalytic methods
are combined intelligently, they are able generate even more accurate results.
This is the prime focus of StegExpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6663</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6663</id><created>2014-10-24</created><authors><author><keyname>Bulteau</keyname><forenames>Laurent</forenames></author><author><keyname>Sacomoto</keyname><forenames>Gustavo</forenames></author><author><keyname>Sinaimeri</keyname><forenames>Blerina</forenames></author></authors><title>Computing an Evolutionary Ordering is Hard</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that computing an evolutionary ordering of a family of sets, i.e. an
ordering where each set intersects with --but is not included in-- the union
earlier sets, is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6669</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6669</id><created>2014-10-24</created><authors><author><keyname>Turau</keyname><forenames>Volker</forenames></author></authors><title>Analyzing the Fault-Containment Time of Self-Stabilizing Algorithms - A
  Case Study for Graph Coloring</title><categories>cs.DC</categories><comments>23 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents techniques to derive upper bounds for the mean time to
recover from a single fault for self-stabilizing algorithms in the message
passing model. For a new Delta+1-coloring algorithm we analytically derive a
bound for the mean time to recover and show that the variance is bounded by a
small constant independent of the network's size. For the class of
bounded-independence graphs (e.g. unit disc graphs) all containment metrics are
in O(1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6671</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6671</id><created>2014-10-24</created><authors><author><keyname>Lai</keyname><forenames>Yong</forenames></author><author><keyname>Liu</keyname><forenames>Dayou</forenames></author><author><keyname>Yin</keyname><forenames>Minghao</forenames></author></authors><title>Augmenting Ordered Binary Decision Diagrams with Conjunctive
  Decomposition</title><categories>cs.AI</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper augments OBDD with conjunctive decomposition to propose a
generalization called OBDD[$\wedge$]. By imposing reducedness and the finest
$\wedge$-decomposition bounded by integer $i$
($\wedge_{\widehat{i}}$-decomposition) on OBDD[$\wedge$], we identify a family
of canonical languages called ROBDD[$\wedge_{\widehat{i}}$], where
ROBDD[$\wedge_{\widehat{0}}$] is equivalent to ROBDD. We show that the
succinctness of ROBDD[$\wedge_{\widehat{i}}$] is strictly increasing when $i$
increases. We introduce a new time-efficiency criterion called rapidity which
reflects that exponential operations may be preferable if the language can be
exponentially more succinct, and show that: the rapidity of each operation on
ROBDD[$\wedge_{\widehat{i}}$] is increasing when $i$ increases; particularly,
the rapidity of some operations (e.g., conjoining) is strictly increasing.
Finally, our empirical results show that: a) the size of
ROBDD[$\wedge_{\widehat{i}}$] is normally not larger than that of the
equivalent \ROBDDC{\widehat{i+1}}; b) conjoining two
ROBDD[$\wedge_{\widehat{1}}$]s is more efficient than conjoining two
ROBDD[$\wedge_{\widehat{0}}$]s in most cases, where the former is NP-hard but
the latter is in P; and c) the space-efficiency of
ROBDD[$\wedge_{\widehat{\infty}}$] is comparable with that of d-DNNF and that
of another canonical generalization of \ROBDD{} called SDD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6674</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6674</id><created>2014-10-24</created><authors><author><keyname>Koved</keyname><forenames>Larry</forenames></author><author><keyname>Singh</keyname><forenames>Kapil</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Just</keyname><forenames>Mike</forenames></author></authors><title>Proceedings of the Third Workshop on Mobile Security Technologies (MoST)
  2014</title><categories>cs.CR</categories><comments>10 papers in the proceedings (of 11 presented at the MoST 2014
  workshop)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Third Workshop on Mobile Security Technologies
(MoST) 2014, held in San Jose, CA, USA, on May 17, 2014. The workshop was held
as part of the IEEE Computer Society Security and Privacy Workshops, in
conjunction with the IEEE Symposium on Security and Privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6680</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6680</id><created>2014-10-24</created><authors><author><keyname>Elshaer</keyname><forenames>Hisham</forenames></author><author><keyname>Boccardi</keyname><forenames>Federico</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author><author><keyname>Irmer</keyname><forenames>Ralf</forenames></author></authors><title>Load &amp; Backhaul Aware Decoupled Downlink/Uplink Access in 5G Systems</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 6 figures. Submitted to the IEEE International Conference on
  Communications (ICC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until the 4th Generation (4G) cellular 3GPP systems, a user equipment's (UE)
cell association has been based on the downlink received power from the
strongest base station. Recent work has shown that - with an increasing degree
of heterogeneity in emerging 5G systems - such an approach is dramatically
suboptimal, advocating for an independent association of the downlink and
uplink where the downlink is served by the macro cell and the uplink by the
nearest small cell. In this paper, we advance prior art by explicitly
considering the cell-load as well as the available backhaul capacity during the
association process. We introduce a novel association algorithm and prove its
superiority w.r.t. prior art by means of simulations that are based on
Vodafone's small cell trial network and employing a high resolution pathloss
prediction and realistic user distributions. We also study the effect that
different power control settings have on the performance of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6685</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6685</id><created>2014-10-24</created><updated>2015-08-21</updated><authors><author><keyname>Kolesnichenko</keyname><forenames>Alexey</forenames></author><author><keyname>Poskitt</keyname><forenames>Christopher M.</forenames></author><author><keyname>Nanz</keyname><forenames>Sebastian</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Contract-Based General-Purpose GPU Programming</title><categories>cs.DC cs.SE</categories><acm-class>D.3.2; D.3.4</acm-class><journal-ref>Proc. International Conference on Generative Programming: Concepts
  and Experiences (GPCE 2015), pages 75-84. ACM, 2015</journal-ref><doi>10.1145/2814204.2814216</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using GPUs as general-purpose processors has revolutionized parallel
computing by offering, for a large and growing set of algorithms, massive
data-parallelization on desktop machines. An obstacle to widespread adoption,
however, is the difficulty of programming them and the low-level control of the
hardware required to achieve good performance. This paper suggests a
programming library, SafeGPU, that aims at striking a balance between
programmer productivity and performance, by making GPU data-parallel operations
accessible from within a classical object-oriented programming language. The
solution is integrated with the design-by-contract approach, which increases
confidence in functional program correctness by embedding executable program
specifications into the program text. We show that our library leads to modular
and maintainable code that is accessible to GPGPU non-experts, while providing
performance that is comparable with hand-written CUDA code. Furthermore,
runtime contract checking turns out to be feasible, as the contracts can be
executed on the GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6690</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6690</id><created>2014-10-24</created><authors><author><keyname>Berre</keyname><forenames>Daniel Le</forenames></author><author><keyname>Lonca</keyname><forenames>Emmanuel</forenames></author><author><keyname>Marquis</keyname><forenames>Pierre</forenames></author></authors><title>On the Complexity of Optimization Problems based on Compiled NNF
  Representations</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization is a key task in a number of applications. When the set of
feasible solutions under consideration is of combinatorial nature and described
in an implicit way as a set of constraints, optimization is typically NP-hard.
Fortunately, in many problems, the set of feasible solutions does not often
change and is independent from the user's request. In such cases, compiling the
set of constraints describing the set of feasible solutions during an off-line
phase makes sense, if this compilation step renders computationally easier the
generation of a non-dominated, yet feasible solution matching the user's
requirements and preferences (which are only known at the on-line step). In
this article, we focus on propositional constraints. The subsets L of the NNF
language analyzed in Darwiche and Marquis' knowledge compilation map are
considered. A number of families F of representations of objective functions
over propositional variables, including linear pseudo-Boolean functions and
more sophisticated ones, are considered. For each language L and each family F,
the complexity of generating an optimal solution when the constraints are
compiled into L and optimality is to be considered w.r.t. a function from F is
identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6717</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6717</id><created>2014-10-24</created><updated>2014-11-04</updated><authors><author><keyname>Peled</keyname><forenames>Olga</forenames></author><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Rokach</keyname><forenames>Lior</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>Matching Entities Across Online Social Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Social Networks (OSNs), such as Facebook and Twitter, have become an
integral part of our daily lives. There are hundreds of OSNs, each with its own
focus in that each offers particular services and functionalities. Recent
studies show that many OSN users create several accounts on multiple OSNs using
the same or different personal information. Collecting all the available data
of an individual from several OSNs and fusing it into a single profile can be
useful for many purposes. In this paper, we introduce novel machine learning
based methods for solving Entity Resolution (ER), a problem for matching user
profiles across multiple OSNs. The presented methods are able to match between
two user profiles from two different OSNs based on supervised learning
techniques, which use features extracted from each one of the user profiles. By
using the extracted features and supervised learning techniques, we developed
classifiers which can perform entity matching between two profiles for the
following scenarios: (a) matching entities across two OSNs; (b) searching for a
user by similar name; and (c) de-anonymizing a user's identity.
  The constructed classifiers were tested by using data collected from two
popular OSNs, Facebook and Xing. We then evaluated the classifiers'
performances using various evaluation measures, such as true and false positive
rates, accuracy, and the Area Under the receiver operator Curve (AUC). The
constructed classifiers were evaluated and their classification performance
measured by AUC was quite remarkable, with an AUC of up to 0.982 and an
accuracy of up to 95.9% in identifying user profiles across two OSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6725</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6725</id><created>2014-10-24</created><authors><author><keyname>Taylor</keyname><forenames>Mark</forenames></author></authors><title>Visualising Large Datasets in TOPCAT v4</title><categories>astro-ph.IM cs.GR</categories><comments>4 pages, 2 figures, conference paper submitted to arXiv a year after
  acceptance</comments><journal-ref>Astronomical Data Anaylsis Softward and Systems XXIII. Proceedings
  of a meeting held 29 September - 3 October 2013 at Waikoloa Beach Marriott,
  Hawaii, USA. Edited by N. Manset and P. Forshay ASP conference series, vol.
  485, 2014, p.257</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TOPCAT is a widely used desktop application for manipulation of astronomical
catalogues and other tables, which has long provided fast interactive
visualisation features including 1, 2 and 3-d plots, multiple datasets, linked
views, color coding, transparency and more. In Version 4 a new plotting library
has been written from scratch to deliver new and enhanced visualisation
capabilities. This paper describes some of the considerations in the design and
implementation, particularly in regard to providing comprehensible interactive
visualisation for multi-million point datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6726</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6726</id><created>2014-10-24</created><authors><author><keyname>Czyzowicz</keyname><forenames>J.</forenames></author><author><keyname>Kranakis</keyname><forenames>E.</forenames></author><author><keyname>Krizanc</keyname><forenames>D.</forenames></author><author><keyname>Narayanan</keyname><forenames>L.</forenames></author><author><keyname>Opatrny</keyname><forenames>J.</forenames></author></authors><title>Optimal online and offline algorithms for robot-assisted restoration of
  barrier coverage</title><categories>cs.DS cs.RO</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation between mobile robots and wireless sensor networks is a line of
research that is currently attracting a lot of attention. In this context, we
study the following problem of barrier coverage by stationary wireless sensors
that are assisted by a mobile robot with the capacity to move sensors. Assume
that $n$ sensors are initially arbitrarily distributed on a line segment
barrier. Each sensor is said to cover the portion of the barrier that
intersects with its sensing area. Owing to incorrect initial position, or the
death of some of the sensors, the barrier is not completely covered by the
sensors. We employ a mobile robot to move the sensors to final positions on the
barrier such that barrier coverage is guaranteed. We seek algorithms that
minimize the length of the robot's trajectory, since this allows the
restoration of barrier coverage as soon as possible. We give an optimal
linear-time offline algorithm that gives a minimum-length trajectory for a
robot that starts at one end of the barrier and achieves the restoration of
barrier coverage. We also study two different online models: one in which the
online robot does not know the length of the barrier in advance, and the other
in which the online robot knows the length of the barrier. For the case when
the online robot does not know the length of the barrier, we prove a tight
bound of $3/2$ on the competitive ratio, and we give a tight lower bound of
$5/4$ on the competitive ratio in the other case. Thus for each case we give an
optimal online algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6736</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6736</id><created>2014-10-24</created><authors><author><keyname>Huang</keyname><forenames>Sheng</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author><author><keyname>Yang</keyname><forenames>Dan</forenames></author></authors><title>On The Effect of Hyperedge Weights On Hypergraph Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypergraph is a powerful representation in several computer vision, machine
learning and pattern recognition problems. In the last decade, many researchers
have been keen to develop different hypergraph models. In contrast, no much
attention has been paid to the design of hyperedge weights. However, many
studies on pairwise graphs show that the choice of edge weight can
significantly influence the performances of such graph algorithms. We argue
that this also applies to hypegraphs. In this paper, we empirically discuss the
influence of hyperedge weight on hypegraph learning via proposing three novel
hyperedge weights from the perspectives of geometry, multivariate statistical
analysis and linear regression. Extensive experiments on ORL, COIL20, JAFFE,
Sheffield, Scene15 and Caltech256 databases verify our hypothesis. Similar to
graph learning, several representative hyperedge weighting schemes can be
concluded by our experimental studies. Moreover, the experiments also
demonstrate that the combinations of such weighting schemes and conventional
hypergraph models can get very promising classification and clustering
performances in comparison with some recent state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6739</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6739</id><created>2014-10-24</created><authors><author><keyname>Kroll</keyname><forenames>Martin</forenames></author><author><keyname>Steinmetzer</keyname><forenames>Simone</forenames></author></authors><title>Automated Cryptanalysis of Bloom Filter Encryptions of Health Records</title><categories>cs.CR</categories><comments>Contribution to the 8th International Conference on Health
  Informatics, Lisbon 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy-preserving record linkage with Bloom filters has become increasingly
popular in medical applications, since Bloom filters allow for probabilistic
linkage of sensitive personal data. However, since evidence indicates that
Bloom filters lack sufficiently high security where strong security guarantees
are required, several suggestions for their improvement have been made in
literature. One of those improvements proposes the storage of several
identifiers in one single Bloom filter. In this paper we present an automated
cryptanalysis of this Bloom filter variant. The three steps of this procedure
constitute our main contributions: (1) a new method for the detection of Bloom
filter encrytions of bigrams (so-called atoms), (2) the use of an optimization
algorithm for the assignment of atoms to bigrams, (3) the reconstruction of the
original attribute values by linkage against bigram sets obtained from lists of
frequent attribute values in the underlying population. To sum up, our attack
provides the first convincing attack on Bloom filter encryptions of records
built from more than one identifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6744</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6744</id><created>2014-10-24</created><updated>2016-01-27</updated><authors><author><keyname>Hogg</keyname><forenames>Tad</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Disentangling the Effects of Social Signals</title><categories>physics.soc-ph cs.CY cs.IR cs.SI</categories><comments>to appear in Human Computation Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer recommendation is a crowdsourcing task that leverages the opinions of
many to identify interesting content online, such as news, images, or videos.
Peer recommendation applications often use social signals, e.g., the number of
prior recommendations, to guide people to the more interesting content. How
people react to social signals, in combination with content quality and its
presentation order, determines the outcomes of peer recommendation, i.e., item
popularity. Using Amazon Mechanical Turk, we experimentally measure the effects
of social signals in peer recommendation. Specifically, after controlling for
variation due to item content and its position, we find that social signals
affect item popularity about half as much as position and content do. These
effects are somewhat correlated, so social signals exacerbate the &quot;rich get
richer&quot; phenomenon, which results in a wider variance of popularity. Further,
social signals change individual preferences, creating a &quot;herding&quot; effect that
biases people's judgments about the content. Despite this, we find that social
signals improve the efficiency of peer recommendation by reducing the effort
devoted to evaluating content while maintaining recommendation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6751</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6751</id><created>2014-10-24</created><updated>2014-11-11</updated><authors><author><keyname>Riedl</keyname><forenames>Christoph</forenames></author><author><keyname>Zanibbi</keyname><forenames>Richard</forenames></author><author><keyname>Hearst</keyname><forenames>Marti A.</forenames></author><author><keyname>Zhu</keyname><forenames>Siyu</forenames></author><author><keyname>Menietti</keyname><forenames>Michael</forenames></author><author><keyname>Crusan</keyname><forenames>Jason</forenames></author><author><keyname>Metelsky</keyname><forenames>Ivan</forenames></author><author><keyname>Lakhani</keyname><forenames>Karim R.</forenames></author></authors><title>Detecting Figures and Part Labels in Patents: Competition-Based
  Development of Image Processing Algorithms</title><categories>cs.CV cs.IR</categories><report-no>Harvard-NASA Tournament Lab Technical Report 01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report the findings of a month-long online competition in which
participants developed algorithms for augmenting the digital version of patent
documents published by the United States Patent and Trademark Office (USPTO).
The goal was to detect figures and part labels in U.S. patent drawing pages.
The challenge drew 232 teams of two, of which 70 teams (30%) submitted
solutions. Collectively, teams submitted 1,797 solutions that were compiled on
the competition servers. Participants reported spending an average of 63 hours
developing their solutions, resulting in a total of 5,591 hours of development
time. A manually labeled dataset of 306 patents was used for training, online
system tests, and evaluation. The design and performance of the top-5 systems
are presented, along with a system developed after the competition which
illustrates that winning teams produced near state-of-the-art results under
strict time and computation constraints. For the 1st place system, the harmonic
mean of recall and precision (f-measure) was 88.57% for figure region
detection, 78.81% for figure regions with correctly recognized figure titles,
and 70.98% for part label detection and character recognition. Data and
software from the competition are available through the online UCI Machine
Learning repository to inspire follow-on work by the image processing
community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6752</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6752</id><created>2014-10-24</created><authors><author><keyname>Fard</keyname><forenames>Pouyan R.</forenames></author><author><keyname>Grosse-Wentrup</keyname><forenames>Moritz</forenames></author></authors><title>The Influence of Decoding Accuracy on Perceived Control: A Simulated BCI
  Study</title><categories>q-bio.NC cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the relationship between the decoding accuracy of a
brain-computer interface (BCI) and a subject's subjective feeling of control is
important for determining a lower limit on decoding accuracy for a BCI that is
to be deployed outside a laboratory environment. We investigated this
relationship by systematically varying the level of control in a simulated BCI
task. We find that a binary decoding accuracy of 65% is required for users to
report more often than not that they are feeling in control of the system.
Decoding accuracies above 75%, on the other hand, added little in terms of the
level of perceived control. We further find that the probability of perceived
control does not only depend on the actual decoding accuracy, but is also in
influenced by whether subjects successfully complete the given task in the
allotted time frame.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6754</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6754</id><created>2014-10-24</created><updated>2015-02-25</updated><authors><author><keyname>Axtmann</keyname><forenames>Michael</forenames></author><author><keyname>Bingmann</keyname><forenames>Timo</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Practical Massively Parallel Sorting</title><categories>cs.DS cs.DC</categories><acm-class>D.1.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous parallel sorting algorithms do not scale to the largest available
machines, since they either have prohibitive communication volume or
prohibitive critical path length. We describe algorithms that are a viable
compromise and overcome this gap both in theory and practice. The algorithms
are multi-level generalizations of the known algorithms sample sort and
multiway mergesort. In particular our sample sort variant turns out to be very
scalable. Some tools we develop may be of independent interest -- a simple,
practical, and flexible sorting algorithm for small inputs working in
logarithmic time, a near linear time optimal algorithm for solving a
constrained bin packing problem, and an algorithm for data delivery, that
guarantees a small number of message startups on each processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6771</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6771</id><created>2014-10-12</created><updated>2014-10-26</updated><authors><author><keyname>Rivin</keyname><forenames>Igor</forenames></author></authors><title>Spectral Experiments+</title><categories>math.SP cond-mat.stat-mech cs.CG hep-th math-ph math.MP math.PR</categories><comments>24 pages</comments><msc-class>60B20, 60D05, 05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe extensive computational experiments on spectral properties of
random objects - random cubic graphs, random planar triangulations, and Voronoi
and Delaunay diagrams of random (uniformly distributed) point sets on the
sphere). We look at bulk eigenvalue distribution, eigenvalue spacings, and
locality properties of eigenvectors. We also look at the statistics of
\emph{nodal domains} of eigenvectors on these graphs. In all cases we discover
completely new (at least to this author) phenomena. The author has tried to
refrain from making specific conjectures, inviting the reader, instead, to
meditate on the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6776</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6776</id><created>2014-10-24</created><authors><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author><author><keyname>Narasimhan</keyname><forenames>Harikrishna</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author></authors><title>Online and Stochastic Gradient Methods for Non-decomposable Loss
  Functions</title><categories>cs.LG stat.ML</categories><comments>25 pages, 3 figures, To appear in the proceedings of the 28th Annual
  Conference on Neural Information Processing Systems, NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern applications in sensitive domains such as biometrics and medicine
frequently require the use of non-decomposable loss functions such as
precision@k, F-measure etc. Compared to point loss functions such as
hinge-loss, these offer much more fine grained control over prediction, but at
the same time present novel challenges in terms of algorithm design and
analysis. In this work we initiate a study of online learning techniques for
such non-decomposable loss functions with an aim to enable incremental learning
as well as design scalable solvers for batch problems. To this end, we propose
an online learning framework for such loss functions. Our model enjoys several
nice properties, chief amongst them being the existence of efficient online
learning algorithms with sublinear regret and online to batch conversion
bounds. Our model is a provable extension of existing online learning models
for point loss functions. We instantiate two popular losses, prec@k and pAUC,
in our model and prove sublinear regret bounds for both of them. Our proofs
require a novel structural lemma over ranked lists which may be of independent
interest. We then develop scalable stochastic gradient descent solvers for
non-decomposable loss functions. We show that for a large family of loss
functions satisfying a certain uniform convergence property (that includes
prec@k, pAUC, and F-measure), our methods provably converge to the empirical
risk minimizer. Such uniform convergence results were not known for these
losses and we establish these using novel proof techniques. We then use
extensive experimentation on real life and benchmark datasets to establish that
our method can be orders of magnitude faster than a recently proposed cutting
plane method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6793</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6793</id><created>2014-10-24</created><updated>2014-11-05</updated><authors><author><keyname>O'Brien</keyname><forenames>Michael P.</forenames></author><author><keyname>Sullivan</keyname><forenames>Blair D.</forenames></author></authors><title>Locally Estimating Core Numbers</title><categories>cs.SI</categories><comments>Main paper body is identical to previous version (ICDM version).
  Appendix with additional data sets and enlarged figures has been added to the
  end</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are a powerful way to model interactions and relationships in data
from a wide variety of application domains. In this setting, entities
represented by vertices at the &quot;center&quot; of the graph are often more important
than those associated with vertices on the &quot;fringes&quot;. For example, central
nodes tend to be more critical in the spread of information or disease and play
an important role in clustering/community formation. Identifying such &quot;core&quot;
vertices has recently received additional attention in the context of {\em
network experiments}, which analyze the response when a random subset of
vertices are exposed to a treatment (e.g. inoculation, free product samples,
etc). Specifically, the likelihood of having many central vertices in any
exposure subset can have a significant impact on the experiment.
  We focus on using $k$-cores and core numbers to measure the extent to which a
vertex is central in a graph. Existing algorithms for computing the core number
of a vertex require the entire graph as input, an unrealistic scenario in many
real world applications. Moreover, in the context of network experiments, the
subgraph induced by the treated vertices is only known in a probabilistic
sense. We introduce a new method for estimating the core number based only on
the properties of the graph within a region of radius $\delta$ around the
vertex, and prove an asymptotic error bound of our estimator on random graphs.
Further, we empirically validate the accuracy of our estimator for small values
of $\delta$ on a representative corpus of real data sets. Finally, we evaluate
the impact of improved local estimation on an open problem in network
experimentation posed by Ugander et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6795</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6795</id><created>2014-09-11</created><authors><author><keyname>Sindhwani</keyname><forenames>Nidhi</forenames></author><author><keyname>Singh</keyname><forenames>Manjit</forenames></author></authors><title>Transmit antenna subset selection in MIMO OFDM system using adaptive
  mutation Genetic algorithm</title><categories>cs.IT math.IT</categories><comments>13 pages,8 figures</comments><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics ( IJMNCT) Vol. 4, No.4, pp.17-29 Aug 2014</journal-ref><doi>10.5121/ijmnct.2014.4402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple input multiple output techniques are considered attractive for
future wireless communication systems, due to the continuing demand for high
data rates, spectral efficiency, suppress interference ability and robustness
of transmission. MIMO-OFDM is very helpful to transmit high data rate in
wireless transmission and provides good maximum system capacity by getting the
advantages of both MIMO and OFDM. The main problem in this system is that
increase in number of transmit and receive antennas lead to hardware
complexity. To tackle this issue, an effective optimal transmit antenna subset
selection method is proposed in paper with the aid of Adaptive Mutation Genetic
Algorithm (AGA). Here, the selection of transmit antenna subsets are done by
the adaptive mutation of Genetic Algorithm in MIMO-OFDM system. For all the
mutation points, the fitness function are evaluated and from that value, best
fitness based mutation points are chosen. After the selection of best mutation
points, the mutation process is carried out, accordingly. The implementation of
proposed work is done in the working platform MATLAB and the performance are
evaluated with various selection of transmit antenna subsets. Moreover, the
comparison results between the existing GA with mutation and the proposed GA
with adaptive mutation are discussed. Hence, using the proposed work, the
selection of transmit antenna with the maximum capacity is made and which leads
to the reduced hardware complexity and undisturbed data rate in the MIMO-OFDM
system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6796</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6796</id><created>2014-08-27</created><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Caviglione</keyname><forenames>Luca</forenames></author></authors><title>Steganography in Modern Smartphones and Mitigation Techniques</title><categories>cs.MM cs.CR</categories><comments>25 pages, 8 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By offering sophisticated services and centralizing a huge volume of personal
data, modern smartphones changed the way we socialize, entertain and work. To
this aim, they rely upon complex hardware/software frameworks leading to a
number of vulnerabilities, attacks and hazards to profile individuals or gather
sensitive information. However, the majority of works evaluating the security
degree of smartphones neglects steganography, which can be mainly used to: i)
exfiltrate confidential data via camouflage methods, and ii) conceal valuable
or personal information into innocent looking carriers.
  Therefore, this paper surveys the state of the art of steganographic
techniques for smartphones, with emphasis on methods developed over the period
2005 to the second quarter of 2014. The different approaches are grouped
according to the portion of the device used to hide information, leading to
three different covert channels, i.e., local, object and network. Also, it
reviews the relevant approaches used to detect and mitigate steganographic
attacks or threats. Lastly, it showcases the most popular software applications
to embed secret data into carriers, as well as possible future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6801</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6801</id><created>2014-10-24</created><updated>2015-04-02</updated><authors><author><keyname>Cohen</keyname><forenames>Michael B.</forenames></author><author><keyname>Elder</keyname><forenames>Sam</forenames></author><author><keyname>Musco</keyname><forenames>Cameron</forenames></author><author><keyname>Musco</keyname><forenames>Christopher</forenames></author><author><keyname>Persu</keyname><forenames>Madalina</forenames></author></authors><title>Dimensionality Reduction for k-Means Clustering and Low Rank
  Approximation</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to approximate a data matrix $\mathbf{A}$ with a much smaller
sketch $\mathbf{\tilde A}$ that can be used to solve a general class of
constrained k-rank approximation problems to within $(1+\epsilon)$ error.
Importantly, this class of problems includes $k$-means clustering and
unconstrained low rank approximation (i.e. principal component analysis). By
reducing data points to just $O(k)$ dimensions, our methods generically
accelerate any exact, approximate, or heuristic algorithm for these ubiquitous
problems.
  For $k$-means dimensionality reduction, we provide $(1+\epsilon)$ relative
error results for many common sketching techniques, including random row
projection, column selection, and approximate SVD. For approximate principal
component analysis, we give a simple alternative to known algorithms that has
applications in the streaming setting. Additionally, we extend recent work on
column-based matrix reconstruction, giving column subsets that not only `cover'
a good subspace for $\bv{A}$, but can be used directly to compute this
subspace.
  Finally, for $k$-means clustering, we show how to achieve a $(9+\epsilon)$
approximation by Johnson-Lindenstrauss projecting data points to just $O(\log
k/\epsilon^2)$ dimensions. This gives the first result that leverages the
specific structure of $k$-means to achieve dimension independent of input size
and sublinear in $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6806</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6806</id><created>2014-10-24</created><authors><author><keyname>De Loera</keyname><forenames>Jes&#xfa;s A.</forenames></author><author><keyname>Margulies</keyname><forenames>Susan</forenames></author><author><keyname>Pernpeintner</keyname><forenames>Michael</forenames></author><author><keyname>Riedl</keyname><forenames>Eric</forenames></author><author><keyname>Rolnick</keyname><forenames>David</forenames></author><author><keyname>Spencer</keyname><forenames>Gwen</forenames></author><author><keyname>Stasi</keyname><forenames>Despina</forenames></author><author><keyname>Swenson</keyname><forenames>Jon</forenames></author></authors><title>Gr\&quot;obner Bases and Nullstellens\&quot;atze for Graph-Coloring Ideals</title><categories>cs.SC cs.CC math.AC math.AG math.CO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit a well-known family of polynomial ideals encoding the problem of
graph-$k$-colorability. Our paper describes how the inherent combinatorial
structure of the ideals implies several interesting algebraic properties.
Specifically, we provide lower bounds on the difficulty of computing Gr\&quot;obner
bases and Nullstellensatz certificates for the coloring ideals of general
graphs. For chordal graphs, however, we explicitly describe a Gr\&quot;obner basis
for the coloring ideal, and provide a polynomial-time algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6824</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6824</id><created>2014-10-24</created><authors><author><keyname>Medhat</keyname><forenames>Ramy</forenames></author><author><keyname>Bonakdarpour</keyname><forenames>Borzoo</forenames></author><author><keyname>Fischmeister</keyname><forenames>Sebastian</forenames></author></authors><title>Power Redistribution for Optimizing Performance in MPI Clusters</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power efficiency has recently become a major concern in the high-performance
computing domain. HPC centers are provisioned by a power bound which impacts
execution time. Naturally, a tradeoff arises between power efficiency and
computational efficiency. This paper tackles the problem of performance
optimization for MPI applications, where a power bound is assumed. The paper
exposes a subset of HPC applications that leverage cluster parallelism using
MPI, where nodes encounter multiple synchronization points and exhibit
inter-node dependency. We abstract this structure into a dependency graph, and
leverage the asymmetry in execution time of parallel jobs on different nodes by
redistributing power gained from idling a blocked node to nodes that are
lagging in their jobs. We introduce a solution based on integer linear
programming (ILP) for optimal power distribution algorithm that minimizes total
execution time, while maintaining an upper power bound. We then present an
online heuristic that dynamically redistributes power at run time. The
heuristic shows significant reductions in total execution time of a set of
parallel benchmarks with speedup up to 2.25x.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6830</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6830</id><created>2014-10-24</created><authors><author><keyname>Fidaner</keyname><forenames>I&#x15f;&#x131;k Bar&#x131;&#x15f;</forenames></author><author><keyname>Cemgil</keyname><forenames>Ali Taylan</forenames></author></authors><title>Clustering Words by Projection Entropy</title><categories>cs.CL cs.LG</categories><comments>Accepted to NIPS 2014 Modern ML+NLP Workshop:
  http://www.cs.cmu.edu/~apparikh/nips2014ml-nlp/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We apply entropy agglomeration (EA), a recently introduced algorithm, to
cluster the words of a literary text. EA is a greedy agglomerative procedure
that minimizes projection entropy (PE), a function that can quantify the
segmentedness of an element set. To apply it, the text is reduced to a feature
allocation, a combinatorial object to represent the word occurences in the
text's paragraphs. The experiment results demonstrate that EA, despite its
reduction and simplicity, is useful in capturing significant relationships
among the words in the text. This procedure was implemented in Python and
published as a free software: REBUS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6831</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6831</id><created>2014-09-05</created><authors><author><keyname>Larrousse</keyname><forenames>Benjamin</forenames></author><author><keyname>Beaude</keyname><forenames>Olivier</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author></authors><title>Crawford-Sobel meet Lloyd-Max on the grid</title><categories>cs.OH cs.GT</categories><comments>ICASSP 2014, 5 pages</comments><journal-ref>IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Florence, Italy, May 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main contribution of this work is twofold. First, we apply, for the first
time, a framework borrowed from economics to a problem in the smart grid
namely, the design of signaling schemes between a consumer and an electricity
aggregator when these have non-aligned objectives. The consumer's objective is
to meet its need in terms of power and send a request (a message) to the
aggregator which does not correspond, in general, to its actual need. The
aggregator, which receives this request, not only wants to satisfy it but also
wants to manage the cost induced by the residential electricity distribution
network. Second, we establish connections between the exploited framework and
the quantization problem. Although the model assumed for the payoff functions
for the consumer and aggregator is quite simple, it allows one to extract
insights of practical interest from the analysis conducted. This allows us to
establish a direct connection with quantization, and more importantly, to open
a much more general challenge for source and channel coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6836</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6836</id><created>2014-10-24</created><updated>2015-06-23</updated><authors><author><keyname>Korkali</keyname><forenames>Mert</forenames></author><author><keyname>Veneman</keyname><forenames>Jason G.</forenames></author><author><keyname>Tivnan</keyname><forenames>Brian F.</forenames></author><author><keyname>Hines</keyname><forenames>Paul D. H.</forenames></author></authors><title>Reducing Cascading Failure Risk by Increasing Infrastructure Network
  Interdependency</title><categories>physics.soc-ph cs.SI physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increased coupling between critical infrastructure networks, such as power
and communication systems, will have important implications for the reliability
and security of these systems. To understand the effects of power-communication
coupling, several have studied interdependent network models and reported that
increased coupling can increase system vulnerability. However, these results
come from models that have substantially different mechanisms of cascading,
relative to those found in actual power and communication networks. This paper
reports on two sets of experiments that compare the network vulnerability
implications resulting from simple topological models and models that more
accurately capture the dynamics of cascading in power systems. First, we
compare a simple model of topological contagion to a model of cascading in
power systems and find that the power grid shows a much higher level of
vulnerability, relative to the contagion model. Second, we compare a model of
topological cascades in coupled networks to three different physics-based
models of power grids coupled to communication networks. Again, the more
accurate models suggest very different conclusions. In all but the most extreme
case, the physics-based power grid models indicate that increased
power-communication coupling decreases vulnerability. This is opposite from
what one would conclude from the coupled topological model, in which zero
coupling is optimal. Finally, an extreme case in which communication failures
immediately cause grid failures, suggests that if systems are poorly designed,
increased coupling can be harmful. Together these results suggest design
strategies for reducing the risk of cascades in interdependent infrastructure
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6853</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6853</id><created>2014-10-24</created><updated>2014-12-08</updated><authors><author><keyname>Giordano</keyname><forenames>Ryan</forenames></author><author><keyname>Broderick</keyname><forenames>Tamara</forenames></author></authors><title>Covariance Matrices for Mean Field Variational Bayes</title><categories>stat.ML cs.LG stat.ME</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean Field Variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However, it is well
known that a major failing of MFVB is its (sometimes severe) underestimates of
the uncertainty of model variables and lack of information about model variable
covariance. We develop a fast, general methodology for exponential families
that augments MFVB to deliver accurate uncertainty estimates for model
variables -- both for individual variables and coherently across variables.
MFVB for exponential families defines a fixed-point equation in the means of
the approximating posterior, and our approach yields a covariance estimate by
perturbing this fixed point. Inspired by linear response theory, we call our
method linear response variational Bayes (LRVB). We demonstrate the accuracy of
our method on simulated data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6854</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6854</id><created>2014-10-24</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author><author><keyname>Veloz</keyname><forenames>Tomas</forenames></author></authors><title>The Quantum Nature of Identity in Human Thought: Bose-Einstein
  Statistics for Conceptual Indistinguishability</title><categories>cs.AI quant-ph</categories><comments>12 pages</comments><journal-ref>International Journal of Theoretical Physics, 54, pp 4430-4443,
  2015</journal-ref><doi>10.1007/s10773-015-2620-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing experimental evidence shows that humans combine concepts in a way
that violates the rules of classical logic and probability theory. On the other
hand, mathematical models inspired by the formalism of quantum theory are in
accordance with data on concepts and their combinations. In this paper, we
investigate a novel type of concept combination were a number is combined with
a noun, e.g., `Eleven Animals. Our aim is to study 'conceptual identity' and
the effects of 'indistinguishability' - in the combination 'Eleven Animals',
the 'animals' are identical and indistinguishable - on the mechanisms of
conceptual combination. We perform experiments on human subjects and find
significant evidence of deviation from the predictions of classical statistical
theories, more specifically deviations with respect to Maxwell-Boltzmann
statistics. This deviation is of the 'same type' of the deviation of quantum
mechanical from classical mechanical statistics, due to indistinguishability of
microscopic quantum particles, i.e we find convincing evidence of the presence
of Bose-Einstein statistics. We also present preliminary promising evidence of
this phenomenon in a web-based study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6858</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6858</id><created>2014-10-24</created><updated>2014-10-30</updated><authors><author><keyname>Dainotti</keyname><forenames>Alberto</forenames></author><author><keyname>Benson</keyname><forenames>Karyn</forenames></author><author><keyname>King</keyname><forenames>Alistair</forenames></author><author><keyname>claffy</keyname><forenames>kc</forenames></author><author><keyname>Glatz</keyname><forenames>Eduard</forenames></author><author><keyname>Dimitropoulos</keyname><forenames>Xenofontas</forenames></author><author><keyname>Richter</keyname><forenames>Philipp</forenames></author><author><keyname>Finamore</keyname><forenames>Alessandro</forenames></author><author><keyname>Snoeren</keyname><forenames>Alex C.</forenames></author></authors><title>Lost in Space: Improving Inference of IPv4 Address Space Utilization</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One challenge in understanding the evolution of Internet infrastructure is
the lack of systematic mechanisms for monitoring the extent to which allocated
IP addresses are actually used. In this paper we try to advance the science of
inferring IPv4 address space utilization by analyzing and correlating results
obtained through different types of measurements. We have previously studied an
approach based on passive measurements that can reveal used portions of the
address space unseen by active approaches. In this paper, we study such passive
approaches in detail, extending our methodology to four different types of
vantage points, identifying traffic components that most significantly
contribute to discovering used IPv4 network blocks. We then combine the results
we obtained through passive measurements together with data from active
measurement studies, as well as measurements from BGP and additional datasets
available to researchers. Through the analysis of this large collection of
heterogeneous datasets, we substantially improve the state of the art in terms
of: (i) understanding the challenges and opportunities in using passive and
active techniques to study address utilization; and (ii) knowledge of the
utilization of the IPv4 space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6863</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6863</id><created>2014-10-24</created><authors><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Hof</keyname><forenames>Pim van 't</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Editing to Eulerian Graphs</title><categories>cs.DM</categories><comments>33 pages. An extended abstract of this paper will appear in the
  proceedings of FSTTCS 2014</comments><msc-class>05C85 (Primary) 05C45, 05C20 (Secondary)</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of modifying a graph into a connected graph in
which the degree of each vertex satisfies a prescribed parity constraint. Let
$ea$, $ed$ and $vd$ denote the operations edge addition, edge deletion and
vertex deletion respectively. For any $S\subseteq \{ea,ed,vd\}$, we define
Connected Degree Parity Editing$(S)$ (CDPE($S$)) to be the problem that takes
as input a graph $G$, an integer $k$ and a function $\delta\colon
V(G)\rightarrow\{0,1\}$, and asks whether $G$ can be modified into a connected
graph $H$ with $d_{H}(v)\equiv\delta(v)~(\bmod~2)$ for each $v\in V(H)$, using
at most $k$ operations from $S$. We prove that
  1. if $S=\{ea\}$ or $S=\{ea,ed\}$, then CDPE($S$) can be solved in polynomial
time;
  2. if $\{vd\} \subseteq S\subseteq \{ea,ed,vd\}$, then CDPE($S$) is
NP-complete and W[1]-hard when parameterized by $k$, even if $\delta\equiv 0$.
  Together with known results by Cai and Yang and by Cygan, Marx, Pilipczuk,
Pilipczuk and Schlotter, our results completely classify the classical and
parameterized complexity of the CDPE($S$) problem for all $S\subseteq
\{ea,ed,vd\}$. We obtain the same classification for a natural variant of the
CDPE($S$) problem on directed graphs, where the target is a weakly connected
digraph in which the difference between the in- and out-degree of every vertex
equals a prescribed value. As an important implication of our results, we
obtain polynomial-time algorithms for the Eulerian Editing problem and its
directed variant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6880</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6880</id><created>2014-10-25</created><authors><author><keyname>Lee</keyname><forenames>Seunghak</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Screening Rules for Overlapping Group Lasso</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, to solve large-scale lasso and group lasso problems, screening
rules have been developed, the goal of which is to reduce the problem size by
efficiently discarding zero coefficients using simple rules independently of
the others. However, screening for overlapping group lasso remains an open
challenge because the overlaps between groups make it infeasible to test each
group independently. In this paper, we develop screening rules for overlapping
group lasso. To address the challenge arising from groups with overlaps, we
take into account overlapping groups only if they are inclusive of the group
being tested, and then we derive screening rules, adopting the dual polytope
projection approach. This strategy allows us to screen each group independently
of each other. In our experiments, we demonstrate the efficiency of our
screening rules on various datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6885</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6885</id><created>2014-10-25</created><authors><author><keyname>Gao</keyname><forenames>Qian</forenames></author><author><keyname>Gong</keyname><forenames>Chen</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Xu</keyname><forenames>Zhengyuan</forenames></author><author><keyname>Hua</keyname><forenames>Yingbo</forenames></author></authors><title>From DC-Biased to DC-Informative Optical OFDM</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel modulation scheme for intensity modulation and direct
detection (IM/DD) based optical communication system employing orthogonal
frequency division multiplexing (OFDM). This method utilizes the DC-bias, which
typically is discarded at the receiver-end, to carry information to achieve
higher power efficiency. By formulating and solving a convex optimization
problem, a constellation in high dimensional space is designed offline for the
input of the transmitter-side inverse fast Fourier transform (IFFT) block. We
point out that one can choose partial or full DC power for information
transmission. Under the condition that the spectrum efficiency is fixed and
attainable, this method bears notable power gain over traditional DC-biased
optical OFDM (DCO-OFDM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6890</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6890</id><created>2014-10-25</created><authors><author><keyname>Ruangwises</keyname><forenames>Suthee</forenames></author><author><keyname>Watanabe</keyname><forenames>Osamu</forenames></author></authors><title>Random Almost-Popular Matchings</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a set $A$ of $n$ people and a set $B$ of $m$ items, with each person
having a preference list that ranks all items from most wanted to least wanted,
we consider the problem of matching every person with a unique item. A matching
$M$ is called $\epsilon$-popular if for any other matching $M'$, the number of
people who prefer $M'$ to $M$ is at most $\epsilon n$ plus the number of those
who prefer $M$ to $M'$. In 2006, Mahdian showed that when randomly generating
people's preference lists, if $m/n &gt; 1.42$, then a 0-popular matching exists
with $1-o(1)$ probability; and if $m/n &lt; 1.42$, then a 0-popular matching
exists with $o(1)$ probability. The ratio 1.42 can be viewed as a transition
point, at which the probability rises from asymptotically zero to
asymptotically one, for the case $\epsilon=0$. In this paper, we introduce an
upper bound and a lower bound of the transition point in more general cases. In
particular, we show that when randomly generating each person's preference
list, if $\alpha(1-e^{-1/\alpha}) &gt; 1-\epsilon$, then an $\epsilon$-popular
matching exists with $1-o(1)$ probability (upper bound); and if
$\alpha(1-e^{-(1+e^{1/\alpha})/\alpha}) &lt; 1-2\epsilon$, then an
$\epsilon$-popular matching exists with $o(1)$ probability (lower bound).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6895</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6895</id><created>2014-10-25</created><updated>2014-12-13</updated><authors><author><keyname>Lee</keyname><forenames>Namgil</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Very Large-Scale Singular Value Decomposition Using Tensor Train
  Networks</title><categories>math.NA cs.NA</categories><msc-class>15A18, 65F15, 65F30</msc-class><journal-ref>SIAM. J. Matrix Anal. Appl. 36(3):994-1014, 2015</journal-ref><doi>10.1137/140983410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose new algorithms for singular value decomposition (SVD) of very
large-scale matrices based on a low-rank tensor approximation technique called
the tensor train (TT) format. The proposed algorithms can compute several
dominant singular values and corresponding singular vectors for large-scale
structured matrices given in a TT format. The computational complexity of the
proposed methods scales logarithmically with the matrix size under the
assumption that both the matrix and the singular vectors admit low-rank TT
decompositions. The proposed methods, which are called the alternating least
squares for SVD (ALS-SVD) and modified alternating least squares for SVD
(MALS-SVD), compute the left and right singular vectors approximately through
block TT decompositions. The very large-scale optimization problem is reduced
to sequential small-scale optimization problems, and each core tensor of the
block TT decompositions can be updated by applying any standard optimization
methods. The optimal ranks of the block TT decompositions are determined
adaptively during iteration process, so that we can achieve high approximation
accuracy. Extensive numerical simulations are conducted for several types of
TT-structured matrices such as Hilbert matrix, Toeplitz matrix, random matrix
with prescribed singular values, and tridiagonal matrix. The simulation results
demonstrate the effectiveness of the proposed methods compared with standard
SVD algorithms and TT-based algorithms developed for symmetric eigenvalue
decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6902</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6902</id><created>2014-10-25</created><authors><author><keyname>Masood</keyname><forenames>Abdullah</forenames></author><author><keyname>Ali</keyname><forenames>M. Asim</forenames></author></authors><title>Applying Agile Requirements Engineering Approach for Re-engineering &amp;
  Changes in existing Brownfield Adaptive Systems</title><categories>cs.SE</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Requirements Engineering (RE) is a key activity in the development of
software systems and is concerned with the identification of the goals of
stakeholders and their elaboration into precise statements of desired services
and behavior. The research describes an Agile Requirements Engineering approach
for re-engineering &amp; changes in existing Brownfield adaptive system. The
approach has few modifications that can be used as a part of SCRUM development
process for re-engineering &amp; changes. The approach illustrates the
re-engineering &amp; changes requirements through introduction of GAP analysis &amp;
requirements structuring &amp; prioritization by creating AS-IS &amp; TO-BE models with
80 / 20 rule. An attempt to close the gap between requirements engineering &amp;
agile methods in form of this approach is provided for practical
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6903</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6903</id><created>2014-10-25</created><authors><author><keyname>M.</keyname><forenames>Laxmi Narayana</forenames></author><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech</title><categories>cs.SD cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used
speech features in most speech and speaker recognition applications. In this
paper, we study the effect of resampling a speech signal on these speech
features. We first derive a relationship between the MFCC param- eters of the
resampled speech and the MFCC parameters of the original speech. We propose six
methods of calculating the MFCC parameters of downsampled speech by
transforming the Mel filter bank used to com- pute MFCC of the original speech.
We then experimentally compute the MFCC parameters of the down sampled speech
using the proposed meth- ods and compute the Pearson coefficient between the
MFCC parameters of the downsampled speech and that of the original speech to
identify the most effective choice of Mel-filter band that enables the computed
MFCC of the resampled speech to be as close as possible to the original speech
sample MFCC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6905</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6905</id><created>2014-10-25</created><authors><author><keyname>M.</keyname><forenames>Laxmi Narayana</forenames></author><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>On the use of Stress information in Speech for Speaker Recognition</title><categories>cs.SD</categories><doi>10.1109/TENCON.2009.5396003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of a speaker recognition system decreases when the speaker is
under stress or emotion. In this paper we explore and identify a mechanism that
enables use of inherent stress-in-speech or speaking style information present
in speech of a person as additional cues for speaker recognition. We quantify
the the inherent stress present in the speech of a speaker mainly using 3
features, namely, pitch, amplitude and duration (together called PAD) We
experimentally observe that the PAD vectors of similar phones in different
words of a speaker are close to each other in the three dimensional (PAD) space
confirming that the way a speaker stresses different syllables in their speech
is unique to them, thus we propose the use of PAD based speaking style of a
speaker as an additional feature for speaker recognition applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6909</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6909</id><created>2014-10-25</created><authors><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author><author><keyname>L</keyname><forenames>Lajish V.</forenames></author></authors><title>A Framework for On-Line Devanagari Handwritten Character Recognition</title><categories>cs.CV</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main challenge in on-line handwritten character recognition in Indian
lan- guage is the large size of the character set, larger similarity between
different characters in the script and the huge variation in writing style. In
this paper we propose a framework for on-line handwitten script recognition
taking cues from speech signal processing literature. The framework is based on
identify- ing strokes, which in turn lead to recognition of handwritten on-line
characters rather that the conventional character identification. Though the
framework is described for Devanagari script, the framework is general and can
be applied to any language.
  The proposed platform consists of pre-processing, feature extraction, recog-
nition and post processing like the conventional character recognition but ap-
plied to strokes. The on-line Devanagari character recognition reduces to one
of recognizing one of 69 primitives and recognition of a character is performed
by recognizing a sequence of such primitives. We further show the impact of
noise removal on on-line raw data which is usually noisy. The use of Fuzzy
Direc- tional Features to enhance the accuracy of stroke recognition is also
described. The recognition results are compared with commonly used directional
features in literature using several classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6910</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6910</id><created>2014-10-25</created><updated>2015-04-15</updated><authors><author><keyname>Kreuz</keyname><forenames>Thomas</forenames></author><author><keyname>Mulansky</keyname><forenames>Mario</forenames></author><author><keyname>Bozanic</keyname><forenames>Nebojsa</forenames></author></authors><title>SPIKY: A graphical user interface for monitoring spike train synchrony</title><categories>physics.data-an cs.MS cs.SE physics.bio-ph physics.med-ph q-bio.NC</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Techniques for recording large-scale neuronal spiking activity are developing
very fast. This leads to an increasing demand for algorithms capable of
analyzing large amounts of experimental spike train data. One of the most
crucial and demanding tasks is the identification of similarity patterns with a
very high temporal resolution and across different spatial scales. To address
this task, in recent years three time-resolved measures of spike train
synchrony have been proposed, the ISI-distance, the SPIKE-distance, and event
synchronization. The Matlab source codes for calculating and visualizing these
measures have been made publicly available. However, due to the many different
possible representations of the results the use of these codes is rather
complicated and their application requires some basic knowledge of Matlab. Thus
it became desirable to provide a more user-friendly and interactive interface.
Here we address this need and present SPIKY, a graphical user interface which
facilitates the application of time-resolved measures of spike train synchrony
to both simulated and real data. SPIKY includes implementations of the
ISI-distance, the SPIKE-distance and SPIKE-synchronization (an improved and
simplified extension of event synchronization) which have been optimized with
respect to computation speed and memory demand. It also comprises a spike train
generator and an event detector which makes it capable of analyzing continuous
data. Finally, the SPIKY package includes additional complementary programs
aimed at the analysis of large numbers of datasets and the estimation of
significance levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6913</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6913</id><created>2014-10-25</created><authors><author><keyname>Kueng</keyname><forenames>Richard</forenames></author><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author><author><keyname>Terstiege</keyname><forenames>Ulrich</forenames></author></authors><title>Low rank matrix recovery from rank one measurements</title><categories>cs.IT math.IT math.PR quant-ph</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the recovery of Hermitian low rank matrices $X \in \mathbb{C}^{n
\times n}$ from undersampled measurements via nuclear norm minimization. We
consider the particular scenario where the measurements are Frobenius inner
products with random rank-one matrices of the form $a_j a_j^*$ for some
measurement vectors $a_1,...,a_m$, i.e., the measurements are given by $y_j =
\mathrm{tr}(X a_j a_j^*)$. The case where the matrix $X=x x^*$ to be recovered
is of rank one reduces to the problem of phaseless estimation (from
measurements, $y_j = |\langle x,a_j\rangle|^2$ via the PhaseLift approach,
which has been introduced recently. We derive bounds for the number $m$ of
measurements that guarantee successful uniform recovery of Hermitian rank $r$
matrices, either for the vectors $a_j$, $j=1,...,m$, being chosen independently
at random according to a standard Gaussian distribution, or $a_j$ being sampled
independently from an (approximate) complex projective $t$-design with $t=4$.
In the Gaussian case, we require $m \geq C r n$ measurements, while in the case
of $4$-designs we need $m \geq Cr n \log(n)$. Our results are uniform in the
sense that one random choice of the measurement vectors $a_j$ guarantees
recovery of all rank $r$-matrices simultaneously with high probability.
Moreover, we prove robustness of recovery under perturbation of the
measurements by noise. The result for approximate $4$-designs generalizes and
improves a recent bound on phase retrieval due to Gross, Kueng and Krahmer. In
addition, it has applications in quantum state tomography. Our proofs employ
the so-called bowling scheme which is based on recent ideas by Mendelson and
Koltchinskii.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6915</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6915</id><created>2014-10-25</created><authors><author><keyname>Veitas</keyname><forenames>Viktoras</forenames><affiliation>Weaver</affiliation></author><author><keyname>Weinbaum</keyname><forenames>David</forenames><affiliation>Weaver</affiliation></author></authors><title>A World of Views: A World of Interacting Post-human Intelligences</title><categories>cs.CY</categories><comments>Preprint of a book chapter, 36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What would a human hundreds or thousands times more intelligent than the
brightest human ever born be like? We must admit we can hardly guess. A human
being of such intelligence will be so radically different from us that it can
hardly, if at all, be recognized as human. If we had to go back along the
evolutionary tree to identify a creature 1000 times less intelligent than the
average contemporary human, we will have to go really far back. Would it be a
kind of a lizard? An insect perhaps? Considering this, how can we possibly
aspire to have a grasp of something a thousand times more intelligent than us?
When it comes to intelligence, even the very attempt to quantify it is highly
misleading. Now if we attend to a seemingly adjacent question, what would a
machine with such capacity for intelligence be like? Just coming up with an
approximate metaphor requires a huge stretch of the imagination, meaning that
almost anything goes... What would a society of such super intelligent agents,
be they human, machines or an amalgam of both, be like? Well, here we are
transported into the realm of pure speculation. Technological Singularity is
referred to as the event of artificial intelligence surpassing the intelligence
of humans and shortly after augmenting itself far beyond that. It is no wonder
that the mathematical concept of singularity has become the symbol of an event
so disruptive and so far reaching that it is impossible to conceptually or even
metaphorically grasp, much less to predict.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6935</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6935</id><created>2014-10-25</created><authors><author><keyname>Margolies</keyname><forenames>Robert</forenames></author><author><keyname>Gorlatova</keyname><forenames>Maria</forenames></author><author><keyname>Sarik</keyname><forenames>John</forenames></author><author><keyname>Kinget</keyname><forenames>Peter</forenames></author><author><keyname>Kymissis</keyname><forenames>Ioannis</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Project-based Learning within a Large-Scale Interdisciplinary Research
  Effort</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern engineering landscape increasingly requires a range of skills to
successfully integrate complex systems. Project-based learning is used to help
students build professional skills. However, it is typically applied to small
teams and small efforts. This paper describes an experience in engaging a large
number of students in research projects within a multi-year interdisciplinary
research effort. The projects expose the students to various disciplines in
Computer Science (embedded systems, algorithm design, networking), Electrical
Engineering (circuit design, wireless communications, hardware prototyping),
and Applied Physics (thin-film battery design, solar cell fabrication). While a
student project is usually focused on one discipline area, it requires
interaction with at least two other areas. Over 5 years, 180 semester-long
projects have been completed. The students were a diverse group of high school,
undergraduate, and M.S. Computer Science, Computer Engineering, and Electrical
Engineering students. Some of the approaches that were taken to facilitate
student learning are real-world system development constraints, regular
cross-group meetings, and extensive involvement of Ph.D. students in student
mentorship and knowledge transfer. To assess the approaches, a survey was
conducted among the participating students. The results demonstrate the
effectiveness of the approaches. For example, 70% of the students surveyed
indicated that working on their research project improved their ability to
function on multidisciplinary teams more than coursework, internships, or any
other activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6937</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6937</id><created>2014-10-25</created><authors><author><keyname>Cariow</keyname><forenames>Aleksandr</forenames></author><author><keyname>Cariowa</keyname><forenames>Galina</forenames></author></authors><title>A Hardware-oriented Algorithm for Complex-valued Constant Matrix-vector
  Multiplication</title><categories>cs.DS</categories><comments>4 pages, 3 fgures</comments><msc-class>65F30, 68W10, 68W35</msc-class><acm-class>B.2.4; C.1.4; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a hardware-oriented algorithm for constant
matrix-vector product calculating, when the all elements of vector and matrix
are complex numbers. The proposed algorithm versus the naive method of
analogous calculations drastically reduces the number of multipliers required
for FPGA implementation of complex-valued constant matrix-vector
multiplication.If the fully parallel hardware implementation of naive
(schoolbook) method for complex-valued matrix-vector multiplication requires
4MN multipliers, 2M N-inputs adders and 2MN two-input adders, the proposed
algorithm requires only 3N(M+1)/2 multipliers and 3M(N+2)+1,5N+2 two-input
adders and 3(M+1) N/2-input adders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6951</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6951</id><created>2014-10-25</created><updated>2015-06-26</updated><authors><author><keyname>Ghaffari</keyname><forenames>H. O.</forenames></author><author><keyname>Benson</keyname><forenames>P.</forenames></author><author><keyname>Xia</keyname><forenames>K.</forenames></author><author><keyname>Young</keyname><forenames>R. P.</forenames></author></authors><title>Observation of the Kibble-Zurek Mechanism in Microscopic Acoustic
  Cracking Noises</title><categories>cs.CE cond-mat.mtrl-sci cond-mat.other physics.geo-ph</categories><doi>10.1038/srep21210</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The fast evolution of microstructure is key to understanding crackling
phenomena. It has been proposed that formation of a nonlinear zone around a
moving crack tip controls the crack tip velocity. Progress in understanding the
physics of this critical zone has been limited due to the lack of hard data
describing the detailed complex physical processes that occur within. For the
first time, we show that the signature of the non-linear elastic zone around a
microscopic dynamic crack maps directly to generic phases of acoustic noises,
supporting the formation of a strongly weak zone near the moving crack tips. We
additionally show that the rate of traversing to non-linear zone controls the
rate of weakening, i.e. speed of global rupture propagation. We measure the
power-law dependence of nonlinear zone size on the traversing rate, and show
that our observations are in agreement with the Kibble-Zurek mechanism (KZM) .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6952</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6952</id><created>2014-10-25</created><authors><author><keyname>Mitra</keyname><forenames>Karan</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>&#xc5;hlund</keyname><forenames>Christer</forenames></author></authors><title>QoE Modelling, Measurement and Prediction: A Review</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mobile computing systems, users can access network services anywhere and
anytime using mobile devices such as tablets and smart phones. These devices
connect to the Internet via network or telecommunications operators. Users
usually have some expectations about the services provided to them by different
operators. Users' expectations along with additional factors such as cognitive
and behavioural states, cost, and network quality of service (QoS) may
determine their quality of experience (QoE). If users are not satisfied with
their QoE, they may switch to different providers or may stop using a
particular application or service. Thus, QoE measurement and prediction
techniques may benefit users in availing personalized services from service
providers. On the other hand, it can help service providers to achieve lower
user-operator switchover. This paper presents a review of the state-the-art
research in the area of QoE modelling, measurement and prediction. In
particular, we investigate and discuss the strengths and shortcomings of
existing techniques. Finally, we present future research directions for
developing novel QoE measurement and prediction techniques
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6956</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6956</id><created>2014-10-25</created><authors><author><keyname>Morral</keyname><forenames>Gemma</forenames></author><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author><author><keyname>Fort</keyname><forenames>Gersende</forenames></author></authors><title>Success and Failure of Adaptation-Diffusion Algorithms for Consensus in
  Multi-Agent Networks</title><categories>cs.MA cs.NA cs.SY</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of distributed stochastic approximation
in multi-agent systems. The algorithm under study consists of two steps: a
local stochastic approximation step and a diffusion step which drives the
network to a consensus. The diffusion step uses row-stochastic matrices to
weight the network exchanges. As opposed to previous works, exchange matrices
are not supposed to be doubly stochastic, and may also depend on the past
estimate.
  We prove that non-doubly stochastic matrices generally influence the limit
points of the algorithm. Nevertheless, the limit points are not affected by the
choice of the matrices provided that the latter are doubly-stochastic in
expectation. This conclusion legitimates the use of broadcast-like diffusion
protocols, which are easier to implement. Next, by means of a central limit
theorem, we prove that doubly stochastic protocols perform asymptotically as
well as centralized algorithms and we quantify the degradation caused by the
use of non doubly stochastic matrices. Throughout the paper, a special emphasis
is put on the special case of distributed non-convex optimization as an
illustration of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6960</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6960</id><created>2014-10-25</created><authors><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Parameterizing the semantics of fuzzy attribute implications by systems
  of isotone Galois connections</title><categories>cs.AI</categories><msc-class>03B52, 68P20, 06A15</msc-class><acm-class>I.2.3; I.2.4</acm-class><doi>10.1109/TFUZZ.2015.2470530</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the semantics of fuzzy if-then rules called fuzzy attribute
implications parameterized by systems of isotone Galois connections. The rules
express dependencies between fuzzy attributes in object-attribute incidence
data. The proposed parameterizations are general and include as special cases
the parameterizations by linguistic hedges used in earlier approaches. We
formalize the general parameterizations, propose bivalent and graded notions of
semantic entailment of fuzzy attribute implications, show their
characterization in terms of least models and complete axiomatization, and
provide characterization of bases of fuzzy attribute implications derived from
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6968</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6968</id><created>2014-10-25</created><updated>2015-05-10</updated><authors><author><keyname>Becker</keyname><forenames>Christoph</forenames></author><author><keyname>Chitchyan</keyname><forenames>Ruzanna</forenames></author><author><keyname>Duboc</keyname><forenames>Leticia</forenames></author><author><keyname>Easterbrook</keyname><forenames>Steve</forenames></author><author><keyname>Mahaux</keyname><forenames>Martin</forenames></author><author><keyname>Penzenstadler</keyname><forenames>Birgit</forenames></author><author><keyname>Rodriguez-Navas</keyname><forenames>Guillermo</forenames></author><author><keyname>Salinesi</keyname><forenames>Camille</forenames></author><author><keyname>Seyff</keyname><forenames>Norbert</forenames></author><author><keyname>Venters</keyname><forenames>Colin</forenames></author><author><keyname>Calero</keyname><forenames>Coral</forenames></author><author><keyname>Kocak</keyname><forenames>Sedef Akinli</forenames></author><author><keyname>Betz</keyname><forenames>Stefanie</forenames></author></authors><title>The Karlskrona manifesto for sustainability design</title><categories>cs.SE cs.GL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sustainability is a central concern for our society, and software systems
increasingly play a central role in it. As designers of software technology, we
cause change and are responsible for the effects of our design choices. We
recognize that there is a rapidly increasing awareness of the fundamental need
and desire for a more sustainable world, and there is a lot of genuine
goodwill. However, this alone will be ineffective unless we come to understand
and address our persistent misperceptions. The Karlskrona Manifesto for
Sustainability Design aims to initiate a much needed conversation in and beyond
the software community by highlighting such perceptions and proposing a set of
fundamental principles for sustainability design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6973</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6973</id><created>2014-10-25</created><updated>2015-02-05</updated><authors><author><keyname>Bojarski</keyname><forenames>Mariusz</forenames></author><author><keyname>Choromanska</keyname><forenames>Anna</forenames></author><author><keyname>Choromanski</keyname><forenames>Krzysztof</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Differentially- and non-differentially-private random decision trees</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider supervised learning with random decision trees, where the tree
construction is completely random. The method is popularly used and works well
in practice despite the simplicity of the setting, but its statistical
mechanism is not yet well-understood. In this paper we provide strong
theoretical guarantees regarding learning with random decision trees. We
analyze and compare three different variants of the algorithm that have minimal
memory requirements: majority voting, threshold averaging and probabilistic
averaging. The random structure of the tree enables us to adapt these methods
to a differentially-private setting thus we also propose differentially-private
versions of all three schemes. We give upper-bounds on the generalization error
and mathematically explain how the accuracy depends on the number of random
decision trees. Furthermore, we prove that only logarithmic (in the size of the
dataset) number of independently selected random decision trees suffice to
correctly classify most of the data, even when differential-privacy guarantees
must be maintained. We empirically show that majority voting and threshold
averaging give the best accuracy, also for conservative users requiring high
privacy guarantees. Furthermore, we demonstrate that a simple majority voting
rule is an especially good candidate for the differentially-private classifier
since it is much less sensitive to the choice of forest parameters than other
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6975</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6975</id><created>2014-10-25</created><authors><author><keyname>Agarwal</keyname><forenames>Apoorv</forenames></author><author><keyname>Choromanska</keyname><forenames>Anna</forenames></author><author><keyname>Choromanski</keyname><forenames>Krzysztof</forenames></author></authors><title>Notes on using Determinantal Point Processes for Clustering with
  Applications to Text Clustering</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we compare three initialization schemes for the KMEANS
clustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and
3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k
needs to be set by the user of the algorithms. (Kang 2013) recently proposed a
novel use of determinantal point processes for sampling the initial centroids
for the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide
any evaluation establishing that KMEANSD++ is better than other algorithms. In
this paper, we show that the performance of KMEANSD++ is comparable to KMEANS++
(both of which are better than KMEANSRAND) with KMEANSD++ having an additional
that it can automatically approximate the value of k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6976</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6976</id><created>2014-10-25</created><updated>2016-01-31</updated><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author><author><keyname>Delling</keyname><forenames>Daniel</forenames></author><author><keyname>Pajor</keyname><forenames>Thomas</forenames></author><author><keyname>Werneck</keyname><forenames>Renato F.</forenames></author></authors><title>Distance-Based Influence in Networks: Computation and Maximization</title><categories>cs.SI</categories><comments>20 pages, 5 figures</comments><acm-class>H.2.8, G.2.2, G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A premise at a heart of network analysis is that entities in a network derive
utilities from their connections. The {\em influence} of a seed set $S$ of
nodes is defined as the sum over nodes $u$ of the {\em utility} of $S$ to $u$.
{\em Distance-based} utility, which is a decreasing function of the distance
from $S$ to $u$, was explored in several successful research threads from
social network analysis and economics: Network formation games [Bloch
andJackson 2007], Reachability-based influence [Richardson and Domingos 2002,
Kempe et al. 2003], &quot;threshold&quot; influence [Gomez-Rodriguez et al. 2011], and
{\em closeness centrality} [Bavelas 1948].
  We formulate a model that unifies and extends this previous work and address
the two fundamental computational problems in this domain: {\em Influence
oracles} and {\em influence maximization} (IM). An oracle performs some
preprocessing, after which influence queries for arbitrary seed sets can be
efficiently computed. With IM, we seek a set of nodes of a given size with
maximum influence. Since the IM problem is computationally hard, we instead
seek a {\em greedy sequence} of nodes, with each prefix having influence that
is at least $1-1/e$ of that of the optimal seed set of the same size. We
present the first highly scalable algorithms for both problems, providing
statistical guarantees on approximation quality and near-linear worst-case
bounds on the computation. We perform an experimental evaluation which
demonstrates the effectiveness of our designs on networks with hundreds of
millions of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6990</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6990</id><created>2014-10-26</created><authors><author><keyname>Xu</keyname><forenames>Chang</forenames></author><author><keyname>Liu</keyname><forenames>Tongliang</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author></authors><title>Local Rademacher Complexity for Multi-label Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the local Rademacher complexity of empirical risk minimization
(ERM)-based multi-label learning algorithms, and in doing so propose a new
algorithm for multi-label learning. Rather than using the trace norm to
regularize the multi-label predictor, we instead minimize the tail sum of the
singular values of the predictor in multi-label learning. Benefiting from the
use of the local Rademacher complexity, our algorithm, therefore, has a sharper
generalization error bound and a faster convergence rate. Compared to methods
that minimize over all singular values, concentrating on the tail singular
values results in better recovery of the low-rank structure of the multi-label
predictor, which plays an import role in exploiting label correlations. We
propose a new conditional singular value thresholding algorithm to solve the
resulting objective function. Empirical studies on real-world datasets validate
our theoretical results and demonstrate the effectiveness of the proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6991</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6991</id><created>2014-10-26</created><updated>2014-11-04</updated><authors><author><keyname>Bansal</keyname><forenames>Trapit</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Chiranjib</forenames></author><author><keyname>Kannan</keyname><forenames>Ravindran</forenames></author></authors><title>A provable SVD-based algorithm for learning topics in dominant admixture
  corpus</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents
are drawn from admixtures of distributions over words, known as topics. The
inference problem of recovering topics from admixtures, is NP-hard. Assuming
separability, a strong assumption, [4] gave the first provable algorithm for
inference. For LDA model, [6] gave a provable algorithm using tensor-methods.
But [4,6] do not learn topic vectors with bounded $l_1$ error (a natural
measure for probability vectors). Our aim is to develop a model which makes
intuitive and empirically supported assumptions and to design an algorithm with
natural, simple components such as SVD, which provably solves the inference
problem for the model with bounded $l_1$ error. A topic in LDA and other models
is essentially characterized by a group of co-occurring words. Motivated by
this, we introduce topic specific Catchwords, group of words which occur with
strictly greater frequency in a topic than any other topic individually and are
required to have high frequency together rather than individually. A major
contribution of the paper is to show that under this more realistic assumption,
which is empirically verified on real corpora, a singular value decomposition
(SVD) based algorithm with a crucial pre-processing step of thresholding, can
provably recover the topics from a collection of documents drawn from Dominant
admixtures. Dominant admixtures are convex combination of distributions in
which one distribution has a significantly higher contribution than others.
Apart from the simplicity of the algorithm, the sample complexity has near
optimal dependence on $w_0$, the lowest probability that a topic is dominant,
and is better than [4]. Empirical evidence shows that on several real world
corpora, both Catchwords and Dominant admixture assumptions hold and the
proposed algorithm substantially outperforms the state of the art [5].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.6996</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.6996</id><created>2014-10-26</created><authors><author><keyname>Maharramov</keyname><forenames>Musa</forenames></author><author><keyname>Biondi</keyname><forenames>Biondo</forenames></author></authors><title>Improved depth imaging by constrained full-waveform inversion</title><categories>physics.geo-ph cs.CV</categories><comments>5 pages, 2 figures</comments><report-no>SEP 155</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a formulation of full-wavefield inversion (FWI) as a constrained
optimization problem, and describe a computationally efficient technique for
solving constrained full-wavefield inversion (CFWI). The technique is based on
using a total-variation regularization method, with the regularization weighted
in favor of constraining deeper subsurface model sections. The method helps to
promote &quot;edge-preserving&quot; blocky model inversion where fitting the seismic data
alone fails to adequately constrain the model. The method is demonstrated on
synthetic datasets with added noise, and is shown to enhance the sharpness of
the inverted model and correctly reposition mispositioned reflectors by better
constraining the velocity model at depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7005</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7005</id><created>2014-10-26</created><authors><author><keyname>Huleihel</keyname><forenames>Wasim</forenames></author><author><keyname>Weinberger</keyname><forenames>Nir</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Erasure/List Random Coding Error Exponents Are Not Universally
  Achievable</title><categories>cs.IT math.IT</categories><comments>26 pages; submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of universal decoding for unknown discrete memoryless
channels in the presence of erasure/list option at the decoder, in the random
coding regime. Specifically, we harness a universal version of Forney's
classical erasure/list decoder developed in earlier studies, which is based on
the competitive minimax methodology, and guarantees universal achievability of
a certain fraction of the optimum random coding error exponents. In this paper,
we derive an exact single-letter expression for the maximum achievable
fraction. Examples are given in which the maximal achievable fraction is
strictly less than unity, which imply that, in general, there is no universal
erasure/list decoder which achieves the same random coding error exponents as
the optimal decoder for a known channel. This is in contrast to the situation
in ordinary decoding (without the erasure/list option), where optimum exponents
are universally achievable, as is well known. It is also demonstrated that
previous lower bounds derived for the maximal achievable fraction are not tight
in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7012</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7012</id><created>2014-10-26</created><authors><author><keyname>Boissonnat</keyname><forenames>Jean-Daniel</forenames></author><author><keyname>Dyer</keyname><forenames>Ramsay</forenames></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames></author><author><keyname>Oudot</keyname><forenames>Steve Y.</forenames></author></authors><title>Only distances are required to reconstruct submanifolds</title><categories>cs.CG</categories><comments>35 pages</comments><msc-class>68W05</msc-class><acm-class>I.3.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we give the first algorithm that outputs a faithful
reconstruction of a submanifold of Euclidean space without maintaining or even
constructing complicated data structures such as Voronoi diagrams or Delaunay
complexes. Our algorithm uses the witness complex and relies on the stability
of power protection, a notion introduced in this paper. The complexity of the
algorithm depends exponentially on the intrinsic dimension of the manifold,
rather than the dimension of ambient space, and linearly on the dimension of
the ambient space. Another interesting feature of this work is that no explicit
coordinates of the points in the point sample is needed. The algorithm only
needs the distance matrix as input, i.e., only distance between points in the
point sample as input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7013</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7013</id><created>2014-10-26</created><authors><author><keyname>Lanese</keyname><forenames>Ivan</forenames><affiliation>Focus Team, University of Bologna/INRIA</affiliation></author><author><keyname>Lafuente</keyname><forenames>Alberto Lluch</forenames><affiliation>DTU Compute, Technical University of Denmark</affiliation></author><author><keyname>Sokolova</keyname><forenames>Ana</forenames><affiliation>University of Salzburg</affiliation></author><author><keyname>Vieira</keyname><forenames>Hugo Torres</forenames><affiliation>LaSIGE, University of Lisbon</affiliation></author></authors><title>Proceedings 7th Interaction and Concurrency Experience</title><categories>cs.LO cs.PL cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 166, 2014</journal-ref><doi>10.4204/EPTCS.166</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of ICE 2014, the 7th Interaction and
Concurrency Experience, which was held in Berlin, Germany on the 6th of June
2014 as a satellite event of DisCoTec 2014. The ICE procedure for paper
selection allows PC members to interact, anonymously, with authors. During the
review phase, each submitted paper is published on a Wiki and associated with a
discussion forum whose access is restricted to the authors and to all the PC
members not declaring a conflict of interests. The PC members post comments and
questions that the authors reply to. Each paper was reviewed by three PC
members, and altogether 8 papers (including 3 short papers) were accepted for
publication. We were proud to host two invited talks, by Pavol Cerny and Kim
Larsen, whose abstracts are included in this volume together with the regular
papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7026</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7026</id><created>2014-10-26</created><authors><author><keyname>Ma</keyname><forenames>Jingying</forenames></author><author><keyname>Zheng</keyname><forenames>Yuanshi</forenames></author><author><keyname>Wu</keyname><forenames>Bin</forenames></author><author><keyname>Wang</keyname><forenames>Long</forenames></author></authors><title>Optimal topology of multi-agent systems with two leaders: a zero-sum
  game perspective</title><categories>cs.SY cs.GT cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is typical to assume that there is no conflict of interest among leaders.
Under such assumption, it is known that, for a multi-agent system with two
leaders, if the followers' interaction subgraph is undirected and connected,
then followers will converge to a convex combination of two leaders' states
with linear consensus protocol. In this paper, we introduce the conflict
between leaders: by choosing k followers to connect with, every leader attempts
all followers converge to himself closer than that of the other. By using graph
theory and matrix theory, we formulate this conflict as a standard two-player
zero-sum game and give some properties about it. It is noteworthy that the
interaction graph here is generated from the conflict between leaders.
Interestingly, we find that to find the optimal topology of the system is
equivalent to solve a Nash equilibrium. Especially for the case of choosing one
connected follower, the necessary and sufficient condition for an interaction
graph to be the optimal one is given. Moreover, if followers' interaction graph
is a circulant graph or a graph with a center node, then the system's optimal
topology is obtained. Simulation examples are provided to validate the
effectiveness of the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7041</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7041</id><created>2014-10-26</created><authors><author><keyname>Huo</keyname><forenames>Qiang</forenames></author><author><keyname>Yang</keyname><forenames>Kun</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Compressed Relaying for Two-Way Relay Networks with Correlated Sources</title><categories>cs.IT math.IT</categories><comments>11 pages, 4 figures. appears in IEEE Wireless Communications Letters,
  2014</comments><journal-ref>IEEE Wireless Communications Letters, Vol. 4, No. 1, February 2015</journal-ref><doi>10.1109/LWC.2014.2362748</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, a compressed relaying scheme via Huffman and physical-layer
network coding (HPNC) is proposed for two-way relay networks with correlated
sources (TWRN-CS). In the HPNC scheme, both sources first transmit the
correlated raw source messages to the relay simultaneously. The relay performs
physical-layer network coding (PNC) on the received symbols, compresses the
PNC-coded symbols using Huffman coding, and broadcasts the compressed symbols
to both source nodes. Then, each source decodes the other source's messages by
using its own messages as side information. Compression rate and block error
rate (BLER) of the proposed scheme are analyzed. Simulation results demonstrate
that the HPNC scheme can effectively improve the network throughput, and
meanwhile, achieve the superior BLER performance compared with the conventional
non-compressed relaying scheme in TWRN-CS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7050</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7050</id><created>2014-10-26</created><updated>2015-06-25</updated><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author></authors><title>A PTAS for Agnostically Learning Halfspaces</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a PTAS for agnostically learning halfspaces w.r.t. the uniform
distribution on the $d$ dimensional sphere. Namely, we show that for every
$\mu&gt;0$ there is an algorithm that runs in time
$\mathrm{poly}(d,\frac{1}{\epsilon})$, and is guaranteed to return a classifier
with error at most $(1+\mu)\mathrm{opt}+\epsilon$, where $\mathrm{opt}$ is the
error of the best halfspace classifier. This improves on Awasthi, Balcan and
Long [ABL14] who showed an algorithm with an (unspecified) constant
approximation ratio. Our algorithm combines the classical technique of
polynomial regression (e.g. [LMN89, KKMS05]), together with the new
localization technique of [ABL14].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7052</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7052</id><created>2014-10-26</created><updated>2014-11-13</updated><authors><author><keyname>Ethier</keyname><forenames>S. N.</forenames></author><author><keyname>Lee</keyname><forenames>Jiyeon</forenames></author></authors><title>On the three-person game baccara banque</title><categories>math.OC cs.GT math.PR</categories><comments>22 pages, 4 figures, and a 54-page appendix; new figure and minor
  corrections in v2</comments><msc-class>91A06</msc-class><journal-ref>Games 6 (2015) 57-78</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Baccara banque is a three-person zero-sum game parameterized by
$\theta\in(0,1)$. A study of the game by Downton and Lockwood claimed that the
Nash equilibrium is of only academic interest. Their preferred alternative is
what we call the independent cooperative equilibrium. But this solution exists
only for certain $\theta$. A third solution, which we call the correlated
cooperative equilibrium, always exists. Under a &quot;with replacement&quot; assumption
as well as a simplifying assumption concerning the information available to one
of the players, we derive each of the three solutions for all $\theta$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7057</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7057</id><created>2014-10-26</created><authors><author><keyname>Das</keyname><forenames>Bijit Kumar</forenames></author><author><keyname>Chakraborty</keyname><forenames>Mrityunjoy</forenames></author><author><keyname>Arenas-Garc&#xed;a</keyname><forenames>Jer&#xf3;nimo</forenames></author></authors><title>Sparse Distributed Learning via Heterogeneous Diffusion Adaptive
  Networks</title><categories>cs.LG cs.DC cs.SY stat.ML</categories><comments>4 pages, 1 figure, conference, submitted to IEEE ISCAS 2015, Lisbon,
  Portugal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-network distributed estimation of sparse parameter vectors via diffusion
LMS strategies has been studied and investigated in recent years. In all the
existing works, some convex regularization approach has been used at each node
of the network in order to achieve an overall network performance superior to
that of the simple diffusion LMS, albeit at the cost of increased computational
overhead. In this paper, we provide analytical as well as experimental results
which show that the convex regularization can be selectively applied only to
some chosen nodes keeping rest of the nodes sparsity agnostic, while still
enjoying the same optimum behavior as can be realized by deploying the convex
regularization at all the nodes. Due to the incorporation of unregularized
learning at a subset of nodes, less computational cost is needed in the
proposed approach. We also provide a guideline for selection of the sparsity
aware nodes and a closed form expression for the optimum regularization
parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7063</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7063</id><created>2014-10-26</created><updated>2015-10-29</updated><authors><author><keyname>Beckers</keyname><forenames>Sander</forenames></author><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author></authors><title>Towards a General Framework for Actual Causation Using CP-logic</title><categories>cs.AI</categories><comments>http://ceur-ws.org/Vol-1413/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since Pearl's seminal work on providing a formal language for causality, the
subject has garnered a lot of interest among philosophers and researchers in
artificial intelligence alike. One of the most debated topics in this context
regards the notion of actual causation, which concerns itself with specific -
as opposed to general - causal claims. The search for a proper formal
definition of actual causation has evolved into a controversial debate, that is
pervaded with ambiguities and confusion. The goal of our research is twofold.
First, we wish to provide a clear way to compare competing definitions. Second,
we also want to improve upon these definitions so they can be applied to a more
diverse range of instances, including non-deterministic ones. To achieve these
goals we will provide a general, abstract definition of actual causation,
formulated in the context of the expressive language of CP-logic (Causal
Probabilistic logic). We will then show that three recent definitions by Ned
Hall (originally formulated for structural models) and a definition of our own
(formulated for CP-logic directly) can be viewed and directly compared as
instantiations of this abstract definition, which allows them to deal with a
broader range of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7074</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7074</id><created>2014-10-26</created><updated>2015-08-21</updated><authors><author><keyname>Beijbom</keyname><forenames>Oscar</forenames></author></authors><title>Random Sampling in an Age of Automation: Minimizing Expenditures through
  Balanced Collection and Annotation</title><categories>cs.CY cs.LG stat.ME</categories><comments>PDF contains 9 pages of manuscript followed by 7 pages of
  Supplementary Information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods for automated collection and annotation are changing the
cost-structures of sampling surveys for a wide range of applications. Digital
samples in the form of images or audio recordings can be collected rapidly, and
annotated by computer programs or crowd workers. We consider the problem of
estimating a population mean under these new cost-structures, and propose a
Hybrid-Offset sampling design. This design utilizes two annotators: a primary,
which is accurate but costly (e.g. a human expert) and an auxiliary which is
noisy but cheap (e.g. a computer program), in order to minimize total sampling
expenditures. Our analysis gives necessary conditions for the Hybrid-Offset
design and specifies optimal sample sizes for both annotators. Simulations on
data from a coral reef survey program indicate that the Hybrid-Offset design
outperforms several alternative sampling designs. In particular, sampling
expenditures are reduced 50% compared to the Conventional design currently
deployed by the coral ecologists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7082</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7082</id><created>2014-10-26</created><authors><author><keyname>Maksimenko</keyname><forenames>Aleksandr</forenames></author></authors><title>Complexity of LP in Terms of the Face Lattice</title><categories>cs.CC math.CO</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X$ be a finite set in $Z^d$. We consider the problem of optimizing
linear function $f(x) = c^T x$ on $X$, where $c\in Z^d$ is an input vector. We
call it a problem $X$. A problem $X$ is related with linear program
$\max\limits_{x \in P} f(x)$, where polytope $P$ is a convex hull of $X$. The
key parameters for evaluating the complexity of a problem $X$ are the dimension
$d$, the cardinality $|X|$, and the encoding size $S(X) = \log_2
\left(\max\limits_{x\in X} \|x\|_{\infty}\right)$. We show that if the (time
and space) complexity of some algorithm $A$ for solving a problem $X$ is
defined only in terms of combinatorial structure of $P$ and the size $S(X)$,
then for every $d$ and $n$ there exists polynomially (in $d$, $\log n$, and
$S$) solvable problem $Y$ with $\dim Y = d$, $|Y| = n$, such that the algorithm
$A$ requires exponential time or space for solving $Y$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7091</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7091</id><created>2014-10-26</created><authors><author><keyname>Szajowski</keyname><forenames>Krzysztof</forenames></author></authors><title>On Some Distributed Disorder Detection</title><categories>math.OC cs.SY math.ST stat.TH</categories><comments>8. arXiv admin note: substantial text overlap with arXiv:1111.4504,
  arXiv:1304.6986</comments><msc-class>60G40</msc-class><acm-class>B.1.3; B.4.5; B.5.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate data sources with components of different information value seem
to appear frequently in practice. Models in which the components change their
homogeneity at different times are of significant importance. The fact whether
any changes are influential for the whole process is determined not only by the
moments of the change, but also depends on which coordinates. This is
particularly important in issues such as reliability analysis of complex
systems and the location of an intruder in surveillance systems. In this paper
we developed a mathematical model for such sources of signals with discrete
time having the Markov property given the times of change. The research also
comprises a multivariate detection of the transition probabilities changes at
certain sensitivity level in the multidimensional process. Additionally, the
observation of the random vector is depicted. Each chosen coordinate forms the
Markov process with different transition probabilities before and after some
unknown moment. The aim of statisticians is to estimate the moments based on
the observation of the process. The Bayesian approach is used with the risk
function depending on measure of chance of a false alarm and some cost of
overestimation. The moment of the system's disorder is determined by the
detection of transition probabilities changes at some coordinates. The overall
modeling of the critical coordinates is based on the simple game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7092</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7092</id><created>2014-10-26</created><authors><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Golin</keyname><forenames>Mordecai</forenames></author><author><keyname>Lam</keyname><forenames>Tak-Wah</forenames></author><author><keyname>Nogneng</keyname><forenames>Dorian</forenames></author></authors><title>Scheduling with Gaps: New Models and Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider scheduling problems for unit jobs with release times, where the
number or size of the gaps in the schedule is taken into consideration, either
in the objective function or as a constraint. Except for a few papers on energy
minimization, there is no work in the scheduling literature that uses
performance metrics depending on the gap structure of a schedule. One of our
objectives is to initiate the study of such scheduling problems with gaps. We
show that such problems often lead to interesting algorithmic problems, with
connections to other areas of algorithmics. We focus on the model with unit
jobs. First we examine scheduling problems with deadlines, where we consider
variants of minimum-gap scheduling, including maximizing throughput with a
budget for gaps or minimizing the number of gaps with a throughput requirement.
We then turn to other objective functions. For example, in some scenarios, gaps
in a schedule may be actually desirable, leading to the problem of maximizing
the number of gaps. Other versions we study include minimizing maximum gap or
maximizing minimum gap. The second part of the paper examines the model without
deadlines, where we focus on the tradeoff between the number of gaps and the
total or maximum flow time. For all these problems we provide polynomial time
algorithms, with running times ranging from $O(n \log n)$ for some problems, to
$O(n^7)$ for other. The solutions involve a spectrum of algo- rithmic
techniques, including different dynamic programming formulations, speed-up
techniques based on searching Monge arrays, searching X + Y matrices, or
implicit binary search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7100</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7100</id><created>2014-10-26</created><authors><author><keyname>Georgiou</keyname><forenames>Harris V.</forenames></author></authors><title>Estimating the intrinsic dimension in fMRI space via dataset fractal
  analysis - Counting the `cpu cores' of the human brain</title><categories>cs.AI cs.CV q-bio.NC stat.ML</categories><comments>27 pages, 10 figures, 2 tables, 47 references</comments><report-no>HG/AI.1014.27v1 (draft/preprint)</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Functional Magnetic Resonance Imaging (fMRI) is a powerful non-invasive tool
for localizing and analyzing brain activity. This study focuses on one very
important aspect of the functional properties of human brain, specifically the
estimation of the level of parallelism when performing complex cognitive tasks.
Using fMRI as the main modality, the human brain activity is investigated
through a purely data-driven signal processing and dimensionality analysis
approach. Specifically, the fMRI signal is treated as a multi-dimensional data
space and its intrinsic `complexity' is studied via dataset fractal analysis
and blind-source separation (BSS) methods. One simulated and two real fMRI
datasets are used in combination with Independent Component Analysis (ICA) and
fractal analysis for estimating the intrinsic (true) dimensionality, in order
to provide data-driven experimental evidence on the number of independent brain
processes that run in parallel when visual or visuo-motor tasks are performed.
Although this number is can not be defined as a strict threshold but rather as
a continuous range, when a specific activation level is defined, a
corresponding number of parallel processes or the casual equivalent of `cpu
cores' can be detected in normal human brain activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7103</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7103</id><created>2014-10-26</created><updated>2014-11-05</updated><authors><author><keyname>Jay</keyname><forenames>Barry</forenames></author><author><keyname>Vergara</keyname><forenames>Jose</forenames></author></authors><title>Confusion in the Church-Turing Thesis</title><categories>cs.LO cs.PL</categories><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Church-Turing Thesis confuses numerical computations with symbolic
computations. In particular, any model of computability in which equality is
not definable, such as the lambda-models underpinning higher-order programming
languages, is not equivalent to the Turing model. However, a modern combinatory
calculus, the SF-calculus, can define equality of its closed normal forms, and
so yields a model of computability that is equivalent to the Turing model. This
has profound implications for programming language design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7140</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7140</id><created>2014-10-27</created><updated>2016-02-24</updated><authors><author><keyname>Zhang</keyname><forenames>Nevin L.</forenames></author><author><keyname>Fu</keyname><forenames>Chen</forenames></author><author><keyname>Liu</keyname><forenames>Teng Fei</forenames></author><author><keyname>Chen</keyname><forenames>Bao Xin</forenames></author><author><keyname>Poon</keyname><forenames>Kin Man</forenames></author><author><keyname>Chen</keyname><forenames>Pei Xian</forenames></author><author><keyname>Zhang</keyname><forenames>Yun Ling</forenames></author></authors><title>A data-driven method for syndrome type identification and classification
  in traditional Chinese medicine</title><categories>cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: The efficacy of traditional Chinese medicine (TCM) treatments for
Western medicine (WM) diseases relies heavily on the proper classification of
patients into TCM syndrome types. We develop a data-driven method for solving
the classification problem, where syndrome types are identified and quantified
based on patterns detected in unlabeled symptom survey data.
  Method: Latent class analysis (LCA) has been applied in WM research to solve
a similar problem, i.e., to identify subtypes of a patient population in the
absence of a gold standard. A widely known weakness of LCA is that it makes an
unrealistically strong independence assumption. We relax the assumption by
first detecting symptom co-occurrence patterns from survey data and use those
patterns instead of the symptoms as features for LCA. Results: The result of
the investigation is a six-step method: Data collection, symptom co-occurrence
pattern discovery, pattern interpretation, syndrome identification, syndrome
type identification, and syndrome type classification. A software package
called Lantern is developed to support the application of the method. The
method is illustrated using a data set on Vascular Mild Cognitive Impairment
(VMCI).
  Conclusions: A data-driven method for TCM syndrome identification and
classification is presented. The method can be used to answer the following
questions about a Western medicine disease: What TCM syndrome types are there
among the patients with the disease? What is the prevalence of each syndrome
type? What are the statistical characteristics of each syndrome type in terms
of occurrence of symptoms? How can we determine the syndrome type(s) of a
patient?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7143</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7143</id><created>2014-10-27</created><authors><author><keyname>Bao</keyname><forenames>Peng</forenames></author><author><keyname>Shen</keyname><forenames>Hua-Wei</forenames></author><author><keyname>Cheng</keyname><forenames>Xue-Qi</forenames></author></authors><title>Prediction of &quot;Forwarding Whom&quot; Behavior in Information Diffusion</title><categories>cs.SI physics.soc-ph</categories><comments>in Chinese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Follow-ship network among users underlies the diffusion dynamics of messages
on online social networks. Generally, the structure of underlying social
network determines the visibility of messages and the diffusion process. In
this paper, we study forwarding behavior of individuals, taking Sina Weibo as
an example. We investigate multiple exposures in information diffusion and the
&quot;forwarding whom&quot; problem associated with multiple exposures. Finally, we model
and predict the &quot;forwarding whom&quot; behavior of individuals, combining
structural, temporal, historical, and content features. Experimental results
demonstrate that our method achieves a high accuracy 91.3%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7164</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7164</id><created>2014-10-27</created><authors><author><keyname>Venkatesh</keyname><forenames>Manasij</forenames></author><author><keyname>Seelamantula</keyname><forenames>Chandra Sekhar</forenames></author></authors><title>Directional Bilateral Filters</title><categories>cs.CV</categories><doi>10.1109/ICASSP.2015.7178236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a bilateral filter with a locally controlled domain kernel for
directional edge-preserving smoothing. Traditional bilateral filters use a
range kernel, which is responsible for edge preservation, and a fixed domain
kernel that performs smoothing. Our intuition is that orientation and
anisotropy of image structures should be incorporated into the domain kernel
while smoothing. For this purpose, we employ an oriented Gaussian domain kernel
locally controlled by a structure tensor. The oriented domain kernel combined
with a range kernel forms the directional bilateral filter. The two kernels
assist each other in effectively suppressing the influence of the outliers
while smoothing. To find the optimal parameters of the directional bilateral
filter, we propose the use of Stein's unbiased risk estimate (SURE). We test
the capabilities of the kernels separately as well as together, first on
synthetic images, and then on real endoscopic images. The directional bilateral
filter has better denoising performance than the Gaussian bilateral filter at
various noise levels in terms of peak signal-to-noise ratio (PSNR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7171</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7171</id><created>2014-10-27</created><updated>2015-02-05</updated><authors><author><keyname>Eghbali</keyname><forenames>Reza</forenames></author><author><keyname>Swenson</keyname><forenames>Jon</forenames></author><author><keyname>Fazel</keyname><forenames>Maryam</forenames></author></authors><title>Exponentiated Subgradient Algorithm for Online Optimization under the
  Random Permutation Model</title><categories>math.OC cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online optimization problems arise in many resource allocation tasks, where
the future demands for each resource and the associated utility functions
change over time and are not known apriori, yet resources need to be allocated
at every point in time despite the future uncertainty. In this paper, we
consider online optimization problems with general concave utilities. We modify
and extend an online optimization algorithm proposed by Devanur et al. for
linear programming to this general setting. The model we use for the arrival of
the utilities and demands is known as the random permutation model, where a
fixed collection of utilities and demands are presented to the algorithm in
random order. We prove that under this model the algorithm achieves a
competitive ratio of $1-O(\epsilon)$ under a near-optimal assumption that the
bid to budget ratio is $O (\frac{\epsilon^2}{\log({m}/{\epsilon})})$, where $m$
is the number of resources, while enjoying a significantly lower computational
cost than the optimal algorithm proposed by Kesselheim et al. We draw a
connection between the proposed algorithm and subgradient methods used in
convex optimization. In addition, we present numerical experiments that
demonstrate the performance and speed of this algorithm in comparison to
existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7172</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7172</id><created>2014-10-27</created><updated>2015-03-04</updated><authors><author><keyname>Assael</keyname><forenames>John-Alexander M.</forenames></author><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author><author><keyname>Shahriari</keyname><forenames>Bobak</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Heteroscedastic Treed Bayesian Optimisation</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimising black-box functions is important in many disciplines, such as
tuning machine learning models, robotics, finance and mining exploration.
Bayesian optimisation is a state-of-the-art technique for the global
optimisation of black-box functions which are expensive to evaluate. At the
core of this approach is a Gaussian process prior that captures our belief
about the distribution over functions. However, in many cases a single Gaussian
process is not flexible enough to capture non-stationarity in the objective
function. Consequently, heteroscedasticity negatively affects performance of
traditional Bayesian methods. In this paper, we propose a novel prior model
with hierarchical parameter learning that tackles the problem of
non-stationarity in Bayesian optimisation. Our results demonstrate substantial
improvements in a wide range of applications, including automatic machine
learning and mining exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7174</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7174</id><created>2014-10-27</created><authors><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Knopp</keyname><forenames>Raymond</forenames></author></authors><title>The Approximate Optimality of Simple Schedules for Half-Duplex
  Multi-Relay Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Information Theory Workshop (ITW) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In ISIT'12 Brahma, \&quot;{O}zg\&quot;{u}r and Fragouli conjectured that in a
half-duplex diamond relay network (a Gaussian noise network without a direct
source-destination link and with $N$ non-interfering relays) an approximately
optimal relay scheduling (achieving the cut-set upper bound to within a
constant gap uniformly over all channel gains) exists with at most $N+1$ active
states (only $N+1$ out of the $2^N$ possible relay listen-transmit
configurations have a strictly positive probability). Such relay scheduling
policies are said to be simple. In ITW'13 we conjectured that simple relay
policies are optimal for any half-duplex Gaussian multi-relay network, that is,
simple schedules are not a consequence of the diamond network's sparse
topology. In this paper we formally prove the conjecture beyond Gaussian
networks. In particular, for any memoryless half-duplex $N$-relay network with
independent noises and for which independent inputs are approximately optimal
in the cut-set upper bound, an optimal schedule exists with at most $N+1$
active states. The key step of our proof is to write the minimum of a
submodular function by means of its Lov\'{a}sz extension and use the greedy
algorithm for submodular polyhedra to highlight structural properties of the
optimal solution. This, together with the saddle-point property of min-max
problems and the existence of optimal basic feasible solutions in linear
programs, proves the claim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7176</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7176</id><created>2014-10-27</created><updated>2015-06-09</updated><authors><author><keyname>Johansson</keyname><forenames>Fredrik</forenames></author></authors><title>Efficient implementation of elementary functions in the medium-precision
  range</title><categories>cs.MS cs.NA</categories><comments>Submitted to ARITH 22</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new implementation of the elementary transcendental functions
exp, sin, cos, log and atan for variable precision up to approximately 4096
bits. Compared to the MPFR library, we achieve a maximum speedup ranging from a
factor 3 for cos to 30 for atan. Our implementation uses table-based argument
reduction together with rectangular splitting to evaluate Taylor series. We
collect denominators to reduce the number of divisions in the Taylor series,
and avoid overhead by doing all multiprecision arithmetic using the mpn layer
of the GMP library. Our implementation provides rigorous error bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7182</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7182</id><created>2014-10-27</created><authors><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Maynard</keyname><forenames>Diana</forenames></author><author><keyname>Rizzo</keyname><forenames>Giuseppe</forenames></author><author><keyname>van Erp</keyname><forenames>Marieke</forenames></author><author><keyname>Gorrell</keyname><forenames>Genevieve</forenames></author><author><keyname>Troncy</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Petrak</keyname><forenames>Johann</forenames></author><author><keyname>Bontcheva</keyname><forenames>Kalina</forenames></author></authors><title>Analysis of Named Entity Recognition and Linking for Tweets</title><categories>cs.CL</categories><comments>35 pages, accepted to journal Information Processing and Management</comments><journal-ref>Information Processing &amp; Management 51 (2), 32-49, 2014</journal-ref><doi>10.1016/j.ipm.2014.10.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying natural language processing for mining and intelligent information
access to tweets (a form of microblog) is a challenging, emerging research
area. Unlike carefully authored news text and other longer content, tweets pose
a number of new challenges, due to their short, noisy, context-dependent, and
dynamic nature. Information extraction from tweets is typically performed in a
pipeline, comprising consecutive stages of language identification,
tokenisation, part-of-speech tagging, named entity recognition and entity
disambiguation (e.g. with respect to DBpedia). In this work, we describe a new
Twitter entity disambiguation dataset, and conduct an empirical analysis of
named entity recognition and disambiguation, investigating how robust a number
of state-of-the-art systems are on such noisy texts, what the main sources of
error are, and which problems should be further investigated to improve the
state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7184</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7184</id><created>2014-10-27</created><authors><author><keyname>Schmidt</keyname><forenames>Kai-Uwe</forenames></author></authors><title>Symmetric bilinear forms over finite fields with applications to coding
  theory</title><categories>math.CO cs.IT math.IT</categories><comments>33 pages</comments><msc-class>Primary: 15A63, 05E30, Secondary: 11T71, 94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $q$ be an odd prime power and let $X(m,q)$ be the set of symmetric
bilinear forms on an $m$-dimensional vector space over $\mathbb{F}_q$. The
partition of $X(m,q)$ induced by the action of the general linear group gives
rise to a commutative translation association scheme. We give explicit
expressions for the eigenvalues of this scheme in terms of linear combinations
of generalised Krawtchouk polynomials. We then study $d$-codes in this scheme,
namely subsets $Y$ of $X(m,q)$ with the property that, for all distinct $A,B\in
Y$, the rank of $A-B$ is at least $d$. We prove bounds on the size of a
$d$-code and show that, under certain conditions, the inner distribution of a
$d$-code is determined by its parameters. Constructions of $d$-codes are given,
which are optimal among the $d$-codes that are subgroups of $X(m,q)$. Finally,
with every subset $Y$ of $X(m,q)$, we associate two classical codes over
$\mathbb{F}_q$ and show that their Hamming distance enumerators can be
expressed in terms of the inner distribution of $Y$. As an example, we obtain
the distance enumerators of certain cyclic codes, for which many special cases
have been previously obtained using long ad hoc calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7189</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7189</id><created>2014-10-27</created><authors><author><keyname>Rassouli</keyname><forenames>Borzoo</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>On the Capacity of Vector Gaussian Channels With Bounded Inputs</title><categories>cs.IT math.IT</categories><comments>31 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of a deterministic multiple-input multiple-output (MIMO) channel
under the peak and average power constraints is investigated. For the identity
channel matrix, the approach of Shamai et al. is generalized to the higher
dimension settings to derive the necessary and sufficient conditions for the
optimal input probability density function. This approach prevents the usage of
the identity theorem of the holomorphic functions of several complex variables
which seems to fail in the multi-dimensional scenarios. It is proved that the
support of the capacity-achieving distribution is a finite set of hyper-spheres
with mutual independent phases and amplitude in the spherical domain.
Subsequently, it is shown that when the average power constraint is relaxed, if
the number of antennas is large enough, the capacity has a closed form solution
and constant amplitude signaling at the peak power achieves it. Moreover, it
will be observed that in a discrete-time memoryless Gaussian channel, the
average power constrained capacity, which results from a Gaussian input
distribution, can be closely obtained by an input where the support of its
magnitude is a discrete finite set. Finally, we investigate some upper and
lower bounds for the capacity of the non-identity channel matrix and evaluate
their performance as a function of the condition number of the channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7190</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7190</id><created>2014-10-27</created><updated>2015-02-04</updated><authors><author><keyname>Guang</keyname><forenames>Xuan</forenames></author><author><keyname>Lu</keyname><forenames>Jiyong</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>Repairable Threshold Secret Sharing Schemes</title><categories>cs.CR cs.IT math.IT</categories><comments>One column and 17 pages, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a class of threshold secret sharing schemes with
repairing function between shares without the help of the dealer, that we
called repairable threshold secret sharing schemes. Specifically, if a share
fails, such as broken or lost, it will be repaired just by some other shares. A
construction of such repairable threshold secret sharing schemes is designed by
applying linearized polynomials and regenerating codes in distributed storage
systems. In addition, a new repairing rate is introduced to characterize the
performance and efficiency of the repairing function. Then an achievable upper
bound on the repairing rate is derived, which implies the optimality of the
repair and describes the security between different shares. Under this
optimality of the repair, we further discuss traditional information rate and
also indicate its optimality, that can describe the efficiency of secret
sharing schemes in the aspect of storage. Finally, by applying the minimum
bandwidth regenerating (MBR) codes, our construction designs repairable
threshold secret sharing schemes achieving both optimal repairing and
information rates simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7207</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7207</id><created>2014-10-27</created><updated>2015-10-15</updated><authors><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>Generalized weights: an anticode approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study generalized weights as an algebraic invariant of a
code. We first describe anticodes in the Hamming and in the rank metric,
proving in particular that optimal anticodes in the rank metric coincide with
Frobenius-closed spaces. Then we characterize both generalized Hamming and rank
weights of a code in terms of the intersection of the code with optimal
anticodes in the respective metrics. Inspired by this description, we propose a
new algebraic invariant, which we call &quot;Delsarte generalized weights&quot;, for
Delsarte rank-metric codes based on optimal anticodes of matrices. We show that
our invariant refines the generalized rank weights for Gabidulin codes proposed
by Kurihara, Matsumoto and Uyematsu, and establish a series of properties of
Delsarte generalized weights. In particular, we characterize Delsarte optimal
codes and anticodes in terms of their generalized weights. We also present a
duality theory for the new algebraic invariant, proving that the Delsarte
generalized weights of a code completely determine the Delsarte generalized
weights of the dual code. Our results extend the theory of generalized rank
weights for Gabidulin codes. Finally, we prove the analogue for Gabidulin codes
of a theorem of Wei, proving that their generalized rank weights characterize
the worst-case security drops of a Gabidulin rank-metric code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7211</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7211</id><created>2014-10-27</created><authors><author><keyname>Castro</keyname><forenames>Daniel</forenames></author><author><keyname>F&#xe9;lix</keyname><forenames>Paulo</forenames></author><author><keyname>Presedo</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>A method for context-based adaptive QRS clustering in real-time</title><categories>cs.CV physics.med-ph</categories><comments>12 pages, 6 figures</comments><doi>10.1109/JBHI.2014.2361659</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous follow-up of heart condition through long-term electrocardiogram
monitoring is an invaluable tool for diagnosing some cardiac arrhythmias. In
such context, providing tools for fast locating alterations of normal
conduction patterns is mandatory and still remains an open issue. This work
presents a real-time method for adaptive clustering QRS complexes from
multilead ECG signals that provides the set of QRS morphologies that appear
during an ECG recording. The method processes the QRS complexes sequentially,
grouping them into a dynamic set of clusters based on the information content
of the temporal context. The clusters are represented by templates which evolve
over time and adapt to the QRS morphology changes. Rules to create, merge and
remove clusters are defined along with techniques for noise detection in order
to avoid their proliferation. To cope with beat misalignment, Derivative
Dynamic Time Warping is used. The proposed method has been validated against
the MIT-BIH Arrhythmia Database and the AHA ECG Database showing a global
purity of 98.56% and 99.56%, respectively. Results show that our proposal not
only provides better results than previous offline solutions but also fulfills
real-time requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7220</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7220</id><created>2014-10-27</created><updated>2015-05-08</updated><authors><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author><author><keyname>Kumar</keyname><forenames>Abhishek</forenames></author></authors><title>Exact and Heuristic Algorithms for Semi-Nonnegative Matrix Factorization</title><categories>math.NA cs.LG cs.NA math.OC stat.ML</categories><comments>22 pages, 6 figures. New: comparison with k-means initialization,
  numerical results on real data, ill-posedness of semi-NMF</comments><journal-ref>SIAM J. Matrix Anal. &amp; Appl. 36 (4), pp. 1404-1424, 2015</journal-ref><doi>10.1137/140993272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a matrix $M$ (not necessarily nonnegative) and a factorization rank
$r$, semi-nonnegative matrix factorization (semi-NMF) looks for a matrix $U$
with $r$ columns and a nonnegative matrix $V$ with $r$ rows such that $UV$ is
the best possible approximation of $M$ according to some metric. In this paper,
we study the properties of semi-NMF from which we develop exact and heuristic
algorithms. Our contribution is threefold. First, we prove that the error of a
semi-NMF of rank $r$ has to be smaller than the best unconstrained
approximation of rank $r-1$. This leads us to a new initialization procedure
based on the singular value decomposition (SVD) with a guarantee on the quality
of the approximation. Second, we propose an exact algorithm (that is, an
algorithm that finds an optimal solution), also based on the SVD, for a certain
class of matrices (including nonnegative irreducible matrices) from which we
derive an initialization for matrices not belonging to that class. Numerical
experiments illustrate that this second approach performs extremely well, and
allows us to compute optimal semi-NMF decompositions in many situations.
Finally, we analyze the computational complexity of semi-NMF proving its
NP-hardness, already in the rank-one case (that is, for $r = 1$), and we show
that semi-NMF is sometimes ill-posed (that is, an optimal solution does not
exist).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7223</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7223</id><created>2014-10-27</created><authors><author><keyname>Diaz-Hermida</keyname><forenames>Felix</forenames></author><author><keyname>Bugarin</keyname><forenames>Alberto</forenames></author><author><keyname>Losada</keyname><forenames>David E.</forenames></author></authors><title>The probatilistic Quantifier Fuzzification Mechanism FA: A theoretical
  analysis</title><categories>cs.AI</categories><comments>58 pages, 1 figure</comments><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this work is to analyze the behaviour of the FA quantifier
fuzzification mechanism. As we prove in the paper, this model has a very solid
theorethical behaviour, superior to most of the models defined in the
literature. Moreover, we show that the underlying probabilistic interpretation
has very interesting consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7225</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7225</id><created>2014-10-27</created><authors><author><keyname>Kaminski</keyname><forenames>Benjamin Lucien</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author></authors><title>Analyzing Expected Outcomes and Almost-Sure Termination of Probabilistic
  Programs is Hard</title><categories>cs.LO</categories><msc-class>68Q87</msc-class><acm-class>F.1.2; F.1.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the computational hardness of computing expected
outcomes and deciding almost-sure termination of probabilistic programs. We
show that deciding almost-sure termination and deciding whether the expected
outcome of a program equals a given rational value is $\Pi^0_2$-complete.
Computing lower and upper bounds on the expected outcome is shown to be
recursively enumerable and $\Sigma^0_2$-complete, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7237</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7237</id><created>2014-10-27</created><authors><author><keyname>Jacobs</keyname><forenames>Tobias</forenames></author><author><keyname>Longo</keyname><forenames>Salvatore</forenames></author></authors><title>A New Perspective on the Windows Scheduling Problem</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Windows Scheduling Problem, also known as the Pinwheel Problem, is to
schedule periodic jobs subject to their processing frequency demands. Instances
are given as a set of jobs that have to be processed infinitely often such that
the time interval between two consecutive executions of the same job j is no
longer than the job's given period $p_j$.
  The key contribution of this work is a new interpretation of the problem
variant with exact periods, where the time interval between consecutive
executions must be strictly $p_j$. We show that this version is equivalent to a
natural combinatorial problem we call Partial Coding. Reductions in both
directions can be realized in polynomial time, so that both hardness proofs and
algorithms for Partial Coding transfer to Windows Scheduling.
  Applying this new perspective, we obtain a number of new results regarding
the computational complexity of various Windows Scheduling Problem variants. We
prove that even the case of one processor and unit-length jobs does not admit a
pseudo-polynomial time algorithm unless SAT can be solved by a randomized
method in expected quasi-polynomial time. This result also extends to the case
of inexact periods, which answers a question that has remained open for more
than two decades. Furthermore, we report an error found in a hardness proof
previously given for the multi-machine case without machine migration, and we
show that this variant reduces to the single-machine case. Finally, we prove
that even with unit-length jobs the problem is co-NP-hard when jobs are allowed
to migrate between machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7249</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7249</id><created>2014-10-27</created><updated>2015-06-04</updated><authors><author><keyname>Guermouche</keyname><forenames>Abdou</forenames></author><author><keyname>Marchal</keyname><forenames>Loris</forenames></author><author><keyname>Simon</keyname><forenames>Bertrand</forenames></author><author><keyname>Vivien</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Scheduling Trees of Malleable Tasks for Sparse Linear Algebra</title><categories>cs.DC</categories><comments>Paper accepted for publication at EuroPar 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific workloads are often described as directed acyclic task graphs. In
this paper, we focus on the multifrontal factorization of sparse matrices,
whose task graph is structured as a tree of parallel tasks. Among the existing
models for parallel tasks, the concept of malleable tasks is especially
powerful as it allows each task to be processed on a time-varying number of
processors. Following the model advocated by Prasanna and Musicus for matrix
computations, we consider malleable tasks whose speedup is $p^\alpha$, where
$p$ is the fractional share of processors on which a task executes, and
$\alpha$ ($0 &lt; \alpha \leq 1$) is a parameter which does not depend on the
task. We first motivate the relevance of this model for our application with
actual experiments on multicore platforms. Then, we study the optimal
allocation proposed by Prasanna and Musicus for makespan minimization using
optimal control theory. We largely simplify their proofs by resorting only to
pure scheduling arguments. Building on the insight gained thanks to these new
proofs, we extend the study to distributed multicore platforms. There, a task
cannot be distributed among several distributed nodes. In such a distributed
setting (homogeneous or heterogeneous), we prove the NP-completeness of the
corresponding scheduling problem, and propose some approximation algorithms. We
finally assess the relevance of our approach by simulations on realistic trees.
We show that the average performance gain of our allocations with respect to
existing solutions (that are thus unaware of the actual speedup functions) is
up to 16% for $\alpha=0.9$ (the value observed in the real experiments).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7252</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7252</id><created>2014-10-27</created><authors><author><keyname>Sarin</keyname><forenames>Abhimanyu</forenames></author><author><keyname>Nayak</keyname><forenames>Dr. Jagadish</forenames></author></authors><title>Iris Biometric System using a hybrid approach</title><categories>cs.CV</categories><comments>11 pages, 11 pictures, Computer Science &amp; Information Technology-CSCP
  2014</comments><msc-class>47G20</msc-class><doi>10.5121/csit.2014.4914</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iris Recognition Systems are ocular- based biometric devices used primarily
for security reasons. The complexity and the randomness of the Iris, amongst
various other factors, ensure that this biometric system is inarguably an exact
and reliable method of identification. The algorithm is responsible for
automatic localization and segmentation of boundaries using circular Hough
Transform, noise reductions, image enhancement and feature extraction across
numerous distinct images present in the database. This paper delves into the
various kinds of techniques required to approximate the pupillary and limbic
boundaries of the enrolled iris image, captured using a suitable image
acquisition device and perform feature extraction on the normalized iris image
with the help of Haar Wavelets to encode the input data into a binary string
format. These techniques were validated using images from the CASIA database,
and various other procedures were also tried and tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7253</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7253</id><created>2014-10-27</created><authors><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author><author><keyname>Gabizon</keyname><forenames>Ariel</forenames></author><author><keyname>L&#xea;</keyname><forenames>Th&#xe1;i Ho&#xe0;ng</forenames></author><author><keyname>Zuckerman</keyname><forenames>David</forenames></author></authors><title>Deterministic Extractors for Additive Sources</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new model of a weakly random source that admits randomness
extraction. Our model of additive sources includes such natural sources as
uniform distributions on arithmetic progressions (APs), generalized arithmetic
progressions (GAPs), and Bohr sets, each of which generalizes affine sources.
We give an explicit extractor for additive sources with linear min-entropy over
both $\mathbb{Z}_p$ and $\mathbb{Z}_p^n$, for large prime $p$, although our
results over $\mathbb{Z}_p^n$ require that the source further satisfy a
list-decodability condition. As a corollary, we obtain explicit extractors for
APs, GAPs, and Bohr sources with linear min-entropy, although again our results
over $\mathbb{Z}_p^n$ require the list-decodability condition. We further
explore special cases of additive sources. We improve previous constructions of
line sources (affine sources of dimension 1), requiring a field of size linear
in $n$, rather than $\Omega(n^2)$ by Gabizon and Raz. This beats the
non-explicit bound of $\Theta(n \log n)$ obtained by the probabilistic method.
We then generalize this result to APs and GAPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7254</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7254</id><created>2014-10-27</created><authors><author><keyname>Gmeiner</keyname><forenames>B.</forenames></author><author><keyname>Gradl</keyname><forenames>T.</forenames></author><author><keyname>Gaspar</keyname><forenames>F.</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>U.</forenames></author></authors><title>Optimization of the Multigrid-Convergence Rate on Semi-structured Meshes
  by Local Fourier Analysis</title><categories>cs.NA math.NA</categories><journal-ref>Computers &amp; Mathematics with Applications, 65(4), 694-711 (2013)</journal-ref><doi>10.1016/j.camwa.2012.12.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a local Fourier analysis for multigrid methods on tetrahedral
grids is presented. Different smoothers for the discretization of the Laplace
operator by linear finite elements on such grids are analyzed. A four-color
smoother is presented as an efficient choice for regular tetrahedral grids,
whereas line and plane relaxations are needed for poorly shaped tetrahedra. A
novel partitioning of the Fourier space is proposed to analyze the four-color
smoother. Numerical test calculations validate the theoretical predictions. A
multigrid method is constructed in a block-wise form, by using different
smoothers and different numbers of pre- and post-smoothing steps in each
tetrahedron of the coarsest grid of the domain. Some numerical experiments are
presented to illustrate the efficiency of this multigrid algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7256</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7256</id><created>2014-10-27</created><updated>2015-07-27</updated><authors><author><keyname>Diamantopoulos</keyname><forenames>Panos</forenames></author><author><keyname>Maneas</keyname><forenames>Stathis</forenames></author><author><keyname>Patsonakis</keyname><forenames>Christos</forenames></author><author><keyname>Chondros</keyname><forenames>Nikos</forenames></author><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author></authors><title>Interactive Consistency in practical, mostly-asynchronous systems</title><categories>cs.DC</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive consistency is the problem in which n nodes, where up to t may be
byzantine, each with its own private value, run an algorithm that allows all
non-faulty nodes to infer the values of each other node. This problem is
relevant to critical applications that rely on the combination of the opinions
of multiple peers to provide a service. Examples include monitoring a content
source to prevent equivocation or to track variability in the content provided,
and resolving divergent state amongst the nodes of a distributed system.
Previous works assume a fully synchronous system, where one can make strong
assumptions such as negligible message delivery delays and/or detection of
absent messages. However, practical, real-world systems are mostly
asynchronous, i.e., they exhibit only some periods of synchrony during which
message delivery is timely, thus requiring a different approach. In this paper,
we present a thorough study on practical interactive consistency. We leverage
the vast prior work on broadcast and byzantine consensus algorithms to design,
implement and evaluate a set of algorithms, with varying timing assumptions and
message complexity, that can be used to achieve interactive consistency in
real-world distributed systems. We provide a complete, open-source
implementation of each proposed interactive consistency algorithm by building a
multi-layered stack of protocols that include several broadcast protocols, as
well as a binary and a multi-valued consensus protocol. Most of these protocols
have never been implemented and evaluated in a real system before. We analyze
the performance of our suite of algorithms experimentally by engaging in both
single instance and multiple parallel instances of each alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7259</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7259</id><created>2014-10-27</created><authors><author><keyname>Andjelkovi&#x107;</keyname><forenames>Miroslav</forenames></author><author><keyname>Tadi&#x107;</keyname><forenames>Bosiljka</forenames></author><author><keyname>Maleti&#x107;</keyname><forenames>Slobodan</forenames></author><author><keyname>Rajkovi&#x107;</keyname><forenames>Milan</forenames></author></authors><title>Hierarchical sequencing of online social graphs</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In online communications, patterns of conduct of individual actors and use of
emotions in the process can lead to a complex social graph exhibiting
multilayered structure and mesoscopic communities. Using simplicial complexes
representation of graphs, we investigate in-depth topology of online social
network which is based on MySpace dialogs. The network exhibits original
community structure. In addition, we simulate emotion spreading in this network
that enables to identify two emotion-propagating layers. The analysis resulting
in three structure vectors quantifies the graph's architecture at different
topology levels. Notably, structures emerging through shared links, triangles
and tetrahedral faces, frequently occur and range from tree-like to maximal
5-cliques and their respective complexes. On the other hand, the structures
which spread only negative or only positive emotion messages appear to have
much simpler topology consisting of links and triangles. Furthermore, we
introduce the node's structure vector which represents the number of simplices
at each topology level in which the node resides. The total number of such
simplices determines what we define as the node's topological dimension. The
presented results suggest that the node's topological dimension provides a
suitable measure of the social capital which measures the agent's ability to
act as a broker in compact communities, the so called Simmelian brokerage. We
also generalize the results to a wider class of computer-generated networks.
Investigating components of the node's vector over network layers reveals that
same nodes develop different socio-emotional relations and that the influential
nodes build social capital by combining their connections in different layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7263</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7263</id><created>2014-10-27</created><updated>2015-02-06</updated><authors><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author><author><keyname>Chen</keyname><forenames>Xujin</forenames></author><author><keyname>Hu</keyname><forenames>Xiaodong</forenames></author><author><keyname>Wang</keyname><forenames>Changjun</forenames></author></authors><title>Pricing in Social Networks with Negative Externalities</title><categories>cs.GT cs.DM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problems of pricing an indivisible product to consumers who are
embedded in a given social network. The goal is to maximize the revenue of the
seller. We assume impatient consumers who buy the product as soon as the seller
posts a price not greater than their values of the product. The product's value
for a consumer is determined by two factors: a fixed consumer-specified
intrinsic value and a variable externality that is exerted from the consumer's
neighbors in a linear way. We study the scenario of negative externalities,
which captures many interesting situations, but is much less understood in
comparison with its positive externality counterpart. We assume complete
information about the network, consumers' intrinsic values, and the negative
externalities. The maximum revenue is in general achieved by iterative pricing,
which offers impatient consumers a sequence of prices over time.
  We prove that it is NP-hard to find an optimal iterative pricing, even for
unweighted tree networks with uniform intrinsic values. Complementary to the
hardness result, we design a 2-approximation algorithm for finding iterative
pricing in general weighted networks with (possibly) nonuniform intrinsic
values. We show that, as an approximation to optimal iterative pricing, single
pricing can work rather well for many interesting cases, but theoretically it
can behave arbitrarily bad.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7265</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7265</id><created>2014-10-27</created><authors><author><keyname>Antal</keyname><forenames>Balint</forenames></author><author><keyname>Remenyik</keyname><forenames>Bence</forenames></author><author><keyname>Hajdu</keyname><forenames>Andras</forenames></author></authors><title>An Unsupervised Ensemble-based Markov Random Field Approach to
  Microscope Cell Image Segmentation</title><categories>cs.CV cs.AI q-bio.QM</categories><journal-ref>Proceeingds of the 10th International Conference on Signal
  Processing and Multimedia Applications (SIGMAP 2013), Reykjavik, Iceland,
  2013, pp. 94-99</journal-ref><doi>10.5220/0004612900940099</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an approach to the unsupervised segmentation of
images using Markov Random Field. The proposed approach is based on the idea of
Bit Plane Slicing. We use the planes as initial labellings for an ensemble of
segmentations. With pixelwise voting, a robust segmentation approach can be
achieved, which we demonstrate on microscope cell images. We tested our
approach on a publicly available database, where it proven to be competitive
with other methods and manual segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7270</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7270</id><created>2014-10-27</created><authors><author><keyname>Smiljkovikj</keyname><forenames>Katerina</forenames></author><author><keyname>Elshaer</keyname><forenames>Hisham</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Boccardi</keyname><forenames>Federico</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author><author><keyname>Gavrilovska</keyname><forenames>Liljana</forenames></author><author><keyname>Irmer</keyname><forenames>Ralf</forenames></author></authors><title>Capacity Analysis of Decoupled Downlink and Uplink Access in 5G
  Heterogeneous Systems</title><categories>cs.NI cs.IT math.IT</categories><comments>26 pages, 10 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our traditional notion of a cell is changing dramatically given the
increasing degree of heterogeneity in 4G and emerging 5G systems. Rather than
belonging to a specific cell, a device would choose the most suitable
connection from the plethora of connections available. In such a setting, given
the transmission powers differ significantly between downlink (DL) and uplink
(UL), a wireless device that sees multiple Base Stations (BSs) may access the
infrastructure in a way that it receives the downlink (DL) traffic from one BS
and sends uplink (UL) traffic through another BS. This situation is referred to
as Downlink and Uplink Decoupling (DUDe). In this paper, the capacity and
throughput gains brought by decoupling are rigorously derived using stochastic
geometry. Theoretical findings are then corroborated by means of simulation
results. A further constituent of this paper is the verification of the
theoretically derived results by means of a real-world system simulation
platform. Despite theoretical assumptions differing from the very complete
system simulator, the trends in the association probabilities and capacity
gains are similar. Based on the promising results, we then outline
architectural changes needed to facilitate the decoupling of DL and UL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7276</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7276</id><created>2014-10-27</created><updated>2014-11-03</updated><authors><author><keyname>Hu</keyname><forenames>Yang</forenames></author><author><keyname>Meng</keyname><forenames>Huadong</forenames></author><author><keyname>Liu</keyname><forenames>Yimin</forenames></author><author><keyname>Wang</keyname><forenames>Xiqin</forenames></author></authors><title>High Range Resolution Profiling in Missing Data Case: A New Approach</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have proposed a novel method for Synthetic High Range Resolution (HRR)
profiling, under the condition of missing frequency domain samples. This new
approach estimates the autocovariance function (ACF) of the signal by valid
sample pairs. Autocovariance matrix is formed from ACF estimations. Even with
large part of data missing, new approach exhibits robust profiling result.
Simulations are presented to show a advantage over other approaches in missing
data case. Moreover, a real radar experiment was conducted to validate the new
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7284</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7284</id><created>2014-10-27</created><updated>2015-08-07</updated><authors><author><keyname>Herzog</keyname><forenames>Alexander</forenames></author><author><keyname>Rotella</keyname><forenames>Nicholas</forenames></author><author><keyname>Mason</keyname><forenames>Sean</forenames></author><author><keyname>Grimminger</keyname><forenames>Felix</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author><author><keyname>Righetti</keyname><forenames>Ludovic</forenames></author></authors><title>Momentum Control with Hierarchical Inverse Dynamics on a
  Torque-Controlled Humanoid</title><categories>cs.RO</categories><comments>21 pages, 11 figures, 4 tables in Autonomous Robots (2015)</comments><doi>10.1007/s10514-015-9476-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical inverse dynamics based on cascades of quadratic programs have
been proposed for the control of legged robots. They have important benefits
but to the best of our knowledge have never been implemented on a torque
controlled humanoid where model inaccuracies, sensor noise and real-time
computation requirements can be problematic. Using a reformulation of existing
algorithms, we propose a simplification of the problem that allows to achieve
real-time control. Momentum-based control is integrated in the task hierarchy
and a LQR design approach is used to compute the desired associated closed-loop
behavior and improve performance. Extensive experiments on various balancing
and tracking tasks show very robust performance in the face of unknown
disturbances, even when the humanoid is standing on one foot. Our results
demonstrate that hierarchical inverse dynamics together with momentum control
can be efficiently used for feedback control under real robot conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7295</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7295</id><created>2014-10-27</created><updated>2014-10-29</updated><authors><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Chen</keyname><forenames>Jung-Chieh</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>On Sparse Vector Recovery Performance in Structurally Orthogonal
  Matrices via LASSO</title><categories>cs.IT math.IT</categories><comments>39 pages, 8 figures, 4 tables, minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a compressed sensing problem of reconstructing a
sparse signal from an undersampled set of noisy linear measurements. The
regularized least squares or least absolute shrinkage and selection operator
(LASSO) formulation is used for signal estimation. The measurement matrix is
assumed to be constructed by concatenating several randomly orthogonal bases,
referred to as structurally orthogonal matrices. Such measurement matrix is
highly relevant to large-scale compressive sensing applications because it
facilitates fast computation and also supports parallel processing. Using the
replica method from statistical physics, we derive the mean-squared-error (MSE)
formula of reconstruction over the structurally orthogonal matrix in the
large-system regime. Extensive numerical experiments are provided to verify the
analytical result. We then use the analytical result to study the MSE behaviors
of LASSO over the structurally orthogonal matrix, with a particular focus on
performance comparisons to matrices with independent and identically
distributed (i.i.d.) Gaussian entries. We demonstrate that the structurally
orthogonal matrices are at least as well performed as their i.i.d. Gaussian
counterparts, and therefore the use of structurally orthogonal matrices is
highly motivated in practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7326</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7326</id><created>2014-10-27</created><updated>2015-11-03</updated><authors><author><keyname>Risi</keyname><forenames>Sebastian</forenames></author><author><keyname>Togelius</keyname><forenames>Julian</forenames></author></authors><title>Neuroevolution in Games: State of the Art and Open Challenges</title><categories>cs.NE</categories><comments>- Added more references - Corrected typos - Added an overview table
  (Table 1)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper surveys research on applying neuroevolution (NE) to games. In
neuroevolution, artificial neural networks are trained through evolutionary
algorithms, taking inspiration from the way biological brains evolved. We
analyse the application of NE in games along five different axes, which are the
role NE is chosen to play in a game, the different types of neural networks
used, the way these networks are evolved, how the fitness is determined and
what type of input the network receives. The article also highlights important
open research challenges in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7328</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7328</id><created>2014-10-27</created><updated>2015-10-02</updated><authors><author><keyname>Vitanyi</keyname><forenames>P. M. B.</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Exact Expression For Information Distance</title><categories>cs.IT cs.CC cs.CV cs.DM math.IT</categories><comments>7 pages LaTeX. added materal and corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information distance can be defined not only between two strings but also in
a finite multiset of strings of cardinality greater than two. We give an
elementary proof for expressing the information distance. It is exact since for
each cardinality of the multiset the lower bound for some multiset equals the
upper bound for all multisets up to a constant additive term. We discuss
overlap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7330</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7330</id><created>2014-10-27</created><updated>2015-04-06</updated><authors><author><keyname>Hegarty</keyname><forenames>Peter</forenames></author><author><keyname>Martinsson</keyname><forenames>Anders</forenames></author><author><keyname>Wedin</keyname><forenames>Edvin</forenames></author></authors><title>The Hegselmann-Krause dynamics on the circle converge</title><categories>cs.SY math.CO</categories><comments>9 pages, 2 figures. Version 2: A small error in the proof of Theorem
  1.1 is corrected and an acknowledgement added. Bibliography updated</comments><msc-class>93A14, 39A60, 91D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Hegselmann-Krause dynamics on a one-dimensional torus and
provide the first proof of convergence of this system. The proof requires only
fairly minor modifications of existing methods for proving convergence in
Euclidean space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7340</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7340</id><created>2014-10-27</created><authors><author><keyname>Nagabhyrava</keyname><forenames>Divya Harika</forenames></author></authors><title>Efficient Key Generation for Dynamic Blom's Scheme</title><categories>cs.CR</categories><comments>17 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:1103.5712 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an efficient key management scheme by generating
new public and private keys in the Blom scheme. We also focus on making this
scheme dynamic by randomly changing the secret key that is generated by the
base station and we propose the use of the mesh array for matrix multiplication
for reducing the computation overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7346</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7346</id><created>2014-10-27</created><updated>2015-05-19</updated><authors><author><keyname>Liu</keyname><forenames>Bing</forenames></author><author><keyname>Kong</keyname><forenames>Soonho</forenames></author><author><keyname>Gao</keyname><forenames>Sicun</forenames></author><author><keyname>Zuliani</keyname><forenames>Paolo</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund M.</forenames></author></authors><title>Towards Personalized Prostate Cancer Therapy Using Delta-Reachability
  Analysis</title><categories>q-bio.QM cs.LO</categories><comments>HSCC 2015</comments><doi>10.1145/2728606.2728634</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent clinical studies suggest that the efficacy of hormone therapy for
prostate cancer depends on the characteristics of individual patients. In this
paper, we develop a computational framework for identifying patient-specific
androgen ablation therapy schedules for postponing the potential cancer
relapse. We model the population dynamics of heterogeneous prostate cancer
cells in response to androgen suppression as a nonlinear hybrid automaton. We
estimate personalized kinetic parameters to characterize patients and employ
$\delta$-reachability analysis to predict patient-specific therapeutic
strategies. The results show that our methods are promising and may lead to a
prognostic tool for personalized cancer therapy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7351</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7351</id><created>2014-10-27</created><authors><author><keyname>Yapar</keyname><forenames>&#xc7;a&#x11f;kan</forenames></author><author><keyname>Pohl</keyname><forenames>Volker</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>Fast Compressive Phase Retrieval from Fourier Measurements</title><categories>cs.IT math.IT</categories><comments>8 pages, 4 figures, submitted to ICASSP 2015 on Oct 6th, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of recovering a $k$-sparse, $N$-dimensional
complex signal from Fourier magnitude measurements. It proposes a Fourier
optics setup such that signal recovery up to a global phase factor is possible
with very high probability whenever $M \gtrsim 4k\log_2(N/k)$ random Fourier
intensity measurements are available. The proposed algorithm is comprised of
two stages: An algebraic phase retrieval stage and a compressive sensing step
subsequent to it. Simulation results are provided to demonstrate the
applicability of the algorithm for noiseless and noisy scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7353</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7353</id><created>2014-10-27</created><updated>2015-11-05</updated><authors><author><keyname>Mo</keyname><forenames>Jianhua</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Capacity Analysis of One-Bit Quantized MIMO Systems with Transmitter
  Channel State Information</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Signal Processing, Oct. 2015</journal-ref><doi>10.1109/TSP.2015.2455527</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With bandwidths on the order of a gigahertz in emerging wireless systems,
high-resolution analog-to-digital convertors (ADCs) become a power consumption
bottleneck. One solution is to employ low resolution one-bit ADCs. In this
paper, we analyze the flat fading multiple-input multiple-output (MIMO) channel
with one-bit ADCs. Channel state information is assumed to be known at both the
transmitter and receiver. For the multiple-input single-output channel, we
derive the exact channel capacity. For the single-input multiple-output and
MIMO channel, the capacity at infinite signal-to-noise ratio (SNR) is found. We
also derive upper bound at finite SNR, which is tight when the channel has full
row rank. In addition, we propose an efficient method to design the input
symbols to approach the capacity achieving solution. We incorporate millimeter
wave channel characteristics and find the bounds on the infinite SNR capacity.
The results show how the number of paths and number of receive antennas impact
the capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7357</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7357</id><created>2014-10-27</created><updated>2015-10-17</updated><authors><author><keyname>Karwa</keyname><forenames>Vishesh</forenames></author><author><keyname>Pelsmajer</keyname><forenames>Michael J.</forenames></author><author><keyname>Petrovi&#x107;</keyname><forenames>Sonja</forenames></author><author><keyname>Stasi</keyname><forenames>Despina</forenames></author><author><keyname>Wilburne</keyname><forenames>Dane</forenames></author></authors><title>Statistical models for cores decomposition of an undirected random graph</title><categories>math.ST cs.SI physics.soc-ph stat.CO stat.TH</categories><comments>24 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-core decomposition is a widely studied summary statistic that
describes a graph's global connectivity structure. In this paper, we move
beyond using $k$-core decomposition as a tool to summarize a graph and propose
using $k$-core decomposition as a tool to model random graphs. We propose using
the shell distribution vector, a way of summarizing the decomposition, as a
sufficient statistic for a family of exponential random graph models. We study
the properties and behavior of the model family, implement a Markov chain Monte
Carlo algorithm for simulating graphs from the model, implement a direct
sampler from the set of graphs with a given shell distribution, and explore the
sampling distributions of some of the commonly used complementary statistics as
good candidates for heuristic model fitting. These algorithms the provide first
fundamental steps necessary for solving the following problems: parameter
estimation in this ERGM, extending the model to its Bayesian relative, and
developing a rigorous methodology for testing goodness of fit of the model and
model selection. The methods are applied to a synthetic network as well as the
well-known Sampson monks dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7367</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7367</id><created>2014-10-27</created><authors><author><keyname>Basha</keyname><forenames>Elizabeth</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>In-Network Distributed Solar Current Prediction</title><categories>cs.DC cs.NI</categories><comments>28 pages, accepted at TOSN and awaiting publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long-term sensor network deployments demand careful power management. While
managing power requires understanding the amount of energy harvestable from the
local environment, current solar prediction methods rely only on recent local
history, which makes them susceptible to high variability. In this paper, we
present a model and algorithms for distributed solar current prediction, based
on multiple linear regression to predict future solar current based on local,
in-situ climatic and solar measurements. These algorithms leverage spatial
information from neighbors and adapt to the changing local conditions not
captured by global climatic information. We implement these algorithms on our
Fleck platform and run a 7-week-long experiment validating our work. In
analyzing our results from this experiment, we determined that computing our
model requires an increased energy expenditure of 4.5mJ over simpler models (on
the order of 10^{-7}% of the harvested energy) to gain a prediction improvement
of 39.7%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7372</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7372</id><created>2014-10-27</created><authors><author><keyname>Jayadeva</keyname></author><author><keyname>Batra</keyname><forenames>Sanjit S.</forenames></author><author><keyname>Sabharwal</keyname><forenames>Siddharth</forenames></author></authors><title>Feature Selection through Minimization of the VC dimension</title><categories>cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1410.4573</comments><msc-class>68T05, 68T10, 68Q32</msc-class><acm-class>I.5.1; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection involes identifying the most relevant subset of input
features, with a view to improving generalization of predictive models by
reducing overfitting. Directly searching for the most relevant combination of
attributes is NP-hard. Variable selection is of critical importance in many
applications, such as micro-array data analysis, where selecting a small number
of discriminative features is crucial to developing useful models of disease
mechanisms, as well as for prioritizing targets for drug discovery. The
recently proposed Minimal Complexity Machine (MCM) provides a way to learn a
hyperplane classifier by minimizing an exact (\boldmath{$\Theta$}) bound on its
VC dimension. It is well known that a lower VC dimension contributes to good
generalization. For a linear hyperplane classifier in the input space, the VC
dimension is upper bounded by the number of features; hence, a linear
classifier with a small VC dimension is parsimonious in the set of features it
employs. In this paper, we use the linear MCM to learn a classifier in which a
large number of weights are zero; features with non-zero weights are the ones
that are chosen. Selected features are used to learn a kernel SVM classifier.
On a number of benchmark datasets, the features chosen by the linear MCM yield
comparable or better test set accuracy than when methods such as ReliefF and
FCBF are used for the task. The linear MCM typically chooses one-tenth the
number of attributes chosen by the other methods; on some very high dimensional
datasets, the MCM chooses about $0.6\%$ of the features; in comparison, ReliefF
and FCBF choose 70 to 140 times more features, thus demonstrating that
minimizing the VC dimension may provide a new, and very effective route for
feature selection and for learning sparse representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7376</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7376</id><created>2014-10-27</created><updated>2015-03-16</updated><authors><author><keyname>Rhinehart</keyname><forenames>Nicholas</forenames></author><author><keyname>Zhou</keyname><forenames>Jiaji</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author></authors><title>Visual Chunking: A List Prediction Framework for Region-Based Object
  Detection</title><categories>cs.CV</categories><comments>to appear at ICRA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider detecting objects in an image by iteratively selecting from a set
of arbitrarily shaped candidate regions. Our generic approach, which we term
visual chunking, reasons about the locations of multiple object instances in an
image while expressively describing object boundaries. We design an
optimization criterion for measuring the performance of a list of such
detections as a natural extension to a common per-instance metric. We present
an efficient algorithm with provable performance for building a high-quality
list of detections from any candidate set of region-based proposals. We also
develop a simple class-specific algorithm to generate a candidate region
instance in near-linear time in the number of low-level superpixels that
outperforms other region generating methods. In order to make predictions on
novel images at testing time without access to ground truth, we develop
learning approaches to emulate these algorithms' behaviors. We demonstrate that
our new approach outperforms sophisticated baselines on benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7382</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7382</id><created>2014-10-25</created><authors><author><keyname>Bhuvanagiri</keyname><forenames>Kiran Kumar</forenames></author><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>Modified Mel Filter Bank to Compute MFCC of Subsampled Speech</title><categories>cs.CL cs.SD</categories><comments>arXiv admin note: substantial text overlap with arXiv:1410.6903</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used
speech features in most speech and speaker recognition applications. In this
work, we propose a modified Mel filter bank to extract MFCCs from subsampled
speech. We also propose a stronger metric which effectively captures the
correlation between MFCCs of original speech and MFCC of resampled speech. It
is found that the proposed method of filter bank construction performs
distinguishably well and gives recognition performance on resampled speech
close to recognition accuracies on original speech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7404</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7404</id><created>2014-10-27</created><updated>2015-01-30</updated><authors><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Maximally Informative Hierarchical Representations of High-Dimensional
  Data</title><categories>stat.ML cs.LG physics.data-an</categories><comments>13 pages, 8 figures. Appearing in Proceedings of the 18th
  International Conference on Artificial Intelligence and Statistics (AISTATS)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a set of probabilistic functions of some input variables as a
representation of the inputs. We present bounds on how informative a
representation is about input data. We extend these bounds to hierarchical
representations so that we can quantify the contribution of each layer towards
capturing the information in the original data. The special form of these
bounds leads to a simple, bottom-up optimization procedure to construct
hierarchical representations that are also maximally informative about the
data. This optimization has linear computational complexity and constant sample
complexity in the number of variables. These results establish a new approach
to unsupervised learning of deep representations that is both principled and
practical. We demonstrate the usefulness of the approach on both synthetic and
real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7414</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7414</id><created>2014-10-27</created><authors><author><keyname>Oliva</keyname><forenames>Junier</forenames></author><author><keyname>Neiswanger</keyname><forenames>Willie</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author></authors><title>Fast Function to Function Regression</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the problem of regression when both input covariates and output
responses are functions from a nonparametric function class. Function to
function regression (FFR) covers a large range of interesting applications
including time-series prediction problems, and also more general tasks like
studying a mapping between two separate types of distributions. However,
previous nonparametric estimators for FFR type problems scale badly
computationally with the number of input/output pairs in a data-set. Given the
complexity of a mapping between general functions it may be necessary to
consider large data-sets in order to achieve a low estimation risk. To address
this issue, we develop a novel scalable nonparametric estimator, the
Triple-Basis Estimator (3BE), which is capable of operating over datasets with
many instances. To the best of our knowledge, the 3BE is the first
nonparametric FFR estimator that can scale to massive datasets. We analyze the
3BE's risk and derive an upperbound rate. Furthermore, we show an improvement
of several orders of magnitude in terms of prediction speed and a reduction in
error over previous estimators in various real-world data-sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7429</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7429</id><created>2014-10-27</created><updated>2015-10-24</updated><authors><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author></authors><title>Higher-order MRFs based image super resolution: why not MAP?</title><categories>cs.CV</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A trainable filter-based higher-order Markov Random Fields (MRFs) model - the
so called Fields of Experts (FoE), has proved a highly effective image prior
model for many classic image restoration problems. Generally, two options are
available to incorporate the learned FoE prior in the inference procedure: (1)
sampling-based minimum mean square error (MMSE) estimate, and (2) energy
minimization-based maximum a posteriori (MAP) estimate. This letter is devoted
to the FoE prior based single image super resolution (SR) problem, and we
suggest to make use of the MAP estimate for inference based on two facts: (I)
It is well-known that the MAP inference has a remarkable advantage of high
computational efficiency, while the sampling-based MMSE estimate is very time
consuming. (II) Practical SR experiment results demonstrate that the MAP
estimate works equally well compared to the MMSE estimate with exactly the same
FoE prior model. Moreover, it can lead to even further improvements by
incorporating our discriminatively trained FoE prior model. In summary, we hold
that for higher-order natural image prior based SR problem, it is better to
employ the MAP estimate for inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7452</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7452</id><created>2014-10-27</created><updated>2015-01-26</updated><authors><author><keyname>Jampani</keyname><forenames>Varun</forenames></author><author><keyname>Eslami</keyname><forenames>S. M. Ali</forenames></author><author><keyname>Tarlow</keyname><forenames>Daniel</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>Winn</keyname><forenames>John</forenames></author></authors><title>Consensus Message Passing for Layered Graphical Models</title><categories>cs.CV cs.AI cs.LG</categories><comments>Appearing in Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics (AISTATS) 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Generative models provide a powerful framework for probabilistic reasoning.
However, in many domains their use has been hampered by the practical
difficulties of inference. This is particularly the case in computer vision,
where models of the imaging process tend to be large, loopy and layered. For
this reason bottom-up conditional models have traditionally dominated in such
domains. We find that widely-used, general-purpose message passing inference
algorithms such as Expectation Propagation (EP) and Variational Message Passing
(VMP) fail on the simplest of vision models. With these models in mind, we
introduce a modification to message passing that learns to exploit their
layered structure by passing 'consensus' messages that guide inference towards
good solutions. Experiments on a variety of problems show that the proposed
technique leads to significantly more accurate inference results, not only when
compared to standard EP and VMP, but also when compared to competitive
bottom-up conditional models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7454</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7454</id><created>2014-10-27</created><updated>2014-12-04</updated><authors><author><keyname>Dhungel</keyname><forenames>Neeraj</forenames></author><author><keyname>Carneiro</keyname><forenames>Gustavo</forenames></author><author><keyname>Bradley</keyname><forenames>Andrew P.</forenames></author></authors><title>Deep Structured learning for mass segmentation from Mammograms</title><categories>cs.CV</categories><comments>4 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present a novel method for the segmentation of breast
masses from mammograms exploring structured and deep learning. Specifically,
using structured support vector machine (SSVM), we formulate a model that
combines different types of potential functions, including one that classifies
image regions using deep learning. Our main goal with this work is to show the
accuracy and efficiency improvements that these relatively new techniques can
provide for the segmentation of breast masses from mammograms. We also propose
an easily reproducible quantitative analysis to as- sess the performance of
breast mass segmentation methodologies based on widely accepted accuracy and
running time measurements on public datasets, which will facilitate further
comparisons for this segmentation problem. In particular, we use two publicly
available datasets (DDSM-BCRP and INbreast) and propose the computa- tion of
the running time taken for the methodology to produce a mass segmentation given
an input image and the use of the Dice index to quantitatively measure the
segmentation accuracy. For both databases, we show that our proposed
methodology produces competitive results in terms of accuracy and running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7455</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7455</id><created>2014-10-27</created><updated>2015-06-22</updated><authors><author><keyname>Povey</keyname><forenames>Daniel</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaohui</forenames></author><author><keyname>Khudanpur</keyname><forenames>Sanjeev</forenames></author></authors><title>Parallel training of DNNs with Natural Gradient and Parameter Averaging</title><categories>cs.NE cs.LG stat.ML</categories><comments>Accepted as workshop contribution to ICLR 2015. 12 pages plus 16
  pages of appendices, International Conference on Learning Representations
  (ICLR): Workshop track, 2015. [2 sets of minor fixes post-publication.]</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We describe the neural-network training framework used in the Kaldi speech
recognition toolkit, which is geared towards training DNNs with large amounts
of training data using multiple GPU-equipped or multi-core machines. In order
to be as hardware-agnostic as possible, we needed a way to use multiple
machines without generating excessive network traffic. Our method is to average
the neural network parameters periodically (typically every minute or two), and
redistribute the averaged parameters to the machines for further training. Each
machine sees different data. By itself, this method does not work very well.
However, we have another method, an approximate and efficient implementation of
Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow
our periodic-averaging method to work well, as well as substantially improving
the convergence of SGD on a single machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7460</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7460</id><created>2014-10-27</created><updated>2015-12-29</updated><authors><author><keyname>Ewaisha</keyname><forenames>Ahmed</forenames></author><author><keyname>Tepedelenlio&#x11f;lu</keyname><forenames>Cihan</forenames></author></authors><title>Throughput Optimization in Multi-Channel Cognitive Radios with Hard
  Deadline Constraints</title><categories>cs.IT math.IT</categories><comments>Keywords: Delay Constraint, Optimal Stopping Rule, Water Filling,
  Stochastic Optimization, Optimal Channel Selection</comments><doi>10.1109/TVT.2015.2425951</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a cognitive radio scenario we consider a single secondary user (SU)
accessing a multi-channel system. The SU senses the channels sequentially to
detect if a primary user (PU) is occupying the channels, and stops its search
to access a channel if it offers a significantly high throughput. The optimal
stopping rule and power control problem is considered. The problem is
formulated as a SU's throughput-maximization problem under a power,
interference and packet delay constraints. We first show the effect of the
optimal stopping rule on the packet delay, then solve this optimization problem
for both the overlay system where the SU transmits only at the spectrum holes
as well as the underlay system where tolerable interference (or tolerable
collision probability) is allowed. We provide closed-form expressions for the
optimal stopping rule, and show that the optimal power control strategy for
this multi-channel problem is a modified water-filling approach. We extend the
work to multiple SU scenario and show that when the number of SUs is large the
complexity of the solution becomes smaller than that of the single SU case. We
discuss the application of this problem in typical networks where packets
arrive simultaneously and have the same departure deadline. We further propose
an online adaptation policy to the optimal stopping rule that meets the
packets' hard-deadline constraint and, at the same time, gives higher
throughput than the offline policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7466</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7466</id><created>2014-10-27</created><authors><author><keyname>Normann</keyname><forenames>H&#xe5;kon</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Prisacariu</keyname><forenames>Cristian</forenames><affiliation>Institute for Informatics, University of Oslo</affiliation></author><author><keyname>Hildebrandt</keyname><forenames>Thomas</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>Concurrency Models with Causality and Events as Psi-calculi</title><categories>cs.LO cs.DC</categories><comments>In Proceedings ICE 2014, arXiv:1410.7013</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 166, 2014, pp. 4-20</journal-ref><doi>10.4204/EPTCS.166.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Psi-calculi are a parametric framework for nominal calculi, where standard
calculi are found as instances, like the pi-calculus, or the cryptographic
spi-calculus and applied-pi. Psi-calculi have an interleaving operational
semantics, with a strong foundation on the theory of nominal sets and process
algebras. Much of the expressive power of psi-calculi comes from their logical
part, i.e., assertions, conditions, and entailment, which are left quite open
thus accommodating a wide range of logics. We are interested in how this
expressiveness can deal with event-based models of concurrency. We thus take
the popular prime event structures model and give an encoding into an instance
of psi-calculi. We also take the recent and expressive model of Dynamic
Condition Response Graphs (in which event structures are strictly included) and
give an encoding into another corresponding instance of psi-calculi. The
encodings that we achieve look rather natural and intuitive. Additional results
about these encodings give us more confidence in their correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7467</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7467</id><created>2014-10-27</created><authors><author><keyname>Jongmans</keyname><forenames>Sung-Shik T. Q.</forenames><affiliation>CWI</affiliation></author><author><keyname>Arbab</keyname><forenames>Farhad</forenames><affiliation>CWI</affiliation></author></authors><title>Toward Sequentializing Overparallelized Protocol Code</title><categories>cs.PL</categories><comments>In Proceedings ICE 2014, arXiv:1410.7013</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 166, 2014, pp. 38-44</journal-ref><doi>10.4204/EPTCS.166.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our ongoing work, we use constraint automata to compile protocol
specifications expressed as Reo connectors into efficient executable code,
e.g., in C. We have by now studied this automata based compilation approach
rather well, and have devised effective solutions to some of its problems.
Because our approach is based on constraint automata, the approach, its
problems, and our solutions are in fact useful and relevant well beyond the
specific case of compiling Reo. In this short paper, we identify and analyze
two such rather unexpected problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7469</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7469</id><created>2014-10-27</created><authors><author><keyname>Latella</keyname><forenames>Diego</forenames><affiliation>ISTI - CNR</affiliation></author><author><keyname>Loreti</keyname><forenames>Michele</forenames><affiliation>Universit&#xe0; di Firenze</affiliation></author><author><keyname>Massink</keyname><forenames>Mieke</forenames><affiliation>ISTI - CNR</affiliation></author></authors><title>On-the-fly Probabilistic Model Checking</title><categories>cs.LO</categories><comments>In Proceedings ICE 2014, arXiv:1410.7013</comments><proxy>EPTCS</proxy><acm-class>F.3.1; D.2.4</acm-class><journal-ref>EPTCS 166, 2014, pp. 45-59</journal-ref><doi>10.4204/EPTCS.166.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking approaches can be divided into two broad categories: global
approaches that determine the set of all states in a model M that satisfy a
temporal logic formula f, and local approaches in which, given a state s in M,
the procedure determines whether s satisfies f. When s is a term of a process
language, the model checking procedure can be executed &quot;on-the-fly&quot;, driven by
the syntactical structure of s. For certain classes of systems, e.g. those
composed of many parallel components, the local approach is preferable because,
depending on the specific property, it may be sufficient to generate and
inspect only a relatively small part of the state space. We propose an
efficient, on-the-fly, PCTL model checking procedure that is parametric with
respect to the semantic interpretation of the language. The procedure comprises
both bounded and unbounded until modalities. The correctness of the procedure
is shown and its efficiency is compared with a global PCTL model checker on
representative applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7470</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7470</id><created>2014-10-27</created><authors><author><keyname>Ninin</keyname><forenames>Nicolas</forenames><affiliation>CEA, LIST and University Paris-Sud, France</affiliation></author><author><keyname>Haucourt</keyname><forenames>Emmanuel</forenames><affiliation>CEA, LIST</affiliation></author></authors><title>The Boolean Algebra of Cubical Areas as a Tensor Product in the Category
  of Semilattices with Zero</title><categories>cs.LO cs.DC</categories><comments>In Proceedings ICE 2014, arXiv:1410.7013</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 166, 2014, pp. 60-66</journal-ref><doi>10.4204/EPTCS.166.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a model of concurrency together with an algebraic
structure reflecting the parallel composition. For the sake of simplicity we
restrict to linear concurrent programs i.e. the ones with no loops nor
branching. Such programs are given a semantics using cubical areas. Such a
semantics is said to be geometric. The collection of all these cubical areas
enjoys a structure of tensor product in the category of semi-lattice with zero.
These results naturally extend to fully fledged concurrent programs up to some
technical tricks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7471</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7471</id><created>2014-10-27</created><authors><author><keyname>Basile</keyname><forenames>Davide</forenames><affiliation>Dipartimento di Informatica, Universita' di Pisa, Italy</affiliation></author><author><keyname>Degano</keyname><forenames>Pierpaolo</forenames><affiliation>Dipartimento di Informatica, Universita' di Pisa, Italy</affiliation></author><author><keyname>Ferrari</keyname><forenames>Gian-Luigi</forenames><affiliation>Dipartimento di Informatica, Universita' di Pisa, Italy</affiliation></author><author><keyname>Tuosto</keyname><forenames>Emilio</forenames><affiliation>Computer Science Department, University of Leicester</affiliation></author></authors><title>From Orchestration to Choreography through Contract Automata</title><categories>cs.FL cs.LO</categories><comments>In Proceedings ICE 2014, arXiv:1410.7013</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 166, 2014, pp. 67-85</journal-ref><doi>10.4204/EPTCS.166.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relations between a contract automata and an interaction model.
In the former model, distributed services are abstracted away as automata -
oblivious of their partners - that coordinate with each other through an
orchestrator. The interaction model relies on channel-based asynchronous
communication and choreography to coordinate distributed services.
  We define a notion of strong agreement on the contract model, exhibit a
natural mapping from the contract model to the interaction model, and give
conditions to ensure that strong agreement corresponds to well-formed
choreography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7472</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7472</id><created>2014-10-27</created><authors><author><keyname>Bartoletti</keyname><forenames>Massimo</forenames><affiliation>University of Cagliari</affiliation></author><author><keyname>Cimoli</keyname><forenames>Tiziana</forenames><affiliation>University of Cagliari</affiliation></author><author><keyname>Pinna</keyname><forenames>G. Michele</forenames><affiliation>University of Cagliari</affiliation></author></authors><title>A note on two notions of compliance</title><categories>cs.PL cs.GT</categories><comments>In Proceedings ICE 2014, arXiv:1410.7013</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 166, 2014, pp. 86-93</journal-ref><doi>10.4204/EPTCS.166.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a relation between two models of contracts: binary session
types, and a model based on event structures and game-theoretic notions. In
particular, we show that compliance in session types corresponds to the
existence of certain winning strategies in game-based contracts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7477</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7477</id><created>2014-10-27</created><authors><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Tepekule</keyname><forenames>Burcu</forenames></author><author><keyname>Pusane</keyname><forenames>Ali E.</forenames></author></authors><title>Arrival Modeling and Error Analysis for Molecular Communication via
  Diffusion with Drift</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The arrival of molecules in molecular communication via diffusion (MCvD) is a
counting process, exhibiting by its nature binomial distribution. Even if the
binomial process describes well the arrival of molecules, when considering
consecutively sent symbols, the process struggles to work with the binomial
cumulative distribution function (CDF). Therefore, in the literature, Poisson
and Gaussian approximations of the binomial distribution are used. In this
paper, we analyze these two approximations of the binomial model of the arrival
process in MCvD with drift. Considering the distance, drift velocity, and the
number of emitted molecules, we investigate the regions in which either Poisson
or Gaussian model is better in terms of root mean squared error (RMSE) of the
CDFs; we confirm the boundaries of the region via numerical simulations.
Moreover, we derive the error probabilities for continuous communication and
analyze which model approximates it more accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7484</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7484</id><created>2014-10-27</created><updated>2015-01-21</updated><authors><author><keyname>Zhou</keyname><forenames>Tianfei</forenames></author><author><keyname>Lu</keyname><forenames>Yao</forenames></author><author><keyname>Lv</keyname><forenames>Feng</forenames></author><author><keyname>Di</keyname><forenames>Huijun</forenames></author><author><keyname>Zhao</keyname><forenames>Qingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Jian</forenames></author></authors><title>Abrupt Motion Tracking via Nearest Neighbor Field Driven Stochastic
  Sampling</title><categories>cs.CV</categories><comments>submitted to Elsevier Neurocomputing</comments><doi>10.1016/j.neucom.2015.03.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic sampling based trackers have shown good performance for abrupt
motion tracking so that they have gained popularity in recent years. However,
conventional methods tend to use a two-stage sampling paradigm, in which the
search space needs to be uniformly explored with an inefficient preliminary
sampling phase. In this paper, we propose a novel sampling-based method in the
Bayesian filtering framework to address the problem. Within the framework,
nearest neighbor field estimation is utilized to compute the importance
proposal probabilities, which guide the Markov chain search towards promising
regions and thus enhance the sampling efficiency; given the motion priors, a
smoothing stochastic sampling Monte Carlo algorithm is proposed to approximate
the posterior distribution through a smoothing weight-updating scheme.
Moreover, to track the abrupt and the smooth motions simultaneously, we develop
an abrupt-motion detection scheme which can discover the presence of abrupt
motions during online tracking. Extensive experiments on challenging image
sequences demonstrate the effectiveness and the robustness of our algorithm in
handling the abrupt motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7496</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7496</id><created>2014-10-27</created><updated>2015-02-02</updated><authors><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author><author><keyname>Duan</keyname><forenames>Zhisheng</forenames></author></authors><title>Distributed Adaptive Consensus Protocols for Linear Multi-agent Systems
  with Directed Graphs and External Disturbances</title><categories>cs.SY</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the distributed consensus design problem for linear
multi-agent systems with directed communication graphs and external
disturbances. Both the cases with strongly connected communication graphs and
leader-follower graphs containing a directed spanning tree with the leader as
the root are discussed. Distributed adaptive consensus protocols based on the
relative states of neighboring agents are designed, which can ensure the
ultimate boundedness of the consensus error and adaptive gains in the presence
of external disturbances. The upper bounds of the consensus error are further
explicitly given. Compared to the existing consensus protocols, the merit of
the adaptive protocols proposed in this paper is that they can be computed and
implemented in a fully distributed fashion and meanwhile are robust with
respect to external disturbances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7502</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7502</id><created>2014-10-27</created><authors><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Baccelli</keyname><forenames>Francois</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Spectral Efficiency Scaling Laws in Dense Random Wireless Networks with
  Multiple Receive Antennas</title><categories>cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers large random wireless networks where
transmit-and-receive node pairs communicate within a certain range while
sharing a common spectrum. By modeling the spatial locations of nodes based on
stochastic geometry, analytical expressions for the ergodic spectral efficiency
of a typical node pair are derived as a function of the channel state
information available at a receiver (CSIR) in terms of relevant system
parameters: the density of communication links, the number of receive antennas,
the path loss exponent, and the operating signal-to-noise ratio. One key
finding is that when the receiver only exploits CSIR for the direct link, the
sum of spectral efficiencies linearly improves as the density increases, when
the number of receive antennas increases as a certain super-linear function of
the density. When each receiver exploits CSIR for a set of dominant interfering
links in addition to the direct link, the sum of spectral efficiencies linearly
increases with both the density and the path loss exponent if the number of
antennas is a linear function of the density. This observation demonstrates
that having CSIR for dominant interfering links provides a multiplicative gain
in the scaling law. It is also shown that this linear scaling holds for direct
CSIR when incorporating the effect of the receive antenna correlation, provided
that the rank of the spatial correlation matrix scales super-linearly with the
density. Simulation results back scaling laws derived from stochastic geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7506</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7506</id><created>2014-10-27</created><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Li</keyname><forenames>Shi</forenames></author></authors><title>On $(1,\epsilon)$-Restricted Assignment Makespan Minimization</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Makespan minimization on unrelated machines is a classic problem in
approximation algorithms. No polynomial time $(2-\delta)$-approximation
algorithm is known for the problem for constant $\delta&gt; 0$. This is true even
for certain special cases, most notably the restricted assignment problem where
each job has the same load on any machine but can be assigned to one from a
specified subset. Recently in a breakthrough result, Svensson [Svensson, 2011]
proved that the integrality gap of a certain configuration LP relaxation is
upper bounded by $1.95$ for the restricted assignment problem; however, the
rounding algorithm is not known to run in polynomial time.
  In this paper we consider the $(1,\varepsilon)$-restricted assignment problem
where each job is either heavy ($p_j = 1$) or light ($p_j = \varepsilon$), for
some parameter $\varepsilon &gt; 0$. Our main result is a $(2-\delta)$-approximate
polynomial time algorithm for the $(1,\epsilon)$-restricted assignment problem
for a fixed constant $\delta&gt; 0$. Even for this special case, the best
polynomial-time approximation factor known so far is 2. We obtain this result
by rounding the configuration LP relaxation for this problem. A simple
reduction from vertex cover shows that this special case remains NP-hard to
approximate to within a factor better than 7/6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7528</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7528</id><created>2014-10-28</created><authors><author><keyname>Kumar</keyname><forenames>Abhinav</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author><author><keyname>Pillai</keyname><forenames>Sibi Raj B</forenames></author><author><keyname>Gopalan</keyname><forenames>Aditya</forenames></author></authors><title>Optimal WiFi Sensing via Dynamic Programming</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding an optimal sensing schedule for a mobile device that
encounters an intermittent WiFi access opportunity is considered. At any given
time, the WiFi is in any of the two modes, ON or OFF, and the mobile's
incentive is to connect to the WiFi in the ON mode as soon as possible, while
spending as little sensing energy. We introduce a dynamic programming framework
which enables the characterization of an explicit solution for several models,
particularly when the OFF periods are exponentially distributed. While the
problem for non-exponential OFF periods is ill-posed in general, a usual
workaround in literature is to make the mobile device aware if one ON period is
completely missed. In this restricted setting, using the DP framework, the
deterministic nature of the optimal sensing policy is established, and value
iterations are shown to converge to the optimal solution. Finally, we address
the blind situation where the distributions of ON and OFF periods are unknown.
A continuous bandit based learning algorithm that has vanishing regret (loss
compared to the optimal strategy with the knowledge of distributions) is
presented, and comparisons with the optimal schemes are provided for
exponential ON and OFF times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7530</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7530</id><created>2014-10-28</created><authors><author><keyname>Friedmann</keyname><forenames>Oliver</forenames></author><author><keyname>Hansen</keyname><forenames>Thomas Dueholm</forenames></author><author><keyname>Zwick</keyname><forenames>Uri</forenames></author></authors><title>Random-Facet and Random-Bland require subexponential time even for
  shortest paths</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Random-Facet algorithm of Kalai and of Matousek, Sharir and Welzl is an
elegant randomized algorithm for solving linear programs and more general
LP-type problems. Its expected subexponential time of
$2^{\tilde{O}(\sqrt{m})}$, where $m$ is the number of inequalities, makes it
the fastest known combinatorial algorithm for solving linear programs. We
previously showed that Random-Facet performs an expected number of
$2^{\tilde{\Omega}(\sqrt[3]{m})}$ pivoting steps on some LPs with $m$
inequalities that correspond to $m$-action Markov Decision Processes (MDPs). We
also showed that Random-Facet-1P, a one permutation variant of Random-Facet,
performs an expected number of $2^{\tilde{O}(\sqrt{m})}$ pivoting steps on
these examples. Here we show that the same results can be obtained using LPs
that correspond to instances of the classical shortest paths problem. This
shows that the stochasticity of the MDPs, which is essential for obtaining
lower bounds for Random-Edge, is not needed in order to obtain lower bounds for
Random-Facet. We also show that our new $2^{\tilde{\Omega}(\sqrt{m})}$ lower
bound applies to Random-Bland, a randomized variant of the classical
anti-cycling rule suggested by Bland.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7534</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7534</id><created>2014-10-28</created><authors><author><keyname>Ciebiera</keyname><forenames>Krzysztof</forenames></author><author><keyname>Godlewski</keyname><forenames>Piotr</forenames></author><author><keyname>Sankowski</keyname><forenames>Piotr</forenames></author><author><keyname>Wygocki</keyname><forenames>Piotr</forenames></author></authors><title>Approximation Algorithms for Steiner Tree Problems Based on Universal
  Solution Frameworks</title><categories>cs.DS cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes the work on implementing few solutions for the Steiner
Tree problem which we undertook in the PAAL project. The main focus of the
project is the development of generic implementations of approximation
algorithms together with universal solution frameworks. In particular, we have
implemented Zelikovsky 11/6-approximation using local search framework, and
1.39-approximation by Byrka et al. using iterative rounding framework. These
two algorithms are experimentally compared with greedy 2-approximation, with
exact but exponential time Dreyfus-Wagner algorithm, as well as with results
given by a state-of-the-art local search techniques by Uchoa and Werneck. The
results of this paper are twofold. On one hand, we demonstrate that high level
algorithmic concepts can be designed and efficiently used in C++. On the other
hand, we show that the above algorithms with good theoretical guarantees, give
decent results in practice, but are inferior to state-of-the-art heuristical
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7540</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7540</id><created>2014-10-28</created><authors><author><keyname>Saeed</keyname><forenames>Swaleha</forenames></author><author><keyname>Umar</keyname><forenames>M Sarosh</forenames></author><author><keyname>Ali</keyname><forenames>M Athar</forenames></author><author><keyname>Ahmad</keyname><forenames>Musheer</forenames></author></authors><title>Fisher-Yates Chaotic Shuffling Based Image Encryption</title><categories>cs.CR cs.MM</categories><journal-ref>International Journal of Information Processing, 8(3), 31-41, 2014
  ISSN : 0973-8215 IK International Publishing House Pvt. Ltd., New Delhi,
  India</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Present era, information security is of utmost concern and encryption is
one of the alternatives to ensure security. Chaos based cryptography has
brought a secure and efficient way to meet the challenges of secure multimedia
transmission over the networks. In this paper, we have proposed a secure
Grayscale image encryption methodology in wavelet domain. The proposed
algorithm performs shuffling followed by encryption using states of chaotic map
in a secure manner. Firstly, the image is transformed from spatial domain to
wavelet domain by the Haar wavelet. Subsequently, Fisher Yates chaotic
shuffling technique is employed to shuffle the image in wavelet domain to
confuse the relationship between plain image and cipher image. A key dependent
piece-wise linear chaotic map is used to generate chaos for the chaotic
shuffling. Further, the resultant shuffled approximate coefficients are
chaotically modulated. To enhance the statistical characteristics from
cryptographic point of view, the shuffled image is self keyed diffused and
mixing operation is carried out using keystream extracted from one-dimensional
chaotic map and the plain-image. The proposed algorithm is tested over some
standard image dataset. The results of several experimental, statistical and
sensitivity analyses proved that the algorithm provides an efficient and secure
method to achieve trusted gray scale image encryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7550</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7550</id><created>2014-10-28</created><authors><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Niklas</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author></authors><title>Learning deep dynamical models from image pixels</title><categories>stat.ML cs.LG cs.NE cs.SY</categories><comments>10 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7551</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7551</id><created>2014-10-28</created><authors><author><keyname>Aminof</keyname><forenames>Benjamin</forenames></author><author><keyname>Murano</keyname><forenames>Aniello</forenames></author><author><keyname>Rubin</keyname><forenames>Sasha</forenames></author></authors><title>Satisfiability and Model Checking of CTL* with Graded Path Modalities</title><categories>cs.LO</categories><comments>13 pages + Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graded path modalities count the number of paths satisfying a property, and
generalize the existential (E) and universal (A) path modalities of CTL*. The
resulting logic is called GCTL*. We settle the complexity of satisfiability of
GCTL*, i.e., 2ExpTime-Complete, and the complexity of the model checking
problem for GCTL*, i.e., PSpace-Complete. The lower bounds already hold for
CTL*, and so, using the automata-theoretic approach we supply the upper bounds.
The significance of this work is two-fold: GCTL* is more expressive than CTL*
at no extra cost in computational complexity, and GCTL* has all the advantages
over GCTL (CTL with graded path modalities) that CTL* has over CTL, e.g., the
ability to express fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7560</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7560</id><created>2014-10-28</created><authors><author><keyname>Paul</keyname><forenames>Rourab</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Ghosh</keyname><forenames>Ranjan</forenames></author></authors><title>Multi Core SSL/TLS Security Processor Architecture Prototype Design with
  automated Preferential Algorithm in FPGA</title><categories>cs.AR</categories><comments>This is Manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a pipelined architecture of a high speed network security
processor (NSP) for SSL,TLS protocol is implemented on a system on chip (SOC)
where hardware information of all encryption, hashing and key exchange
algorithms are stored in flash memory in terms of bit files, in contrary to
related works where all are actually implemented in hardware. The NSP finds
applications in e-commerce, virtual private network (VPN) and in other fields
that require data confidentiality. The motivation of the present work is to
dynamically execute applications with stipulated throughput within budgeted
hardware resource and power. A preferential algorithm choosing an appropriate
cipher suite is proposed, which is based on E?cient System Index (ESI) budget
comprising of power, throughput and resource given by the user. The bit files
of the chosen security algorithms are downloaded from the flash memory to the
partial region of field programmable gate array (FPGA). The proposed SOC
controls data communication between an application running in a system through
a PCI and the Ethernet interface of a network. Partial configuration feature is
used in ISE14.4 suite with ZYNQ 7z020-clg484 FPGA platform. The performances
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7580</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7580</id><created>2014-10-28</created><authors><author><keyname>Bao</keyname><forenames>Linchao</forenames></author><author><keyname>Yang</keyname><forenames>Qingxiong</forenames></author></authors><title>Robust Piecewise-Constant Smoothing: M-Smoother Revisited</title><categories>cs.CV</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust estimator, namely M-smoother, for piecewise-constant smoothing is
revisited in this paper. Starting from its generalized formulation, we propose
a numerical scheme/framework for solving it via a series of weighted-average
filtering (e.g., box filtering, Gaussian filtering, bilateral filtering, and
guided filtering). Because of the equivalence between M-smoother and
local-histogram-based filters (such as median filter and mode filter), the
proposed framework enables fast approximation of histogram filters via a number
of box filtering or Gaussian filtering. In addition, high-quality
piecewise-constant smoothing can be achieved via a number of bilateral
filtering or guided filtering integrated in the proposed framework. Experiments
on depth map denoising show the effectiveness of our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7582</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7582</id><created>2014-10-28</created><authors><author><keyname>Y&#x131;ld&#x131;r&#x131;m</keyname><forenames>Kas&#x131;m Sinan</forenames></author><author><keyname>G&#xfc;rcan</keyname><forenames>&#xd6;nder</forenames></author></authors><title>Adaptive Synchronization of Robotic Sensor Networks</title><categories>cs.DC</categories><comments>First International Workshop on Robotic Sensor Networks part of
  Cyber-Physical Systems Week, Berlin, Germany, 14 April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main focus of recent time synchronization research is developing
power-efficient synchronization methods that meet pre-defined accuracy
requirements. However, an aspect that has been often overlooked is the high
dynamics of the network topology due to the mobility of the nodes. Employing
existing flooding-based and peer-to-peer synchronization methods, are networked
robots still be able to adapt themselves and self-adjust their logical clocks
under mobile network dynamics? In this paper, we present the application and
the evaluation of the existing synchronization methods on robotic sensor
networks. We show through simulations that Adaptive Value Tracking
synchronization is robust and efficient under mobility. Hence, deducing the
time synchronization problem in robotic sensor networks into a dynamic value
searching problem is preferable to existing synchronization methods in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7583</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7583</id><created>2014-10-28</created><authors><author><keyname>Hollanders</keyname><forenames>Romain</forenames></author><author><keyname>Gerencs&#xe9;r</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author></authors><title>Improved bound on the worst case complexity of Policy Iteration</title><categories>cs.CC cs.DM</categories><comments>11 pages, 1 figure, submitted to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving Markov Decision Processes (MDPs) is a recurrent task in engineering.
Even though it is known that solutions for minimizing the infinite horizon
expected reward can be found in polynomial time using Linear Programming
techniques, iterative methods like the Policy Iteration algorithm (PI) remain
usually the most efficient in practice. This method is guaranteed to converge
in a finite number of steps. Unfortunately, it is known that it may require an
exponential number of steps in the size of the problem to converge. On the
other hand, many open questions remain considering the actual worst case
complexity. In this work, we provide the first improvement over the fifteen
years old upper bound from Mansour &amp; Singh (1999) by showing that PI requires
at most k/(k-1)*k^n/n + o(k^n/n) iterations to converge, where n is the number
of states of the MDP and k is the maximum number of actions per state. Perhaps
more importantly, we also show that this bound is optimal for an important
relaxation of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7596</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7596</id><created>2014-10-28</created><authors><author><keyname>Agrawal</keyname><forenames>Shipra</forenames></author><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author></authors><title>Fast Algorithms for Online Stochastic Convex Programming</title><categories>cs.LG cs.DS math.OC</categories><comments>To appear in SODA 2015</comments><acm-class>F.1.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the online stochastic Convex Programming (CP) problem, a very
general version of stochastic online problems which allows arbitrary concave
objectives and convex feasibility constraints. Many well-studied problems like
online stochastic packing and covering, online stochastic matching with concave
returns, etc. form a special case of online stochastic CP. We present fast
algorithms for these problems, which achieve near-optimal regret guarantees for
both the i.i.d. and the random permutation models of stochastic inputs. When
applied to the special case online packing, our ideas yield a simpler and
faster primal-dual algorithm for this well studied problem, which achieves the
optimal competitive ratio. Our techniques make explicit the connection of
primal-dual paradigm and online learning to online stochastic CP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7613</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7613</id><created>2014-10-28</created><authors><author><keyname>Wang</keyname><forenames>Xian</forenames></author><author><keyname>Ma</keyname><forenames>Shaopeng</forenames></author></authors><title>A Short Image Series Based Scheme for Time Series Digital Image
  Correlation</title><categories>physics.optics cs.CV</categories><comments>15 pages, 10 figures</comments><msc-class>78Mxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new scheme for digital image correlation, i.e., short time series DIC
(STS-DIC) is proposed. Instead of processing the original deformed speckle
images individually, STS-DIC combines several adjacent deformed speckle images
from a short time series and then processes the averaged image, for which
deformation continuity over time is introduced. The deformation of several
adjacent images is assumed to be linear in time and a new spatial-temporal
displacement representation method with eight unknowns is presented based on
the subset-based representation method. Then, the model of STS-DIC is created
and a solving scheme is developed based on the Newton-Raphson iteration. The
proposed method is verified for numerical and experimental cases. The results
show that the proposed STS-DIC greatly improves the accuracy of traditional
DIC, both under simple and complicated deformation conditions, while retaining
acceptable actual computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7614</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7614</id><created>2014-10-28</created><updated>2015-02-05</updated><authors><author><keyname>Zhang</keyname><forenames>Zhifei</forenames></author><author><keyname>Sarlette</keyname><forenames>Alain</forenames></author><author><keyname>Ling</keyname><forenames>Zhihao</forenames></author></authors><title>Integral Control on Lie Groups</title><categories>cs.SY</categories><comments>Resubmitted to Systems and Control Letters, February 2015</comments><msc-class>93Bxx, 93Cxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the popular integral control technique to systems
evolving on Lie groups. More explicitly, we provide an alternative definition
of &quot;integral action&quot; for proportional(-derivative)-controlled systems whose
configuration evolves on a nonlinear space, where configuration errors cannot
be simply added up to compute a definite integral. We then prove that the
proposed integral control allows to cancel the drift induced by a constant bias
in both first order (velocity) and second order (torque) control inputs for
fully actuated systems evolving on abstract Lie groups. We illustrate the
approach by 3-dimensional motion control applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7619</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7619</id><created>2014-10-28</created><updated>2015-02-18</updated><authors><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author></authors><title>Some &quot;Goodness&quot; Properties of LDA Lattices</title><categories>cs.IT math.IT</categories><comments>21 pages, 4 figures. A 5-page version of this article has been
  accepted for presentation at the 2015 IEEE Information Theory Workshop. Some
  minor corrections have been made in v2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study some structural properties of Construction-A lattices obtained from
low density parity check (LDPC) codes over prime fields. Such lattices are
called low density Construction-A (LDA) lattices, and permit low-complexity
belief propagation decoding for transmission over Gaussian channels. It has
been shown that LDA lattices achieve the capacity of the power constrained AWGN
channel with closest lattice-point decoding, and simulations suggested that
they perform well under belief propagation decoding. We continue this line of
work, and prove that these lattices are good for packing and mean squared error
(MSE) quantization, and that their duals are good for packing. With this, we
can conclude that codes constructed using nested LDA lattices can achieve the
capacity of the AWGN channel, the capacity of the dirty paper channel, the
rates guaranteed by the compute-and-forward protocol, and the best known rates
for bidirectional relaying with perfect secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7632</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7632</id><created>2014-10-15</created><updated>2015-03-22</updated><authors><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author><author><keyname>Barczyk</keyname><forenames>Martin</forenames></author><author><keyname>Goulette</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>On the covariance of scan-matching techniques for localization</title><categories>cs.CV cs.RO cs.SY</categories><comments>Submitted to 54th IEEE Conference on Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of estimating the covariance of the relative
displacement measurement output by the Iterative Closest Point (ICP) algorithm.
The problem is relevant for localization of mobile robots (using a sensor of
the Kinect type) or for autonomous vehicles (using a sensor of the Velodyne
type) where the relative displacement measurement must be fused with other
sensors' measurements, such as wheel encoders or inertial sensors, in a Kalman
or a particle filter. The closed-form formulas proposed in previous literature
generally build upon the fact that the solution to ICP is obtained by
minimizing a function of the data. In this paper, we prove this approach is on
shaky ground because the rematching step of the ICP is not explicitly accounted
for, and a blind application to point-to-point ICP leads to completely
erroneous covariances. Yet we justify that the method is applicable to
point-to-plane ICP as already known to practitioners, by proposing a geometric
mathematical proof bounding the errors made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7654</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7654</id><created>2014-10-27</created><authors><author><keyname>D.</keyname><forenames>Suma</forenames></author><author><keyname>Acharya</keyname><forenames>U. Dinesh</forenames></author><author><keyname>M.</keyname><forenames>Geetha</forenames></author><author><keyname>M</keyname><forenames>Raviraja Holla</forenames></author></authors><title>XML Information Retrieval:An overview</title><categories>cs.IR</categories><comments>7 pages, 0 figures</comments><journal-ref>International Global Journal For Engineering Research, Volume 10
  Issue 1, 2014 pg. 26-32</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locating and distilling the valuable relevant information continued to be the
major challenges of Information Retrieval (IR) Systems owing to the explosive
growth of online web information. These challenges can be considered the XML
Information Retrieval challenges as XML has become a de facto standard over the
Web. The research on XML IR starts with the classical IR strategies customized
to XML IR. Later novel IR strategies specific to XML IR are evolved. Meanwhile
literatures reveal development of the rapid and intelligent IR systems. Despite
their success in their specified constrained domains, they have additional
limitations in the complex information space. The effectiveness of IR systems
is thus unsolved in satisfying the most. This article attemptsan overview of
earlier efforts and the gaps in XML IR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7659</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7659</id><created>2014-10-28</created><updated>2014-11-28</updated><authors><author><keyname>Bresler</keyname><forenames>Guy</forenames></author><author><keyname>Gamarnik</keyname><forenames>David</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Learning graphical models from the Glauber dynamics</title><categories>cs.LG cs.IT math.IT stat.CO stat.ML</categories><comments>9 pages. Appeared in Allerton Conference 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of learning undirected graphical models
from data generated according to the Glauber dynamics. The Glauber dynamics is
a Markov chain that sequentially updates individual nodes (variables) in a
graphical model and it is frequently used to sample from the stationary
distribution (to which it converges given sufficient time). Additionally, the
Glauber dynamics is a natural dynamical model in a variety of settings. This
work deviates from the standard formulation of graphical model learning in the
literature, where one assumes access to i.i.d. samples from the distribution.
  Much of the research on graphical model learning has been directed towards
finding algorithms with low computational cost. As the main result of this
work, we establish that the problem of reconstructing binary pairwise graphical
models is computationally tractable when we observe the Glauber dynamics.
Specifically, we show that a binary pairwise graphical model on $p$ nodes with
maximum degree $d$ can be learned in time $f(d)p^2\log p$, for a function
$f(d)$, using nearly the information-theoretic minimum number of samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7660</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7660</id><created>2014-10-28</created><authors><author><keyname>Netrapalli</keyname><forenames>Praneeth</forenames></author><author><keyname>Niranjan</keyname><forenames>U N</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author></authors><title>Non-convex Robust PCA</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>Extended abstract to appear in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for robust PCA -- the task of recovering a low-rank
matrix from sparse corruptions that are of unknown value and support. Our
method involves alternating between projecting appropriate residuals onto the
set of low-rank matrices, and the set of sparse matrices; each projection is
{\em non-convex} but easy to compute. In spite of this non-convexity, we
establish exact recovery of the low-rank matrix, under the same conditions that
are required by existing methods (which are based on convex optimization). For
an $m \times n$ input matrix ($m \leq n)$, our method has a running time of
$O(r^2mn)$ per iteration, and needs $O(\log(1/\epsilon))$ iterations to reach
an accuracy of $\epsilon$. This is close to the running time of simple PCA via
the power method, which requires $O(rmn)$ per iteration, and
$O(\log(1/\epsilon))$ iterations. In contrast, existing methods for robust PCA,
which are based on convex optimization, have $O(m^2n)$ complexity per
iteration, and take $O(1/\epsilon)$ iterations, i.e., exponentially more
iterations for the same accuracy.
  Experiments on both synthetic and real data establishes the improved speed
and accuracy of our method over existing convex implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7669</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7669</id><created>2014-10-28</created><updated>2014-10-29</updated><authors><author><keyname>Regnault</keyname><forenames>Damien</forenames></author><author><keyname>R&#xe9;mila</keyname><forenames>Eric</forenames></author></authors><title>Lost in Self-stabilization</title><categories>cs.DM</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the questions addressed here is How can a twisted thread correct
itself?. We consider a theoretical model where the studied mathematical object
represents a 2D twisted discrete thread linking two points. This thread is made
of a chain of agents which are lost, i.e. they have no knowledge of the global
setting and no sense of direction. Thus, the modifications made by the agents
are local and all the decisions use only minimal information about the local
neighborhood. We introduce a random process such that the thread reorganizes
itself efficiently to become a discrete line between these two points. The
second question addressed here is to reorder a word by local flips in order to
scatter the letters to avoid long successions of the same letter. These two
questions are equivalent. The work presented here is at the crossroad of many
different domains such as modeling cooling process in crystallography [2, 3,
8], stochastic cellular automata [6, 7], organizing a line of robots in
distributed algorithms (the robot chain problem [5, 11]), and Christoffel words
in language theory [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7670</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7670</id><created>2014-10-28</created><authors><author><keyname>Donalek</keyname><forenames>Ciro</forenames></author><author><keyname>Djorgovski</keyname><forenames>S. G.</forenames></author><author><keyname>Davidoff</keyname><forenames>Scott</forenames></author><author><keyname>Cioc</keyname><forenames>Alex</forenames></author><author><keyname>Wang</keyname><forenames>Anwell</forenames></author><author><keyname>Longo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Norris</keyname><forenames>Jeffrey S.</forenames></author><author><keyname>Zhang</keyname><forenames>Jerry</forenames></author><author><keyname>Lawler</keyname><forenames>Elizabeth</forenames></author><author><keyname>Yeh</keyname><forenames>Stacy</forenames></author><author><keyname>Mahabal</keyname><forenames>Ashish</forenames></author><author><keyname>Graham</keyname><forenames>Matthew</forenames></author><author><keyname>Drake</keyname><forenames>Andrew</forenames></author></authors><title>Immersive and Collaborative Data Visualization Using Virtual Reality
  Platforms</title><categories>cs.HC astro-ph.IM</categories><comments>6 pages, refereed proceedings of 2014 IEEE International Conference
  on Big Data, page 609, ISBN 978-1-4799-5665-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective data visualization is a key part of the discovery process in the
era of big data. It is the bridge between the quantitative content of the data
and human intuition, and thus an essential component of the scientific path
from data into knowledge and understanding. Visualization is also essential in
the data mining process, directing the choice of the applicable algorithms, and
in helping to identify and remove bad data from the analysis. However, a high
complexity or a high dimensionality of modern data sets represents a critical
obstacle. How do we visualize interesting structures and patterns that may
exist in hyper-dimensional data spaces? A better understanding of how we can
perceive and interact with multi dimensional information poses some deep
questions in the field of cognition technology and human computer interaction.
To this effect, we are exploring the use of immersive virtual reality platforms
for scientific data visualization, both as software and inexpensive commodity
hardware. These potentially powerful and innovative tools for multi dimensional
data visualization can also provide an easy and natural path to a collaborative
data visualization and exploration, where scientists can interact with their
data and their colleagues in the same visual space. Immersion provides benefits
beyond the traditional desktop visualization tools: it leads to a demonstrably
better perception of a datascape geometry, more intuitive data understanding,
and a better retention of the perceived relationships in the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7675</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7675</id><created>2014-10-28</created><authors><author><keyname>Lu</keyname><forenames>Songtao</forenames></author><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author></authors><title>Achievable Rates and Training Optimization for Uplink Multiuser Massive
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of uplink transmission in a large-scale (massive)
MIMO system, where all the transmitters have single antennas and the receiver
(base station) has a large number of antennas. Specifically, we first derive
the rates that are possible through minimum mean-squared error (MMSE) channel
estimation and three linear receivers: maximum ratio combining (MRC),
zero-forcing (ZF), and MMSE. Based on the derived rates, we quantify the amount
of energy savings that are possible through increased number of base-station
antennas or increased coherence interval. We also analyze achievable total
degrees of freedom of such a system without assuming channel state information
at the receiver, which is shown to be the same as that of a point-to-point MIMO
channel. Linear receiver is sufficient when the number of users is less than
the number of antennas. Otherwise, nonlinear processing is necessary to achieve
the full degrees of freedom. Finally, the training period and optimal training
energy allocation under the average and peak power constraints are optimized
jointly to maximize the achievable sum rate when either MRC or ZF receiver is
adopted at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7679</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7679</id><created>2014-10-16</created><authors><author><keyname>Mboula</keyname><forenames>Fred Maurice Ngol&#xe8;</forenames></author><author><keyname>Starck</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Ronayette</keyname><forenames>Samuel</forenames></author><author><keyname>Okumura</keyname><forenames>Koryo</forenames></author><author><keyname>Amiaux</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Super-resolution method using sparse regularization for point-spread
  function recovery</title><categories>cs.CV astro-ph.IM</categories><doi>10.1051/0004-6361/201424167</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In large-scale spatial surveys, such as the forthcoming ESA Euclid mission,
images may be undersampled due to the optical sensors sizes. Therefore, one may
consider using a super-resolution (SR) method to recover aliased frequencies,
prior to further analysis. This is particularly relevant for point-source
images, which provide direct measurements of the instrument point-spread
function (PSF). We introduce SPRITE, SParse Recovery of InsTrumental rEsponse,
which is an SR algorithm using a sparse analysis prior. We show that such a
prior provides significant improvements over existing methods, especially on
low SNR PSFs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7682</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7682</id><created>2014-10-28</created><authors><author><keyname>Ghosh</keyname><forenames>Sutanu</forenames></author></authors><title>Performance evaluation with a Comparative analysis of mimo channel on
  The basis of doppler shift and other Probabilistic parameters in fading
  Environment</title><categories>cs.NI cs.IT math.IT</categories><comments>Journal Paper With 10 Pages (Number of Figures - 07), International
  Journal of Mobile Network Communications &amp; Telematics (IJMNCT), October 2014</comments><doi>10.5121/ijmnct.2014.4503</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  At this present scenario, the demand of the system capacity is very high in
wireless network. MIMO technology is used from the last decade to provide this
requirement for wireless network antenna technology. MIMO channels are mostly
used for advanced antenna array technology. But it is most important to control
the error rate with enhanced system capacity in MIMO for present-day
progressive wireless communication. This paper explores the frame error rate
with respect to different path gain of MIMO channel. This work has been done in
different fading scenario and produces a comparative analysis of MIMO on the
basis of those fading models in various conditions. Here, it is to be
considered that modulation technique as QPSK to observe these comparative
evaluations for different Doppler frequencies. From the comparative analysis,
minimum amount of frame error rate is viewed for Rician distribution at LOS
path Doppler shift of 0 Hz. At last, this work is concluded with a comparative
bit error rate study on the basis of singular parameters at different SNR
levels to produce the system performance for uncoded QPSK modulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7690</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7690</id><created>2014-10-28</created><updated>2015-08-11</updated><authors><author><keyname>Wang</keyname><forenames>Yu-Xiang</forenames></author><author><keyname>Sharpnack</keyname><forenames>James</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author><author><keyname>Tibshirani</keyname><forenames>Ryan J.</forenames></author></authors><title>Trend Filtering on Graphs</title><categories>stat.ML cs.AI cs.LG stat.ME</categories><msc-class>62G05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a family of adaptive estimators on graphs, based on penalizing
the $\ell_1$ norm of discrete graph differences. This generalizes the idea of
trend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate
nonparametric regression, to graphs. Analogous to the univariate case, graph
trend filtering exhibits a level of local adaptivity unmatched by the usual
$\ell_2$-based graph smoothers. It is also defined by a convex minimization
problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We
demonstrate the merits of graph trend filtering through examples and theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7694</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7694</id><created>2014-10-28</created><updated>2014-11-04</updated><authors><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Li</keyname><forenames>Shujun</forenames></author></authors><title>Network analysis of the state space of chaotic map in digital domain</title><categories>cs.CR nlin.CD</categories><comments>The author list and funding acknowledgement are corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex dynamics of chaotic maps under an infinite-precision mathematical
framework have been well known. The case in a finite-precision computer remains
to be further explored. Previous work treated a digital chaotic map as a black
box and gave different explanations according to the test results of the
output. Using the Logistic map as a typical example, we disclose some dynamical
properties of chaotic maps in fixed-point arithmetic by studying its
corresponding state network, where every possible value is considered as a node
and every possible mapping relation between a pair of nodes works as a directed
edge. The scale-free properties of the state network are quantitatively proven.
The obtained results can be extended to the scenario of floating-point
arithmetic and to other chaotic maps. Understanding the real network structure
of the state space of a chaotic map in the digital domain will help evaluate
and improve the randomness of pseudo-random number sequences generated by
chaotic maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7704</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7704</id><created>2014-10-28</created><updated>2015-01-16</updated><authors><author><keyname>Giacobbe</keyname><forenames>Mirco</forenames></author><author><keyname>Guet</keyname><forenames>Calin C.</forenames></author><author><keyname>Gupta</keyname><forenames>Ashutosh</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Paixao</keyname><forenames>Tiago</forenames></author><author><keyname>Petrov</keyname><forenames>Tatjana</forenames></author></authors><title>Model Checking Gene Regulatory Networks</title><categories>cs.CE cs.LO q-bio.MN</categories><comments>19 pages, 20 references, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The behaviour of gene regulatory networks (GRNs) is typically analysed using
simulation-based statistical testing-like methods. In this paper, we
demonstrate that we can replace this approach by a formal verification-like
method that gives higher assurance and scalability. We focus on Wagner weighted
GRN model with varying weights, which is used in evolutionary biology. In the
model, weight parameters represent the gene interaction strength that may
change due to genetic mutations. For a property of interest, we synthesise the
constraints over the parameter space that represent the set of GRNs satisfying
the property. We experimentally show that our parameter synthesis procedure
computes the mutational robustness of GRNs -an important problem of interest in
evolutionary biology- more efficiently than the classical simulation method. We
specify the property in linear temporal logics. We employ symbolic bounded
model checking and SMT solving to compute the space of GRNs that satisfy the
property, which amounts to synthesizing a set of linear constraints on the
weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7709</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7709</id><created>2014-10-28</created><authors><author><keyname>Juvonen</keyname><forenames>Antti</forenames></author><author><keyname>Sipola</keyname><forenames>Tuomo</forenames></author></authors><title>Anomaly Detection Framework Using Rule Extraction for Efficient
  Intrusion Detection</title><categories>cs.LG cs.CR</categories><comments>35 pages, 12 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Huge datasets in cyber security, such as network traffic logs, can be
analyzed using machine learning and data mining methods. However, the amount of
collected data is increasing, which makes analysis more difficult. Many machine
learning methods have not been designed for big datasets, and consequently are
slow and difficult to understand. We address the issue of efficient network
traffic classification by creating an intrusion detection framework that
applies dimensionality reduction and conjunctive rule extraction. The system
can perform unsupervised anomaly detection and use this information to create
conjunctive rules that classify huge amounts of traffic in real time. We test
the implemented system with the widely used KDD Cup 99 dataset and real-world
network logs to confirm that the performance is satisfactory. This system is
transparent and does not work like a black box, making it intuitive for domain
experts, such as network administrators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7724</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7724</id><created>2014-10-28</created><updated>2014-11-25</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author><author><keyname>Pavlogiannis</keyname><forenames>Andreas</forenames></author><author><keyname>Goyal</keyname><forenames>Prateesh</forenames></author></authors><title>Faster Algorithms for Algebraic Path Properties in RSMs with Constant
  Treewidth</title><categories>cs.PL cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interprocedural analysis is at the heart of numerous applications in
programming languages, such as alias analysis, constant propagation, etc.
Recursive state machines (RSMs) are standard models for interprocedural
analysis. We consider a general framework with RSMs where the transitions are
labeled from a semiring, and path properties are algebraic with semiring
operations. RSMs with algebraic path properties can model interprocedural
dataflow analysis problems, the shortest path problem, the most probable path
problem, etc. The traditional algorithms for interprocedural analysis focus on
path properties where the starting point is \emph{fixed} as the entry point of
a specific method. In this work, we consider possible multiple queries as
required in many applications such as in alias analysis. The study of multiple
queries allows us to bring in a very important algorithmic distinction between
the resource usage of the \emph{one-time} preprocessing vs for \emph{each
individual} query. The second aspect that we consider is that the control flow
graphs for most programs have constant treewidth.
  Our main contributions are simple and implementable algorithms that support
multiple queries for algebraic path properties for RSMs that have constant
treewidth. Our theoretical results show that our algorithms have small
additional one-time preprocessing, but can answer subsequent queries
significantly faster as compared to the current best-known solutions for
several important problems, such as interprocedural reachability and shortest
path. We provide a prototype implementation for interprocedural reachability
and intraprocedural shortest path that gives a significant speed-up on several
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7730</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7730</id><created>2014-10-28</created><authors><author><keyname>Garc&#xe9;s</keyname><forenames>Yasel</forenames></author><author><keyname>Torres</keyname><forenames>Esley</forenames></author><author><keyname>Pereira</keyname><forenames>Osvaldo</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>Roberto</forenames></author></authors><title>New similarity index based on entropy and group theory</title><categories>cs.CV</categories><comments>in Spanish</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a new similarity index for images considering the
entropy function and group theory. This index considers an algebraic group of
images, it is defined by an inner law that provides a novel approach for the
subtraction of images. Through an equivalence relationship in the field of
images, we prove the existence of the quotient group, on which the new
similarity index is defined. We also present the main properties of the new
index, and the immediate application thereof as a stopping criterion of the
&quot;Mean Shift Iterative Algorithm&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7743</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7743</id><created>2014-10-28</created><authors><author><keyname>Kayacik</keyname><forenames>Hilmi Gunes</forenames></author><author><keyname>Just</keyname><forenames>Mike</forenames></author><author><keyname>Baillie</keyname><forenames>Lynne</forenames></author><author><keyname>Aspinall</keyname><forenames>David</forenames></author><author><keyname>Micallef</keyname><forenames>Nicholas</forenames></author></authors><title>Data Driven Authentication: On the Effectiveness of User Behaviour
  Modelling with Mobile Device Sensors</title><categories>cs.CR</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a lightweight, and temporally and spatially aware user behaviour
modelling technique for sensor-based authentication. Operating in the
background, our data driven technique compares current behaviour with a user
profile. If the behaviour deviates sufficiently from the established norm,
actions such as explicit authentication can be triggered. To support a quick
and lightweight deployment, our solution automatically switches from training
mode to deployment mode when the user's behaviour is sufficiently learned.
Furthermore, it allows the device to automatically determine a suitable
detection threshold. We use our model to investigate practical aspects of
sensor-based authentication by applying it to three publicly available data
sets, computing expected times for training duration and behaviour drift. We
also test our model with scenarios involving an attacker with varying knowledge
and capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7744</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7744</id><created>2014-10-28</created><authors><author><keyname>Primault</keyname><forenames>Vincent</forenames></author><author><keyname>Mokhtar</keyname><forenames>Sonia Ben</forenames></author><author><keyname>Lauradoux</keyname><forenames>Cedric</forenames></author><author><keyname>Brunie</keyname><forenames>Lionel</forenames></author></authors><title>Differentially Private Location Privacy in Practice</title><categories>cs.CR</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the wide adoption of handheld devices (e.g. smartphones, tablets) a
large number of location-based services (also called LBSs) have flourished
providing mobile users with real-time and contextual information on the move.
Accounting for the amount of location information they are given by users,
these services are able to track users wherever they go and to learn sensitive
information about them (e.g. their points of interest including home, work,
religious or political places regularly visited). A number of solutions have
been proposed in the past few years to protect users location information while
still allowing them to enjoy geo-located services. Among the most robust
solutions are those that apply the popular notion of differential privacy to
location privacy (e.g. Geo-Indistinguishability), promising strong theoretical
privacy guarantees with a bounded accuracy loss. While these theoretical
guarantees are attracting, it might be difficult for end users or practitioners
to assess their effectiveness in the wild. In this paper, we carry on a
practical study using real mobility traces coming from two different datasets,
to assess the ability of Geo-Indistinguishability to protect users' points of
interest (POIs). We show that a curious LBS collecting obfuscated location
information sent by mobile users is still able to infer most of the users POIs
with a reasonable both geographic and semantic precision. This precision
depends on the degree of obfuscation applied by Geo-Indistinguishability.
Nevertheless, the latter also has an impact on the overhead incurred on mobile
devices resulting in a privacy versus overhead trade-off. Finally, we show in
our study that POIs constitute a quasi-identifier for mobile users and that
obfuscating them using Geo-Indistinguishability is not sufficient as an
attacker is able to re-identify at least 63% of them despite a high degree of
obfuscation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7745</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7745</id><created>2014-10-28</created><authors><author><keyname>Tendulkar</keyname><forenames>Vasant</forenames></author><author><keyname>Enck</keyname><forenames>William</forenames></author></authors><title>An Application Package Configuration Approach to Mitigating Android SSL
  Vulnerabilities</title><categories>cs.CR</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing platforms such as smartphones frequently access Web content using
many separate applications rather than a single Web browser application. These
applications often deal with sensitive user information such as financial data
or passwords, and use Secure Sockets Layer (SSL) to protect it from
unauthorized eavesdropping. However, recent studies have confirmed a
wide-spread misconfiguration of SSL verification in applications. This paper
considers the difficulty faced by Android application developers when modifying
SSL code for using common features like pinning or using a self-signed SSL
certificate. For example, developing an application that accesses a test Web
server with a self-signed certificate requires additional code to remove SSL
verification; however, this code is not always removed in production versions
of the application. To mitigate vulnerabilities introduced because of the
complexity of customizing SSL code in Android applications, we propose that
common SSL configuration should be specified in the application's package
manifest. We provide two concrete suggestions: 1) linking the application's
debug state to SSL verification, and 2) pinning certificates and CAs in the
manifest. We evaluate the appropriateness of these two suggestions on over
13,000 applications from Google's Play Store, of which 3,302 use SSL in
non-advertisement code, and find that 1,889 (57.20%) of these SSL applications
would benefit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7746</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7746</id><created>2014-10-28</created><authors><author><keyname>Song</keyname><forenames>Yihang</forenames></author><author><keyname>Kukreti</keyname><forenames>Madhur</forenames></author><author><keyname>Rawat</keyname><forenames>Rahul</forenames></author><author><keyname>Hengartner</keyname><forenames>Urs</forenames></author></authors><title>Two Novel Defenses against Motion-Based Keystroke Inference Attacks</title><categories>cs.CR</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays smartphones come embedded with multiple motion sensors, such as an
accelerometer, a gyroscope and an orientation sensor. With these sensors, apps
can gather more information and therefore provide end users with more
functionality. However, these sensors also introduce the potential risk of
leaking a user's private information because apps can access these sensors
without requiring security permissions. By monitoring a device's motion, a
malicious app may be able to infer sensitive information about the owner of the
device. For example, related work has shown that sensitive information entered
by a user on a device's touchscreen, such as numerical PINs or passwords, can
be inferred from accelerometer and gyroscope data.
  In this paper, we study these motion-based keystroke inference attacks to
determine what information they need to succeed. Based on this study, we
propose two novel approaches to defend against keystroke inference attacks: 1)
Reducing sensor data accuracy; 2) Random keyboard layout generation. We present
the design and the implementation of these two defences on the Android platform
and show how they significantly reduce the accuracy of keystroke inference
attacks. We also conduct multiple user studies to evaluate the usability and
feasibility of these two defences. Finally, we determine the impact of the
defences on apps that have legitimate reasons to access motion sensors and show
that the impact is negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7747</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7747</id><created>2014-10-28</created><authors><author><keyname>Ge</keyname><forenames>Xinyang</forenames></author><author><keyname>Vijayakumar</keyname><forenames>Hayawardh</forenames></author><author><keyname>Jaeger</keyname><forenames>Trent</forenames></author></authors><title>Sprobes: Enforcing Kernel Code Integrity on the TrustZone Architecture</title><categories>cs.CR cs.OS</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many smartphones now deploy conventional operating systems, so the rootkit
attacks so prevalent on desktop and server systems are now a threat to
smartphones. While researchers have advocated using virtualization to detect
and prevent attacks on operating systems (e.g., VM introspection and trusted
virtual domains), virtualization is not practical on smartphone systems due to
the lack of virtualization support and/or the expense of virtualization.
Current smartphone processors do have hardware support for running a protected
environment, such as the ARM TrustZone extensions, but such hardware does not
control the operating system operations sufficiently to enable VM
introspection. In particular, a conventional operating system running with
TrustZone still retains full control of memory management, which a rootkit can
use to prevent traps on sensitive instructions or memory accesses necessary for
effective introspection. In this paper, we present SPROBES, a novel primitive
that enables introspection of operating systems running on ARM TrustZone
hardware. Using SPROBES, an introspection mechanism protected by TrustZone can
instrument individual operating system instructions of its choice, receiving an
unforgeable trap whenever any SPROBE is executed. The key challenge in
designing SPROBES is preventing the rootkit from removing them, but we identify
a set of five invariants whose enforcement is sufficient to restrict rootkits
to execute only approved, SPROBE-injected kernel code. We implemented a
proof-of-concept version of SPROBES for the ARM Fast Models emulator,
demonstrating that in Linux kernel 2.6.38, only 12 SPROBES are sufficient to
enforce all five of these invariants. With SPROBES we show that it is possible
to leverage the limited TrustZone extensions to limit conventional kernel
execution to approved code comprehensively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7749</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7749</id><created>2014-10-28</created><authors><author><keyname>Neuner</keyname><forenames>Sebastian</forenames></author><author><keyname>van der Veen</keyname><forenames>Victor</forenames></author><author><keyname>Lindorfer</keyname><forenames>Martina</forenames></author><author><keyname>Huber</keyname><forenames>Markus</forenames></author><author><keyname>Merzdovnik</keyname><forenames>Georg</forenames></author><author><keyname>Mulazzani</keyname><forenames>Martin</forenames></author><author><keyname>Weippl</keyname><forenames>Edgar</forenames></author></authors><title>Enter Sandbox: Android Sandbox Comparison</title><categories>cs.CR</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expecting the shipment of 1 billion Android devices in 2017, cyber criminals
have naturally extended their vicious activities towards Google's mobile
operating system. With an estimated number of 700 new Android applications
released every day, keeping control over malware is an increasingly challenging
task. In recent years, a vast number of static and dynamic code analysis
platforms for analyzing Android applications and making decision regarding
their maliciousness have been introduced in academia and in the commercial
world. These platforms differ heavily in terms of feature support and
application properties being analyzed. In this paper, we give an overview of
the state-of-the-art dynamic code analysis platforms for Android and evaluate
their effectiveness with samples from known malware corpora as well as known
Android bugs like Master Key. Our results indicate a low level of diversity in
analysis platforms resulting from code reuse that leaves the evaluated systems
vulnerable to evasion. Furthermore the Master Key bugs could be exploited by
malware to hide malicious behavior from the sandboxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7751</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7751</id><created>2014-10-28</created><authors><author><keyname>Bierma</keyname><forenames>Michael</forenames></author><author><keyname>Gustafson</keyname><forenames>Eric</forenames></author><author><keyname>Erickson</keyname><forenames>Jeremy</forenames></author><author><keyname>Fritz</keyname><forenames>David</forenames></author><author><keyname>Choe</keyname><forenames>Yung Ryn</forenames></author></authors><title>Andlantis: Large-scale Android Dynamic Analysis</title><categories>cs.CR</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing Android applications for malicious behavior is an important area of
research, and is made difficult, in part, by the increasingly large number of
applications available for the platform. While techniques exist to perform
static analysis on a large number of applications, dynamic analysis techniques
are relatively limited in scale due to the computational resources required to
emulate the full Android system to achieve accurate execution. We present
Andlantis, a scalable dynamic analysis system capable of processing over 3000
Android applications per hour. During this processing, the system is able to
collect valuable forensic data, which helps reverse-engineers and malware
researchers identify and understand anomalous application behavior. We discuss
the results of running 1261 malware samples through the system, and provide
examples of malware analysis performed with the resulting data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7752</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7752</id><created>2014-10-28</created><authors><author><keyname>Ratazzi</keyname><forenames>Paul</forenames></author><author><keyname>Aafer</keyname><forenames>Yousra</forenames></author><author><keyname>Ahlawat</keyname><forenames>Amit</forenames></author><author><keyname>Hao</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Yifei</forenames></author><author><keyname>Du</keyname><forenames>Wenliang</forenames></author></authors><title>A Systematic Security Evaluation of Android's Multi-User Framework</title><categories>cs.CR</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Like many desktop operating systems in the 1990s, Android is now in the
process of including support for multi-user scenarios. Because these scenarios
introduce new threats to the system, we should have an understanding of how
well the system design addresses them. Since the security implications of
multi-user support are truly pervasive, we developed a systematic approach to
studying the system and identifying problems. Unlike other approaches that
focus on specific attacks or threat models, ours systematically identifies
critical places where access controls are not present or do not properly
identify the subject and object of a decision. Finding these places gives us
insight into hypothetical attacks that could result, and allows us to design
specific experiments to test our hypothesis.
  Following an overview of the new features and their implementation, we
describe our methodology, present a partial list of our most interesting
hypotheses, and describe the experiments we used to test them. Our findings
indicate that the current system only partially addresses the new threats,
leaving the door open to a number of significant vulnerabilities and privacy
issues. Our findings span a spectrum of root causes, from simple oversights,
all the way to major system design problems. We conclude that there is still a
long way to go before the system can be used in anything more than the most
casual of sharing environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7754</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7754</id><created>2014-10-28</created><authors><author><keyname>Defreez</keyname><forenames>Daniel</forenames></author><author><keyname>Shastry</keyname><forenames>Bhargava</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Seifert</keyname><forenames>Jean-Pierre</forenames></author></authors><title>A First Look at Firefox OS Security</title><categories>cs.CR cs.OS</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With Firefox OS, Mozilla is making a serious push for an HTML5-based mobile
platform. In order to assuage security concerns over providing hardware access
to web applications, Mozilla has introduced a number of mechanisms that make
the security landscape of Firefox OS distinct from both the desktop web and
other mobile operating systems. From an application security perspective, the
two most significant of these mechanisms are the the introduction of a default
Content Security Policy and code review in the market. This paper describes how
lightweight static analysis can augment these mechanisms to find
vulnerabilities which have otherwise been missed. We provide examples of
privileged applications in the market that contain vulnerabilities that can be
automatically detected.
  In addition to these findings, we show some of the challenges that occur when
desktop software is repurposed for a mobile operating system. In particular, we
argue that the caching of certificate overrides across applications--a known
problem in Firefox OS--generates a counter-intuitive user experience that
detracts from the security of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7756</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7756</id><created>2014-10-28</created><authors><author><keyname>Jin</keyname><forenames>Xing</forenames></author><author><keyname>Luo</keyname><forenames>Tongbo</forenames></author><author><keyname>Tsui</keyname><forenames>Derek G.</forenames></author><author><keyname>Du</keyname><forenames>Wenliang</forenames></author></authors><title>Code Injection Attacks on HTML5-based Mobile Apps</title><categories>cs.CR</categories><comments>In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</comments><proxy>Mike Just</proxy><report-no>MoST/2014/11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HTML5-based mobile apps become more and more popular, mostly because they are
much easier to be ported across different mobile platforms than native apps.
HTML5-based apps are implemented using the standard web technologies, including
HTML5, JavaScript and CSS; they depend on some middlewares, such as PhoneGap,
to interact with the underlying OS.
  Knowing that JavaScript is subject to code injection attacks, we have
conducted a systematic study on HTML5-based mobile apps, trying to evaluate
whether it is safe to rely on the web technologies for mobile app development.
Our discoveries are quite surprising. We found out that if HTML5-based mobile
apps become popular--it seems to go that direction based on the current
projection--many of the things that we normally do today may become dangerous,
including reading from 2D barcodes, scanning Wi-Fi access points, playing MP4
videos, pairing with Bluetooth devices, etc. This paper describes how
HTML5-based apps can become vulnerable, how attackers can exploit their
vulnerabilities through a variety of channels, and what damage can be achieved
by the attackers. In addition to demonstrating the attacks through example
apps, we have studied 186 PhoneGap plugins, used by apps to achieve a variety
of functionalities, and we found that 11 are vulnerable. We also found two real
HTML5-based apps that are vulnerable to the attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7758</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7758</id><created>2014-10-28</created><authors><author><keyname>Blanke</keyname><forenames>Tobias</forenames></author><author><keyname>Hedges</keyname><forenames>Mark</forenames></author></authors><title>Towards a Virtual Data Centre for Classics</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The paper presents some of our work on integrating datasets in Classics. We
present the results of various projects we had in this domain. The conclusions
from LaQuAT concerned limitations to the approach rather than solutions. The
relational model followed by OGSA-DAI was more effective for resources that
consist primarily of structured data (which we call data-centric) rather than
for largely unstructured text (which we call text-centric), which makes up a
significant component of the datasets we were using. This approach was,
moreover, insufficiently flexible to deal with the semantic issues. The gMan
project, on the other hand, addressed these problems by virtualizing data
resources using full-text indexes, which can then be used to provide different
views onto the collections and services that more closely match the sort of
information organization and retrieval activities found in the humanities, in
an environment that is more interactive, researcher-focused, and
researcher-driven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7762</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7762</id><created>2014-10-28</created><authors><author><keyname>Moazzezi</keyname><forenames>Reza</forenames></author></authors><title>A hierarchical framework for object recognition</title><categories>cs.CV</categories><comments>23 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object recognition in the presence of background clutter and distractors is a
central problem both in neuroscience and in machine learning. However, the
performance level of the models that are inspired by cortical mechanisms,
including deep networks such as convolutional neural networks and deep belief
networks, is shown to significantly decrease in the presence of noise and
background objects [19, 24]. Here we develop a computational framework that is
hierarchical, relies heavily on key properties of the visual cortex including
mid-level feature selectivity in visual area V4 and Inferotemporal cortex (IT)
[4, 9, 12, 18], high degrees of selectivity and invariance in IT [13, 17, 18]
and the prior knowledge that is built into cortical circuits (such as the
emergence of edge detector neurons in primary visual cortex before the onset of
the visual experience) [1, 21], and addresses the problem of object recognition
in the presence of background noise and distractors. Our approach is
specifically designed to address large deformations, allows flexible
communication between different layers of representation and learns highly
selective filters from a small number of training examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7787</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7787</id><created>2014-10-28</created><authors><author><keyname>Zajic</keyname><forenames>David</forenames></author><author><keyname>Maxwell</keyname><forenames>Michael</forenames></author><author><keyname>Doermann</keyname><forenames>David</forenames></author><author><keyname>Rodrigues</keyname><forenames>Paul</forenames></author><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author></authors><title>Correcting Errors in Digital Lexicographic Resources Using a Dictionary
  Manipulation Language</title><categories>cs.CL</categories><comments>5 pages, 3 figures, 1 table; appeared in Proceedings of Electronic
  Lexicography in the 21st Century (eLex), November 2011</comments><journal-ref>In Proceedings of Electronic Lexicography in the 21st Century
  (eLex), pages 297-301, Bled, Slovenia, November 2011. Trojina Institute for
  Applied Slovene Studies</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a paradigm for combining manual and automatic error correction of
noisy structured lexicographic data. Modifications to the structure and
underlying text of the lexicographic data are expressed in a simple,
interpreted programming language. Dictionary Manipulation Language (DML)
commands identify nodes by unique identifiers, and manipulations are performed
using simple commands such as create, move, set text, etc. Corrected lexicons
are produced by applying sequences of DML commands to the source version of the
lexicon. DML commands can be written manually to repair one-off errors or
generated automatically to correct recurring problems. We discuss advantages of
the paradigm for the task of editing digital bilingual dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7795</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7795</id><created>2014-10-20</created><authors><author><keyname>Jamal</keyname><forenames>Wasifa</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Oprescu</keyname><forenames>Ioana-Anastasia</forenames></author><author><keyname>Maharatna</keyname><forenames>Koushik</forenames></author><author><keyname>Apicella</keyname><forenames>Fabio</forenames></author><author><keyname>Sicca</keyname><forenames>Federico</forenames></author></authors><title>Classification of Autism Spectrum Disorder Using Supervised Learning of
  Brain Connectivity Measures Extracted from Synchrostates</title><categories>physics.med-ph cs.CV stat.AP stat.ML</categories><comments>27 pages, 17 figures</comments><journal-ref>Journal of Neural Engineering, Volume 11, Number 4, pp. 046019,
  August 2014</journal-ref><doi>10.1088/1741-2560/11/4/046019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective. The paper investigates the presence of autism using the functional
brain connectivity measures derived from electro-encephalogram (EEG) of
children during face perception tasks. Approach. Phase synchronized patterns
from 128-channel EEG signals are obtained for typical children and children
with autism spectrum disorder (ASD). The phase synchronized states or
synchrostates temporally switch amongst themselves as an underlying process for
the completion of a particular cognitive task. We used 12 subjects in each
group (ASD and typical) for analyzing their EEG while processing fearful, happy
and neutral faces. The minimal and maximally occurring synchrostates for each
subject are chosen for extraction of brain connectivity features, which are
used for classification between these two groups of subjects. Among different
supervised learning techniques, we here explored the discriminant analysis and
support vector machine both with polynomial kernels for the classification
task. Main results. The leave one out cross-validation of the classification
algorithm gives 94.7% accuracy as the best performance with corresponding
sensitivity and specificity values as 85.7% and 100% respectively.
Significance. The proposed method gives high classification accuracies and
outperforms other contemporary research results. The effectiveness of the
proposed method for classification of autistic and typical children suggests
the possibility of using it on a larger population to validate it for clinical
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7815</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7815</id><created>2014-10-28</created><authors><author><keyname>Quang-Hung</keyname><forenames>Nguyen</forenames></author><author><keyname>Thoai</keyname><forenames>Nam</forenames></author><author><keyname>Son</keyname><forenames>Nguyen Thanh</forenames></author><author><keyname>Le</keyname><forenames>Duy-Khanh</forenames></author></authors><title>Energy-Aware Lease Scheduling in Virtualized Data Centers</title><categories>cs.DC cs.NI cs.PF</categories><comments>10 pages, 2 figures, Proceedings of the Fifth International
  Conference on High Performance Scientific Computing, March 5-9, 2012, Hanoi,
  Vietnam</comments><acm-class>C.2.4; D.4.7</acm-class><journal-ref>Modeling, Simulation and Optimization of Complex Processes - HPSC
  2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Energy efficiency has become an important measurement of scheduling
algorithms in virtualized data centers. One of the challenges of
energy-efficient scheduling algorithms, however, is the trade-off between
minimizing energy consumption and satisfying quality of service (e.g.
performance, resource availability on time for reservation requests). We
consider resource needs in the context of virtualized data centers of a private
cloud system, which provides resource leases in terms of virtual machines (VMs)
for user applications. In this paper, we propose heuristics for scheduling VMs
that address the above challenge. On performance evaluation, simulated results
have shown a significant reduction on total energy consumption of our proposed
algorithms compared with an existing First-Come-First-Serve (FCFS) scheduling
algorithm with the same fulfillment of performance requirements. We also
discuss the improvement of energy saving when additionally using migration
policies to the above mentioned algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7827</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7827</id><created>2014-10-28</created><updated>2015-11-23</updated><authors><author><keyname>Cao</keyname><forenames>Yanshuai</forenames></author><author><keyname>Fleet</keyname><forenames>David J.</forenames></author></authors><title>Generalized Product of Experts for Automatic and Principled Fusion of
  Gaussian Process Predictions</title><categories>cs.LG cs.AI stat.ML</categories><comments>Modern Nonparametrics 3: Automating the Learning Pipeline workshop at
  NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a generalized product of experts (gPoE) framework
for combining the predictions of multiple probabilistic models. We identify
four desirable properties that are important for scalability, expressiveness
and robustness, when learning and inferring with a combination of multiple
models. Through analysis and experiments, we show that gPoE of Gaussian
processes (GP) have these qualities, while no other existing combination
schemes satisfy all of them at the same time. The resulting GP-gPoE is highly
scalable as individual GP experts can be independently learned in parallel;
very expressive as the way experts are combined depends on the input rather
than fixed; the combined prediction is still a valid probabilistic model with
natural interpretation; and finally robust to unreliable predictions from
individual experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7833</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7833</id><created>2014-10-28</created><updated>2014-11-01</updated><authors><author><keyname>Nazi</keyname><forenames>Azade</forenames></author><author><keyname>Zhou</keyname><forenames>Zhuojie</forenames></author><author><keyname>Thirumuruganathan</keyname><forenames>Saravanan</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Walk, Not Wait: Faster Sampling Over Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel, general purpose, technique for faster
sampling of nodes over an online social network. Specifically, unlike
traditional random walk which wait for the convergence of sampling distribution
to a predetermined target distribution - a waiting process that incurs a high
query cost - we develop WALK-ESTIMATE, which starts with a much shorter random
walk, and then proactively estimate the sampling probability for the node taken
before using acceptance-rejection sampling to adjust the sampling probability
to the predetermined target distribution. We present a novel backward random
walk technique which provides provably unbiased estimations for the sampling
probability, and demonstrate the superiority of WALK-ESTIMATE over traditional
random walks through theoretical analysis and extensive experiments over real
world online social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7835</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7835</id><created>2014-10-28</created><updated>2014-12-08</updated><authors><author><keyname>Schulte</keyname><forenames>Oliver</forenames></author><author><keyname>Qian</keyname><forenames>Zhensong</forenames></author><author><keyname>Kirkpatrick</keyname><forenames>Arthur E.</forenames></author><author><keyname>Yin</keyname><forenames>Xiaoqian</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author></authors><title>Fast Learning of Relational Dependency Networks</title><categories>cs.LG</categories><comments>17 pages, 2 figures, 3 tables, Accepted as long paper by ILP 2014,
  September 14- 16th, Nancy, France. Added the Appendix: Proof of Consistency
  Characterization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Relational Dependency Network (RDN) is a directed graphical model widely
used for multi-relational data. These networks allow cyclic dependencies,
necessary to represent relational autocorrelations. We describe an approach for
learning both the RDN's structure and its parameters, given an input relational
database: First learn a Bayesian network (BN), then transform the Bayesian
network to an RDN. Thus fast Bayes net learning can provide fast RDN learning.
The BN-to-RDN transform comprises a simple, local adjustment of the Bayes net
structure and a closed-form transform of the Bayes net parameters. This method
can learn an RDN for a dataset with a million tuples in minutes. We empirically
compare our approach to state-of-the art RDN learning methods that use
functional gradient boosting, on five benchmark datasets. Learning RDNs via BNs
scales much better to large datasets than learning RDNs with boosting, and
provides competitive accuracy in predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7842</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7842</id><created>2014-10-28</created><authors><author><keyname>Saito</keyname><forenames>Naoki</forenames></author><author><keyname>Woei</keyname><forenames>Ernest</forenames></author></authors><title>Tree simplification and the 'plateaux' phenomenon of graph Laplacian
  eigenvalues</title><categories>cs.DM</categories><msc-class>05C07, 05C50, 15A42, 65F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed a procedure of reducing the number of vertices and edges of a
given tree, which we call the &quot;tree simplification procedure,&quot; without changing
its topological information. Our motivation for developing this procedure was
to reduce computational costs of graph Laplacian eigenvalues of such trees.
When we applied this procedure to a set of trees representing dendritic
structures of retinal ganglion cells of a mouse and computed their graph
Laplacian eigenvalues, we observed two &quot;plateaux&quot; (i.e., two sets of multiple
eigenvalues) in the eigenvalue distribution of each such simplified tree. In
this article, after describing our tree simplification procedure, we analyze
why such eigenvalue plateaux occur in a simplified tree, and explain such
plateaux can occur in a more general graph if it satisfies a certain condition,
identify these two eigenvalues specifically as well as the lower bound to their
multiplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7849</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7849</id><created>2014-10-28</created><authors><author><keyname>Connor</keyname><forenames>A. M.</forenames></author></authors><title>A mutli-thread tabu search algorithm</title><categories>cs.AI</categories><journal-ref>Design Optimization: International Journal of Product and Process
  Improvement, 1(3), 293-304, 1999</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel refinement to a Tabu search algorithm that has
been implemented in an attempt to improve the robustness of the search when
applied to particularly complex problems. In this approach, two Tabu searches
are carried out in parallel. Each search thread is characterised by it's own
short term memory which forces that point out of local optima. However, the two
search threads share an intermediate term memory so allowing a degree of
information to be passed between them. Results are presented for both
unconstrained and constrained numerical functions as well as a problem in the
field of hydraulic circuit optimization. Simulation of hydraulic circuit
performance is achieved by linking the optimization algorithm to the commercial
simulation package Bathfp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7850</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7850</id><created>2014-10-28</created><authors><author><keyname>Benzm&#xfc;ller</keyname><forenames>Christoph</forenames></author><author><keyname>Paleo</keyname><forenames>Bruno Woltzenlogel</forenames></author></authors><title>Proceedings Eleventh Workshop on User Interfaces for Theorem Provers</title><categories>cs.LO cs.HC</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 167, 2014</journal-ref><doi>10.4204/EPTCS.167</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The UITP workshop series brings together researchers interested in designing,
developing and evaluating user interfaces for automated reasoning tools, such
as interactive proof assistants, automated theorem provers, model finders,
tools for formal methods, and tools for visualising and manipulating logical
formulas and proofs. The eleventh edition of UITP took place in Vienna,
Austria, and was part of the Vienna Summer of Logic, the largest ever joint
conference in the area of Logic. This proceedings contains the eight
contributed papers that were accepted for presentation at the workshop as well
as the two invited papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7851</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7851</id><created>2014-10-28</created><authors><author><keyname>Connor</keyname><forenames>Andy M.</forenames></author><author><keyname>Seffen</keyname><forenames>Keith A.</forenames></author><author><keyname>Parks</keyname><forenames>Geoffrey T.</forenames></author><author><keyname>Clarkson</keyname><forenames>P. John</forenames></author></authors><title>Efficient optimisation of structures using tabu search</title><categories>cs.CE cs.AI</categories><journal-ref>Connor, A.M., Seffen, K.A., Clarkson, P.J. &amp; Parks, G.T. (1999)
  &quot;Efficient optimisation of structures using tabu search&quot; Proceedings of the
  1st ASMO/ISSMO Conference on Engineering Design Optimization, 127-134</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to the optimisation of structures using
a Tabu search (TS) method. TS is a metaheuristic which is used to guide local
search methods towards a globally optimal solution by using flexible memory
cycles of differing time spans. Results are presented for the well established
ten bar truss problem and compared to results published in the literature. In
the first example a truss is optimised to minimise mass and the results
compared to results obtained using an alternative TS implementation. In the
second example, the problem has multiple objectives that are compounded into a
single objective function value using game theory. In general the results
demonstrate that the TS method is capable of solving structural optimisation
problems at least as efficiently as other numerical optimisation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7852</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7852</id><created>2014-10-28</created><authors><author><keyname>Zhao</keyname><forenames>Xiaoting</forenames></author><author><keyname>Frazier</keyname><forenames>Peter I.</forenames></author></authors><title>A Markov Decision Process Analysis of the Cold Start Problem in Bayesian
  Information Filtering</title><categories>cs.LG cs.IR math.OC</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the information filtering problem, in which we face a stream of
items, and must decide which ones to forward to a user to maximize the number
of relevant items shown, minus a penalty for each irrelevant item shown.
Forwarding decisions are made separately in a personalized way for each user.
We focus on the cold-start setting for this problem, in which we have limited
historical data on the user's preferences, and must rely on feedback from
forwarded articles to learn which the fraction of items relevant to the user in
each of several item categories. Performing well in this setting requires
trading exploration vs. exploitation, forwarding items that are likely to be
irrelevant, to allow learning that will improve later performance. In a
Bayesian setting, and using Markov decision processes, we show how the
Bayes-optimal forwarding algorithm can be computed efficiently when the user
will examine each forwarded article, and how an upper bound on the
Bayes-optimal procedure and a heuristic index policy can be obtained for the
setting when the user will examine only a limited number of forwarded items. We
present results from simulation experiments using parameters estimated using
historical data from arXiv.org.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7856</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7856</id><created>2014-10-28</created><authors><author><keyname>Soufiani</keyname><forenames>Hossein Azari</forenames></author><author><keyname>Parkes</keyname><forenames>David C.</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>A Statistical Decision-Theoretic Framework for Social Choice</title><categories>cs.AI cs.MA</categories><comments>Full version of a NIPS-14 paper under the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we take a statistical decision-theoretic viewpoint on social
choice, putting a focus on the decision to be made on behalf of a system of
agents. In our framework, we are given a statistical ranking model, a decision
space, and a loss function defined on (parameter, decision) pairs, and
formulate social choice mechanisms as decision rules that minimize expected
loss. This suggests a general framework for the design and analysis of new
social choice mechanisms. We compare Bayesian estimators, which minimize
Bayesian expected loss, for the Mallows model and the Condorcet model
respectively, and the Kemeny rule. We consider various normative properties, in
addition to computational complexity and asymptotic behavior. In particular, we
show that the Bayesian estimator for the Condorcet model satisfies some desired
properties such as anonymity, neutrality, and monotonicity, can be computed in
polynomial time, and is asymptotically different from the other two rules when
the data are generated from the Condorcet model for some ground truth
parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7867</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7867</id><created>2014-10-28</created><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Cheng</keyname><forenames>Aolin</forenames></author><author><keyname>Yu</keyname><forenames>Yuling</forenames></author><author><keyname>Wang</keyname><forenames>Chonggang</forenames></author></authors><title>Resource Allocation Optimization for Delay-Sensitive Traffic in
  Fronthaul Constrained Cloud Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud radio access network (C-RAN) provides high spectral and energy
efficiency performances, low expenditures and intelligent centralized system
structures to operators, which has attracted intense interests in both academia
and industry. In this paper, a hybrid coordinated multi-point transmission
(H-CoMP) scheme is designed for the downlink transmission in C-RANs, which
fulfills the flexible tradeoff between cooperation gain and fronthaul
consumption. The queue-aware power and rate allocation with constraints of
average fronthaul consumption for the delay-sensitive traffic are formulated as
an infinite horizon constrained partially observed Markov decision process
(POMDP), which takes both the urgent queue state information (QSI) and the
imperfect channel state information at transmitters (CSIT) into account. To
deal with the curse of dimensionality involved with the equivalent Bellman
equation, the linear approximation of post-decision value functions is
utilized. A stochastic gradient algorithm is presented to allocate the
queue-aware power and transmission rate with H-CoMP, which is robust against
unpredicted traffic arrivals and uncertainties caused by the imperfect CSIT.
Furthermore, to substantially reduce the computing complexity, an online
learning algorithm is proposed to estimate the per-queue post-decision value
functions and update the Lagrange multipliers. The simulation results
demonstrate performance gains of the proposed stochastic gradient algorithms,
and confirm the asymptotical convergence of the proposed online learning
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7868</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7868</id><created>2014-10-28</created><authors><author><keyname>Scott</keyname><forenames>Paul</forenames></author><author><keyname>Thi&#xe9;baux</keyname><forenames>Sylvie</forenames></author></authors><title>Dynamic Optimal Power Flow in Microgrids using the Alternating Direction
  Method of Multipliers</title><categories>math.OC cs.SY</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart devices, storage and other distributed technologies have the potential
to greatly improve the utilisation of network infrastructure and renewable
generation. Decentralised control of these technologies overcomes many
scalability and privacy concerns, but in general still requires the underlying
problem to be convex in order to guarantee convergence to a global optimum.
Considering that AC power flows are non-convex in nature, and the operation of
household devices often requires discrete decisions, there has been uncertainty
surrounding the use of distributed methods in a realistic setting. This paper
extends prior work on the alternating direction method of multipliers (ADMM)
for solving the dynamic optimal power flow (D-OPF) problem. We utilise more
realistic line and load models, and introduce a two-stage approach to managing
discrete decisions and uncertainty. Our experiments on a suburb-sized microgrid
show that this approach provides near optimal results, in a time that is fast
enough for receding horizon control. This work brings distributed control of
smart-grid technologies closer to reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7871</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7871</id><created>2014-10-29</created><authors><author><keyname>Friedmann</keyname><forenames>Oliver</forenames></author><author><keyname>Hansen</keyname><forenames>Thomas Dueholm</forenames></author><author><keyname>Zwick</keyname><forenames>Uri</forenames></author></authors><title>Errata for: A subexponential lower bound for the Random Facet algorithm
  for Parity Games</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Friedmann, Hansen, and Zwick (2011) we claimed that the expected number of
pivoting steps performed by the Random-Facet algorithm of Kalai and of
Matousek, Sharir, and Welzl is equal to the expected number of pivoting steps
performed by Random-Facet^*, a variant of Random-Facet that bases its random
decisions on one random permutation. We then obtained a lower bound on the
expected number of pivoting steps performed by Random-Facet^* and claimed that
the same lower bound holds also for Random-Facet. Unfortunately, the claim that
the expected numbers of steps performed by Random-Facet and Random-Facet^* are
the same is false. We provide here simple examples that show that the expected
numbers of steps performed by the two algorithms are not the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7876</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7876</id><created>2014-10-29</created><authors><author><keyname>Dao</keyname><forenames>Minh</forenames></author><author><keyname>Nguyen</keyname><forenames>Nam H.</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Nasser M.</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Collaborative Multi-sensor Classification via Sparsity-based
  Representation</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a general collaborative sparse representation
framework for multi-sensor classification, which takes into account the
correlations as well as complementary information between heterogeneous sensors
simultaneously while considering joint sparsity within each sensor's
observations. We also robustify our models to deal with the presence of sparse
noise and low-rank interference signals. Specifically, we demonstrate that
incorporating the noise or interference signal as a low-rank component in our
models is essential in a multi-sensor classification problem when multiple
co-located sources/sensors simultaneously record the same physical event. We
further extend our frameworks to kernelized models which rely on sparsely
representing a test sample in terms of all the training samples in a feature
space induced by a kernel function. A fast and efficient algorithm based on
alternative direction method is proposed where its convergence to optimal
solution is guaranteed. Extensive experiments are conducted on real data sets
collected by researchers at the U.S. Army Research Laboratory and the results
are compared with the conventional classifiers to verify the effectiveness of
the proposed methods in the application of automatic multi-sensor border patrol
control, where we often have to discriminate between human and animal
footsteps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7881</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7881</id><created>2014-10-29</created><authors><author><keyname>Santurkar</keyname><forenames>Shibani</forenames></author><author><keyname>Rajendran</keyname><forenames>Bipin</forenames></author></authors><title>A neural circuit for navigation inspired by C. elegans Chemotaxis</title><categories>cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an artificial neural circuit for contour tracking and navigation
inspired by the chemotaxis of the nematode Caenorhabditis elegans. In order to
harness the computational advantages spiking neural networks promise over their
non-spiking counterparts, we develop a network comprising 7-spiking neurons
with non-plastic synapses which we show is extremely robust in tracking a range
of concentrations. Our worm uses information regarding local temporal gradients
in sodium chloride concentration to decide the instantaneous path for foraging,
exploration and tracking. A key neuron pair in the C. elegans chemotaxis
network is the ASEL &amp; ASER neuron pair, which capture the gradient of
concentration sensed by the worm in their graded membrane potentials. The
primary sensory neurons for our network are a pair of artificial spiking
neurons that function as gradient detectors whose design is adapted from a
computational model of the ASE neuron pair in C. elegans. Simulations show that
our worm is able to detect the set-point with approximately four times higher
probability than the optimal memoryless Levy foraging model. We also show that
our spiking neural network is much more efficient and noise-resilient while
navigating and tracking a contour, as compared to an equivalent non-spiking
network. We demonstrate that our model is extremely robust to noise and with
slight modifications can be used for other practical applications such as
obstacle avoidance. Our network model could also be extended for use in
three-dimensional contour tracking or obstacle avoidance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7883</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7883</id><created>2014-10-29</created><authors><author><keyname>Santurkar</keyname><forenames>Shibani</forenames></author><author><keyname>Rajendran</keyname><forenames>Bipin</forenames></author></authors><title>Sub-threshold CMOS Spiking Neuron Circuit Design for Navigation Inspired
  by C. elegans Chemotaxis</title><categories>cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate a spiking neural network for navigation motivated by the
chemotaxis network of Caenorhabditis elegans. Our network uses information
regarding temporal gradients in the tracking variable's concentration to make
navigational decisions. The gradient information is determined by mimicking the
underlying mechanisms of the ASE neurons of C. elegans. Simulations show that
our model is able to forage and track a target set-point in extremely noisy
environments. We develop a VLSI implementation for the main gradient detector
neurons, which could be integrated with standard comparator circuitry to
develop a robust circuit for navigation and contour tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7890</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7890</id><created>2014-10-29</created><authors><author><keyname>Atan</keyname><forenames>Onur</forenames></author><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Global Bandits with Holder Continuity</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard Multi-Armed Bandit (MAB) problems assume that the arms are
independent. However, in many application scenarios, the information obtained
by playing an arm provides information about the remainder of the arms. Hence,
in such applications, this informativeness can and should be exploited to
enable faster convergence to the optimal solution. In this paper, we introduce
and formalize the Global MAB (GMAB), in which arms are globally informative
through a global parameter, i.e., choosing an arm reveals information about all
the arms. We propose a greedy policy for the GMAB which always selects the arm
with the highest estimated expected reward, and prove that it achieves bounded
parameter-dependent regret. Hence, this policy selects suboptimal arms only
finitely many times, and after a finite number of initial time steps, the
optimal arm is selected in all of the remaining time steps with probability
one. In addition, we also study how the informativeness of the arms about each
other's rewards affects the speed of learning. Specifically, we prove that the
parameter-free (worst-case) regret is sublinear in time, and decreases with the
informativeness of the arms. We also prove a sublinear in time Bayesian risk
bound for the GMAB which reduces to the well-known Bayesian risk bound for
linearly parameterized bandits when the arms are fully informative. GMABs have
applications ranging from drug and treatment discovery to dynamic pricing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7895</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7895</id><created>2014-10-29</created><authors><author><keyname>Heren</keyname><forenames>Akif Cem</forenames></author><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Tugcu</keyname><forenames>Tuna</forenames></author></authors><title>Effect of Degradation in Molecular Communication: Impairment or
  Enhancement?</title><categories>cs.ET cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the nanonetworking literature, many solutions have been suggested to
enable the nanomachine-to-nanomachine communication. Among these solutions, we
focus on what constitutes the basis for molecular communication paradigms
--molecular communication via diffusion (MCvD). In this paper, we start with an
analytical modeling of a spherical absorbing receiver under messenger molecule
degradation and show that our formulations are in agreement with the simulation
results of a similar topology. Next, we identify how such signal
characteristics as pulse peak time and pulse amplitude are affected by
degradation. Indeed, we show analytically how in MCvD, signal shaping is
achieved through degradation. We also compare communication under messenger
molecule degradation with the case of no-degradation and electromagnetic
communication in terms of channel characteristics. Lastly, we evaluate the
communication performance of the scenarios having various degradation rates.
Here, we assess the system performance according to traditional network metrics
such as the level of inter-symbol interference, detection performance, bit
error rate, and channel capacity. Our results indicate that introducing
degradation significantly improves the system performance when the rate of
degradation is appropriately selected. We make a thorough analysis of the
communication scenario by taking into account different detection thresholds,
symbol durations, and communication distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7912</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7912</id><created>2014-10-29</created><authors><author><keyname>M&#xe4;cker</keyname><forenames>Alexander</forenames></author><author><keyname>Malatyali</keyname><forenames>Manuel</forenames></author><author><keyname>der Heide</keyname><forenames>Friedhelm Meyer auf</forenames></author></authors><title>Online Top-k-Position Monitoring of Distributed Data Streams</title><categories>cs.DS</categories><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider n nodes connected to a single coordinator. Each node receives an
individual online data stream of numbers and, at any point in time, the
coordinator has to know the k nodes currently observing the largest values, for
a given k between 1 and n. We design and analyze an algorithm that solves this
problem while bounding the amount of messages exchanged between the nodes and
the coordinator. Our algorithm employs the idea of using filters which,
intuitively speaking, leads to few messages to be sent, if the new input is
&quot;similar&quot; to the previous ones. The algorithm uses a number of messages that is
on expectation by a factor of O((log {\Delta} + k) log n) larger than that of
an offline algorithm that sets filters in an optimal way, where {\Delta} is
upper bounded by the largest value observed by any node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7918</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7918</id><created>2014-10-29</created><authors><author><keyname>Movahednasab</keyname><forenames>Mohammad</forenames></author><author><keyname>Soleimanifar</keyname><forenames>Mehdi</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Kenari</keyname><forenames>Masoumeh Nasiri</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Adaptive Molecule Transmission Rate for Diffusion Based Molecular
  Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a simple memory limited transmitter for molecular
communication is proposed, in which information is encoded in the diffusion
rate of the molecules. Taking advantage of memory, the proposed transmitter
reduces the ISI problem by properly adjusting its diffusion rate. The error
probability of the proposed scheme is derived and the result is compared with
the lower bound on error probability of the optimum transmitter. It is shown
that the performance of introduced transmitter is near optimal (under certain
simplifications). Simplicity is the key feature of the presented communication
system: the transmitter follows a simple rule, the receiver is a simple
threshold decoder and only one type of molecule is used to convey the
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7921</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7921</id><created>2014-10-29</created><updated>2015-04-29</updated><authors><author><keyname>Musco</keyname><forenames>Vincenzo</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author><author><keyname>Preux</keyname><forenames>Philippe</forenames></author></authors><title>A Generative Model of Software Dependency Graphs to Better Understand
  Software Evolution</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software systems are composed of many interacting elements. A natural way to
abstract over software systems is to model them as graphs. In this paper we
consider software dependency graphs of object-oriented software and we study
one topological property: the degree distribution. Based on the analysis of ten
software systems written in Java, we show that there exists completely
different systems that have the same degree distribution. Then, we propose a
generative model of software dependency graphs which synthesizes graphs whose
degree distribution is close to the empirical ones observed in real software
systems. This model gives us novel insights on the potential fundamental rules
of software evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7922</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7922</id><created>2014-10-29</created><authors><author><keyname>Mozerov</keyname><forenames>Mikhail G.</forenames></author></authors><title>Extended Dynamic Programming and Fast Multidimensional Search Algorithm
  for Energy Minization in Stereo and Motion</title><categories>cs.CV</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel extended dynamic programming approach for energy
minimization (EDP) to solve the correspondence problem for stereo and motion. A
significant speedup is achieved using a recursive minimum search strategy
(RMS). The mentioned speedup is particularly important if the disparity space
is 2D as well as 3D. The proposed RMS can also be applied in the well-known
dynamic programming (DP) approach for stereo and motion. In this case, the
general 2D problem of the global discrete energy minimization is reduced to
several mutually independent sub-problems of the one-dimensional minimization.
The EDP method is used when the approximation of the general 2D discrete energy
minimization problem is considered. Then the RMS algorithm is an essential part
of the EDP method. Using the EDP algorithm we obtain a lower energy bound than
the graph cuts (GC) expansion technique on stereo and motion problems. The
proposed calculation scheme possesses natural parallelism and can be realized
on graphics processing unit (GPU) platforms, and can be potentially restricted
further by the number of scanlines in the image plane. Furthermore, the RMS and
EDP methods can be used in any optimization problem where the objective
function meets specific conditions in the smoothness term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7923</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7923</id><created>2014-10-29</created><updated>2015-02-12</updated><authors><author><keyname>Mikkelsen</keyname><forenames>Jesper W.</forenames></author></authors><title>Optimal Online Edge Coloring of Planar Graphs with Advice</title><categories>cs.DS</categories><comments>CIAC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the framework of advice complexity, we study the amount of knowledge
about the future that an online algorithm needs to color the edges of a graph
optimally, i.e., using as few colors as possible. For graphs of maximum degree
$\Delta$, it follows from Vizing's Theorem that $O(m\log \Delta)$ bits of
advice suffice to achieve optimality, where $m$ is the number of edges. We show
that for graphs of bounded degeneracy (a class of graphs including e.g. trees
and planar graphs), only $O(m)$ bits of advice are needed to compute an optimal
solution online, independently of how large $\Delta$ is. On the other hand, we
show that $\Omega (m)$ bits of advice are necessary just to achieve a
competitive ratio better than that of the best deterministic online algorithm
without advice. Furthermore, we consider algorithms which use a fixed number of
advice bits per edge (our algorithm for graphs of bounded degeneracy belongs to
this class of algorithms). We show that for bipartite graphs, any such
algorithm must use at least $\Omega(m\log \Delta)$ bits of advice to achieve
optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7924</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7924</id><created>2014-10-29</created><authors><author><keyname>Sanabria-Russo</keyname><forenames>Luis</forenames></author><author><keyname>Gringoli</keyname><forenames>Francesco</forenames></author><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>Implementation and Experimental Evaluation of a Collision-Free MAC
  Protocol for WLANs</title><categories>cs.NI</categories><comments>This paper was submitted to the IEEE International Conference on
  Communications 2015 and it is waiting for approval</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Collisions are a main cause of throughput degradation in Wireless LANs. The
current contention mechanism for these networks is based on a random backoff
strategy to avoid collisions with other transmitters. Even though it can reduce
the probability of collisions, the random backoff prevents users from achieving
Collision-Free schedules, where the channel would be used more efficiently.
Modifying the contention mechanism by waiting for a deterministic timer after
successful transmissions, users would be able to construct a Collision-Free
schedule among successful contenders. This work shows the experimental results
of a Collision-Free MAC (CF-MAC) protocol for WLANs using commercial hardware
and open firmware for wireless network cards which is able to support many
users. Testbed results show that the proposed CF-MAC protocol leads to a better
distribution of the available bandwidth among users, higher throughput and
lower losses than the unmodified WLANs clients using a legacy firmware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7927</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7927</id><created>2014-10-29</created><authors><author><keyname>Davtyan</keyname><forenames>Narine N.</forenames></author><author><keyname>Khachatryan</keyname><forenames>Arpine M.</forenames></author><author><keyname>Kamalian</keyname><forenames>Rafayel R.</forenames></author></authors><title>A Description of the Subgraph Induced at a Labeling of a Graph by the
  Subset of Vertices with an Interval Spectrum</title><categories>cs.DM math.CO</categories><msc-class>05C15, 05C78</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sets of vertices and edges of an undirected, simple, finite, connected
graph $G$ are denoted by $V(G)$ and $E(G)$, respectively. An arbitrary nonempty
finite subset of consecutive integers is called an interval. An injective
mapping $\varphi:E(G)\rightarrow \{1,2,...,|E(G)|\}$ is called a labeling of
the graph $G$. If $G$ is a graph, $x$ is its arbitrary vertex, and $\varphi$ is
its arbitrary labeling, then the set $S_G(x,\varphi)\equiv\{\varphi(e)/ e\in
E(G), e \textrm{is incident with} x$\} is called a spectrum of the vertex $x$
of the graph $G$ at its labeling $\varphi$. For any graph $G$ and its arbitrary
labeling $\varphi$, a structure of the subgraph of $G$, induced by the subset
of vertices of $G$ with an interval spectrum, is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7930</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7930</id><created>2014-10-29</created><authors><author><keyname>Keimel</keyname><forenames>Klaus</forenames></author></authors><title>On the equivalence of state transformer semantics and predicate
  transformer semantics</title><categories>cs.LO</categories><journal-ref>Proceedings of the Workshop Informatics and Information
  Technologies in Education: Theory, Practice, Didactics, Novosibirsk, vol. 1
  (2012), 78--104</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  G. Plotkin and the author have worked out the equivalence between state
transformer semantics and predicate transformer semantics in a domain
theoretical setting for programs combining nondeterminism and probability.
Works of C. Morgan and co-authors, Keimel, Rosenbusch and Streicher, already go
in the same direction using only discrete state spaces. It is the aim of this
paper to exhibit a general framework in which one can hope that state
transformer semantics and predicate transformer semantics are equivalent. We
use a notion of entropicity borrowed from universal algebra and a relaxed
setting adapted to the domain theoretical situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7942</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7942</id><created>2014-10-29</created><authors><author><keyname>Kochemazov</keyname><forenames>Stepan</forenames></author><author><keyname>Semenov</keyname><forenames>Alexander</forenames></author></authors><title>Using synchronous Boolean networks to model several phenomena of
  collective behavior</title><categories>cs.SI cs.AI cs.MA</categories><doi>10.1371/journal.pone.0115156</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an approach for modeling and analysis of a number
of phenomena of collective behavior. By collectives we mean multi-agent systems
that transition from one state to another at discrete moments of time. The
behavior of a member of a collective (agent) is called conforming if the
opinion of this agent at current time moment conforms to the opinion of some
other agents at the previous time moment. We presume that at each moment of
time every agent makes a decision by choosing from the set {0,1} (where
1-decision corresponds to action and 0-decision corresponds to inaction). In
our approach we model collective behavior with synchronous Boolean networks. We
presume that in a network there can be agents that act at every moment of time.
Such agents are called instigators. Also there can be agents that never act.
Such agents are called loyalists. Agents that are neither instigators nor
loyalists are called simple agents. We study two combinatorial problems. The
first problem is to find a disposition of instigators that in several time
moments transforms a network from a state where a majority of simple agents are
inactive to a state with a majority of active agents. The second problem is to
find a disposition of loyalists that returns the network to a state with a
majority of inactive agents. Similar problems are studied for networks in which
simple agents demonstrate the contrary to conforming behavior that we call
anticonforming. We obtained several theoretical results regarding the behavior
of collectives of agents with conforming or anticonforming behavior. In
computational experiments we solved the described problems for randomly
generated networks with several hundred vertices. We reduced corresponding
combinatorial problems to the Boolean satisfiability problem (SAT) and used
modern SAT solvers to solve the instances obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7953</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7953</id><created>2014-10-29</created><authors><author><keyname>Motz</keyname><forenames>Regina</forenames></author><author><keyname>Rohrer</keyname><forenames>Edelweis</forenames></author><author><keyname>Severi</keyname><forenames>Paula</forenames></author></authors><title>Reasoning for ALCQ extended with a flexible meta-modelling hierarchy</title><categories>cs.AI</categories><comments>This is the long version of the paper submitted to JIST2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This works is motivated by a real-world case study where it is necessary to
integrate and relate existing ontologies through meta- modelling. For this, we
introduce the Description Logic ALCQM which is obtained from ALCQ by adding
statements that equate individuals to concepts in a knowledge base. In this new
extension, a concept can be an individual of another concept (called
meta-concept) which themselves can be individuals of yet another concept
(called meta meta-concept) and so on. We define a tableau algorithm for
checking consistency of an ontology in ALCQM and prove its correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7955</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7955</id><created>2014-10-29</created><authors><author><keyname>Ding</keyname><forenames>Jie</forenames></author><author><keyname>Wang</keyname><forenames>Min-Yi</forenames></author><author><keyname>Wang</keyname><forenames>Qiao</forenames></author><author><keyname>Zhu</keyname><forenames>Xin-Shan</forenames></author></authors><title>A novel wireless sensor network topology with fewer links</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper, based on $k$-NN graph, presents symmetric $(k,j)$-NN graph $(1
\leq j &lt; k)$, a brand new topology which could be adopted by a series of
network-based structures. We show that the $k$ nearest neighbors of a node
exert disparate influence on guaranteeing network connectivity, and connections
with the farthest $j$ ones among these $k$ neighbors are competent to build up
a connected network, contrast to the current popular strategy of connecting all
these $k$ neighbors. In particular, for a network with node amount $n$ up to
$10^3$, as experiments demonstrate, connecting with the farthest three, rather
than all, of the five nearest neighbor nodes, i.e. $(k,j)=(5,3)$, can guarantee
the network connectivity in high probabilities. We further reveal that more
than $0.75n$ links or edges in $5$-NN graph are not necessary for the
connectivity. Moreover, a composite topology combining symmetric $(k,j)$-NN and
random geometric graph (RGG) is constructed for constrained transmission radii
in wireless sensor networks (WSNs) application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7967</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7967</id><created>2014-10-29</created><authors><author><keyname>Lim</keyname><forenames>Chia Wei</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>Technical Report: Compressive Temporal Higher Order Cyclostationary
  Statistics</title><categories>cs.IT math.IT stat.OT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of nonlinear transformations to a cyclostationary signal for
the purpose of revealing hidden periodicities has proven to be useful for
applications requiring signal selectivity and noise tolerance. The fact that
the hidden periodicities, referred to as cyclic moments, are often compressible
in the Fourier domain motivates the use of compressive sensing (CS) as an
efficient acquisition protocol for capturing such signals. In this work, we
consider the class of Temporal Higher Order Cyclostationary Statistics (THOCS)
estimators when CS is used to acquire the cyclostationary signal assuming
compressible cyclic moments in the Fourier domain. We develop a theoretical
framework for estimating THOCS using the low-rate nonuniform sampling protocol
from CS and illustrate the performance of this framework using simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7990</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7990</id><created>2014-10-27</created><authors><author><keyname>Michelfeit</keyname><forenames>Jan</forenames></author><author><keyname>Knap</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Ne&#x10d;ask&#xfd;</keyname><forenames>Martin</forenames></author></authors><title>Linked Data Integration with Conflicts</title><categories>cs.DB cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linked Data have emerged as a successful publication format and one of its
main strengths is its fitness for integration of data from multiple sources.
This gives them a great potential both for semantic applications and the
enterprise environment where data integration is crucial. Linked Data
integration poses new challenges, however, and new algorithms and tools
covering all steps of the integration process need to be developed. This paper
explores Linked Data integration and its specifics. We focus on data fusion and
conflict resolution: two novel algorithms for Linked Data fusion with
provenance tracking and quality assessment of fused data are proposed. The
algorithms are implemented as part of the ODCleanStore framework and evaluated
on real Linked Open Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.7998</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.7998</id><created>2014-10-17</created><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Leijnen</keyname><forenames>Stefan</forenames></author></authors><title>The Relationship between Creativity, Imitation, and Cultural Diversity</title><categories>q-bio.NC cs.CY cs.MA cs.SI</categories><comments>13 pages. arXiv admin note: substantial text overlap with
  arXiv:0911.2390, arXiv:1005.1516, arXiv:1310.3781, arXiv:1310.0522</comments><journal-ref>International Journal of Software and Informatics, 7(4), 615-627
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are both benefits and drawbacks to cultural diversity. It can lead to
friction and exacerbate differences. However, as with biological diversity,
cultural diversity is valuable in times of upheaval; if a previously effective
solution no longer works, it is good to have alternatives available. What
factors give rise to cultural diversity? This paper describes a preliminary
investigation of this question using a computational model of cultural
evolution. The model is composed of neural network based agents that evolve
fitter ideas for actions by (1) inventing new ideas through modification of
existing ones, and (2) imitating neighbors' ideas. Numerical simulations
indicate that the diversity of ideas in a population is positively correlated
with both the proportion of creators to imitators in the population, and the
rate at which creators create. This is the case for both minimum and peak
diversity of actions over the duration of a run.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8007</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8007</id><created>2014-10-29</created><authors><author><keyname>Patra</keyname><forenames>Chiranjib</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Samiran</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Matangini</forenames></author><author><keyname>Bhaumik</keyname><forenames>Parama</forenames></author></authors><title>Understanding the Mechanics of Some Localized Protocols by Theory of
  Complex Networks</title><categories>cs.NI</categories><comments>13 pages</comments><msc-class>34</msc-class><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of ad hoc sensor networks, clustering plays an important role in
energy conservation therefore analyzing the mechanics of such topology can be
helpful to make logistic decisions .Using the theory of complex network the
topological model is extended, where we account for the probability of
preferential attachment and anti preferential attachment policy of sensor nodes
to analyze the formation of clusters and calculate the probability of
clustering. The theoretical analysis is conducted to determine nature of
topology and quantify some of the observed facts during the execution of
topology control protocols. The quantification of the observed facts leads to
the alternative understanding of the energy efficiency of the routing
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8023</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8023</id><created>2014-10-29</created><updated>2015-05-04</updated><authors><author><keyname>Williamson</keyname><forenames>Adam R.</forenames></author><author><keyname>Chen</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Variable-length Convolutional Coding for Short Blocklengths with
  Decision Feedback</title><categories>cs.IT math.IT</categories><comments>Accepted for publication to IEEE Transactions on Communications. 15
  single-spaced, double-column pages; 8 figures; 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a variable-length decision-feedback scheme that uses
tail-biting convolutional codes and the tail-biting Reliability-Output Viterbi
Algoritm (ROVA). Comparing with recent results in finite-blocklength
information theory, simulation results for both the BSC and the AWGN channel
show that the decision-feedback scheme using ROVA can surpass the random-coding
lower bound on throughput for feedback codes at average blocklengths less than
100 symbols. This paper explores ROVA-based decision feedback both with
decoding after every symbol and with decoding limited to a small number of
increments. The performance of the reliability-based stopping rule with the
ROVA is compared to retransmission decisions based on CRCs. For short
blocklengths where the latency overhead of the CRC bits is severe, the
ROVA-based approach delivers superior rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8024</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8024</id><created>2014-10-29</created><authors><author><keyname>Duan</keyname><forenames>Ruifeng</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author><author><keyname>Elmusrati</keyname><forenames>Mohammed S.</forenames></author></authors><title>Performance Analysis of Mean Value-Based Power Allocation with Primary
  User Interference in Spectrum Sharing Systems</title><categories>cs.IT math.IT</categories><comments>14 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide an exact expression for the ergodic capacity of the
secondary user, and a unified closed-form expression for its bounds with taking
the consideration of the primary interference at the secondary receiver. In
addition, a simple but accurate approximation of the the ergodic capacity of
the primary user is presented. Moreover, a primary user capacity loss based
power allocation scheme for the secondary user is also proposed. Finally, we
compare the performance of the two power allocation schemes in terms of the sum
capacity. The results are validated through using numerical results and
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8027</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8027</id><created>2014-10-29</created><updated>2015-05-05</updated><authors><author><keyname>Malinowski</keyname><forenames>Mateusz</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>Towards a Visual Turing Challenge</title><categories>cs.AI cs.CL cs.CV cs.GL cs.LG</categories><comments>Published in the NIPS 2014 Workshop on Learning Semantics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As language and visual understanding by machines progresses rapidly, we are
observing an increasing interest in holistic architectures that tightly
interlink both modalities in a joint learning and inference process. This trend
has allowed the community to progress towards more challenging and open tasks
and refueled the hope at achieving the old AI dream of building machines that
could pass a turing test in open domains. In order to steadily make progress
towards this goal, we realize that quantifying performance becomes increasingly
difficult. Therefore we ask how we can precisely define such challenges and how
we can evaluate different algorithms on this open tasks? In this paper, we
summarize and discuss such challenges as well as try to give answers where
appropriate options are available in the literature. We exemplify some of the
solutions on a recently presented dataset of question-answering task based on
real-world indoor images that establishes a visual turing challenge. Finally,
we argue despite the success of unique ground-truth annotation, we likely have
to step away from carefully curated dataset and rather rely on 'social
consensus' as the main driving force to create suitable benchmarks. Providing
coverage in this inherently ambiguous output space is an emerging challenge
that we face in order to make quantifiable progress in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8034</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8034</id><created>2014-10-29</created><authors><author><keyname>Liu</keyname><forenames>Xudong</forenames></author><author><keyname>Zhang</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Ting</forenames></author><author><keyname>Liu</keyname><forenames>Chang</forenames></author></authors><title>Latent Feature Based FM Model For Rating Prediction</title><categories>cs.LG cs.IR stat.ML</categories><comments>4 pages, 3 figures, Large Scale Recommender Systems:workshop of
  Recsys 2014</comments><msc-class>68-XX</msc-class><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rating Prediction is a basic problem in Recommender System, and one of the
most widely used method is Factorization Machines(FM). However, traditional
matrix factorization methods fail to utilize the benefit of implicit feedback,
which has been proved to be important in Rating Prediction problem. In this
work, we consider a specific situation, movie rating prediction, where we
assume that watching history has a big influence on his/her rating behavior on
an item. We introduce two models, Latent Dirichlet Allocation(LDA) and
word2vec, both of which perform state-of-the-art results in training latent
features. Based on that, we propose two feature based models. One is the
Topic-based FM Model which provides the implicit feedback to the matrix
factorization. The other is the Vector-based FM Model which expresses the order
info of watching history. Empirical results on three datasets demonstrate that
our method performs better than the baseline model and confirm that
Vector-based FM Model usually works better as it contains the order info.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8039</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8039</id><created>2014-10-29</created><authors><author><keyname>Sassi</keyname><forenames>Aymen</forenames></author><author><keyname>Charfi</keyname><forenames>Faiza</forenames></author><author><keyname>Kamoun</keyname><forenames>Lotfi</forenames></author><author><keyname>Elhillali</keyname><forenames>Yassin</forenames></author><author><keyname>Rivenq</keyname><forenames>Atika</forenames></author></authors><title>OFDM Transmission Performance Evaluation in V2X Communication</title><categories>cs.NI</categories><comments>8 pages, 17 Figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 2, No 3, March 2012 pages 141-148</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Vehicle to Vehicle and Vehicle to Infrastructure V2X communication
systems are one of the main topics in research domain. Its performance
evaluation is an important step before their on board integration into vehicles
and its probable real deployment. This paper studies the physical layer PHY of
the upcoming vehicular communication standard IEEE 802.11p. This standard PHY
Layer model, with much associated phenomena, is implemented in V2V and V2I,
situations through different scenarios. The series of simulation results
carried out, perform data exchange between high speed vehicles over different
channels models and different transmitted packet size. We underline several
propagation channel and other important parameters, which affect both the
physical layer network performance and the QoT. The Bit Error Rate BER versus
Signal to Noise Ratio SNR of all coding rates is used to evaluate the
performance of the communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8043</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8043</id><created>2014-10-29</created><authors><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Kumar</keyname><forenames>Abhimanu</forenames></author><author><keyname>Wei</keyname><forenames>Jinliang</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Gibson</keyname><forenames>Garth</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>High-Performance Distributed ML at Scale through Parameter Server
  Consistency Models</title><categories>cs.LG stat.ML</categories><comments>19 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As Machine Learning (ML) applications increase in data size and model
complexity, practitioners turn to distributed clusters to satisfy the increased
computational and memory demands. Unfortunately, effective use of clusters for
ML requires considerable expertise in writing distributed code, while
highly-abstracted frameworks like Hadoop have not, in practice, approached the
performance seen in specialized ML implementations. The recent Parameter Server
(PS) paradigm is a middle ground between these extremes, allowing easy
conversion of single-machine parallel ML applications into distributed ones,
while maintaining high throughput through relaxed &quot;consistency models&quot; that
allow inconsistent parameter reads. However, due to insufficient theoretical
study, it is not clear which of these consistency models can really ensure
correct ML algorithm output; at the same time, there remain many
theoretically-motivated but undiscovered opportunities to maximize
computational throughput. Motivated by this challenge, we study both the
theoretical guarantees and empirical behavior of iterative-convergent ML
algorithms in existing PS consistency models. We then use the gleaned insights
to improve a consistency model using an &quot;eager&quot; PS communication mechanism, and
implement it as a new PS system that enables ML algorithms to reach their
solution more quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8054</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8054</id><created>2014-10-29</created><updated>2015-07-06</updated><authors><author><keyname>Lesser</keyname><forenames>Kendra</forenames></author><author><keyname>Oishi</keyname><forenames>Meeko</forenames></author></authors><title>Approximate Safety Verification and Control of Partially Observable
  Stochastic Hybrid Systems</title><categories>cs.SY math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1404.5906</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assuring safety in discrete time stochastic hybrid systems is particularly
difficult when only noisy or incomplete observations of the state are
available. We first review a formulation of the probabilistic safety problem
under noisy hybrid observations as a dynamic program over an equivalent
information state. Two methods for approximately solving the dynamic program
are presented. The first method approximates the hybrid system as an equivalent
finite state Markov decision process, so that the information state is a
probability mass function. The second approach approximates an indicator
function over the safe region using radial basis functions, to represent the
information state as a Gaussian mixture. In both cases, we discretize the
hybrid observation process and generate a sampled set of information states,
then use point-based value iteration to under-approximate the safety
probability and synthesize a suboptimal control policy. We obtain error bounds
and convergence results in both cases, assuming switched affine dynamics and
additive Gaussian noise on the continuous states and observations. We compare
the performance of the finite state and Gaussian mixture approaches on a simple
numerical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8060</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8060</id><created>2014-10-29</created><updated>2015-03-05</updated><authors><author><keyname>Shmarov</keyname><forenames>Fedor</forenames></author><author><keyname>Zuliani</keyname><forenames>Paolo</forenames></author></authors><title>ProbReach: Verified Probabilistic Delta-Reachability for Stochastic
  Hybrid Systems</title><categories>cs.LO cs.SY</categories><comments>HSCC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ProbReach, a tool for verifying probabilistic reachability for
stochastic hybrid systems, i.e., computing the probability that the system
reaches an unsafe region of the state space. In particular, ProbReach will
compute an arbitrarily small interval which is guaranteed to contain the
required probability. Standard (non-probabilistic) reachability is undecidable
even for linear hybrid systems. In ProbReach we adopt the weaker notion of
delta-reachability, in which the unsafe region is overapproximated by a
user-defined parameter (delta). This choice leads to false alarms, but also
makes the reachability problem decidable for virtually any hybrid system. In
ProbReach we have implemented a probabilistic version of delta-reachability
that is suited for hybrid systems whose stochastic behaviour is given in terms
of random initial conditions. In this paper we introduce the capabilities of
ProbReach, give an overview of the parallel implementation, and present results
for several benchmarks involving highly non-linear hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8068</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8068</id><created>2014-10-27</created><authors><author><keyname>Sushmita</keyname><forenames>Shanu</forenames></author><author><keyname>Chin</keyname><forenames>Si-Chi</forenames></author></authors><title>Health Information Search Behavior on the Web: A Pilot Study</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching health information on web has become an integral part of today's
world, and many people turn to the Web for healthcare information and
healthcare assessment. Our pilot study investigates users' preferences for the
type of search results (image, news, video, etc.), and investigates users'
ability to accurately interpret online health information for the purpose of
self diagnosis. The preliminary results reveal that blog and news articles are
most sought by users when searching online information and there exist
challenges in the use of online health information for self-diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8075</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8075</id><created>2014-10-29</created><updated>2016-01-27</updated><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author></authors><title>Achievable Rates for Shaped Bit-Metric Decoding</title><categories>cs.IT math.IT</categories><comments>Revised version. Compared to version v4, the proof of Theorem 1 is
  rewritten</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new achievable rate for bit-metric decoding (BMD) is derived using random
coding arguments. The rate expression can be evaluated for any input
distribution, and in particular the bit-levels of binary input labels can be
stochastically dependent. Probabilistic shaping with dependent bit-levels
(shaped BMD), shaping of independent bit-levels (bit-shaped BMD) and uniformly
distributed independent bit-levels (uniform BMD) are evaluated on the additive
white Gaussian noise (AWGN) channel with Gray labelled bipolar amplitude shift
keying (ASK). For 32-ASK at a rate of 3.8 bits/channel use, the gap to 32-ASK
capacity is 0.008 dB for shaped BMD, 0.46 dB for bit-shaped BMD, and 1.42 dB
for uniform BMD. These numerical results illustrate that dependence between the
bit-levels is beneficial on the AWGN channel. The relation to the generalized
mutual information (GMI) is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8078</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8078</id><created>2014-10-29</created><updated>2015-12-18</updated><authors><author><keyname>Wiese</keyname><forenames>Moritz</forenames></author><author><keyname>N&#xf6;tzel</keyname><forenames>Janis</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>A channel under simultaneous jamming and eavesdropping
  attack---correlated random coding capacities under strong secrecy criteria</title><categories>cs.IT math.IT</categories><comments>32 pages. With slight differences in Remarks 7-4) and Remark 17
  submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a complete characterization of the correlated random coding secrecy
capacity of arbitrarily varying wiretap channels (AVWCs). We apply two
alternative strong secrecy criteria, which both lead to the same multi-letter
formula. The difference of these criteria lies in the treatment of correlated
randomness, they coincide in the case of uncorrelated codes. On the basis of
the derived formula, we show that the correlated random coding secrecy capacity
is continuous as a function of the AVWC, in contrast to the discontinuous
uncorrelated coding secrecy capacity. In the proof of the secrecy capacity
formula for correlated random codes, we apply an auxiliary channel which is
compound from the sender to the intended receiver and arbitrarily varying from
the sender to the eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8082</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8082</id><created>2014-09-24</created><authors><author><keyname>Smith</keyname><forenames>Reginald D.</forenames></author></authors><title>Malware &quot;Ecology&quot; Viewed as Ecological Succession: Historical Trends and
  Future Prospects</title><categories>cs.CR q-bio.PE</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development and evolution of malware including computer viruses, worms,
and trojan horses, is shown to be closely analogous to the process of community
succession long recognized in ecology. In particular, both changes in the
overall environment by external disturbances, as well as, feedback effects from
malware competition and antivirus coevolution have driven community succession
and the development of different types of malware with varying modes of
transmission and adaptability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8091</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8091</id><created>2014-10-23</created><authors><author><keyname>Canzian</keyname><forenames>Luca</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Wong</keyname><forenames>Gerard C. L.</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>A Dynamic Network Formation Model for Understanding Bacterial
  Self-Organization into Micro-Colonies</title><categories>q-bio.PE cs.GT q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general parametrizable model to capture the dynamic interaction
among bacteria in the formation of micro-colonies. micro-colonies represent the
first social step towards the formation of structured multicellular communities
known as bacterial biofilms, which protect the bacteria against antimicrobials.
In our model, bacteria can form links in the form of intercellular adhesins
(such as polysaccharides) to collaborate in the production of resources that
are fundamental to protect them against antimicrobials. Since maintaining a
link can be costly, we assume that each bacterium forms and maintains a link
only if the benefit received from the link is larger than the cost, and we
formalize the interaction among bacteria as a dynamic network formation game.
We rigorously characterize some of the key properties of the network evolution
depending on the parameters of the system. In particular, we derive the
parameters under which it is guaranteed that all bacteria will join
micro-colonies and the parameters under which it is guaranteed that some
bacteria will not join micro-colonies. Importantly, our study does not only
characterize the properties of networks emerging in equilibrium, but it also
provides important insights on how the network dynamically evolves and on how
the formation history impacts the emerging networks in equilibrium. This
analysis can be used to develop methods to influence on- the-fly the evolution
of the network, and such methods can be useful to treat or prevent
biofilm-related diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8099</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8099</id><created>2014-10-29</created><authors><author><keyname>Manikonda</keyname><forenames>Lydia</forenames></author><author><keyname>Hu</keyname><forenames>Yuheng</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Analyzing User Activities, Demographics, Social Network Structure and
  User-Generated Content on Instagram</title><categories>cs.SI physics.soc-ph</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instagram is a relatively new form of communication where users can instantly
share their current status by taking pictures and tweaking them using filters.
It has seen a rapid growth in the number of users as well as uploads since it
was launched in October 2010. Inspite of the fact that it is the most popular
photo sharing application, it has attracted relatively less attention from the
web and social media research community. In this paper, we present a
large-scale quantitative analysis on millions of users and pictures we crawled
over 1 month from Instagram. Our analysis reveals several insights on Instagram
which were never studied before: 1) its social network properties are quite
different from other popular social media like Twitter and Flickr, 2) people
typically post once a week, and 3) people like to share their locations with
friends. To the best of our knowledge, this is the first in-depth analysis of
user activities, demographics, social network structure and user-generated
content on Instagram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8100</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8100</id><created>2014-10-29</created><updated>2015-09-06</updated><authors><author><keyname>Nadendla</keyname><forenames>V. Sriram Siddhardh</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Design of Binary Quantizers for Distributed Detection under Secrecy
  Constraints</title><categories>cs.IT cs.CR cs.SY math.IT</categories><comments>13 pages, 7 figures; Submitted to IEEE Transactions in Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the design of distributed detection networks in
the presence of an eavesdropper (Eve). We consider the problem of designing
binary quantizers at the sensors that maximize the Kullback-Leibler (KL)
Divergence at the fusion center (FC), subject to a tolerable constraint on the
KL Divergence at Eve. In the case of i.i.d. received symbols at both the FC and
Eve, we prove that the structure of the optimal binary quantizers is a
likelihood ratio test (LRT). We also present an algorithm to find the threshold
of the optimal LRT, and illustrate it for the case of Additive White Gaussian
Noise (AWGN) observation models at the sensors. In the case of non-i.i.d.
received symbols at both FC and Eve, we propose a dynamic-programming based
algorithm to find efficient quantizers at the sensors. Numerical results are
presented to illustrate the performance of the proposed network design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8104</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8104</id><created>2014-10-28</created><updated>2015-08-04</updated><authors><author><keyname>Jensen</keyname><forenames>Kristian</forenames></author></authors><title>Solving Stress and Compliance Constrained Volume Minimization using
  Anisotropic Mesh Adaptation, the Method of Moving Asymptotes and a Global
  p-norm</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The p-norm often used in stress constrained topology optimisation supposedly
mimics a delta function and it is thus characterised by a small length scale
and ideally one would also prefer to have the solid-void transition occur over
a small length scale, since the material in this transition does not have a
clear physical interpretation. We propose to resolve these small length scales
using anisotropic mesh adaptation. We use the method of moving asymptotes with
interpolation of sensitivities, asymptotes and design variables between
iterations. We demonstrate this combination for the portal and L-bracket
problems with p=10, and we are able to investigate mesh dependence. Finally, we
suggest relaxing the L-bracket problem statement by introducing a rounded
corner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8105</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8105</id><created>2014-10-29</created><updated>2015-01-11</updated><authors><author><keyname>Fusaroli</keyname><forenames>Riccardo</forenames></author><author><keyname>Perlman</keyname><forenames>Marcus</forenames></author><author><keyname>Mislove</keyname><forenames>Alan</forenames></author><author><keyname>Paxton</keyname><forenames>Alexandra</forenames></author><author><keyname>Matlock</keyname><forenames>Teenie</forenames></author><author><keyname>Dale</keyname><forenames>Rick</forenames></author></authors><title>Timescales of Massive Human Entrainment</title><categories>physics.soc-ph cs.SI</categories><comments>20 pages, 7 figures, 6 tables, 4 supplementary figures. 2nd version
  revised according to peer reviewers' comments: more detailed explanation of
  the methods, and grounding of the hypotheses</comments><journal-ref>PLoS ONE 10(4): e0122742 (2015)</journal-ref><doi>10.1371/journal.pone.0122742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past two decades have seen an upsurge of interest in the collective
behaviors of complex systems composed of many agents entrained to each other
and to external events. In this paper, we extend concepts of entrainment to the
dynamics of human collective attention. We conducted a detailed investigation
of the unfolding of human entrainment - as expressed by the content and
patterns of hundreds of thousands of messages on Twitter - during the 2012 US
presidential debates. By time locking these data sources, we quantify the
impact of the unfolding debate on human attention. We show that collective
social behavior covaries second-by-second to the interactional dynamics of the
debates: A candidate speaking induces rapid increases in mentions of his name
on social media and decreases in mentions of the other candidate. Moreover,
interruptions by an interlocutor increase the attention received. We also
highlight a distinct time scale for the impact of salient moments in the
debate: Mentions in social media start within 5-10 seconds after the moment;
peak at approximately one minute; and slowly decay in a consistent fashion
across well-known events during the debates. Finally, we show that public
attention after an initial burst slowly decays through the course of the
debates. Thus we demonstrate that large-scale human entrainment may hold across
a number of distinct scales, in an exquisitely time-locked fashion. The methods
and results pave the way for careful study of the dynamics and mechanisms of
large-scale human entrainment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8111</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8111</id><created>2014-10-29</created><updated>2015-12-10</updated><authors><author><keyname>Esik</keyname><forenames>Zoltan</forenames></author></authors><title>Equational properties of stratified least fixed points</title><categories>cs.LO</categories><msc-class>68Q55</msc-class><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a novel fixed point operation has been introduced over certain
non-monotonic functions between stratified complete lattices and used to give
semantics to logic programs with negation and boolean context-free grammars. We
prove that this new operation satisfies `the standard' identities of fixed
point operations as described by the axioms of iteration theories. We also
study this new fixed point operation in connection with lambda-abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8119</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8119</id><created>2014-10-29</created><authors><author><keyname>Tehrani</keyname><forenames>Ali Soltani</forenames></author><author><keyname>Cao</keyname><forenames>Haiying</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Fager</keyname><forenames>Christian</forenames></author></authors><title>Black-box Modeling and Compensation of Bursty Communication Signals in
  RF Power Amplifiers with Power-Dependent Parameters</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new black-box technique for modeling long term memory
effects in radio frequency power amplifiers. The proposed technique extends
commonly used behavioral models by utilizing parameters that dynamically change
depending on a long term memory effect while keeping the original model
structure intact. This enables us to accurately track and model transient
changes in power amplifier characteristics that vary slowly and are induced by
the input signal.
  Identification of long term memory effects is discussed and an iterative
identification algorithm for the model parameters is proposed. The model is
experimentally tested on a 100 Watt Doherty power amplifier with a 4 MHz
Gaussian noise signal that has a step--like change in the amplitude,
representative of a realistic communication signal with bursty behavior and a
20 MHz 3GPP LTE test data. Results of behavioral modeling show a 2-2.5 dB and
5-6 dB improvement in average and peak NMSE modeling performance respectively,
which shows the suitability of the technique to model bursty signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8127</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8127</id><created>2014-10-29</created><authors><author><keyname>Tehrani</keyname><forenames>Ali Soltani</forenames></author><author><keyname>Chani</keyname><forenames>Jessica</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Fager</keyname><forenames>Christian</forenames></author></authors><title>Investigation of Parameter Adaptation in RF Power Amplifier Behavioral
  Models</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an investigation into parameter adaptation in behavioral
model--based digital predistortion for radio frequency power amplifiers. A
novel measurement setup framework that emulates real--time adaptation in
transmitters is developed that allows evaluation of different parameters,
configurations and adaptation algorithms. This setup relieves the need for full
feedback loops for parameter adaptation while providing the flexibility needed
in the design process of parameter adaptation.
  Issues such as convergence speed, sensitivity to quantization noise in the
feedback loop and predistortion performance are investigated for some different
parameter update algorithms using the proposed measurement setup. The approach
presented in this paper allows the possibility to analyze different aspects of
digital predistortion adaptation algorithms, and is an important enabling step
for further research on parameter adaptation before the real--time hardware is
implemented for use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8129</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8129</id><created>2014-10-29</created><updated>2016-02-15</updated><authors><author><keyname>Qi</keyname><forenames>Yang</forenames></author><author><keyname>Comon</keyname><forenames>Pierre</forenames></author><author><keyname>Lim</keyname><forenames>Lek-Heng</forenames></author></authors><title>Uniqueness of Nonnegative Tensor Approximations</title><categories>cs.NA cs.IT math.IT math.NA</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We show that for a nonnegative tensor, a best nonnegative rank-r
approximation is almost always unique, its best rank-one approximation may
always be chosen to be a best nonnegative rank-one approximation, and that the
set of nonnegative tensors with non-unique best rank-one approximations form an
algebraic hypersurface. We show that the last part holds true more generally
for real tensors and thereby determine a polynomial equation so that a real or
nonnegative tensor which does not satisfy this equation is guaranteed to have a
unique best rank-one approximation. We also establish an analogue for real or
nonnegative symmetric tensors. In addition, we prove a singular vector variant
of the Perron--Frobenius Theorem for positive tensors and apply it to show that
a best nonnegative rank-r approximation of a positive tensor can never be
obtained by deflation. As an aside, we verify that the Euclidean distance (ED)
discriminants of the Segre variety and the Veronese variety are hypersurfaces
and give defining equations of these ED discriminants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8149</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8149</id><created>2014-10-29</created><authors><author><keyname>Rodrigues</keyname><forenames>Paul</forenames></author><author><keyname>Zajic</keyname><forenames>David</forenames></author><author><keyname>Doermann</keyname><forenames>David</forenames></author><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Ye</keyname><forenames>Peng</forenames></author></authors><title>Detecting Structural Irregularity in Electronic Dictionaries Using
  Language Modeling</title><categories>cs.CL cs.LG</categories><comments>6 pages, 2 figures, 11 tables; appeared in Proceedings of Electronic
  Lexicography in the 21st Century (eLex), November 2011</comments><acm-class>I.2.7; I.2.6; I.5.1; I.5.4</acm-class><journal-ref>In Proceedings of Electronic Lexicography in the 21st Century
  (eLex), pages 227-232, Bled, Slovenia, November 2011. Trojina Institute for
  Applied Slovene Studies</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionaries are often developed using tools that save to Extensible Markup
Language (XML)-based standards. These standards often allow high-level
repeating elements to represent lexical entries, and utilize descendants of
these repeating elements to represent the structure within each lexical entry,
in the form of an XML tree. In many cases, dictionaries are published that have
errors and inconsistencies that are expensive to find manually. This paper
discusses a method for dictionary writers to quickly audit structural
regularity across entries in a dictionary by using statistical language
modeling. The approach learns the patterns of XML nodes that could occur within
an XML tree, and then calculates the probability of each XML tree in the
dictionary against these patterns to look for entries that diverge from the
norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8151</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8151</id><created>2014-10-29</created><updated>2015-04-17</updated><authors><author><keyname>Iscen</keyname><forenames>Ahmet</forenames></author><author><keyname>Tolias</keyname><forenames>Giorgos</forenames></author><author><keyname>Gosselin</keyname><forenames>Philippe-Henri</forenames></author><author><keyname>J&#xe9;gou</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>A comparison of dense region detectors for image search and fine-grained
  classification</title><categories>cs.CV</categories><comments>Accepted to IEEE Transactions on Image Processing</comments><doi>10.1109/TIP.2015.2423557</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a pipeline for image classification or search based on coding
approaches like Bag of Words or Fisher vectors. In this context, the most
common approach is to extract the image patches regularly in a dense manner on
several scales. This paper proposes and evaluates alternative choices to
extract patches densely. Beyond simple strategies derived from regular interest
region detectors, we propose approaches based on super-pixels, edges, and a
bank of Zernike filters used as detectors. The different approaches are
evaluated on recent image retrieval and fine-grain classification benchmarks.
Our results show that the regular dense detector is outperformed by other
methods in most situations, leading us to improve the state of the art in
comparable setups on standard retrieval and fined-grain benchmarks. As a
byproduct of our study, we show that existing methods for blob and super-pixel
extraction achieve high accuracy if the patches are extracted along the edges
and not around the detected regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8154</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8154</id><created>2014-10-29</created><authors><author><keyname>Khoshkhah</keyname><forenames>Kaveh</forenames></author></authors><title>On finding orientations with fewest number of vartices with small
  out-degree</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph, each of the two end-vertices of an edge can own
the edge. Call a vertex poor, if it owns at most one edge. We give a polynomial
time algorithm for the problem of finding an assignment of owners to the edges
which minimizes the number of poor vertices. In the terminology of graph
orientation, this means finding an orientation for the edges of a graph
minimizing the number of edges with out-degree at most 1, and answers a
question of Asahiro Jansson, Miyano, Ono (2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8155</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8155</id><created>2014-10-29</created><authors><author><keyname>Moosavi</keyname><forenames>Azam S. Zavar</forenames></author><author><keyname>Tranquilli</keyname><forenames>Paul</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Solving stochastic chemical kinetics by Metropolis Hastings sampling</title><categories>cs.NA</categories><report-no>17 pages, 3 figures</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study considers using Metropolis-Hastings algorithm for stochastic
simulation of chemical reactions. The proposed method uses SSA (Stochastic
Simulation Algorithm) distribution which is a standard method for solving
well-stirred chemically reacting systems as a desired distribution. A new
numerical solvers based on exponential form of exact and approximate solutions
of CME (Chemical Master Equation) is employed for obtaining target and proposal
distributions in Metropolis-Hastings algorithm to accelerate the accuracy of
the tau-leap method. Samples generated by this technique have the same
distribution as SSA and the histogram of samples show it's convergence to SSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8158</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8158</id><created>2014-10-29</created><authors><author><keyname>Wang</keyname><forenames>Haobo</forenames></author><author><keyname>Chen</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Histogram-Based Flash Channel Estimation</title><categories>cs.IT math.IT</categories><comments>6 pages, 8 figures, Submitted to the IEEE International
  Communications Conference (ICC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current generation Flash devices experience significant read-channel
degradation from damage to the oxide layer during program and erase operations.
Information about the read-channel degradation drives advanced signal
processing methods in Flash to mitigate its effect. In this context, channel
estimation must be ongoing since channel degradation evolves over time and as a
function of the number of program/erase (P/E) cycles. This paper proposes a
framework for ongoing model-based channel estimation using limited channel
measurements (reads). This paper uses a channel model characterizing
degradation resulting from retention time and the amount of charge programmed
and erased. For channel histogram measurements, bin selection to achieve
approximately equal-probability bins yields a good approximation to the
original distribution using only ten bins (i.e. nine reads). With the channel
model and binning strategy in place, this paper explores candidate numerical
least squares algorithms and ultimately demonstrates the effectiveness of the
Levenberg-Marquardt algorithm which provides both speed and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8175</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8175</id><created>2014-10-29</created><authors><author><keyname>Mehrabian</keyname><forenames>Abbas</forenames></author><author><keyname>Pourmiri</keyname><forenames>Ali</forenames></author></authors><title>Randomized Rumor Spreading in Poorly Connected Small-World Networks</title><categories>cs.SI math.CO</categories><comments>28 pages, preliminary version appeared in DISC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Push-Pull is a well-studied round-robin rumor spreading protocol defined as
follows: initially a node knows a rumor and wants to spread it to all nodes in
a network quickly. In each round, every informed node sends the rumor to a
random neighbor, and every uninformed node contacts a random neighbor and gets
the rumor from her if she knows it. We analyze this protocol on random
$k$-trees, a class of power law graphs, which are small-world and have large
clustering coefficients, built as follows: initially we have a $k$-clique. In
every step a new node is born, a random $k$-clique of the current graph is
chosen, and the new node is joined to all nodes of the $k$-clique. When $k&gt;1$
is fixed, we show that if initially a random node is aware of the rumor, then
with probability $1-o(1)$ after $O\left((\log n)^{1+2/k}\cdot\log\log n\cdot
f(n)\right)$ rounds the rumor propagates to $n-o(n)$ nodes, where $n$ is the
number of nodes and $f(n)$ is any slowly growing function. Since these graphs
have polynomially small conductance, vertex expansion $O(1/n)$ and constant
treewidth, these results demonstrate that Push-Pull can be efficient even on
poorly connected networks. On the negative side, we prove that with probability
$1-o(1)$ the protocol needs at least
$\Omega\left(n^{(k-1)/(k^2+k-1)}/f^2(n)\right)$ rounds to inform all nodes.
This exponential dichotomy between time required for informing almost all and
all nodes is striking. Our main contribution is to present, for the first time,
a natural class of random graphs in which such a phenomenon can be observed.
Our technique for proving the upper bound carries over to a closely related
class of graphs, random $k$-Apollonian networks, for which we prove an upper
bound of $O\left((\log n)^{c_k}\cdot\log\log n\cdot f(n)\right)$ rounds for
informing $n-o(n)$ nodes with probability $1-o(1)$ when $k&gt;2$ is fixed. Here,
$c_k=(k^2-3)/(k-1)^2&lt;1 + 2/k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8176</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8176</id><created>2014-10-29</created><authors><author><keyname>Y&#x131;ld&#x131;r&#x131;m</keyname><forenames>Kas&#x131;m Sinan</forenames></author><author><keyname>Carli</keyname><forenames>Ruggero</forenames></author><author><keyname>Schenato</keyname><forenames>Luca</forenames></author></authors><title>Proportional-Integral Clock Synchronization in Wireless Sensor Networks</title><categories>cs.DC</categories><acm-class>C.2.2; I.2.9; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present a new control theoretic distributed time
synchronization algorithm, named PISync, in order to synchronize sensor nodes
in Wireless Sensor Networks (WSNs). PISync algorithm is based on a
Proportional-Integral (PI) controller. It applies a proportional feedback (P)
and an integral feedback (I) on the local measured synchronization errors to
compensate the differences between the clock offsets and clock speeds. We
present practical flooding-based and fully distributed protocol implementations
of the PISync algorithm, and we provide theoretical analysis to highlight the
benefits of this approach in terms of improved steady state error and
scalability as compared to existing synchronization algorithms. We show through
real-world experiments and simulations that PISync protocols have several
advantages over existing protocols in the WSN literature, namely no need for
memory allocation, minimal CPU overhead and code size independent of network
size and topology, and graceful performance degradation in terms of network
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8202</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8202</id><created>2014-10-29</created><updated>2015-01-23</updated><authors><author><keyname>H&#xfc;ttenhain</keyname><forenames>Jesko</forenames></author><author><keyname>Ikenmeyer</keyname><forenames>Christian</forenames></author></authors><title>Binary Determinantal Complexity</title><categories>cs.CC</categories><comments>10 pages, C source code for the computation available as ancillary
  files</comments><msc-class>68Q05, 68-04</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for writing the 3 by 3 permanent polynomial as a determinant of
a matrix consisting only of zeros, ones, and variables as entries, a 7 by 7
matrix is required. Our proof is computer based and uses the enumeration of
bipartite graphs. Furthermore, we analyze sequences of polynomials that are
determinants of polynomially sized matrices consisting only of zeros, ones, and
variables. We show that these are exactly the sequences in the complexity class
of constant free polynomially sized (weakly) skew circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8205</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8205</id><created>2014-10-29</created><authors><author><keyname>Chan</keyname><forenames>Timothy M.</forenames></author><author><keyname>Frati</keyname><forenames>Fabrizio</forenames></author><author><keyname>Gutwenger</keyname><forenames>Carsten</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Mutzel</keyname><forenames>Petra</forenames></author><author><keyname>Schaefer</keyname><forenames>Marcus</forenames></author></authors><title>Drawing Partially Embedded and Simultaneously Planar Graphs</title><categories>cs.CG cs.DM cs.DS</categories><comments>Preliminary version appeared at the 22nd International Symposium on
  Graph Drawing (GD '14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of constructing planar drawings with few bends for
two related problems, the partially embedded graph problem---to extend a
straight-line planar drawing of a subgraph to a planar drawing of the whole
graph---and the simultaneous planarity problem---to find planar drawings of two
graphs that coincide on shared vertices and edges. In both cases we show that
if the required planar drawings exist, then there are planar drawings with a
linear number of bends per edge and, in the case of simultaneous planarity, a
constant number of crossings between every pair of edges. Our proofs provide
efficient algorithms if the combinatorial embedding of the drawing is given.
Our result on partially embedded graph drawing generalizes a classic result by
Pach and Wenger which shows that any planar graph can be drawn with a linear
number of bends per edge if the location of each vertex is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8206</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8206</id><created>2014-10-29</created><updated>2015-05-30</updated><authors><author><keyname>Luong</keyname><forenames>Minh-Thang</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author><author><keyname>Le</keyname><forenames>Quoc V.</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames></author></authors><title>Addressing the Rare Word Problem in Neural Machine Translation</title><categories>cs.CL cs.LG cs.NE</categories><comments>ACL 2015 camera-ready version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural Machine Translation (NMT) is a new approach to machine translation
that has shown promising results that are comparable to traditional approaches.
A significant weakness in conventional NMT systems is their inability to
correctly translate very rare words: end-to-end NMTs tend to have relatively
small vocabularies with a single unk symbol that represents every possible
out-of-vocabulary (OOV) word. In this paper, we propose and implement an
effective technique to address this problem. We train an NMT system on data
that is augmented by the output of a word alignment algorithm, allowing the NMT
system to emit, for each OOV word in the target sentence, the position of its
corresponding word in the source sentence. This information is later utilized
in a post-processing step that translates every OOV word using a dictionary.
Our experiments on the WMT14 English to French translation task show that this
method provides a substantial improvement of up to 2.8 BLEU points over an
equivalent NMT system that does not use this technique. With 37.5 BLEU points,
our NMT system is the first to surpass the best result achieved on a WMT14
contest task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8215</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8215</id><created>2014-10-29</created><authors><author><keyname>Beckert</keyname><forenames>Bernhard</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Grebing</keyname><forenames>Sarah</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>B&#xf6;hl</keyname><forenames>Florian</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>How to Put Usability into Focus: Using Focus Groups to Evaluate the
  Usability of Interactive Theorem Provers</title><categories>cs.LO cs.HC</categories><comments>In Proceedings UITP 2014, arXiv:1410.7850</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 167, 2014, pp. 4-13</journal-ref><doi>10.4204/EPTCS.167.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years the effectiveness of interactive theorem provers has
increased to an extent that the bottleneck in the interactive process shifted
to efficiency: while in principle large and complex theorems are provable
(effectiveness), it takes a lot of effort for the user interacting with the
system (lack of efficiency). We conducted focus groups to evaluate the
usability of Isabelle/HOL and the KeY system with two goals: (a) detect
usability issues in the interaction between interactive theorem provers and
their user, and (b) analyze how evaluation and survey methods commonly used in
the area of human-computer interaction, such as focus groups and co-operative
evaluation, are applicable to the specific field of interactive theorem proving
(ITP).
  In this paper, we report on our experience using the evaluation method focus
groups and how we adapted this method to ITP. We describe our results and
conclusions mainly on the &quot;meta-level,&quot; i.e., we focus on the impact that
specific characteristics of ITPs have on the setup and the results of focus
groups. On the concrete level, we briefly summarise insights into the usability
of the ITPs used in our case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8216</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8216</id><created>2014-10-29</created><authors><author><keyname>Butterfield</keyname><forenames>Andrew</forenames><affiliation>Trinity College Dublin</affiliation></author></authors><title>UTP2: Higher-Order Equational Reasoning by Pointing</title><categories>cs.LO cs.HC</categories><comments>In Proceedings UITP 2014, arXiv:1410.7850</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 167, 2014, pp. 14-22</journal-ref><doi>10.4204/EPTCS.167.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a prototype theorem prover, UTP2, developed to match the style of
hand-written proof work in the Unifying Theories of Programming semantical
framework. This is based on alphabetised predicates in a 2nd-order logic, with
a strong emphasis on equational reasoning. We present here an overview of the
user-interface of this prover, which was developed from the outset using a
point-and-click approach. We contrast this with the command-line paradigm that
continues to dominate the mainstream theorem provers, and raises the question:
can we have the best of both worlds?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8217</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8217</id><created>2014-10-29</created><authors><author><keyname>Grov</keyname><forenames>Gudmund</forenames><affiliation>Heriot-Watt University</affiliation></author><author><keyname>Kissinger</keyname><forenames>Aleks</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Lin</keyname><forenames>Yuhui</forenames><affiliation>Heriot-Watt University</affiliation></author></authors><title>Tinker, tailor, solver, proof</title><categories>cs.LO cs.HC</categories><comments>In Proceedings UITP 2014, arXiv:1410.7850</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 167, 2014, pp. 23-34</journal-ref><doi>10.4204/EPTCS.167.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Tinker, a tool for designing and evaluating proof strategies
based on proof-strategy graphs, a formalism previously introduced by the
authors. We represent proof strategies as open-graphs, which are directed
graphs with additional input/output edges. Tactics appear as nodes in a graph,
and can be `piped' together by adding edges between them. Goals are added to
the input edges of such a graph, and flow through the graph as the strategy is
evaluated. Properties of the edges ensure that only the right `type' of goals
are accepted. In this paper, we detail the Tinker tool and show how it can be
integrated with two different theorem provers: Isabelle and ProofPower.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8218</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8218</id><created>2014-10-29</created><authors><author><keyname>Libal</keyname><forenames>Tomer</forenames><affiliation>Microsoft Research - Inria Joint Center, Ecole Polytechnique</affiliation></author><author><keyname>Riener</keyname><forenames>Martin</forenames><affiliation>Institute of Computer Languages, Vienna University of Technology</affiliation></author><author><keyname>Rukhaia</keyname><forenames>Mikheil</forenames><affiliation>Institute of Applied Mathematics, Tbilisi State University</affiliation></author></authors><title>Advanced Proof Viewing in ProofTool</title><categories>cs.LO cs.HC</categories><comments>In Proceedings UITP 2014, arXiv:1410.7850</comments><proxy>EPTCS</proxy><acm-class>H.5.2.; F.4.1</acm-class><journal-ref>EPTCS 167, 2014, pp. 35-47</journal-ref><doi>10.4204/EPTCS.167.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequent calculus is widely used for formalizing proofs. However, due to the
proliferation of data, understanding the proofs of even simple mathematical
arguments soon becomes impossible. Graphical user interfaces help in this
matter, but since they normally utilize Gentzen's original notation, some of
the problems persist. In this paper, we introduce a number of criteria for
proof visualization which we have found out to be crucial for analyzing proofs.
We then evaluate recent developments in tree visualization with regard to these
criteria and propose the Sunburst Tree layout as a complement to the
traditional tree structure. This layout constructs inferences as concentric
circle arcs around the root inference, allowing the user to focus on the
proof's structural content. Finally, we describe its integration into ProofTool
and explain how it interacts with the Gentzen layout.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8219</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8219</id><created>2014-10-29</created><authors><author><keyname>Rabe</keyname><forenames>Florian</forenames><affiliation>Jacobs University Bremen</affiliation></author></authors><title>A Logic-Independent IDE</title><categories>cs.LO cs.HC</categories><comments>In Proceedings UITP 2014, arXiv:1410.7850</comments><proxy>EPTCS</proxy><acm-class>F.4.1; D.2.6</acm-class><journal-ref>EPTCS 167, 2014, pp. 48-60</journal-ref><doi>10.4204/EPTCS.167.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The author's MMT system provides a framework for defining and implementing
logical systems. By combining MMT with the jEdit text editor, we obtain a
logic-independent IDE. The IDE functionality includes advanced features such as
context-sensitive auto-completion, search, and change management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8220</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8220</id><created>2014-10-29</created><authors><author><keyname>Sternagel</keyname><forenames>Christian</forenames><affiliation>University of Innsbruck, Austria</affiliation></author><author><keyname>Thiemann</keyname><forenames>Ren&#xe9;</forenames><affiliation>University of Innsbruck, Austria</affiliation></author></authors><title>The Certification Problem Format</title><categories>cs.LO</categories><comments>In Proceedings UITP 2014, arXiv:1410.7850</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 167, 2014, pp. 61-72</journal-ref><doi>10.4204/EPTCS.167.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an overview of CPF, the certification problem format, and explain
some design decisions. Whereas CPF was originally invented to combine three
different formats for termination proofs into a single one, in the meanwhile
proofs for several other properties of term rewrite systems are also
expressible: like confluence, complexity, and completion. As a consequence, the
format is already supported by several tools and certifiers. Its acceptance is
also demonstrated in international competitions: the certified tracks of both
the termination and the confluence competition utilized CPF as exchange format
between automated tools and trusted certifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8221</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8221</id><created>2014-10-29</created><authors><author><keyname>Tankink</keyname><forenames>Carst</forenames><affiliation>Inria Saclay - &#xce;le-de-France</affiliation></author></authors><title>PIDE for Asynchronous Interaction with Coq</title><categories>cs.HC cs.LO</categories><comments>In Proceedings UITP 2014, arXiv:1410.7850</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 167, 2014, pp. 73-83</journal-ref><doi>10.4204/EPTCS.167.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the initial progress towards integrating the Coq proof
assistant with the PIDE architecture initially developed for Isabelle. The
architecture is aimed at asynchronous, parallel interaction with proof
assistants, and is tied in heavily with a plugin that allows the jEdit editor
to work with Isabelle.
  We have made some generalizations to the PIDE architecture to accommodate for
more provers than just Isabelle, and adapted Coq to understand the core
protocol: this delivered a working system in about two man-months.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8222</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8222</id><created>2014-10-29</created><authors><author><keyname>Wenzel</keyname><forenames>Makarius</forenames><affiliation>Univ. Paris-Sud, Laboratoire LRI, UMR8623</affiliation></author></authors><title>System description: Isabelle/jEdit in 2014</title><categories>cs.LO cs.HC</categories><comments>In Proceedings UITP 2014, arXiv:1410.7850</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 167, 2014, pp. 84-94</journal-ref><doi>10.4204/EPTCS.167.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is an updated system description for Isabelle/jEdit, according to the
official release Isabelle2014 (August 2014). The following new PIDE concepts
are explained: asynchronous print functions and document overlays, syntactic
and semantic completion, editor navigation, management of auxiliary files
within the document-model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8233</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8233</id><created>2014-10-29</created><authors><author><keyname>Tomasik</keyname><forenames>Brian</forenames></author></authors><title>Do Artificial Reinforcement-Learning Agents Matter Morally?</title><categories>cs.AI</categories><comments>37 pages</comments><acm-class>I.2.0; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial reinforcement learning (RL) is a widely used technique in
artificial intelligence that provides a general method for training agents to
perform a wide variety of behaviours. RL as used in computer science has
striking parallels to reward and punishment learning in animal and human
brains. I argue that present-day artificial RL agents have a very small but
nonzero degree of ethical importance. This is particularly plausible for views
according to which sentience comes in degrees based on the abilities and
complexities of minds, but even binary views on consciousness should assign
nonzero probability to RL programs having morally relevant experiences. While
RL programs are not a top ethical priority today, they may become more
significant in the coming decades as RL is increasingly applied to industry,
robotics, video games, and other areas. I encourage scientists, philosophers,
and citizens to begin a conversation about our ethical duties to reduce the
harm that we inflict on powerless, voiceless RL agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8251</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8251</id><created>2014-10-30</created><authors><author><keyname>Dyer</keyname><forenames>Chris</forenames></author></authors><title>Notes on Noise Contrastive Estimation and Negative Sampling</title><categories>cs.LG</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the parameters of probabilistic models of language such as maxent
models and probabilistic neural models is computationally difficult since it
involves evaluating partition functions by summing over an entire vocabulary,
which may be millions of word types in size. Two closely related
strategies---noise contrastive estimation (Mnih and Teh, 2012; Mnih and
Kavukcuoglu, 2013; Vaswani et al., 2013) and negative sampling (Mikolov et al.,
2012; Goldberg and Levy, 2014)---have emerged as popular solutions to this
computational problem, but some confusion remains as to which is more
appropriate and when. This document explicates their relationships to each
other and to other estimation techniques. The analysis shows that, although
they are superficially similar, NCE is a general parameter estimation technique
that is asymptotically unbiased, while negative sampling is best understood as
a family of binary classification models that are useful for learning word
representations but not as a general-purpose estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8253</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8253</id><created>2014-10-30</created><authors><author><keyname>Lehmann</keyname><forenames>Karsten</forenames></author><author><keyname>Grastien</keyname><forenames>Alban</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>AC-Feasibility on Tree Networks is NP-Hard</title><categories>cs.CC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed significant interest in convex relaxations of the
power flows, several papers showing that the second-order cone relaxation is
tight for tree networks under various conditions on loads or voltages. This
paper shows that AC-feasibility, i.e., to find whether some generator dispatch
can satisfy a given demand, is NP-Hard for tree networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8273</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8273</id><created>2014-10-30</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Distribution Statistics and Random Matrix Formalism of Multicarrier
  Continuous-Variable Quantum Key Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>47 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a combined mathematical framework of order statistics and random
matrix theory for multicarrier continuous-variable (CV) quantum key
distribution (QKD). In a multicarrier CVQKD scheme, the information is
granulated into Gaussian subcarrier CVs, and the physical Gaussian link is
divided into Gaussian sub-channels. The sub-channels are dedicated to the
conveying of the subcarrier CVs. The distribution statistics analysis covers
the study of the distribution of the sub-channel transmittance coefficients in
the presence of a Gaussian noise and the utilization of the moment generation
function (MGF) in the error analysis. We reveal the mathematical formalism of
sub-channel selection and formulation of the transmittance coefficients, and
show a reduced complexity progressive sub-channel scanning method. We define a
random matrix formalism for multicarrier CVQKD to evaluate the statistical
properties of the information flowing process. Using random matrix theory, we
express the achievable secret key rates and study the efficiency of the
AMQD-MQA (adaptive multicarrier quadrature division-multiuser quadrature
allocation) multiple-access multicarrier CVQKD. The proposed combined framework
is particularly convenient for the characterization of the physical processes
of experimental multicarrier CVQKD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8275</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8275</id><created>2014-10-30</created><updated>2014-11-21</updated><authors><author><keyname>Josse</keyname><forenames>Julie</forenames></author><author><keyname>Wager</keyname><forenames>Stefan</forenames></author></authors><title>Stable Autoencoding: A Flexible Framework for Regularized Low-Rank
  Matrix Estimation</title><categories>stat.ME cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework for low-rank matrix estimation that allows us to
transform noise models into regularization schemes via a simple parametric
bootstrap. Effectively, our procedure seeks an autoencoding basis for the
observed matrix that is robust with respect to the specified noise model. In
the simplest case, with an isotropic noise model, our procedure is equivalent
to a classical singular value shrinkage estimator. For non-isotropic noise
models, however, our method does not reduce to singular value shrinkage, and
instead yields new estimators that perform well in experiments. Moreover, by
iterating our stable autoencoding scheme, we can automatically generate
low-rank estimates without specifying the target rank as a tuning parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8292</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8292</id><created>2014-10-30</created><updated>2015-02-27</updated><authors><author><keyname>Harik</keyname><forenames>El Houssein Chouaib</forenames></author><author><keyname>Gu&#xe9;rin</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Guinand</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Breth&#xe9;</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Pelvillain</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>A Decentralized Interactive Architecture for Aerial and Ground Mobile
  Robots Cooperation</title><categories>cs.RO</categories><comments>Submitted to 2015 International Conference on Control, Automation and
  Robotics (ICCAR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel decentralized interactive architecture for aerial
and ground mobile robots cooperation. The aerial mobile robot is used to
provide a global coverage during an area inspection, while the ground mobile
robot is used to provide a local coverage of ground features. We include a
human-in-the-loop to provide waypoints for the ground mobile robot to progress
safely in the inspected area. The aerial mobile robot follows continuously the
ground mobile robot in order to always keep it in its coverage view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8313</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8313</id><created>2014-10-30</created><authors><author><keyname>Tepekule</keyname><forenames>Burcu</forenames></author><author><keyname>Pusane</keyname><forenames>Ali E.</forenames></author><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Tugcu</keyname><forenames>Tuna</forenames></author></authors><title>ISI Mitigation Techniques in Molecular Communication</title><categories>cs.ET cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular communication is a new field of communication where molecules are
used to transfer information. Among the proposed methods, molecular
communication via diffusion (MCvD) is particularly effective. One of the main
challenges in MCvD is the intersymbol interference (ISI), which inhibits
communication at high data rates. Furthermore, at the nano scale, energy
efficiency becomes an essential problem. Before addressing these problems, a
pre-determined threshold for the received signal must be calculated to make a
decision. In this paper, an analytical technique is proposed to determine the
optimum threshold, whereas in the literature, these thresholds are generally
calculated empirically. Since the main goal of this paper is to build an MCvD
system suitable for operating at high data rates without sacrificing quality,
new modulation and filtering techniques are proposed to decrease the effects of
ISI and enhance energy efficiency. As a transmitter-based solution, a
modulation technique for MCvD, molecular transition shift keying (MTSK), is
proposed in order to increase the data rate via suppressing the ISI.
Furthermore, for energy efficiency, a power adjustment technique that utilizes
the residual molecules is proposed. Finally, as a receiver-based solution, a
new energy efficient decision feedback filter (DFF) is proposed as a substitute
for the decoders such as minimum mean squared error (MMSE) and decision
feedback equalizer (DFE). The error performance of DFF and MMSE equalizers are
compared in terms of bit error rates, and it is concluded that DFF may be more
advantageous when energy efficiency is concerned, due to its lower
computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8314</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8314</id><created>2014-10-30</created><updated>2015-02-16</updated><authors><author><keyname>Turrini</keyname><forenames>Andrea</forenames><affiliation>State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China</affiliation></author><author><keyname>Hermanns</keyname><forenames>Holger</forenames><affiliation>Saarland University -- Computer Science, Saarbruecken, Germany</affiliation></author></authors><title>Cost Preserving Bisimulations for Probabilistic Automata</title><categories>cs.FL cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  18, 2014) lmcs:1050</journal-ref><doi>10.2168/LMCS-10(4:11)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic automata constitute a versatile and elegant model for
concurrent probabilistic systems. They are equipped with a compositional theory
supporting abstraction, enabled by weak probabilistic bisimulation serving as
the reference notion for summarising the effect of abstraction. This paper
considers probabilistic automata augmented with costs. It extends the notions
of weak transitions in probabilistic automata in such a way that the costs
incurred along a weak transition are captured. This gives rise to
cost-preserving and cost-bounding variations of weak probabilistic
bisimilarity, for which we establish compositionality properties with respect
to parallel composition. Furthermore, polynomial-time decision algorithms are
proposed, that can be effectively used to compute reward-bounding abstractions
of Markov decision processes in a compositional manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8317</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8317</id><created>2014-10-30</created><updated>2015-01-18</updated><authors><author><keyname>Rzeszutko</keyname><forenames>Elzbieta</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author></authors><title>Insights from Nature for Cybersecurity</title><categories>cs.CR</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The alarming rise in the quantity of malware in the last few years poses a
serious challenge to the security community and requires urgent response.
However, current countermeasures seem to be no longer effective. Thus, it is
our belief that it is now time for researchers and security experts to turn to
nature in the search for novel inspirations for defense systems. Nature has
provided species with a whole range of offensive and defensive techniques,
which have been developing and improving in the course of billions of years of
evolution. The extremely diverse living conditions have promoted a large
variation in the devised bio-security solutions. In this paper we introduce a
novel PROTECTION framework in which common denominators of the encountered
offensive and defensive means are proposed and presented. The bio-inspired
solutions are discussed in the context of cybersecurity, where some principles
have already been adopted. The deployment of the whole nature-based framework
should aid the design and improvement process of modern cyber-defense systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8326</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8326</id><created>2014-10-30</created><authors><author><keyname>Kirk</keyname><forenames>Nicholas H.</forenames></author></authors><title>Towards Learning Object Affordance Priors from Technical Texts</title><categories>cs.LG cs.AI cs.CL cs.RO</categories><comments>&quot;Active Learning in Robotics&quot; Workshop, IEEE-RAS International
  Conference on Humanoid Robots [accepted]</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Everyday activities performed by artificial assistants can potentially be
executed naively and dangerously given their lack of common sense knowledge.
This paper presents conceptual work towards obtaining prior knowledge on the
usual modality (passive or active) of any given entity, and their affordance
estimates, by extracting high-confidence ability modality semantic relations (X
can Y relationship) from non-figurative texts, by analyzing co-occurrence of
grammatical instances of subjects and verbs, and verbs and objects. The
discussion includes an outline of the concept, potential and limitations, and
possible feature and learning framework adoption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8336</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8336</id><created>2014-10-30</created><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>Kowalik</keyname><forenames>Lukasz</forenames></author></authors><title>A 13k-kernel for Planar Feedback Vertex Set via Region Decomposition</title><categories>cs.DS cs.DM</categories><comments>22 pages, short version accepted to IPEC'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a kernel of at most $13k$ vertices for the Planar Feedback Vertex Set
problem restricted to planar graphs, i.e., a polynomial-time algorithm that
transforms an input instance $(G,k)$ to an equivalent instance with at most
$13k$ vertices. To this end we introduce a few new reduction rules. However,
our main contribution is an application of the region decomposition technique
in the analysis of the kernel size. We show that our analysis is tight, up to a
constant additive term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8349</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8349</id><created>2014-10-30</created><updated>2014-11-02</updated><authors><author><keyname>Baber</keyname><forenames>Rahil</forenames></author><author><keyname>Christofides</keyname><forenames>Demetres</forenames></author><author><keyname>Dang</keyname><forenames>Anh N.</forenames></author><author><keyname>Riis</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Vaughan</keyname><forenames>Emil</forenames></author></authors><title>Graph Guessing Games and non-Shannon Information Inequalities</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Guessing games for directed graphs were introduced by Riis for studying
multiple unicast network coding problems. In a guessing game, the players toss
generalised dice and can see some of the other outcomes depending on the
structure of an underlying digraph. They later guess simultaneously the outcome
of their own die. Their objective is to find a strategy which maximises the
probability that they all guess correctly. The performance of the optimal
strategy for a graph is measured by the guessing number of the digraph.
  Christofides and Markstr\&quot;om studied guessing numbers of undirected graphs
and defined a strategy which they conjectured to be optimal. One of the main
results of this paper is a disproof of this conjecture.
  The main tool so far for computing guessing numbers of graphs is information
theoretic inequalities. In the paper we show that Shannon's information
inequalities, which work particularly well for a wide range of graph classes,
are not sufficient for computing the guessing number.
  Finally we pose a few more interesting questions some of which we can answer
and some which we leave as open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8357</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8357</id><created>2014-10-30</created><authors><author><keyname>Thai</keyname><forenames>Long</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>Executing Bag of Distributed Tasks on the Cloud: Investigating the
  Trade-offs Between Performance and Cost</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bag of Distributed Tasks (BoDT) can benefit from decentralised execution on
the Cloud. However, there is a trade-off between the performance that can be
achieved by employing a large number of Cloud VMs for the tasks and the
monetary constraints that are often placed by a user. The research reported in
this paper is motivated towards investigating this trade-off so that an optimal
plan for deploying BoDT applications on the cloud can be generated. A heuristic
algorithm, which considers the user's preference of performance and cost is
proposed and implemented. The feasibility of the algorithm is demonstrated by
generating execution plans for a sample application. The key result is that the
algorithm generates optimal execution plans for the application over 91\% of
the time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8359</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8359</id><created>2014-10-30</created><authors><author><keyname>Thai</keyname><forenames>Long</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Akgun</keyname><forenames>Ozgur</forenames></author><author><keyname>Miguel</keyname><forenames>Ian</forenames></author></authors><title>Optimal Deployment of Geographically Distributed Workflow Engines on the
  Cloud</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When orchestrating Web service workflows, the geographical placement of the
orchestration engine(s) can greatly affect workflow performance. Data may have
to be transferred across long geographical distances, which in turn increases
execution time and degrades the overall performance of a workflow. In this
paper, we present a framework that, given a DAG-based workflow specification,
computes the op- timal Amazon EC2 cloud regions to deploy the orchestration
engines and execute a workflow. The framework incorporates a constraint model
that solves the workflow deployment problem, which is generated using an
automated constraint modelling system. The feasibility of the framework is
evaluated by executing different sample workflows representative of sci-
entific workloads. The experimental results indicate that the framework reduces
the workflow execution time and provides a speed up of 1.3x-2.5x over
centralised approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8366</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8366</id><created>2014-10-30</created><authors><author><keyname>Sedaghat</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Ralf</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>On Optimum Asymptotic Multiuser Efficiency of Randomly Spread CDMA</title><categories>cs.IT math.IT</categories><comments>19 pages, 1 figure, submitted to Transaction on Information Theory</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We extend the result by Tse and Verd\'{u} on the optimum asymptotic multiuser
efficiency of randomly spread CDMA with Binary Phase Shift Keying (BPSK) input.
Random Gaussian and random binary antipodal spreading are considered. We obtain
the optimum asymptotic multiuser efficiency of a $K$-user system with spreading
gain $N$ when $K$ and $N\rightarrow\infty$ and the loading factor,
$\frac{K}{N}$, grows logarithmically with $K$ under some conditions. It is
shown that the optimum detector in a Gaussian randomly spread CDMA system has a
performance close to the single user system at high Signal to Noise Ratio (SNR)
when $K$ and $N\rightarrow\infty$ and the loading factor, $\frac{K}{N}$, is
kept less than $\frac{\log_3K}{2}$. Random binary antipodal matrices are also
studied and a lower bound for the optimum asymptotic multiuser efficiency is
obtained. Furthermore, we investigate the connection between detecting matrices
in the coin weighing problem and optimum asymptotic multiuser efficiency. We
obtain a condition such that for any binary input, an $N\times K$ random matrix
whose entries are chosen randomly from a finite set, is a detecting matrix as
$K$ and $N\rightarrow \infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8385</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8385</id><created>2014-10-30</created><updated>2015-03-29</updated><authors><author><keyname>D'yachkov</keyname><forenames>Arkadii</forenames></author><author><keyname>Vorobyev</keyname><forenames>Ilya</forenames></author><author><keyname>Polyanskii</keyname><forenames>Nikita</forenames></author><author><keyname>Shchukin</keyname><forenames>Vladislav</forenames></author></authors><title>Symmetric Disjunctive List-Decoding Codes</title><categories>cs.IT math.IT</categories><comments>18 pages, 1 figure, 1 table, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary code is said to be a disjunctive list-decoding $s_L$-code (LD
$s_L$-code), $s \ge 2$, $L \ge 1$, if the code is identified by the incidence
matrix of a family of finite sets in which the union (or disjunctive sum) of
any $s$ sets can cover not more than $L-1$ other sets of the family. In this
paper, we consider a similar class of binary codes which are based on a {\em
symmetric disjunctive sum} (SDS) of binary symbols. By definition, the
symmetric disjunctive sum (SDS) takes values from the ternary alphabet $\{0, 1,
*\}$, where the symbol~$*$ denotes &quot;erasure&quot;. Namely: SDS is equal to $0$ ($1$)
if all its binary symbols are equal to $0$ ($1$), otherwise SDS is equal
to~$*$. List decoding codes for symmetric disjunctive sum are said to be {\em
symmetric disjunctive list-decoding $s_L$-codes} (SLD $s_L$-codes). In the
given paper, we remind some applications of SLD $s_L$-codes which motivate the
concept of symmetric disjunctive sum. We refine the known relations between
parameters of LD $s_L$-codes and SLD $s_L$-codes. For the ensemble of binary
constant-weight codes we develop a random coding method to obtain lower bounds
on the rate of these codes. Our lower bounds improve the known random coding
bounds obtained up to now using the ensemble with independent symbols of
codewords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8402</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8402</id><created>2014-10-30</created><authors><author><keyname>Mondal</keyname><forenames>Nabarun</forenames></author><author><keyname>Ghosh</keyname><forenames>Partha P.</forenames></author></authors><title>On The Dynamical Nature Of Computation</title><categories>cs.OH</categories><comments>arXiv admin note: substantial text overlap with arXiv:1407.7417,
  arXiv:1111.4949</comments><msc-class>03D10, 65P20, 68Q05, 68Q87, 68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamical Systems theory generally deals with fixed point iterations of
continuous functions. Computation by Turing machine although is a fixed point
iteration but is not continuous. This specific category of fixed point
iterations can only be studied using their orbits. Therefore the standard
notion of chaos is not immediately applicable. However, when a suitable
definition is used, it is found that the notion of chaos and fractal sets
exists even in computation. It is found that a non terminating Computation will
be almost surely chaotic, and autonomous learning will almost surely identify
fractal only sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8420</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8420</id><created>2014-10-30</created><authors><author><keyname>Blais</keyname><forenames>Eric</forenames></author><author><keyname>Canonne</keyname><forenames>Cl&#xe9;ment L.</forenames></author><author><keyname>Oliveira</keyname><forenames>Igor C.</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author></authors><title>Learning circuits with few negations</title><categories>cs.CC cs.DM cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monotone Boolean functions, and the monotone Boolean circuits that compute
them, have been intensively studied in complexity theory. In this paper we
study the structure of Boolean functions in terms of the minimum number of
negations in any circuit computing them, a complexity measure that interpolates
between monotone functions and the class of all functions. We study this
generalization of monotonicity from the vantage point of learning theory,
giving near-matching upper and lower bounds on the uniform-distribution
learnability of circuits in terms of the number of negations they contain. Our
upper bounds are based on a new structural characterization of negation-limited
circuits that extends a classical result of A. A. Markov. Our lower bounds,
which employ Fourier-analytic tools from hardness amplification, give new
results even for circuits with no negations (i.e. monotone functions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8422</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8422</id><created>2014-10-30</created><authors><author><keyname>Zhu</keyname><forenames>Yitao</forenames></author><author><keyname>Dopico</keyname><forenames>Daniel</forenames></author><author><keyname>Sandu</keyname><forenames>Corina</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Dynamic Response Optimization of Complex Multibody Systems in a Penalty
  Formulation using Adjoint Sensitivity</title><categories>nlin.CD cs.SY</categories><comments>11 pages, 7 figures, 7 tables, submitted to journal of computational
  nonlinear dynamics. arXiv admin note: substantial text overlap with
  arXiv:1405.5197</comments><msc-class>70E55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multibody dynamics simulations are currently widely accepted as valuable
means for dynamic performance analysis of mechanical systems. The evolution of
theoretical and computational aspects of the multibody dynamics discipline make
it conducive these days for other types of applications, in addition to pure
simulations. One very important such application is design optimization. A very
important first step towards design optimization is sensitivity analysis of
multibody system dynamics. Dynamic sensitivities are often calculated by means
of finite differences. Depending of the number of parameters involved, this
procedure can be computationally expensive. Moreover, in many cases, the
results suffer from low accuracy when real perturbations are used. The main
contribution to the state-of-the-art brought by this study is the development
of the adjoint sensitivity approach of multibody systems in the context of the
penalty formulation. The theory developed is demonstrated on one academic case
study, a five-bar mechanism, and on one real-life system, a 14-DOF vehicle
model. The five-bar mechanism is used to illustrate the sensitivity approach
derived in this paper. The full vehicle model is used to demonstrate the
capability of the new approach developed to perform sensitivity analysis and
gradient-based optimization for large and complex multibody systems with
respect to multiple design parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8433</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8433</id><created>2014-10-30</created><updated>2015-03-06</updated><authors><author><keyname>Presman</keyname><forenames>Noam</forenames></author><author><keyname>Shapira</keyname><forenames>Ofer</forenames></author><author><keyname>Litsyn</keyname><forenames>Simon</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author></authors><title>Binary Polarization Kernels from Code Decompositions</title><categories>cs.IT math.IT</categories><comments>The paper was accepted for publication in the Transactions on
  Information Theory. It can be considered as an extended version of &quot;Binary
  Polar Code Kernels from Code Decompositions&quot; arXiv:1101.0764</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, code decompositions (a.k.a. code nestings) are used to design
binary polarization kernels. The proposed kernels are in general non-linear.
They provide a better polarization exponent than the previously known kernels
of the same dimensions. In particular, non-linear kernels of dimensions 14, 15,
and 16 are constructed and are shown to have optimal asymptotic
error-correction performance. The optimality is proved by showing that the
exponents of these kernels achieve a new upper bound that is developed in this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8440</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8440</id><created>2014-10-30</created><updated>2014-12-15</updated><authors><author><keyname>Ganesan</keyname><forenames>Abhinav</forenames></author><author><keyname>Ebrahimi</keyname><forenames>Javad</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Non-Adaptive Group Testing with Inhibitors</title><categories>cs.IT math.IT</categories><comments>Updated with results for the case of knowledge of only upper bounds
  on no. of defectives and inhibitors; 11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group testing with inhibitors (GTI) introduced by Farach at al. is studied in
this paper. There are three types of items, $d$ defectives, $r$ inhibitors and
$n-d-r$ normal items in a population of $n$ items. The presence of any
inhibitor in a test can prevent the expression of a defective. For this model,
we propose a probabilistic non-adaptive pooling design with a low complexity
decoding algorithm. We show that the sample complexity of the number of tests
required for guaranteed recovery with vanishing error probability using the
proposed algorithm scales as $T=O(d \log n)$ and $T=O(\frac{r^2}{d}\log n)$ in
the regimes $r=O(d)$ and $d=o(r)$ respectively. In the former regime, the
number of tests meets the lower bound order while in the latter regime, the
number of tests is shown to exceed the lower bound order by a $\log
\frac{r}{d}$ multiplicative factor. When only upper bounds on the number of
defectives $D$ and the number of inhibitors $R$ are given instead of their
exact values, the sample complexity of the number of tests using the proposed
algorithm scales as $T=O(D \log n)$ and $T=O(R^2 \log n)$ in the regimes
$R^2=O(D)$ and $D=o(R^2)$ respectively. In the former regime, the number of
tests meets the lower bound order while in the latter regime, the number of
tests exceeds the lower bound order by a $\log R$ multiplicative factor. The
time complexity of the proposed decoding algorithms scale as $O(nT)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8464</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8464</id><created>2014-10-30</created><updated>2015-03-25</updated><authors><author><keyname>Mart&#xed;n-Mart&#xed;n</keyname><forenames>Alberto</forenames></author><author><keyname>Ordu&#xf1;a-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ayll&#xf3;n</keyname><forenames>Juan Manuel</forenames></author><author><keyname>L&#xf3;pez-C&#xf3;zar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Does Google Scholar contain all highly cited documents (1950-2013)?</title><categories>cs.DL</categories><comments>Full raw data available at:
  http://dx.doi.org/10.6084/m9.figshare.1224314</comments><report-no>EC3's Working Papers 19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of highly cited documents on Google Scholar (GS) has never been
addressed to date in a comprehensive manner. The objective of this work is to
identify the set of highly cited documents in Google Scholar and define their
core characteristics: their languages, their file format, or how many of them
can be accessed free of charge. We will also try to answer some additional
questions that hopefully shed some light about the use of GS as a tool for
assessing scientific impact through citations. The decalogue of research
questions is shown below:
  1. Which are the most cited documents in GS?
  2. Which are the most cited document types in GS?
  3. What languages are the most cited documents written in GS?
  4. How many highly cited documents are freely accessible?
  4.1 What file types are the most commonly used to store these highly cited
documents?
  4.2 Which are the main providers of these documents?
  5. How many of the highly cited documents indexed by GS are also indexed by
WoS?
  6. Is there a correlation between the number of citations that these highly
cited documents have received in GS and the number of citations they have
received in WoS?
  7. How many versions of these highly cited documents has GS detected?
  8. Is there a correlation between the number of versions GS has detected for
these documents, and the number citations they have received?
  9. Is there a correlation between the number of versions GS has detected for
these documents, and their position in the search engine result pages?
  10. Is there some relation between the positions these documents occupy in
the search engine result pages, and the number of citations they have received?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8470</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8470</id><created>2014-10-30</created><authors><author><keyname>Dowek</keyname><forenames>Gilles</forenames></author><author><keyname>Jiang</keyname><forenames>Ying</forenames></author></authors><title>Cut-elimination and the decidability of reachability in alternating
  pushdown systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new proof of the decidability of reachability in alternating
pushdown systems, showing that it is a simple consequence of a cut-elimination
theorem for some natural-deduction style inference systems. Then, we show how
this result can be used to extend an alternating pushdown system into a
complete system where for every configuration $A$, either $A$ or $\neg A$ is
provable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8489</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8489</id><created>2014-10-28</created><authors><author><keyname>Norris</keyname><forenames>Scott A.</forenames></author></authors><title>PyCraters: A Python framework for crater function analysis</title><categories>physics.comp-ph cond-mat.mtrl-sci cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a Python framework designed to automate the most common tasks
associated with the extraction and upscaling of the statistics of single-impact
crater functions to inform coefficients of continuum equations describing
surface morphology evolution. Designed with ease-of-use in mind, the framework
allows users to extract meaningful statistical estimates with very short Python
programs. Wrappers to interface with specific simulation packages, routines for
statistical extraction of output, and fitting and differentiation libraries are
all hidden behind simple, high-level user-facing functions. In addition, the
framework is extensible, allowing advanced users to specify the collection of
specialized statistics or the creation of customized plots. The framework is
hosted on the BitBucket service under an open-source license, with the aim of
helping non-specialists easily extract preliminary estimates of relevant crater
function results associated with a particular experimental system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8498</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8498</id><created>2014-10-30</created><updated>2014-12-19</updated><authors><author><keyname>Strubell</keyname><forenames>Emma</forenames></author><author><keyname>Vilnis</keyname><forenames>Luke</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Training for Fast Sequential Prediction Using Dynamic Feature Selection</title><categories>cs.CL cs.AI</categories><comments>5 pages, NIPS Modern ML + NLP Workshop 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present paired learning and inference algorithms for significantly
reducing computation and increasing speed of the vector dot products in the
classifiers that are at the heart of many NLP components. This is accomplished
by partitioning the features into a sequence of templates which are ordered
such that high confidence can often be reached using only a small fraction of
all features. Parameter estimation is arranged to maximize accuracy and early
confidence in this sequence. We present experiments in left-to-right
part-of-speech tagging on WSJ, demonstrating that we can preserve accuracy
above 97% with over a five-fold reduction in run-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8507</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8507</id><created>2014-10-30</created><authors><author><keyname>Taylor</keyname><forenames>M. B.</forenames></author></authors><title>External Use of TOPCAT's Plotting Library</title><categories>astro-ph.IM cs.MS</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The table analysis application TOPCAT uses a custom Java plotting library for
highly configurable high-performance interactive or exported visualisations in
two and three dimensions. We present here a variety of ways for end users or
application developers to make use of this library outside of the TOPCAT
application: via the command-line suite STILTS or its Jython variant JyStilts,
via a traditional Java API, or by programmatically assigning values to a set of
parameters in java code or using some form of inter-process communication. The
library has been built with large datasets in mind; interactive plots scale
well up to several million points, and static output to standard graphics
formats is possible for unlimited sized input data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8509</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8509</id><created>2014-10-30</created><updated>2014-11-10</updated><authors><author><keyname>Kim</keyname><forenames>Su</forenames></author></authors><title>Photomapping Using Aerial Vehicle</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Creating a photomap plays a critical role in navigation. Therefore, flying
vehicles are usually used to create topdown maps of the environment. In this
report we used two different aerial vehicles to create a map in a simulated
environment
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8515</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8515</id><created>2014-10-30</created><authors><author><keyname>Tutunov</keyname><forenames>Rasul</forenames></author><author><keyname>Ammar</keyname><forenames>Haitham Bou</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author><author><keyname>Eaton</keyname><forenames>Eric</forenames></author></authors><title>On the Degree Distribution of P\'{o}lya Urn Graph Processes</title><categories>math.PR cs.SI physics.soc-ph</categories><comments>26 pages, 2 figures</comments><msc-class>60Cxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tighter bound on the degree distribution of arbitrary
P\'{o}lya urn graph processes, proving that the proportion of vertices with
degree $d$ obeys a power-law distribution $P(d) \propto d^{-\gamma}$ for $d
\leq n^{\frac{1}{6}-\epsilon}$ for any $\epsilon &gt; 0$, where $n$ represents the
number of vertices in the network. Previous work by Bollob\'{a}s et al.
formalized the well-known preferential attachment model of Barab\'{a}si and
Albert, and showed that the power-law distribution held for $d \leq
n^{\frac{1}{15}}$ with $\gamma = 3$. Our revised bound represents a significant
improvement over existing models of degree distribution in scale-free networks,
where its tightness is restricted by the Azuma-Hoeffding concentration
inequality for martingales. We achieve this tighter bound through a careful
analysis of the first set of vertices in the network generation process, and
show that the newly acquired is at the edge of exhausting Bollob\'as model in
the sense that the degree expectation breaks down for other powers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8516</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8516</id><created>2014-10-30</created><updated>2015-04-10</updated><authors><author><keyname>Dinh</keyname><forenames>Laurent</forenames></author><author><keyname>Krueger</keyname><forenames>David</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>NICE: Non-linear Independent Components Estimation</title><categories>cs.LG</categories><comments>11 pages and 2 pages Appendix, workshop paper at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a deep learning framework for modeling complex high-dimensional
densities called Non-linear Independent Component Estimation (NICE). It is
based on the idea that a good representation is one in which the data has a
distribution that is easy to model. For this purpose, a non-linear
deterministic transformation of the data is learned that maps it to a latent
space so as to make the transformed data conform to a factorized distribution,
i.e., resulting in independent latent variables. We parametrize this
transformation so that computing the Jacobian determinant and inverse transform
is trivial, yet we maintain the ability to learn complex non-linear
transformations, via a composition of simple building blocks, each based on a
deep neural network. The training criterion is simply the exact log-likelihood,
which is tractable. Unbiased ancestral sampling is also easy. We show that this
approach yields good generative models on four image datasets and can be used
for inpainting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8521</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8521</id><created>2014-10-23</created><updated>2016-02-17</updated><authors><author><keyname>Giles</keyname><forenames>Alexander P.</forenames></author><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author></authors><title>Betweenness Centrality in Dense Random Geometric Networks</title><categories>cs.SI cond-mat.stat-mech cs.CG cs.NI math.PR physics.soc-ph</categories><comments>6 pages, 3 figures</comments><journal-ref>Proc. IEEE ICC 2015, pp. 6450-6455 (London, UK)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random geometric networks consist of 1) a set of nodes embedded randomly in a
bounded domain $\mathcal{V} \subseteq \mathbb{R}^d$ and 2) links formed
probabilistically according to a function of mutual Euclidean separation. We
quantify how often all paths in the network characterisable as topologically
`shortest' contain a given node (betweenness centrality), deriving an
expression in terms of a known integral whenever 1) the network boundary is the
perimeter of a disk and 2) the network is extremely dense. Our method shows how
similar formulas can be obtained for any convex geometry. Numerical
corroboration is provided, as well as a discussion of our formula's potential
use for cluster head election and boundary detection in densely deployed
wireless ad hoc networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8525</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8525</id><created>2014-10-30</created><updated>2015-10-30</updated><authors><author><keyname>Schieber</keyname><forenames>Tiago A.</forenames></author><author><keyname>Carpi</keyname><forenames>Laura</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author><author><keyname>Rosso</keyname><forenames>Osvaldo A</forenames></author><author><keyname>Pardalos</keyname><forenames>Panos M.</forenames></author><author><keyname>Ravetti</keyname><forenames>Martin G.</forenames></author></authors><title>Information Theory Perspective on Network Robustness</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>5 pages, 2 figures, submitted</comments><doi>10.1016/j.physleta.2015.10.055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crucial challenge in network theory is the study of the robustness of a
network after facing a sequence of failures. In this work, we propose a
dynamical definition of network's robustness based on Information Theory, that
considers measurements of the structural changes caused by failures of the
network's components. Failures are defined here, as a temporal process defined
in a sequence. The robustness of the network is then evaluated by measuring
dissimilarities between topologies after each time step of the sequence,
providing a dynamical information about the topological damage. We thoroughly
analyze the efficiency of the method in capturing small perturbations by
considering both, the degree and distance distributions. We found the network's
distance distribution more consistent in capturing network structural
deviations, as better reflects the consequences of the failures. Theoretical
examples and real networks are used to study the performance of this
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8536</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8536</id><created>2014-10-30</created><authors><author><keyname>Lantzaki</keyname><forenames>Christina</forenames></author><author><keyname>Tzitzikas</keyname><forenames>Yannis</forenames></author></authors><title>Tasks that Require, or can Benefit from, Matching Blank Nodes</title><categories>cs.AI cs.DB</categories><comments>19 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In various domains and cases, we observe the creation and usage of
information elements which are unnamed. Such elements do not have a name, or
may have a name that is not externally referable (usually meaningless and not
persistent over time). This paper discusses why we will never `escape' from the
problem of having to construct mappings between such unnamed elements in
information systems. Since unnamed elements nowadays occur very often in the
framework of the Semantic Web and Linked Data as blank nodes, the paper
describes scenarios that can benefit from methods that compute mappings between
the unnamed elements. For each scenario, the corresponding bnode matching
problem is formally defined. Based on this analysis, we try to reach to more a
general formulation of the problem, which can be useful for guiding the
required technological advances. To this end, the paper finally discusses
methods to realize blank node matching, the implementations that exist, and
identifies open issues and challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8541</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8541</id><created>2014-10-30</created><updated>2015-02-10</updated><authors><author><keyname>Kim</keyname><forenames>Yongjune</forenames></author><author><keyname>Mateescu</keyname><forenames>Robert</forenames></author><author><keyname>Song</keyname><forenames>Seung-Hwan</forenames></author><author><keyname>Bandic</keyname><forenames>Zvonimir</forenames></author><author><keyname>Kumar</keyname><forenames>B. V. K. Vijaya</forenames></author></authors><title>Coding scheme for 3D vertical flash memory</title><categories>cs.IT math.IT</categories><comments>7 pages, 9 figures. accepted to ICC 2015. arXiv admin note: text
  overlap with arXiv:1410.1775</comments><doi>10.1109/ICC.2015.7248332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently introduced 3D vertical flash memory is expected to be a disruptive
technology since it overcomes scaling challenges of conventional 2D planar
flash memory by stacking up cells in the vertical direction. However, 3D
vertical flash memory suffers from a new problem known as fast detrapping,
which is a rapid charge loss problem. In this paper, we propose a scheme to
compensate the effect of fast detrapping by intentional inter-cell interference
(ICI). In order to properly control the intentional ICI, our scheme relies on a
coding technique that incorporates the side information of fast detrapping
during the encoding stage. This technique is closely connected to the
well-known problem of coding in a memory with defective cells. Numerical
results show that the proposed scheme can effectively address the problem of
fast detrapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8544</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8544</id><created>2014-10-30</created><authors><author><keyname>Lariviere</keyname><forenames>Vincent</forenames></author><author><keyname>Sugimoto</keyname><forenames>Cassidy</forenames></author><author><keyname>Tsou</keyname><forenames>Andrew</forenames></author><author><keyname>Gingras</keyname><forenames>Yves</forenames></author></authors><title>Team size matters: Collaboration and scientific impact since 1900</title><categories>cs.DL</categories><comments>18 pages, 6 figures, Journal of the Association for Information
  Science and Technology, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides the first historical analysis of the relationship between
collaboration and scientific impact, using three indicators of collaboration
(number of authors, number of addresses, and number of countries) and including
articles published between 1900 and 2011. The results demonstrate that an
increase in the number of authors leads to an increase in impact--from the
beginning of the last century onwards--and that this is not simply due to
self-citations. A similar trend is also observed for the number of addresses
and number of countries represented in the byline of an article. However, the
constant inflation of collaboration since 1900 has resulted in diminishing
citation returns: larger and more diverse (in terms of institutional and
country affiliation) teams are necessary to realize higher impact. The paper
concludes with a discussion of the potential causes of the impact gain in
citations of collaborative papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8546</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8546</id><created>2014-10-30</created><updated>2015-04-14</updated><authors><author><keyname>Bernard</keyname><forenames>Florian</forenames></author><author><keyname>Thunberg</keyname><forenames>Johan</forenames></author><author><keyname>Gemmar</keyname><forenames>Peter</forenames></author><author><keyname>Hertel</keyname><forenames>Frank</forenames></author><author><keyname>Husch</keyname><forenames>Andreas</forenames></author><author><keyname>Goncalves</keyname><forenames>Jorge</forenames></author></authors><title>A Solution for Multi-Alignment by Transformation Synchronisation</title><categories>cs.CV stat.ML</categories><comments>Accepted for CVPR 2015 (please cite CVPR version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The alignment of a set of objects by means of transformations plays an
important role in computer vision. Whilst the case for only two objects can be
solved globally, when multiple objects are considered usually iterative methods
are used. In practice the iterative methods perform well if the relative
transformations between any pair of objects are free of noise. However, if only
noisy relative transformations are available (e.g. due to missing data or wrong
correspondences) the iterative methods may fail.
  Based on the observation that the underlying noise-free transformations can
be retrieved from the null space of a matrix that can directly be obtained from
pairwise alignments, this paper presents a novel method for the synchronisation
of pairwise transformations such that they are transitively consistent.
  Simulations demonstrate that for noisy transformations, a large proportion of
missing data and even for wrong correspondence assignments the method delivers
encouraging results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8553</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8553</id><created>2014-10-30</created><authors><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Ye</keyname><forenames>Peng</forenames></author><author><keyname>Rodrigues</keyname><forenames>Paul</forenames></author><author><keyname>Zajic</keyname><forenames>David</forenames></author><author><keyname>Doermann</keyname><forenames>David</forenames></author></authors><title>A random forest system combination approach for error detection in
  digital dictionaries</title><categories>cs.CL cs.LG stat.ML</categories><comments>9 pages, 7 figures, 10 tables; appeared in Proceedings of the
  Workshop on Innovative Hybrid Approaches to the Processing of Textual Data,
  April 2012</comments><acm-class>I.2.7; I.2.6; I.5.1; I.5.4</acm-class><journal-ref>In Proceedings of the Workshop on Innovative Hybrid Approaches to
  the Processing of Textual Data, pages 78-86, Avignon, France, April 2012.
  Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When digitizing a print bilingual dictionary, whether via optical character
recognition or manual entry, it is inevitable that errors are introduced into
the electronic version that is created. We investigate automating the process
of detecting errors in an XML representation of a digitized print dictionary
using a hybrid approach that combines rule-based, feature-based, and language
model-based methods. We investigate combining methods and show that using
random forests is a promising approach. We find that in isolation, unsupervised
methods rival the performance of supervised methods. Random forests typically
require training data so we investigate how we can apply random forests to
combine individual base methods that are themselves unsupervised without
requiring large amounts of training data. Experiments reveal empirically that a
relatively small amount of data is sufficient and can potentially be further
reduced through specific selection criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8566</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8566</id><created>2014-10-30</created><updated>2015-03-25</updated><authors><author><keyname>D'yachkov</keyname><forenames>Arkadii</forenames></author><author><keyname>Vorobyev</keyname><forenames>Ilya</forenames></author><author><keyname>Polyanskii</keyname><forenames>Nikita</forenames></author><author><keyname>Shchukin</keyname><forenames>Vladislav</forenames></author></authors><title>Almost Cover-Free Codes and Designs</title><categories>cs.IT math.IT</categories><comments>18 pages, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $s$-subset of codewords of a binary code $X$ is said to be an {\em
$(s,\ell)$-bad} in $X$ if the code $X$ contains a subset of other $\ell$
codewords such that the conjunction of the $\ell$ codewords is covered by the
disjunctive sum of the $s$ codewords. Otherwise, the $s$-subset of codewords of
$X$ is said to be an {\em $(s,\ell)$-good} in~$X$.mA binary code $X$ is said to
be a cover-free $(s,\ell)$-code if the code $X$ does not contain $(s,\ell)$-bad
subsets. In this paper, we introduce a natural {\em probabilistic}
generalization of cover-free $(s,\ell)$-codes, namely: a binary code is said to
be an almost cover-free $(s,\ell)$-code if {\em almost all} $s$-subsets of its
codewords are $(s,\ell)$-good. We discuss the concept of almost cover-free
$(s,\ell)$-codes arising in combinatorial group testing problems connected with
the nonadaptive search of defective supersets (complexes). We develop a random
coding method based on the ensemble of binary constant weight codes to obtain
lower bounds on the capacity of such codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8568</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8568</id><created>2014-10-30</created><authors><author><keyname>Shin</keyname><forenames>S.</forenames></author><author><keyname>Chergui</keyname><forenames>J.</forenames></author><author><keyname>Juric</keyname><forenames>D.</forenames></author></authors><title>A Solver for Massively Parallel Direct Numerical Simulation of
  Three-Dimensional Multiphase Flows</title><categories>physics.flu-dyn cs.DC physics.comp-ph</categories><comments>42 pages, 14 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new solver for massively parallel simulations of fully
three-dimensional multiphase flows. The solver runs on a variety of computer
architectures from laptops to supercomputers and on 65536 threads or more
(limited only by the availability to us of more threads). The code is wholly
written by the authors in Fortran 2003 and uses a domain decomposition strategy
for parallelization with MPI. The fluid interface solver is based on a parallel
implementation of the LCRM hybrid Front Tracking/Level Set method designed to
handle highly deforming interfaces with complex topology changes. We discuss
the implementation of this interface method and its particular suitability to
distributed processing where all operations are carried out locally on
distributed subdomains. We have developed parallel GMRES and Multigrid
iterative solvers suited to the linear systems arising from the implicit
solution of the fluid velocities and pressure in the presence of strong density
and viscosity discontinuities across fluid phases. Particular attention is
drawn to the details and performance of the parallel Multigrid solver. The code
includes modules for flow interaction with immersed solid objects, contact line
dynamics, species and thermal transport with phase change. Here, however, we
focus on the simulation of the canonical problem of drop splash onto a liquid
film and report on the parallel performance of the code on varying numbers of
threads. The 3D simulations were run on mesh resolutions up to $1024^3$ with
results at the higher resolutions showing the fine details and features of
droplet ejection, crown formation and rim instability observed under similar
experimental conditions. Keywords:
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8576</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8576</id><created>2014-10-30</created><authors><author><keyname>Antal</keyname><forenames>Balint</forenames></author><author><keyname>Hajdu</keyname><forenames>Andras</forenames></author></authors><title>An ensemble-based system for automatic screening of diabetic retinopathy</title><categories>cs.CV cs.LG stat.AP stat.ML</categories><journal-ref>Knowledge-Based Systems, Elsevier, Volume 60, April 2014, Pages
  20-27</journal-ref><doi>10.1016/j.knosys.2013.12.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an ensemble-based method for the screening of diabetic
retinopathy (DR) is proposed. This approach is based on features extracted from
the output of several retinal image processing algorithms, such as image-level
(quality assessment, pre-screening, AM/FM), lesion-specific (microaneurysms,
exudates) and anatomical (macula, optic disc) components. The actual decision
about the presence of the disease is then made by an ensemble of machine
learning classifiers. We have tested our approach on the publicly available
Messidor database, where 90% sensitivity, 91% specificity and 90% accuracy and
0.989 AUC are achieved in a disease/no-disease setting. These results are
highly competitive in this field and suggest that retinal image processing is a
valid approach for automatic DR screening.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8577</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8577</id><created>2014-10-30</created><authors><author><keyname>Antal</keyname><forenames>Balint</forenames></author><author><keyname>Hajdu</keyname><forenames>Andras</forenames></author></authors><title>An Ensemble-based System for Microaneurysm Detection and Diabetic
  Retinopathy Grading</title><categories>cs.CV cs.AI stat.AP stat.ML</categories><journal-ref>IEEE Transactions on Biomedical Engineering, vol.59, no.6, pp.
  1720-1726, June 2012</journal-ref><doi>10.1109/TBME.2012.2193126</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable microaneurysm detection in digital fundus images is still an open
issue in medical image processing. We propose an ensemble-based framework to
improve microaneurysm detection. Unlike the well-known approach of considering
the output of multiple classifiers, we propose a combination of internal
components of microaneurysm detectors, namely preprocessing methods and
candidate extractors. We have evaluated our approach for microaneurysm
detection in an online competition, where this algorithm is currently ranked as
first and also on two other databases. Since microaneurysm detection is
decisive in diabetic retinopathy grading, we also tested the proposed method
for this task on the publicly available Messidor database, where a promising
AUC 0.90 with 0.01 uncertainty is achieved in a 'DR/non-DR'-type classification
based on the presence or absence of the microaneurysms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8580</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8580</id><created>2014-10-30</created><authors><author><keyname>Lawlor</keyname><forenames>Matthew</forenames></author><author><keyname>Zucker</keyname><forenames>Steven</forenames></author></authors><title>An Online Algorithm for Learning Selectivity to Mixture Means</title><categories>q-bio.NC cs.LG</categories><comments>Extended technical companion to a presentation at NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a biologically-plausible learning rule called Triplet BCM that
provably converges to the class means of general mixture models. This rule
generalizes the classical BCM neural rule, and provides a novel interpretation
of classical BCM as performing a kind of tensor decomposition. It achieves a
substantial generalization over classical BCM by incorporating triplets of
samples from the mixtures, which provides a novel information processing
interpretation to spike-timing-dependent plasticity. We provide complete proofs
of convergence of this learning rule, and an extended discussion of the
connection between BCM and tensor learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8581</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8581</id><created>2014-10-30</created><authors><author><keyname>K&#xfc;&#xe7;&#xfc;k</keyname><forenames>Dilek</forenames></author><author><keyname>Arslan</keyname><forenames>Yusuf</forenames></author></authors><title>Semi-Automatic Construction of a Domain Ontology for Wind Energy Using
  Wikipedia Articles</title><categories>cs.CL cs.CE</categories><journal-ref>Renewable Energy, Volume 62, pp. 484-489, February 2014</journal-ref><doi>10.1016/j.renene.2013.08.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain ontologies are important information sources for knowledge-based
systems. Yet, building domain ontologies from scratch is known to be a very
labor-intensive process. In this study, we present our semi-automatic approach
to building an ontology for the domain of wind energy which is an important
type of renewable energy with a growing share in electricity generation all
over the world. Related Wikipedia articles are first processed in an automated
manner to determine the basic concepts of the domain together with their
properties and next the concepts, properties, and relationships are organized
to arrive at the ultimate ontology. We also provide pointers to other
engineering ontologies which could be utilized together with the proposed wind
energy ontology in addition to its prospective application areas. The current
study is significant as, to the best of our knowledge, it proposes the first
considerably wide-coverage ontology for the wind energy domain and the ontology
is built through a semi-automatic process which makes use of the related Web
resources, thereby reducing the overall cost of the ontology building process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8584</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8584</id><created>2014-10-30</created><updated>2015-03-24</updated><authors><author><keyname>Basu</keyname><forenames>Amitabh</forenames></author><author><keyname>Hildebrand</keyname><forenames>Robert</forenames></author><author><keyname>K&#xf6;ppe</keyname><forenames>Matthias</forenames></author></authors><title>Light on the Infinite Group Relaxation</title><categories>math.OC cs.DM</categories><comments>45 pages</comments><msc-class>90C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a survey on the infinite group problem, an infinite-dimensional
relaxation of integer linear optimization problems introduced by Ralph Gomory
and Ellis Johnson in their groundbreaking papers titled &quot;Some continuous
functions related to corner polyhedra I, II&quot; [Math. Programming 3 (1972),
23-85, 359-389]. The survey presents the infinite group problem in the modern
context of cut generating functions. It focuses on the recent developments,
such as algorithms for testing extremality and breakthroughs for the k-row
problem for general k &gt;= 1 that extend previous work on the single-row and
two-row problems. The survey also includes some previously unpublished results;
among other things, it unveils piecewise linear extreme functions with more
than four different slopes. An interactive companion program, implemented in
the open-source computer algebra package Sage, provides an updated compendium
of known extreme functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8585</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8585</id><created>2014-10-30</created><authors><author><keyname>Kumar</keyname><forenames>Shrawan</forenames></author><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author></authors><title>Connections between conjectures of Alon-Tarsi, Hadamard-Howe, and
  integrals over the special unitary group</title><categories>math.AG cs.CC math.CO math.RT</categories><comments>7 pages</comments><msc-class>68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the Alon-Tarsi conjecture on Latin squares is equivalent to a very
special case of a conjecture made independently by Hadamard and Howe, and to
the non-vanishing of some interesting integrals over SU(n). Our investigations
were motivated by geometric complexity theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8586</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8586</id><created>2014-10-30</created><authors><author><keyname>Chen</keyname><forenames>Tao</forenames></author><author><keyname>Borth</keyname><forenames>Damian</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>DeepSentiBank: Visual Sentiment Concept Classification with Deep
  Convolutional Neural Networks</title><categories>cs.CV cs.LG cs.MM cs.NE</categories><comments>7 pages, 4 figures</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a visual sentiment concept classification method based
on deep convolutional neural networks (CNNs). The visual sentiment concepts are
adjective noun pairs (ANPs) automatically discovered from the tags of web
photos, and can be utilized as effective statistical cues for detecting
emotions depicted in the images. Nearly one million Flickr images tagged with
these ANPs are downloaded to train the classifiers of the concepts. We adopt
the popular model of deep convolutional neural networks which recently shows
great performance improvement on classifying large-scale web-based image
dataset such as ImageNet. Our deep CNNs model is trained based on Caffe, a
newly developed deep learning framework. To deal with the biased training data
which only contains images with strong sentiment and to prevent overfitting, we
initialize the model with the model weights trained from ImageNet. Performance
evaluation shows the newly trained deep CNNs model SentiBank 2.0 (or called
DeepSentiBank) is significantly improved in both annotation accuracy and
retrieval performance, compared to its predecessors which mainly use binary SVM
classification models.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="67000" completeListSize="102538">1122234|68001</resumptionToken>
</ListRecords>
</OAI-PMH>
