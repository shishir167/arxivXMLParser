<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:32:53Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|62001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2783</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2783</id><created>2014-06-11</created><authors><author><keyname>Rybakov</keyname><forenames>Vladimir</forenames></author></authors><title>Linear Non-Transitive Temporal Logic, Knowledge Operations, Algorithms
  for Admissibility</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1405.0559</comments><msc-class>03B44</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies problems of satisfiability, decidability and admissibility
of inference rules, conceptions of knowledge and agent's knowledge in
non-transitive temporal linear logic LTL(Past,m). We find algorithms solving
mentioned problems, justify our approach to consider linear non-transitive time
with several examples. Main, most complicated, technical new result is
decidability of LTL(Past,m) w.r.t. admissible rules. We discuss several ways to
formalize conceptions of knowledge and agent's knowledge within given approach
in non-transitive linear logic with models directed to past.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2785</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2785</id><created>2014-06-11</created><authors><author><keyname>Liang</keyname><forenames>Chulong</forenames></author><author><keyname>Hu</keyname><forenames>Jingnan</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author></authors><title>A New Class of Multiple-rate Codes Based on Block Markov Superposition
  Transmission</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hadamard transform~(HT) as over the binary field provides a natural way to
implement multiple-rate codes~(referred to as {\em HT-coset codes}), where the
code length $N=2^p$ is fixed but the code dimension $K$ can be varied from $1$
to $N-1$ by adjusting the set of frozen bits. The HT-coset codes, including
Reed-Muller~(RM) codes and polar codes as typical examples, can share a pair of
encoder and decoder with implementation complexity of order $O(N \log N)$.
However, to guarantee that all codes with designated rates perform well,
HT-coset coding usually requires a sufficiently large code length, which in
turn causes difficulties in the determination of which bits are better for
being frozen. In this paper, we propose to transmit short HT-coset codes in the
so-called block Markov superposition transmission~(BMST) manner. At the
transmitter, signals are spatially coupled via superposition, resulting in long
codes. At the receiver, these coupled signals are recovered by a sliding-window
iterative soft successive cancellation decoding algorithm. Most importantly,
the performance around or below the bit-error-rate~(BER) of $10^{-5}$ can be
predicted by a simple genie-aided lower bound. Both these bounds and simulation
results show that the BMST of short HT-coset codes performs well~(within one dB
away from the corresponding Shannon limits) in a wide range of code rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2791</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2791</id><created>2014-06-11</created><authors><author><keyname>Norouzi</keyname><forenames>Monire</forenames></author><author><keyname>Parsa</keyname><forenames>Saeed</forenames></author><author><keyname>Mahjur</keyname><forenames>Ali</forenames></author></authors><title>A new approach for formal behavioral modeling of protection services in
  antivirus systems</title><categories>cs.SE cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal method techniques provides a suitable platform for the software
development in software systems. Formal methods and formal verification is
necessary to prove the correctness and improve performance of software systems
in various levels of design and implementation, too. Security Discussion is an
important issue in computer systems. Since the antivirus applications have very
important role in computer systems security, verifying these applications is
very essential and necessary. In this paper, we present four new approaches for
antivirus system behavior and a behavioral model of protection services in the
antivirus system is proposed. We divided the behavioral model in to preventive
behavior and control behavior and then we formal these behaviors. Finally by
using some definitions we explain the way these behaviors are mapped on each
other by using our new approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2793</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2793</id><created>2014-06-11</created><authors><author><keyname>Gao</keyname><forenames>Song</forenames></author></authors><title>Towards a Frontier of Spatial Scientometric Studies</title><categories>cs.DL</categories><comments>10 pages,9 figures</comments><acm-class>H.2.8; H.3.5; H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The research field of spatial scientometrics is dedicated to measuring and
analyzing science with spatial components (e.g., location, place, mapping).
Because of the dynamic nature of this field, researchers from multidisciplinary
domains constantly contribute qualitative, quantitative and computational
approaches and technologies into scientometric analysis. This article aims to
giving a brief overview about this field by analyzing the publications in
(spatial) scientometrics collected from the Scopus database and introduces
recent frontier researches which integrate advanced spatial analysis and
geovisualization with Semantic Web technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2794</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2794</id><created>2014-06-11</created><authors><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>The Mean Interference-to-Signal Ratio and its Key Role in Cellular and
  Amorphous Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple yet powerful and versatile analytical framework to
approximate the SIR distribution in the downlink of cellular systems. It is
based on the mean interference-to-signal ratio and yields the horizontal gap
(SIR gain) between the SIR distribution in question and a reference SIR
distribution. As applications, we determine the SIR gain for base station
silencing, cooperation, and lattice deployment over a baseline architecture
that is based on a Poisson deployment of base stations and strongest-base
station association. The applications demonstrate that the proposed approach
unifies several recent results and provides a convenient framework for the
analysis and comparison of future network architectures and transmission
schemes, including amorphous networks where a user is served by multiple base
stations and, consequently, (hard) cell association becomes obsolete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2795</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2795</id><created>2014-06-11</created><authors><author><keyname>Das</keyname><forenames>Shantanu</forenames><affiliation>LIF</affiliation></author><author><keyname>Dereniowski</keyname><forenames>Dariusz</forenames><affiliation>INRIA Rocquencourt, LIAFA</affiliation></author><author><keyname>Kosowski</keyname><forenames>Adrian</forenames><affiliation>INRIA Rocquencourt, LIAFA</affiliation></author><author><keyname>Uznanski</keyname><forenames>Przemyslaw</forenames><affiliation>LIF</affiliation></author></authors><title>Rendezvous of Distance-aware Mobile Agents in Unknown Graphs</title><categories>cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of rendezvous of two mobile agents starting at distinct
locations in an unknown graph. The agents have distinct labels and walk in
synchronous steps. However the graph is unlabelled and the agents have no means
of marking the nodes of the graph and cannot communicate with or see each other
until they meet at a node. When the graph is very large we want the time to
rendezvous to be independent of the graph size and to depend only on the
initial distance between the agents and some local parameters such as the
degree of the vertices, and the size of the agent's label. It is well known
that even for simple graphs of degree $\Delta$, the rendezvous time can be
exponential in $\Delta$ in the worst case. In this paper, we introduce a new
version of the rendezvous problem where the agents are equipped with a device
that measures its distance to the other agent after every step. We show that
these \emph{distance-aware} agents are able to rendezvous in any unknown graph,
in time polynomial in all the local parameters such the degree of the nodes,
the initial distance $D$ and the size of the smaller of the two agent labels $l
= \min(l_1, l_2)$. Our algorithm has a time complexity of
$O(\Delta(D+\log{l}))$ and we show an almost matching lower bound of
$\Omega(\Delta(D+\log{l}/\log{\Delta}))$ on the time complexity of any
rendezvous algorithm in our scenario. Further, this lower bound extends
existing lower bounds for the general rendezvous problem without distance
awareness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2807</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2807</id><created>2014-06-11</created><updated>2014-06-12</updated><authors><author><keyname>Li</keyname><forenames>Yin</forenames></author><author><keyname>Hou</keyname><forenames>Xiaodi</forenames></author><author><keyname>Koch</keyname><forenames>Christof</forenames></author><author><keyname>Rehg</keyname><forenames>James M.</forenames></author><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author></authors><title>The Secrets of Salient Object Segmentation</title><categories>cs.CV</categories><comments>15 pages, 8 figures. Conference version was accepted by CVPR 2014</comments><report-no>CBMM Memmo #14</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we provide an extensive evaluation of fixation prediction and
salient object segmentation algorithms as well as statistics of major datasets.
Our analysis identifies serious design flaws of existing salient object
benchmarks, called the dataset design bias, by over emphasizing the
stereotypical concepts of saliency. The dataset design bias does not only
create the discomforting disconnection between fixations and salient object
segmentation, but also misleads the algorithm designing. Based on our analysis,
we propose a new high quality dataset that offers both fixation and salient
object segmentation ground-truth. With fixations and salient object being
presented simultaneously, we are able to bridge the gap between fixations and
salient objects, and propose a novel method for salient object segmentation.
Finally, we report significant benchmark progress on three existing datasets of
segmenting salient objects
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2808</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2808</id><created>2014-06-11</created><authors><author><keyname>Kanso</keyname><forenames>Bilal</forenames></author><author><keyname>Chebaro</keyname><forenames>Omar</forenames></author></authors><title>Compositional Testing For FSM-Based Models</title><categories>cs.SE cs.FL</categories><comments>20 pages, ijsea journal</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), May 2014, Volume 5, Number 3</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The contribution of this paper is threefold: first, it defines a framework
for modelling component-based systems, as well as a formalization of
integration rules to combine their behavior. This is based on finite state
machines (FSM). Second, it studies compositional conformance testing i.e.
checking whether an implementation made of conforming components combined with
integration operators is conform to its specification. Third, it shows the
correctness of the global system can be obtained by testing the components
involved into it towards the projection of the global specification on the
specifications of the components. This result is useful to build adequate test
purposes for testing components taking into account the system where they are
plugged in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2812</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2812</id><created>2014-06-11</created><authors><author><keyname>Ahmedi</keyname><forenames>Basri</forenames></author><author><keyname>Mitrevski</keyname><forenames>Pece</forenames></author></authors><title>On the Development of Methodology for Planning and Cost-Modeling of a
  Wide Area Network</title><categories>cs.NI</categories><comments>20 pages, 5 figures</comments><acm-class>C.2.1; C.2.5</acm-class><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.6, No.3, May 2014</journal-ref><doi>10.5121/ijcnc.2014.6307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most important stages in designing a computer network in a wider
geographical area include: definition of requirements, topological description,
identification and calculation of relevant parameters (i.e. traffic matrix),
determining the shortest path between nodes, quantification of the effect of
various levels of technical and technological development of urban areas
involved, the cost of technology, and the cost of services. These parameters
differ for WAN networks in different regions - their calculation depends
directly on the data &quot;in the field&quot;: number of inhabitants, distance between
populated areas, network traffic density, as well as available bandwidth. The
main reason for identification and evaluation of these parameters is to develop
a model that could meet the constraints imposed by potential beneficiaries. In
this paper, we develop a methodology for planning and cost-modeling of a wide
area network and validate it in a case study, under the supposition that
behavioral interactions of individuals and groups play a significant role and
have to be taken into consideration by employing either simple or composite
indicators of socioeconomic status.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2817</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2817</id><created>2014-06-11</created><authors><author><keyname>Zechner</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Marussig</keyname><forenames>Benjamin</forenames></author><author><keyname>Beer</keyname><forenames>Gernot</forenames></author><author><keyname>Fries</keyname><forenames>Thomas-Peter</forenames></author></authors><title>Isogeometric Boundary Element Method with Hierarchical Matrices</title><categories>cs.NA math.NA</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the complexity problem of the isogeometric Boundary
Element Method by proposing a collocation scheme for practical problems in
linear elasticity and the application of hierarchical matrices. For mixed
boundary value problems, a block system of matrices similar to Galerkin
formulations is constructed allowing an effective application of that matrix
format. We introduce a strategy for the geometric bisection of surfaces based
on NURBS patches. The approximation of system matrices is carried out by means
of kernel interpolation. Numerical results are shown that prove the success of
the formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2822</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2822</id><created>2014-06-11</created><authors><author><keyname>Araujo</keyname><forenames>Virginia Maria</forenames></author><author><keyname>Vazquez</keyname><forenames>Jose Ayude</forenames></author><author><keyname>Cota</keyname><forenames>Manuel Perez</forenames></author></authors><title>A Framework for the Evaluation of SaaS Impact</title><categories>cs.SE</categories><comments>16 pages</comments><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol.4, No.3, May 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays the technological progress allows us to have highly flexible
solutions, easily accessible with lower levels of investment, which leads to
many companies adopting SaaS (Software-as-a-Service) to support their business
processes. Associated with this movement and considering the advantages of
SaaS, it is important to understand whether work is being developed that is
underutilized because companies are not taking advantage of it, and in this
case it is necessary to understand the reasons thereof. This knowledge is
important even for people who do not use or do not develop/provide SaaS, since
sooner or later it will be unavoidable due to current trends. In the near
future, nearly all decision-makers of IT strategies will be forced to consider
adopting SaaS as an IT solution for the convenience benefits associated with
technology or market competition. At that time they will have to know how to
evaluate impacts and decide. Often, decision-makers of business strategies
consider only the attractive incentives of using SaaS ignoring the impacts
associated with new technologies. The need for tools and processes to assess
these impacts before adopting a SaaS solution is crucial to ensure the
sustainability of the information system, reduce uncertainty and facilitate
decision making. This article presents a framework for evaluating impacts of
SaaS called SIE (SaaS Impact Evaluation) which in addition to guidance for the
present research, aims to provide guidelines for the collection, data analysis,
impact assessment and decision making about including SaaS on the organizations
strategic plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2823</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2823</id><created>2014-06-11</created><authors><author><keyname>Lopez-Herrejon</keyname><forenames>Roberto E.</forenames></author><author><keyname>Ferrer</keyname><forenames>Javier</forenames></author><author><keyname>Chicano</keyname><forenames>Francisco</forenames></author><author><keyname>Linsbauer</keyname><forenames>Lukas</forenames></author><author><keyname>Egyed</keyname><forenames>Alexander</forenames></author><author><keyname>Alba</keyname><forenames>Enrique</forenames></author></authors><title>A Hitchhiker's Guide to Search-Based Software Engineering for Software
  Product Lines</title><categories>cs.SE cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search Based Software Engineering (SBSE) is an emerging discipline that
focuses on the application of search-based optimization techniques to software
engineering problems. The capacity of SBSE techniques to tackle problems
involving large search spaces make their application attractive for Software
Product Lines (SPLs). In recent years, several publications have appeared that
apply SBSE techniques to SPL problems. In this paper, we present the results of
a systematic mapping study of such publications. We identified the stages of
the SPL life cycle where SBSE techniques have been used, what case studies have
been employed and how they have been analysed. This mapping study revealed
potential venues for further research as well as common misunderstanding and
pitfalls when applying SBSE techniques that we address by providing a guideline
for researchers and practitioners interested in exploiting these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2824</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2824</id><created>2014-06-11</created><authors><author><keyname>Grov</keyname><forenames>Gudmund</forenames></author></authors><title>Some Ideas for Program Verifier Tactics</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A program verifier is a tool that can be used to verify that a &quot;contract&quot; for
a program holds - i.e. given a precondition the program guarantees that a given
postcondition holds - by only working at the level of the annotated program. An
alternative approach is to use an interactive theorem prover, which enables
users to encode common proof patterns as special programs called &quot;tactics&quot;.
This offers more flexibility than program verifiers, but at the expense of
skills required by the user. Here, we add such flexibility to program verifiers
by developing &quot;tactics&quot; as a form of program refactoring called DTacs. A formal
characterisation and set of examples are given, illustrated with a case study
from NASA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2834</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2834</id><created>2014-06-11</created><authors><author><keyname>Huang</keyname><forenames>Shao-Lun</forenames></author><author><keyname>Zheng</keyname><forenames>Lizhong</forenames></author></authors><title>The Linear Information Coupling Problems</title><categories>cs.IT math.IT</categories><comments>27 pages, submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many network information theory problems face the similar difficulty of
single-letterization. We argue that this is due to the lack of a geometric
structure on the space of probability distribution. In this paper, we develop
such a structure by assuming that the distributions of interest are close to
each other. Under this assumption, the K-L divergence is reduced to the squared
Euclidean metric in an Euclidean space. In addition, we construct the notion of
coordinate and inner product, which will facilitate solving communication
problems. We will present the application of this approach to the
point-to-point channel, general broadcast channel, and the multiple access
channel (MAC) with the common source. It can be shown that with this approach,
information theory problems, such as the single-letterization, can be reduced
to some linear algebra problems. Moreover, we show that for the general
broadcast channel, transmitting the common message to receivers can be
formulated as the trade-off between linear systems. We also provide an example
to visualize this trade-off in a geometric way. Finally, for the MAC with the
common source, we observe a coherent combining gain due to the cooperation
between transmitters, and this gain can be quantified by applying our
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2844</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2844</id><created>2014-06-11</created><authors><author><keyname>Menouer</keyname><forenames>Tarek</forenames><affiliation>PRISM</affiliation></author><author><keyname>Cun</keyname><forenames>Bertrand Le</forenames><affiliation>PRISM</affiliation></author></authors><title>Partitionnement D\'eterministe pour R\'esoudre les Probl\`emes de
  Programmation Par Contraintes en utilisant le Framework Parall\`ele Bobpp</title><categories>cs.DC</categories><comments>in French, ComPAS 2014 : conf\'erence en parall\'elisme, architecture
  et syst\`emes (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a deterministic parallelization to explore a Constraint
Programming search space. This work is an answer to an industrial project named
PAJERO, which is in need of a parallel constraint solver which always responds
with the same solution whether using sequential or parallel machines. It is
well known that parallel tree search changes the order in which the exploration
of solution space is done. In the context where the first solution found is
returned, using a different number of cores may change the returned solution.
In the literature, several non deterministic strategies have been proposed to
parallelize the exploration of Constraint Programming search space. Most of
them are based on the Work Stealing technique used to partition the Constraint
Programming search space on demand and during the execution of the search
algorithm. Our study focuses on the determinism of the parallel search versus
the sequential one. We consider that the sequential search algorithm is
deterministic, then propose an elegant solution introducing a total order on
the nodes in which the parallel algorithm always gives the same solution as the
sequential one regardless of the number of cores used. To evaluate this
deterministic strategy, we ran tests using the Google OR-Tools Constraint
Programming solver on top of our parallel Bobpp framework. The performances are
illustrated by solving Constraint Programming problems modeled in FlatZinc
format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2852</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2852</id><created>2014-06-11</created><updated>2014-06-16</updated><authors><author><keyname>Jurdzinski</keyname><forenames>Tomasz</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author><author><keyname>Rozanski</keyname><forenames>Michal</forenames></author><author><keyname>Stachowiak</keyname><forenames>Grzegorz</forenames></author></authors><title>On the Impact of Geometry on Ad Hoc Communication in Wireless Networks</title><categories>cs.DC</categories><acm-class>C.2.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the question how important is the knowledge of
geometric location and network density to the efficiency of (distributed)
wireless communication in ad hoc networks. We study fundamental communication
task of broadcast and develop well-scalable, randomized algorithms that do not
rely on GPS information, and which efficiency formulas do not depend on how
dense the geometric network is. We consider two settings: with and without
spontaneous wake-up of nodes. In the former setting, in which all nodes start
the protocol at the same time, our algorithm accomplishes broadcast in $O(D\log
n + \log^2 n)$ rounds under the SINR model, with high probability (whp), where
$D$ is the diameter of the communication graph and $n$ is the number of
stations. In the latter setting, in which only the source node containing the
original message is active in the beginning, we develop a slightly slower
algorithm working in $O(D\log^2 n)$ rounds whp. Both algorithms are based on a
novel distributed coloring method, which is of independent interest and
potential applicability to other communication tasks under the SINR wireless
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2855</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2855</id><created>2014-06-11</created><authors><author><keyname>Grandi</keyname><forenames>Umberto</forenames></author></authors><title>The Common Structure of Paradoxes in Aggregation Theory</title><categories>cs.MA cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyse some of the classical paradoxes in Social Choice
Theory (namely, the Condorcet paradox, the discursive dilemma, the Ostrogorski
paradox and the multiple election paradox) using a general framework for the
study of aggregation problems called binary aggregation with integrity
constraints. We provide a definition of paradox that is general enough to
account for the four cases mentioned, and identify a common structure in the
syntactic properties of the rationality assumptions that lie behind such
paradoxes. We generalise this observation by providing a full characterisation
of the set of rationality assumptions on which the majority rule does not
generate a paradox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2858</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2858</id><created>2014-06-11</created><updated>2014-10-01</updated><authors><author><keyname>Barry</keyname><forenames>Jennifer</forenames></author><author><keyname>Barry</keyname><forenames>Daniel T.</forenames></author><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author></authors><title>Quantum POMDPs</title><categories>cs.AI quant-ph</categories><comments>13 pages, 3 figures, revised version (fixes several errors, discusses
  related work)</comments><journal-ref>Phys. Rev. A 90, 032311, 2014</journal-ref><doi>10.1103/PhysRevA.90.032311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present quantum observable Markov decision processes (QOMDPs), the quantum
analogues of partially observable Markov decision processes (POMDPs). In a
QOMDP, an agent's state is represented as a quantum state and the agent can
choose a superoperator to apply. This is similar to the POMDP belief state,
which is a probability distribution over world states and evolves via a
stochastic matrix. We show that the existence of a policy of at least a certain
value has the same complexity for QOMDPs and POMDPs in the polynomial and
infinite horizon cases. However, we also prove that the existence of a policy
that can reach a goal state is decidable for goal POMDPs and undecidable for
goal QOMDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2863</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2863</id><created>2014-06-11</created><authors><author><keyname>Teixeira</keyname><forenames>Jose</forenames></author><author><keyname>Suomi</keyname><forenames>Reima</forenames></author></authors><title>A Literature Review on Information Systems Supporting the Physical
  Wellbeing of Elderly People</title><categories>cs.CY</categories><comments>As presented in the IADIS ICT2012 conference held in Lisbon,
  Portugal, 21-23 July 2012, Proceedings of the IADIS International Conference
  ICT, Society and Human Beings 2012, IADIS press, 2012</comments><acm-class>K.4.1; K.4.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper reviews multi-disciplinary research on information systems
supporting the physical wellbeing of the elderly population. By taking a
systematic approach, it screens journals, conference proceedings and books on
how computer-based systems are used for improving both the health and wellbeing
of the most senior individuals. By using different Internet-databases indexing
academic publications and a set of conceptual keywords the authors searched for
and identified 62 major publications on the topic that were carefully reviewed.
Each publication item was classified according different category sets and the
aggregated data was then analyzed from different socio-technological
perspectives. Our findings suggest that research on the topic is very focused
on diseases over health and wellbeing, since most of the studied information
systems focused much more on the protective and curative medical procedures
over other important dimensions such as prevention, education and health
promotion. An overview on what and where the studied systems are used is
presented and a new information systems research agenda is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2871</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2871</id><created>2014-06-11</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Multi-Objective Signal Processing Optimization: The Way to Balance
  Conflicting Metrics in 5G Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted for publication in IEEE Signal Processing Magazine (Special
  Issue on Signal Processing for the 5G Revolution), 10 pages, 8 figures</comments><journal-ref>IEEE Signal Processing Magazine, vol. 31, no. 6, pp. 14-23,
  November 2014</journal-ref><doi>10.1109/MSP.2014.2330661</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of cellular networks is driven by the dream of ubiquitous
wireless connectivity: Any data service is instantly accessible everywhere.
With each generation of cellular networks, we have moved closer to this
wireless dream; first by delivering wireless access to voice communications,
then by providing wireless data services, and recently by delivering a
WiFi-like experience with wide-area coverage and user mobility management. The
support for high data rates has been the main objective in recent years, as
seen from the academic focus on sum-rate optimization and the efforts from
standardization bodies to meet the peak rate requirements specified in
IMT-Advanced. In contrast, a variety of metrics/objectives are put forward in
the technological preparations for 5G networks: higher peak rates, improved
coverage with uniform user experience, higher reliability and lower latency,
better energy efficiency, lower-cost user devices and services, better
scalability with number of devices, etc. These multiple objectives are coupled,
often in a conflicting manner such that improvements in one objective lead to
degradation in the other objectives. Hence, the design of future networks calls
for new optimization tools that properly handle the existence and tradeoffs
between multiple objectives.
  In this article, we provide a review of multi-objective optimization (MOO),
which is a mathematical framework to solve design problems with multiple
conflicting objectives. (...) We provide a survey of the basic definitions,
properties, and algorithmic tools in MOO. This reveals how signal processing
algorithms are used to visualize the inherent conflicts between 5G performance
objectives, thereby allowing the network designer to understand the possible
operating points and how to balance the objectives in an efficient and
satisfactory way. For clarity, we provide a case study on massive MIMO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2880</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2880</id><created>2014-06-11</created><authors><author><keyname>Sch&#xf6;neberg</keyname><forenames>Ulf</forenames></author><author><keyname>Sperber</keyname><forenames>Wolfram</forenames></author></authors><title>POS Tagging and its Applications for Mathematics</title><categories>cs.DL cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content analysis of scientific publications is a nontrivial task, but a
useful and important one for scientific information services. In the Gutenberg
era it was a domain of human experts; in the digital age many machine-based
methods, e.g., graph analysis tools and machine-learning techniques, have been
developed for it. Natural Language Processing (NLP) is a powerful
machine-learning approach to semiautomatic speech and language processing,
which is also applicable to mathematics. The well established methods of NLP
have to be adjusted for the special needs of mathematics, in particular for
handling mathematical formulae. We demonstrate a mathematics-aware part of
speech tagger and give a short overview about our adaptation of NLP methods for
mathematical publications. We show the use of the tools developed for key
phrase extraction and classification in the database zbMATH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2886</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2886</id><created>2014-06-11</created><authors><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author><author><keyname>Sugimoto</keyname><forenames>Cassidy R.</forenames></author><author><keyname>Larivi&#xe8;re</keyname><forenames>Vincent</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author><author><keyname>Ding</keyname><forenames>Ying</forenames></author></authors><title>The role of handbooks in knowledge creation and diffusion: A case of
  science and technology studies</title><categories>cs.DL</categories><comments>Accepted for publication in Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genre is considered to be an important element in scholarly communication and
in the practice of scientific disciplines. However, scientometric studies have
typically focused on a single genre, the journal article. The goal of this
study is to understand the role that handbooks play in knowledge creation and
diffusion and their relationship with the genre of journal articles,
particularly in highly interdisciplinary and emergent social science and
humanities disciplines. To shed light on these questions we focused on
handbooks and journal articles published over the last four decades belonging
to the research area of Science and Technology Studies (STS), broadly defined.
To get a detailed picture we used the full-text of five handbooks (500,000
words) and a well-defined set of 11,700 STS articles. We confirmed the
methodological split of STS into qualitative and quantitative (scientometric)
approaches. Even when the two traditions explore similar topics (e.g., science
and gender) they approach them from different starting points. The change in
cognitive foci in both handbooks and articles partially reflects the changing
trends in STS research, often driven by technology. Using text similarity
measures we found that, in the case of STS, handbooks play no special role in
either focusing the research efforts or marking their decline. In general, they
do not represent the summaries of research directions that have emerged since
the previous edition of the handbook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2889</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2889</id><created>2014-06-11</created><authors><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>de Chazal</keyname><forenames>Philip</forenames></author><author><keyname>van Schaik</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Explicit Computation of Input Weights in Extreme Learning Machines</title><categories>cs.NE</categories><comments>In submission for the ELM 2014 Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a closed form expression for initializing the input weights in a
multi-layer perceptron, which can be used as the first step in synthesis of an
Extreme Learning Ma-chine. The expression is based on the standard function for
a separating hyperplane as computed in multilayer perceptrons and linear
Support Vector Machines; that is, as a linear combination of input data
samples. In the absence of supervised training for the input weights, random
linear combinations of training data samples are used to project the input data
to a higher dimensional hidden layer. The hidden layer weights are solved in
the standard ELM fashion by computing the pseudoinverse of the hidden layer
outputs and multiplying by the desired output values. All weights for this
method can be computed in a single pass, and the resulting networks are more
accurate and more consistent on some standard problems than regular ELM
networks of the same size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2895</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2895</id><created>2014-06-11</created><authors><author><keyname>Geiger</keyname><forenames>J&#xfc;rgen T.</forenames></author><author><keyname>Knei&#xdf;l</keyname><forenames>Maximilian</forenames></author><author><keyname>Schuller</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Rigoll</keyname><forenames>Gerhard</forenames></author></authors><title>Acoustic Gait-based Person Identification using Hidden Markov Models</title><categories>cs.HC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system for identifying humans by their walking sounds. This
problem is also known as acoustic gait recognition. The goal of the system is
to analyse sounds emitted by walking persons (mostly the step sounds) and
identify those persons. These sounds are characterised by the gait pattern and
are influenced by the movements of the arms and legs, but also depend on the
type of shoe. We extract cepstral features from the recorded audio signals and
use hidden Markov models for dynamic classification. A cyclic model topology is
employed to represent individual gait cycles. This topology allows to model and
detect individual steps, leading to very promising identification rates. For
experimental validation, we use the publicly available TUM GAID database, which
is a large gait recognition database containing 3050 recordings of 305 subjects
in three variations. In the best setup, an identification rate of 65.5 % is
achieved out of 155 subjects. This is a relative improvement of almost 30 %
compared to our previous work, which used various audio features and support
vector machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2897</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2897</id><created>2014-06-11</created><updated>2015-06-17</updated><authors><author><keyname>Noshad</keyname><forenames>Mohammad</forenames></author><author><keyname>Brandt-Pearce</keyname><forenames>Maite</forenames></author></authors><title>Hadamard Coded Modulation for Visible Light Communications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible light communication (VLC) systems using the indoor lighting system to
also provide downlink communications require high average optical powers in
order to satisfy the illumination needs. This can cause high amplitude signals
common in higher-order modulation schemes to be clipped by the peak power
constraint of the light emitting diode (LED) and lead to high signal
distortion. In this paper we introduce Hadamard coded modulation (HCM) to
achieve low error probabilities in LED-based VLC systems needing high average
optical powers. This technique uses a fast Walsh-Hadamard transform (FWHT) to
modulate the data as an alternative modulation technique to orthogonal
frequency division multiplexing (OFDM). HCM achieves a better performance for
high illumination levels because of its small peak to average power ratio
(PAPR). The power efficiency of HCM can be improved by reducing the DC part of
the transmitted signals without losing any information. The resulting so-called
DC-reduced HCM (DCR-HCM) is well suited to environments requiring dimmer
lighting as it transmits signals with lower peak amplitudes compared to HCM,
which are thus subject to less nonlinear distortion. Interleaving can be
applied to HCM to make the resulting signals more resistant against
inter-symbol interference (ISI) in dispersive VLC links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2901</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2901</id><created>2014-06-11</created><updated>2015-03-19</updated><authors><author><keyname>Wendzel</keyname><forenames>Steffen</forenames></author><author><keyname>Zander</keyname><forenames>Sebastian</forenames></author><author><keyname>Fechner</keyname><forenames>Bernhard</forenames></author><author><keyname>Herdin</keyname><forenames>Christian</forenames></author></authors><title>A Pattern-based Survey and Categorization of Network Covert Channel
  Techniques</title><categories>cs.CR</categories><comments>27 pages, 4 figures, 3 tables, accepted for publication in ACM
  Computing Surveys (CSUR, submitted on Dec-2013, accepted in Oct-2014). The
  final publication will be available via ACM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network covert channels are used to hide communication inside network
protocols. Within the last decades, various techniques for covert channels
arose. We surveyed and analyzed 109 techniques developed between 1987 and 2013
and show that these techniques can be reduced to only 11 different patterns.
Moreover, the majority (69.7%) of techniques can be categorized in only four
different patterns, i.e. most of the techniques we surveyed are very similar.
We represent the patterns in a hierarchical catalog using a pattern language.
Our pattern catalog will serve as a base for future covert channel novelty
evaluation. Furthermore, we apply the concept of pattern variations to network
covert channels. With pattern variations, the context of a pattern can change.
For example, a channel developed for IPv4 can automatically be adapted to other
network protocols. We also propose the pattern-based covert channel
optimizations pattern hopping and pattern combination. Finally, we lay the
foundation for pattern-based countermeasures: While many current
countermeasures were developed for specific channels, a pattern-oriented
approach allows to apply one countermeasure to multiple channels. Hence, future
countermeasure development can focus on patterns, and the development of
real-world protection against covert channels is greatly simplified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2903</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2903</id><created>2014-06-11</created><updated>2014-06-11</updated><authors><author><keyname>Safwat</keyname><forenames>Hazem</forenames></author><author><keyname>Davis</keyname><forenames>Brian</forenames></author></authors><title>A Brief State of the Art for Ontology Authoring</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges for building the Semantic web is Ontology
Authoring. Controlled Natural Languages CNLs offer a user friendly means for
non-experts to author ontologies. This paper provides a snapshot of the
state-of-the-art for the core CNLs for ontology authoring and reviews their
respective evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2909</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2909</id><created>2014-06-11</created><updated>2015-06-11</updated><authors><author><keyname>Antulov-Fantulin</keyname><forenames>Nino</forenames></author><author><keyname>Lancic</keyname><forenames>Alen</forenames></author><author><keyname>Smuc</keyname><forenames>Tomislav</forenames></author><author><keyname>Stefancic</keyname><forenames>Hrvoje</forenames></author><author><keyname>Sikic</keyname><forenames>Mile</forenames></author></authors><title>Identification of Patient Zero in Static and Temporal Networks -
  Robustness and Limitations</title><categories>cs.SI physics.soc-ph</categories><comments>Additional experiments and results regarding the detectability limits
  are included in v2. Supplemental material is added in the Ancillary files
  section on this arXiv page</comments><journal-ref>Phys. Rev. Lett. 114, 248701 (2015)</journal-ref><doi>10.1103/PhysRevLett.114.248701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of patient-zero can give new insights to the epidemiologists about
the nature of first transmissions into a population. In this paper, we study
the statistical inference problem of detecting the source of epidemics from a
snapshot of spreading on an arbitrary network structure. By using exact
analytic calculations and Monte Carlo estimators, we demonstrate the
detectability limits for the SIR model, which primarily depend on the spreading
process characteristics. Finally, we demonstrate the applicability of the
approach in a case of a simulated sexually transmitted infection spreading over
an empirical temporal network of sexual interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2930</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2930</id><created>2014-06-11</created><authors><author><keyname>Samvedi</keyname><forenames>Abhishek</forenames></author><author><keyname>Owlak</keyname><forenames>Sparsh</forenames></author><author><keyname>Chaurasia</keyname><forenames>Vijay Kumar</forenames></author></authors><title>Improved Secure Address Resolution Protocol</title><categories>cs.CR</categories><comments>10 pages, 15 figures, paper selected in fifth international
  conference of communications security and information assurance 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an improved secure address resolution protocol is presented
where ARP spoofing attack is prevented. The proposed methodology is a
centralised methodology for preventing ARP spoofing attack. In the proposed
model there is a central server on a network or subnet which prevents ARP
spoofing attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2938</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2938</id><created>2014-06-11</created><authors><author><keyname>Youn</keyname><forenames>Hyejin</forenames></author><author><keyname>Bettencourt</keyname><forenames>Luis M. A.</forenames></author><author><keyname>Strumsky</keyname><forenames>Deborah</forenames></author><author><keyname>Lobo</keyname><forenames>Jose</forenames></author></authors><title>Invention as a Combinatorial Process: Evidence from U.S. Patents</title><categories>physics.soc-ph cs.SI</categories><comments>Press embargo in place until publication</comments><journal-ref>J. R. Soc. Interface 12: 20150272 2015</journal-ref><doi>10.1098/rsif.2015.0272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Invention has been commonly conceptualized as a search over a space of
combinatorial possibilities. Despite the existence of a rich literature,
spanning a variety of disciplines, elaborating on the recombinant nature of
invention, we lack a formal and quantitative characterization of the
combinatorial process underpinning inventive activity. Here we utilize U.S.
patent records dating from 1790 to 2010 to formally characterize the invention
as a combinatorial process. To do this we treat patented inventions as carriers
of technologies and avail ourselves of the elaborate system of technology codes
used by the U.S. Patent Office to classify the technologies responsible for an
invention's novelty. We find that the combinatorial inventive process exhibits
an invariant rate of &quot;exploitation&quot; (refinements of existing combinations of
technologies) and &quot;exploration&quot; (the development of new technological
combinations). This combinatorial dynamic contrasts sharply with the creation
of new technological capabilities -- the building blocks to be combined --
which has significantly slowed down. We also find that notwithstanding the very
reduced rate at which new technologies are introduced, the generation of novel
technological combinations engenders a practically infinite space of
technological configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2943</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2943</id><created>2014-06-11</created><updated>2015-02-03</updated><authors><author><keyname>Nasser</keyname><forenames>Rajai</forenames></author></authors><title>Ergodic Theory Meets Polarization. I: An Ergodic Theory for Binary
  Operations</title><categories>math.CO cs.IT math.IT</categories><comments>33 pages, 1 figure. Submitted to IEEE Trans. Inform. Theory and in
  part to ISIT'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open problem in polarization theory is to determine the binary operations
that always lead to polarization when they are used in Ar{\i}kan style
constructions. This paper, which is presented in two parts, solves this problem
by providing a necessary and sufficient condition for a binary operation to be
polarizing. This (first) part of the paper introduces the mathematical
framework that we will use in the second part to characterize the polarizing
operations. We define uniformity preserving, irreducible, ergodic and strongly
ergodic operations and we study their properties. The concepts of a stable
partition and the residue of a stable partition are introduced. We show that an
ergodic operation is strongly ergodic if and only if all its stable partitions
are their own residues. We also study the products of binary operations and the
structure of their stable partitions. We show that the product of a sequence of
binary operations is strongly ergodic if and only if all the operations in the
sequence are strongly ergodic. In the second part of the paper, we provide a
foundation of polarization theory based on the ergodic theory of binary
operations that we develop in this part.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2946</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2946</id><created>2014-06-11</created><updated>2014-11-22</updated><authors><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Strong converse rates for quantum communication</title><categories>quant-ph cs.IT math.IT</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit a fundamental open problem in quantum information theory, namely
whether it is possible to transmit quantum information at a rate exceeding the
channel capacity if we allow for a non-vanishing probability of decoding error.
Here we establish that the Rains information of any quantum channel is a strong
converse rate for quantum communication: For any code with a rate exceeding the
Rains information of the channel, we show that the fidelity vanishes
exponentially fast as the number of channel uses increases. This remains true
even if we consider codes that perform classical post-processing on the
transmitted quantum data. Our result has several applications. Most
importantly, for generalized dephasing channels we show that the Rains
information is also achievable, and thereby establish the strong converse
property for quantum communication over such channels. This for the first time
conclusively settles the strong converse question for a class of quantum
channels that have a non-trivial quantum capacity. Moreover, we show that the
classical post-processing assisted quantum capacity of the quantum binary
erasure channel satisfies the strong converse property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2949</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2949</id><created>2014-06-11</created><updated>2015-02-03</updated><authors><author><keyname>Nasser</keyname><forenames>Rajai</forenames></author></authors><title>Ergodic Theory Meets Polarization. II: A Foundation of Polarization
  Theory</title><categories>cs.IT math.CO math.IT</categories><comments>31 pages. Submitted to IEEE Trans. Inform. Theory and in part to
  ISIT'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open problem in polarization theory is to determine the binary operations
that always lead to polarization when they are used in Ar{\i}kan style
constructions. This paper, which is presented in two parts, solves this problem
by providing a necessary and sufficient condition for a binary operation to be
polarizing. This (second) part provides a foundation of polarization theory
based on the ergodic theory of binary operations which we developed in the
first part. We show that a binary operation is polarizing if and only if it is
uniformity preserving and its inverse is strongly ergodic. The rate of
polarization of single user channels is studied. It is shown that the exponent
of any polarizing operation cannot exceed 1/2, which is the exponent of
quasigroup operations. We also study the polarization of multiple access
channels (MAC). In particular, we show that a sequence of binary operations is
MAC-polarizing if and only if each binary operation in the sequence is
polarizing. It is shown that the exponent of any MAC-polarizing sequence cannot
exceed 1/2, which is the exponent of sequences of quasigroup operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2951</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2951</id><created>2014-06-11</created><updated>2015-10-07</updated><authors><author><keyname>Byrka</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Pensyl</keyname><forenames>Thomas</forenames></author><author><keyname>Rybicki</keyname><forenames>Bartosz</forenames></author><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author><author><keyname>Trinh</keyname><forenames>Khoa</forenames></author></authors><title>An Improved Approximation for $k$-median, and Positive Correlation in
  Budgeted Optimization</title><categories>cs.DS</categories><journal-ref>Proceedings of ACM-Siam Symposium on Discrete Algorithms (SODA),
  pages 737-756, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependent rounding is a useful technique for optimization problems with hard
budget constraints. This framework naturally leads to \emph{negative
correlation} properties. However, what if an application naturally calls for
dependent rounding on the one hand, and desires \emph{positive} correlation on
the other? More generally, we develop algorithms that guarantee the known
properties of dependent rounding, but also have nearly best-possible behavior -
near-independence, which generalizes positive correlation - on &quot;small&quot; subsets
of the variables. The recent breakthrough of Li &amp; Svensson for the classical
$k$-median problem has to handle positive correlation in certain
dependent-rounding settings, and does so implicitly. We improve upon
Li-Svensson's approximation ratio for $k$-median from $2.732 + \epsilon$ to
$2.675 + \epsilon$ by developing an algorithm that improves upon various
aspects of their work. Our dependent-rounding approach helps us improve the
dependence of the runtime on the parameter $\epsilon$ from Li-Svensson's
$N^{O(1/\epsilon^2)}$ to $N^{O((1/\epsilon) \log(1/\epsilon))}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2952</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2952</id><created>2014-06-11</created><authors><author><keyname>Branson</keyname><forenames>Steve</forenames></author><author><keyname>Van Horn</keyname><forenames>Grant</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author><author><keyname>Perona</keyname><forenames>Pietro</forenames></author></authors><title>Bird Species Categorization Using Pose Normalized Deep Convolutional
  Nets</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an architecture for fine-grained visual categorization that
approaches expert human performance in the classification of bird species. Our
architecture first computes an estimate of the object's pose; this is used to
compute local image features which are, in turn, used for classification. The
features are computed by applying deep convolutional nets to image patches that
are located and normalized by the pose. We perform an empirical study of a
number of pose normalization schemes, including an investigation of higher
order geometric warping functions. We propose a novel graph-based clustering
algorithm for learning a compact pose normalization space. We perform a
detailed investigation of state-of-the-art deep convolutional feature
implementations and fine-tuning feature learning for fine-grained
classification. We observe that a model that integrates lower-level feature
layers with pose-normalized extraction routines and higher-level feature layers
with unaligned image features works best. Our experiments advance
state-of-the-art performance on bird species recognition, with a large
improvement of correct classification rates over previous methods (75% vs.
55-65%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2960</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2960</id><created>2014-06-11</created><authors><author><keyname>Mowla</keyname><forenames>M. M.</forenames></author><author><keyname>Ali</keyname><forenames>M. Y.</forenames></author><author><keyname>Hasan</keyname><forenames>S. M. M</forenames></author></authors><title>A Modified Design of ACF Operation for Reducing PAPR of OFDM Signal</title><categories>cs.NI</categories><comments>12 Pages, 8 Figures, 5 Tables. arXiv admin note: substantial text
  overlap with arXiv:1404.2233, arXiv:1403.3349, arXiv:1404.2300</comments><journal-ref>International Journal of Next-Generation Networks (IJNGN), Vol. 6,
  No. 1, pp.31-42, March 2014</journal-ref><doi>10.5121/ijngn.2014.6103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next generation wireless communication technology long term evolution (LTE)
has implemented orthogonal frequency division multiplexing (OFDM) technique as
a strong candidate for radio access systems. It has several attributes such as
providing robustness to multipath fading &amp; impulse noise, eliminating
intersymbol interference (ISI), inter carrier interference (ICI) &amp; the need for
equalizers. The major challenging issue of OFDM technique is the high peak to
average power ratio (PAPR) which is defined as the ratio of the peak power to
the average power of the OFDM signal. A trade-off is necessary for reducing
PAPR with increasing bit error rate (BER), computational complexity or data
rate loss etc. In this paper, a moderately modified design of amplitude
clipping &amp; filtering operation (ACF) is proposed and implemented which shows
the significant improvement in case of PAPR reduction for both quadrature phase
shift keying (QPSK) &amp; quadrature amplitude modulation (QAM) while increasing
slight BER match up to to an existing method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2963</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2963</id><created>2014-06-11</created><updated>2014-07-19</updated><authors><author><keyname>Peters</keyname><forenames>Shanan E.</forenames></author><author><keyname>Zhang</keyname><forenames>Ce</forenames></author><author><keyname>Livny</keyname><forenames>Miron</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>A machine-compiled macroevolutionary history of Phanerozoic life</title><categories>cs.DB cs.CL cs.LG q-bio.PE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Many aspects of macroevolutionary theory and our understanding of biotic
responses to global environmental change derive from literature-based
compilations of palaeontological data. Existing manually assembled databases
are, however, incomplete and difficult to assess and enhance. Here, we develop
and validate the quality of a machine reading system, PaleoDeepDive, that
automatically locates and extracts data from heterogeneous text, tables, and
figures in publications. PaleoDeepDive performs comparably to humans in complex
data extraction and inference tasks and generates congruent synthetic
macroevolutionary results. Unlike traditional databases, PaleoDeepDive produces
a probabilistic database that systematically improves as information is added.
We also show that the system can readily accommodate sophisticated data types,
such as morphological data in biological illustrations and associated textual
descriptions. Our machine reading approach to scientific data integration and
synthesis brings within reach many questions that are currently underdetermined
and does so in ways that may stimulate entirely new modes of inquiry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2969</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2969</id><created>2014-06-11</created><authors><author><keyname>Wang</keyname><forenames>Yilun</forenames></author><author><keyname>Su</keyname><forenames>Xinhua</forenames></author></authors><title>Truncated Nuclear Norm Minimization for Image Restoration Based On
  Iterative Support Detection</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovering a large matrix from limited measurements is a challenging task
arising in many real applications, such as image inpainting, compressive
sensing and medical imaging, and this kind of problems are mostly formulated as
low-rank matrix approximation problems. Due to the rank operator being
non-convex and discontinuous, most of the recent theoretical studies use the
nuclear norm as a convex relaxation and the low-rank matrix recovery problem is
solved through minimization of the nuclear norm regularized problem. However, a
major limitation of nuclear norm minimization is that all the singular values
are simultaneously minimized and the rank may not be well approximated
\cite{hu2012fast}. Correspondingly, in this paper, we propose a new multi-stage
algorithm, which makes use of the concept of Truncated Nuclear Norm
Regularization (TNNR) proposed in \citep{hu2012fast} and Iterative Support
Detection (ISD) proposed in \citep{wang2010sparse} to overcome the above
limitation. Besides matrix completion problems considered in
\citep{hu2012fast}, the proposed method can be also extended to the general
low-rank matrix recovery problems. Extensive experiments well validate the
superiority of our new algorithms over other state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2977</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2977</id><created>2014-06-11</created><authors><author><keyname>Safadi</keyname><forenames>Hani</forenames></author><author><keyname>Faraj</keyname><forenames>Samer</forenames></author></authors><title>Toward a Local Perspective on Online Collaboration</title><categories>cs.SI physics.soc-ph</categories><comments>In the Proceedings of Collective Intelligence 2014</comments><proxy>Walter Lasecki</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structural properties of large scale collaboration in online
communities of innovation and the role that position in the community plays in
determining knowledge contribution. Contrary to previous research, we argue for
a more local perspective when examining online collaboration. We demonstrate
that a member's centrality and spanning within his/her local neighborhood is a
better predictor of contribution than global centrality and spanning within the
whole community. We contribute both theoretically and methodologically to
research on large scale collaboration. On the theoretical front, a local view
of position implies a more confined and local organization of work in online
communities than previously thought. From a methodological perspective,
evaluating the local structure of large networks involves radically different
algorithms that have only recently become feasible with the increase of
processing power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2984</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2984</id><created>2014-06-11</created><updated>2014-09-17</updated><authors><author><keyname>Tompson</keyname><forenames>Jonathan</forenames></author><author><keyname>Jain</keyname><forenames>Arjun</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author><author><keyname>Bregler</keyname><forenames>Christoph</forenames></author></authors><title>Joint Training of a Convolutional Network and a Graphical Model for
  Human Pose Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new hybrid architecture that consists of a deep
Convolutional Network and a Markov Random Field. We show how this architecture
is successfully applied to the challenging problem of articulated human pose
estimation in monocular images. The architecture can exploit structural domain
constraints such as geometric relationships between body joint locations. We
show that joint training of these two model paradigms improves performance and
allows us to significantly outperform existing state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.2989</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.2989</id><created>2014-06-11</created><updated>2015-04-09</updated><authors><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author><author><keyname>Berglund</keyname><forenames>Mathias</forenames></author><author><keyname>Alain</keyname><forenames>Guillaume</forenames></author><author><keyname>Dinh</keyname><forenames>Laurent</forenames></author></authors><title>Techniques for Learning Binary Stochastic Feedforward Neural Networks</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic binary hidden units in a multi-layer perceptron (MLP) network give
at least three potential benefits when compared to deterministic MLP networks.
(1) They allow to learn one-to-many type of mappings. (2) They can be used in
structured prediction problems, where modeling the internal structure of the
output is important. (3) Stochasticity has been shown to be an excellent
regularizer, which makes generalization performance potentially better in
general. However, training stochastic networks is considerably more difficult.
We study training using M samples of hidden activations per input. We show that
the case M=1 leads to a fundamentally different behavior where the network
tries to avoid stochasticity. We propose two new estimators for the training
gradient and propose benchmark tests for comparing training algorithms. Our
experiments confirm that training stochastic networks is difficult and show
that the proposed two estimators perform favorably among all the five known
estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3002</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3002</id><created>2014-05-21</created><updated>2015-08-25</updated><authors><author><keyname>Kaczmarek</keyname><forenames>Tyler</forenames></author><author><keyname>Kobsa</keyname><forenames>Alfed</forenames></author><author><keyname>Sy</keyname><forenames>Robert</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author></authors><title>The Effect of Visual Noise on The Completion of Security Critical Tasks</title><categories>cs.HC cs.CR</categories><comments>10 pages, 5 figures</comments><doi>10.14722/usec.2015.23014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User errors while performing security-critical tasks can lead to undesirable
or even disastrous consequences. One major factor influencing mistakes and
failures is complexity of such tasks, which has been studied extensively in
prior research. Another important issue which hardly received any attention is
the impact of both accidental and intended distractions on users performing
security-critical tasks. In particular, it is unclear whether, and to what
extent, unexpected sensory cues (e.g., auditory or visual) can influence user
behavior and/or trigger mistakes. Better understanding of the effects of
intended distractions will help clarify their role in adversarial models. As
part of the research effort described in this paper, we administered a range of
naturally occurring -- yet unexpected -- sounds while study participants
attempted to perform a security-critical task. We found that, although these
auditory cues lowered participants' failure rates, they had no discernible
effect on their task completion times. To this end, we overview some relevant
literature that explains these somewhat counter-intuitive findings.
  Conducting a thorough and meaningful study on user errors requires a large
number of participants, since errors are typically infrequent and should not be
instigated more than once per subject. To reduce the effort of running numerous
subjects, we developed a novel experimental setup that was fully automated and
unattended. We discuss our experience with this setup and highlight the pros
and cons of generalizing its usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3010</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3010</id><created>2014-06-11</created><updated>2014-12-05</updated><authors><author><keyname>Ding</keyname><forenames>Weiguang</forenames></author><author><keyname>Taylor</keyname><forenames>Graham W.</forenames></author></authors><title>&quot;Mental Rotation&quot; by Optimizing Transforming Distance</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human visual system is able to recognize objects despite transformations
that can drastically alter their appearance. To this end, much effort has been
devoted to the invariance properties of recognition systems. Invariance can be
engineered (e.g. convolutional nets), or learned from data explicitly (e.g.
temporal coherence) or implicitly (e.g. by data augmentation). One idea that
has not, to date, been explored is the integration of latent variables which
permit a search over a learned space of transformations. Motivated by evidence
that people mentally simulate transformations in space while comparing
examples, so-called &quot;mental rotation&quot;, we propose a transforming distance.
Here, a trained relational model actively transforms pairs of examples so that
they are maximally similar in some feature space yet respect the learned
transformational constraints. We apply our method to nearest-neighbour problems
on the Toronto Face Database and NORB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3019</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3019</id><created>2014-06-11</created><authors><author><keyname>Mowla</keyname><forenames>Md. Munjure</forenames></author><author><keyname>Paul</keyname><forenames>Liton Chandra</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Rabiul</forenames></author></authors><title>Comparative Performance Analysis of Different Modulation Techniques for
  PAPR Reduction of OFDM Signal</title><categories>cs.NI cs.IT math.IT</categories><comments>11 Pages, 5 Figures. arXiv admin note: substantial text overlap with
  arXiv:1404.2233, arXiv:1403.3349, arXiv:1406.2960</comments><journal-ref>International Journal of Computer Networks and Communications
  (IJCNC), Vol. 6, No. 3, pp.63-73, May 2014 (ISSN: 0974-9322)</journal-ref><doi>10.5121/ijcnc.2014.6306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important multi-carrier transmission techniques used in the
latest wireless communication arena is known as Orthogonal Frequency Division
Multiplexing (OFDM). It has several characteristics such as providing greater
immunity to multipath fading &amp; impulse noise, eliminating Inter Symbol
Interference (ISI) &amp; Inter Carrier Interference (ICI) using a guard interval
known as Cyclic Prefix (CP). A regular difficulty of OFDM signal is high peak
to average power ratio (PAPR) which is defined as the ratio of the peak power
to the average power of OFDM Signal. An improved design of amplitude clipping &amp;
filtering technique of us previously reduced significant amount of PAPR with
slightly increase bit error rate (BER) compare to an existing method in case of
Quadrature Phase Shift Keying (QPSK) &amp; Quadrature Amplitude Modulation (QAM).
This paper investigates a comparative performance analysis of the different
higher order modulation techniques on that design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3047</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3047</id><created>2014-06-11</created><updated>2015-05-13</updated><authors><author><keyname>Bienvenu</keyname><forenames>Meghyn</forenames></author><author><keyname>Kikot</keyname><forenames>Stanislav</forenames></author><author><keyname>Podolskii</keyname><forenames>Vladimir</forenames></author></authors><title>Tree-like Queries in OWL 2 QL: Succinctness and Complexity Results</title><categories>cs.AI cs.CC cs.DB</categories><comments>This is an extended version of a paper accepted at LICS'15. It
  contains both succinctness and complexity results and adopts FOL notation.
  The appendix contains proofs that had to be omitted from the conference
  version for lack of space. The previous arxiv version (a long version of our
  DL'14 workshop paper) only contained the succinctness results and used
  description logic notation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the impact of query topology on the difficulty of
answering conjunctive queries in the presence of OWL 2 QL ontologies. Our first
contribution is to clarify the worst-case size of positive existential (PE),
non-recursive Datalog (NDL), and first-order (FO) rewritings for various
classes of tree-like conjunctive queries, ranging from linear queries to
bounded treewidth queries. Perhaps our most surprising result is a
superpolynomial lower bound on the size of PE-rewritings that holds already for
linear queries and ontologies of depth 2. More positively, we show that
polynomial-size NDL-rewritings always exist for tree-shaped queries with a
bounded number of leaves (and arbitrary ontologies), and for bounded treewidth
queries paired with bounded depth ontologies. For FO-rewritings, we equate the
existence of polysize rewritings with well-known problems in Boolean circuit
complexity. As our second contribution, we analyze the computational complexity
of query answering and establish tractability results (either NL- or
LOGCFL-completeness) for a range of query-ontology pairs. Combining our new
results with those from the literature yields a complete picture of the
succinctness and complexity landscapes for the considered classes of queries
and ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3058</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3058</id><created>2014-06-11</created><updated>2015-07-17</updated><authors><author><keyname>Matousek</keyname><forenames>Jiri</forenames></author><author><keyname>Patakova</keyname><forenames>Zuzana</forenames></author></authors><title>Multilevel polynomial partitions and simplified range searching</title><categories>cs.DS math.AG</categories><comments>19 pages; The proof that the Groebner basis can be effectively
  computed is stated in more detail</comments><msc-class>68U05, 68W30, 14Q20</msc-class><journal-ref>Disc. Comput. Geom. 54(1):22-41, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The polynomial partitioning method of Guth and Katz [arXiv:1011.4105] has
numerous applications in discrete and computational geometry. It partitions a
given $n$-point set $P\subset\mathbb{R}^d$ using the zero set $Z(f)$ of a
suitable $d$-variate polynomial $f$. Applications of this result are often
complicated by the problem, what should be done with the points of $P$ lying
within $Z(f)$? A natural approach is to partition these points with another
polynomial and continue further in a similar manner. So far it has been pursued
with limited success---several authors managed to construct and apply a second
partitioning polynomial, but further progress has been prevented by technical
obstacles. We provide a polynomial partitioning method with up to $d$
polynomials in dimension $d$, which allows for a complete decomposition of the
given point set. We apply it to obtain a new algorithm for the semialgebraic
range searching problem. Our algorithm has running time bounds similar to a
recent algorithm by Agarwal, Sharir, and the first author [SIAM~J.~Comput.
42(2013) 2039--2062], but it is simpler both conceptually and technically.
While this paper has been in preparation, Basu and Sombra, as well as Fox,
Pach, Sheffer, Suk, and Zahl, obtained results concerning polynomial partitions
which overlap with ours to some extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3065</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3065</id><created>2014-06-11</created><updated>2014-07-29</updated><authors><author><keyname>Jukna</keyname><forenames>Stasys</forenames></author></authors><title>Lower Bounds for Tropical Circuits and Dynamic Programs</title><categories>cs.CC</categories><comments>Corrected reduction to arithmetic circuits (holds only for
  multilinear polynomials, now Sect. 4). Solved Open Problem 3 about Min/Max
  gaps (now Lemma 10). Added lower bounds for the depth of tropical circuits
  (Sect. 15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tropical circuits are circuits with Min and Plus, or Max and Plus operations
as gates. Their importance stems from their intimate relation to dynamic
programming algorithms. The power of tropical circuits lies somewhere between
that of monotone boolean circuits and monotone arithmetic circuits. In this
paper we present some lower bounds arguments for tropical circuits, and hence,
for dynamic programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3074</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3074</id><created>2014-06-11</created><authors><author><keyname>Kotagiri</keyname><forenames>Vamsi Sashank</forenames></author></authors><title>A New Result on the Random Residue Sequence Algorithm</title><categories>cs.CR</categories><comments>6 pages,2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random residue sequences (RR) may be used in many random number applications
including those related to multiple access in communications. This paper
investigates variations on an algorithm to generate RR sequences that was
proposed earlier by the author. This makes it possible to obtain many more
random sequences than was possible to do by the previous algorithm.
Experimental results are presented on a variety of sequences of length 16 and
24. To obtain a variety of RR sequences of a specific length can have obvious
applications in cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3084</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3084</id><created>2014-06-11</created><updated>2016-02-22</updated><authors><author><keyname>Phung-Duc</keyname><forenames>Tuan</forenames></author></authors><title>Exact Solutions for M/M/c/Setup Queues</title><categories>cs.PF</categories><comments>Submitted for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently multiserver queues with setup times have been extensively studied
because they have applications in power-saving data centers. The most
challenging model is the M/M/$c$/Setup queue where a server is turned off when
it is idle and is turned on if there are some waiting jobs. Recently, Gandhi et
al.~(SIGMETRICS 2013, QUESTA 2014) present the recursive renewal reward
approach as a new mathematical tool to analyze the model. In this paper, we
derive exact solutions for the same model using two alternative methodologies:
generating function approach and matrix analytic method. The former yields
several theoretical insights into the systems while the latter provides an
exact recursive algorithm to calculate the joint stationary distribution and
then some performance measures so as to give new application insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3092</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3092</id><created>2014-06-11</created><authors><author><keyname>Wu</keyname><forenames>Yingjie</forenames></author><author><keyname>Zhu</keyname><forenames>Daxin</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>A note on the largest number of red nodes in red-black trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are interested in the number of red nodes in red-black
trees. We first present an $O(n^2\log n)$ time dynamic programming solution for
computing $r(n)$, the largest number of red internal nodes in a red-black tree
on $n$ keys. Then the algorithm is improved to some $O(\log n)$ time recursive
and nonrecursive algorithms. Based on these improved algorithms we finally find
a closed-form solution of $r(n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3100</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3100</id><created>2014-06-11</created><authors><author><keyname>de Chazal</keyname><forenames>Philip</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>van Schaik</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Learning ELM network weights using linear discriminant analysis</title><categories>cs.NE cs.LG stat.ML</categories><comments>In submission to the ELM 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an alternative to the pseudo-inverse method for determining the
hidden to output weight values for Extreme Learning Machines performing
classification tasks. The method is based on linear discriminant analysis and
provides Bayes optimal single point estimates for the weight values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3103</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3103</id><created>2014-06-11</created><authors><author><keyname>Kang</keyname><forenames>Wei</forenames></author><author><keyname>Cao</keyname><forenames>Daming</forenames></author><author><keyname>Liu</keyname><forenames>Nan</forenames></author></authors><title>Deception with Side Information in Biometric Authentication Systems</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the probability of successful deception of an
uncompressed biometric authentication system with side information at the
adversary. It represents the scenario where the adversary may have correlated
side information, e.g.,~a partial finger print or a DNA sequence of a relative
of the legitimate user. We find the optimal exponent of the deception
probability by proving both the achievability and the converse. Our proofs are
based on the connection between the problem of deception with side information
and the rate distortion problem with side information at both the encoder and
decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3110</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3110</id><created>2014-06-11</created><authors><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author><author><keyname>Rokach</keyname><forenames>Lior</forenames></author></authors><title>Reaction to New Security Threat Class</title><categories>cs.DL cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Each new identified security threat class triggers new research and
development efforts by the scientific and professional communities. In this
study, we investigate the rate at which the scientific and professional
communities react to new identified threat classes as it is reflected in the
number of patents, scientific articles and professional publications over a
long period of time. The following threat classes were studied: Phishing; SQL
Injection; BotNet; Distributed Denial of Service; and Advanced Persistent
Threat. Our findings suggest that in most cases it takes a year for the
scientific community and more than two years for industry to react to a new
threat class with patents. Since new products follow patents, it is reasonable
to expect that there will be a window of approximately two to three years in
which no effective product is available to cope with the new threat class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3117</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3117</id><created>2014-06-12</created><authors><author><keyname>Nguyen</keyname><forenames>Anh</forenames></author><author><keyname>Banic</keyname><forenames>Amy</forenames></author></authors><title>Low-cost Augmented Reality prototype for controlling network devices</title><categories>cs.HC cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the evolution of mobile devices, and smart-phones in particular, comes
the ability to create new experiences that enhance the way we see, interact,
and manipulate objects, within the world that surrounds us. It is now possible
to blend data from our senses and our devices in numerous ways that simply were
not possible before using Augmented Reality technology. In a near future, when
all of the office devices as well as your personal electronic gadgets are on a
common wireless network, operating them using a universal remote controller
would be possible. This paper presents an off-the-shelf, low-cost prototype
that leverages the Augmented Reality technology to deliver a novel and
interactive way of operating office network devices around using a mobile
device. We believe this type of system may provide benefits to controlling
multiple integrated devices and visualizing interconnectivity or utilizing
visual elements to pass information from one device to another, or may be
especially beneficial to control devices when interacting with them physically
may be difficult or pose danger or harm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3121</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3121</id><created>2014-06-12</created><authors><author><keyname>Mishra</keyname><forenames>Saraswati</forenames></author><author><keyname>Dhankar</keyname><forenames>Shikha</forenames></author><author><keyname>Choudhary</keyname><forenames>Kavita</forenames></author></authors><title>Impact of Internet Governance</title><categories>cs.CY</categories><comments>4 pages, 2 figures, Internation Journal of Computer Trends and
  Technology- IJCTT, Vol 4, Issue 10,October 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper represents the overview of the influence of internet governance in
all the transactions, trading, business, social services, educational
activities, research etc. occurring through the internet. It is very essential
to implement laws and regulations in the field wherever the transaction takes
place, either in the form of money, material or services. To avoid any kind of
fraud and cheating and to establish a peaceful environment in physical as well
as virtual word, it is essential to have some organization which assures safety
and security. Here I-governance is the governing body that tries to take care
of all those requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3123</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3123</id><created>2014-06-12</created><authors><author><keyname>Hasan</keyname><forenames>Monowar</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Distributed Resource Allocation for Relay-Aided Device-to-Device
  Communication: A Message Passing Approach</title><categories>cs.NI</categories><comments>IEEE Transactions on Wireless Communications, to appear. arXiv admin
  note: text overlap with arXiv:1401.6683</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communication underlaying cellular wireless networks
is a promising concept to improve user experience and resource utilization by
allowing direct transmission between two cellular devices. In this paper,
performance of network-assisted D2D communication is investigated where D2D
traffic is carried through relay nodes. Considering a multi-user and
multi-relay network, we propose a distributed solution for resource allocation
with a view to maximizing network sum-rate. An optimization problem is
formulated for radio resource allocation at the relays. The objective is to
maximize end-to-end rate as well as satisfy the data rate requirements for
cellular and D2D user equipments under total power constraint. Due to
intractability of the resource allocation problem, we propose a solution
approach using message passing technique where each user equipment sends and
receives information messages to/from the relay node in an iterative manner
with the goal of achieving an optimal allocation. Therefore, the computational
effort is distributed among all the user equipments and the corresponding relay
node. The convergence and optimality of the proposed scheme are proved and a
possible distributed implementation of the scheme in practical LTE-Advanced
networks is outlined. The numerical results show that there is a distance
threshold beyond which relay-aided D2D communication significantly improves
network performance with a small increase in end-to-end delay when compared to
direct communication between D2D peers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3124</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3124</id><created>2014-06-12</created><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Guarantees and Limits of Preprocessing in Constraint Satisfaction and
  Reasoning</title><categories>cs.AI cs.CC cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1104.2541,
  arXiv:1104.5566</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a first theoretical analysis of the power of polynomial-time
preprocessing for important combinatorial problems from various areas in AI. We
consider problems from Constraint Satisfaction, Global Constraints,
Satisfiability, Nonmonotonic and Bayesian Reasoning under structural
restrictions. All these problems involve two tasks: (i) identifying the
structure in the input as required by the restriction, and (ii) using the
identified structure to solve the reasoning task efficiently. We show that for
most of the considered problems, task (i) admits a polynomial-time
preprocessing to a problem kernel whose size is polynomial in a structural
problem parameter of the input, in contrast to task (ii) which does not admit
such a reduction to a problem kernel of polynomial size, subject to a
complexity theoretic assumption. As a notable exception we show that the
consistency problem for the AtMost-NValue constraint admits a polynomial kernel
consisting of a quadratic number of variables and domain values. Our results
provide a firm worst-case guarantees and theoretical boundaries for the
performance of polynomial-time preprocessing algorithms for the considered
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3139</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3139</id><created>2014-06-12</created><updated>2014-10-09</updated><authors><author><keyname>Zankl</keyname><forenames>Harald</forenames></author><author><keyname>Felgenhauer</keyname><forenames>Bertram</forenames></author><author><keyname>Middeldorp</keyname><forenames>Aart</forenames></author></authors><title>Labelings for Decreasing Diagrams</title><categories>cs.LO</categories><journal-ref>Journal of Automated Reasoning 54(2) 101-133 2015</journal-ref><doi>10.1007/s10817-014-9316-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is concerned with automating the decreasing diagrams technique
of van Oostrom for establishing confluence of term rewrite systems. We study
abstract criteria that allow to lexicographically combine labelings to show
local diagrams decreasing. This approach has two immediate benefits. First, it
allows to use labelings for linear rewrite systems also for left-linear ones,
provided some mild conditions are satisfied. Second, it admits an incremental
method for proving confluence which subsumes recent developments in automating
decreasing diagrams. The techniques proposed in the article have been
implemented and experimental results demonstrate how, e.g., the rule labeling
benefits from our contributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3147</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3147</id><created>2014-06-12</created><authors><author><keyname>Ling</keyname><forenames>Jonathan</forenames></author><author><keyname>Kanugovi</keyname><forenames>Satish</forenames></author><author><keyname>Vasudevan</keyname><forenames>Subramanian</forenames></author><author><keyname>Pramod</keyname><forenames>A Krishna</forenames></author></authors><title>Enhanced capacity &amp; coverage by Wi-Fi LTE Integration</title><categories>cs.NI</categories><comments>Submitted as a candidate article for IEEE Communications Magazine,
  Topic: The Future of Wi-Fi (November 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wi-Fi provides cost-effective data capacity at hotspots in conjunction with
broadband cellular networks. The hotspots are required to capture a large
number of users and provide high data rates. Data rates, over the Wi-Fi
interface, are influenced by the media access protocol, which loses throughput
due to delays and unintended collisions when a large number of users are
active. The hotspot range which determines the number of users, that can
associate, is limited by the lower power of the client rather than the access
point. By diverting the traffic destined to the access point via another access
network, both range and efficiency can be improved. This uplink redirection or
diversion is achieved by simultaneous use of the Wi-Fi and LTE radio
interfaces. Three options - loose, tight, and hybrid integration are presented
towards providing enhanced capacity and coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3149</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3149</id><created>2014-06-12</created><authors><author><keyname>Bonanno</keyname><forenames>Francesco</forenames></author><author><keyname>Capizzi</keyname><forenames>Giacomo</forenames></author><author><keyname>Sciuto</keyname><forenames>Grazia Lo</forenames></author><author><keyname>Napoli</keyname><forenames>Christian</forenames></author><author><keyname>Pappalardo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Tramontana</keyname><forenames>Emiliano</forenames></author></authors><title>A Cascade Neural Network Architecture investigating Surface Plasmon
  Polaritons propagation for thin metals in OpenMP</title><categories>cs.NE cond-mat.mes-hall cond-mat.mtrl-sci cs.DC cs.LG</categories><msc-class>68T05</msc-class><acm-class>I.2.6; I.5.1; I.2.11</acm-class><journal-ref>International conference on Artificial Intelligence and Soft
  Computing (ICAISC 2014), Vol I, 22-33 (2014)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Surface plasmon polaritons (SPPs) confined along metal-dielectric interface
have attracted a relevant interest in the area of ultracompact photonic
circuits, photovoltaic devices and other applications due to their strong field
confinement and enhancement. This paper investigates a novel cascade neural
network (NN) architecture to find the dependance of metal thickness on the SPP
propagation. Additionally, a novel training procedure for the proposed cascade
NN has been developed using an OpenMP-based framework, thus greatly reducing
training time. The performed experiments confirm the effectiveness of the
proposed NN architecture for the problem at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3156</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3156</id><created>2014-06-12</created><authors><author><keyname>Napoli</keyname><forenames>Christian</forenames></author><author><keyname>Pappalardo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Tramontana</keyname><forenames>Emiliano</forenames></author></authors><title>A hybrid neuro--wavelet predictor for QoS control and stability</title><categories>cs.NE cs.DC cs.NI cs.PF cs.SY</categories><msc-class>68T05</msc-class><acm-class>I.2.6; I.5.1; I.2.11</acm-class><journal-ref>Proceedings of AI*IA 2013: Advances in Artificial Intelligence,
  pages 527-538. Springer, 2013</journal-ref><doi>10.1007/978-3-319-03524-6_45</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  For distributed systems to properly react to peaks of requests, their
adaptation activities would benefit from the estimation of the amount of
requests. This paper proposes a solution to produce a short-term forecast based
on data characterising user behaviour of online services. We use \emph{wavelet
analysis}, providing compression and denoising on the observed time series of
the amount of past user requests; and a \emph{recurrent neural network} trained
with observed data and designed so as to provide well-timed estimations of
future requests. The said ensemble has the ability to predict the amount of
future user requests with a root mean squared error below 0.06\%. Thanks to
prediction, advance resource provision can be performed for the duration of a
request peak and for just the right amount of resources, hence avoiding
over-provisioning and associated costs. Moreover, reliable provision lets users
enjoy a level of availability of services unaffected by load variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3161</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3161</id><created>2014-06-12</created><updated>2014-10-16</updated><authors><author><keyname>Toni</keyname><forenames>Laura</forenames></author><author><keyname>Aparicio-Pardo</keyname><forenames>Ramon</forenames></author><author><keyname>Pires</keyname><forenames>Karine</forenames></author><author><keyname>Simon</keyname><forenames>Gwendal</forenames></author><author><keyname>Blanc</keyname><forenames>Alberto</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Optimized Adaptive Streaming Representations based on System Dynamics</title><categories>cs.MM</categories><doi>10.1145/2700294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive streaming addresses the increasing and heterogenous demand of
multimedia content over the Internet by offering several encoded versions for
each video sequence. Each version (or representation) has a different
resolution and bit rate, aimed at a specific set of users, like TV or mobile
phone clients. While most existing works on adaptive streaming deal with
effective playout-control strategies at the client side, we take in this paper
a providers' perspective and propose solutions to improve user satisfaction by
optimizing the encoding rates of the video sequences. We formulate an integer
linear program that maximizes users' average satisfaction, taking into account
the network dynamics, the video content information, and the user population
characteristics. The solution of the optimization is a set of encoding
parameters that permit to create different streams to robustly satisfy users'
requests over time. We simulate multiple adaptive streaming sessions
characterized by realistic network connections models, where the proposed
solution outperforms commonly used vendor recommendations, in terms of user
satisfaction but also in terms of fairness and outage probability. The
simulation results further show that video content information as well as
network constraints and users' statistics play a crucial role in selecting
proper encoding parameters to provide fairness a mong users and to reduce
network resource usage. We finally propose a few practical guidelines that can
be used to choose the encoding parameters based on the user base
characteristics, the network capacity and the type of video content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3163</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3163</id><created>2014-06-12</created><updated>2014-12-22</updated><authors><author><keyname>Pl&#xfb;t</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Fouque</keyname><forenames>Pierre-Alain</forenames></author><author><keyname>Macario-Rat</keyname><forenames>Gilles</forenames></author></authors><title>Solving the &quot;Isomorphism of Polynomials with Two Secrets&quot; Problem for
  all Pairs of Quadratic Forms</title><categories>cs.SC cs.CR</categories><msc-class>15A22 (Primary), 15A21, 15A63, 11T71 (Secondary)</msc-class><acm-class>F.2.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We study the Isomorphism of Polynomial (IP2S) problem with m=2 homogeneous
quadratic polynomials of n variables over a finite field of odd characteristic:
given two quadratic polynomials (a, b) on n variables, we find two bijective
linear maps (s,t) such that b=t . a . s. We give an algorithm computing s and t
in time complexity O~(n^4) for all instances, and O~(n^3) in a dominant set of
instances.
  The IP2S problem was introduced in cryptography by Patarin back in 1996. The
special case of this problem when t is the identity is called the isomorphism
with one secret (IP1S) problem. Generic algebraic equation solvers (for example
using Gr\&quot;obner bases) solve quite well random instances of the IP1S problem.
For the particular cyclic instances of IP1S, a cubic-time algorithm was later
given and explained in terms of pencils of quadratic forms over all finite
fields; in particular, the cyclic IP1S problem in odd characteristic reduces to
the computation of the square root of a matrix.
  We give here an algorithm solving all cases of the IP1S problem in odd
characteristic using two new tools, the Kronecker form for a singular quadratic
pencil, and the reduction of bilinear forms over a non-commutative algebra.
Finally, we show that the second secret in the IP2S problem may be recovered in
cubic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3164</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3164</id><created>2014-06-12</created><authors><author><keyname>Yang</keyname><forenames>Ang</forenames></author><author><keyname>He</keyname><forenames>Zunwen</forenames></author><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Kuang</keyname><forenames>Jingming</forenames></author></authors><title>The Role of Large-Scale Fading in Uplink Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Vehicular Technology as a
  correspondence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this correspondence, we analyze the ergodic capacity of a large uplink
multi-user multiple-input multiple-output (MU-MIMO) system over generalized-$K$
fading channels. In the considered scenario, multiple users transmit their
information to a base station equipped with a very large number of antennas.
Since the effect of fast fading asymptotically disappears in massive MIMO
systems, large-scale fading becomes the most dominant factor for the ergodic
capacity of massive MIMO systems. Regarding this fact, in our work we
concentrate our attention on the effects of large-scale fading for massive MIMO
systems. Specifically, some interesting and novel lower bounds of the ergodic
capacity have been derived with both perfect channel state information (CSI)
and imperfect CSI. Simulation results assess the accuracy of these analytical
expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3170</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3170</id><created>2014-06-12</created><authors><author><keyname>Gog</keyname><forenames>Simon</forenames></author><author><keyname>Petri</keyname><forenames>Matthias</forenames></author></authors><title>Compact Indexes for Flexible Top-k Retrieval</title><categories>cs.DS cs.IR</categories><comments>14 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We engineer a self-index based retrieval system capable of rank-safe
evaluation of top-k queries. The framework generalizes the GREEDY approach of
Culpepper et al. (ESA 2010) to handle multi-term queries, including over
phrases. We propose two techniques which significantly reduce the ranking time
for a wide range of popular Information Retrieval (IR) relevance measures, such
as TFxIDF and BM25. First, we reorder elements in the document array according
to document weight. Second, we introduce the repetition array, which
generalizes Sadakane's (JDA 2007) document frequency structure to document
subsets. Combining document and repetition array, we achieve attractive
functionality-space trade-offs. We provide an extensive evaluation of our
system on terabyte-sized IR collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3172</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3172</id><created>2014-06-12</created><authors><author><keyname>Kopparapu</keyname><forenames>Sunil</forenames></author><author><keyname>Satish</keyname><forenames>M</forenames></author></authors><title>Optimal Gaussian Filter for Effective Noise Filtering</title><categories>cs.OH</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the knowledge of noise statistics contaminating a
signal can be effectively used to choose an optimal Gaussian filter to
eliminate noise. Very specifically, we show that the additive white Gaussian
noise (AWGN) contaminating a signal can be filtered best by using a Gaussian
filter of specific characteristics. The design of the Gaussian filter bears
relationship with the noise statistics and also some basic information about
the signal. We first derive a relationship between the properties of the
Gaussian filter, noise statistics and the signal and later show through
experiments that this relationship can be used effectively to identify the
optimal Gaussian filter that can effectively filter noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3185</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3185</id><created>2014-06-12</created><updated>2015-01-02</updated><authors><author><keyname>Shankar</keyname><forenames>Karthik H.</forenames></author></authors><title>Generic construction of scale-invariantly coarse grained memory</title><categories>q-bio.NC cond-mat.dis-nn cs.AI</categories><journal-ref>Lecture Notes in Artificial Intelligence, vol: 8955, pp: 175-184,
  2015</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Encoding temporal information from the recent past as spatially distributed
activations is essential in order for the entire recent past to be
simultaneously accessible. Any biological or synthetic agent that relies on the
past to predict/plan the future, would be endowed with such a spatially
distributed temporal memory. Simplistically, we would expect that resource
limitations would demand the memory system to store only the most useful
information for future prediction. For natural signals in real world which show
scale free temporal fluctuations, the predictive information encoded in memory
is maximal if the past information is scale invariantly coarse grained. Here we
examine the general mechanism to construct a scale invariantly coarse grained
memory system. Remarkably, the generic construction is equivalent to encoding
the linear combinations of Laplace transform of the past information and their
approximated inverses. This reveals a fundamental construction constraint on
memory networks that attempt to maximize predictive information storage
relevant to the natural world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3188</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3188</id><created>2014-06-12</created><authors><author><keyname>Lex</keyname><forenames>Elisabeth</forenames></author><author><keyname>Khan</keyname><forenames>Inayat</forenames></author><author><keyname>Bischof</keyname><forenames>Horst</forenames></author><author><keyname>Granitzer</keyname><forenames>Michael</forenames></author></authors><title>Assessing the Quality of Web Content</title><categories>cs.IR</categories><comments>4 pages, ECML/PKDD 2010 Discovery Challenge Workshop</comments><acm-class>H.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes our approach towards the ECML/PKDD Discovery Challenge
2010. The challenge consists of three tasks: (1) a Web genre and facet
classification task for English hosts, (2) an English quality task, and (3) a
multilingual quality task (German and French). In our approach, we create an
ensemble of three classifiers to predict unseen Web hosts whereas each
classifier is trained on a different feature set. Our final NDCG on the whole
test set is 0:575 for Task 1, 0:852 for Task 2, and 0:81 (French) and 0:77
(German) for Task 3, which ranks second place in the ECML/PKDD Discovery
Challenge 2010.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3190</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3190</id><created>2014-06-12</created><updated>2015-05-06</updated><authors><author><keyname>Shen</keyname><forenames>Jie</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Stochastic Optimization for Max-Norm Regularizer via Matrix
  Factorization</title><categories>stat.ML cs.LG</categories><comments>A conference version appears in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max-norm regularizer has been extensively studied in the last decade as it
promotes an effective low rank estimation of the underlying data. However,
max-norm regularized problems are typically formulated and solved in a batch
manner, which prevents it from processing big data due to possible memory
bottleneck. In this paper, we propose an online algorithm for solving max-norm
regularized problems that is scalable to large problems. Particularly, we
consider the matrix decomposition problem as an example, although our analysis
can also be applied in other problems such as matrix completion. The key
technique in our algorithm is to reformulate the max-norm into a matrix
factorization form, consisting of a basis component and a coefficients one. In
this way, we can solve the optimal basis and coefficients alternatively. We
prove that the basis produced by our algorithm converges to a stationary point
asymptotically. Experiments demonstrate encouraging results for the
effectiveness and robustness of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3191</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3191</id><created>2014-06-12</created><updated>2014-06-13</updated><authors><author><keyname>Fanaee-T</keyname><forenames>Hadi</forenames></author><author><keyname>Gama</keyname><forenames>Joao</forenames></author></authors><title>An eigenvector-based hotspot detection</title><categories>cs.AI</categories><journal-ref>In Proceedings of 16th Portuguese Conference on Artificial
  Intelligence (EPIA 2013), Acores, Portugal, 9-12 September 2013, PP. 290-301</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Space and time are two critical components of many real world systems. For
this reason, analysis of anomalies in spatiotemporal data has been a great of
interest. In this work, application of tensor decomposition and eigenspace
techniques on spatiotemporal hotspot detection is investigated. An algorithm
called SST-Hotspot is proposed which accounts for spatiotemporal variations in
data and detect hotspots using matching of eigenvector elements of two cases
and population tensors. The experimental results reveal the interesting
application of tensor decomposition and eigenvector-based techniques in hotspot
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3194</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3194</id><created>2014-06-12</created><authors><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Baig</keyname><forenames>Adeel</forenames></author><author><keyname>Ali</keyname><forenames>Asad</forenames></author><author><keyname>Shafi</keyname><forenames>Quratulain</forenames></author></authors><title>Multicasting in Cognitive Radio Networks: Algorithms, Techniques and
  Protocols</title><categories>cs.NI</categories><journal-ref>Elsevier Journal of Network and Computer Applications, October,
  2014</journal-ref><doi>10.1016/j.jnca.2014.07.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicasting is a fundamental networking primitive utilized by numerous
applications. This also holds true for cognitive radio networks (CRNs) which
have been proposed as a solution to the problems that emanate from the static
non-adaptive features of classical wireless networks. A prime application of
CRNs is dynamic spectrum access (DSA), which improves the efficiency of
spectrum allocation by allowing a secondary network, comprising of secondary
users (SUs), to share spectrum licensed to a primary licensed networks
comprising of primary users (PUs). Multicasting in CRNs is a challenging
problem due to the dynamic nature of spectrum opportunities available to the
SUs. Various approaches, including those based in optimization theory, network
coding, algorithms, have been proposed for performing efficient multicast in
CRNs. In this paper, we provide a self-contained tutorial on algorithms and
techniques useful for solving the multicast problem, and then provide a
comprehensive survey of protocols that have been proposed for multicasting in
CRNs. We conclude this paper by identifying open research questions and future
research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3204</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3204</id><created>2014-06-12</created><updated>2014-07-23</updated><authors><author><keyname>P</keyname><forenames>Raghu Vamsi.</forenames></author><author><keyname>Kant</keyname><forenames>Krishna</forenames></author></authors><title>Systematic Design of Trust Management Systems for Wireless Sensor
  Networks: A Review</title><categories>cs.NI cs.CR</categories><comments>The paper has been withdrawn due to typesetting mistakes</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Conventional cryptography methods alone are not adequate for secure routing
in Wireless Sensor Networks (WSNs). These networks are more vulnerable to
security attacks due to their diverse applications, lack of supervision and
limitations in view of resource, processing and storage. To mitigate these
problems, trust is widely used as a tool to provide better security by aiding
routing protocols. In recent years, numerous researchers have proposed wide
variety of solutions based on trust. However, all these solutions carry their
own design. In this paper, we attempt to present steps for a systematic design
of trust management systems for WSNs. In addition, we address the techniques
followed by scholars in implementing trust frameworks. Furthermore, we provide
discussion on state-of-the-art research in designing trust systems with summary
and comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3209</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3209</id><created>2014-06-12</created><updated>2014-07-23</updated><authors><author><keyname>P</keyname><forenames>Raghu Vamsi.</forenames></author><author><keyname>Batra</keyname><forenames>Payal Khurana</forenames></author><author><keyname>Kant</keyname><forenames>Krishna</forenames></author></authors><title>BT-GPSR: An Integrated Trust Model for Secure Geographic Routing in
  Wireless Sensor Networks</title><categories>cs.NI</categories><comments>The paper has been withdrawn due to improper display of equations</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Geographic routing offers guaranteed packet deliv- ery in a dense network. In
this routing, packets are forwarded to a node which is nearer to the
destination with an extensive use of location information. However, research
studies in Mobile Adhoc Networks (MANETs) and Wireless Sensor Networks (WSNs)
have shown that packet delivery percentage can degrade substantially when
malicious nodes are found in the network. Conventional cryptography techniques
can be adopted in order to deal with ma- licious nodes, but they cannot
mitigate outsider attacks. In recent years, a societal pattern called trust is
used as a tool to mitigate security attacks. Numerous researchers have proposed
security solutions by adopting trust in routing algorithms. However, each
solution has its own strength and weakness. In this paper, an integrated
approach by using reputation and weight based trust systems backed by Greedy
Perimeter Stateless Routing (BT-GPSR) is presented. The proposed approach
outperforms the conventional reputation and weight based methods. The
effectiveness of the proposed BT-GPSR is validated through simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3214</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3214</id><created>2014-06-12</created><authors><author><keyname>Caron</keyname><forenames>Pascal</forenames></author><author><keyname>Flouret</keyname><forenames>Marianne</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author></authors><title>(k,l)-Unambiguity and Quasi-Deterministic Structures</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the family of $(k,l)$-unambiguous automata that encompasses the
one of deterministic $k$-lookahead automata introduced by Han and Wood. We show
that this family presents nice theoretical properties that allow us to compute
quasi-deterministic structures. These structures are smaller than DFAs and can
be used to solve the membership problem faster than NFAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3216</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3216</id><created>2014-06-12</created><updated>2014-09-17</updated><authors><author><keyname>Burattin</keyname><forenames>Andrea</forenames></author><author><keyname>Cascavilla</keyname><forenames>Giuseppe</forenames></author><author><keyname>Conti</keyname><forenames>Mauro</forenames></author></authors><title>SocialSpy: Browsing (Supposedly) Hidden Information in Online Social
  Networks</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Social Networks are becoming the most important &quot;places&quot; where people
share information about their lives. With the increasing concern that users
have about privacy, most social networks offer ways to control the privacy of
the user. Unfortunately, we believe that current privacy settings are not as
effective as users might think.
  In this paper, we highlight this problem focusing on one of the most popular
social networks, Facebook. In particular, we show how easy it is to retrieve
information that a user might have set as (and hence thought as) &quot;private&quot;. As
a case study, we focus on retrieving the list of friends for users that did set
this information as &quot;hidden&quot; (to non-friends). We propose four different
strategies to achieve this goal, and we evaluate them. The results of our
thorough experiments show the feasibility of our strategies as well as their
effectiveness: our approach is able to retrieve a significant percentage of the
names of the &quot;hidden&quot; friends: i.e., some 25% on average, and more than 70% for
some users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3224</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3224</id><created>2014-06-12</created><updated>2015-11-24</updated><authors><author><keyname>Geiselhart</keyname><forenames>Roman</forenames></author><author><keyname>Wirth</keyname><forenames>Fabian R.</forenames></author></authors><title>Relaxed ISS Small-Gain Theorems for Discrete-Time Systems</title><categories>math.DS cs.SY math.OC</categories><comments>input-to-state stability, Lyapunov methods, small-gain conditions,
  discrete-time non-linear systems, large-scale interconnections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper ISS small-gain theorems for discrete-time systems are stated,
which do not require input-to-state stability (ISS) of each subsystem. This
approach weakens conservatism in ISS small-gain theory, and for the class of
exponentially ISS systems we are able to prove that the proposed relaxed
small-gain theorems are non-conservative in a sense to be made precise. The
proofs of the small-gain theorems rely on the construction of a dissipative
finite-step ISS Lyapunov function which is introduced in this work.
Furthermore, dissipative finite-step ISS Lyapunov functions, as relaxations of
ISS Lyapunov functions, are shown to be sufficient and necessary to conclude
ISS of the overall system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3225</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3225</id><created>2014-06-12</created><authors><author><keyname>M&#xf6;ller</keyname><forenames>Andreas</forenames></author><author><keyname>Diewald</keyname><forenames>Stefan</forenames></author><author><keyname>Roalter</keyname><forenames>Luis</forenames></author><author><keyname>Kranz</keyname><forenames>Matthias</forenames></author></authors><title>Supporting Mobile Multimodal Interaction with a Rule-Based Framework</title><categories>cs.HC</categories><comments>8 pages, 4 figures, extended version of the short paper at Mensch und
  Computer 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimodality can make (especially mobile) device interaction more efficient.
Sensors and communication capabilities of modern smartphones and tablets lay
the technical basis for its implementation. Still, mobile platforms do not make
multimodal interaction support trivial. Building multimodal applications
requires various APIs with different paradigms, high-level interpretation of
contextual data, and a method for fusing individual inputs and outputs. To
reduce this effort, we created a framework that simplifies and accelerates the
creation of multimodal applications for prototyping and research. It provides
an abstraction of information representations in different modalities, unifies
access to implicit and explicit information, and wires together the logic
behind context-sensitive modality switches. In the paper, we present the
structure and features of our framework, and validate it by four implemented
demonstrations of different complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3238</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3238</id><created>2014-06-12</created><authors><author><keyname>Duport</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Akrout</keyname><forenames>Akram</forenames></author><author><keyname>Smerieri</keyname><forenames>Anteo</forenames></author><author><keyname>Haelterman</keyname><forenames>Marc</forenames></author><author><keyname>Massar</keyname><forenames>Serge</forenames></author></authors><title>Analog input layer for optical reservoir computers</title><categories>cs.ET</categories><comments>submitted to New Journal of Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reservoir computing is an information processing technique, derived from the
theory of neural networks, which is easy to implement in hardware. Several
reservoir computer hardware implementations have been realized recently with
performance comparable to digital implementations, which demonstrated the
potential of reservoir computing for ultrahigh bandwidth signal processing
tasks. In all these implementations however the signal pre-processing necessary
to efficiently address the reservoir was performed digitally. Here we show how
this digital pre-processing can be replaced by an analog input layer. We study
both numerically and experimentally to what extent the pre-processing can be
replaced by a modulation of the input signal by either a single sine-wave or by
a sum of two sine functions, since harmonic oscillations are particularly easy
to generate in hardware. We find that the modulation by a single sine gives
performance worse than state of the art. On the other hand, on many -but not
all- tasks, the modulation by two sines gives performances comparable to the
state of the art. The present work thus represents an important step towards
fully autonomous, ultrahigh bandwidth analog reservoir computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3240</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3240</id><created>2014-06-12</created><updated>2014-06-16</updated><authors><author><keyname>Yi</keyname><forenames>Wentan</forenames></author><author><keyname>Chen</keyname><forenames>Shaozhen</forenames></author><author><keyname>Wei</keyname><forenames>Kuanyang</forenames></author></authors><title>Zero-Correlation Linear Cryptanalysis of Reduced Round ARIA with
  Partial-sum and FFT</title><categories>cs.CR</categories><comments>arXiv admin note: text overlap with arXiv:1404.6100</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Block cipher ARIA was first proposed by some South Korean experts in 2003,
and later, it was established as a Korean Standard block cipher algorithm by
Korean Agency for Technology and Standards. In this paper, we focus on the
security evaluation of ARIA block cipher against the recent zero-correlation
linear cryptanalysis. In addition, Partial-sum technique and FFT (Fast Fourier
Transform) technique are used to speed up the cryptanalysis, respectively.
  We first introduce some 4-round linear approximations of ARIA with
zero-correlation, and then present some key-recovery attacks on 6/7-round
ARIA-128/256 with Partial-sum technique and FFT technique.The key-recovery
attack with Partial-sum technique on 6-round ARIA-128 needs 2^{123.6}known
plaintexts (KPs), 2^{121} encryptions and 2^{90.3} bytes memory, and the attack
with FFT technique requires 2^{124.1} KPs, 2^{121.5} encryptions and
2^{90.3}bytes memory. Moreover, applying Partial-sum technique, we can attack
7-round ARIA-256 with 2^{124.6} KPs, 2^{203.5} encryptions and 2^{152} bytes
and 7-round ARIA-256 employing FFT technique, requires 2^{124.7} KPs, 2^{209.5}
encryptions and 2^{152} bytes. Our results are the first zero-correlation
linear cryptanalysis results on ARIA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3247</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3247</id><created>2014-06-12</created><authors><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>Lagerkvist</keyname><forenames>Victor</forenames></author><author><keyname>Schmidt</keyname><forenames>Johannes</forenames></author><author><keyname>Uppman</keyname><forenames>Hannes</forenames></author></authors><title>Relating the Time Complexity of Optimization Problems in Light of the
  Exponential-Time Hypothesis</title><categories>cs.CC</categories><comments>This is an extended version of Relating the Time Complexity of
  Optimization Problems in Light of the Exponential-Time Hypothesis, appearing
  in Proceedings of the 39th International Symposium on Mathematical
  Foundations of Computer Science MFCS 2014 Budapest, August 25-29, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obtaining lower bounds for NP-hard problems has for a long time been an
active area of research. Recent algebraic techniques introduced by Jonsson et
al. (SODA 2013) show that the time complexity of the parameterized SAT($\cdot$)
problem correlates to the lattice of strong partial clones. With this ordering
they isolated a relation $R$ such that SAT($R$) can be solved at least as fast
as any other NP-hard SAT($\cdot$) problem. In this paper we extend this method
and show that such languages also exist for the max ones problem
(MaxOnes($\Gamma$)) and the Boolean valued constraint satisfaction problem over
finite-valued constraint languages (VCSP($\Delta$)). With the help of these
languages we relate MaxOnes and VCSP to the exponential time hypothesis in
several different ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3252</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3252</id><created>2014-06-12</created><authors><author><keyname>Egarter</keyname><forenames>Dominik</forenames></author><author><keyname>Monacchi</keyname><forenames>Andrea</forenames></author><author><keyname>Khatib</keyname><forenames>Tamer</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>Integration of Legacy Appliances into Home Energy Management Systems</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The progressive installation of renewable energy sources requires the
coordination of energy consuming devices. At consumer level, this coordination
can be done by a home energy management system (HEMS). Interoperability issues
need to be solved among smart appliances as well as between smart and
non-smart, i.e., legacy devices. We expect current standardization efforts to
soon provide technologies to design smart appliances in order to cope with the
current interoperability issues. Nevertheless, common electrical devices affect
energy consumption significantly and therefore deserve consideration within
energy management applications. This paper discusses the integration of smart
and legacy devices into a generic system architecture and, subsequently,
elaborates the requirements and components which are necessary to realize such
an architecture including an application of load detection for the
identification of running loads and their integration into existing HEM
systems. We assess the feasibility of such an approach with a case study based
on a measurement campaign on real households. We show how the information of
detected appliances can be extracted in order to create device profiles
allowing for their integration and management within a HEMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3266</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3266</id><created>2014-06-12</created><authors><author><keyname>Fanaee-T</keyname><forenames>Hadi</forenames></author><author><keyname>Oliveira</keyname><forenames>M&#xe1;rcia D. B.</forenames></author><author><keyname>Gama</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Malinowski</keyname><forenames>Simon</forenames></author><author><keyname>Morla</keyname><forenames>Ricardo</forenames></author></authors><title>Event and Anomaly Detection Using Tucker3 Decomposition</title><categories>cs.AI</categories><journal-ref>In Proceedings of 20th European Conference on Artificial
  Intelligence (ECAI'2013)- Ubiquitous Data Mining Workshop, pp. 8-12, vol. 1,
  August 27-31, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Failure detection in telecommunication networks is a vital task. So far,
several supervised and unsupervised solutions have been provided for
discovering failures in such networks. Among them unsupervised approaches has
attracted more attention since no label data is required. Often, network
devices are not able to provide information about the type of failure. In such
cases the type of failure is not known in advance and the unsupervised setting
is more appropriate for diagnosis. Among unsupervised approaches, Principal
Component Analysis (PCA) is a well-known solution which has been widely used in
the anomaly detection literature and can be applied to matrix data (e.g.
Users-Features). However, one of the important properties of network data is
their temporal sequential nature. So considering the interaction of dimensions
over a third dimension, such as time, may provide us better insights into the
nature of network failures. In this paper we demonstrate the power of three-way
analysis to detect events and anomalies in time-evolving network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3269</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3269</id><created>2014-06-12</created><updated>2015-04-10</updated><authors><author><keyname>Geras</keyname><forenames>Krzysztof J.</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>Scheduled denoising autoencoders</title><categories>cs.LG stat.ML</categories><comments>Published as a conference paper at ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a representation learning method that learns features at multiple
different levels of scale. Working within the unsupervised framework of
denoising autoencoders, we observe that when the input is heavily corrupted
during training, the network tends to learn coarse-grained features, whereas
when the input is only slightly corrupted, the network tends to learn
fine-grained features. This motivates the scheduled denoising autoencoder,
which starts with a high level of noise that lowers as training progresses. We
find that the resulting representation yields a significant boost on a later
supervised task compared to the original input, or to a standard denoising
autoencoder trained at a single noise level. After supervised fine-tuning our
best model achieves the lowest ever reported error on the CIFAR-10 data set
among permutation-invariant methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3270</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3270</id><created>2014-01-16</created><authors><author><keyname>Geist</keyname><forenames>Matthieu</forenames></author><author><keyname>Pietquin</keyname><forenames>Olivier</forenames></author></authors><title>Kalman Temporal Differences</title><categories>cs.LG</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 39, pages
  483-532, 2010</journal-ref><doi>10.1613/jair.3077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because reinforcement learning suffers from a lack of scalability, online
value (and Q-) function approximation has received increasing interest this
last decade. This contribution introduces a novel approximation scheme, namely
the Kalman Temporal Differences (KTD) framework, that exhibits the following
features: sample-efficiency, non-linear approximation, non-stationarity
handling and uncertainty management. A first KTD-based algorithm is provided
for deterministic Markov Decision Processes (MDP) which produces biased
estimates in the case of stochastic transitions. Than the eXtended KTD
framework (XKTD), solving stochastic MDP, is described. Convergence is analyzed
for special cases for both deterministic and stochastic transitions. Related
algorithms are experimented on classical benchmarks. They compare favorably to
the state of the art while exhibiting the announced features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3277</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3277</id><created>2014-06-12</created><authors><author><keyname>Fanaee-T</keyname><forenames>Hadi</forenames></author><author><keyname>Yazdi</keyname><forenames>Mehran</forenames></author></authors><title>A Semantic VSM-Based Recommender System</title><categories>cs.IR</categories><journal-ref>International Journal of Computer Theory and Engineering vol. 5,
  no. 2, pp. 331-336, 2013</journal-ref><doi>10.7763/IJCTE.2013.V5.704</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online forums enable users to discuss together around various topics. One of
the serious problems of these environments is high volume of discussions and
thus information overload problem. Unfortunately without considering the users
interests, traditional Information Retrieval (IR) techniques are not able to
solve the problem. Therefore, employment of a Recommender System (RS) that
could suggest favorite's topics of users according to their tastes could
increases the dynamism of forum and prevent the users from duplicate posts. In
addition, consideration of semantics can be useful for increasing the
performance of IR based RS. Our goal is study of impact of ontology and data
mining techniques on improving of content-based RS. For this purpose, at first,
three type of ontologies will be constructed from the domain corpus with
utilization of text mining, Natural Language Processing (NLP) and Wordnet and
then they will be used as an input in two kind of RS: one, fully ontology-based
and one with enriching the user profile vector with ontology in vector space
model (VSM) (proposed method). Afterward the results will be compared with the
simple VSM based RS. Given results show that the proposed RS presents the
highest performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3278</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3278</id><created>2014-06-12</created><updated>2014-06-23</updated><authors><author><keyname>Yao</keyname><forenames>Andrew Chi-Chih</forenames></author></authors><title>An n-to-1 Bidder Reduction for Multi-item Auctions and its Applications</title><categories>cs.GT</categories><comments>Minor changes and corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel approach for reducing the $k$-item
$n$-bidder auction with additive valuation to $k$-item $1$-bidder auctions.
This approach, called the \emph{Best-Guess} reduction, can be applied to
address several central questions in optimal revenue auction theory such as the
power of randomization, and Bayesian versus dominant-strategy implementations.
First, when the items have independent valuation distributions, we present a
deterministic mechanism called {\it Deterministic Best-Guess} that yields at
least a constant fraction of the optimal revenue by any randomized mechanism.
Second, if all the $nk$ valuation random variables are independent, the optimal
revenue achievable in {\it dominant strategy incentive compatibility} (DSIC) is
shown to be at least a constant fraction of that achievable in {\it Bayesian
incentive compatibility} (BIC). Third, when all the $nk$ values are identically
distributed according to a common one-dimensional distribution $F$, the optimal
revenue is shown to be expressible in the closed form $\Theta(k(r+\int_0^{mr}
(1-F(x)^n) \ud x))$ where $r= sup_{x\geq 0} \, x(1 - F(x)^n)$ and $m=\lceil
k/n\rceil$; this revenue is achievable by a simple mechanism called
\emph{2nd-Price Bundling}. All our results apply to arbitrary distributions,
regular or irregular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3279</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3279</id><created>2014-06-12</created><authors><author><keyname>Jacob</keyname><forenames>Riko</forenames></author><author><keyname>Lieber</keyname><forenames>Tobias</forenames></author><author><keyname>Sitchinava</keyname><forenames>Nodari</forenames></author></authors><title>On the Complexity of List Ranking in the Parallel External Memory Model</title><categories>cs.DS</categories><doi>10.1007/978-3-662-44465-8_33</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of list ranking in the parallel external memory (PEM)
model. We observe an interesting dual nature for the hardness of the problem
due to limited information exchange among the processors about the structure of
the list, on the one hand, and its close relationship to the problem of
permuting data, which is known to be hard for the external memory models, on
the other hand.
  By carefully defining the power of the computational model, we prove a
permuting lower bound in the PEM model. Furthermore, we present a stronger
\Omega(log^2 N) lower bound for a special variant of the problem and for a
specific range of the model parameters, which takes us a step closer toward
proving a non-trivial lower bound for the list ranking problem in the
bulk-synchronous parallel (BSP) and MapReduce models. Finally, we also present
an algorithm that is tight for a larger range of parameters of the model than
in prior work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3280</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3280</id><created>2014-06-12</created><updated>2016-02-03</updated><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author><author><keyname>Ponse</keyname><forenames>Alban</forenames></author></authors><title>Three Datatype Defining Rewrite Systems for Datatypes of Integers each
  extending a Datatype of Naturals</title><categories>cs.LO cs.DS</categories><comments>22 pages, 18 tables. Version 3: Some new non-confluence and
  termination results recorded in [12] are mentioned and some errors recorded
  in [12] are corrected; ground-completeness of Z_{bud} (Tables 7 and 8, pages
  7, 8) is proven; a DDRS for the ring of Integers is added (Table 18, pages
  18, 19) and its ground-completeness is proven</comments><report-no>TCS1409v3</report-no><acm-class>D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integer arithmetic is specified according to three views: unary, binary, and
decimal notation. In each case we find a ground confluent and terminating
datatype defining rewrite system. In each case the resulting datatype is a
canonical term algebra which extends a corresponding canonical term algebra for
natural numbers. For each view, we also consider an alternative rewrite system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3282</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3282</id><created>2014-06-12</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Cienfuegos</keyname><forenames>Miguel</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author></authors><title>A swarm optimization algorithm inspired in the behavior of the
  social-spider</title><categories>cs.NE</categories><comments>21 Pages</comments><journal-ref>Expert Systems with Applications, 40 (16), (2013), pp. 6374-6384</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm intelligence is a research field that models the collective behavior in
swarms of insects or animals. Several algorithms arising from such models have
been proposed to solve a wide range of complex optimization problems. In this
paper, a novel swarm algorithm called the Social Spider Optimization (SSO) is
proposed for solving optimization tasks. The SSO algorithm is based on the
simulation of cooperative behavior of social-spiders. In the proposed
algorithm, individuals emulate a group of spiders which interact to each other
based on the biological laws of the cooperative colony. The algorithm considers
two different search agents (spiders): males and females. Depending on gender,
each individual is conducted by a set of different evolutionary operators which
mimic different cooperative behaviors that are typically found in the colony.
In order to illustrate the proficiency and robustness of the proposed approach,
it is compared to other well-known evolutionary methods. The comparison
examines several standard benchmark functions that are commonly considered
within the literature of evolutionary algorithms. The outcome shows a high
performance of the proposed method for searching a global optimum with several
benchmark functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3284</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3284</id><created>2014-06-12</created><authors><author><keyname>Cadieu</keyname><forenames>Charles F.</forenames></author><author><keyname>Hong</keyname><forenames>Ha</forenames></author><author><keyname>Yamins</keyname><forenames>Daniel L. K.</forenames></author><author><keyname>Pinto</keyname><forenames>Nicolas</forenames></author><author><keyname>Ardila</keyname><forenames>Diego</forenames></author><author><keyname>Solomon</keyname><forenames>Ethan A.</forenames></author><author><keyname>Majaj</keyname><forenames>Najib J.</forenames></author><author><keyname>DiCarlo</keyname><forenames>James J.</forenames></author></authors><title>Deep Neural Networks Rival the Representation of Primate IT Cortex for
  Core Visual Object Recognition</title><categories>q-bio.NC cs.NE</categories><comments>35 pages, 12 figures, extends and expands upon arXiv:1301.3530</comments><doi>10.1371/journal.pcbi.1003963</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primate visual system achieves remarkable visual object recognition
performance even in brief presentations and under changes to object exemplar,
geometric transformations, and background variation (a.k.a. core visual object
recognition). This remarkable performance is mediated by the representation
formed in inferior temporal (IT) cortex. In parallel, recent advances in
machine learning have led to ever higher performing models of object
recognition using artificial deep neural networks (DNNs). It remains unclear,
however, whether the representational performance of DNNs rivals that of the
brain. To accurately produce such a comparison, a major difficulty has been a
unifying metric that accounts for experimental limitations such as the amount
of noise, the number of neural recording sites, and the number trials, and
computational limitations such as the complexity of the decoding classifier and
the number of classifier training examples. In this work we perform a direct
comparison that corrects for these experimental limitations and computational
considerations. As part of our methodology, we propose an extension of &quot;kernel
analysis&quot; that measures the generalization accuracy as a function of
representational complexity. Our evaluations show that, unlike previous
bio-inspired models, the latest DNNs rival the representational performance of
IT cortex on this visual object recognition task. Furthermore, we show that
models that perform well on measures of representational performance also
perform well on measures of representational similarity to IT and on measures
of predicting individual IT multi-unit responses. Whether these DNNs rely on
computational mechanisms similar to the primate visual system is yet to be
determined, but, unlike all previous bio-inspired models, that possibility
cannot be ruled out merely on representational performance grounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3287</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3287</id><created>2014-06-12</created><updated>2015-02-18</updated><authors><author><keyname>Mayo</keyname><forenames>Matthew</forenames></author></authors><title>A Clustering Analysis of Tweet Length and its Relation to Sentiment</title><categories>cs.CL cs.IR cs.SI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis of Twitter data is performed. The researcher has made the
following contributions via this paper: (1) an innovative method for deriving
sentiment score dictionaries using an existing sentiment dictionary as seed
words is explored, and (2) an analysis of clustered tweet sentiment scores
based on tweet length is performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3289</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3289</id><created>2014-06-12</created><authors><author><keyname>Christodoulakis</keyname><forenames>Manolis</forenames></author><author><keyname>Ryan</keyname><forenames>P. J.</forenames></author><author><keyname>Smyth</keyname><forenames>W. F.</forenames></author><author><keyname>Wang</keyname><forenames>Shu</forenames></author></authors><title>Indeterminate Strings, Prefix Arrays &amp; Undirected Graphs</title><categories>cs.DM math.CO</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An integer array y = y[1..n] is said to be feasible if and only if y[1] = n
and, for every i \in 2..n, i \le i+y[i] \le n+1. A string is said to be
indeterminate if and only if at least one of its elements is a subset of
cardinality greater than one of a given alphabet Sigma; otherwise it is said to
be regular. A feasible array y is said to be regular if and only if it is the
prefix array of some regular string. We show using a graph model that every
feasible array of integers is a prefix array of some (indeterminate or regular)
string, and for regular strings corresponding to y, we use the model to provide
a lower bound on the alphabet size. We show further that there is a 1-1
correspondence between labelled simple graphs and indeterminate strings, and we
show how to determine the minimum alphabet size |Sigma| of an indeterminate
string x based on its associated graph Gx. Thus, in this sense, indeterminate
strings are a more natural object of combinatorial interest than the strings on
elements of Sigma that have traditionally been studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3295</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3295</id><created>2014-05-22</created><updated>2014-06-30</updated><authors><author><keyname>Caiafa</keyname><forenames>Cesar F.</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Stable, Robust and Super Fast Reconstruction of Tensors Using Multi-Way
  Projections</title><categories>cs.IT cs.DS math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2014.2385040</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of multidimensional Compressed Sensing (CS), we introduce an
analytical reconstruction formula that allows one to recover an $N$th-order
$(I_1\times I_2\times \cdots \times I_N)$ data tensor $\underline{\mathbf{X}}$
from a reduced set of multi-way compressive measurements by exploiting its low
multilinear-rank structure. Moreover, we show that, an interesting property of
multi-way measurements allows us to build the reconstruction based on
compressive linear measurements taken only in two selected modes, independently
of the tensor order $N$. In addition, it is proved that, in the matrix case and
in a particular case with $3$rd-order tensors where the same 2D sensor operator
is applied to all mode-3 slices, the proposed reconstruction
$\underline{\mathbf{X}}_\tau$ is stable in the sense that the approximation
error is comparable to the one provided by the best low-multilinear-rank
approximation, where $\tau$ is a threshold parameter that controls the
approximation error. Through the analysis of the upper bound of the
approximation error we show that, in the 2D case, an optimal value for the
threshold parameter $\tau=\tau_0 &gt; 0$ exists, which is confirmed by our
simulation results. On the other hand, our experiments on 3D datasets show that
very good reconstructions are obtained using $\tau=0$, which means that this
parameter does not need to be tuned. Our extensive simulation results
demonstrate the stability and robustness of the method when it is applied to
real-world 2D and 3D signals. A comparison with state-of-the-arts sparsity
based CS methods specialized for multidimensional signals is also included. A
very attractive characteristic of the proposed method is that it provides a
direct computation, i.e. it is non-iterative in contrast to all existing
sparsity based CS algorithms, thus providing super fast computations, even for
large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3296</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3296</id><created>2014-06-12</created><authors><author><keyname>Wei</keyname><forenames>Hongchuan</forenames></author><author><keyname>Lu</keyname><forenames>Wenjie</forenames></author><author><keyname>Ferrari</keyname><forenames>Silvia</forenames></author></authors><title>An Information Value Function for Nonparametric Gaussian Processes</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures, 2012 NIPS workshop on Bayesian Nonparametric
  Models For Reliable Planning And Decision-Making Under Uncertainty</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel information value function that can be used in
online sensor planning to monitor a spatial phenomenon in which the spatial
phenomenon is modeled by nonparametric Gaussian processes. The information
value function is derived from the Kullback-Leibler (KL) divergence and
represents the information value brought by sensor decision. The sensor
decision at every time step is to select the sensing location that maximizes
the information value function associated with the measurement taken. Gaussian
processes (GPs) are employed to obtain the posterior distribution of the
spatial phenomenon given a number of sensor measurements, because GPs have
sufficient flexibilities to adopt the complexity from data. Furthermore, a
greedy algorithm is designed based on the information value function. By
comparing the greedy algorithm with the random algorithm, it is shown that the
error decreases faster defined as the difference between the estimated
posterior distribution and the true distribution of the spatial phenomenon via
the greedy algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3313</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3313</id><created>2014-06-12</created><authors><author><keyname>Donaldson</keyname><forenames>Alastair F.</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Vasconcelos</keyname><forenames>Vasco T.</forenames><affiliation>University of Lisbon</affiliation></author></authors><title>Proceedings 7th Workshop on Programming Language Approaches to
  Concurrency and Communication-cEntric Software</title><categories>cs.DC cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 155, 2014</journal-ref><doi>10.4204/EPTCS.155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the post-proceedings of PLACES 2014, the seventh
Workshop on Programming Language Approaches to Concurrency and
Communication-cEntric Software, which was held in Grenoble, France, on April
12th 2014, and co-located with ETAPS, the European Joint Conferences on Theory
and Practice of Software. The PLACES workshop series aims to offer a forum
where researchers from different fields exchange new ideas on one of the
central challenges for programming in the near future: the development of
programming languages, methodologies and infrastructures where concurrency and
distribution are the norm rather than a marginal concern. Previous editions of
PLACES were held in Rome (2013), Tallin (2012), Saarbrueken (2011), Paphos
(2010) and York (2009), all co-located with ETAPS, and the first PLACES was
held in Oslo and co-located with DisCoTec (2008).
  The Program Committee, after a careful and thorough reviewing process,
selected nine papers out of 12 submissions for presentation at the workshop and
inclusion in this post-proceedings. Each submission was evaluated by three
referees (with one paper receiving a fourth review), and the accepted papers
were selected during a week-long electronic discussion. One of the nine
accepted papers was conditionally accepted subject to a process of shepherding
by a PC member, which was successful and led to the paper's full acceptance.
  In addition to the contributed papers, the workshop will feature an invited
talk by Akash Lal, Microsoft Research India, entitled &quot;Finding Concurrency Bugs
Under Imprecise Harnesses&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3327</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3327</id><created>2014-06-12</created><authors><author><keyname>Mennle</keyname><forenames>Timo</forenames></author><author><keyname>Seuken</keyname><forenames>Sven</forenames></author></authors><title>The Naive versus the Adaptive Boston Mechanism</title><categories>cs.GT</categories><comments>Working Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Boston mechanism is often criticized for its manipulability and the
resulting negative implications for welfare and fairness. Nonetheless, it is
one of the most popular school choice mechanisms used in practice. In this
paper, we first study the traditional (naive) Boston mechanism (NBM) in a
setting with no priority structure and single uniform tie-breaking. We show
that it imperfectly rank dominates the strategyproof Random Serial Dictatorship
(RSD). We then formalize an adaptive variant of the Boston mechanism (ABM),
which is also sometimes used in practice. We show that ABM has significantly
better incentive properties than NBM (it is partially strategyproof), while it
shares most of NBM's efficiency advantages over RSD as markets get large.
However, while a direct efficiency comparison of NBM and ABM via imperfect
dominance is inconclusive, numerical simulations suggest that NBM is still
somewhat more efficient than ABM, which can be interpreted as the cost of
partial strategyproofness one pays when choosing ABM over NBM. Our results
highlight the subtle trade-off between efficiency and strategyproofness a
market designer must make when choosing between the two Boston mechanisms and
RSD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3332</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3332</id><created>2014-06-12</created><updated>2014-11-14</updated><authors><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author><author><keyname>Koniusz</keyname><forenames>Piotr</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author></authors><title>Convolutional Kernel Networks</title><categories>cs.CV cs.LG stat.ML</categories><comments>appears in Advances in Neural Information Processing Systems (NIPS),
  Dec 2014, Montreal, Canada, http://nips.cc</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important goal in visual recognition is to devise image representations
that are invariant to particular transformations. In this paper, we address
this goal with a new type of convolutional neural network (CNN) whose
invariance is encoded by a reproducing kernel. Unlike traditional approaches
where neural networks are learned either to represent data or for solving a
classification task, our network learns to approximate the kernel feature map
on training data. Such an approach enjoys several benefits over classical ones.
First, by teaching CNNs to be invariant, we obtain simple network architectures
that achieve a similar accuracy to more complex ones, while being easy to train
and robust to overfitting. Second, we bridge a gap between the neural network
literature and kernels, which are natural tools to model invariance. We
evaluate our methodology on visual recognition tasks where CNNs have proven to
perform well, e.g., digit recognition with the MNIST dataset, and the more
challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive
with the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3337</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3337</id><created>2014-06-12</created><updated>2014-07-10</updated><authors><author><keyname>Moore</keyname><forenames>Jared</forenames></author><author><keyname>Clark</keyname><forenames>Anthony</forenames></author><author><keyname>McKinley</keyname><forenames>Philip</forenames></author></authors><title>Evolutionary Robotics on the Web with WebGL and Javascript</title><categories>cs.NE cs.HC</categories><comments>Presented at WebAL-1: Workshop on Artificial Life and the Web 2014
  (arXiv:1406.2507)</comments><report-no>WebAL1/2014/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web-based applications are highly accessible to users, providing rich,
interactive content while eliminating the need to install software locally.
However, evolutionary robotics (ER) has faced challenges in this domain as
web-based technologies have not been amenable to 3D physics simulations.
Traditionally, physics-based simulations require a local installation and a
high degree of user knowledge to configure an environment, but the emergence of
Javascript-based physics engines enables complex simulations to be executed in
web browsers. These developments create opportunities for ER research to reach
new audiences by increasing accessibility. In this work, we introduce two
web-based tools we have built to facilitate the exchange of ideas with other
researchers as well as outreach to K-12 students and the general public. The
first tool is intended to distribute and exchange ER research results, while
the second is a completely browser-based implementation of an ER environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3339</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3339</id><created>2014-06-12</created><updated>2014-07-10</updated><authors><author><keyname>Chow</keyname><forenames>Yinlam</forenames></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames></author></authors><title>Algorithms for CVaR Optimization in MDPs</title><categories>cs.AI math.OC</categories><comments>Submitted to NIPS 14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many sequential decision-making problems we may want to manage risk by
minimizing some measure of variability in costs in addition to minimizing a
standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk
measure that addresses some of the shortcomings of the well-known
variance-related risk measures, and because of its computational efficiencies
has gained popularity in finance and operations research. In this paper, we
consider the mean-CVaR optimization problem in MDPs. We first derive a formula
for computing the gradient of this risk-sensitive objective function. We then
devise policy gradient and actor-critic algorithms that each uses a specific
method to estimate this gradient and updates the policy parameters in the
descent direction. We establish the convergence of our algorithms to locally
risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our
algorithms in an optimal stopping problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3368</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3368</id><created>2014-06-12</created><updated>2014-06-17</updated><authors><author><keyname>Huang</keyname><forenames>Yu-Chih</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author></authors><title>Lattices from Codes for Harnessing Interference: An Overview and
  Generalizations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, using compute-and-forward as an example, we provide an
overview of constructions of lattices from codes that possess the right
algebraic structures for harnessing interference. This includes Construction A,
Construction D, and Construction $\pi_A$ (previously called product
construction) recently proposed by the authors. We then discuss two
generalizations where the first one is a general construction of lattices named
Construction $\pi_D$ subsuming the above three constructions as special cases
and the second one is to go beyond principal ideal domains and build lattices
over algebraic integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3373</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3373</id><created>2014-06-12</created><updated>2014-06-17</updated><authors><author><keyname>Smith</keyname><forenames>Tim</forenames></author></authors><title>On Infinite Words Determined by Indexed Languages</title><categories>cs.FL</categories><comments>Full version of paper accepted for publication at MFCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the infinite words determined by indexed languages. An
infinite language $L$ determines an infinite word $\alpha$ if every string in
$L$ is a prefix of $\alpha$. If $L$ is regular or context-free, it is known
that $\alpha$ must be ultimately periodic. We show that if $L$ is an indexed
language, then $\alpha$ is a morphic word, i.e., $\alpha$ can be generated by
iterating a morphism under a coding. Since the other direction, that every
morphic word is determined by some indexed language, also holds, this implies
that the infinite words determined by indexed languages are exactly the morphic
words. To obtain this result, we prove a new pumping lemma for the indexed
languages, which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3378</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3378</id><created>2014-06-12</created><updated>2014-06-25</updated><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Zuppiroli</keyname><forenames>Sara</forenames></author></authors><title>Probabilistic Recursion Theory and Implicit Computational Complexity
  (Long Version)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that probabilistic computable functions, i.e., those functions
outputting distributions and computed by probabilistic Turing machines, can be
characterized by a natural generalization of Church and Kleene's partial
recursive functions. The obtained algebra, following Leivant, can be restricted
so as to capture the notion of polytime sampleable distributions, a key concept
in average-case complexity and cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3387</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3387</id><created>2014-06-12</created><updated>2015-03-26</updated><authors><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author><author><keyname>Yan</keyname><forenames>Xiaoran</forenames></author></authors><title>The Interplay Between Dynamics and Networks: Centrality, Communities,
  and Cheeger Inequality</title><categories>cs.SI cs.DM physics.soc-ph</categories><comments>This is the full length version. A shorter conference paper appeared
  in ACM-SIGKDD 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the interplay between a dynamic process and the structure of the
network on which it is defined. Specifically, we examine the impact of this
interaction on the quality-measure of network clusters and node centrality.
This enables us to effectively identify network communities and important nodes
participating in the dynamics. As the first step towards this objective, we
introduce an umbrella framework for defining and characterizing an ensemble of
dynamic processes on a network. This framework generalizes the traditional
Laplacian framework to continuous-time biased random walks and also allows us
to model some epidemic processes over a network. For each dynamic process in
our framework, we can define a function that measures the quality of every
subset of nodes as a potential cluster (or community) with respect to this
process on a given network. This subset-quality function generalizes the
traditional conductance measure for graph partitioning. We partially justify
our choice of the quality function by showing that the classic Cheeger's
inequality, which relates the conductance of the best cluster in a network with
a spectral quantity of its Laplacian matrix, can be extended from the
Laplacian-conductance setting to this more general setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3395</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3395</id><created>2014-06-12</created><authors><author><keyname>Lekeas</keyname><forenames>Paraskevas V.</forenames></author></authors><title>An Evolutionary Approach to Coalition Formation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Cooperative Games with Externalities when the members of a set S \subset N
of agents wish to deviate they need to calculate their worth. This worth
depends on what the non-members (outsiders) N \setminus S will do, which in
turn depends on which coalition structure the outsiders will form. Since this
coalition formation problem is NP-hard, various approaches have been adopted.
In this paper using an evolutionary game theoretic approach we provide a set of
equations that can help agents in S reason about the coalition structures the
outsiders may form in terms of minimum distances on an n-s dimensional space,
where n=|N|, s=|S|.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3399</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3399</id><created>2014-06-12</created><authors><author><keyname>Hartig</keyname><forenames>Olaf</forenames></author><author><keyname>Thompson</keyname><forenames>Bryan</forenames></author></authors><title>Foundations of an Alternative Approach to Reification in RDF</title><categories>cs.DB</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document defines extensions of the RDF data model and of the SPARQL
query language that capture an alternative approach to represent
statement-level metadata. While this alternative approach is backwards
compatible with RDF reification as defined by the RDF standard, the approach
aims to address usability and data management shortcomings of RDF reification.
One of the great advantages of the proposed approach is that it clarifies a
means to (i) understand sparse matrices, the property graph model, hypergraphs,
and other data structures with an emphasis on link attributes, (ii) map such
data onto RDF, and (iii) query such data using SPARQL. Further, the proposal
greatly expands both the freedom that database designers enjoy when creating
physical indexing schemes and query plans for graph data annotated with link
attributes and the interoperability of those database solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3400</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3400</id><created>2014-06-12</created><authors><author><keyname>Jokic</keyname><forenames>Sasa</forenames></author><author><keyname>Novikov</keyname><forenames>Petr</forenames></author><author><keyname>Maggs</keyname><forenames>Stuart</forenames></author><author><keyname>Sadan</keyname><forenames>Dori</forenames></author><author><keyname>Jin</keyname><forenames>Shihui</forenames></author><author><keyname>Nan</keyname><forenames>Cristina</forenames></author></authors><title>Robotic positioning device for three-dimensional printing</title><categories>cs.RO</categories><comments>14 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Additive manufacturing brings a variety of new possibilities to the
construction industry, extending the capabilities of existing fabrication
methods whilst also creating new possibilities. Currently three-dimensional
printing is used to produce small-scale objects; large-scale three-dimensional
printing is difficult due to the size of positioning devices and machine
elements. Presently fixed Computer Numerically Controlled (CNC) routers and
robotic arms are used to position print-heads. Fixed machines have work
envelope limitations and can't produce objects outside of these limits.
Large-scale three-dimensional printing requires large machines that are costly
to build and hard to transport. This paper presents a compact print-head
positioning device for Fused Deposition Modeling (FDM) a method of
three-dimensional printing independent from the size of the object it prints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3404</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3404</id><created>2014-06-12</created><authors><author><keyname>So</keyname><forenames>Jungho</forenames></author><author><keyname>Kim</keyname><forenames>Donggun</forenames></author><author><keyname>Lee</keyname><forenames>Yuni</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author></authors><title>Pilot Signal Design for Massive MIMO Systems: A Received
  Signal-To-Noise-Ratio-Based Approach</title><categories>cs.IT math.IT</categories><comments>5 pages, double column, 1 figure. Submitted to IEEE Signal Processing
  Letters</comments><doi>10.1109/LSP.2014.2364180</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the pilot signal design for massive MIMO systems to maximize
the training-based received signal-to-noise ratio (SNR) is considered under two
channel models: block Gauss-Markov and block independent and identically
distributed (i.i.d.) channel models. First, it is shown that under the block
Gauss-Markov channel model, the optimal pilot design problem reduces to a
semi-definite programming (SDP) problem, which can be solved numerically by a
standard convex optimization tool. Second, under the block i.i.d. channel
model, an optimal solution is obtained in closed form. Numerical results show
that the proposed method yields noticeably better performance than other
existing pilot design methods in terms of received SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3405</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3405</id><created>2014-06-12</created><authors><author><keyname>Rajasekaran</keyname><forenames>Sanguthevar</forenames></author><author><keyname>Nicolae</keyname><forenames>Marius</forenames></author></authors><title>An error correcting parser for context free grammars that takes less
  than cubic time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of parsing has been studied extensively for various formal
grammars. Given an input string and a grammar, the parsing problem is to check
if the input string belongs to the language generated by the grammar. A closely
related problem of great importance is one where the input are a string ${\cal
I}$ and a grammar $G$ and the task is to produce a string ${\cal I}'$ that
belongs to the language generated by $G$ and the `distance' between ${\cal I}$
and ${\cal I}'$ is the smallest (from among all the strings in the language).
Specifically, if ${\cal I}$ is in the language generated by $G$, then the
output should be ${\cal I}$. Any parser that solves this version of the problem
is called an {\em error correcting parser}. In 1972 Aho and Peterson presented
a cubic time error correcting parser for context free grammars. Since then this
asymptotic time bound has not been improved under the (standard) assumption
that the grammar size is a constant. In this paper we present an error
correcting parser for context free grammars that runs in $O(T(n))$ time, where
$n$ is the length of the input string and $T(n)$ is the time needed to compute
the tropical product of two $n\times n$ matrices.
  In this paper we also present an $\frac{n}{M}$-approximation algorithm for
the {\em language edit distance problem} that has a run time of $O(Mn^\omega)$,
where $O(n^\omega)$ is the time taken to multiply two $n\times n$ matrices. To
the best of our knowledge, no approximation algorithms have been proposed for
error correcting parsing for general context free grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3407</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3407</id><created>2014-06-12</created><updated>2015-04-20</updated><authors><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Srihari</keyname><forenames>Sargur H.</forenames></author></authors><title>Restricted Boltzmann Machine for Classification with Hierarchical
  Correlated Prior</title><categories>cs.LG</categories><comments>13 pages, 5 figures</comments><msc-class>68T10</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Restricted Boltzmann machines (RBM) and its variants have become hot research
topics recently, and widely applied to many classification problems, such as
character recognition and document categorization. Often, classification RBM
ignores the interclass relationship or prior knowledge of sharing information
among classes. In this paper, we are interested in RBM with the hierarchical
prior over classes. We assume parameters for nearby nodes are correlated in the
hierarchical tree, and further the parameters at each node of the tree be
orthogonal to those at its ancestors. We propose a hierarchical correlated RBM
for classification problem, which generalizes the classification RBM with
sharing information among different classes. In order to reduce the redundancy
between node parameters in the hierarchy, we also introduce orthogonal
restrictions to our objective function. We test our method on challenge
datasets, and show promising results compared to competitive baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3411</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3411</id><created>2014-06-12</created><authors><author><keyname>Koutra</keyname><forenames>Danai</forenames></author><author><keyname>Kang</keyname><forenames>U</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>VoG: Summarizing and Understanding Large Graphs</title><categories>cs.SI physics.soc-ph</categories><comments>SIAM International Conference on Data Mining (SDM) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we succinctly describe a million-node graph with a few simple
sentences? How can we measure the &quot;importance&quot; of a set of discovered subgraphs
in a large graph? These are exactly the problems we focus on. Our main ideas
are to construct a &quot;vocabulary&quot; of subgraph-types that often occur in real
graphs (e.g., stars, cliques, chains), and from a set of subgraphs, find the
most succinct description of a graph in terms of this vocabulary. We measure
success in a well-founded way by means of the Minimum Description Length (MDL)
principle: a subgraph is included in the summary if it decreases the total
description length of the graph.
  Our contributions are three-fold: (a) formulation: we provide a principled
encoding scheme to choose vocabulary subgraphs; (b) algorithm: we develop
\method, an efficient method to minimize the description cost, and (c)
applicability: we report experimental results on multi-million-edge real
graphs, including Flickr and the Notre Dame web graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3412</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3412</id><created>2014-06-12</created><authors><author><keyname>Hua</keyname><forenames>Min</forenames></author><author><keyname>Yang</keyname><forenames>Kristo W.</forenames></author><author><keyname>Wang</keyname><forenames>M.</forenames></author><author><keyname>Zou</keyname><forenames>Kingsley J.</forenames></author></authors><title>Analysis of the Frequency Offset Effect on Zadoff-Chu Sequence Timing
  Performance</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zadoff-Chu (ZC) sequences have been used as synchronization sequences in
modern wireless communication systems, replacing the conventional pseudo-random
noise (PN) sequences due to their perfect autocorrelation properties. We first
study the ambiguity problem between a timing offset and a frequency offset when
a ZC sequence is used as a synchronization signal. We show how a frequency
offset can impair the timing property of a ZC sequence, causing irreducible
timing errors. An analytical framework is developed, which completely
characterizes a ZC sequence's timing behavior and its fundamental limitation as
a synchronization sequence in the presence of a frequency offset between the
transmitter and the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3414</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3414</id><created>2014-06-13</created><authors><author><keyname>Furer</keyname><forenames>Martin</forenames></author><author><keyname>Yu</keyname><forenames>Huiwen</forenames></author></authors><title>Space Saving by Dynamic Algebraization</title><categories>cs.DS</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic programming is widely used for exact computations based on tree
decompositions of graphs. However, the space complexity is usually exponential
in the treewidth. We study the problem of designing efficient dynamic
programming algorithm based on tree decompositions in polynomial space. We show
how to construct a tree decomposition and extend the algebraic techniques of
Lokshtanov and Nederlof such that the dynamic programming algorithm runs in
time $O^*(2^h)$, where $h$ is the maximum number of vertices in the union of
bags on the root to leaf paths on a given tree decomposition, which is a
parameter closely related to the tree-depth of a graph. We apply our algorithm
to the problem of counting perfect matchings on grids and show that it
outperforms other polynomial-space solutions. We also apply the algorithm to
other set covering and partitioning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3418</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3418</id><created>2014-06-13</created><authors><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author><author><keyname>Raheja</keyname><forenames>J. L.</forenames></author><author><keyname>Das</keyname><forenames>K.</forenames></author><author><keyname>Raheja</keyname><forenames>S.</forenames></author></authors><title>Fingers' Angle Calculation using Level-Set Method</title><categories>cs.CV</categories><comments>7 pages, IGI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current age, use of natural communication in human computer
interaction is a known and well installed thought. Hand gesture recognition and
gesture based applications has gained a significant amount of popularity
amongst people all over the world. It has a number of applications ranging from
security to entertainment. These applications generally are real time
applications and need fast, accurate communication with machines. On the other
end, gesture based communications have few limitations also like bent finger
information is not provided in vision based techniques. In this paper, a novel
method for fingertip detection and for angle calculation of both hands bent
fingers is discussed. Angle calculation has been done before with sensor based
gloves/devices. This study has been conducted in the context of natural
computing for calculating angles without using any wired equipment, colors,
marker or any device. The pre-processing and segmentation of the region of
interest is performed in a HSV color space and a binary format respectively.
Fingertips are detected using level-set method and angles were calculated using
geometrical analysis. This technique requires no training for system to perform
the task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3433</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3433</id><created>2014-06-13</created><authors><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Lakin</keyname><forenames>Matthew R.</forenames></author><author><keyname>Stefanovic</keyname><forenames>Darko</forenames></author><author><keyname>Teuscher</keyname><forenames>Christof</forenames></author></authors><title>A Model for Variation- and Fault-Tolerant Digital Logic using
  Self-Assembled Nanowire Architectures</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconfiguration has been used for both defect- and fault-tolerant nanoscale
architectures with regular structure. Recent advances in self-assembled
nanowires have opened doors to a new class of electronic devices with irregular
structure. For such devices, reservoir computing has been shown to be a viable
approach to implement computation. This approach exploits the dynamical
properties of a system rather than specifics of its structure. Here, we extend
a model of reservoir computing, called the echo state network, to reflect more
realistic aspects of self-assembled nanowire networks. As a proof of concept,
we use echo state networks to implement basic building blocks of digital
computing: AND, OR, and XOR gates, and 2-bit adder and multiplier circuits. We
show that the system can operate perfectly in the presence of variations five
orders of magnitude higher than ITRS's 2005 target, $\bm{6\%}$, and achieves
success rates $\bm{6}$ times higher than related approaches at half the cost.
We also describe an adaptive algorithm that can detect faults in the system and
reconfigure it to resume perfect operational condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3454</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3454</id><created>2014-06-13</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Maier</keyname><forenames>Henning</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>The Degrees-of-Freedom of Multi-way Device-to-Device Communications is
  Limited by 2</title><categories>cs.IT math.IT</categories><comments>5 pages, ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 3-user device-to-device (D2D) communications scenario is studied where each
user wants to send and receive a message from each other user. This scenario
resembles a 3-way communication channel. The capacity of this channel is
unknown in general. In this paper, a sum-capacity upper bound that
characterizes the degrees-of-freedom of the channel is derived by using
genie-aided arguments. It is further shown that the derived upper bound is
achievable within a gap of 2 bits, thus leading to an approximate sum-capacity
characterization for the 3-way channel. As a by-product, interesting analogies
between multi-way communications and multi-way relay communications are
concluded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3460</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3460</id><created>2014-06-13</created><authors><author><keyname>Suchowolec</keyname><forenames>Karolina</forenames></author></authors><title>Are Style Guides Controlled Languages? The Case of Koenig &amp; Bauer AG</title><categories>cs.CL</categories><comments>Fourth Workshop on Controlled Natural Language (CNL 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlled natural languages for industrial application are often regarded as
a response to the challenges of translation and multilingual communication.
This paper presents a quite different approach taken by Koenig &amp; Bauer AG,
where the main goal was the improvement of the authoring process for technical
documentation. Most importantly, this paper explores the notion of a controlled
language and demonstrates how style guides can emerge from non-linguistic
considerations. Moreover, it shows the transition from loose language
recommendations into precise and prescriptive rules and investigates whether
such rules can be regarded as a full-fledged controlled language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3461</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3461</id><created>2014-06-13</created><authors><author><keyname>Kalinovsky</keyname><forenames>Yakiv O.</forenames></author><author><keyname>Lande</keyname><forenames>Dmitry V.</forenames></author><author><keyname>Sc.</keyname><forenames>Dr.</forenames></author><author><keyname>Boyarinova</keyname><forenames>Yuliya E.</forenames></author><author><keyname>Turenko</keyname><forenames>Alina S.</forenames></author></authors><title>Clifford Type Algebra Characteristics Investigation</title><categories>cs.NA</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main properties of hypercomplex generalization of quaternion system as
antiquaternion are presented in this article. Definitions and studied of
antiquaternions conjugation are introduced, their norm and zero divisor, and
how to perform operations on them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3466</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3466</id><created>2014-06-13</created><authors><author><keyname>Astola</keyname><forenames>Helena</forenames></author><author><keyname>Tabus</keyname><forenames>Ioan</forenames></author></authors><title>On the Linear Programming Bound for Lee-codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the largest code with a given minimum distance is one of the most
basic problems in coding theory. In this paper, we study the linear programming
bound for codes in the Lee metric. We introduce refinements on the linear
programming bound for linear codes in the Lee metric, which give a tighter
bound for linear codes. We also discuss the computational aspects of the
problem and introduce a more compact program by the obtained refinements, and a
recursive method for computing the so-called Lee-numbers, which are important
in the computation of the linear programming bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3474</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3474</id><created>2014-06-13</created><authors><author><keyname>Li</keyname><forenames>Sijin</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Chan</keyname><forenames>Antoni B.</forenames></author></authors><title>Heterogeneous Multi-task Learning for Human Pose Estimation with Deep
  Convolutional Neural Network</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an heterogeneous multi-task learning framework for human pose
estimation from monocular image with deep convolutional neural network. In
particular, we simultaneously learn a pose-joint regressor and a sliding-window
body-part detector in a deep network architecture. We show that including the
body-part detection task helps to regularize the network, directing it to
converge to a good solution. We report competitive and state-of-art results on
several data sets. We also empirically show that the learned neurons in the
middle layer of our network are tuned to localized body parts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3478</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3478</id><created>2014-06-13</created><authors><author><keyname>Mostrous</keyname><forenames>Dimitris</forenames><affiliation>LaSIGE, Faculty of Sciences, University of Lisbon</affiliation></author></authors><title>Multiparty Sessions based on Proof Nets</title><categories>cs.LO cs.PL</categories><comments>In Proceedings PLACES 2014, arXiv:1406.3313</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 155, 2014, pp. 1-8</journal-ref><doi>10.4204/EPTCS.155.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We interpret Linear Logic Proof Nets in a term language based on Solos
calculus. The system includes a synchronisation mechanism, obtained by a
conservative extension of the logic, that enables to define non-deterministic
behaviours and multiparty sessions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3479</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3479</id><created>2014-06-13</created><authors><author><keyname>Lindley</keyname><forenames>Sam</forenames><affiliation>The University of Edinburgh</affiliation></author><author><keyname>Morris</keyname><forenames>J. Garrett</forenames><affiliation>The University of Edinburgh</affiliation></author></authors><title>Sessions as Propositions</title><categories>cs.PL cs.DC cs.LO</categories><comments>In Proceedings PLACES 2014, arXiv:1406.3313</comments><proxy>EPTCS</proxy><acm-class>D.3.2;F.3.2</acm-class><journal-ref>EPTCS 155, 2014, pp. 9-16</journal-ref><doi>10.4204/EPTCS.155.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Wadler presented a continuation-passing translation from a
session-typed functional language, GV, to a process calculus based on classical
linear logic, CP. However, this translation is one-way: CP is more expressive
than GV. We propose an extension of GV, called HGV, and give translations
showing that it is as expressive as CP. The new translations shed light both on
the original translation from GV to CP, and on the limitations in
expressiveness of GV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3480</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3480</id><created>2014-06-13</created><authors><author><keyname>Tiezzi</keyname><forenames>Francesco</forenames><affiliation>IMT Institute for Advanced Studies, Lucca, Italy</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College, London, U.K.</affiliation></author></authors><title>Towards Reversible Sessions</title><categories>cs.PL cs.LO cs.SE</categories><comments>In Proceedings PLACES 2014, arXiv:1406.3313</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 155, 2014, pp. 17-24</journal-ref><doi>10.4204/EPTCS.155.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we incorporate reversibility into structured
communication-based programming, to allow parties of a session to automatically
undo, in a rollback fashion, the effect of previously executed interactions.
This permits taking different computation paths along the same session, as well
as reverting the whole session and starting a new one. Our aim is to define a
theoretical basis for examining the interplay in concurrent systems between
reversible computation and session-based interaction. We thus enrich a
session-based variant of pi-calculus with memory devices, dedicated to keep
track of the computation history of sessions in order to reverse it. We discuss
our initial investigation concerning the definition of a session type
discipline for the proposed reversible calculus, and its practical advantages
for static verification of safe composition in communication-centric
distributed software performing reversible computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3481</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3481</id><created>2014-06-13</created><authors><author><keyname>Kouzapas</keyname><forenames>Dimitrios</forenames><affiliation>University of Glasgow</affiliation></author><author><keyname>Gutkovas</keyname><forenames>Ram&#x16b;nas</forenames><affiliation>Uppsala University</affiliation></author><author><keyname>Gay</keyname><forenames>Simon J.</forenames><affiliation>University of Glasgow</affiliation></author></authors><title>Session Types for Broadcasting</title><categories>cs.PL cs.LO</categories><comments>In Proceedings PLACES 2014, arXiv:1406.3313</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 155, 2014, pp. 25-31</journal-ref><doi>10.4204/EPTCS.155.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Up to now session types have been used under the assumptions of point to
point communication, to ensure the linearity of session endpoints, and reliable
communication, to ensure send/receive duality. In this paper we define a
session type theory for broadcast communication semantics that by definition do
not assume point to point and reliable communication. Our session framework
lies on top of the parametric framework of broadcasting psi-calculi, giving
insights on developing session types within a parametric framework. Our session
type theory enjoys the properties of soundness and safety. We further believe
that the solutions proposed will eventually provide a deeper understanding of
how session types principles should be applied in the general case of
communication semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3482</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3482</id><created>2014-06-13</created><authors><author><keyname>Neykova</keyname><forenames>Rumyana</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Multiparty Session Actors</title><categories>cs.DC cs.SE</categories><comments>In Proceedings PLACES 2014, arXiv:1406.3313</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 155, 2014, pp. 32-37</journal-ref><doi>10.4204/EPTCS.155.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actor coordination armoured with a suitable protocol description language has
been a pressing problem in the actors community. We study the applicability of
multiparty session type (MPST) protocols for verification of actor programs. We
incorporate sessions to actors by introducing minimum additions to the model
such as the notion of actor roles and protocol mailbox. The framework uses
Scribble, which is a protocol description language based on multiparty session
types. Our programming model supports actor-like syntax and runtime
verification mechanism guaranteeing type-safety and progress of the
communicating entities. An actor can implement multiple roles in a similar way
as an object can implement multiple interfaces. Multiple roles allow for
inter-concurrency in a single actor still preserving its progress property. We
demonstrate our framework by designing and implementing a session actor library
in Python and its runtime verification mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3483</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3483</id><created>2014-06-13</created><authors><author><keyname>Chen</keyname><forenames>Tzu-chun</forenames><affiliation>Dipartimento di Informatica, Universita' di Torino, Italy</affiliation></author></authors><title>Lightening Global Types</title><categories>cs.PL cs.DC</categories><comments>In Proceedings PLACES 2014, arXiv:1406.3313</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 155, 2014, pp. 38-46</journal-ref><doi>10.4204/EPTCS.155.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global session types prevent participants from waiting for never coming
messages. Some interactions take place just for the purpose of informing
receivers that some message will never arrive or the session is terminated. By
decomposing a big global type into several light global types, one can avoid
such kind of redundant interactions. Lightening global types gives us cleaner
global types, which keep all necessary communications. This work proposes a
framework which allows to easily decompose global types into light global
types, preserving the interaction sequences of the original ones but for
redundant interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3484</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3484</id><created>2014-06-13</created><authors><author><keyname>Blom</keyname><forenames>Stefan</forenames><affiliation>University of Twente</affiliation></author><author><keyname>Darabi</keyname><forenames>Saeed</forenames><affiliation>University of Twente</affiliation></author><author><keyname>Huisman</keyname><forenames>Marieke</forenames><affiliation>University of Twente</affiliation></author></authors><title>Verifying Parallel Loops with Separation Logic</title><categories>cs.SE cs.PL</categories><comments>In Proceedings PLACES 2014, arXiv:1406.3313</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 155, 2014, pp. 47-53</journal-ref><doi>10.4204/EPTCS.155.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a technique to specify and verify whether a loop can be
parallelised. Our approach can be used as an additional step in a parallelising
compiler to verify user annotations about loop dependences. Essentially, our
technique requires each loop iteration to be specified with the locations it
will read and write. From the loop iteration specifications, the loop
(in)dependences can be derived. Moreover, the loop iteration specifications
also reveal where synchronisation is needed in the parallelised program. The
loop iteration specifications can be verified using permission-based separation
logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3485</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3485</id><created>2014-06-13</created><authors><author><keyname>Swalens</keyname><forenames>Janwillem</forenames><affiliation>Vrije Universiteit Brussel</affiliation></author><author><keyname>Marr</keyname><forenames>Stefan</forenames><affiliation>Vrije Universiteit Brussel</affiliation></author><author><keyname>De Koster</keyname><forenames>Joeri</forenames><affiliation>Vrije Universiteit Brussel</affiliation></author><author><keyname>Van Cutsem</keyname><forenames>Tom</forenames><affiliation>Vrije Universiteit Brussel</affiliation></author></authors><title>Towards Composable Concurrency Abstractions</title><categories>cs.PL cs.DC cs.SE</categories><comments>In Proceedings PLACES 2014, arXiv:1406.3313</comments><proxy>EPTCS</proxy><acm-class>D.1.3; D.3; D.3.3</acm-class><journal-ref>EPTCS 155, 2014, pp. 54-60</journal-ref><doi>10.4204/EPTCS.155.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past decades, many different programming models for managing
concurrency in applications have been proposed, such as the actor model,
Communicating Sequential Processes, and Software Transactional Memory. The
ubiquity of multi-core processors has made harnessing concurrency even more
important. We observe that modern languages, such as Scala, Clojure, or F#,
provide not one, but multiple concurrency models that help developers manage
concurrency. Large end-user applications are rarely built using just a single
concurrency model. Programmers need to manage a responsive UI, deal with file
or network I/O, asynchronous workflows, and shared resources. Different
concurrency models facilitate different requirements. This raises the issue of
how these concurrency models interact, and whether they are composable. After
all, combining different concurrency models may lead to subtle bugs or
inconsistencies.
  In this paper, we perform an in-depth study of the concurrency abstractions
provided by the Clojure language. We study all pairwise combinations of the
abstractions, noting which ones compose without issues, and which do not. We
make an attempt to abstract from the specifics of Clojure, identifying the
general properties of concurrency models that facilitate or hinder composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3486</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3486</id><created>2014-06-13</created><authors><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames><affiliation>Universita' di Torino</affiliation></author><author><keyname>Padovani</keyname><forenames>Luca</forenames><affiliation>Universita' di Torino</affiliation></author><author><keyname>Pantovic</keyname><forenames>Jovanka</forenames><affiliation>Univerzitet u Novom Sadu</affiliation></author></authors><title>Session Type Isomorphisms</title><categories>cs.LO</categories><comments>In Proceedings PLACES 2014, arXiv:1406.3313</comments><proxy>EPTCS</proxy><acm-class>D.3.3, F.3.3</acm-class><journal-ref>EPTCS 155, 2014, pp. 61-71</journal-ref><doi>10.4204/EPTCS.155.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a considerable amount of work on retrieving functions in
function libraries using their type as search key. The availability of rich
component specifications, in the form of behavioral types, enables similar
queries where one can search a component library using the behavioral type of a
component as the search key. Just like for function libraries, however,
component libraries will contain components whose type differs from the
searched one in the order of messages or in the position of the branching
points. Thus, it makes sense to also look for those components whose type is
different from, but isomorphic to, the searched one.
  In this article we give semantic and axiomatic characterizations of
isomorphic session types. The theory of session type isomorphisms turns out to
be subtle. In part this is due to the fact that it relies on a non-standard
notion of equivalence between processes. In addition, we do not know whether
the axiomatization is complete. It is known that the isomorphisms for arrow,
product and sum types are not finitely axiomatisable, but it is not clear yet
whether this negative results holds also for the family of types we consider in
this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3495</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3495</id><created>2014-06-13</created><authors><author><keyname>Taruna</keyname><forenames>S.</forenames></author><author><keyname>Pahwa</keyname><forenames>Bhumika</forenames></author></authors><title>A Novel Scheme to improve spectrum sensing performance</title><categories>cs.NI cs.IT math.IT</categories><comments>8 pages,International Journal of Computer network and communication
  (IJCNC), 2014</comments><report-no>Vol.6, No.3, May 2014</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to limited availability of spectrum for licensed users only, the need for
secondary access by unlicensed users is increasing. Cognitive radio turns out
to be helping this situation because all that is needed is a technique that
could efficiently detect the empty spaces and provide them to the secondary
devices without causing any interference to the primary (licensed) users.
Spectrum sensing is the foremost function of the cognitive radio which senses
the environment for white spaces. Energy detection is one of the various
spectrum sensing techniques that are under research. Earlier it was shown that
energy detection works better under AWGN channel as compared to Rayleigh
channel, however the conventional spectrum sensing techniques have a high
probability of false alarm and also show a better probability of detection for
higher values of SNR. There is a need for a new technique that shows a reduced
probability of false alarm as well as an increase in the probability of
detection for lower values of SNR. In the present work the conventional energy
detection technique has been enhanced to get better results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3496</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3496</id><created>2014-06-13</created><authors><author><keyname>Fanaee-T</keyname><forenames>Hadi</forenames></author><author><keyname>Gama</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>EigenEvent: An Algorithm for Event Detection from Complex Data Streams
  in Syndromic Surveillance</title><categories>cs.AI cs.LG stat.AP</categories><comments>To appear in Intelligent Data Analysis Journal, vol. 19(3), 2015</comments><journal-ref>PP. 597-616, Vol. 19, No. 3, June 2015, Intelligent Data Analysis</journal-ref><doi>10.3233/IDA-150734</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syndromic surveillance systems continuously monitor multiple pre-diagnostic
daily streams of indicators from different regions with the aim of early
detection of disease outbreaks. The main objective of these systems is to
detect outbreaks hours or days before the clinical and laboratory confirmation.
The type of data that is being generated via these systems is usually
multivariate and seasonal with spatial and temporal dimensions. The algorithm
What's Strange About Recent Events (WSARE) is the state-of-the-art method for
such problems. It exhaustively searches for contrast sets in the multivariate
data and signals an alarm when find statistically significant rules. This
bottom-up approach presents a much lower detection delay comparing the existing
top-down approaches. However, WSARE is very sensitive to the small-scale
changes and subsequently comes with a relatively high rate of false alarms. We
propose a new approach called EigenEvent that is neither fully top-down nor
bottom-up. In this method, we instead of top-down or bottom-up search, track
changes in data correlation structure via eigenspace techniques. This new
methodology enables us to detect both overall changes (via eigenvalue) and
dimension-level changes (via eigenvectors). Experimental results on hundred
sets of benchmark data reveals that EigenEvent presents a better overall
performance comparing state-of-the-art, in particular in terms of the false
alarm rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3497</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3497</id><created>2014-06-13</created><updated>2014-11-18</updated><authors><author><keyname>Pirotta</keyname><forenames>Matteo</forenames></author><author><keyname>Parisi</keyname><forenames>Simone</forenames></author><author><keyname>Restelli</keyname><forenames>Marcello</forenames></author></authors><title>Multi-objective Reinforcement Learning with Continuous Pareto Frontier
  Approximation Supplementary Material</title><categories>cs.AI cs.LG</categories><comments>AAAI-15 Supplement. Updated upon acceptance at the Twenty-Ninth AAAI
  Conference on Artificial Intelligence (AAAI-15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document contains supplementary material for the paper &quot;Multi-objective
Reinforcement Learning with Continuous Pareto Frontier Approximation&quot;,
published at the Twenty-Ninth AAAI Conference on Artificial Intelligence
(AAAI-15). The paper is about learning a continuous approximation of the Pareto
frontier in Multi-Objective Markov Decision Problems (MOMDPs). We propose a
policy-based approach that exploits gradient information to generate solutions
close to the Pareto ones. Differently from previous policy-gradient
multi-objective algorithms, where n optimization routines are use to have n
solutions, our approach performs a single gradient-ascent run that at each step
generates an improved continuous approximation of the Pareto frontier. The idea
is to exploit a gradient-based approach to optimize the parameters of a
function that defines a manifold in the policy parameter space so that the
corresponding image in the objective space gets as close as possible to the
Pareto frontier. Besides deriving how to compute and estimate such gradient, we
will also discuss the non-trivial issue of defining a metric to assess the
quality of the candidate Pareto frontiers. Finally, the properties of the
proposed approach are empirically evaluated on two interesting MOMDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3499</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3499</id><created>2014-06-13</created><authors><author><keyname>Beer</keyname><forenames>Gernot</forenames></author><author><keyname>Marussig</keyname><forenames>Benjamin</forenames></author><author><keyname>Zechner</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>D&#xfc;nser</keyname><forenames>Christian</forenames></author><author><keyname>Fries</keyname><forenames>Thomas-Peter</forenames></author></authors><title>Boundary Element Analysis with trimmed NURBS and a generalized IGA
  approach</title><categories>cs.NA math.NA</categories><comments>11 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel approach to the simulation with the boundary element method using
trimmed NURBS patches is presented. The advantage of this approach is its
efficiency and easy implementation. The analysis with trimmed NURBS is achieved
by double mapping. The variation of the unknowns on the boundary is specified
in a local coordinate system and is completely independent of the description
of the geometry. The method is tested on a branched tunnel and the results
compared with those obtained from a conventional analysis. The conclusion is
that the proposed approach is superior in terms of number of unknowns and
effort required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3506</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3506</id><created>2014-06-13</created><authors><author><keyname>Fanaee-T</keyname><forenames>Hadi</forenames></author><author><keyname>Gama</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Eigenspace Method for Spatiotemporal Hotspot Detection</title><categories>cs.AI stat.AP</categories><comments>To appear in Expert Systems Journal</comments><doi>10.1111/exsy.12088</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hotspot detection aims at identifying subgroups in the observations that are
unexpected, with respect to the some baseline information. For instance, in
disease surveillance, the purpose is to detect sub-regions in spatiotemporal
space, where the count of reported diseases (e.g. Cancer) is higher than
expected, with respect to the population. The state-of-the-art method for this
kind of problem is the Space-Time Scan Statistics (STScan), which exhaustively
search the whole space through a sliding window looking for significant
spatiotemporal clusters. STScan makes some restrictive assumptions about the
distribution of data, the shape of the hotspots and the quality of data, which
can be unrealistic for some nontraditional data sources. A novel methodology
called EigenSpot is proposed where instead of an exhaustive search over the
space, tracks the changes in a space-time correlation structure. Not only does
the new approach presents much more computational efficiency, but also makes no
assumption about the data distribution, hotspot shape or the data quality. The
principal idea is that with the joint combination of abnormal elements in the
principal spatial and the temporal singular vectors, the location of hotspots
in the spatiotemporal space can be approximated. A comprehensive experimental
evaluation, both on simulated and real data sets reveals the effectiveness of
the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3512</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3512</id><created>2014-06-13</created><authors><author><keyname>Di Summa</keyname><forenames>Marco</forenames></author><author><keyname>Eisenbrand</keyname><forenames>Friedrich</forenames></author><author><keyname>Faenza</keyname><forenames>Yuri</forenames></author><author><keyname>Moldenhauer</keyname><forenames>Carsten</forenames></author></authors><title>On largest volume simplices and sub-determinants</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the problem of finding the simplex of largest volume in the
convex hull of $n$ points in $\mathbb{Q}^d$ can be approximated with a factor
of $O(\log d)^{d/2}$ in polynomial time. This improves upon the previously best
known approximation guarantee of $d^{(d-1)/2}$ by Khachiyan. On the other hand,
we show that there exists a constant $c&gt;1$ such that this problem cannot be
approximated with a factor of $c^d$, unless $P=NP$. % This improves over the
$1.09$ inapproximability that was previously known. Our hardness result holds
even if $n = O(d)$, in which case there exists a $\bar c\,^{d}$-approximation
algorithm that relies on recent sampling techniques, where $\bar c$ is again a
constant. We show that similar results hold for the problem of finding the
largest absolute value of a subdeterminant of a $d\times n$ matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3514</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3514</id><created>2014-06-13</created><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Mark&#xf3;</keyname><forenames>Roland</forenames></author></authors><title>Limits of CSP Problems and Efficient Parameter Testing</title><categories>cs.DS math.CO math.PR</categories><comments>47 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a unified framework on the limits of \emph{constraint satisfaction
problems} (CSPs) and efficient parameter testing which depends only on array
exchangeability and the method of cut decomposition without recourse to the
weakly regular partitions. In particular, we formulate and prove a
representation theorem for compact colored $r$-uniform directed hypergraph
($r$-graph) limits, and apply this to $r$CSP limits. We investigate the sample
complexity of testable $r$-graph parameters, we discuss the generalized ground
state energies and demonstrate that they are efficiently testable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3523</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3523</id><created>2014-06-13</created><updated>2014-08-31</updated><authors><author><keyname>Huang</keyname><forenames>Dandan</forenames></author><author><keyname>Deng</keyname><forenames>Yingpu</forenames></author></authors><title>Deterministic polynomial-time test for prime ideals in a Dedekind domain
  with finite rank</title><categories>math.RA cs.SC</categories><comments>14 pages</comments><msc-class>11Y11</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a deterministic polynomial-time test that determining whether a
nonzero ideal is a prime ideal in a Dedekind domain with finite rank. The
techniques which we used are basis representation of finite rings and the
Hermite and Smith normal forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3526</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3526</id><created>2014-06-13</created><updated>2015-04-27</updated><authors><author><keyname>Kramer</keyname><forenames>Simon</forenames></author></authors><title>Quantum Logic as Classical Logic</title><categories>quant-ph cs.LO math-ph math.LO math.MP math.QA</categories><comments>amplified the first paragraph on Page 3, the paragraph in the middle
  of Page 5; and the second-last paragraph on Page 9; added Footnote 1 on Page
  2, acknowledgements, and references</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a semantic representation of the standard quantum logic QL within
a classical, normal modal logic, and this via a lattice-embedding of
orthomodular lattices into Boolean algebras with one modal operator. Thus our
classical logic is a completion of the quantum logic QL. In other words, we
refute Birkhoff and von Neumann's classic thesis that the logic (the formal
character) of Quantum Mechanics would be non-classical as well as Putnam's
thesis that quantum logic (of his kind) would be the correct logic for
propositional inference in general. The propositional logic of Quantum
Mechanics is modal but classical, and the correct logic for propositional
inference need not have an extroverted quantum character. One normal necessity
modality suffices to capture the subjectivity of observation in quantum
experiments, and this thanks to its failure to distribute over classical
disjunction. The key to our result is the translation of quantum negation as
classical negation of observability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3550</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3550</id><created>2014-06-13</created><authors><author><keyname>Mishra</keyname><forenames>Saraswati</forenames></author><author><keyname>Kaur</keyname><forenames>Prabhjot</forenames></author></authors><title>Energy efficient neighbor selection for flat wireless sensor networks</title><categories>cs.NI cs.CY</categories><comments>7 pages, 5 figures, 1 table,WiMON 2014 conference proceedings, may
  24-25</comments><doi>10.5121/csit.2014.4511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have analyzed energy efficient neighbour selection
algorithms for routing in wireless sensor networks. Since energy saving or
consumption is an important aspect of wireless sensor networks, its precise
usage is highly desirable both for the faithful performance of network and to
increase the network life time. For this work, we have considered a flat
network topology where every node has the same responsibility and capability.
We have compared two energy efficient algorithms and analyzed their
performances with increase in number of nodes, time rounds and node failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3554</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3554</id><created>2014-06-13</created><authors><author><keyname>Lahlouhi</keyname><forenames>Ammar</forenames></author></authors><title>Methodological Societies</title><categories>cs.SE</categories><comments>International Journal in Foundations of Computer Science &amp; Technology
  (IJFCST), Vol.4, No.3, May 2014</comments><doi>10.5121/ijfcst.2014.4305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of self-adaptive systems poses the problems of their coherence
and the resume of the systems' functioning taking into account the accomplished
work. While they are the base of the self-adaptive systems, these two aspects
are not considered in the related works. In this paper, we propose a
methodological based approach. In such approach, the adaptive system's
evolution is thought at its model level where its execution is made on the
system by exploiting a methodological process. For its concretization, we use
colored Petri nets to describe the agents' individual tasks. To handle the
system's functioning resume, we exploit the property of Petri nets on which the
control flow depends on last marking only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3561</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3561</id><created>2014-06-13</created><authors><author><keyname>Di</keyname><forenames>Wei</forenames></author><author><keyname>Bhardwaj</keyname><forenames>Anurag</forenames></author><author><keyname>Jagadeesh</keyname><forenames>Vignesh</forenames></author><author><keyname>Piramuthu</keyname><forenames>Robinson</forenames></author><author><keyname>Churchill</keyname><forenames>Elizabeth</forenames></author></authors><title>When relevance is not Enough: Promoting Visual Attractiveness for
  Fashion E-commerce</title><categories>cs.HC</categories><acm-class>K.4.4; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fashion, and especially apparel, is the fastest-growing category in online
shopping. As consumers requires sensory experience especially for apparel goods
for which their appearance matters most, images play a key role not only in
conveying crucial information that is hard to express in text, but also in
affecting consumer's attitude and emotion towards the product. However,
research related to e-commerce product image has mostly focused on quality at
perceptual level, but not the quality of content, and the way of presenting.
This study aims to address the effectiveness of types of image in showcasing
fashion apparel in terms of its attractiveness, i.e. the ability to draw
consumer's attention, interest, and in return their engagement. We apply
advanced vision technique to quantize attractiveness using three common display
types in fashion filed, i.e. human model, mannequin, and flat. We perform
two-stage study by starting with large scale behavior data from real online
market, then moving to well designed user experiment to further deepen our
understandings on consumer's reasoning logic behind the action. We propose a
Fisher noncentral hypergeometric distribution based user choice model to
quantitatively evaluate user's preference. Further, we investigate the
potentials to leverage visual impact for a better search that caters to user's
preference. A visual attractiveness based re-ranking model that incorporates
both presentation efficacy and user preference is proposed. We show
quantitative improvement by promoting visual attractiveness into search on top
of relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3567</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3567</id><created>2014-06-13</created><authors><author><keyname>Sarmady</keyname><forenames>Siamak</forenames></author><author><keyname>Haron</keyname><forenames>Fazilah</forenames></author><author><keyname>Talib</keyname><forenames>Abdullah Zawawi</forenames></author></authors><title>Simulation of Pedestrian Movements Using Fine Grid Cellular Automata
  Model</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd simulation is used for evacuation and crowd safety inspections, study
of performance in crowd systems and animations. Cellular automata has been
extensively used in modelling the crowd. In regular cellular automata models,
each pedestrian occupies a single cell with the size of a pedestrian body.
Since the space is divided into relatively large cells, the movements of
pedestrians look like the movements of pieces on a chess board. Furthermore,
all pedestrians have the same body size and speed. In this paper, a method
called fine grid cellular automata is proposed in which smaller cells are used
and pedestrian body may occupy several cells. The model allows the use of
different body sizes, shapes and speeds for pedestrian.
  The proposed model is used for simulating movements of pedestrians toward a
target. A typical walkway scenario is used to test and evaluate the model. The
movements of pedestrians are smoother because of the finer grain discretization
of movements and the simulation results match empirical speed-density graphs
with good accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3568</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3568</id><created>2014-06-13</created><authors><author><keyname>Lonardo</keyname><forenames>A.</forenames></author><author><keyname>Ameli</keyname><forenames>F.</forenames></author><author><keyname>Ammendola</keyname><forenames>R.</forenames></author><author><keyname>Biagioni</keyname><forenames>A.</forenames></author><author><keyname>Frezza</keyname><forenames>O.</forenames></author><author><keyname>Lamanna</keyname><forenames>G.</forenames></author><author><keyname>Cicero</keyname><forenames>F. Lo</forenames></author><author><keyname>Martinelli</keyname><forenames>M.</forenames></author><author><keyname>Paolucci</keyname><forenames>P. S.</forenames></author><author><keyname>Pastorelli</keyname><forenames>E.</forenames></author><author><keyname>Pontisso</keyname><forenames>L.</forenames></author><author><keyname>Rossetti</keyname><forenames>D.</forenames></author><author><keyname>Simeone</keyname><forenames>F.</forenames></author><author><keyname>Simula</keyname><forenames>F.</forenames></author><author><keyname>Sozzi</keyname><forenames>M.</forenames></author><author><keyname>Tosoratto</keyname><forenames>L.</forenames></author><author><keyname>Vicini</keyname><forenames>P.</forenames></author></authors><title>NaNet: a Low-Latency, Real-Time, Multi-Standard Network Interface Card
  with GPUDirect Features</title><categories>physics.ins-det cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the GPGPU paradigm is widely recognized as an effective approach to
high performance computing, its adoption in low-latency, real-time systems is
still in its early stages.
  Although GPUs typically show deterministic behaviour in terms of latency in
executing computational kernels as soon as data is available in their internal
memories, assessment of real-time features of a standard GPGPU system needs
careful characterization of all subsystems along data stream path.
  The networking subsystem results in being the most critical one in terms of
absolute value and fluctuations of its response latency.
  Our envisioned solution to this issue is NaNet, a FPGA-based PCIe Network
Interface Card (NIC) design featuring a configurable and extensible set of
network channels with direct access through GPUDirect to NVIDIA Fermi/Kepler
GPU memories.
  NaNet design currently supports both standard - GbE (1000BASE-T) and 10GbE
(10Base-R) - and custom - 34~Gbps APElink and 2.5~Gbps deterministic latency
KM3link - channels, but its modularity allows for a straightforward inclusion
of other link technologies.
  To avoid host OS intervention on data stream and remove a possible source of
jitter, the design includes a network/transport layer offload module with
cycle-accurate, upper-bound latency, supporting UDP, KM3link Time Division
Multiplexing and APElink protocols.
  After NaNet architecture description and its latency/bandwidth
characterization for all supported links, two real world use cases will be
presented: the GPU-based low level trigger for the RICH detector in the NA62
experiment at CERN and the on-/off-shore data link for KM3 underwater neutrino
telescope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3582</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3582</id><created>2014-06-13</created><authors><author><keyname>Mishra</keyname><forenames>Kumar Vijay</forenames></author><author><keyname>Kruger</keyname><forenames>Anton</forenames></author><author><keyname>Krajewski</keyname><forenames>Witold F.</forenames></author></authors><title>Compressed Sensing Applied to Weather Radar</title><categories>cs.IT math.IT</categories><comments>4 pages, 5 figrues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an innovative meteorological radar, which uses reduced number of
spatiotemporal samples without compromising the accuracy of target information.
Our approach extends recent research on compressed sensing (CS) for radar
remote sensing of hard point scatterers to volumetric targets. The previously
published CS-based radar techniques are not applicable for sampling weather
since the precipitation echoes lack sparsity in both range-time and Doppler
domains. We propose an alternative approach by adopting the latest advances in
matrix completion algorithms to demonstrate the sparse sensing of weather
echoes. We use Iowa X-band Polarimetric (XPOL) radar data to test and
illustrate our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3583</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3583</id><created>2014-06-13</created><authors><author><keyname>Jaggard</keyname><forenames>Aaron D.</forenames></author><author><keyname>Johnson</keyname><forenames>Aaron</forenames></author><author><keyname>Syverson</keyname><forenames>Paul</forenames></author><author><keyname>Feigenbaum</keyname><forenames>Joan</forenames></author></authors><title>Representing Network Trust and Using It to Improve Anonymous
  Communication</title><categories>cs.CR</categories><comments>24 pages; talk to be presented at HotPETs 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the effectiveness of correlation attacks against Tor, the
censorship arms race, and observations of malicious relays in Tor, we propose
that Tor users capture their trust in network elements using probability
distributions over the sets of elements observed by network adversaries. We
present a modular system that allows users to efficiently and conveniently
create such distributions and use them to improve their security. The major
components of this system are (i) an ontology of network-element types that
represents the main threats to and vulnerabilities of anonymous communication
over Tor, (ii) a formal language that allows users to naturally express trust
beliefs about network elements, and (iii) a conversion procedure that takes the
ontology, public information about the network, and user beliefs written in the
trust language and produce a Bayesian Belief Network that represents the
probability distribution in a way that is concise and easily sampleable. We
also present preliminary experimental results that show the distribution
produced by our system can improve security when employed by users; further
improvement is seen when the system is employed by both users and services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3587</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3587</id><created>2014-06-13</created><authors><author><keyname>Xu</keyname><forenames>Dongpo</forenames></author><author><keyname>Mandic</keyname><forenames>Danilo P.</forenames></author></authors><title>Quaternion Gradient and Hessian</title><categories>math.NA cs.LG</categories><comments>23 pages</comments><journal-ref>IEEE Transactions on Neural Networks and Learning Systems, 2016,
  27(2):249-261</journal-ref><doi>10.1109/TNNLS.2015.2440473</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimization of real scalar functions of quaternion variables, such as
the mean square error or array output power, underpins many practical
applications. Solutions often require the calculation of the gradient and
Hessian, however, real functions of quaternion variables are essentially
non-analytic. To address this issue, we propose new definitions of quaternion
gradient and Hessian, based on the novel generalized HR (GHR) calculus, thus
making possible efficient derivation of optimization algorithms directly in the
quaternion field, rather than transforming the problem to the real domain, as
is current practice. In addition, unlike the existing quaternion gradients, the
GHR calculus allows for the product and chain rule, and for a one-to-one
correspondence of the proposed quaternion gradient and Hessian with their real
counterparts. Properties of the quaternion gradient and Hessian relevant to
numerical applications are elaborated, and the results illuminate the
usefulness of the GHR calculus in greatly simplifying the derivation of the
quaternion least mean squares, and in quaternion least square and Newton
algorithm. The proposed gradient and Hessian are also shown to enable the same
generic forms as the corresponding real- and complex-valued algorithms, further
illustrating the advantages in algorithm design and evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3597</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3597</id><created>2014-06-13</created><authors><author><keyname>Mamageishvili</keyname><forenames>Akaki</forenames></author><author><keyname>Mihal&#xe1;k</keyname><forenames>Mat&#xfa;&#x161;</forenames></author><author><keyname>Montemezzani</keyname><forenames>Simone</forenames></author></authors><title>An $H_{n/2}$ Upper Bound on the Price of Stability of Undirected Network
  Design Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the network design game with $n$ players, every player chooses a path in
an edge-weighted graph to connect her pair of terminals, sharing costs of the
edges on her path with all other players fairly. We study the price of
stability of the game, i.e., the ratio of the social costs of a best Nash
equilibrium (with respect to the social cost) and of an optimal play. It has
been shown that the price of stability of any network design game is at most
$H_n$, the $n$-th harmonic number. This bound is tight for directed graphs. For
undirected graphs, the situation is dramatically different, and tight bounds
are not known. It has only recently been shown that the price of stability is
at most $H_n \left(1-\frac{1}{\Theta(n^4)} \right)$, while the worst-case known
example has price of stability around 2.25. In this paper we improve the upper
bound considerably by showing that the price of stability is at most $H_{n/2} +
\epsilon$ for any $\epsilon$ starting from some suitable $n \geq n(\epsilon)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3602</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3602</id><created>2014-06-13</created><authors><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Perez</keyname><forenames>Marco A.</forenames></author><author><keyname>Sossa</keyname><forenames>Juan H.</forenames></author><author><keyname>Rodriguez</keyname><forenames>Jose G.</forenames></author><author><keyname>Palafox</keyname><forenames>Edgar O.</forenames></author></authors><title>An Educational Fuzzy-based Control platform using LEGO Robots</title><categories>cs.RO</categories><journal-ref>International Journal of Electrical Engineering Education, Volume
  50, Number 2 (April 2013), Manchester University Press, pp. 157-171</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy controllers have gained popularity in the past few decades with
successful implementations in many fields that have enabled designers to
control complex systems through linguistic-based rules in contrast to
traditional methods. This paper presents an educational platform based on
LEGO\c{opyright} NXT to assist the learning of fuzzy logic control principles
at undergraduate level by providing a simple and easy-to-follow teaching setup.
The proposed fuzzy control study aims to accompany students to the learning of
fuzzy control fundamentals by building hands-on robotic experiments. The
proposed educational platform has been successfully applied to several
undergraduate courses within the Electronics Department in the University of
Guadalajara. The description of robotic experiments and the evaluation of their
impact in the student performance are both provided in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3617</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3617</id><created>2014-06-13</created><updated>2015-04-17</updated><authors><author><keyname>Efthymiou</keyname><forenames>Charilaos</forenames></author></authors><title>Reconstruction/Non-reconstruction Thresholds for Colourings of General
  Galton-Watson Trees</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broadcasting models on trees arise in many contexts such as discrete
mathematics, biology statistical physics and cs. In this work, we consider the
colouring model. A basic question here is whether the root's assignment affects
the distribution of the colourings at the vertices at distance h from the root.
This is the so-called &quot;reconstruction problem&quot;. For a d-ary tree it is well
known that d/ln (d) is the reconstruction threshold. That is, for
k=(1+eps)d/ln(d) we have non-reconstruction while for k=(1-eps)d/ln(d) we have.
  Here, we consider the largely unstudied case where the underlying tree is
chosen according to a predefined distribution. In particular, our focus is on
the well-known Galton-Watson trees. This model arises naturally in many
contexts, e.g. the theory of spin-glasses and its applications on random
Constraint Satisfaction Problems (rCSP). The aforementioned study focuses on
Galton-Watson trees with offspring distribution B(n,d/n), i.e. the binomial
with parameters n and d/n, where d is fixed. Here we consider a broader version
of the problem, as we assume general offspring distribution, which includes
B(n,d/n) as a special case.
  Our approach relates the corresponding bounds for (non)reconstruction to
certain concentration properties of the offspring distribution. This allows to
derive reconstruction thresholds for a very wide family of offspring
distributions, which includes B(n,d/n). A very interesting corollary is that
for distributions with expected offspring d, we get reconstruction threshold
d/ln(d) under weaker concentration conditions than what we have in B(n,d/n).
  Furthermore, our reconstruction threshold for the random colorings of
Galton-Watson with offspring B(n,d/n), implies the reconstruction threshold for
the random colourings of G(n,d/n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3619</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3619</id><created>2014-06-13</created><updated>2014-12-16</updated><authors><author><keyname>Zhang</keyname><forenames>Xinlin</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Coldrey</keyname><forenames>Mikael</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>On the MIMO Capacity with Residual Transceiver Hardware Impairments</title><categories>cs.IT math.IT</categories><comments>Accepted for publication at the IEEE International Conference on
  Communications (ICC 2014), 7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio-frequency (RF) impairments in the transceiver hardware of communication
systems (e.g., phase noise (PN), high power amplifier (HPA) nonlinearities, or
in-phase/quadrature-phase (I/Q) imbalance) can severely degrade the performance
of traditional multiple-input multiple-output (MIMO) systems. Although
calibration algorithms can partially compensate these impairments, the
remaining distortion still has substantial impact. Despite this, most prior
works have not analyzed this type of distortion. In this paper, we investigate
the impact of residual transceiver hardware impairments on the MIMO system
performance. In particular, we consider a transceiver impairment model, which
has been experimentally validated, and derive analytical ergodic capacity
expressions for both exact and high signal-to-noise ratios (SNRs). We
demonstrate that the capacity saturates in the high-SNR regime, thereby
creating a finite capacity ceiling. We also present a linear approximation for
the ergodic capacity in the low-SNR regime, and show that impairments have only
a second-order impact on the capacity. Furthermore, we analyze the effect of
transceiver impairments on large-scale MIMO systems; interestingly, we prove
that if one increases the number of antennas at one side only, the capacity
behaves similar to the finite-dimensional case. On the contrary, if the number
of antennas on both sides increases with a fixed ratio, the capacity ceiling
vanishes; thus, impairments cause only a bounded offset in the capacity
compared to the ideal transceiver hardware case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3638</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3638</id><created>2014-06-13</created><authors><author><keyname>Zhang</keyname><forenames>Xinlin</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Coldrey</keyname><forenames>Mikael</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author></authors><title>Impact of Residual Transmit RF Impairments on Training-Based MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>Accepted for publication at the IEEE International Conference on
  Communications (ICC 2014), 6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio-frequency (RF) impairments, that exist intimately in wireless
communications systems, can severely degrade the performance of traditional
multiple-input multiple-output (MIMO) systems. Although compensation schemes
can cancel out part of these RF impairments, there still remains a certain
amount of impairments. These residual impairments have fundamental impact on
the MIMO system performance. However, most of the previous works have neglected
this factor. In this paper, a training-based MIMO system with residual transmit
RF impairments (RTRI) is considered. In particular, we derive a new channel
estimator for the proposed model, and find that RTRI can create an irreducible
estimation error floor. Moreover, we show that, in the presence of RTRI, the
optimal training sequence length can be larger than the number of transmit
antennas, especially in the low and high signal-to-noise ratio (SNR) regimes.
An increase in the proposed approximated achievable rate is also observed by
adopting the optimal training sequence length. When the training and data
symbol powers are required to be equal, we demonstrate that, at high SNRs,
systems with RTRI demand more training, whereas at low SNRs, such demands are
nearly the same for all practical levels of RTRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3650</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3650</id><created>2014-06-13</created><updated>2014-11-17</updated><authors><author><keyname>Mandt</keyname><forenames>Stephan</forenames></author><author><keyname>Blei</keyname><forenames>David</forenames></author></authors><title>Smoothed Gradients for Stochastic Variational Inference</title><categories>stat.ML cs.LG</categories><comments>Appears in Neural Information Processing Systems, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic variational inference (SVI) lets us scale up Bayesian computation
to massive data. It uses stochastic optimization to fit a variational
distribution, following easy-to-compute noisy natural gradients. As with most
traditional stochastic optimization methods, SVI takes precautions to use
unbiased stochastic gradients whose expectations are equal to the true
gradients. In this paper, we explore the idea of following biased stochastic
gradients in SVI. Our method replaces the natural gradient with a similarly
constructed vector that uses a fixed-window moving average of some of its
previous terms. We will demonstrate the many advantages of this technique.
First, its computational cost is the same as for SVI and storage requirements
only multiply by a constant factor. Second, it enjoys significant variance
reduction over the unbiased estimates, smaller bias than averaged gradients,
and leads to smaller mean-squared error against the full gradient. We test our
method on latent Dirichlet allocation with three large corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3655</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3655</id><created>2014-06-13</created><authors><author><keyname>Saettler</keyname><forenames>Aline</forenames></author><author><keyname>Laber</keyname><forenames>Eduardo</forenames></author><author><keyname>Cicalese</keyname><forenames>Ferdinando</forenames></author></authors><title>Trading off Worst and Expected Cost in Decision Tree Problems and a
  Value Dependent Model</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1309.2796</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of evaluating a discrete function by adaptively querying
the values of its variables until the values read uniquely determine the value
of the function. Reading the value of a variable is done at the expense of some
cost, and the goal is to design a strategy (decision tree) for evaluating the
function incurring as little cost as possible in the worst case or in
expectation (according to a prior distribution on the possible variables
assignments). Except for particular cases of the problem, in general, only the
minimization of one of these two measures is addressed in the literature.
However, there are instances of the problem for which the minimization of one
measure leads to a strategy with a high cost with respect to the other measure
(even exponentially bigger than the optimal). We provide a new construction
which can guarantee a trade-off between the two criteria. More precisely, given
a decision tree guaranteeing expected cost $E$ and a decision tree guaranteeing
worst cost $W$ our method can guarantee for any chosen trade-off value $\rho$
to produce a decision tree whose worst cost is $(1 + \rho)W$ and whose expected
cost is $(1 + \frac{1}{\rho})E.$ These bounds are improved for the relevant
case of uniform testing costs. Motivated by applications, we also study a
variant of the problem where the cost of reading a variable depends on the
variable's value. We provide an $O(\log n)$ approximation algorithm for the
minimization of the worst cost measure, which is best possible under the
assumption $P \neq NP$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3661</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3661</id><created>2014-06-13</created><authors><author><keyname>Bianculli</keyname><forenames>Domenico</forenames></author><author><keyname>Ghezzi</keyname><forenames>Carlo</forenames></author><author><keyname>Krstic</keyname><forenames>Srdan</forenames></author></authors><title>Trace checking of Metric Temporal Logic with Aggregating Modalities
  using MapReduce</title><categories>cs.SE</categories><comments>16 pages, 6 figures, Extended version of the SEFM 2014 paper</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern complex software systems produce a large amount of execution data,
often stored in logs. These logs can be analyzed using trace checking
techniques to check whether the system complies with its requirements
specifications. Often these specifications express quantitative properties of
the system, which include timing constraints as well as higher-level
constraints on the occurrences of significant events, expressed using aggregate
operators. In this paper we present an algorithm that exploits the MapReduce
programming model to check specifications expressed in a metric temporal logic
with aggregating modalities, over large execution traces. The algorithm
exploits the structure of the formula to parallelize the evaluation, with a
significant gain in time. We report on the assessment of the implementation -
based on the Hadoop framework - of the proposed algorithm and comment on its
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3663</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3663</id><created>2014-06-13</created><authors><author><keyname>Charilaos</keyname><forenames>Mylonas</forenames></author></authors><title>Analysis of networking characteristics of different personality types</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MBTI personality test and a personal facebook network were used in order
to gain some insights on the relationship of social network centrality and path
length measures and different personality types. Although the personality
classification data were scarce, there were some intuitive quantitative results
supporting anecdotal statements, based on empirical observations, about the
expected social behavior of personality types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3668</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3668</id><created>2014-06-13</created><updated>2014-06-16</updated><authors><author><keyname>Bi</keyname><forenames>Zedong</forenames></author><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author></authors><title>Optimal cooperation-trap strategies for the iterated Rock-Paper-Scissors
  game</title><categories>physics.soc-ph cs.GT</categories><comments>5 pages including 3 figures</comments><doi>10.1371/journal.pone.0111278</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an iterated non-cooperative game, if all the players act to maximize their
individual accumulated payoff, the system as a whole usually converges to a
Nash equilibrium that poorly benefits any player. Here we show that such an
undesirable destiny is avoidable in an iterated Rock-Paper-Scissors (RPS) game
involving two players X and Y. Player X has the option of proactively adopting
a cooperation-trap strategy, which enforces complete cooperation from the
rational player Y and leads to a highly beneficial as well as maximally fair
situation to both players. That maximal degree of cooperation is achievable in
such a competitive system with cyclic dominance of actions may stimulate
creative thinking on how to resolve conflicts and enhance cooperation in human
societies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3671</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3671</id><created>2014-06-13</created><updated>2014-11-09</updated><authors><author><keyname>Marasevic</keyname><forenames>Jelena</forenames></author><author><keyname>Stein</keyname><forenames>Cliff</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Max-min Fair Rate Allocation and Routing in Energy Harvesting Networks:
  Algorithmic Analysis</title><categories>cs.NI cs.DS</categories><comments>Full version of the paper published at ACM MobiHoc'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers max-min fair rate allocation and routing in energy
harvesting networks where fairness is required among both the nodes and the
time slots. Unlike most previous work on fairness, we focus on multihop
topologies and consider different routing methods. We assume a predictable
energy profile and focus on the design of efficient and optimal algorithms that
can serve as benchmarks for distributed and approximate algorithms. We first
develop an algorithm that obtains a max-min fair rate assignment for any given
(time-variable or time-invariable) unsplittable routing or a routing tree. For
time-invariable unsplittable routing, we also develop an algorithm that finds
routes that maximize the minimum rate assigned to any node in any slot. For
fractional routing, we study the joint routing and rate assignment problem. We
develop an algorithm for the time-invariable case with constant rates. We show
that the time-variable case is at least as hard as the 2-commodity feasible
flow problem and design an FPTAS to combat the high running time. Finally, we
show that finding an unsplittable routing or a routing tree that provides
lexicographically maximum rate assignment (i.e., that is the best in the
max-min fairness terms) is NP-hard, even for a time horizon of a single slot.
Our analysis provides insights into the problem structure and can be applied to
other related fairness problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3672</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3672</id><created>2014-06-13</created><updated>2015-12-14</updated><authors><author><keyname>Roy</keyname><forenames>Aurko</forenames></author></authors><title>Deterministic polynomial factoring under the assumption of the Extended
  Riemann Hypothesis (ERH)</title><categories>cs.DM</categories><comments>Master's Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of deterministically factoring a univariate
polynomial over a finite field under the assumption of the Extended Riemann
Hypothesis (ERH). This work builds upon the line of approach first explored by
Gao in $2001$. The general approach has been to implicitly construct a graph
with the roots as vertices and the edges formed by some polynomial time
computable relation. The algorithm then fails to factor a polynomial if this
associated graph turns out to be \emph{regular}. In the first part of our work
we strengthen the edge relation so that the resulting set of graphs we obtain
are subgraphs of Gao's graphs, all of which must be \emph{regular}.
  In the second part of our work we strengthen the regularity condition of
these graphs. This is accomplished by finding a parallel between their
algorithms and the $1$-dimensional Weisfeiler-Leman algorithm for solving the
Graph Isomorphism problem. We observe that the general principle behind their
algorithms is to separate the roots by computing the $1$-dimensional
Weisfeiler-Leman approximation to the orbits of this graph. This leads us to
the natural question of whether this approximation may be improved. We then go
on to show how to implicitly compute the $2$-dimensional Weisfeiler-Leman
approximation of the orbits of these graphs. The polynomials that our algorithm
fails to factor form graphs that are \emph{strongly regular} and their set of
adjacency matrices forms a combinatorial structure called an \emph{Association
scheme}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3676</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3676</id><created>2014-06-13</created><updated>2014-09-03</updated><authors><author><keyname>Bordes</keyname><forenames>Antoine</forenames></author><author><keyname>Chopra</keyname><forenames>Sumit</forenames></author><author><keyname>Weston</keyname><forenames>Jason</forenames></author></authors><title>Question Answering with Subgraph Embeddings</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a system which learns to answer questions on a broad
range of topics from a knowledge base using few hand-crafted features. Our
model learns low-dimensional embeddings of words and knowledge base
constituents; these representations are used to score natural language
questions against candidate answers. Training our system using pairs of
questions and structured representations of their answers, and pairs of
question paraphrases, yields competitive results on a competitive benchmark of
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3682</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3682</id><created>2014-06-14</created><authors><author><keyname>Gupta</keyname><forenames>Srishti</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Emerging Phishing Trends and Effectiveness of the Anti-Phishing Landing
  Page</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Each month, more attacks are launched with the aim of making web users
believe that they are communicating with a trusted entity which compels them to
share their personal, financial information. Phishing costs Internet users
billions of dollars every year. Researchers at Carnegie Mellon University (CMU)
created an anti-phishing landing page supported by Anti-Phishing Working Group
(APWG) with the aim to train users on how to prevent themselves from phishing
attacks. It is used by financial institutions, phish site take down vendors,
government organizations, and online merchants. When a potential victim clicks
on a phishing link that has been taken down, he / she is redirected to the
landing page. In this paper, we present the comparative analysis on two
datasets that we obtained from APWG's landing page log files; one, from
September 7, 2008 - November 11, 2009, and other from January 1, 2014 - April
30, 2014. We found that the landing page has been successful in training users
against phishing. Forty six percent users clicked lesser number of phishing
URLs from January 2014 to April 2014 which shows that training from the landing
page helped users not to fall for phishing attacks. Our analysis shows that
phishers have started to modify their techniques by creating more legitimate
looking URLs and buying large number of domains to increase their activity. We
observed that phishers are exploiting ICANN accredited registrars to launch
their attacks even after strict surveillance. We saw that phishers are trying
to exploit free subdomain registration services to carry out attacks. In this
paper, we also compared the phishing e-mails used by phishers to lure victims
in 2008 and 2014. We found that the phishing e-mails have changed considerably
over time. Phishers have adopted new techniques like sending promotional
e-mails and emotionally targeting users in clicking phishing URLs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3687</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3687</id><created>2014-06-14</created><authors><author><keyname>Gupta</keyname><forenames>Neha</forenames></author><author><keyname>Aggarwal</keyname><forenames>Anupama</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>bit.ly/malicious: Deep Dive into Short URL based e-Crime Detection</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1405.1511</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existence of spam URLs over emails and Online Social Media (OSM) has become a
massive e-crime. To counter the dissemination of long complex URLs in emails
and character limit imposed on various OSM (like Twitter), the concept of URL
shortening has gained a lot of traction. URL shorteners take as input a long
URL and output a short URL with the same landing page (as in the long URL) in
return. With their immense popularity over time, URL shorteners have become a
prime target for the attackers giving them an advantage to conceal malicious
content. Bitly, a leading service among all shortening services is being
exploited heavily to carry out phishing attacks, work-from-home scams,
pornographic content propagation, etc. This imposes additional performance
pressure on Bitly and other URL shorteners to be able to detect and take a
timely action against the illegitimate content. In this study, we analyzed a
dataset of 763,160 short URLs marked suspicious by Bitly in the month of
October 2013. Our results reveal that Bitly is not using its claimed spam
detection services very effectively. We also show how a suspicious Bitly
account goes unnoticed despite of a prolonged recurrent illegitimate activity.
Bitly displays a warning page on identification of suspicious links, but we
observed this approach to be weak in controlling the overall propagation of
spam. We also identified some short URL based features and coupled them with
two domain specific features to classify a Bitly URL as malicious or benign and
achieved an accuracy of 86.41%. The feature set identified can be generalized
to other URL shortening services as well. To the best of our knowledge, this is
the first large scale study to highlight the issues with the implementation of
Bitly's spam detection policies and proposing suitable countermeasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3692</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3692</id><created>2014-06-14</created><authors><author><keyname>Dewan</keyname><forenames>Prateek</forenames></author><author><keyname>Kashyap</keyname><forenames>Anand</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Analyzing Social and Stylometric Features to Identify Spear phishing
  Emails</title><categories>cs.CY cs.LG cs.SI</categories><comments>Detection of spear phishing using social media features</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spear phishing is a complex targeted attack in which, an attacker harvests
information about the victim prior to the attack. This information is then used
to create sophisticated, genuine-looking attack vectors, drawing the victim to
compromise confidential information. What makes spear phishing different, and
more powerful than normal phishing, is this contextual information about the
victim. Online social media services can be one such source for gathering vital
information about an individual. In this paper, we characterize and examine a
true positive dataset of spear phishing, spam, and normal phishing emails from
Symantec's enterprise email scanning service. We then present a model to detect
spear phishing emails sent to employees of 14 international organizations, by
using social features extracted from LinkedIn. Our dataset consists of 4,742
targeted attack emails sent to 2,434 victims, and 9,353 non targeted attack
emails sent to 5,912 non victims; and publicly available information from their
LinkedIn profiles. We applied various machine learning algorithms to this
labeled data, and achieved an overall maximum accuracy of 97.76% in identifying
spear phishing emails. We used a combination of social features from LinkedIn
profiles, and stylometric features extracted from email subjects, bodies, and
attachments. However, we achieved a slightly better accuracy of 98.28% without
the social features. Our analysis revealed that social features extracted from
LinkedIn do not help in identifying spear phishing emails. To the best of our
knowledge, this is one of the first attempts to make use of a combination of
stylometric features extracted from emails, and social features extracted from
an online social network to detect targeted spear phishing emails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3693</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3693</id><created>2014-06-14</created><authors><author><keyname>Ray</keyname><forenames>Partha Pratim</forenames></author></authors><title>Channel Modeling of Human Somatosensory Nanonetwork: Body Discriminative
  Touch and Proprioception Perspective</title><categories>cs.ET</categories><comments>11 pages, 6 figures</comments><journal-ref>International Journal on Computer Science and Engineering, Vol. 5
  No. 10, pp. 874-884, Oct 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanonetwork design and analysis has become a very interesting topic in recent
years. Though this area of research is in its formative stage, it definitely
posses a strong integrity in finding out numerous applications in medical and
allied sciences. Nanonetworking is indeed a nature built foundation which
comprises human intra body communications. Somatosensory system is the one of
the critical and must have systems of human body. This literature concentrates
on the body discriminative touch and proprioception mechanism of somatosensory
system. This particular system is well architecture by medial lemniscal
pathway, in human body for transduction of touch and proprioceptive
information. This paper seeks out the novel communication channel model of
somatosensory system. The working principle of the channel model is established
by an equivalent Moore machine. A novel algorithm MLP is proposed after its
name, medial lemniscal pathway. A novel naomachine and appropriate processing
unit are also devised, based on the automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3699</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3699</id><created>2014-06-14</created><authors><author><keyname>Jimenez</keyname><forenames>Ivo</forenames></author><author><keyname>Maltzahn</keyname><forenames>Carlos</forenames></author><author><keyname>Lofstead</keyname><forenames>Jay</forenames></author></authors><title>Distributed Versioned Object Storage -- Alternatives at the OSD layer
  (Poster Extended Abstract)</title><categories>cs.DC</categories><comments>2 pages, 2 tables, poster extended abstract, HPDC '14, The ACM
  International Symposium on High-Performance Parallel and Distributed
  Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to store multiple versions of a data item is a powerful primitive
that has had a wide variety of uses: relational databases, transactional
memory, version control systems, to name a few. However, each implementation
uses a very particular form of versioning that is customized to the domain in
question and hidden away from the user. In our going project, we are reviewing
and analyzing multiple uses of versioning in distinct domains, with the goal of
identifying the basic components required to provide a generic distributed
multiversioning object storage service, and define how these can be customized
in order to serve distinct needs. With this primitive, new services can
leverage multiversioning to ease development and provide specific consistency
guarantees that address particular use cases. This work presents early results
that quantify the trade-offs in implementing versioning at the local storage
layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3700</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3700</id><created>2014-06-14</created><updated>2014-11-01</updated><authors><author><keyname>Lin</keyname><forenames>Bingkai</forenames></author></authors><title>The Parameterized Complexity of k-Biclique</title><categories>cs.CC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G$ and a parameter $k$, the $k$-biclique problem asks whether
$G$ contains a complete bipartite subgraph $K_{k,k}$. This is the most easily
stated problem on graphs whose parameterized complexity is still unknown. We
provide an fpt-reduction from $k$-clique to $k$-biclique, thus solving this
longstanding open problem.
  Our reduction use a class of bipartite graphs with a threshold property of
independent interest. More specifically, for positive integers $n$, $s$ and
$t$, we consider a bipartite graph $G=(A\;\dot\cup\;B, E)$ such that $A$ can be
partitioned into $A=V_1\;\dot\cup \;V_2\;\dot\cup\cdots\dot\cup\; V_n$ and for
every $s$ distinct indices $i_1\cdots i_s$, there exist $v_{i_1}\in
V_{i_1}\cdots v_{i_s}\in V_{i_s}$ such that $v_{i_1}\cdots v_{i_s}$ have at
least $t+1$ common neighbors in $B$; on the other hand, every $s+1$ distinct
vertices in $A$ have at most $t$ common neighbors in $B$.
  Using the Paley-type graphs and Weil's character sum theorem, we show that
for $t=(s+1)!$ and $n$ large enough, such threshold bipartite graphs can be
computed in $n^{O(1)}$. One corollary of our reduction is that there is no
$f(k)\cdot n^{o(k)}$ time algorithm to decide whether a graph contains a
subgraph isomorphic to $K_{k!,k!}$ unless the ETH(Exponential Time Hypothesis)
fails. We also provide a probabilistic construction with better parameters
$t=\Theta(s^2)$, which indicates that $k$-biclique has no $f(k)\cdot
n^{o(\sqrt{k})}$-time algorithm unless 3-SAT with $m$ clauses can be solved in
$2^{o(m)}$-time with high probability. Our result also implies the dichotomy
classification of the parameterized complexity of cardinality constrain
satisfaction problem and the inapproximability of maximum $k$-intersection
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3714</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3714</id><created>2014-06-14</created><authors><author><keyname>Sharma</keyname><forenames>Richa</forenames></author><author><keyname>Nigam</keyname><forenames>Shweta</forenames></author><author><keyname>Jain</keyname><forenames>Rekha</forenames></author></authors><title>Mining of product reviews at aspect level</title><categories>cs.CL cs.IR</categories><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol.4, No.3, May 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Todays world is a world of Internet, almost all work can be done with the
help of it, from simple mobile phone recharge to biggest business deals can be
done with the help of this technology. People spent their most of the times on
surfing on the Web it becomes a new source of entertainment, education,
communication, shopping etc. Users not only use these websites but also give
their feedback and suggestions that will be useful for other users. In this way
a large amount of reviews of users are collected on the Web that needs to be
explored, analyse and organized for better decision making. Opinion Mining or
Sentiment Analysis is a Natural Language Processing and Information Extraction
task that identifies the users views or opinions explained in the form of
positive, negative or neutral comments and quotes underlying the text. Aspect
based opinion mining is one of the level of Opinion mining that determines the
aspect of the given reviews and classify the review for each feature. In this
paper an aspect based opinion mining system is proposed to classify the reviews
as positive, negative and neutral for each feature. Negation is also handled in
the proposed system. Experimental results using reviews of products show the
effectiveness of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3715</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3715</id><created>2014-06-14</created><updated>2014-09-11</updated><authors><author><keyname>Fouch&#xe9;</keyname><forenames>Willem Louw</forenames><affiliation>University of South Africa</affiliation></author><author><keyname>Mukeru</keyname><forenames>Safari</forenames><affiliation>University of South Africa</affiliation></author><author><keyname>Davie</keyname><forenames>George</forenames><affiliation>University of South Africa</affiliation></author></authors><title>Fourier spectra of measures associated with algorithmically random
  Brownian motion</title><categories>cs.CC</categories><comments>24 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  12, 2014) lmcs:819</journal-ref><doi>10.2168/LMCS-10(3:20)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the behaviour at infinity of the Fourier transform of
Radon measures supported by the images of fractal sets under an algorithmically
random Brownian motion. We show that, under some computability conditions on
these sets, the Fourier transform of the associated measures have, relative to
the Hausdorff dimensions of these sets, optimal asymptotic decay at infinity.
The argument relies heavily on a direct characterisation, due to Asarin and
Pokrovskii, of algorithmically random Brownian motion in terms of the prefix
free Kolmogorov complexity of finite binary sequences. The study also
necessitates a closer look at the potential theory over fractals from a
computable point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3726</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3726</id><created>2014-06-14</created><authors><author><keyname>Sahai</keyname><forenames>Ankur</forenames></author></authors><title>Evaluation of Machine Learning Techniques for Green Energy Prediction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluate the following Machine Learning techniques for Green Energy (Wind,
Solar) Prediction: Bayesian Inference, Neural Networks, Support Vector
Machines, Clustering techniques (PCA). Our objective is to predict green energy
using weather forecasts, predict deviations from forecast green energy, find
correlation amongst different weather parameters and green energy availability,
recover lost or missing energy (/ weather) data. We use historical weather data
and weather forecasts for the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3727</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3727</id><created>2014-06-14</created><authors><author><keyname>Basha</keyname><forenames>N. Md Jubair</forenames></author><author><keyname>Mohan</keyname><forenames>Chandra</forenames></author></authors><title>A methodology to identify the level of reuse using template factors</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1203.1328, arXiv:1207.4938,
  arXiv:1202.5609</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To build large scale software systems, Component Based Software Engineering
(CBSE) has played a vital role. The current practices of software industry
demands more development of a software within time and budget which is highly
productive to them. It became so necessary to achieve how effectively the
software component is reusable. In order to meet this, the component level
reuse, in terms of both class and method level can be possibly done. The
traditional approaches are presented in the literature upto the level of extent
of achievement of reuse. Any how still effective reuse is a challenging issue
as a part. In this paper, a methodology has proposed for the identification of
reuse level which has been considered by the using reuse metrics such as the
Class Template Factor(CTF) and Method Template Factor(MTF). By considering
these measures makes easy to identify the level of reuse so that helps in the
growth the productivity in the organization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3728</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3728</id><created>2014-06-14</created><authors><author><keyname>Basha</keyname><forenames>N. Md Jubair</forenames></author><author><keyname>Moiz</keyname><forenames>Salman Abdul</forenames></author></authors><title>Component Based Software Development: A State of Art</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the goals of Software design is to model a system in such a way that
it is reused. Actively reusing designs or code allows taking advantage of the
investment made on reusable components. However development of domain specific
components and its impact on effort in terms of cost and time is still a
challenging issue. The component based technology has transformed over a period
of time from a simple component to the domain specific components. This paper
presents a state of art of the drastic change in component technology from
component engineering to domain engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3740</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3740</id><created>2014-06-14</created><authors><author><keyname>Dyer</keyname><forenames>Ramsay</forenames></author><author><keyname>Vegter</keyname><forenames>Gert</forenames></author><author><keyname>Wintraecken</keyname><forenames>Mathijs</forenames></author></authors><title>Riemannian simplices and triangulations</title><categories>math.DG cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a natural intrinsic definition of geometric simplices in Riemannian
manifolds of arbitrary dimension $n$, and exploit these simplices to obtain
criteria for triangulating compact Riemannian manifolds. These geometric
simplices are defined using Karcher means. Given a finite set of vertices in a
convex set on the manifold, the point that minimises the weighted sum of
squared distances to the vertices is the Karcher mean relative to the weights.
Using barycentric coordinates as the weights, we obtain a smooth map from the
standard Euclidean simplex to the manifold. A Riemannian simplex is defined as
the image of this barycentric coordinate map. In this work we articulate
criteria that guarantee that the barycentric coordinate map is a smooth
embedding. If it is not, we say the Riemannian simplex is degenerate. Quality
measures for the &quot;thickness&quot; or &quot;fatness&quot; of Euclidean simplices can be adapted
to apply to these Riemannian simplices. For manifolds of dimension 2, the
simplex is non-degenerate if it has a positive quality measure, as in the
Euclidean case. However, when the dimension is greater than two, non-degeneracy
can be guaranteed only when the quality exceeds a positive bound that depends
on the size of the simplex and local bounds on the absolute values of the
sectional curvatures of the manifold. An analysis of the geometry of
non-degenerate Riemannian simplices leads to conditions which guarantee that a
simplicial complex is homeomorphic to the manifold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3753</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3753</id><created>2014-06-14</created><authors><author><keyname>Marinello</keyname><forenames>Jos&#xe9; Carlos</forenames></author><author><keyname>Abr&#xe3;o</keyname><forenames>Taufik</forenames></author></authors><title>BER Analysis of Multi-Cellular MIMO Systems with Increasing Number of BS
  Antennas</title><categories>cs.NI cs.IT math.IT</categories><comments>8 pages, 5 figures, under submission (journal)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, salient characteristics of a wireless communication system
deploying a great number of antennas in the base station (BS), namely Massive
MIMO system, are investigated. In particular, we found a simple and meaningful
relationship that corroborates a fundamental assumption in recent related
works: according the number of BS antennas $N$ grows, the product of the
small-scale fading channel matrix with its conjugate transpose tends to a
scaled identity, with a variability measure inversely proportional to $N$.
Furthermore, analysis of the Massive MIMO system downlink is carried out from a
bit-error-rate (BER) performance viewpoint, including some realistic adverse
effects, such as interference from neighboring cells, channel estimation errors
due to background thermal noise, and pilot contamination, which was recently
shown to be the only impairment that remains in the MIMO multicell system with
infinite number of BS antennas. Our numerical result findings show that, in the
same way as with the sum capacity, the pilot contamination also limits the BER
performance of a noncooperative multi-cell MIMO system with infinite number of
BS antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3781</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3781</id><created>2014-06-14</created><updated>2014-11-22</updated><authors><author><keyname>Mehta</keyname><forenames>Nishant A.</forenames></author><author><keyname>Williamson</keyname><forenames>Robert C.</forenames></author></authors><title>From Stochastic Mixability to Fast Rates</title><categories>cs.LG stat.ML</categories><comments>21 pages, accepted to NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical risk minimization (ERM) is a fundamental learning rule for
statistical learning problems where the data is generated according to some
unknown distribution $\mathsf{P}$ and returns a hypothesis $f$ chosen from a
fixed class $\mathcal{F}$ with small loss $\ell$. In the parametric setting,
depending upon $(\ell, \mathcal{F},\mathsf{P})$ ERM can have slow
$(1/\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as a
function of the sample size $n$. There exist several results that give
sufficient conditions for fast rates in terms of joint properties of $\ell$,
$\mathcal{F}$, and $\mathsf{P}$, such as the margin condition and the Bernstein
condition. In the non-statistical prediction with expert advice setting, there
is an analogous slow and fast rate phenomenon, and it is entirely characterized
in terms of the mixability of the loss $\ell$ (there being no role there for
$\mathcal{F}$ or $\mathsf{P}$). The notion of stochastic mixability builds a
bridge between these two models of learning, reducing to classical mixability
in a special case. The present paper presents a direct proof of fast rates for
ERM in terms of stochastic mixability of $(\ell,\mathcal{F}, \mathsf{P})$, and
in so doing provides new insight into the fast-rates phenomenon. The proof
exploits an old result of Kemperman on the solution to the general moment
problem. We also show a partial converse that suggests a characterization of
fast rates for ERM in terms of stochastic mixability is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3782</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3782</id><created>2014-06-14</created><updated>2014-10-07</updated><authors><author><keyname>Xu</keyname><forenames>Yingying</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Zdeborova</keyname><forenames>Lenka</forenames></author></authors><title>Bayesian signal reconstruction for 1-bit compressed sensing</title><categories>physics.data-an cs.IT math.IT</categories><comments>24pages,9figures</comments><journal-ref>Journal of Statistical Mechanics: Theory and Experiment, 2014(11),
  P11015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 1-bit compressed sensing framework enables the recovery of a sparse
vector x from the sign information of each entry of its linear transformation.
Discarding the amplitude information can significantly reduce the amount of
data, which is highly beneficial in practical applications. In this paper, we
present a Bayesian approach to signal reconstruction for 1-bit compressed
sensing, and analyze its typical performance using statistical mechanics.
Utilizing the replica method, we show that the Bayesian approach enables better
reconstruction than the L1-norm minimization approach, asymptotically
saturating the performance obtained when the non-zero entries positions of the
signal are known. We also test a message passing algorithm for signal
reconstruction on the basis of belief propagation. The results of numerical
experiments are consistent with those of the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3790</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3790</id><created>2014-06-14</created><authors><author><keyname>Qian</keyname><forenames>Zhiliang</forenames></author></authors><title>High Performance Network-on-Chips (NoCs) Design: Performance Modeling,
  Routing Algorithm and Architecture Optimization</title><categories>cs.OH</categories><comments>A Ph.D. thesis of Zhiliang Qian in the Hong Kong University of
  Science and Technology; Thesis supervisor: Prof. Chi-Ying Tsui</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With technology scaling down, hundreds and thousands processing elements
(PEs) can be integrated on a single chip. Network-on-chip (NoC) has been
proposed as an efficient solution to handle this distinctive challenge. In this
thesis, we have explored the high performance NoC design for MPSoC and CMP
structures from the performance modeling in the offline design phase to the
routing algorithm and NoC architecture optimization. More specifically, we
first deal with the issue of how to estimate an NoC design fast and accurately
in the synthesis inner loop. For this purpose, we propose a machine learning
based latency regression model to evaluate the NoC designs with respect to
different configurations. Then, for high performance NoC designs, we tackle one
of the most important problems, i.e., the routing algorithms design. For
avoiding temperature hotspots, a thermal-aware routing algorithm is proposed to
achieve an even temperature profile for application-specific Network-on-chips
(NoCs). For improving the reliability, a routing algorithm to achieve maximum
performance under fault is proposed. Finally, in the architecture level, we
propose two new NoC structures using bi-directional links for the performance
optimization. In particular, we propose a flit-level speedup scheme to enhance
the network-on-chip(NoC) performance utilizing bidirectional channels. We also
propose a flexible NoC architecture which takes advantage of a dynamic
distributed routing algorithm and improves the NoC communication performance
with moderate energy overhead. From the simulation results on both synthetic
traffic and real workload traces, significant performance improvement in terms
of latency and throughput can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3792</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3792</id><created>2014-06-14</created><authors><author><keyname>Xiong</keyname><forenames>Tao</forenames></author><author><keyname>Bao</keyname><forenames>Yukun</forenames></author><author><keyname>Hu</keyname><forenames>Zhongyi</forenames></author></authors><title>Interval Forecasting of Electricity Demand: A Novel Bivariate EMD-based
  Support Vector Regression Modeling Framework</title><categories>cs.LG stat.AP</categories><doi>10.1016/j.ijepes.2014.06.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly accurate interval forecasting of electricity demand is fundamental to
the success of reducing the risk when making power system planning and
operational decisions by providing a range rather than point estimation. In
this study, a novel modeling framework integrating bivariate empirical mode
decomposition (BEMD) and support vector regression (SVR), extended from the
well-established empirical mode decomposition (EMD) based time series modeling
framework in the energy demand forecasting literature, is proposed for interval
forecasting of electricity demand. The novelty of this study arises from the
employment of BEMD, a new extension of classical empirical model decomposition
(EMD) destined to handle bivariate time series treated as complex-valued time
series, as decomposition method instead of classical EMD only capable of
decomposing one-dimensional single-valued time series. This proposed modeling
framework is endowed with BEMD to decompose simultaneously both the lower and
upper bounds time series, constructed in forms of complex-valued time series,
of electricity demand on a monthly per hour basis, resulting in capturing the
potential interrelationship between lower and upper bounds. The proposed
modeling framework is justified with monthly interval-valued electricity demand
data per hour in Pennsylvania-New Jersey-Maryland Interconnection, indicating
it as a promising method for interval-valued electricity demand forecasting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3793</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3793</id><created>2014-06-14</created><authors><author><keyname>Tan</keyname><forenames>Cheston</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Neural tuning size is a key factor underlying holistic face processing</title><categories>cs.AI cs.CV cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Faces are a class of visual stimuli with unique significance, for a variety
of reasons. They are ubiquitous throughout the course of a person's life, and
face recognition is crucial for daily social interaction. Faces are also unlike
any other stimulus class in terms of certain physical stimulus characteristics.
Furthermore, faces have been empirically found to elicit certain characteristic
behavioral phenomena, which are widely held to be evidence of &quot;holistic&quot;
processing of faces. However, little is known about the neural mechanisms
underlying such holistic face processing. In other words, for the processing of
faces by the primate visual system, the input and output characteristics are
relatively well known, but the internal neural computations are not. The main
aim of this work is to further the fundamental understanding of what causes the
visual processing of faces to be different from that of objects. In this
computational modeling work, we show that a single factor - &quot;neural tuning
size&quot; - is able to account for three key phenomena that are characteristic of
face processing, namely the Composite Face Effect (CFE), Face Inversion Effect
(FIE) and Whole-Part Effect (WPE). Our computational proof-of-principle
provides specific neural tuning properties that correspond to the
poorly-understood notion of holistic face processing, and connects these neural
properties to psychophysical behavior. Overall, our work provides a unified and
parsimonious theoretical account for the disparate empirical data on
face-specific processing, deepening the fundamental understanding of face
processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3807</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3807</id><created>2014-06-15</created><updated>2014-07-10</updated><authors><author><keyname>Solomon</keyname><forenames>Yaar</forenames></author><author><keyname>Weiss</keyname><forenames>Barak</forenames></author></authors><title>Dense forests and Danzer sets</title><categories>math.MG cs.CG math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set $Y\subseteq\mathbb{R}^d$ that intersects every convex set of volume $1$
is called a Danzer set. It is not known whether there are Danzer sets in
$\mathbb{R}^d$ with growth rate $O(T^d)$. We prove that natural candidates,
such as discrete sets that arise from substitutions and from cut-and-project
constructions, are not Danzer sets. For cut and project sets our proof relies
on the dynamics of homogeneous flows. We consider a weakening of the Danzer
problem, the existence of uniformly discrete dense forests, and we use
homogeneous dynamics (in particular Ratner's theorems on unipotent flows) to
construct such sets. We also prove an equivalence between the above problem and
a well-known combinatorial problem, and deduce the existence of Danzer sets
with growth rate $O(T^d\log T)$, improving the previous bound of
$O(T^d\log^{d-1} T)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3812</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3812</id><created>2014-06-15</created><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Heggernes</keyname><forenames>Pinar</forenames></author><author><keyname>Hof</keyname><forenames>Pim van 't</forenames></author><author><keyname>Paul</keyname><forenames>Christophe</forenames></author></authors><title>Hadwiger number of graphs with small chordality</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hadwiger number of a graph G is the largest integer h such that G has the
complete graph K_h as a minor. We show that the problem of determining the
Hadwiger number of a graph is NP-hard on co-bipartite graphs, but can be solved
in polynomial time on cographs and on bipartite permutation graphs. We also
consider a natural generalization of this problem that asks for the largest
integer h such that G has a minor with h vertices and diameter at most $s$. We
show that this problem can be solved in polynomial time on AT-free graphs when
s&gt;=2, but is NP-hard on chordal graphs for every fixed s&gt;=2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3816</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3816</id><created>2014-06-15</created><authors><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author></authors><title>Simultaneous Model Selection and Optimization through Parameter-free
  Stochastic Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient descent algorithms for training linear and kernel
predictors are gaining more and more importance, thanks to their scalability.
While various methods have been proposed to speed up their convergence, the
model selection phase is often ignored. In fact, in theoretical works most of
the time assumptions are made, for example, on the prior knowledge of the norm
of the optimal solution, while in the practical world validation methods remain
the only viable approach. In this paper, we propose a new kernel-based
stochastic gradient descent algorithm that performs model selection while
training, with no parameters to tune, nor any form of cross-validation. The
algorithm builds on recent advancement in online learning theory for
unconstrained settings, to estimate over time the right regularization in a
data-dependent way. Optimal rates of convergence are proved under standard
smoothness assumptions on the target function, using the range space of the
fractional integral operator associated with the kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3830</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3830</id><created>2014-06-15</created><authors><author><keyname>Denil</keyname><forenames>Misha</forenames></author><author><keyname>Demiraj</keyname><forenames>Alban</forenames></author><author><keyname>Kalchbrenner</keyname><forenames>Nal</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Modelling, Visualising and Summarising Documents with a Single
  Convolutional Neural Network</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capturing the compositional process which maps the meaning of words to that
of documents is a central challenge for researchers in Natural Language
Processing and Information Retrieval. We introduce a model that is able to
represent the meaning of documents by embedding them in a low dimensional
vector space, while preserving distinctions of word and sentence order crucial
for capturing nuanced semantics. Our model is based on an extended Dynamic
Convolution Neural Network, which learns convolution filters at both the
sentence and document level, hierarchically learning to capture and compose low
level lexical features into high level semantic concepts. We demonstrate the
effectiveness of this model on a range of document modelling tasks, achieving
strong results with no feature engineering and with a more compact model.
Inspired by recent advances in visualising deep convolution networks for
computer vision, we present a novel visualisation technique for our document
networks which not only provides insight into their learning process, but also
can be interpreted to produce a compelling automatic summarisation system for
texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3835</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3835</id><created>2014-06-15</created><updated>2014-06-20</updated><authors><author><keyname>Nakajima</keyname><forenames>Kohei</forenames></author></authors><title>Dynamics Underneath Symbols: A Case Study in Autonomous Agents</title><categories>nlin.AO cs.MA</categories><comments>15 pages, 9 figures; Color figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our cognition is structuring the informational layer, consisting of
perception, anticipation, and action, and it should also be sustained on a
physical basis. In this paper, we aim to explore the relationship between the
informational layer and the physical layer from a dynamical systems point of
view. As an example, the fluctuation of choice is investigated by using a
simulated agent. By setting a T-maze, the agent should choose one arm of the
maze if a corresponding token is presented. We prepared two types of tokens,
corresponding to the left and right arm of the maze. After training the network
of the agent to successfully choose the corresponding arm, we presented two
tokens simultaneously to the agent and observed its behavior. As a result, we
found several behaviors, which are difficult to speculate on from a case in
which only a single token is presented to the agent. Detailed analyses and the
implications of the model are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3837</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3837</id><created>2014-06-15</created><authors><author><keyname>Bresson</keyname><forenames>Xavier</forenames></author><author><keyname>Hu</keyname><forenames>Huiyi</forenames></author><author><keyname>Laurent</keyname><forenames>Thomas</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>von Brecht</keyname><forenames>James</forenames></author></authors><title>An Incremental Reseeding Strategy for Clustering</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a simple and easily parallelizable algorithm for
multiway graph partitioning. The algorithm alternates between three basic
components: diffusing seed vertices over the graph, thresholding the diffused
seeds, and then randomly reseeding the thresholded clusters. We demonstrate
experimentally that the proper combination of these ingredients leads to an
algorithm that achieves state-of-the-art performance in terms of cluster purity
on standard benchmarks datasets. Moreover, the algorithm runs an order of
magnitude faster than the other algorithms that achieve comparable results in
terms of accuracy. We also describe a coarsen, cluster and refine approach
similar to GRACLUS and METIS that removes an additional order of magnitude from
the runtime of our algorithm while still maintaining competitive accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3838</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3838</id><created>2014-06-15</created><authors><author><keyname>Liu</keyname><forenames>Paul</forenames></author><author><keyname>Lu</keyname><forenames>Daniel</forenames></author></authors><title>A fast 25/6-approximation for the minimum unit disk cover problem</title><categories>cs.CG</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a point set P in 2D, the problem of finding the smallest set of unit
disks that cover all of P is NP-hard. We present a simple algorithm for this
problem with an approximation factor of 25/6 in the Euclidean norm and 2 in the
max norm, by restricting the disk centers to lie on parallel lines. The run
time and space of this algorithm is O(n log n) and O(n) respectively. This
algorithm extends to any Lp norm and is asymptotically faster than known
alternative approximation algorithms for the same approximation factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3840</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3840</id><created>2014-06-15</created><authors><author><keyname>Lattimore</keyname><forenames>Tor</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author></authors><title>Optimal Resource Allocation with Semi-Bandit Feedback</title><categories>cs.LG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a sequential resource allocation problem involving a fixed number of
recurring jobs. At each time-step the manager should distribute available
resources among the jobs in order to maximise the expected number of completed
jobs. Allocating more resources to a given job increases the probability that
it completes, but with a cut-off. Specifically, we assume a linear model where
the probability increases linearly until it equals one, after which allocating
additional resources is wasteful. We assume the difficulty of each job is
unknown and present the first algorithm for this problem and prove upper and
lower bounds on its regret. Despite its apparent simplicity, the problem has a
rich structure: we show that an appropriate optimistic algorithm can improve
its learning speed dramatically beyond the results one normally expects for
similar problems as the problem becomes resource-laden.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3843</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3843</id><created>2014-06-15</created><authors><author><keyname>Zhang</keyname><forenames>Yichuan</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian
  Hierarchical Models</title><categories>stat.CO cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling from hierarchical Bayesian models is often difficult for MCMC
methods, because of the strong correlations between the model parameters and
the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC)
methods have significant potential advantages in this setting, but are
computationally expensive. We introduce a new RMHMC method, which we call
semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass
matrix that allows the joint Hamiltonian over model parameters and
hyperparameters to decompose into two simpler Hamiltonians. This structure is
exploited by a new integrator which we call the alternating blockwise leapfrog
algorithm. The resulting method can mix faster than simpler Gibbs sampling
while being simpler and more efficient than previous instances of RMHMC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3848</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3848</id><created>2014-06-15</created><authors><author><keyname>Radianti</keyname><forenames>Jaziar</forenames><affiliation>LIG</affiliation></author><author><keyname>Dugdale</keyname><forenames>Julie</forenames><affiliation>LIG</affiliation></author><author><keyname>Gonzalez</keyname><forenames>Jose J.</forenames></author><author><keyname>Granmo</keyname><forenames>Ole-Christoffer</forenames></author></authors><title>Smartphone sensing platform for emergency management</title><categories>cs.CY cs.MA</categories><comments>11th International Conference on Information Systems for Crisis
  Response and Management ISCRAM2014 (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasingly sophisticated sensors supported by modern smartphones open
up novel research opportunities, such as mobile phone sensing. One of the most
challenging of these research areas is context-aware and activity recognition.
The SmartRescue project takes advantage of smartphone sensing, processing and
communication capabilities to monitor hazards and track people in a disaster.
The goal is to help crisis managers and members of the public in early hazard
detection, prediction, and in devising risk-minimizing evacuation plans when
disaster strikes. In this paper we suggest a novel smartphone-based
communication framework. It uses specific machine learning techniques that
intelligently process sensor readings into useful information for the crisis
responders. Core to the framework is a content-based publish-subscribe
mechanism that allows flexible sharing of sensor data and computation results.
We also evaluate a preliminary implementation of the platform, involving a
smartphone app that reads and shares mobile phone sensor data for activity
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3852</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3852</id><created>2014-06-15</created><updated>2015-05-27</updated><authors><author><keyname>Bounliphone</keyname><forenames>Wacha</forenames><affiliation>E3S</affiliation></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames><affiliation>E3S</affiliation></author><author><keyname>Tenenhaus</keyname><forenames>Arthur</forenames><affiliation>E3S</affiliation></author><author><keyname>Blaschko</keyname><forenames>Matthew</forenames><affiliation>INRIA Saclay - Ile de France, CVN</affiliation></author></authors><title>A low variance consistent test of relative dependency</title><categories>stat.ML cs.LG stat.CO</categories><comments>International Conference on Machine Learning, Jul 2015, Lille, France</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a novel non-parametric statistical hypothesis test of relative
dependence between a source variable and two candidate target variables. Such a
test enables us to determine whether one source variable is significantly more
dependent on a first target variable or a second. Dependence is measured via
the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of
empirical dependence measures (source-target 1, source-target 2). We test
whether the first dependence measure is significantly larger than the second.
Modeling the covariance between these HSIC statistics leads to a provably more
powerful test than the construction of independent HSIC statistics by
sub-sampling. The resulting test is consistent and unbiased, and (being based
on U-statistics) has favorable convergence properties. The test can be computed
in quadratic time, matching the computational complexity of standard empirical
HSIC estimators. The effectiveness of the test is demonstrated on several
real-world problems: we identify language groups from a multilingual corpus,
and we prove that tumor location is more dependent on gene expression than
chromosomal imbalances. Source code is available for download at
https://github.com/wbounliphone/reldep.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3855</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3855</id><created>2014-06-15</created><authors><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author><author><keyname>Clark</keyname><forenames>Eric M.</forenames></author><author><keyname>Desu</keyname><forenames>Suma</forenames></author><author><keyname>Frank</keyname><forenames>Morgan R.</forenames></author><author><keyname>Reagan</keyname><forenames>Andrew J.</forenames></author><author><keyname>Williams</keyname><forenames>Jake Ryland</forenames></author><author><keyname>Mitchell</keyname><forenames>Lewis</forenames></author><author><keyname>Harris</keyname><forenames>Kameron Decker</forenames></author><author><keyname>Kloumann</keyname><forenames>Isabel M.</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Megerdoomian</keyname><forenames>Karine</forenames></author><author><keyname>McMahon</keyname><forenames>Matthew T.</forenames></author><author><keyname>Tivnan</keyname><forenames>Brian F.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author></authors><title>Human language reveals a universal positivity bias</title><categories>physics.soc-ph cs.CL cs.SI</categories><comments>Manuscript: 7 pages, 4 figures; Supplementary Material: 49 pages, 43
  figures, 6 tables. Online appendices available at
  http://www.uvm.edu/storylab/share/papers/dodds2014a/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using human evaluation of 100,000 words spread across 24 corpora in 10
languages diverse in origin and culture, we present evidence of a deep imprint
of human sociality in language, observing that (1) the words of natural human
language possess a universal positivity bias; (2) the estimated emotional
content of words is consistent between languages under translation; and (3)
this positivity bias is strongly independent of frequency of word usage.
Alongside these general regularities, we describe inter-language variations in
the emotional spectrum of languages which allow us to rank corpora. We also
show how our word evaluations can be used to construct physical-like
instruments for both real-time and offline measurement of the emotional content
of large-scale texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3860</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3860</id><created>2014-06-15</created><authors><author><keyname>Gordon</keyname><forenames>Taylor</forenames></author></authors><title>The Minimum Bends in a Polyline Drawing with Fixed Vertex Locations</title><categories>cs.CG math.CO</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider embeddings of planar graphs in $R^2$ where vertices map to points
and edges map to polylines. We refer to such an embedding as a polyline
drawing, and ask how few bends are required to form such a drawing for an
arbitrary planar graph. It has long been known that even when the vertex
locations are completely fixed, a planar graph admits a polyline drawing where
edges bend a total of $O(n^2)$ times. Our results show that this number of
bends is optimal. In particular, we show that $\Omega(n^2)$ total bends is
required to form a polyline drawing on any set of fixed vertex locations for
almost all planar graphs. This result generalizes all previously known lower
bounds, which only applied to convex point sets, and settles 2 open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3861</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3861</id><created>2014-06-15</created><authors><author><keyname>Luo</keyname><forenames>X.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Zu</keyname><forenames>K.</forenames></author></authors><title>Successive Optimization Tomlinson-Harashima Precoding Strategies for
  Physical-Layer Security in Wireless Networks</title><categories>cs.IT math.IT</categories><comments>7 figures, 6 pages, ITS, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose successive optimization non-linear precoding
strategies for physical-layer security in wireless networks. We also
investigate different precoding techniques for multi-user MIMO systems under
various conditions of channel state information (CSI) between the access point
and the users and the eavesdroppers. A non-linear precoding technique based on
Successive Optimization Tomlinson-Harashima Precoding (SO-THP) and Simplified
Generalized Matrix Inversion (S-GMI) technique is proposed along with a
strategy for injecting artificial noise prior to transmission. Simulation
results show that the proposed SO-THP+S-GMI precoding technique outperforms
existing non-linear and linear precoding algorithms in terms of BER and secrecy
rate performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3870</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3870</id><created>2014-06-15</created><authors><author><keyname>Exman</keyname><forenames>Iaakov</forenames></author><author><keyname>Krepch</keyname><forenames>Alex</forenames></author></authors><title>An Anti_Turing Test: Reduced Variables for Social Network Friends'
  Recommendations</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>11 pages, 4 figures, Extended version of paper originally published
  in the SKY International Workshop on Software Knowledge, September 2013</comments><acm-class>H.3.3</acm-class><doi>10.5220/0004641600550061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A routine activity of social networks servers is to recommend candidate
friends that one may know and stimulate addition of these people to one's
contacts. An intriguing issue is how these recommendation lists are composed.
This work investigates the main variables involved in the recommendation
activity, in order to reproduce these lists including its time dependent
characteristics. We propose relevant algorithms. Besides conventional
approaches, such as friend_of_a_friend, two techniques of importance have not
been emphasized in previous works: randomization and direct use of
interestingness criteria. An automatic software tool to implement these
techniques is proposed. Its architecture and implementation are discussed.
After a preliminary analysis of actual data collected from social networks, the
tool is used to simulate social network friends' recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3876</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3876</id><created>2014-06-15</created><authors><author><keyname>Jones</keyname><forenames>Shawn M.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author><author><keyname>Shankar</keyname><forenames>Harihar</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author></authors><title>Bringing Web Time Travel to MediaWiki: An Assessment of the Memento
  MediaWiki Extension</title><categories>cs.DL</categories><comments>23 pages, 18 figures, 9 tables, 17 listings</comments><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have implemented the Memento MediaWiki Extension Version 2.0, which brings
the Memento Protocol to MediaWiki, used by Wikipedia and the Wikimedia
Foundation. Test results show that the extension has a negligible impact on
performance. Two 302 status code datetime negotiation patterns, as defined by
Memento, have been examined for the extension: Pattern 1.1, which requires 2
requests, versus Pattern 2.1, which requires 3 requests. Our test results and
mathematical review find that, contrary to intuition, Pattern 2.1 performs
better than Pattern 1.1 due to idiosyncrasies in MediaWiki. In addition to
implementing Memento, Version 2.0 allows administrators to choose the optional
200-style datetime negotiation Pattern 1.2 instead of Pattern 2.1. It also
permits administrators the ability to have the Memento MediaWiki Extension
return full HTTP 400 and 500 status codes rather than using standard MediaWiki
error pages. Finally, version 2.0 permits administrators to turn off
recommended Memento headers if desired. Seeing as much of our work focuses on
producing the correct revision of a wiki page in response to a user's datetime
input, we also examine the problem of finding the correct revisions of the
embedded resources, including images, stylesheets, and JavaScript; identifying
the issues and discussing whether or not MediaWiki must be changed to support
this functionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3877</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3877</id><created>2014-06-15</created><updated>2014-07-16</updated><authors><author><keyname>Pu</keyname><forenames>Fuan</forenames></author><author><keyname>Luo</keyname><forenames>Jian</forenames></author><author><keyname>Zhang</keyname><forenames>Yulai</forenames></author><author><keyname>Luo</keyname><forenames>Guiming</forenames></author></authors><title>Argument Ranking with Categoriser Function</title><categories>cs.AI</categories><doi>10.1007/978-3-319-12096-6_26</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recently, ranking-based semantics is proposed to rank-order arguments from
the most acceptable to the weakest one(s), which provides a graded assessment
to arguments. In general, the ranking on arguments is derived from the strength
values of the arguments. Categoriser function is a common approach that assigns
a strength value to a tree of arguments. When it encounters an argument system
with cycles, then the categoriser strength is the solution of the non-linear
equations. However, there is no detail about the existence and uniqueness of
the solution, and how to find the solution (if exists). In this paper, we will
cope with these issues via fixed point technique. In addition, we define the
categoriser-based ranking semantics in light of categoriser strength, and
investigate some general properties of it. Finally, the semantics is shown to
satisfy some of the axioms that a ranking-based semantics should satisfy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3882</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3882</id><created>2014-06-15</created><authors><author><keyname>Noma</keyname><forenames>Yui</forenames></author><author><keyname>Konoshima</keyname><forenames>Makiko</forenames></author></authors><title>Eclipse Hashing: Alexandrov Compactification and Hashing with
  Hyperspheres for Fast Similarity Search</title><categories>cs.IR</categories><comments>10 pages, 11 figures</comments><acm-class>H.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The similarity searches that use high-dimensional feature vectors consisting
of a vast amount of data have a wide range of application. One way of
conducting a fast similarity search is to transform the feature vectors into
binary vectors and perform the similarity search by using the Hamming distance.
Such a transformation is a hashing method, and the choice of hashing function
is important. Hashing methods using hyperplanes or hyperspheres are proposed.
One study reported here is inspired by Spherical LSH, and we use hypersperes to
hash the feature vectors. Our method, called Eclipse-hashing, performs a
compactification of R^n by using the inverse stereographic projection, which is
a kind of Alexandrov compactification. By using Eclipse-hashing, one can obtain
the hypersphere-hash function without explicitly using hyperspheres. Hence, the
number of nonlinear operations is reduced and the processing time of hashing
becomes shorter. Furthermore, we also show that as a result of improving the
approximation accuracy, Eclipse-hashing is more accurate than
hyperplane-hashing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3884</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3884</id><created>2014-06-15</created><authors><author><keyname>Evangelopoulos</keyname><forenames>Georgios</forenames></author><author><keyname>Voinea</keyname><forenames>Stephen</forenames></author><author><keyname>Zhang</keyname><forenames>Chiyuan</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Learning An Invariant Speech Representation</title><categories>cs.SD cs.LG</categories><comments>CBMM Memo No. 022, 5 pages, 2 figures</comments><report-no>CBMM Memo No. 022</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition of speech, and in particular the ability to generalize and learn
from small sets of labelled examples like humans do, depends on an appropriate
representation of the acoustic input. We formulate the problem of finding
robust speech features for supervised learning with small sample complexity as
a problem of learning representations of the signal that are maximally
invariant to intraclass transformations and deformations. We propose an
extension of a theory for unsupervised learning of invariant visual
representations to the auditory domain and empirically evaluate its validity
for voiced speech sound classification. Our version of the theory requires the
memory-based, unsupervised storage of acoustic templates -- such as specific
phones or words -- together with all the transformations of each that normally
occur. A quasi-invariant representation for a speech segment can be obtained by
projecting it to each template orbit, i.e., the set of transformed signals, and
computing the associated one-dimensional empirical probability distributions.
The computations can be performed by modules of filtering and pooling, and
extended to hierarchical architectures. In this paper, we apply a single-layer,
multicomponent representation for phonemes and demonstrate improved accuracy
and decreased sample complexity for vowel classification compared to standard
spectral, cepstral and perceptual features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3889</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3889</id><created>2014-06-15</created><authors><author><keyname>Hu</keyname><forenames>Honggang</forenames></author></authors><title>Comments on &quot;A New Method to Compute the 2-Adic Complexity of Binary
  Sequences&quot;</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there is a very simple approach to determine the 2-adic
complexity of periodic binary sequences with ideal two-level autocorrelation.
This is the first main result by H. Xiong, L. Qu, and C. Li, IEEE Transactions
on Information Theory, vol. 60, no. 4, pp. 2399-2406, Apr. 2014, and the main
result by T. Tian and W. Qi, IEEE Transactions on Information Theory, vol. 56,
no. 1, pp. 450-454, Jan. 2010.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3895</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3895</id><created>2014-06-15</created><authors><author><keyname>Wang</keyname><forenames>Weiran</forenames></author><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author></authors><title>The Laplacian K-modes algorithm for clustering</title><categories>cs.LG stat.ME stat.ML</categories><comments>14 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In addition to finding meaningful clusters, centroid-based clustering
algorithms such as K-means or mean-shift should ideally find centroids that are
valid patterns in the input space, representative of data in their cluster.
This is challenging with data having a nonconvex or manifold structure, as with
images or text. We introduce a new algorithm, Laplacian K-modes, which
naturally combines three powerful ideas in clustering: the explicit use of
assignment variables (as in K-means); the estimation of cluster centroids which
are modes of each cluster's density estimate (as in mean-shift); and the
regularizing effect of the graph Laplacian, which encourages similar
assignments for nearby points (as in spectral clustering). The optimization
algorithm alternates an assignment step, which is a convex quadratic program,
and a mean-shift step, which separates for each cluster centroid. The algorithm
finds meaningful density estimates for each cluster, even with challenging
problems where the clusters have manifold structure, are highly nonconvex or in
high dimension. It also provides centroids that are valid patterns, truly
representative of their cluster (unlike K-means), and an out-of-sample mapping
that predicts soft assignments for a new point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3896</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3896</id><created>2014-06-15</created><authors><author><keyname>Swersky</keyname><forenames>Kevin</forenames></author><author><keyname>Snoek</keyname><forenames>Jasper</forenames></author><author><keyname>Adams</keyname><forenames>Ryan Prescott</forenames></author></authors><title>Freeze-Thaw Bayesian Optimization</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a dynamic form of Bayesian optimization for machine
learning models with the goal of rapidly finding good hyperparameter settings.
Our method uses the partial information gained during the training of a machine
learning model in order to decide whether to pause training and start a new
model, or resume the training of a previously-considered model. We specifically
tailor our method to machine learning problems by developing a novel
positive-definite covariance kernel to capture a variety of training curves.
Furthermore, we develop a Gaussian process prior that scales gracefully with
additional temporal observations. Finally, we provide an information-theoretic
framework to automate the decision process. Experiments on several common
machine learning models show that our approach is extremely effective in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3901</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3901</id><created>2014-06-16</created><authors><author><keyname>Fan</keyname><forenames>Liya</forenames></author><author><keyname>Gao</keyname><forenames>Bo</forenames></author><author><keyname>Zhang</keyname><forenames>Fa</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyong</forenames></author></authors><title>OS4M: Achieving Global Load Balance of MapReduce Workload by Scheduling
  at the Operation Level</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.0355</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of MapReduce is closely related to its load balance. Existing
works on MapReduce load balance focus on coarse-grained scheduling. This study
concerns fine-grained scheduling on MapReduce operations, with each operation
representing one invocation of the Map or Reduce function. By default,
MapReduce adopts the hash-based method to schedule Reduce operations, which
often leads to poor load balance. In addition, the copy phase of Reduce tasks
overlaps with Map tasks, which significantly hinders the progress of Map tasks
due to I/O contention. Moreover, the three phases of Reduce tasks run in
sequence, while consuming different resources, thereby under-utilizing
resources. To overcome these problems, we introduce a set of mechanisms named
OS4M (Operation Scheduling for MapReduce) to improve MapReduce's performance.
OS4M achieves load balance by collecting statistics of all Map operations, and
calculates a globally optimal schedule to distribute Reduce operations. With
OS4M, the copy phase of Reduce tasks no longer overlaps with Map tasks, and the
three phases of Reduce tasks are pipelined based on their operation loads. OS4M
has been transparently incorporated into MapReduce. Evaluations on standard
benchmarks show that OS4M's job duration can be shortened by up to 42%,
compared with a baseline of Hadoop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3906</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3906</id><created>2014-06-16</created><authors><author><keyname>Mottaghi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author><author><keyname>Parikh</keyname><forenames>Devi</forenames></author></authors><title>Human-Machine CRFs for Identifying Bottlenecks in Holistic Scene
  Understanding</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent trends in image understanding have pushed for holistic scene
understanding models that jointly reason about various tasks such as object
detection, scene recognition, shape analysis, contextual reasoning, and local
appearance based classifiers. In this work, we are interested in understanding
the roles of these different tasks in improved scene understanding, in
particular semantic segmentation, object detection and scene recognition.
Towards this goal, we &quot;plug-in&quot; human subjects for each of the various
components in a state-of-the-art conditional random field model. Comparisons
among various hybrid human-machine CRFs give us indications of how much &quot;head
room&quot; there is to improve scene understanding by focusing research efforts on
various individual tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3915</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3915</id><created>2014-06-16</created><authors><author><keyname>Mukherjee</keyname><forenames>Sankar</forenames></author><author><keyname>Mandal</keyname><forenames>Shyamal Kumar Das</forenames></author></authors><title>A Bengali HMM Based Speech Synthesis System</title><categories>cs.SD cs.CL cs.MM</categories><journal-ref>Oriental COCOSDA 2012, pp.225 259</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper presents the capability of an HMM-based TTS system to produce
Bengali speech. In this synthesis method, trajectories of speech parameters are
generated from the trained Hidden Markov Models. A final speech waveform is
synthesized from those speech parameters. In our experiments, spectral
properties were represented by Mel Cepstrum Coefficients. Both the training and
synthesis issues are investigated in this paper using annotated Bengali speech
database. Experimental evaluation depicts that the developed text-to-speech
system is capable of producing adequately natural speech in terms of
intelligibility and intonation for Bengali.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3922</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3922</id><created>2014-06-16</created><updated>2014-06-30</updated><authors><author><keyname>Soliman</keyname><forenames>Yousuf M.</forenames></author></authors><title>Personalized Medical Treatments Using Novel Reinforcement Learning
  Algorithms</title><categories>cs.LG stat.ML</categories><comments>This paper has been withdrawn by the author. Some of the work was
  taken from the work of Dr. Yair Goldberg and Dr. Michael R. Kosorok and they
  have requested for the paper to be withdrawn. arXiv admin note: v1 had
  substantial text overlap with arXiv:1202.5130, arXiv:1205.6659; and text
  overlap with arXiv:1301.2158 by other authors without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In both the fields of computer science and medicine there is very strong
interest in developing personalized treatment policies for patients who have
variable responses to treatments. In particular, I aim to find an optimal
personalized treatment policy which is a non-deterministic function of the
patient specific covariate data that maximizes the expected survival time or
clinical outcome. I developed an algorithmic framework to solve multistage
decision problem with a varying number of stages that are subject to censoring
in which the &quot;rewards&quot; are expected survival times. In specific, I developed a
novel Q-learning algorithm that dynamically adjusts for these parameters.
Furthermore, I found finite upper bounds on the generalized error of the
treatment paths constructed by this algorithm. I have also shown that when the
optimal Q-function is an element of the approximation space, the anticipated
survival times for the treatment regime constructed by the algorithm will
converge to the optimal treatment path. I demonstrated the performance of the
proposed algorithmic framework via simulation studies and through the analysis
of chronic depression data and a hypothetical clinical trial. The censored
Q-learning algorithm I developed is more effective than the state of the art
clinical decision support systems and is able to operate in environments when
many covariate parameters may be unobtainable or censored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3926</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3926</id><created>2014-06-16</created><authors><author><keyname>Abbasi-Yadkori</keyname><forenames>Yasin</forenames></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author></authors><title>Bayesian Optimal Control of Smoothly Parameterized Systems: The Lazy
  Posterior Sampling Algorithm</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Bayesian optimal control of a general class of smoothly
parameterized Markov decision problems. Since computing the optimal control is
computationally expensive, we design an algorithm that trades off performance
for computational efficiency. The algorithm is a lazy posterior sampling method
that maintains a distribution over the unknown parameter. The algorithm changes
its policy only when the variance of the distribution is reduced sufficiently.
Importantly, we analyze the algorithm and show the precise nature of the
performance vs. computation tradeoff. Finally, we show the effectiveness of the
method on a web server control application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3943</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3943</id><created>2014-06-16</created><authors><author><keyname>Sarvabhatla</keyname><forenames>Mrudula</forenames></author><author><keyname>Giri</keyname><forenames>M.</forenames></author><author><keyname>Vorugunti</keyname><forenames>Chandra Sekhar</forenames></author></authors><title>Cryptanalysis of Cryptanalysis and Improvement of Yan et al
  Biometric-Based Authentication Scheme for TMIS</title><categories>cs.CR</categories><comments>arXiv admin note: text overlap with arXiv:1309.4944 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote user authentication is critical requirement in Telecare Medicine
Information System (TMIS) to protect the patient personal details, security and
integrity of the critical medical records of the patient as the patient data is
transmitted over insecure public communication channel called Internet. In
2013, Yan proposed a biometric based remote user authentication scheme and
claimed that his scheme is secure. Recently, Dheerendra et al. demonstrated
some drawbacks in Yan et al scheme and proposed an improved scheme to erase the
drawbacks of Yan et al scheme. We analyze Dheerendra et al scheme and identify
that their scheme is vulnerable to off-line identity guessing attack, and on
successfully mounting it, the attacker can perfom all major cryptographic
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3949</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3949</id><created>2014-06-16</created><authors><author><keyname>Ahmad</keyname><forenames>Jamil</forenames></author><author><keyname>Jan</keyname><forenames>Zahoor</forenames></author><author><keyname>Zia-ud-Din</keyname></author><author><keyname>Khan</keyname><forenames>Shoaib Muhammad</forenames></author></authors><title>A Fusion of Labeled-Grid Shape Descriptors with Weighted Ranking
  Algorithm for Shapes Recognition</title><categories>cs.CV</categories><journal-ref>World Applied Sciences Journal, vol. 31(6), pp. 1207-1213, 2014</journal-ref><doi>10.5829/idosi.wasj.2014.31.06.353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrieving similar images from a large dataset based on the image content has
been a very active research area and is a very challenging task. Studies have
shown that retrieving similar images based on their shape is a very effective
method. For this purpose a large number of methods exist in literature. The
combination of more than one feature has also been investigated for this
purpose and has shown promising results. In this paper a fusion based shapes
recognition method has been proposed. A set of local boundary based and region
based features are derived from the labeled grid based representation of the
shape and are combined with a few global shape features to produce a composite
shape descriptor. This composite shape descriptor is then used in a weighted
ranking algorithm to find similarities among shapes from a large dataset. The
experimental analysis has shown that the proposed method is powerful enough to
discriminate the geometrically similar shapes from the non-similar ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3954</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3954</id><created>2014-06-16</created><authors><author><keyname>Jugo</keyname><forenames>J.</forenames></author><author><keyname>Eguiraun</keyname><forenames>M.</forenames></author><author><keyname>Badillo</keyname><forenames>I.</forenames></author><author><keyname>Arredondo</keyname><forenames>I.</forenames></author><author><keyname>Piso</keyname><forenames>D.</forenames></author></authors><title>Design and Performance Analysis of a Non-Standard EPICS Fast Controller</title><categories>cs.SY physics.ins-det</categories><comments>This is the extended version of the Conference Record presented in
  the IEEE Real-Time Conference 2014, Nara, Japan. This paper has been
  submitted to the IEEE Transactions on Nuclear Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The large scientific projects present new technological challenges, such as
the distributed control over a communication network. In particular, the
middleware EPICS is the most extended communication standard in particle
accelerators. The integration of modern control architectures in these EPICS
networks is becoming common, as for example for the PXI/PXIe and xTCA hardware
alternatives. In this work, a different integration procedure for PXIe real
time controllers from National Instruments is proposed, using LabVIEW as the
design tool. This methodology is considered and its performance is analyzed by
means of a set of laboratory experiments. This control architecture is proposed
for achieving the implementation requirements of the fast controllers, which
need an important amount of computational power and signal processing
capability, with a tight real-time demand. The present work studies the
advantages and drawbacks of this methodology and presents its comprehensive
evaluation by means of a laboratory test bench, designed for the application of
systematic tests. These tests compare the proposed fast controller performance
with a similar system implemented using an standard EPICS IOC provided by the
CODAC system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3969</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3969</id><created>2014-06-16</created><authors><author><keyname>Ghosh</keyname><forenames>Siddhartha</forenames></author><author><keyname>Thamke</keyname><forenames>Sujata</forenames></author><author><keyname>S</keyname><forenames>Kalyani U. R.</forenames></author></authors><title>Translation Of Telugu-Marathi and Vice-Versa using Rule Based Machine
  Translation</title><categories>cs.CL</categories><comments>13 pages, Fourth International Conference on Advances in Computing
  and Information Technology (ACITY 2014) Delhi, India - May 2014</comments><doi>10.5121/csit.2014.4501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In todays digital world automated Machine Translation of one language to
another has covered a long way to achieve different kinds of success stories.
Whereas Babel Fish supports a good number of foreign languages and only Hindi
from Indian languages, the Google Translator takes care of about 10 Indian
languages. Though most of the Automated Machine Translation Systems are doing
well but handling Indian languages needs a major care while handling the local
proverbs/ idioms. Most of the Machine Translation system follows the direct
translation approach while translating one Indian language to other. Our
research at KMIT R&amp;D Lab found that handling the local proverbs/idioms is not
given enough attention by the earlier research work. This paper focuses on two
of the majorly spoken Indian languages Marathi and Telugu, and translation
between them. Handling proverbs and idioms of both the languages have been
given a special care, and the research outcome shows a significant achievement
in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3974</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3974</id><created>2014-06-16</created><authors><author><keyname>Cassaigne</keyname><forenames>J.</forenames></author><author><keyname>Frid</keyname><forenames>A. E.</forenames></author><author><keyname>Puzynina</keyname><forenames>S.</forenames></author><author><keyname>Zamboni</keyname><forenames>L. Q.</forenames></author></authors><title>Subword complexity and decomposition of the set of factors</title><categories>cs.FL cs.DM</categories><comments>The final publication will be available in the Springer proceedings
  of MFCS 2014</comments><msc-class>68R15,</msc-class><acm-class>G.2.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore a new hierarchy of classes of languages and infinite
words and its connection with complexity classes. Namely, we say that a
language belongs to the class $L_k$ if it is a subset of the catenation of $k$
languages $S_1\cdots S_k$, where the number of words of length $n$ in each of
$S_i$ is bounded by a constant. The class of infinite words whose set of
factors is in $L_k$ is denoted by $W_k$. In this paper we focus on the
relations between the classes $W_k$ and the subword complexity of infinite
words, which is as usual defined as the number of factors of the word of length
$n$. In particular, we prove that the class $W_{2}$ coincides with the class of
  infinite words of linear complexity. On the other hand, although the class
$W_{k}$ is included in the class of words of complexity $O(n^{k-1})$, this
inclusion is strict for $k&gt; 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3976</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3976</id><created>2014-06-16</created><authors><author><keyname>Enache</keyname><forenames>Ramona</forenames></author><author><keyname>Listenmaa</keyname><forenames>Inari</forenames></author><author><keyname>Kolachina</keyname><forenames>Prasanth</forenames></author></authors><title>Handling non-compositionality in multilingual CNLs</title><categories>cs.CL</categories><comments>CNL workshop in COLING 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe methods for handling multilingual
non-compositional constructions in the framework of GF. We specifically look at
methods to detect and extract non-compositional phrases from parallel texts and
propose methods to handle such constructions in GF grammars. We expect that the
methods to handle non-compositional constructions will enrich CNLs by providing
more flexibility in the design of controlled languages. We look at two specific
use cases of non-compositional constructions: a general-purpose method to
detect and extract multilingual multiword expressions and a procedure to
identify nominal compounds in German. We evaluate our procedure for multiword
expressions by performing a qualitative analysis of the results. For the
experiments on nominal compounds, we incorporate the detected compounds in a
full SMT pipeline and evaluate the impact of our method in machine translation
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3987</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3987</id><created>2014-06-16</created><authors><author><keyname>Kang</keyname><forenames>Juyeon</forenames></author><author><keyname>Dizier</keyname><forenames>Patrick Saint</forenames></author></authors><title>Towards an Error Correction Memory to Enhance Technical Texts Authoring
  in LELIE</title><categories>cs.CL</categories><comments>10 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we investigate and experiment the notion of error correction
memory applied to error correction in technical texts. The main purpose is to
induce relatively generic correction patterns associated with more contextual
correction recommendations, based on previously memorized and analyzed
corrections. The notion of error correction memory is developed within the
framework of the LELIE project and illustrated on the case of fuzzy lexical
items, which is a major problem in technical texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.3988</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.3988</id><created>2014-06-16</created><updated>2014-06-21</updated><authors><author><keyname>Beyene</keyname><forenames>Tewodros A.</forenames></author><author><keyname>Brockschmidt</keyname><forenames>Marc</forenames></author><author><keyname>Rybalchenko</keyname><forenames>Andrey</forenames></author></authors><title>CTL+FO Verification as Constraint Solving</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expressing program correctness often requires relating program data
throughout (different branches of) an execution. Such properties can be
represented using CTL+FO, a logic that allows mixing temporal and first-order
quantification. Verifying that a program satisfies a CTL+FO property is a
challenging problem that requires both temporal and data reasoning. Temporal
quantifiers require discovery of invariants and ranking functions, while
first-order quantifiers demand instantiation techniques. In this paper, we
present a constraint-based method for proving CTL+FO properties automatically.
Our method makes the interplay between the temporal and first-order
quantification explicit in a constraint encoding that combines recursion and
existential quantification. By integrating this constraint encoding with an
off-the-shelf solver we obtain an automatic verifier for CTL+FO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4005</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4005</id><created>2014-06-16</created><updated>2014-06-25</updated><authors><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>Arge</keyname><forenames>Lars</forenames></author><author><keyname>M&#xf8;lhave</keyname><forenames>Thomas</forenames></author><author><keyname>Revsb&#xe6;k</keyname><forenames>Morten</forenames></author><author><keyname>Yang</keyname><forenames>Jungwoo</forenames></author></authors><title>Maintaining Contour Trees of Dynamic Terrains</title><categories>cs.CG</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider maintaining the contour tree $\mathbb{T}$ of a piecewise-linear
triangulation $\mathbb{M}$ that is the graph of a time varying height function
$h: \mathbb{R}^2 \rightarrow \mathbb{R}$. We carefully describe the
combinatorial change in $\mathbb{T}$ that happen as $h$ varies over time and
how these changes relate to topological changes in $\mathbb{M}$. We present a
kinetic data structure that maintains the contour tree of $h$ over time. Our
data structure maintains certificates that fail only when $h(v)=h(u)$ for two
adjacent vertices $v$ and $u$ in $\mathbb{M}$, or when two saddle vertices lie
on the same contour of $\mathbb{M}$. A certificate failure is handled in
$O(\log(n))$ time. We also show how our data structure can be extended to
handle a set of general update operations on $\mathbb{M}$ and how it can be
applied to maintain topological persistence pairs of time varying functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4007</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4007</id><created>2014-06-16</created><authors><author><keyname>Bora</keyname><forenames>Dibya Jyoti</forenames></author><author><keyname>Gupta</keyname><forenames>Anil Kumar</forenames></author></authors><title>Impact of Exponent Parameter Value for the Partition Matrix on the
  Performance of Fuzzy C Means Algorithm</title><categories>cs.CV</categories><comments>5 pages,8 figures, 2 tables, Soft clustering, Fuzzy C Means.
  IJSRCSAMS, Volume 3, Issue 3, May 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft Clustering plays a very important rule on clustering real world data
where a data item contributes to more than one cluster. Fuzzy logic based
algorithms are always suitable for performing soft clustering tasks. Fuzzy C
Means (FCM) algorithm is a very popular fuzzy logic based algorithm. In case of
fuzzy logic based algorithm, the parameter like exponent for the partition
matrix that we have to fix for the clustering task plays a very important rule
on the performance of the algorithm. In this paper, an experimental analysis is
done on FCM algorithm to observe the impact of this parameter on the
performance of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4011</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4011</id><created>2014-06-16</created><authors><author><keyname>Bithas</keyname><forenames>Petros S.</forenames></author><author><keyname>Rontogiannis</keyname><forenames>Athanasios A.</forenames></author></authors><title>Mobile Communication Systems in the Presence of Fading/Shadowing, Noise
  and Interference</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the effects of interference on composite fading environments,
where multipath fading coexists with shadowing, are investigated. Based on some
mathematical convenient expressions for the sum of squared
$\mathcal{K}$-distributed random variables, which are derived for the first
time, important statistical metrics for the signal to interference and noise
ratio (SINR) are studied for various cases including non identical, identical
and fully correlated statistics. Furthermore, our analysis is extended to
multi-channel receivers and in particular to selection diversity (SD)
receivers, investigating two distinct cases, namely, signal-to-noise ratio
based and SINR-based SD. For all scenarios, simplified expressions are also
provided for the interference limited case, where the influence of thermal
noise is ignored. The derived expressions are used to analyze the performance,
in terms of the average bit error probability (ABEP) and the outage probability
(OP), of such systems operating over composite fading environments. A high SNR
analysis is also presented for the OP and the ABEP, giving a clear physical
insight of the system's performance in terms of the diversity and coding gains.
The analysis is accompanied by numerical evaluated results, clearly
demonstrating the usefulness of the proposed theoretical framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4020</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4020</id><created>2014-06-16</created><authors><author><keyname>Fursin</keyname><forenames>Grigori</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Dubach</keyname><forenames>Christophe</forenames><affiliation>ICSA</affiliation></author></authors><title>Community-driven reviewing and validation of publications</title><categories>cs.DL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we share our practical experience on crowdsourcing evaluation
of research artifacts and reviewing of publications since 2008. We also briefly
discuss encountered problems including reproducibility of experimental results
and possible solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4033</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4033</id><created>2014-06-16</created><authors><author><keyname>Lohn</keyname><forenames>Andrew J.</forenames></author><author><keyname>Mickel</keyname><forenames>Patrick R.</forenames></author><author><keyname>James</keyname><forenames>Conrad D.</forenames></author><author><keyname>Marinella</keyname><forenames>Matthew J.</forenames></author></authors><title>Degenerate Resistive Switching and Ultrahigh Density Storage in
  Resistive Memory</title><categories>cond-mat.mtrl-sci cond-mat.mes-hall cs.ET cs.IT math.IT</categories><doi>10.1063/1.4895526</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, in tantalum oxide resistive memories, activation power provides
a multi-level variable for information storage that can be set and read
separately from the resistance. These two state variables (resistance and
activation power) can be precisely controlled in two steps: (1) the possible
activation power states are selected by partially reducing resistance, then (2)
a subsequent partial increase in resistance specifies the resistance state and
the final activation power state. We show that these states can be precisely
written and read electrically, making this approach potentially amenable for
ultra-high density memories. We provide a theoretical explanation for
information storage and retrieval from activation power and experimentally
demonstrate information storage in a third dimension related to the change in
activation power with resistance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4041</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4041</id><created>2014-06-16</created><updated>2014-06-16</updated><authors><author><keyname>Habibulla</keyname><forenames>Yusupjan</forenames></author></authors><title>CCCP Algorithms to Minimize the Bethe free energy of 3-SAT Problem</title><categories>physics.data-an cs.AI</categories><comments>11 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The k-sat problem is a prototypical constraint satisfaction problem. There
are many algorithms to study k-sat problem, BP algorithm is famous one of them.
But BP algorithm does not converge when $\alpha$(constraint density)is bigger
than some threshold value. In this paper we use CCCP (Concave Convex Procedure)
algorithm to study 3-sat problem and we get better results than BP algorithm
that CCCP algorithm still converges when BP algorithm does not converge. Our
work almost builds on recent results by Yuille \cite{Yuille2002} who apply the
CCCP algorithm to Bethe and Kikuchi free energies and obtained two algorithms
on 2D and 3D spin glasses. Our implementation of CCCP algorithm on 3-sat
problem is some different from his implementation and we have some different
views about CCCP algorithm's some properties. Some difference of these maybe
because of CCCP algorithm have different properties and implementation process
on different problem and some others of these are related to the CCCP algorithm
itself. Our work indicates that CCCP algorithm has more learning and inference
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4047</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4047</id><created>2014-06-16</created><authors><author><keyname>Focchi</keyname><forenames>Michele</forenames></author><author><keyname>Medrano-Cerda</keyname><forenames>Gustavo A.</forenames></author><author><keyname>Boaventura</keyname><forenames>Thiago</forenames></author><author><keyname>Frigerio</keyname><forenames>Marco</forenames></author><author><keyname>Semini</keyname><forenames>Claudio</forenames></author><author><keyname>Buchli</keyname><forenames>Jonas</forenames></author><author><keyname>Caldwell</keyname><forenames>Darwin G.</forenames></author></authors><title>Robot Impedance Control and Passivity Analysis with Inner Torque and
  Velocity Feedback Loops</title><categories>cs.SY cs.RO</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Impedance control is a well-established technique to control interaction
forces in robotics. However, real implementations of impedance control with an
inner loop may suffer from several limitations. Although common practice in
designing nested control systems is to maximize the bandwidth of the inner loop
to improve tracking performance, it may not be the most suitable approach when
a certain range of impedance parameters has to be rendered. In particular, it
turns out that the viable range of stable stiffness and damping values can be
strongly affected by the bandwidth of the inner control loops (e.g. a torque
loop) as well as by the filtering and sampling frequency. This paper provides
an extensive analysis on how these aspects influence the stability region of
impedance parameters as well as the passivity of the system. This will be
supported by both simulations and experimental data. Moreover, a methodology
for designing joint impedance controllers based on an inner torque loop and a
positive velocity feedback loop will be presented. The goal of the velocity
feedback is to increase (given the constraints to preserve stability) the
bandwidth of the torque loop without the need of a complex controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4048</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4048</id><created>2014-06-16</created><authors><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Quantum finite automata: A modern introduction</title><categories>cs.FL cs.CC quant-ph</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present five examples where quantum finite automata (QFAs) outperform
their classical counterparts. This may be useful as a relatively simple
technique to introduce quantum computation concepts to computer scientists. We
also describe a modern QFA model involving superoperators that is able to
simulate all known QFA and classical finite automaton variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4053</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4053</id><created>2014-06-16</created><authors><author><keyname>Maheswaran</keyname><forenames>John</forenames></author><author><keyname>Jackowitz</keyname><forenames>Daniel</forenames></author><author><keyname>Wolinsky</keyname><forenames>David Isaac</forenames></author><author><keyname>Wang</keyname><forenames>Lining</forenames></author><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Crypto-Book: Bootstrapping Privacy Preserving Online Identities from
  Social Networks</title><categories>cs.CR</categories><comments>9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networking sites supporting federated identities offer a convenient
and increasingly popular mechanism for cross-site authentication.
Unfortunately, they also exacerbate many privacy and tracking risks. We propose
Crypto-Book, an anonymizing layer enabling cross-site authentication while
reducing these risks.
  Crypto-Book relies on a set of independently managed servers that
collectively assign each social network identity a public/private keypair. Only
an identity's owner learns all the private key shares, and can therefore
construct the private key, while all participants can obtain any user's public
key, even if the corresponding private key has yet to be retrieved. Having
obtained an appropriate key set, a user can then leverage anonymous
authentication techniques such as linkable ring signatures to log into
third-party web sites while preserving privacy.
  We have implemented a prototype of Crypto-Book and demonstrate its use with
three applications: a Wiki system, an anonymous group communication system, and
a whistleblower submission system. Our results show that for anonymity sets of
size 100, Crypto-Book login takes 0.56s for signature generation by the client,
0.38s for signature verification on the server, and requires 5.6KB of
communication bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4056</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4056</id><created>2014-06-16</created><authors><author><keyname>Curticapean</keyname><forenames>Radu</forenames></author></authors><title>Counting perfect matchings in graphs that exclude a single-crossing
  minor</title><categories>cs.DS cs.CC</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $H$ is single-crossing if it can be drawn in the plane with at most
one crossing. For any single-crossing graph $H$, we give an $O(n^4)$ time
algorithm for counting perfect matchings in graphs excluding $H$ as a minor.
The runtime can be lowered to $O(n^{1.5})$ when $G$ excludes $K_5$ or $K_{3,3}$
as a minor. This is the first generalization of an algorithm for counting
perfect matchings in $K_{3,3}$-free graphs (Little 1974, Vazirani 1989). Our
algorithm uses black-boxes for counting perfect matchings in planar graphs and
for computing certain graph decompositions. Together with an independent recent
result (Straub et al. 2014) for graphs excluding $K_5$, it is one of the first
nontrivial algorithms to not inherently rely on Pfaffian orientations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4057</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4057</id><created>2014-06-16</created><authors><author><keyname>Ranta</keyname><forenames>Aarne</forenames></author></authors><title>Embedded Controlled Languages</title><categories>cs.CL</categories><comments>7 pages, extended abstract, preprint for CNL 2014 in Galway</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by embedded programming languages, an embedded CNL (controlled
natural language) is a proper fragment of an entire natural language (its host
language), but it has a parser that recognizes the entire host language. This
makes it possible to process out-of-CNL input and give useful feedback to
users, instead of just reporting syntax errors. This extended abstract explains
the main concepts of embedded CNL implementation in GF (Grammatical Framework),
with examples from machine translation and some other ongoing work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4060</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4060</id><created>2014-06-16</created><updated>2014-12-02</updated><authors><author><keyname>Gabbay</keyname><forenames>Murdoch J.</forenames></author></authors><title>Consistency of Quine's New Foundations using nominal techniques</title><categories>math.LO cs.LO</categories><comments>This version corrects typos relative to the previous version</comments><msc-class>03E35 (Primary), 03B70 (Secondary)</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We build a model in nominal sets for TST+; typed set theory with typical
ambiguity. It is known that this is equivalent to the consistency of Quine's
New Foundations.
  The model is in the spirit of a representation theorem and is built out of
points, in the sense of filters of predicates. The model is absolute, meaning
that variables are interpreted directly as atoms of the nominal theory.
Predicates are interpreted as possibly non-equivariant sets of points, and sets
are interpreted using nominal atoms-abstraction, which behaves in this context
like a semantic counterpart to sets comprehension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4067</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4067</id><created>2014-06-16</created><authors><author><keyname>Charest</keyname><forenames>Jonathan</forenames></author><author><keyname>Beaudoin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Cadorette</keyname><forenames>Jules</forenames></author><author><keyname>Lecomte</keyname><forenames>Roger</forenames></author><author><keyname>Brunet</keyname><forenames>Charles-Antoine</forenames></author><author><keyname>Fontaine</keyname><forenames>R&#xe9;jean</forenames></author></authors><title>Automatic Channel Fault Detection and Diagnosis System for a Small
  Animal APD-Based Digital PET Scanner</title><categories>cs.AI nucl-ex</categories><comments>Results presented at 19th IEEE Real-Time conference</comments><doi>10.1109/TNS.2014.2346135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault detection and diagnosis is critical to many applications in order to
ensure proper operation and performance over time. Positron emission tomography
(PET) systems that require regular calibrations by qualified scanner operators
are good candidates for such continuous improvements. Furthermore, for scanners
employing one-to-one coupling of crystals to photodetectors to achieve enhanced
spatial resolution and contrast, the calibration task is even more daunting
because of the large number of independent channels involved. To cope with the
additional complexity of the calibration and quality control procedures of
these scanners, an intelligent system (IS) was designed to perform fault
detection and diagnosis (FDD) of malfunctioning channels. The IS can be broken
down into four hierarchical modules: parameter extraction, channel fault
detection, fault prioritization and diagnosis. Of these modules, the first two
have previously been reported and this paper focuses on fault prioritization
and diagnosis. The purpose of the fault prioritization module is to help the
operator to zero in on the faults that need immediate attention. The fault
diagnosis module will then identify the causes of the malfunction and propose
an explanation of the reasons that lead to the diagnosis. The FDD system was
implemented on a LabPET avalanche photodiode (APD)-based digital PET scanner.
Experiments demonstrated a FDD Sensitivity of 99.3 % (with a 95% confidence
interval (CI) of: [98.7, 99.9]) for major faults. Globally, the Balanced
Accuracy of the diagnosis for varying fault severities is 92 %. This suggests
the IS can greatly benefit the operators in their maintenance task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4075</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4075</id><created>2014-06-16</created><authors><author><keyname>Dolce</keyname><forenames>Francesco</forenames></author></authors><title>Regular Interval Exchange Transformations over a Quadratic Field</title><categories>cs.DM math.CO</categories><comments>13 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1305.0120</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a generalization of a result of Boshernitzan and Carroll: an
extension of Lagrange's Theorem on continued fraction expansion of quadratic
irrationals to interval exchange transformations. In order to do this, we use a
two-sided version of the Rauzy induction. In particular, we show that starting
from an interval exchange transforma- tion whose lengths are defined over a
quadratic field and applying the two-sided Rauzy induction, one can obtain only
a finite number of new transformations up to homothety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4077</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4077</id><created>2014-06-16</created><updated>2014-06-18</updated><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author></authors><title>Empirical Coordination for Joint Source-Channel Coding</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. on IT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of simultaneous transmission and coordination for
a point-to-point communication model. The encoder and decoder choose their
symbols/actions (i.e. channel input and decoder output) in order to transmit
information and to coordinate themselves. This paradigm has promising
repercussions for future decentralized networks. Exploiting the coordination
capabilities, devices can implement decentralized policies that are
coordinated. Exploiting transmission capabilities, devices choose their actions
(power control, the channel allocation) in order to encode additional embedded
information (channel state information, future allocations). The feasible
trade-offs between transmissions and coordinations are characterized using
joint probability distributions over the source and channel symbols. Encoder
and decoder implement a coding scheme such that the empirical frequency of
symbols are close to target joint probability distribution. We characterize the
set of achievable probability distribution for a point-to-point source-channel
model with strictly-causal and causal decoding. Compared to classical coding
results, the coordination requirement induces a stronger information
constraint. We determine optimal trade-offs between transmission and
coordination by maximizing a utility function over the set of achievable
probability distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4086</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4086</id><created>2014-06-16</created><authors><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>Social media as political party campaign in Indonesia</title><categories>cs.CY</categories><comments>10 pages</comments><journal-ref>Jurnal Ilmiah MATRIK, vol. 16, pp. 1-10, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media as a trend in the Internet is now used as a medium for political
campaigns. Author explores the advantages and social media implementation of
any political party in Indonesia legislative elections 2014. Author visited and
analyzed social media used by the contestants, such as: Facebook, and Twitter.
Author collected data from social media until the end of April 2014. This
article discusses the use of social media by political parties and their
features. The results of this study indicate that social media are: 1)
effective tool for current and future political campaigns, 2) reach the voters
and supporters instantly, 3) used by Political parties to show their logo/icon,
and 4) last but not least quick count results also show that political parties
which using social media as part of their campaigns won the legislative
elections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4087</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4087</id><created>2014-06-16</created><authors><author><keyname>Melentyev</keyname><forenames>Artem</forenames></author></authors><title>Java Modular Extension for Operator Overloading</title><categories>cs.PL</categories><comments>International Journal of Programming Languages and Applications,
  Volume 4, Number 2, (2014)</comments><doi>10.5121/ijpla.2014.4201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces a modular extension (plugin) for Java language compilers
and Integrated Development Environments (IDE) which adds operator overloading
feature to Java language while preserving backward compatibility.
  The extension use the idea of library-based language extensibility similar to
SugarJ. But unlike most language extensions, it works directly inside the
compiler and does not have any external preprocessors. This gives much faster
compilation, better language compatibility and support of native developer
tools (IDE, build tools).
  The extension plugs into javac and Eclipse Java compilers as well as in all
tools whose use the compilers such as IDEs (Netbeans, Eclipse, IntelliJ IDEA),
build tools (ant, maven, gradle), etc. No compiler, IDE, build tools
modification needed. Just add a jar library to classpath and/or install a
plugin to your IDE.
  The paper also discuss on how to build such Java compiler extensions.
  The extension source code is open on http://amelentev.github.io/java-oo/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4089</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4089</id><created>2014-06-16</created><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Fickus</keyname><forenames>Matthew</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Moreira</keyname><forenames>Joel</forenames></author></authors><title>Derandomizing restricted isometries via the Legendre symbol</title><categories>math.CO cs.IT math.FA math.IT math.NT</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The restricted isometry property (RIP) is an important matrix condition in
compressed sensing, but the best matrix constructions to date use randomness.
This paper leverages pseudorandom properties of the Legendre symbol to reduce
the number of random bits in an RIP matrix with Bernoulli entries. In this
regard, the Legendre symbol is not special---our main result naturally
generalizes to any small-bias sample space. We also conjecture that no random
bits are necessary for our Legendre symbol--based construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4106</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4106</id><created>2014-06-16</created><updated>2014-06-16</updated><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author></authors><title>Beautiful Structures: An Appreciation of the Contributions of Alan
  Selman</title><categories>cs.CC</categories><comments>This article will appear, in slightly different form, in the
  Complexity Theory Column of the September 2014 issue of SIGACT News</comments><msc-class>68Q15 (primary) 68-02 (secondary)</msc-class><acm-class>F.1.3; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Professor Alan Selman has been a giant in the field of computational
complexity for the past forty years. This article is an appreciation, on the
occasion of his retirement, of some of the most lovely concepts and results
that Alan has contributed to the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4110</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4110</id><created>2014-02-03</created><authors><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author><author><keyname>Kr&#xf6;tzsch</keyname><forenames>Markus</forenames></author><author><keyname>Kupke</keyname><forenames>Clemens</forenames></author><author><keyname>Magka</keyname><forenames>Despoina</forenames></author><author><keyname>Motik</keyname><forenames>Boris</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author></authors><title>Acyclicity Notions for Existential Rules and Their Application to Query
  Answering in Ontologies</title><categories>cs.DB cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 47, pages
  741-808, 2013</journal-ref><doi>10.1613/jair.3949</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answering conjunctive queries (CQs) over a set of facts extended with
existential rules is a prominent problem in knowledge representation and
databases. This problem can be solved using the chase algorithm, which extends
the given set of facts with fresh facts in order to satisfy the rules. If the
chase terminates, then CQs can be evaluated directly in the resulting set of
facts. The chase, however, does not terminate necessarily, and checking whether
the chase terminates on a given set of rules and facts is undecidable. Numerous
acyclicity notions were proposed as sufficient conditions for chase
termination. In this paper, we present two new acyclicity notions called
model-faithful acyclicity (MFA) and model-summarising acyclicity (MSA).
Furthermore, we investigate the landscape of the known acyclicity notions and
establish a complete taxonomy of all notions known to us. Finally, we show that
MFA and MSA generalise most of these notions.
  Existential rules are closely related to the Horn fragments of the OWL 2
ontology language; furthermore, several prominent OWL 2 reasoners implement CQ
answering by using the chase to materialise all relevant facts. In order to
avoid termination problems, many of these systems handle only the OWL 2 RL
profile of OWL 2; furthermore, some systems go beyond OWL 2 RL, but without any
termination guarantees. In this paper we also investigate whether various
acyclicity notions can provide a principled and practical solution to these
problems. On the theoretical side, we show that query answering for acyclic
ontologies is of lower complexity than for general ontologies. On the practical
side, we show that many of the commonly used OWL 2 ontologies are MSA, and that
the number of facts obtained by materialisation is not too large. Our results
thus suggest that principled development of materialisation-based OWL 2
reasoners is practically feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4112</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4112</id><created>2014-06-16</created><updated>2015-06-03</updated><authors><author><keyname>Fu</keyname><forenames>Zhen-Yong</forenames></author><author><keyname>Xiang</keyname><forenames>Tao</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author></authors><title>Semantic Graph for Zero-Shot Learning</title><categories>cs.CV cs.LG</categories><comments>9 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Zero-shot learning aims to classify visual objects without any training data
via knowledge transfer between seen and unseen classes. This is typically
achieved by exploring a semantic embedding space where the seen and unseen
classes can be related. Previous works differ in what embedding space is used
and how different classes and a test image can be related. In this paper, we
utilize the annotation-free semantic word space for the former and focus on
solving the latter issue of modeling relatedness. Specifically, in contrast to
previous work which ignores the semantic relationships between seen classes and
focus merely on those between seen and unseen classes, in this paper a novel
approach based on a semantic graph is proposed to represent the relationships
between all the seen and unseen class in a semantic word space. Based on this
semantic graph, we design a special absorbing Markov chain process, in which
each unseen class is viewed as an absorbing state. After incorporating one test
image into the semantic graph, the absorbing probabilities from the test data
to each unseen class can be effectively computed; and zero-shot classification
can be achieved by finding the class label with the highest absorbing
probability. The proposed model has a closed-form solution which is linear with
respect to the number of test images. We demonstrate the effectiveness and
computational efficiency of the proposed method over the state-of-the-arts on
the AwA (animals with attributes) dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4123</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4123</id><created>2014-06-14</created><authors><author><keyname>Basha</keyname><forenames>N. Md Jubair</forenames></author><author><keyname>Mohan</keyname><forenames>Chandra</forenames></author></authors><title>A strategy to identify components using clustering approach for
  component reusability</title><categories>cs.SE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1207.4938,
  arXiv:1202.5609, arXiv:1406.3727</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Component Based Software Engineering (CBSE) has played a very important role
for building larger software systems The current practices of software industry
demands development of a software within time and budget which is highly
productive. It is necessary to achieve how much effectively the software
component is reusable. To achieve this, the component identification is
mandatory. The traditional approaches are presented in the literature. However
effective reuse is still a challenging issue. In this paper, a strategy has
been proposed for the identification of a business component using clustering
methodology. This approach will be useful in identifying the reusable
components for different domains. The proposed approach has identified the
reconfigured component using the CBO measure to reduce the coupling between the
objects. By considering this proposed strategy, the productivity can be
increased in the organization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4125</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4125</id><created>2014-06-16</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Joint Cooperative Spectrum Sensing and MAC Protocol Design for
  Multi-channel Cognitive Radio Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>accepted for publication EURASIP Journal on Wireless Communications
  and Networking, 2014</comments><doi>10.1186/1687-1499-2014-101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a semi-distributed cooperative spectrum sen sing
(SDCSS) and channel access framework for multi-channel cognitive radio networks
(CRNs). In particular, we c onsider a SDCSS scheme where secondary users (SUs)
perform sensing and exchange sensing outcomes with ea ch other to locate
spectrum holes. In addition, we devise the p -persistent CSMA-based cognitive
MAC protocol integrating the SDCSS to enable efficient spectrum sharing among
SUs. We then perform throughput analysis and develop an algorithm to determine
the spectrum sensing and access parameters to maximize the throughput for a
given allocation of channel sensing sets. Moreover, we consider the spectrum
sensing set optimization problem for SUs to maxim ize the overall system
throughput. We present both exhaustive search and low-complexity greedy
algorithms to determine the sensing sets for SUs and analyze their complexity.
We also show how our design and analysis can be extended to consider reporting
errors. Finally, extensive numerical results are presented to demonstrate the
sig nificant performance gain of our optimized design framework with respect to
non-optimized designs as well as the imp acts of different protocol parameters
on the throughput performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4161</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4161</id><created>2014-06-16</created><updated>2014-07-10</updated><authors><author><keyname>Rotolo</keyname><forenames>Daniele</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Matching MEDLINE/PubMed Data with Web of Science (WoS): A Routine in R
  language</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel routine, namely medlineR, based on R-language, that
enables the user to match data from MEDLINE/PubMed with records indexed in the
ISI Web of Science (WoS) database. The matching allows exploiting the rich and
controlled vocabulary of Medical Subject Headings (MeSH) of MEDLINE/PubMed with
additional fields of WoS. The integration provides data (e.g. citation data,
list of cited reference, full list of the addresses of authors' host
organisations, WoS subject categories) to perform a variety of scientometric
analyses. This brief communication describes medlineR, the methodology on which
it relies, and the steps the user should follow to perform the matching across
the two databases. In order to specify the differences from Leydesdorff and
Opthof (2013), we conclude the brief communication by testing the routine on
the case of the &quot;Burgada Syndrome&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4162</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4162</id><created>2014-01-30</created><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author></authors><title>Band-Sweeping M-ary PSK (BS-M-PSK) Modulation and Transceiver Design</title><categories>cs.NI cs.IT math.IT</categories><comments>To appear in IEEE Potentials Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel Estimation is a major problem encountered by receiver designers for
wireless communications systems. The fading channels encountered by the system
are usually time variant for a mobile receiver. Besides, the frequency response
of the channel is frequency selective for urban environments where the delay
spread is quite large compared to the symbol duration. Estimating the channel
is essential for equalizing the received data and removing the Inter-Symbol
Interference (ISI) resulting from the dispersive channel. Hence, conventional
transceivers insert pilot symbols of known values and detect the changes in it
in order to deduce the channel response. Because these pilots carry no
information, the throughput of the system is reduced. A Novel modulation scheme
is presented in this work. The technique depends on using a carrier signal that
has no fixed frequency, the carrier tone sweeps the band dedicated for
transmission and detects the transfer function gain within the band. A carrier
signal that is Frequency Modulated (FM) by a periodic ramp signal becomes
Amplitude Modulated (AM) by the channel transfer function, and thus, the
receiver obtains an estimate for the channel response without using pilots that
decrease the systems throughput or data rate. The carrier signal itself acts as
a dynamic frequency domain pilot. The technique only works for constant energy
systems, and thus it is applied to PSK transceivers. Mathematical formulation,
transceiver design and performance analysis of the proposed modulation
technique are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4173</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4173</id><created>2014-06-16</created><updated>2015-06-04</updated><authors><author><keyname>Erdos</keyname><forenames>Dora</forenames></author><author><keyname>Ishakian</keyname><forenames>Vatche</forenames></author><author><keyname>Bestavros</keyname><forenames>Azer</forenames></author><author><keyname>Terzi</keyname><forenames>Evimaria</forenames></author></authors><title>A Divide-and-Conquer Algorithm for Betweenness Centrality</title><categories>cs.DS</categories><comments>Shorter version of this paper appeared in Siam Data Mining 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of efficiently computing the betweenness centrality of nodes has
been researched extensively. To date, the best known exact and centralized
algorithm for this task is an algorithm proposed in 2001 by Brandes. The
contribution of our paper is Brandes++, an algorithm for exact efficient
computation of betweenness centrality. The crux of our algorithm is that we
create a sketch of the graph, that we call the skeleton, by replacing subgraphs
with simpler graph structures. Depending on the underlying graph structure,
using this skeleton and by keeping appropriate summaries Brandes++ we can
achieve significantly low running times in our computations. Extensive
experimental evaluation on real life datasets demonstrate the efficacy of our
algorithm for different types of graphs. We release our code for benefit of the
research community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4175</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4175</id><created>2014-06-16</created><updated>2014-07-21</updated><authors><author><keyname>Metzler</keyname><forenames>Christopher A.</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>From Denoising to Compressed Sensing</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A denoising algorithm seeks to remove perturbations or errors from a signal.
The last three decades have seen extensive research devoted to this arena, and
as a result, today's denoisers are highly optimized algorithms that effectively
remove large amounts of additive white Gaussian noise. A compressive sensing
(CS) reconstruction algorithm seeks to recover a structured signal acquired
using a small number of randomized measurements. Typical CS reconstruction
algorithms can be cast as iteratively estimating a signal from a perturbed
observation. This paper answers a natural question: How can one effectively
employ a generic denoiser in a CS reconstruction algorithm? In response, in
this paper, we develop a denoising-based approximate message passing (D-AMP)
algorithm that is capable of high-performance reconstruction. We demonstrate
that, for an appropriate choice of denoiser, D-AMP offers state-of-the-art CS
recovery performance for natural images. We explain the exceptional performance
of D-AMP by analyzing some of its theoretical features. A critical insight in
our approach is the use of an appropriate Onsager correction term in the D-AMP
iterations, which coerces the signal perturbation at each iteration to be very
close to the white Gaussian noise that denoisers are typically designed to
remove.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4178</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4178</id><created>2014-06-16</created><updated>2014-07-05</updated><authors><author><keyname>Roman</keyname><forenames>Bogdan</forenames></author><author><keyname>Hansen</keyname><forenames>Anders</forenames></author><author><keyname>Adcock</keyname><forenames>Ben</forenames></author></authors><title>On asymptotic structure in compressed sensing</title><categories>math.FA cs.IT math.IT</categories><comments>10 pages, 13 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper demonstrates how new principles of compressed sensing, namely
asymptotic incoherence, asymptotic sparsity and multilevel sampling, can be
utilised to better understand underlying phenomena in practical compressed
sensing and improve results in real-world applications. The contribution of the
paper is fourfold:
  First, it explains how the sampling strategy depends not only on the signal
sparsity but also on its structure, and shows how to design effective sampling
strategies utilising this.
  Second, it demonstrates that the optimal sampling strategy and the efficiency
of compressed sensing also depends on the resolution of the problem, and shows
how this phenomenon markedly affects compressed sensing results and how to
exploit it.
  Third, as the new framework also fits analog (infinite dimensional) models
that govern many inverse problems in practice, the paper describes how it can
be used to yield substantial improvements.
  Fourth, by using multilevel sampling, which exploits the structure of the
signal, the paper explains how one can outperform random Gaussian/Bernoulli
sampling even when the classical $l^1$ recovery algorithm is replaced by
modified algorithms which aim to exploit structure such as model based or
Bayesian compressed sensing or approximate message passaging. This final
observation raises the question whether universality is desirable even when
such matrices are applicable.
  Examples of practical applications investigated in this paper include
Magnetic Resonance Imaging (MRI), Electron Microscopy (EM), Compressive Imaging
(CI) and Fluorescence Microscopy (FM). For the latter, a new compressed sensing
approach is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4197</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4197</id><created>2014-06-16</created><authors><author><keyname>Furcy</keyname><forenames>David</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author></authors><title>Scaled pier fractals do not strictly self-assemble</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A \emph{pier fractal} is a discrete self-similar fractal whose generator
contains at least one \emph{pier}, that is, a member of the generator with
exactly one adjacent point. Tree fractals and pinch-point fractals are special
cases of pier fractals. In this paper, we study \emph{scaled pier fractals},
where a \emph{scaled fractal} is the shape obtained by replacing each point in
the original fractal by a $c \times c$ block of points, for some $c \in
\mathbb{Z}^+$. We prove that no scaled discrete self-similar pier fractal
strictly self-assembles, at any temperature, in Winfree's abstract Tile
Assembly Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4200</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4200</id><created>2014-06-16</created><updated>2014-06-19</updated><authors><author><keyname>Bui</keyname><forenames>Hung Hai</forenames></author><author><keyname>Huynh</keyname><forenames>Tuyen N.</forenames></author><author><keyname>Sontag</keyname><forenames>David</forenames></author></authors><title>Lifted Tree-Reweighted Variational Inference</title><categories>cs.AI stat.ML</categories><comments>In: UAI (Uncertainty in Artificial Intelligence) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze variational inference for highly symmetric graphical models such
as those arising from first-order probabilistic models. We first show that for
these graphical models, the tree-reweighted variational objective lends itself
to a compact lifted formulation which can be solved much more efficiently than
the standard TRW formulation for the ground graphical model. Compared to
earlier work on lifted belief propagation, our formulation leads to a convex
optimization problem for lifted marginal inference and provides an upper bound
on the partition function. We provide two approaches for improving the lifted
TRW upper bound. The first is a method for efficiently computing maximum
spanning trees in highly symmetric graphs, which can be used to optimize the
TRW edge appearance probabilities. The second is a method for tightening the
relaxation of the marginal polytope using lifted cycle inequalities and novel
exchangeable cluster consistency constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4201</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4201</id><created>2014-06-16</created><authors><author><keyname>Fazlyab</keyname><forenames>Mahyar</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Robust Topology Identification and Control of LTI Networks</title><categories>math.OC cs.SY math.DS</categories><comments>5 double column pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports a robust scheme for topology identification and control of
networks running on linear dynamics. In the proposed method, the unknown
network is enforced to asymptotically follow a reference dynamics using the
combination of Lyapunov based adaptive feedback input and sliding mode control.
The adaptive part controls the dynamics by learning the network structure,
while the sliding mode part rejects the input uncertainty. Simulation studies
are presented in several scenarios (detection of link failure, tracking time
varying topology, achieving dynamic synchronization) to give support to
theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4203</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4203</id><created>2014-06-16</created><authors><author><keyname>Babbush</keyname><forenames>Ryan</forenames></author><author><keyname>Denchev</keyname><forenames>Vasil</forenames></author><author><keyname>Ding</keyname><forenames>Nan</forenames></author><author><keyname>Isakov</keyname><forenames>Sergei</forenames></author><author><keyname>Neven</keyname><forenames>Hartmut</forenames></author></authors><title>Construction of non-convex polynomial loss functions for training a
  binary classifier with quantum annealing</title><categories>cs.LG quant-ph</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum annealing is a heuristic quantum algorithm which exploits quantum
resources to minimize an objective function embedded as the energy levels of a
programmable physical system. To take advantage of a potential quantum
advantage, one needs to be able to map the problem of interest to the native
hardware with reasonably low overhead. Because experimental considerations
constrain our objective function to take the form of a low degree PUBO
(polynomial unconstrained binary optimization), we employ non-convex loss
functions which are polynomial functions of the margin. We show that these loss
functions are robust to label noise and provide a clear advantage over convex
methods. These loss functions may also be useful for classical approaches as
they compile to regularized risk expressions which can be evaluated in constant
time with respect to the number of training examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4205</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4205</id><created>2014-06-16</created><authors><author><keyname>Schultz</keyname><forenames>Peter F.</forenames></author><author><keyname>Paiton</keyname><forenames>Dylan M.</forenames></author><author><keyname>Lu</keyname><forenames>Wei</forenames></author><author><keyname>Kenyon</keyname><forenames>Garrett T.</forenames></author></authors><title>Replicating Kernels with a Short Stride Allows Sparse Reconstructions
  with Fewer Independent Kernels</title><categories>q-bio.QM cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In sparse coding it is common to tile an image into nonoverlapping patches,
and then use a dictionary to create a sparse representation of each tile
independently. In this situation, the overcompleteness of the dictionary is the
number of dictionary elements divided by the patch size. In deconvolutional
neural networks (DCNs), dictionaries learned on nonoverlapping tiles are
replaced by a family of convolution kernels. Hence adjacent points in the
feature maps (V1 layers) have receptive fields in the image that are
translations of each other. The translational distance is determined by the
dimensions of V1 in comparison to the dimensions of the image space. We refer
to this translational distance as the stride.
  We implement a type of DCN using a modified Locally Competitive Algorithm
(LCA) to investigate the relationship between the number of kernels, the
stride, the receptive field size, and the quality of reconstruction. We find,
for example, that for 16x16-pixel receptive fields, using eight kernels and a
stride of 2 leads to sparse reconstructions of comparable quality as using 512
kernels and a stride of 16 (the nonoverlapping case). We also find that for a
given stride and number of kernels, the patch size does not significantly
affect reconstruction quality. Instead, the learned convolution kernels have a
natural support radius independent of the patch size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4211</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4211</id><created>2014-06-16</created><authors><author><keyname>Bourreau</keyname><forenames>Pierre</forenames></author><author><keyname>Poibeau</keyname><forenames>Thierry</forenames></author></authors><title>Mapping the Economic Crisis: Some Preliminary Investigations</title><categories>cs.CL</categories><comments>Technical paper describing the Lattice submission to the 2014
  PoliInformatics Unshared task</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe our contribution to the PoliInformatics 2014
Challenge on the 2007-2008 financial crisis. We propose a state of the art
technique to extract information from texts and provide different
representations, giving first a static overview of the domain and then a
dynamic representation of its main evolutions. We show that this strategy
provides a practical solution to some recent theories in social sciences that
are facing a lack of methods and tools to automatically extract information
from natural language texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4212</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4212</id><created>2014-06-16</created><authors><author><keyname>Mart&#xed;nez</keyname><forenames>F. E. Brochero</forenames></author></authors><title>Number of minimal cyclic codes with given length and dimension</title><categories>cs.IT math.IT</categories><msc-class>20C05 (primary) and 16S34(secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we count the quantity of minimal cyclic codes of length $n$
and dimension $k$ over a finite field $\mathbb F_q$, in the case when the prime
factors of $n$ satisfy a special condition. This problem is equivalent to count
the quantity of irreducible factors of $x^n-1\in \mathbb F_q[x]$ of degree $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4216</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4216</id><created>2014-06-16</created><updated>2015-05-06</updated><authors><author><keyname>Liao</keyname><forenames>Shengcai</forenames></author><author><keyname>Hu</keyname><forenames>Yang</forenames></author><author><keyname>Zhu</keyname><forenames>Xiangyu</forenames></author><author><keyname>Li</keyname><forenames>Stan Z.</forenames></author></authors><title>Person Re-identification by Local Maximal Occurrence Representation and
  Metric Learning</title><categories>cs.CV</categories><comments>This paper has been accepted by CVPR 2015. For source codes and
  extracted features please visit
  http://www.cbsr.ia.ac.cn/users/scliao/projects/lomo_xqda/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person re-identification is an important technique towards automatic search
of a person's presence in a surveillance video. Two fundamental problems are
critical for person re-identification, feature representation and metric
learning. An effective feature representation should be robust to illumination
and viewpoint changes, and a discriminant metric should be learned to match
various person images. In this paper, we propose an effective feature
representation called Local Maximal Occurrence (LOMO), and a subspace and
metric learning method called Cross-view Quadratic Discriminant Analysis
(XQDA). The LOMO feature analyzes the horizontal occurrence of local features,
and maximizes the occurrence to make a stable representation against viewpoint
changes. Besides, to handle illumination variations, we apply the Retinex
transform and a scale invariant texture operator. To learn a discriminant
metric, we propose to learn a discriminant low dimensional subspace by
cross-view quadratic discriminant analysis, and simultaneously, a QDA metric is
learned on the derived subspace. We also present a practical computation method
for XQDA, as well as its regularization. Experiments on four challenging person
re-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show
that the proposed method improves the state-of-the-art rank-1 identification
rates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4235</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4235</id><created>2014-06-17</created><authors><author><keyname>Ye</keyname><forenames>Shunyuan</forenames></author><author><keyname>Shen</keyname><forenames>Yanming</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra S.</forenames></author></authors><title>Distributed Scheduling Algorithms for Crosspoint-Buffered Switches</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1112.4214</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the rapid increase in traffic, greater demands have been put on
high-speed switching systems. Such systems have to simultaneously meet several
constraints, e.g., high throughput, low delay and low complexity. This makes it
challenging to design an efficient scheduling algorithm, and has consequently
drawn considerable research interest. However, previous results either cannot
provide a $100\%$ throughput guarantee without a speedup, or require a complex
centralized scheduler. In this paper, we design a {\it distributed} $100 \%$
throughput algorithm for crosspoint buffered switches, called DISQUO, with very
limited message passing. We prove that DISQUO can achieve $100\%$ throughput
for any admissible Bernoulli traffic, with a low time complexity of $O(1)$ per
port. To the best of our knowledge, it is the first distributed algorithm that
can provide a $100\%$ throughput for a crosspoint buffered switch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4237</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4237</id><created>2014-06-17</created><authors><author><keyname>J</keyname><forenames>Eswari.</forenames></author><author><keyname>Jeyadevi</keyname><forenames>Dr. S.</forenames></author></authors><title>An Evolutionary Approach for Optimal Citing and Sizing of Micro-Grid in
  Radial Distribution Systems</title><categories>cs.NE</categories><journal-ref>J.Eswari , Dr.S.Jeyadevi. &quot;An Evolutionary Approach for Optimal
  Citing and Sizing of Micro-Grid in Radial Distribution Systems&quot;,
  International Journal of Engineering Trends and Technology (IJETT),
  V11(9),429-433 May 2014</journal-ref><doi>10.14445/22315381/IJETT-V11P286</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This Paper presents the methodology of penetration of Micro-Grids (MG) in the
radial distribution system (RDS). The aim of this paper is to minimize a total
real power loss that descends the performance of the radial distribution system
by integrating various renewable resources as Distributed Generation (DG). The
combination of different types of renewable energy resources contributes a
sustainable MG. These resources are optimally sized and located using
evolutionary approach in various penetration levels. The optimal solutions are
experimented with IEEE 33 radial distribution system using Particle Swarm
Optimization (PSO) technique. The results are quite promising and authenticate
its potential to solve problem in radial distribution system effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4248</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4248</id><created>2014-06-17</created><updated>2016-01-07</updated><authors><author><keyname>Gimbert</keyname><forenames>Hugo</forenames></author><author><keyname>Renault</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Sorin</keyname><forenames>Sylvain</forenames></author><author><keyname>Venel</keyname><forenames>Xavier</forenames></author><author><keyname>Zielonka</keyname><forenames>Wies&#x142;aw</forenames></author></authors><title>On values of repeated games with signals</title><categories>math.OC cs.GT</categories><comments>Published at http://dx.doi.org/10.1214/14-AAP1095 in the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP1095</report-no><journal-ref>Annals of Applied Probability 2016, Vol. 26, No. 1, pp. 402-424</journal-ref><doi>10.1214/14-AAP1095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the existence of different notions of value in two-person zero-sum
repeated games where the state evolves and players receive signals. We provide
some examples showing that the limsup value (and the uniform value) may not
exist in general. Then we show the existence of the value for any Borel payoff
function if the players observe a public signal including the actions played.
We also prove two other positive results without assumptions on the signaling
structure: the existence of the $\sup$ value in any game and the existence of
the uniform value in recursive games with nonnegative payoffs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4249</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4249</id><created>2014-06-17</created><updated>2014-11-19</updated><authors><author><keyname>Almagor</keyname><forenames>Shaull</forenames></author><author><keyname>Boker</keyname><forenames>Udi</forenames></author><author><keyname>Kupferman</keyname><forenames>Orna</forenames></author></authors><title>Discounting in LTL</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there is growing need and interest in formalizing and
reasoning about the quality of software and hardware systems. As opposed to
traditional verification, where one handles the question of whether a system
satisfies, or not, a given specification, reasoning about quality addresses the
question of \emph{how well} the system satisfies the specification. One
direction in this effort is to refine the &quot;eventually&quot; operators of temporal
logic to {\em discounting operators}: the satisfaction value of a specification
is a value in $[0,1]$, where the longer it takes to fulfill eventuality
requirements, the smaller the satisfaction value is.
  In this paper we introduce an augmentation by discounting of Linear Temporal
Logic (LTL), and study it, as well as its combination with propositional
quality operators. We show that one can augment LTL with an arbitrary set of
discounting functions, while preserving the decidability of the model-checking
problem. Further augmenting the logic with unary propositional quality
operators preserves decidability, whereas adding an average-operator makes some
problems undecidable. We also discuss the complexity of the problem, as well as
various extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4259</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4259</id><created>2014-06-17</created><authors><author><keyname>Felicetti</keyname><forenames>Luca</forenames></author><author><keyname>Femminella</keyname><forenames>Mauro</forenames></author><author><keyname>Reali</keyname><forenames>Gianluca</forenames></author><author><keyname>Nakano</keyname><forenames>Tadashi</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>TCP-like molecular communications</title><categories>cs.ET cs.NI</categories><comments>Accepted for publication, IEEE Journal on Selected Areas in
  Communications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a communication protocol between a pair of
biological nanomachines, transmitter and receiver, built upon molecular
communications in an aqueous environment. In our proposal, the receiver, acting
as a control node, sends a connection setup signal to the transmitter, which
stokes molecules, to start molecule transmission. The molecules transmitted by
the transmitter propagate in the environment and are absorbed by the receiver
through its receptors. When the receiver absorbs the desired quantity of
molecules, it releases a tear-down signal to notify the transmitter to stop the
transmission. The proposed protocol implements a bidirectional communication by
using a number of techniques originally designed for the TCP. In fact, the
proposed protocol is connection-oriented, and uses the TCP-like probing to find
a suitable transmission rate between transmitter and receiver so as to avoid
receiver congestion. Unlike the TCP, however, explicit acknowledgments are not
used, since they would degrade the communication throughput due to the large
delay, a characteristic feature of molecular communications. Thus, the proposed
protocol uses implicit acknowledgments, and feedback signals are sent by the
receiver to throttle the transmission rate at the transmitter, i.e., explicit
negative feedbacks. We also present the results of an extensive simulation
campaign, used to validate the proposed protocol and to properly dimension the
main protocol parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4277</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4277</id><created>2014-06-17</created><authors><author><keyname>Ernvall</keyname><forenames>Toni</forenames></author><author><keyname>Westerb&#xe4;ck</keyname><forenames>Thomas</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author></authors><title>Constructions of Optimal and Almost Optimal Locally Repairable Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, conference</comments><journal-ref>2014 VITAE (Aalborg), pages 1-5</journal-ref><doi>10.1109/VITAE.2014.6934442</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructions of optimal locally repairable codes (LRCs) in the case of
$(r+1) \nmid n$ and over small finite fields were stated as open problems for
LRCs in [I. Tamo \emph{et al.}, &quot;Optimal locally repairable codes and
connections to matroid theory&quot;, \emph{2013 IEEE ISIT}]. In this paper, these
problems are studied by constructing almost optimal linear LRCs, which are
proven to be optimal for certain parameters, including cases for which $(r+1)
\nmid n$. More precisely, linear codes for given length, dimension, and
all-symbol locality are constructed with almost optimal minimum distance.
`Almost optimal' refers to the fact that their minimum distance differs by at
most one from the optimal value given by a known bound for LRCs. In addition to
these linear LRCs, optimal LRCs which do not require a large field are
constructed for certain classes of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4285</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4285</id><created>2014-06-17</created><updated>2015-12-09</updated><authors><author><keyname>Sanchez</keyname><forenames>David</forenames></author><author><keyname>Batet</keyname><forenames>Montserrat</forenames></author></authors><title>C-sanitized: a privacy model for document redaction and sanitization</title><categories>cs.CR</categories><comments>in Journal of the Association for Information Science and Technology,
  2015</comments><doi>10.1002/asi.23363</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the current context of Information Societies, large amounts of
information are daily exchanged and/or released. The sensitive nature of much
of this information causes a serious privacy threat when documents are
uncontrollably made available to untrusted third parties. In such cases,
appropriate data protection measures should be undertaken by the responsible
organization, especially under the umbrella of current legislations on data
privacy. To do so, human experts are usually requested to redact or sanitize
document contents. To relieve this burdensome task, this paper presents a
privacy model for document redaction/sanitization, which offers several
advantages over other models available in the literature. Based on the
well-established foundations of data semantics and the information theory, our
model provides a framework to develop and implement automated and inherently
semantic redaction/sanitization tools. Moreover, contrary to ad-hoc redaction
methods, our proposal provides a priori privacy guarantees which can be
intuitively defined according to current legislations on data privacy.
Empirical tests performed within the context of several use cases illustrate
the applicability of our model and its ability to mimic the reasoning of human
sanitizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4286</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4286</id><created>2014-06-17</created><authors><author><keyname>Mittal</keyname><forenames>Sudip</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Broker Bots: Analyzing automated activity during High Impact Events on
  Twitter</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is now an established and a widely popular news medium. Be it normal
banter or a discussion on high impact events like Boston marathon blasts,
February 2014 US Icestorm, etc., people use Twitter to get updates. Twitter
bots have today become very common and acceptable. People are using them to get
updates about emergencies like natural disasters, terrorist strikes, etc.
Twitter bots provide these users a means to perform certain tasks on Twitter
that are both simple and structurally repetitive. During high impact events
these Twitter bots tend to provide time critical and comprehensive information.
We present how bots participate in discussions and augment them during high
impact events. We identify bots in high impact events for 2013: Boston blasts,
February 2014 US Icestorm, Washington Navy Yard Shooting, Oklahoma tornado, and
Cyclone Phailin. We identify bots among top tweeters by getting all such
accounts manually annotated. We then study their activity and present many
important insights. We determine the impact bots have on information diffusion
during these events and how they tend to aggregate and broker information from
various sources to different users. We also analyzed their tweets, list down
important differentiating features between bots and non bots (normal or human
accounts) during high impact events. We also show how bots are slowly moving
away from traditional API based posts towards web automation platforms like
IFTTT, dlvr.it, etc. Using standard machine learning, we proposed a methodology
to identify bots/non bots in real time during high impact events. This study
also looks into how the bot scenario has changed by comparing data from high
impact events from 2013 with data from similar type of events from 2011.
Lastly, we also go through an in-depth analysis of Twitter bots who were active
during 2013 Boston Marathon Blast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4287</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4287</id><created>2014-06-17</created><authors><author><keyname>&#x10c;ufar</keyname><forenames>Andreja</forenames></author><author><keyname>Mrhar</keyname><forenames>Ale&#x161;</forenames></author><author><keyname>Robnik-&#x160;ikonja</keyname><forenames>Marko</forenames></author></authors><title>Identifying roles of clinical pharmacy with survey evaluation</title><categories>cs.AI stat.AP</categories><msc-class>68T37</msc-class><acm-class>I.2.1; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The survey data sets are important sources of data and their successful
exploitation is of key importance for informed policy-decision making. We
present how a survey analysis approach initially developed for customer
satisfaction research in marketing can be adapted for the introduction of
clinical pharmacy services into hospital. We use two analytical approaches to
extract relevant managerial consequences. With OrdEval algorithm we first
evaluate the importance of competences for the users of clinical pharmacy and
extract their nature according to the users expectations. Next, we build a
model for predicting a successful introduction of clinical pharmacy to the
clinical departments. We the wards with the highest probability of successful
cooperation with a clinical pharmacist. We obtain useful managerially relevant
information from a relatively small sample of highly relevant respondents. We
show how the OrdEval algorithm exploits the information hidden in the ordering
of class and attribute values and their inherent correlation. Its output can be
effectively visualized and complemented with confidence intervals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4291</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4291</id><created>2014-06-17</created><authors><author><keyname>Meiklejohn</keyname><forenames>Christopher</forenames></author></authors><title>Vector Clocks in Coq: An Experience Report</title><categories>cs.DC cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report documents the process of implementing vector clocks in the Coq
proof assistant for extraction and use in the distributed Dynamo-inspired data
store, Riak. In this report, we focus on the technical challenges of using Core
Erlang code extracted from the proof assistant in a production-grade Erlang
application, as opposed to verification of the model itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4294</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4294</id><created>2014-06-17</created><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author><author><keyname>Marcelletti</keyname><forenames>Alessandra</forenames></author></authors><title>Do good actions inspire good actions in others?</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actions such as sharing food and cooperating to reach a common goal have
played a fundamental role in the evolution of human societies. These good
actions may not maximise the actor's payoff, but they maximise the other's
payoff. Consequently, their existence is puzzling for evolutionary theories.
Why should you make an effort to help others, even when no reward seems to be
at stake? Indeed, experiments typically show that humans are heterogeneous:
some may help others, while others may not. With the aim of favouring the
emergence of 'successful cultures', a number of studies has recently
investigated what mechanisms promote the evolution of a particular good action.
But still little is known about if and how good actions can spread from person
to person. For instance, does being recipient of an altruistic act increase
your probability of being cooperative with others? Plato's quote, 'Good actions
give strength to ourselves and inspire good actions in others', suggests that
is possible. We have conducted an experiment on Amazon Mechanical Turk to test
this mechanism using economic games. We have measured willingness to be
cooperative through a standard Prisoner's dilemma and willingness to act
altruistically using a binary Dictator game. In the baseline treatments, the
endowments needed to play were given by the experimenters, as usual; in the
control treatments, they came from a good action made by someone else. Across
four different comparisons and a total of 572 subjects, we have never found a
significant increase of cooperation or altruism when the endowment came from a
good action. We conclude that good actions do not necessarily inspire good
actions in others, at least in the ideal scenario of a lab experiment with
anonymous subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4296</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4296</id><created>2014-06-17</created><updated>2014-06-18</updated><authors><author><keyname>Gaidon</keyname><forenames>Adrien</forenames><affiliation>Xerox Research Center Europe, France</affiliation></author><author><keyname>Zen</keyname><forenames>Gloria</forenames><affiliation>University of Trento, Italy</affiliation></author><author><keyname>Rodriguez-Serrano</keyname><forenames>Jose A.</forenames><affiliation>Xerox Research Center Europe, France</affiliation></author></authors><title>Self-Learning Camera: Autonomous Adaptation of Object Detectors to
  Unlabeled Video Streams</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning object detectors requires massive amounts of labeled training
samples from the specific data source of interest. This is impractical when
dealing with many different sources (e.g., in camera networks), or constantly
changing ones such as mobile cameras (e.g., in robotics or driving assistant
systems). In this paper, we address the problem of self-learning detectors in
an autonomous manner, i.e. (i) detectors continuously updating themselves to
efficiently adapt to streaming data sources (contrary to transductive
algorithms), (ii) without any labeled data strongly related to the target data
stream (contrary to self-paced learning), and (iii) without manual intervention
to set and update hyper-parameters. To that end, we propose an unsupervised,
on-line, and self-tuning learning algorithm to optimize a multi-task learning
convex objective. Our method uses confident but laconic oracles (high-precision
but low-recall off-the-shelf generic detectors), and exploits the structure of
the problem to jointly learn on-line an ensemble of instance-level trackers,
from which we derive an adapted category-level object detector. Our approach is
validated on real-world publicly available video object datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4311</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4311</id><created>2014-06-17</created><authors><author><keyname>Manoel</keyname><forenames>Andre</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Tramel</keyname><forenames>Eric W.</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Sparse Estimation with the Swept Approximated Message-Passing Algorithm</title><categories>cs.IT cond-mat.dis-nn math.IT physics.data-an stat.ML</categories><comments>11 pages, 3 figures, implementation available at
  https://github.com/eric-tramel/SwAMP-Demo</comments><journal-ref>Proceedings of the 32nd International Conference on Machine
  Learning (ICML), 2015, 1123-1132</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate Message Passing (AMP) has been shown to be a superior method for
inference problems, such as the recovery of signals from sets of noisy,
lower-dimensionality measurements, both in terms of reconstruction accuracy and
in computational efficiency. However, AMP suffers from serious convergence
issues in contexts that do not exactly match its assumptions. We propose a new
approach to stabilizing AMP in these contexts by applying AMP updates to
individual coefficients rather than in parallel. Our results show that this
change to the AMP iteration can provide theoretically expected, but hitherto
unobtainable, performance for problems on which the standard AMP iteration
diverges. Additionally, we find that the computational costs of this swept
coefficient update scheme is not unduly burdensome, allowing it to be applied
efficiently to signals of large dimensionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4324</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4324</id><created>2014-06-17</created><authors><author><keyname>Murthy</keyname><forenames>Garimella Rama</forenames></author></authors><title>Towards a theory of granular sets</title><categories>cs.AI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the application problem of sensor fusion the author introduced
the concept of graded set. It is reasoned that in classification problem
arising in an information system (represented by information table), a novel
set called Granular set naturally arises. It is realized that in any
hierarchical classification problem, Granular set naturally arises. Also when
the target set of objects forms a graded set the lower and upper approximations
of target sets form a graded set. This generalizes the concept of rough set. It
is hoped that a detailed theory of granular/ graded sets finds several
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4328</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4328</id><created>2014-06-17</created><authors><author><keyname>Wen</keyname><forenames>Jinming</forenames></author><author><keyname>Li</keyname><forenames>Dongfang</forenames></author><author><keyname>Zhu</keyname><forenames>Fumin</forenames></author></authors><title>Stable Recovery of Sparse Signals via $l_p-$Minimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that, under the assumption that $\|\e\|_2\leq
\epsilon$, every $k-$sparse signal $\x\in \mathbb{R}^n$ can be stably
($\epsilon\neq0$) or exactly recovered ($\epsilon=0$) from $\y=\A\x+\e$ via
$l_p-$mnimization with $p\in(0, \bar{p}]$, where \beqnn \bar{p}= \begin{cases}
\frac{50}{31}(1-\delta_{2k}), &amp;\delta_{2k}\in[\frac{\sqrt{2}}{2}, 0.7183)\cr
0.4541, &amp;\delta_{2k}\in[0.7183,0.7729)\cr 2(1-\delta_{2k}),
&amp;\delta_{2k}\in[0.7729,1) \end{cases}, \eeqnn even if the restricted isometry
constant of $\A$ satisfies $\delta_{2k}\in[\frac{\sqrt{2}}{2}, 1)$.
Furthermore, under the assumption that $n\leq 4k$, we show that the range of
$p$ can be further improved to $p\in(0,\frac{3+2\sqrt{2}}{2}(1-\delta_{2k})]$.
This not only extends some discussions of only the noiseless recovery (Lai et
al. and Wu et al.) to the noise recovery, but also greatly improves the best
existing results where $p\in(0,\min\{1, 1.0873(1-\delta_{2k}) \})$ (Wu et al.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4331</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4331</id><created>2014-06-17</created><authors><author><keyname>Ordu&#xf1;a-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>The dark side of Open Access in Google and Google Scholar: the case of
  Latin-American repositories</title><categories>cs.DL</categories><comments>16 pages, 6 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Since repositories are a key tool in making scholarly knowledge open access,
determining their presence and impact on the Web is essential, particularly in
Google (search engine par excellence) and Google Scholar (a tool increasingly
used by researchers to search for academic information). The few studies
conducted so far have been limited to very specific geographic areas (USA),
which makes it necessary to find out what is happening in other regions that
are not part of mainstream academia, and where repositories play a decisive
role in the visibility of scholarly production. The main objective of this
study is to ascertain the presence and visibility of Latin American
repositories in Google and Google Scholar through the application of page count
and visibility indicators. For a sample of 137 repositories, the results
indicate that the indexing ratio is low in Google, and virtually nonexistent in
Google Scholar; they also indicate a complete lack of correspondence between
the repository records and the data produced by these two search tools. These
results are mainly attributable to limitations arising from the use of
description schemas that are incompatible with Google Scholar (repository
design) and the reliability of web indicators (search engines). We conclude
that neither Google nor Google Scholar accurately represent the actual size of
open access content published by Latin American repositories; this may indicate
a non-indexed, hidden side to open access, which could be limiting the
dissemination and consumption of open access scholarly literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4335</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4335</id><created>2014-06-17</created><authors><author><keyname>Wen</keyname><forenames>Jinming</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaomei</forenames></author><author><keyname>Li</keyname><forenames>Dongfang</forenames></author></authors><title>Improved Bounds on the Restricted Isometry Constant for Orthogonal
  Matching Pursuit</title><categories>cs.IT math.IT</categories><comments>Electronic Letters, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we first construct a counter example to show that for any
given positive integer $K\geq 2$ and for any $\frac{1}{\sqrt{K+1}}\leq t&lt;1$,
there always exist a $K-$sparse $\x$ and a matrix $\A$ with the restricted
isometry constant $\delta_{K+1}=t$ such that the OMP algorithm fails in $K$
iterations. Secondly, we show that even when
$\delta_{K+1}=\frac{1}{\sqrt{K}+1}$, the OMP algorithm can also perfectly
recover every $K-$sparse vector $\x$ from $\y=\A\x$ in $K$ iteration. This
improves the best existing results which were independently given by Mo et al.
and Wang et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4363</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4363</id><created>2014-06-17</created><updated>2015-06-09</updated><authors><author><keyname>Matsushima</keyname><forenames>Shin</forenames></author><author><keyname>Yun</keyname><forenames>Hyokun</forenames></author><author><keyname>Zhang</keyname><forenames>Xinhua</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author></authors><title>Distributed Stochastic Optimization of the Regularized Risk</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning algorithms minimize a regularized risk, and stochastic
optimization is widely used for this task. When working with massive data, it
is desirable to perform stochastic optimization in parallel. Unfortunately,
many existing stochastic optimization algorithms cannot be parallelized
efficiently. In this paper we show that one can rewrite the regularized risk
minimization problem as an equivalent saddle-point problem, and propose an
efficient distributed stochastic optimization (DSO) algorithm. We prove the
algorithm's rate of convergence; remarkably, our analysis shows that the
algorithm scales almost linearly with the number of processors. We also verify
with empirical evaluations that the proposed algorithm is competitive with
other parallel, general purpose stochastic and batch optimization algorithms
for regularized risk minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4380</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4380</id><created>2014-06-17</created><updated>2015-01-22</updated><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames></author><author><keyname>Montassier</keyname><forenames>Micka&#xeb;l</forenames></author><author><keyname>Pinlou</keyname><forenames>Alexandre</forenames></author></authors><title>Entropy compression method applied to graph colorings</title><categories>cs.DM math.CO</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the algorithmic proof of Lov\'asz local lemma due to Moser and
Tardos, the works of Grytczuk et al. on words, and Dujmovi\'c et al. on
colorings, Esperet and Parreau developed a framework to prove upper bounds for
several chromatic numbers (in particular acyclic chromatic index, star
chromatic number and Thue chromatic number) using the so-called \emph{entropy
compression method}.
  Inspired by this work, we propose a more general framework and a better
analysis. This leads to improved upper bounds on chromatic numbers and indices.
In particular, every graph with maximum degree $\Delta$ has an acyclic
chromatic number at most $\frac{3}{2}\Delta^{\frac43} + O(\Delta)$. Also every
planar graph with maximum degree $\Delta$ has a facial Thue choice number at
most $\Delta + O(\Delta^\frac 12)$ and facial Thue choice index at most $10$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4392</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4392</id><created>2014-06-17</created><updated>2015-06-05</updated><authors><author><keyname>Zonetti</keyname><forenames>Daniele</forenames></author><author><keyname>Ortega</keyname><forenames>Romeo</forenames></author><author><keyname>Benchaib</keyname><forenames>Abdelkrim</forenames></author></authors><title>Modeling and Control of High-Voltage Direct-Current Transmission
  Systems: From Theory to Practice and Back</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of modeling and control of multi-terminal high-voltage
direct-current transmission systems is addressed in this paper, which contains
five main contributions. First, to propose a unified, physically motivated,
modeling framework - based on port-Hamiltonian representations - of the various
network topologies used in this application. Second, to prove that the system
can be globally asymptotically stabilized with a decentralized PI control, that
exploits its passivity properties. Close connections between the proposed PI
and the popular Akagi's PQ instantaneous power method are also established.
Third, to reveal the transient performance limitations of the proposed
controller that, interestingly, is shown to be intrinsic to PI passivity-based
control. Fourth, motivated by the latter, an outer-loop that overcomes the
aforementioned limitations is proposed. The performance limitation of the PI,
and its drastic improvement using outer-loop controls, are verified via
simulations on a three-terminals benchmark example. A final contribution is a
novel formulation of the power flow equations for the centralized references
calculation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4395</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4395</id><created>2014-06-17</created><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Esti&#xe9;venart</keyname><forenames>Morgane</forenames></author><author><keyname>Geeraerts</keyname><forenames>Gilles</forenames></author></authors><title>On MITL and alternating timed automata over infinite words</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One clock alternating timed automata (OCATA) have been introduced as natural
extension of (one clock) timed automata to express the semantics of MTL. In
this paper, we consider the application of OCATA to the problems of
model-checking and satisfiability for MITL (a syntactic fragment of MTL),
interpreted over infinite words. Our approach is based on the interval
semantics (recently introduced in [BEG13] in the case of finite words) extended
to infinite words. We propose region-based and zone-based algorithms, based on
this semantics, for MITL model-checking and satisfiability. We report on the
performance of a prototype tool implementing those algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4399</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4399</id><created>2014-06-17</created><updated>2015-03-18</updated><authors><author><keyname>Rosati</keyname><forenames>S.</forenames></author><author><keyname>Kruzelecki</keyname><forenames>K.</forenames></author><author><keyname>Heitz</keyname><forenames>G.</forenames></author><author><keyname>Floreano</keyname><forenames>D.</forenames></author><author><keyname>Rimoldi</keyname><forenames>B.</forenames></author></authors><title>Dynamic Routing for Flying Ad Hoc Networks</title><categories>cs.NI</categories><comments>Submitted to the IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports experimental results on self-organizing wireless networks
carried by small flying robots. Flying ad hoc networks (FANETs) composed of
small unmanned aerial vehicles (UAVs) are flexible, inexpensive and fast to
deploy. This makes them a very attractive technology for many civilian and
military applications. Due to the high mobility of the nodes, maintaining a
communication link between the UAVs is a challenging task. The topology of
these networks is more dynamic than that of typical mobile ad hoc networks
(MANETs) and of typical vehicle ad hoc networks (VANETs). As a consequence, the
existing routing protocols designed for MANETs partly fail in tracking network
topology changes. In this work, we compare two different routing algorithms for
ad hoc networks: optimized link-state routing (OLSR), and predictive-OLSR
(P-OLSR). The latter is an OLSR extension that we designed for FANETs; it takes
advantage of the GPS information available on board. To the best of our
knowledge, P-OLSR is currently the only FANET-specific routing technique that
has an available Linux implementation. We present results obtained by both
Media Access Control (MAC) layer emulations and real-world experiments. In the
experiments, we used a testbed composed of two autonomous fixed-wing UAVs and a
node on the ground. Our experiments evaluate the link performance and the
communication range, as well as the routing performance. Our emulation and
experimental results show that P-OLSR significantly outperforms OLSR in routing
in the presence of frequent network topology changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4400</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4400</id><created>2014-06-17</created><authors><author><keyname>Grauwin</keyname><forenames>S.</forenames></author><author><keyname>Sobolevsky</keyname><forenames>S.</forenames></author><author><keyname>Moritz</keyname><forenames>S.</forenames></author><author><keyname>G&#xf3;dor</keyname><forenames>I.</forenames></author><author><keyname>Ratti</keyname><forenames>C.</forenames></author></authors><title>Towards a comparative science of cities: using mobile traffic records in
  New York, London and Hong Kong</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an</categories><comments>24 pages, 9 figures, book chapter to be published in &quot;Computational
  Approaches for Urban Environments&quot; (Springer Ed.), October 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter examines the possibility to analyze and compare human activities
in an urban environment based on the detection of mobile phone usage patterns.
Thanks to an unprecedented collection of counter data recording the number of
calls, SMS, and data transfers resolved both in time and space, we confirm the
connection between temporal activity profile and land usage in three global
cities: New York, London and Hong Kong. By comparing whole cities typical
patterns, we provide insights on how cultural, technological and economical
factors shape human dynamics. At a more local scale, we use clustering analysis
to identify locations with similar patterns within a city. Our research reveals
a universal structure of cities, with core financial centers all sharing
similar activity patterns and commercial or residential areas with more
city-specific patterns. These findings hint that as the economy becomes more
global, common patterns emerge in business areas of different cities across the
globe, while the impact of local conditions still remains recognizable on the
level of routine people activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4425</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4425</id><created>2014-06-17</created><updated>2014-06-18</updated><authors><author><keyname>Borges</keyname><forenames>Joaquim</forenames></author><author><keyname>Fern&#xe1;ndez-C&#xf3;rdoba</keyname><forenames>Cristina</forenames></author><author><keyname>Ten-Valls</keyname><forenames>Roger</forenames></author></authors><title>Z2Z4-additive cyclic codes, generator polynomials and dual codes</title><categories>cs.DM cs.IT math.IT</categories><msc-class>68Rxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A ${\mathbb{Z}}_2{\mathbb{Z}}_4$-additive code ${\cal
C}\subseteq{\mathbb{Z}}_2^\alpha\times{\mathbb{Z}}_4^\beta$ is called cyclic
code if the set of coordinates can be partitioned into two subsets, the set of
${\mathbb{Z}}_2$ and the set of ${\mathbb{Z}}_4$ coordinates, such that any
cyclic shift of the coordinates of both subsets leaves invariant the code.
These codes can be identified as submodules of the $\mathbb{Z}_4[x]$-module
$\mathbb{Z}_2[x]/(x^\alpha-1)\times\mathbb{Z}_4[x]/(x^\beta-1)$. The parameters
of a ${\mathbb{Z}}_2{\mathbb{Z}}_4$-additive cyclic code are stated in terms of
the degrees of the generator polynomials of the code. The generator polynomials
of the dual code of a ${\mathbb{Z}}_2{\mathbb{Z}}_4$-additive cyclic code are
determined in terms of the generator polynomials of the code ${\cal C}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4426</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4426</id><created>2014-06-17</created><authors><author><keyname>Cho</keyname><forenames>Keum-Bae</forenames></author></authors><title>The number system hidden inside the Boolean satisfiability problem</title><categories>cs.CC math.LO</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Boolean satisfiability (SAT) problem is the first known example of an
NP-complete problem, and thousands of NP-compete problems have been identified
by reducing the SAT to the problems. Researchers have tried to find a definite
mathematical expression that distinguishes among NL-complete, P-complete, and
NP-complete problems such as 2-SAT, Horn-SAT, and 3-SAT. In this paper, we
introduce the natural number system hidden inside the SAT structure. We reduce
a SAT instance to an integer-programming instance. Then, we focus on the
distance from an integral point to the facets of the projected polytope. We
newly define a dominant variable, decision chain, and chain coupler as a novel
element of a Boolean formula. From the analysis of the SAT structure using the
elements, we show that the coefficients of the normal vector of the facet can
be expressed with the natural number system of which the exponent is
exponential in the input size. Furthermore, we prove that an integral point,
which is not contained in the solution region, can locate exponentially near
the projected polytope by the number system. Finally, we show that the number
system is not formed in 2-SAT, but partially formed in Horn-SAT according to
the feasible value of a dominant variable, and always formed in k-SAT (k&gt;2)
regardless of the feasible value of a dominant variable. Two questions, NL =? P
and P =? NP, have been open problems for several decades. This study presents a
definite supporting evidence for the conjecture that NL is a proper subset of P
and P is a proper subset of NP, and a new solving direction for the P versus NP
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4433</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4433</id><created>2014-06-17</created><updated>2015-07-12</updated><authors><author><keyname>McDiarmid</keyname><forenames>Colin</forenames></author><author><keyname>Scott</keyname><forenames>Alex</forenames></author><author><keyname>Withers</keyname><forenames>Paul</forenames></author></authors><title>Uniform multicommodity flow in the hypercube with random edge capacities</title><categories>math.OC cs.DS math.CO</categories><comments>32 pages, 4 figures</comments><msc-class>05C80, 05C21</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give two results for multicommodity flows in the $d$-dimensional hypercube
${Q}^d$ with independent random edge capacities distributed like $C$ where
$\Pr[C&gt;0]&gt;1/2$. Firstly, with high probability as $d \rightarrow \infty$, the
network can support simultaneous multicommodity flows of volume close to $E[C]$
between all antipodal vertex pairs. Secondly, with high probability, the
network can support simultaneous multicommodity flows of volume close to
$2^{1-d} E[C]$ between all vertex pairs. Both results are best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4441</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4441</id><created>2014-06-17</created><updated>2014-11-04</updated><authors><author><keyname>Gerlach</keyname><forenames>Martin</forenames></author><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author></authors><title>Scaling laws and fluctuations in the statistics of word frequencies</title><categories>physics.soc-ph cs.CL physics.data-an</categories><comments>19 pages, 4 figures</comments><journal-ref>New Journal of Physics 16 (2014), 113010</journal-ref><doi>10.1088/1367-2630/16/11/113010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we combine statistical analysis of large text databases and
simple stochastic models to explain the appearance of scaling laws in the
statistics of word frequencies. Besides the sublinear scaling of the vocabulary
size with database size (Heaps' law), here we report a new scaling of the
fluctuations around this average (fluctuation scaling analysis). We explain
both scaling laws by modeling the usage of words by simple stochastic processes
in which the overall distribution of word-frequencies is fat tailed (Zipf's
law) and the frequency of a single word is subject to fluctuations across
documents (as in topic models). In this framework, the mean and the variance of
the vocabulary size can be expressed as quenched averages, implying that: i)
the inhomogeneous dissemination of words cause a reduction of the average
vocabulary size in comparison to the homogeneous case, and ii) correlations in
the co-occurrence of words lead to an increase in the variance and the
vocabulary size becomes a non-self-averaging quantity. We address the
implications of these observations to the measurement of lexical richness. We
test our results in three large text databases (Google-ngram, Enlgish
Wikipedia, and a collection of scientific articles).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4444</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4444</id><created>2014-06-13</created><updated>2015-05-07</updated><authors><author><keyname>Zhang</keyname><forenames>Ziming</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>PRISM: Person Re-Identification via Structured Matching</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person re-identification (re-id), an emerging problem in visual surveillance,
deals with maintaining entities of individuals whilst they traverse various
locations surveilled by a camera network. From a visual perspective re-id is
challenging due to significant changes in visual appearance of individuals in
cameras with different pose, illumination and calibration. Globally the
challenge arises from the need to maintain structurally consistent matches
among all the individual entities across different camera views. We propose
PRISM, a structured matching method to jointly account for these challenges. We
view the global problem as a weighted graph matching problem and estimate edge
weights by learning to predict them based on the co-occurrences of visual
patterns in the training examples. These co-occurrence based scores in turn
account for appearance changes by inferring likely and unlikely visual
co-occurrences appearing in training instances. We implement PRISM on single
shot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art in
terms of matching rate while being computationally efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4445</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4445</id><created>2014-06-13</created><updated>2014-06-18</updated><authors><author><keyname>Zhang</keyname><forenames>Ziming</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>RAPID: Rapidly Accelerated Proximal Gradient Algorithms for Convex
  Minimization</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new algorithm to speed-up the convergence of
accelerated proximal gradient (APG) methods. In order to minimize a convex
function $f(\mathbf{x})$, our algorithm introduces a simple line search step
after each proximal gradient step in APG so that a biconvex function
$f(\theta\mathbf{x})$ is minimized over scalar variable $\theta&gt;0$ while fixing
variable $\mathbf{x}$. We propose two new ways of constructing the auxiliary
variables in APG based on the intermediate solutions of the proximal gradient
and the line search steps. We prove that at arbitrary iteration step $t
(t\geq1)$, our algorithm can achieve a smaller upper-bound for the gap between
the current and optimal objective values than those in the traditional APG
methods such as FISTA, making it converge faster in practice. In fact, our
algorithm can be potentially applied to many important convex optimization
problems, such as sparse linear regression and kernel SVMs. Our experimental
results clearly demonstrate that our algorithm converges faster than APG in all
of the applications above, even comparable to some sophisticated solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4447</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4447</id><created>2014-06-17</created><authors><author><keyname>Antunes</keyname><forenames>Pedro Gir&#xe3;o</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author><author><keyname>Trancoso</keyname><forenames>Isabel</forenames></author></authors><title>Automatic Fado Music Classification</title><categories>cs.SD cs.AI</categories><comments>4 pages, 1 figure, 5 tables</comments><acm-class>H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In late 2011, Fado was elevated to the oral and intangible heritage of
humanity by UNESCO. This study aims to develop a tool for automatic detection
of Fado music based on the audio signal. To do this, frequency spectrum-related
characteristics were captured form the audio signal: in addition to the Mel
Frequency Cepstral Coefficients (MFCCs) and the energy of the signal, the
signal was further analysed in two frequency ranges, providing additional
information. Tests were run both in a 10-fold cross-validation setup (97.6%
accuracy), and in a traditional train/test setup (95.8% accuracy). The good
results reflect the fact that Fado is a very distinctive musical style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4448</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4448</id><created>2014-06-17</created><authors><author><keyname>Farhadi</keyname><forenames>Farzaneh</forenames></author><author><keyname>Ashtiani</keyname><forenames>Farid</forenames></author></authors><title>Stability Region of a Slotted Aloha Network with K-Exponential Backoff</title><categories>cs.NI</categories><comments>30 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stability region of random access wireless networks is known for only simple
network scenarios. The main problem in this respect is due to interaction among
queues. When transmission probabilities during successive transmissions change,
e.g., when exponential backoff mechanism is exploited, the interactions in the
network are stimulated. In this paper, we derive the stability region of a
buffered slotted Aloha network with K-exponential backoff mechanism,
approximately, when a finite number of nodes exist. To this end, we propose a
new approach in modeling the interaction among wireless nodes. In this
approach, we model the network with inter-related quasi-birth-death (QBD)
processes such that at each QBD corresponding to each node, a finite number of
phases consider the status of the other nodes. Then, by exploiting the
available theorems on stability of QBDs, we find the stability region. We show
that exponential backoff mechanism is able to increase the area of the
stability region of a simple slotted Aloha network with two nodes, more than
40\%. We also show that a slotted Aloha network with exponential backoff may
perform very near to ideal scheduling. The accuracy of our modeling approach is
verified by simulation in different conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4454</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4454</id><created>2014-06-17</created><authors><author><keyname>Li</keyname><forenames>Shanfei</forenames></author></authors><title>An Improved Approximation Algorithm for the Hard Uniform Capacitated
  k-median Problem</title><categories>cs.DS math.OC</categories><comments>19 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the $k$-median problem, given a set of locations, the goal is to select a
subset of at most $k$ centers so as to minimize the total cost of connecting
each location to its nearest center. We study the uniform hard capacitated
version of the $k$-median problem, in which each selected center can only serve
a limited number of locations.
  Inspired by the algorithm of Charikar, Guha, Tardos and Shmoys, we give a
$(6+10\alpha)$-approximation algorithm for this problem with increasing the
capacities by a factor of $2+\frac{2}{\alpha}, \alpha\geq 4$, which improves
the previous best $(32 l^2+28 l+7)$-approximation algorithm proposed by Byrka,
Fleszar, Rybicki and Spoerhase violating the capacities by factor
$2+\frac{3}{l-1}, l\in \{2,3,4,\dots\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4458</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4458</id><created>2014-06-17</created><authors><author><keyname>Shin</keyname><forenames>Jiwon</forenames></author><author><keyname>Rusakov</keyname><forenames>Andrey</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Teaching Software Engineering through Robotics</title><categories>cs.CY cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a newly-developed robotics programming course and reports
the initial results of software engineering education in robotics context.
Robotics programming, as a multidisciplinary course, puts equal emphasis on
software engineering and robotics. It teaches students proper software
engineering -- in particular, modularity and documentation -- by having them
implement four core robotics algorithms for an educational robot. To evaluate
the effect of software engineering education in robotics context, we analyze
pre- and post-class survey data and the four assignments our students completed
for the course. The analysis suggests that the students acquired an
understanding of software engineering techniques and principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4462</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4462</id><created>2014-06-15</created><authors><author><keyname>Khaji</keyname><forenames>Erfan</forenames></author></authors><title>Soccer League Optimization: A heuristic Algorithm Inspired by the
  Football System in European Countries</title><categories>cs.AI</categories><comments>6 Pages, 12 Figures, 4 Tables, Accepted in GEM 2014, but rejected due
  to lack of money</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new heuristic optimization algorithm has been introduced
based on the performance of the major football leagues within each season in EU
countries. The algorithm starts with an initial population including three
different groups of teams: the wealthiest (strongest), the regular, the poorest
(weakest). Each individual of population constitute a football team while each
player is an indication of a player in a post. The optimization can hopefully
occurs when the competition among the teams in all the leagues is imitated as
the strongest teams usually purchase the best players of the regular teams and
in turn, regular teams purchase the best players of the weakest who should
always discover young players instead of buying professionals. It has been
shown that the algorithm can hopefully converge to an acceptable solution
solving various benchmarks. Key words: Heuristic Algorithms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4463</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4463</id><created>2014-06-17</created><authors><author><keyname>Lim</keyname><forenames>Yeon-sup</forenames></author><author><keyname>Chen</keyname><forenames>Yung-Chih</forenames></author><author><keyname>Nahum</keyname><forenames>Erich M.</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Gibbens</keyname><forenames>Richard J.</forenames></author></authors><title>Improving Energy Efficiency of MPTCP for Mobile Devices</title><categories>cs.NI</categories><comments>Submitted to ACM CoNEXT'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-Path TCP (MPTCP) is a new transport protocol that enables systems to
exploit available paths through multiple network interfaces. MPTCP is
particularly useful for mobile devices, which usually have multiple wireless
interfaces. However, these devices have limited power capacity and thus
judicious use of these interfaces is required. In this work, we develop a model
for MPTCP energy consumption derived from experimental measurements using MPTCP
on a mobile device with both cellular and WiFi interfaces. Using our energy
model, we identify an operating region where there is scope to improve power
efficiency compared to both standard TCP and MPTCP. We design and implement an
improved energy-efficient MPTCP, called eMPTCP. We evaluate eMPTCP on a mobile
device across several scenarios, including varying bandwidth, background
traffic, and user mobility. Our results show that eMPTCP can reduce the power
consumption by up to 15% compared with MPTCP, while preserving the availability
and robustness benefits of MPTCP. Furthermore, we show that when compared with
TCP over WiFi, which is more energy efficient than TCP over LTE, eMPTCP obtains
significantly better performance with relatively little additional energy
overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4465</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4465</id><created>2014-06-16</created><updated>2015-06-02</updated><authors><author><keyname>Fan</keyname><forenames>Yaru</forenames></author><author><keyname>Wang</keyname><forenames>Yilun</forenames></author></authors><title>Multi-stage Multi-task feature learning via adaptive threshold</title><categories>cs.LG cs.CV stat.ML</categories><comments>13 pages,12 figures. arXiv admin note: text overlap with
  arXiv:1210.5806 by other authors</comments><msc-class>68T10</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task feature learning aims to identity the shared features among tasks
to improve generalization. It has been shown that by minimizing non-convex
learning models, a better solution than the convex alternatives can be
obtained. Therefore, a non-convex model based on the capped-$\ell_{1},\ell_{1}$
regularization was proposed in \cite{Gong2013}, and a corresponding efficient
multi-stage multi-task feature learning algorithm (MSMTFL) was presented.
However, this algorithm harnesses a prescribed fixed threshold in the
definition of the capped-$\ell_{1},\ell_{1}$ regularization and the lack of
adaptivity might result in suboptimal performance. In this paper we propose to
employ an adaptive threshold in the capped-$\ell_{1},\ell_{1}$ regularized
formulation, where the corresponding variant of MSMTFL will incorporate an
additional component to adaptively determine the threshold value. This variant
is expected to achieve a better feature selection performance over the original
MSMTFL algorithm. In particular, the embedded adaptive threshold component
comes from our previously proposed iterative support detection (ISD) method
\cite{Wang2010}. Empirical studies on both synthetic and real-world data sets
demonstrate the effectiveness of this new variant over the original MSMTFL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4469</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4469</id><created>2014-06-17</created><authors><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author><author><keyname>Eisen</keyname><forenames>Mark</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Authorship Attribution through Function Word Adjacency Networks</title><categories>cs.CL cs.LG stat.ML</categories><doi>10.1109/TSP.2015.2451111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for authorship attribution based on function word adjacency networks
(WANs) is introduced. Function words are parts of speech that express
grammatical relationships between other words but do not carry lexical meaning
on their own. In the WANs in this paper, nodes are function words and directed
edges stand in for the likelihood of finding the sink word in the ordered
vicinity of the source word. WANs of different authors can be interpreted as
transition probabilities of a Markov chain and are therefore compared in terms
of their relative entropies. Optimal selection of WAN parameters is studied and
attribution accuracy is benchmarked across a diverse pool of authors and
varying text lengths. This analysis shows that, since function words are
independent of content, their use tends to be specific to an author and that
the relational data captured by function WANs is a good summary of stylometric
fingerprints. Attribution accuracy is observed to exceed the one achieved by
methods that rely on word frequencies alone. Further combining WANs with
methods that rely on word frequencies alone, results in larger attribution
accuracy, indicating that both sources of information encode different aspects
of authorial styles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4472</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4472</id><created>2014-06-17</created><updated>2014-06-18</updated><authors><author><keyname>Valentini</keyname><forenames>Giorgio</forenames></author></authors><title>Notes on hierarchical ensemble methods for DAG-structured taxonomies</title><categories>cs.AI cs.LG stat.ML</categories><comments>12 pages, 3 figures. Typos corrected. Modified title and abstract.
  Added references and some changes</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several real problems ranging from text classification to computational
biology are characterized by hierarchical multi-label classification tasks.
Most of the methods presented in literature focused on tree-structured
taxonomies, but only few on taxonomies structured according to a Directed
Acyclic Graph (DAG). In this contribution novel classification ensemble
algorithms for DAG-structured taxonomies are introduced. In particular
Hierarchical Top-Down (HTD-DAG) and True Path Rule (TPR-DAG) for DAGs are
presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4473</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4473</id><created>2014-06-13</created><updated>2014-12-19</updated><authors><author><keyname>Tang</keyname><forenames>Juan</forenames></author><author><keyname>Wu</keyname><forenames>Wenyuan</forenames></author><author><keyname>Qin</keyname><forenames>Xiaolin</forenames></author><author><keyname>Feng</keyname><forenames>Yong</forenames></author></authors><title>Structural index reduction algorithms for differential algebraic
  equations via fixed-point iteration</title><categories>cs.NA math.NA</categories><comments>19 pages</comments><msc-class>34A09, 65L80, 65F50, 90C05, 90C27, 90C06</msc-class><acm-class>G.1.7; G.1.6; I.1.2; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by Pryce's structural index reduction method for differential
algebraic equations (DAEs), we show the complexity of the fixed-point iteration
algorithm and propose a fixed-point iteration method with parameters. It leads
to a block fixed-point iteration method which can be applied to large-scale
DAEs with block upper triangular structure. Moreover, its complexity analysis
is also given in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4484</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4484</id><created>2014-06-17</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author></authors><title>Block matching algorithm based on Harmony Search optimization for motion
  estimation</title><categories>cs.CV</categories><comments>25 Pages. arXiv admin note: substantial text overlap with
  arXiv:1405.4721</comments><journal-ref>Applied Intelligence, 39 (1), (2013), pp. 165-183</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion estimation is one of the major problems in developing video coding
applications. Among all motion estimation approaches, Block-matching (BM)
algorithms are the most popular methods due to their effectiveness and
simplicity for both software and hardware implementations. A BM approach
assumes that the movement of pixels within a defined region of the current
frame can be modeled as a translation of pixels contained in the previous
frame. In this procedure, the motion vector is obtained by minimizing a certain
matching metric that is produced for the current frame over a determined search
window from the previous frame. Unfortunately, the evaluation of such matching
measurement is computationally expensive and represents the most consuming
operation in the BM process. Therefore, BM motion estimation can be viewed as
an optimization problem whose goal is to find the best-matching block within a
search space. The simplest available BM method is the Full Search Algorithm
(FSA) which finds the most accurate motion vector through an exhaustive
computation of all the elements of the search space. Recently, several fast BM
algorithms have been proposed to reduce the search positions by calculating
only a fixed subset of motion vectors despite lowering its accuracy. On the
other hand, the Harmony Search (HS) algorithm is a population-based
optimization method that is inspired by the music improvisation process in
which a musician searches for harmony and continues to polish the pitches to
obtain a better harmony. In this paper, a new BM algorithm that combines HS
with a fitness approximation model is proposed. The approach uses motion
vectors belonging to the search window as potential solutions. A fitness
function evaluates the matching quality of each motion vector candidate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4491</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4491</id><created>2014-06-17</created><authors><author><keyname>Meric</keyname><forenames>Hugo</forenames></author><author><keyname>Piquer</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Lacan</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Quasi-optimal grouping for broadcast systems with hierarchical
  modulation</title><categories>cs.NI cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, we proposed to combine time sharing with hierarchical modulation to
increase the transmission rate of broadcast systems. Our proposal involves to
group the receivers in pairs in order to transmit with hierarchical modulation.
We introduced several grouping strategies but the optimal matching remained an
open question. In this letter, we show that the optimal grouping is the
solution of an assignment problem, for which efficient algorithms exist such as
the Hungarian method. Based on this algorithm, we study the performance of the
optimal grouping in terms of spectrum efficiency for a DVB-S2 system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4498</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4498</id><created>2014-06-17</created><updated>2014-10-30</updated><authors><author><keyname>Ghanbarnejad</keyname><forenames>Fakhteh</forenames></author><author><keyname>Gerlach</keyname><forenames>Martin</forenames></author><author><keyname>Miotto</keyname><forenames>Jose M.</forenames></author><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author></authors><title>Extracting information from S-curves of language change</title><categories>physics.soc-ph cs.CL physics.data-an</categories><comments>9 pages, 5 figures, Supplementary Material is available at
  http://dx.doi.org/10.6084/m9.figshare.1221782</comments><journal-ref>J. R. Soc. Interface 6 December 2014 vol. 11 no. 101 20141044</journal-ref><doi>10.1098/rsif.2014.1044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well accepted that adoption of innovations are described by S-curves
(slow start, accelerating period, and slow end). In this paper, we analyze how
much information on the dynamics of innovation spreading can be obtained from a
quantitative description of S-curves. We focus on the adoption of linguistic
innovations for which detailed databases of written texts from the last 200
years allow for an unprecedented statistical precision. Combining data analysis
with simulations of simple models (e.g., the Bass dynamics on complex networks)
we identify signatures of endogenous and exogenous factors in the S-curves of
adoption. We propose a measure to quantify the strength of these factors and
three different methods to estimate it from S-curves. We obtain cases in which
the exogenous factors are dominant (in the adoption of German orthographic
reforms and of one irregular verb) and cases in which endogenous factors are
dominant (in the adoption of conventions for romanization of Russian names and
in the regularization of most studied verbs). These results show that the shape
of S-curve is not universal and contains information on the adoption mechanism.
(published at &quot;J. R. Soc. Interface, vol. 11, no. 101, (2014) 1044&quot;; DOI:
http://dx.doi.org/10.1098/rsif.2014.1044)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4513</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4513</id><created>2014-06-17</created><updated>2015-09-21</updated><authors><author><keyname>von Manteuffel</keyname><forenames>Andreas</forenames></author><author><keyname>Schabinger</keyname><forenames>Robert M.</forenames></author></authors><title>A novel approach to integration by parts reduction</title><categories>hep-ph cs.SC hep-th</categories><comments>4 pages. Version 2 is the final, published version of this article</comments><report-no>MITP/14-009</report-no><doi>10.1016/j.physletb.2015.03.029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integration by parts reduction is a standard component of most modern
multi-loop calculations in quantum field theory. We present a novel strategy
constructed to overcome the limitations of currently available reduction
programs based on Laporta's algorithm. The key idea is to construct algebraic
identities from numerical samples obtained from reductions over finite fields.
We expect the method to be highly amenable to parallelization, show a low
memory footprint during the reduction step, and allow for significantly better
run-times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4516</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4516</id><created>2014-06-17</created><updated>2014-11-27</updated><authors><author><keyname>Messai</keyname><forenames>Mohamed-Lamine</forenames></author></authors><title>Classification of Attacks in Wireless Sensor Networks</title><categories>cs.CR</categories><comments>International Congress on Telecommunication and Application 2014</comments><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless sensor networks (WSNs), security has a vital importance.
Recently, there was a huge interest to propose security solutions in WSNs
because of their applications in both civilian and military domains.
Adversaries can launch different types of attacks, and cryptography is used to
countering these attacks. This paper presents challenges of security and a
classification of the different possible attacks in WSNs. The problems of
security in each layer of the network's OSI model are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4518</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4518</id><created>2014-06-15</created><authors><author><keyname>Khaji</keyname><forenames>Erfan</forenames></author><author><keyname>Mohammadi</keyname><forenames>Amin Satlikh</forenames></author></authors><title>A Heuristic Method to Generate Better Initial Population for
  Evolutionary Methods</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Initial population plays an important role in heuristic algorithms such as GA
as it help to decrease the time those algorithms need to achieve an acceptable
result. Furthermore, it may influence the quality of the final answer given by
evolutionary algorithms. In this paper, we shall introduce a heuristic method
to generate a target based initial population which possess two mentioned
characteristics. The efficiency of the proposed method has been shown by
presenting the results of our tests on the benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4542</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4542</id><created>2014-06-17</created><authors><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Grant</keyname><forenames>Carolyn S.</forenames></author><author><keyname>Thompson</keyname><forenames>Donna</forenames></author><author><keyname>Luker</keyname><forenames>Jay</forenames></author><author><keyname>Chyla</keyname><forenames>Roman</forenames></author><author><keyname>Holachek</keyname><forenames>Alexandra</forenames></author><author><keyname>Murray</keyname><forenames>Stephen S.</forenames></author></authors><title>Computing and Using Metrics in the ADS</title><categories>cs.DL astro-ph.IM</categories><comments>to appear in proceedings of LISA VII conference, Naples, Italy</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Finding measures for research impact, be it for individuals, institutions,
instruments or projects, has gained a lot of popularity. More papers than ever
are being written on new impact measures, and problems with existing measures
are being pointed out on a regular basis. Funding agencies require impact
statistics in their reports, job candidates incorporate them in their resumes,
and publication metrics have even been used in at least one recent court case.
To support this need for research impact indicators, the SAO/NASA Astrophysics
Data System (ADS) has developed a service which provides a broad overview of
various impact measures. In this presentation we discuss how the ADS can be
used to quench the thirst for impact measures. We will also discuss a couple of
the lesser known indicators in the metrics overview and the main issues to be
aware of when compiling publication-based metrics in the ADS, namely author
name ambiguity and citation incompleteness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4547</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4547</id><created>2014-06-17</created><authors><author><keyname>Carlet</keyname><forenames>Claude</forenames></author><author><keyname>Freibert</keyname><forenames>Finley</forenames></author><author><keyname>Guilley</keyname><forenames>Sylvain</forenames></author><author><keyname>Kiermaier</keyname><forenames>Michael</forenames></author><author><keyname>Kim</keyname><forenames>Jon-Lark</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>Higher-order CIS codes</title><categories>cs.IT math.CO math.IT</categories><comments>13 pages; 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce {\bf complementary information set codes} of higher-order. A
binary linear code of length $tk$ and dimension $k$ is called a complementary
information set code of order $t$ ($t$-CIS code for short) if it has $t$
pairwise disjoint information sets. The duals of such codes permit to reduce
the cost of masking cryptographic algorithms against side-channel attacks. As
in the case of codes for error correction, given the length and the dimension
of a $t$-CIS code, we look for the highest possible minimum distance. In this
paper, this new class of codes is investigated. The existence of good long CIS
codes of order $3$ is derived by a counting argument. General constructions
based on cyclic and quasi-cyclic codes and on the building up construction are
given. A formula similar to a mass formula is given. A classification of 3-CIS
codes of length $\le 12$ is given. Nonlinear codes better than linear codes are
derived by taking binary images of $\Z_4$-codes. A general algorithm based on
Edmonds' basis packing algorithm from matroid theory is developed with the
following property: given a binary linear code of rate $1/t$ it either provides
$t$ disjoint information sets or proves that the code is not $t$-CIS. Using
this algorithm, all optimal or best known $[tk, k]$ codes where $t=3, 4, \dots,
256$ and $1 \le k \le \lfloor 256/t \rfloor$ are shown to be $t$-CIS for all
such $k$ and $t$, except for $t=3$ with $k=44$ and $t=4$ with $k=37$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4548</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4548</id><created>2014-06-17</created><authors><author><keyname>Ghorbanzadeh</keyname><forenames>Mo</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>Implementing an Optimal Rate Allocation Tuned to the User Quality of
  Experience</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal resource allocation elegantly kaizens bandwidth utilization in
present-day communications systems carrying distinctive traffic types with
specific quality of service (QoS) requirements, whose fulfillment may elevate
users' quality of Experience (QoE). This paper investigates the QoE of users
running real-life real-time and delay-tolerant applications by implementing an
Internet-connected real-world mobile network which hosts a node with a
centralized convex resource allocation optimization algorithm to calculate and
enforce an optimal bandwidth distribution. The experiments show that leveraging
the rate assignment approach escalates the real-life network traffic QoE
through a fine-grained temporal resource allocation pattern which plummets the
total bandwidth consumption and the cost of employing the services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4549</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4549</id><created>2014-06-17</created><authors><author><keyname>He</keyname><forenames>Zhijian</forenames></author><author><keyname>Owen</keyname><forenames>Art B.</forenames></author></authors><title>Extensible grids: uniform sampling on a space-filling curve</title><categories>stat.ME cs.CC cs.NA</categories><comments>22 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the properties of points in $[0,1]^d$ generated by applying
Hilbert's space-filling curve to uniformly distributed points in $[0,1]$. For
deterministic sampling we obtain a discrepancy of $O(n^{-1/d})$ for $d\ge2$.
For random stratified sampling, and scrambled van der Corput points, we get a
mean squared error of $O(n^{-1-2/d})$ for integration of Lipshitz continuous
integrands, when $d\ge3$. These rates are the same as one gets by sampling on
$d$ dimensional grids and they show a deterioration with increasing $d$. The
rate for Lipshitz functions is however best possible at that level of
smoothness and is better than plain IID sampling. Unlike grids, space-filling
curve sampling provides points at any desired sample size, and the van der
Corput version is extensible in $n$. Additionally we show that certain
discontinuous functions with infinite variation in the sense of Hardy and
Krause can be integrated with a mean squared error of $O(n^{-1-1/d})$. It was
previously known only that the rate was $o(n^{-1})$. Other space-filling
curves, such as those due to Sierpinski and Peano, also attain these rates,
while upper bounds for the Lebesgue curve are somewhat worse, as if the
dimension were $\log_2(3)$ times as high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4566</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4566</id><created>2014-06-17</created><updated>2015-03-16</updated><authors><author><keyname>Huang</keyname><forenames>Furong</forenames></author><author><keyname>N.</keyname><forenames>Niranjan U.</forenames></author><author><keyname>Perros</keyname><forenames>Ioakeim</forenames></author><author><keyname>Chen</keyname><forenames>Robert</forenames></author><author><keyname>Sun</keyname><forenames>Jimeng</forenames></author><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author></authors><title>Scalable Latent Tree Model and its Application to Health Analytics</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an integrated approach to structure and parameter estimation in
latent tree graphical models, where some nodes are hidden. Our overall approach
follows a &quot;divide-and-conquer&quot; strategy that learns models over small groups of
variables and iteratively merges into a global solution. The structure learning
involves combinatorial operations such as minimum spanning tree construction
and local recursive grouping; the parameter learning is based on the method of
moments and on tensor decompositions. Our method is guaranteed to correctly
recover the unknown tree structure and the model parameters with low sample
complexity for the class of linear multivariate latent tree models which
includes discrete and Gaussian distributions, and Gaussian mixtures. Our bulk
asynchronous parallel algorithm is implemented in parallel using the OpenMP
framework and scales logarithmically with the number of variables and linearly
with dimensionality of each variable. Our experiments confirm a high degree of
efficiency and accuracy on large datasets of electronic health records. The
proposed algorithm also generates intuitive and clinically meaningful disease
hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4567</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4567</id><created>2014-06-17</created><authors><author><keyname>Cao</keyname><forenames>Xiwang</forenames></author><author><keyname>Hu</keyname><forenames>Lei</forenames></author></authors><title>Two Boolean functions with five-valued Walsh spectra and high
  nonlinearity</title><categories>cs.IT math.IT</categories><comments>18 pages</comments><msc-class>11T23, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For cryptographic systems the method of confusion and diffusion is used as a
fundamental technique to achieve security. Confusion is reflected in
nonlinearity of certain Boolean functions describing the cryptographic
transformation. In this paper, we present two balanced boolean functions which
have low Walsh spectra and high nonlinearity. In the proof of the nonlinearity,
a new method for evaluating some exponential sums over finite fields was
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4575</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4575</id><created>2014-06-17</created><updated>2014-12-17</updated><authors><author><keyname>Tsai</keyname><forenames>Ming-Hsien</forenames><affiliation>National Taiwan University</affiliation></author><author><keyname>Fogarty</keyname><forenames>Seth</forenames><affiliation>Trinity University</affiliation></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames><affiliation>Rice University</affiliation></author><author><keyname>Tsay</keyname><forenames>Yih-Kuen</forenames><affiliation>National Taiwan University</affiliation></author></authors><title>State of B\&quot;uchi Complementation</title><categories>cs.FL cs.LO</categories><comments>28 pages, 4 figures, a preliminary version of this paper appeared in
  the Proceedings of the 15th International Conference on Implementation and
  Application of Automata (CIAA)</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  18, 2014) lmcs:1059</journal-ref><doi>10.2168/LMCS-10(4:13)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complementation of B\&quot;uchi automata has been studied for over five decades
since the formalism was introduced in 1960. Known complementation constructions
can be classified into Ramsey-based, determinization-based, rank-based, and
slice-based approaches. Regarding the performance of these approaches, there
have been several complexity analyses but very few experimental results. What
especially lacks is a comparative experiment on all of the four approaches to
see how they perform in practice. In this paper, we review the four approaches,
propose several optimization heuristics, and perform comparative
experimentation on four representative constructions that are considered the
most efficient in each approach. The experimental results show that (1) the
determinization-based Safra-Piterman construction outperforms the other three
in producing smaller complements and finishing more tasks in the allocated time
and (2) the proposed heuristics substantially improve the Safra-Piterman and
the slice-based constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4580</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4580</id><created>2014-06-17</created><authors><author><keyname>Lee</keyname><forenames>Seunghak</forenames></author><author><keyname>Kim</keyname><forenames>Jin Kyu</forenames></author><author><keyname>Zheng</keyname><forenames>Xun</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Gibson</keyname><forenames>Garth A.</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Primitives for Dynamic Big Model Parallelism</title><categories>stat.ML cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When training large machine learning models with many variables or
parameters, a single machine is often inadequate since the model may be too
large to fit in memory, while training can take a long time even with
stochastic updates. A natural recourse is to turn to distributed cluster
computing, in order to harness additional memory and processors. However,
naive, unstructured parallelization of ML algorithms can make inefficient use
of distributed memory, while failing to obtain proportional convergence
speedups - or can even result in divergence. We develop a framework of
primitives for dynamic model-parallelism, STRADS, in order to explore
partitioning and update scheduling of model variables in distributed ML
algorithms - thus improving their memory efficiency while presenting new
opportunities to speed up convergence without compromising inference
correctness. We demonstrate the efficacy of model-parallel algorithms
implemented in STRADS versus popular implementations for Topic Modeling, Matrix
Factorization and Lasso.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4600</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4600</id><created>2014-06-18</created><authors><author><keyname>Kuijper</keyname><forenames>Margreta</forenames></author><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>Gr\&quot;obner Bases for Linearized Polynomials</title><categories>cs.SC cs.IT math.IT math.RA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we develop the theory of Gr\&quot;obner bases for modules over the
ring of univariate linearized polynomials with coefficients from a finite
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4607</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4607</id><created>2014-06-18</created><updated>2014-07-08</updated><authors><author><keyname>Jalan</keyname><forenames>Sarika</forenames></author><author><keyname>Sarkar</keyname><forenames>Camellia</forenames></author><author><keyname>Madhusudanan</keyname><forenames>Anagha</forenames></author><author><keyname>Dwivedi</keyname><forenames>Sanjiv Kumar</forenames></author></authors><title>Uncovering Randomness and Success in Society</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>39 pages, 12 figures, 14 tables</comments><journal-ref>PloS one, 9(2), e88249 (2014)</journal-ref><doi>10.1371/journal.pone.0088249</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An understanding of how individuals shape and impact the evolution of society
is vastly limited due to the unavailability of large-scale reliable datasets
that can simultaneously capture information regarding individual movements and
social interactions. We believe that the popular Indian film industry,
'Bollywood', can provide a social network apt for such a study. Bollywood
provides massive amounts of real, unbiased data that spans more than 100 years,
and hence this network has been used as a model for the present paper. The
nodes which maintain a moderate degree or widely cooperate with the other nodes
of the network tend to be more fit (measured as the success of the node in the
industry) in comparison to the other nodes. The analysis carried forth in the
current work, using a conjoined framework of complex network theory and random
matrix theory, aims to quantify the elements that determine the fitness of an
individual node and the factors that contribute to the robustness of a network.
The authors of this paper believe that the method of study used in the current
paper can be extended to study various other industries and organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4610</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4610</id><created>2014-06-18</created><authors><author><keyname>Borujeny</keyname><forenames>Reza Rafie</forenames></author><author><keyname>Noori</keyname><forenames>Moslem</forenames></author><author><keyname>Ardakani</keyname><forenames>Masoud</forenames></author></authors><title>On the Achievable Rates of Pairwise Multiway Relay Channels</title><categories>cs.IT math.IT</categories><comments>Extended version of &quot;On the Achievable Rates of Pairwise Multiway
  Relay Channels&quot; accepted for ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the effect of users' transmission ordering on the
common rate and sum rate of pairwise multiway relay channels (MWRCs) with
functional-decode-forward strategy. To this end, we first develop a graphical
model for the data transmission in a pairwise MWRC. Using this model, we then
find the optimal orderings that achieve the maximum common rate and sum rate of
the system. The achieved maximum common and sum rate are also found. Moreover,
we show that the performance gap between optimal orderings and a random
ordering vanishes when SNR increases. Computer simulations are presented for
better illustration of the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4619</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4619</id><created>2014-06-18</created><authors><author><keyname>Chotard</keyname><forenames>Alexandre</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Holena</keyname><forenames>Martin</forenames></author></authors><title>A Generalized Markov-Chain Modelling Approach to $(1,\lambda)$-ES Linear
  Optimization: Technical Report</title><categories>cs.NA cs.LG cs.NE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several recent publications investigated Markov-chain modelling of linear
optimization by a $(1,\lambda)$-ES, considering both unconstrained and linearly
constrained optimization, and both constant and varying step size. All of them
assume normality of the involved random steps, and while this is consistent
with a black-box scenario, information on the function to be optimized (e.g.
separability) may be exploited by the use of another distribution. The
objective of our contribution is to complement previous studies realized with
normal steps, and to give sufficient conditions on the distribution of the
random steps for the success of a constant step-size $(1,\lambda)$-ES on the
simple problem of a linear function with a linear constraint. The decomposition
of a multidimensional distribution into its marginals and the copula combining
them is applied to the new distributional assumptions, particular attention
being paid to distributions with Archimedean copulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4620</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4620</id><created>2014-06-18</created><authors><author><keyname>Henry</keyname><forenames>Renaud</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Porez</keyname><forenames>Mathieu</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Boyer</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Kanaan</keyname><forenames>Daniel</forenames></author></authors><title>Multi-Objective Design Optimization of the Leg Mechanism for a Piping
  Inspection Robot</title><categories>cs.RO</categories><comments>Proceedings of the ASME 2014 International Design Engineering
  Technical Conferences \&amp; Computers and Information in Engineering Conference,
  Buffalo : United States (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the dimensional synthesis of an adaptive mechanism of
contact points ie a leg mechanism of a piping inspection robot operating in an
irradiated area as a nuclear power plant. This studied mechanism is the leading
part of the robot sub-system responsible of the locomotion. Firstly, three
architectures are chosen from the literature and their properties are
described. Then, a method using a multi-objective optimization is proposed to
determine the best architecture and the optimal geometric parameters of a leg
taking into account environmental and design constraints. In this context, the
objective functions are the minimization of the mechanism size and the
maximization of the transmission force factor. Representations of the Pareto
front versus the objective functions and the design parameters are given.
Finally, the CAD model of several solutions located on the Pareto front are
presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4625</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4625</id><created>2014-06-18</created><updated>2015-03-04</updated><authors><author><keyname>Shahriari</keyname><forenames>Bobak</forenames></author><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author><author><keyname>Hoffman</keyname><forenames>Matthew W.</forenames></author><author><keyname>Bouchard-C&#xf4;t&#xe9;</keyname><forenames>Alexandre</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>An Entropy Search Portfolio for Bayesian Optimization</title><categories>stat.ML cs.LG</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimization is a sample-efficient method for black-box global
optimization. How- ever, the performance of a Bayesian optimization method very
much depends on its exploration strategy, i.e. the choice of acquisition
function, and it is not clear a priori which choice will result in superior
performance. While portfolio methods provide an effective, principled way of
combining a collection of acquisition functions, they are often based on
measures of past performance which can be misleading. To address this issue, we
introduce the Entropy Search Portfolio (ESP): a novel approach to portfolio
construction which is motivated by information theoretic considerations. We
show that ESP outperforms existing portfolio methods on several real and
synthetic problems, including geostatistical datasets and simulated control
tasks. We not only show that ESP is able to offer performance as good as the
best, but unknown, acquisition function, but surprisingly it often gives better
performance. Finally, over a wide range of conditions we find that ESP is
robust to the inclusion of poor acquisition functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4628</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4628</id><created>2014-06-18</created><authors><author><keyname>Khatwal</keyname><forenames>Ravi</forenames></author><author><keyname>Jain</keyname><forenames>Manoj Kumar</forenames></author></authors><title>An Efficient Synchronous Static Memory design for Embedded System</title><categories>cs.AR</categories><comments>Embeddded system, International Journal of Computer
  Applications(2013)</comments><doi>10.5120/11187-6411 10.5120/11187-6411</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Custom memory organization are challenging task in the area of VLSI design.
This study aims to design high speed and low power consumption memory for
embedded system. Synchronous SRAM has been proposed and analyzed using various
simulators. Xilinx simulator simulates the Synchronous SRAM memories which can
perform efficient read/write capability for embedded systems. Xinix tool also
provide the access time that required selecting a word and reading it.
Synchronous Static RAM which has easily read /writes capability and performs
scheduled read /writes operation in efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4631</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4631</id><created>2014-06-18</created><authors><author><keyname>Zhao</keyname><forenames>Han</forenames></author><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author></authors><title>A Sober Look at Spectral Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral learning recently generated lots of excitement in machine learning,
largely because it is the first known method to produce consistent estimates
(under suitable conditions) for several latent variable models. In contrast,
maximum likelihood estimates may get trapped in local optima due to the
non-convex nature of the likelihood function of latent variable models. In this
paper, we do an empirical evaluation of spectral learning (SL) and expectation
maximization (EM), which reveals an important gap between the theory and the
practice. First, SL often leads to negative probabilities. Second, EM often
yields better estimates than spectral learning and it does not seem to get
stuck in local optima. We discuss how the rank of the model parameters and the
amount of training data can yield negative probabilities. We also question the
common belief that maximum likelihood estimators are necessarily inconsistent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4641</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4641</id><created>2014-06-18</created><authors><author><keyname>Thramboulidis</keyname><forenames>Kleanthis</forenames></author></authors><title>Comments on &quot;A model-based design methodology for the development of
  mechatronic systems&quot;</title><categories>cs.SE cs.SY</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper by G. Barbieri et al. (Mechatronics (2014),
http://dx.doi.org/10.1016/j.mechatronics. 2013.12.004), a design methodology,
based on the W life cycle process model, is presented and SysML is proposed as
a tool to support the whole development process. In this letter, we discuss the
presented approach, we point out technical errors and raise additional issues
that might help in making the proposed approach applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4648</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4648</id><created>2014-06-18</created><authors><author><keyname>Horn</keyname><forenames>Florian</forenames></author><author><keyname>Thomas</keyname><forenames>Wolfgang</forenames></author><author><keyname>Wallmeier</keyname><forenames>Nico</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Optimal Strategy Synthesis for Request-Response Games</title><categories>cs.FL cs.LO</categories><comments>The present paper is a revised version with simplified proofs of
  results announced in the conference paper of the same name presented at ATVA
  2008, which in turn extended results of the third author's dissertation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the existence and effective computability of optimal winning
strategies for request-response games in case the quality of a play is measured
by the limit superior of the mean accumulated waiting times between requests
and their responses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4679</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4679</id><created>2014-06-18</created><authors><author><keyname>Berkholz</keyname><forenames>Christoph</forenames></author></authors><title>The Propagation Depth of Local Consistency</title><categories>cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish optimal bounds on the number of nested propagation steps in
$k$-consistency tests. It is known that local consistency algorithms such as
arc-, path- and $k$-consistency are not efficiently parallelizable. Their
inherent sequential nature is caused by long chains of nested propagation
steps, which cannot be executed in parallel. This motivates the question &quot;What
is the minimum number of nested propagation steps that have to be performed by
$k$-consistency algorithms on (binary) constraint networks with $n$ variables
and domain size $d$?&quot;
  It was known before that 2-consistency requires $\Omega(nd)$ and
3-consistency requires $\Omega(n^2)$ sequential propagation steps. We answer
the question exhaustively for every $k\geq 2$: there are binary constraint
networks where any $k$-consistency procedure has to perform
$\Omega(n^{k-1}d^{k-1})$ nested propagation steps before local inconsistencies
were detected. This bound is tight, because the overall number of propagation
steps performed by $k$-consistency is at most $n^{k-1}d^{k-1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4681</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4681</id><created>2014-06-18</created><authors><author><keyname>Kashyap</keyname><forenames>Parul</forenames></author><author><keyname>Singh</keyname><forenames>Rahul</forenames></author></authors><title>Crypto multi tenant: an environment of secure computing using cloud sql</title><categories>cs.CR</categories><comments>9 pages, 5 figures, IEEE journals, connferences 2009,2010; springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our proposed work we mainly focus on data security of tenants so that only
the authorized user can access the data. To provide this feature we use
encryption and decryption process of data so that the only legal tenant can
access their particular data. Hence, for security related to data of tenants we
implement AES (advanced encryption standard) using cloud SQL. The result
obtained from experimental methodology proved that AES gives protection for the
data stored in the cloud. We make use of AES algorithm and Google App Engine to
supply secured data storage, efficiency, assure availability in the condition
of cloud denial-of-service attacks and data security in the cloud. This
Approach is basically implemented by tenants who are going to store their data
in the cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4682</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4682</id><created>2014-06-18</created><authors><author><keyname>Sun</keyname><forenames>Xu</forenames></author></authors><title>Exact Decoding on Latent Variable Conditional Models is NP-Hard</title><categories>cs.AI cs.CC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent variable conditional models, including the latent conditional random
fields as a special case, are popular models for many natural language
processing and vision processing tasks. The computational complexity of the
exact decoding/inference in latent conditional random fields is unclear. In
this paper, we try to clarify the computational complexity of the exact
decoding. We analyze the complexity and demonstrate that it is an NP-hard
problem even on a sequential labeling setting. Furthermore, we propose the
latent-dynamic inference (LDI-Naive) method and its bounded version
(LDI-Bounded), which are able to perform exact-inference or
almost-exact-inference by using top-$n$ search and dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4689</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4689</id><created>2014-06-18</created><updated>2014-09-21</updated><authors><author><keyname>Rached</keyname><forenames>Nadhir Ben</forenames></author><author><keyname>Benkhelifa</keyname><forenames>Fatma</forenames></author><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Tempone</keyname><forenames>Raul</forenames></author></authors><title>A Fast Simulation Method for the Sum of Subexponential Distributions</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the probability that a sum of random variables (RVs) exceeds a
given threshold is a well-known challenging problem. Closed-form expression of
the sum distribution is usually intractable and presents an open problem. A
crude Monte Carlo (MC) simulation is the standard technique for the estimation
of this type of probability. However, this approach is computationally
expensive especially when dealing with rare events (i.e events with very small
probabilities). Importance Sampling (IS) is an alternative approach which
effectively improves the computational efficiency of the MC simulation. In this
paper, we develop a general framework based on IS approach for the efficient
estimation of the probability that the sum of independent and not necessarily
identically distributed heavy-tailed RVs exceeds a given threshold. The
proposed IS approach is based on constructing a new sampling distribution by
twisting the hazard rate of the original underlying distribution of each
component in the summation. A minmax approach is carried out for the
determination of the twisting parameter, for any given threshold. Moreover,
using this minmax optimal choice, the estimation of the probability of interest
is shown to be asymptotically optimal as the threshold goes to infinity. We
also offer some selected simulation results illustrating first the efficiency
of the proposed IS approach compared to the naive MC simulation. The
near-optimality of the minmax approach is then numerically analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4690</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4690</id><created>2014-06-18</created><authors><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author><author><keyname>Clark</keyname><forenames>Stephen</forenames></author><author><keyname>Coecke</keyname><forenames>Bob</forenames></author></authors><title>The Frobenius anatomy of word meanings II: possessive relative pronouns</title><categories>cs.CL math.CT</categories><comments>40 pages, Journal of Logic and Computation, Essays dedicated to Roy
  Dyckhoff on the occasion of his retirement, S. Graham-Lengrand and D.
  Galmiche (eds.), 2014</comments><msc-class>18Dxx, 18Axx</msc-class><acm-class>I.2.7; F.4.1</acm-class><doi>10.1093/logcom/exu027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the categorical compositional distributional model of meaning, we
provide semantic interpretations for the subject and object roles of the
possessive relative pronoun `whose'. This is done in terms of Frobenius
algebras over compact closed categories. These algebras and their diagrammatic
language expose how meanings of words in relative clauses interact with each
other. We show how our interpretation is related to Montague-style semantics
and provide a truth-theoretic interpretation. We also show how vector spaces
provide a concrete interpretation and provide preliminary corpus-based
experimental evidence. In a prequel to this paper, we used similar methods and
dealt with the case of subject and object relative pronouns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4692</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4692</id><created>2014-06-18</created><authors><author><keyname>Heck</keyname><forenames>Petra</forenames></author><author><keyname>Zaidman</keyname><forenames>Andy</forenames></author></authors><title>A Quality Framework for Agile Requirements: A Practitioner's Perspective</title><categories>cs.SE</categories><report-no>TUD-SERG-2014-006</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verification activities are necessary to ensure that the requirements are
specified in a correct way. However, until now requirements verification
research has focused on traditional up-front requirements. Agile or
just-in-time requirements are by definition incomplete, not specific and might
be ambiguous when initially specified, indicating a different notion of
'correctness'. We analyze how verification of agile requirements quality should
be performed, based on literature of traditional and agile requirements. This
leads to an agile quality framework, instantiated for the specific requirement
types of feature requests in open source projects and user stories in agile
projects. We have performed an initial qualitative validation of our framework
for feature requests with eight practitioners from the Dutch agile community,
receiving overall positive feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4698</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4698</id><created>2014-06-18</created><authors><author><keyname>Laitinen</keyname><forenames>Tero</forenames></author><author><keyname>Junttila</keyname><forenames>Tommi</forenames></author><author><keyname>Niemel&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Classifying and Propagating Parity Constraints (extended version)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parity constraints, common in application domains such as circuit
verification, bounded model checking, and logical cryptanalysis, are not
necessarily most efficiently solved if translated into conjunctive normal form.
Thus, specialized parity reasoning techniques have been developed in the past
for propagating parity constraints. This paper studies the questions of
deciding whether unit propagation or equivalence reasoning is enough to achieve
full propagation in a given parity constraint set. Efficient approximating
tests for answering these questions are developed. It is also shown that
equivalence reasoning can be simulated by unit propagation by adding a
polynomial amount of redundant parity constraints to the problem. It is proven
that without using additional variables, an exponential number of new parity
constraints would be needed in the worst case. The presented classification and
propagation methods are evaluated experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4701</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4701</id><created>2014-06-18</created><authors><author><keyname>Lanjewar</keyname><forenames>Rahul R</forenames></author><author><keyname>Adane</keyname><forenames>D S</forenames></author></authors><title>Comparative Study of MAC Layer Protocols in Wireless Sensor Networks: A
  Survey</title><categories>cs.NI</categories><comments>7 pages, 7 figures, &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V12(1),13-19 June 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V12P204</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Wireless Sensor Networks (WSN) is the collection of many small size low cost,
battery operated sensor nodes distributed over the targeted region to collect
the information of interest. We can say these networks can be a fruitful
solution for many applications such as target tracking, intrusion detection
etc. Whenever we are talking about MAC layer protocols we need to give stress
on energy efficiency factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4710</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4710</id><created>2014-06-18</created><authors><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames></author></authors><title>Typed Hilbert Epsilon Operators and the Semantics of Determiner Phrases
  (Invited Lecture)</title><categories>cs.CL cs.AI cs.LO math.LO</categories><msc-class>03B65, 03B15, 03B40, 68T50, 68N18</msc-class><doi>10.1007/978-3-662-44121-3_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantics of determiner phrases, be they definite de- scriptions,
indefinite descriptions or quantified noun phrases, is often as- sumed to be a
fully solved question: common nouns are properties, and determiners are
generalised quantifiers that apply to two predicates: the property
corresponding to the common noun and the one corresponding to the verb phrase.
We first present a criticism of this standard view. Firstly, the semantics of
determiners does not follow the syntactical structure of the sentence. Secondly
the standard interpretation of the indefinite article cannot ac- count for
nominal sentences. Thirdly, the standard view misses the linguis- tic asymmetry
between the two properties of a generalised quantifier. In the sequel, we
propose a treatment of determiners and quantifiers as Hilbert terms in a richly
typed system that we initially developed for lexical semantics, using a many
sorted logic for semantical representations. We present this semantical
framework called the Montagovian generative lexicon and show how these terms
better match the syntactical structure and avoid the aforementioned problems of
the standard approach. Hilbert terms rather differ from choice functions in
that there is one polymorphic operator and not one operator per formula. They
also open an intriguing connection between the logic for meaning assembly, the
typed lambda calculus handling compositionality and the many-sorted logic for
semantical representations. Furthermore epsilon terms naturally introduce
type-judgements and confirm the claim that type judgment are a form of
presupposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4712</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4712</id><created>2014-06-18</created><updated>2014-07-15</updated><authors><author><keyname>Sule</keyname><forenames>Virendra</forenames></author></authors><title>An algorithm for Boolean satisfiability based on generalized orthonormal
  expansion</title><categories>cs.DS math.LO</categories><comments>24 pages</comments><msc-class>03G05, 06E30, 94C10</msc-class><acm-class>I.1.2; F.2.2; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an algorithm for deciding consistency of systems of
Boolean equations in several variables with co-efficients in the two element
Boolean algebra $B_{0}=\{0,1\}$ and find all satisfying assignments. The
algorithm is based on the application of a well known generalized Boole-Shannon
orthonormal (ON) expansion of Boolean functions. A necessary and sufficient
consistency condition for a special class of functions was developed in
\cite{sule} using such an expansion. Paper \cite{sule} develops a condition for
consistency of the equation $f(X)=0$ for the special classes of Boolean
functions 1) $f$ in $B(\Phi(X))$ for an ON set $\Phi$ of Boolean functions in
$X$ over a general Boolean algebra $B$ and 2) $f$ in $B(X_{2})(\Phi(X_{1}))$.
The present paper addresses the problem of obtaining the consistency conditions
for arbitrary Boolean functions in $B_{0}(X)$. Next, the consistency for a
single equation is shown equivalent to another system of Boolean equations
which involves the ON functions and characterizes all solutions. This result is
then extended for Boolean systems in several variables over the algebra
$B_{0}=\{0,1\}$ which does not convert the system into a single equation. This
condition leads to the algorithm for computing all solutions of the Boolean
system without using analogous resolution and determine satisfiability. For
special systems defined by CNF formulas this algorithm results into an
extension of the DPLL algorithm in which the \emph{splitting rule} is
generalized to several variables in terms of ON terms in the sense that
splitting of CNF set in a single variable $x$ is equivalent to ON terms $x,x'$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4718</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4718</id><created>2014-06-18</created><updated>2014-10-16</updated><authors><author><keyname>Bulian</keyname><forenames>Jannis</forenames></author><author><keyname>Dawar</keyname><forenames>Anuj</forenames></author></authors><title>Graph Isomorphism Parameterized by Elimination Distance to Bounded
  Degree</title><categories>cs.DS cs.CC cs.DM</categories><comments>19 pages</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A commonly studied means of parameterizing graph problems is the deletion
distance from triviality (Guo et al. 2004), which counts vertices that need to
be deleted from a graph to place it in some class for which efficient
algorithms are known. In the context of graph isomorphism, we define triviality
to mean a graph with maximum degree bounded by a constant, as such graph
classes admit polynomial-time isomorphism tests. We generalise deletion
distance to a measure we call elimination distance to triviality, based on
elimination trees or tree-depth decompositions. We establish that graph
canonisation, and thus graph isomorphism, is FPT when parameterized by
elimination distance to bounded degree, extending results of Bouland et al.
(2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4729</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4729</id><created>2014-06-18</created><updated>2015-04-23</updated><authors><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Zhang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Ren</keyname><forenames>Shaoqing</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual
  Recognition</title><categories>cs.CV</categories><comments>This manuscript is the accepted version for IEEE Transactions on
  Pattern Analysis and Machine Intelligence (TPAMI) 2015. See Changelog</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing deep convolutional neural networks (CNNs) require a fixed-size
(e.g., 224x224) input image. This requirement is &quot;artificial&quot; and may reduce
the recognition accuracy for the images or sub-images of an arbitrary
size/scale. In this work, we equip the networks with another pooling strategy,
&quot;spatial pyramid pooling&quot;, to eliminate the above requirement. The new network
structure, called SPP-net, can generate a fixed-length representation
regardless of image size/scale. Pyramid pooling is also robust to object
deformations. With these advantages, SPP-net should in general improve all
CNN-based image classification methods. On the ImageNet 2012 dataset, we
demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures
despite their different designs. On the Pascal VOC 2007 and Caltech101
datasets, SPP-net achieves state-of-the-art classification results using a
single full-image representation and no fine-tuning.
  The power of SPP-net is also significant in object detection. Using SPP-net,
we compute the feature maps from the entire image only once, and then pool
features in arbitrary regions (sub-images) to generate fixed-length
representations for training the detectors. This method avoids repeatedly
computing the convolutional features. In processing test images, our method is
24-102x faster than the R-CNN method, while achieving better or comparable
accuracy on Pascal VOC 2007.
  In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our
methods rank #2 in object detection and #3 in image classification among all 38
teams. This manuscript also introduces the improvement made for this
competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4736</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4736</id><created>2014-06-18</created><authors><author><keyname>Cocco</keyname><forenames>Giuseppe</forenames></author><author><keyname>Pfletschinger</keyname><forenames>Stephan</forenames></author><author><keyname>Navarro</keyname><forenames>Monica</forenames></author></authors><title>Seek and Decode: Random Access with Physical-Layer Network Coding and
  Multiuser Detection</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel cross layer approach to random access (RA) that combines
physical-layer network coding (PLNC) with multiuser detection (MUD). PLNC and
MUD are applied jointly at the physical level in order to extract any linear
combination of messages experiencing a collision. The set of combinations
extracted from a whole frame is then processed by the receiver to recover the
original packets. A simple pre-coding stage at the transmitting terminals
allows the receiver to further increase system diversity. We derive an
analytical bound on the system throughput and present simulation results for
the decoding at the physical level as well as several performance measures at
frame level in block fading channels, namely throughput, packet loss rate and
energy efficiency. The results we present are promising and suggest that a
cross layer approach leveraging on the joint use of PLNC and MUD can
significantly improve the performance of RA systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4737</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4737</id><created>2014-06-18</created><authors><author><keyname>Chakraborty</keyname><forenames>Sanjay</forenames></author><author><keyname>Nagwani</keyname><forenames>N. K.</forenames></author></authors><title>Performance Evaluation of Incremental K-means Clustering Algorithm</title><categories>cs.IR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The incremental K-means clustering algorithm has already been proposed and
analysed in paper [Chakraborty and Nagwani, 2011]. It is a very innovative
approach which is applicable in periodically incremental environment and
dealing with a bulk of updates. In this paper the performance evaluation is
done for this incremental K-means clustering algorithm using air pollution
database. This paper also describes the comparison on the performance
evaluations between existing K-means clustering and incremental K-means
clustering using that particular database. It also evaluates that the
particular point of change in the database upto which incremental K-means
clustering performs much better than the existing K-means clustering. That
particular point of change in the database is known as &quot;Threshold value&quot; or &quot;%
delta change in the database&quot;. This paper also defines the basic methodology
for the incremental K-means clustering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4748</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4748</id><created>2014-06-18</created><authors><author><keyname>Majumder</keyname><forenames>Sanjay</forenames></author><author><keyname>Chakraborty</keyname><forenames>Sanjay</forenames></author><author><keyname>Das</keyname><forenames>Suman</forenames></author></authors><title>A New Advanced User Authentication and Confidentiality Security Service</title><categories>cs.CR</categories><doi>10.5120/16257-5904</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network &amp; internet security is the burning question of today's world and they
are deeply related to each other for secure successful data transmission.
Network security approach is totally based on the concept of network security
services. In this paper, a new system of network security service is
implemented which is more secure than conventional network security services.
This technique is mainly deals with two essential network security services,
one is user authentication and other is data confidentiality. For user
authentication this paper introduces Graphical Username &amp; Voice Password
approaches which provides better security than conventional username &amp; password
authentication process. In data confidentiality section this paper introduces
two layer private key for both message encryption &amp; decryption which is mainly
applicable on 8 bit plain text data. This paper also provides the hints of
introducing other two network security services (integrity and non-repudiation)
as a future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4751</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4751</id><created>2014-06-18</created><authors><author><keyname>Chakraborty</keyname><forenames>Sanjay</forenames></author><author><keyname>Nagwani</keyname><forenames>N. K.</forenames></author><author><keyname>Dey</keyname><forenames>Lopamudra</forenames></author></authors><title>Performance Comparison of Incremental K-means and Incremental DBSCAN
  Algorithms</title><categories>cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incremental K-means and DBSCAN are two very important and popular clustering
techniques for today's large dynamic databases (Data warehouses, WWW and so on)
where data are changed at random fashion. The performance of the incremental
K-means and the incremental DBSCAN are different with each other based on their
time analysis characteristics. Both algorithms are efficient compare to their
existing algorithms with respect to time, cost and effort. In this paper, the
performance evaluation of incremental DBSCAN clustering algorithm is
implemented and most importantly it is compared with the performance of
incremental K-means clustering algorithm and it also explains the
characteristics of these two algorithms based on the changes of the data in the
database. This paper also explains some logical differences between these two
most popular clustering algorithms. This paper uses an air pollution database
as original database on which the experiment is performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4754</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4754</id><created>2014-06-18</created><authors><author><keyname>Chakraborty</keyname><forenames>Sanjay</forenames></author><author><keyname>Nagwani</keyname><forenames>N. K.</forenames></author></authors><title>Analysis and Study of Incremental DBSCAN Clustering Algorithm</title><categories>cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the incremental behaviours of Density based clustering.
It specially focuses on the Density Based Spatial Clustering of Applications
with Noise (DBSCAN) algorithm and its incremental approach.DBSCAN relies on a
density based notion of clusters.It discovers clusters of arbitrary shapes in
spatial databases with noise.In incremental approach, the DBSCAN algorithm is
applied to a dynamic database where the data may be frequently updated. After
insertions or deletions to the dynamic database, the clustering discovered by
DBSCAN has to be updated. And we measure the new cluster by directly compute
the new data entering into the existing clusters instead of rerunning the
algorithm.It finally discovers new updated clusters and outliers as well.Thus
it describes at what percent of delta change in the original database the
actual and incremental DBSCAN algorithms behave like same.DBSCAN is widely used
in those situations where large multidimensional databases are maintained such
as Data Warehouse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4756</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4756</id><created>2014-06-18</created><authors><author><keyname>Chakraborty</keyname><forenames>Sanjay</forenames></author><author><keyname>Nagwani</keyname><forenames>N. K.</forenames></author><author><keyname>Dey</keyname><forenames>Lopamudra</forenames></author></authors><title>Weather Forecasting using Incremental K-means Clustering</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is a powerful tool which has been used in several forecasting
works, such as time series forecasting, real time storm detection, flood
forecasting and so on. In this paper, a generic methodology for weather
forecasting is proposed by the help of incremental K-means clustering
algorithm. Weather forecasting plays an important role in day to day
applications.Weather forecasting of this paper is done based on the incremental
air pollution database of west Bengal in the years of 2009 and 2010. This paper
generally uses typical K-means clustering on the main air pollution database
and a list of weather category will be developed based on the maximum mean
values of the clusters.Now when the new data are coming, the incremental
K-means is used to group those data into those clusters whose weather category
has been already defined. Thus it builds up a strategy to predict the weather
of the upcoming data of the upcoming days. This forecasting database is totally
based on the weather of west Bengal and this forecasting methodology is
developed to mitigating the impacts of air pollutions and launch focused
modeling computations for prediction and forecasts of weather events. Here
accuracy of this approach is also measured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4757</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4757</id><created>2014-06-18</created><authors><author><keyname>Bagnall</keyname><forenames>Anthony</forenames></author><author><keyname>Lines</keyname><forenames>Jason</forenames></author></authors><title>An Experimental Evaluation of Nearest Neighbour Time Series
  Classification</title><categories>cs.LG</categories><report-no>CMP-C14-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining research into time series classification (TSC) has focussed on
alternative distance measures for nearest neighbour classifiers. It is standard
practice to use 1-NN with Euclidean or dynamic time warping (DTW) distance as a
straw man for comparison. As part of a wider investigation into elastic
distance measures for TSC~\cite{lines14elastic}, we perform a series of
experiments to test whether this standard practice is valid.
  Specifically, we compare 1-NN classifiers with Euclidean and DTW distance to
standard classifiers, examine whether the performance of 1-NN Euclidean
approaches that of 1-NN DTW as the number of cases increases, assess whether
there is any benefit of setting $k$ for $k$-NN through cross validation whether
it is worth setting the warping path for DTW through cross validation and
finally is it better to use a window or weighting for DTW. Based on experiments
on 77 problems, we conclude that 1-NN with Euclidean distance is fairly easy to
beat but 1-NN with DTW is not, if window size is set through cross validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4770</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4770</id><created>2014-06-18</created><authors><author><keyname>Aroquiaraj</keyname><forenames>I. Laurence</forenames></author><author><keyname>Thangavel</keyname><forenames>K.</forenames></author></authors><title>Mass Classification Method in Mammogram Using Fuzzy K-Nearest Neighbour
  Equality</title><categories>cs.CV</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mass classification of objects is an important area of research and
application in a variety of fields. In this paper, we present an efficient
computer aided mass classification method in digitized mammograms using Fuzzy
K-Nearest Neighbor Equality, which performs benign or malignant classification
on region of interest that contains mass. One of the major mammographic
characteristics for mass classification is texture. Fuzzy K-Nearest Neighbor
Equality exploits this important factor to classify the mass into benign or
malignant. The statistical textural features used in characterizing the masses
are Haralick and Run length features. The main aim of the method is to increase
the effectiveness and efficiency of the classification process in an objective
manner to reduce the numbers of false positive of malignancies. In this paper
proposes a novel Fuzzy K-Nearest Neighbor Equality algorithm for classifying
the marked regions into benign and malignant and 94.46 sensitivity,96.81
specificity and 96.52 accuracy is achieved that is very much promising compare
to the radiologists' accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4773</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4773</id><created>2014-06-18</created><authors><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Deep Learning Face Representation by Joint Identification-Verification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key challenge of face recognition is to develop effective feature
representations for reducing intra-personal variations while enlarging
inter-personal differences. In this paper, we show that it can be well solved
with deep learning and using both face identification and verification signals
as supervision. The Deep IDentification-verification features (DeepID2) are
learned with carefully designed deep convolutional networks. The face
identification task increases the inter-personal variations by drawing DeepID2
extracted from different identities apart, while the face verification task
reduces the intra-personal variations by pulling DeepID2 extracted from the
same identity together, both of which are essential to face recognition. The
learned DeepID2 features can be well generalized to new identities unseen in
the training data. On the challenging LFW dataset, 99.15% face verification
accuracy is achieved. Compared with the best deep learning result on LFW, the
error rate has been significantly reduced by 67%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4775</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4775</id><created>2014-06-18</created><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Richard</keyname><forenames>Emile</forenames></author></authors><title>Non-negative Principal Component Analysis: Message Passing Algorithms
  and Sharp Asymptotics</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>51 pages, 7 pdf figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) aims at estimating the direction of
maximal variability of a high-dimensional dataset. A natural question is: does
this task become easier, and estimation more accurate, when we exploit
additional knowledge on the principal vector? We study the case in which the
principal vector is known to lie in the positive orthant. Similar constraints
arise in a number of applications, ranging from analysis of gene expression
data to spike sorting in neural signal processing.
  In the unconstrained case, the estimation performances of PCA has been
precisely characterized using random matrix theory, under a statistical model
known as the `spiked model.' It is known that the estimation error undergoes a
phase transition as the signal-to-noise ratio crosses a certain threshold.
Unfortunately, tools from random matrix theory have no bearing on the
constrained problem. Despite this challenge, we develop an analogous
characterization in the constrained case, within a one-spike model.
  In particular: $(i)$~We prove that the estimation error undergoes a similar
phase transition, albeit at a different threshold in signal-to-noise ratio that
we determine exactly; $(ii)$~We prove that --unlike in the unconstrained case--
estimation error depends on the spike vector, and characterize the least
favorable vectors; $(iii)$~We show that a non-negative principal component can
be approximately computed --under the spiked model-- in nearly linear time.
This despite the fact that the problem is non-convex and, in general, NP-hard
to solve exactly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4781</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4781</id><created>2014-06-18</created><authors><author><keyname>Bagnall</keyname><forenames>Anthony</forenames></author><author><keyname>Davis</keyname><forenames>Luke</forenames></author></authors><title>Predictive Modelling of Bone Age through Classification and Regression
  of Bone Shapes</title><categories>cs.LG physics.med-ph</categories><report-no>CMPC14-02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bone age assessment is a task performed daily in hospitals worldwide. This
involves a clinician estimating the age of a patient from a radiograph of the
non-dominant hand.
  Our approach to automated bone age assessment is to modularise the algorithm
into the following three stages: segment and verify hand outline; segment and
verify bones; use the bone outlines to construct models of age. In this paper
we address the final question: given outlines of bones, can we learn how to
predict the bone age of the patient? We examine two alternative approaches.
Firstly, we attempt to train classifiers on individual bones to predict the
bone stage categories commonly used in bone ageing. Secondly, we construct
regression models to directly predict patient age.
  We demonstrate that models built on summary features of the bone outline
perform better than those built using the one dimensional representation of the
outline, and also do at least as well as other automated systems. We show that
models constructed on just three bones are as accurate at predicting age as
expert human assessors using the standard technique. We also demonstrate the
utility of the model by quantifying the importance of ethnicity and sex on age
development. Our conclusion is that the feature based system of separating the
image processing from the age modelling is the best approach for automated bone
ageing, since it offers flexibility and transparency and produces accurate
estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4782</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4782</id><created>2014-06-18</created><authors><author><keyname>K&#xf6;nig</keyname><forenames>Barbara</forenames></author><author><keyname>St&#xfc;ckrath</keyname><forenames>Jan</forenames></author></authors><title>A General Framework for Well-Structured Graph Transformation Systems</title><categories>cs.LO</categories><comments>Extended version (including proofs) of a paper accepted at CONCUR
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph transformation systems (GTSs) can be seen as wellstructured transition
systems (WSTSs), thus obtaining decidability results for certain classes of
GTSs. In earlier work it was shown that wellstructuredness can be obtained
using the minor ordering as a well-quasiorder. In this paper we extend this
idea to obtain a general framework in which several types of GTSs can be seen
as (restricted) WSTSs. We instantiate this framework with the subgraph ordering
and the induced subgraph ordering and apply it to analyse a simple access
rights management system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4784</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4784</id><created>2014-06-18</created><authors><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Improved Densification of One Permutation Hashing</title><categories>stat.ME cs.DS cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing work on densification of one permutation hashing reduces the
query processing cost of the $(K,L)$-parameterized Locality Sensitive Hashing
(LSH) algorithm with minwise hashing, from $O(dKL)$ to merely $O(d + KL)$,
where $d$ is the number of nonzeros of the data vector, $K$ is the number of
hashes in each hash table, and $L$ is the number of hash tables. While that is
a substantial improvement, our analysis reveals that the existing densification
scheme is sub-optimal. In particular, there is no enough randomness in that
procedure, which affects its accuracy on very sparse datasets.
  In this paper, we provide a new densification procedure which is provably
better than the existing scheme. This improvement is more significant for very
sparse datasets which are common over the web. The improved technique has the
same cost of $O(d + KL)$ for query processing, thereby making it strictly
preferable over the existing procedure. Experimental evaluations on public
datasets, in the task of hashing based near neighbor search, support our
theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4785</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4785</id><created>2014-06-18</created><updated>2015-05-15</updated><authors><author><keyname>Lawyer</keyname><forenames>Glenn</forenames></author></authors><title>Technical Report: Performance of the Expected Force on AS-level Inernet
  topologies</title><categories>cs.NI</categories><comments>2 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many concepts in network science are based on idealized network models.
Infrastructure networks, however, are strongly constrained by specific
engineering and economic constraints which do not appear in such models. One
result is that concepts which are assumed to be fundamental by theoriticians
severely underperform in real world situations. This motivated us to test if
the Expected Force (ExF) metric, which was developed in the context of more
theoretical network models, would continue to be predictive of epidemic
outcomes on the highly structured topology of the internet at the autonomous
systems (AS) level router connectivity. Results suggest that the ExF performs
extremely well on these topologies, perhaps reflecting their design constraint
of easing the diffusion of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4799</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4799</id><created>2014-06-18</created><authors><author><keyname>Gro&#xdf;</keyname><forenames>Martin</forenames></author><author><keyname>Skutella</keyname><forenames>Martin</forenames></author></authors><title>A tight bound on the speed-up through storage for quickest
  multi-commodity flows</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-commodity flows over time exhibit the non-intuitive property that
letting flow wait can allow us to send flow faster overall. Fleischer and
Skutella (IPCO~2002) show that the speed-up through storage is at most a factor
of~$2$, and that there are instances where the speed-up is as large as a factor
of~$4/3$. We close this gap by presenting a family of instances for which the
speed-up factor through storage converges to~$2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4802</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4802</id><created>2014-01-31</created><updated>2015-03-18</updated><authors><author><keyname>Soussen</keyname><forenames>Charles</forenames></author><author><keyname>Idier</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Duan</keyname><forenames>Junbo</forenames></author><author><keyname>Brie</keyname><forenames>David</forenames></author></authors><title>Homotopy based algorithms for $\ell_0$-regularized least-squares</title><categories>cs.NA cs.LG</categories><comments>38 pages</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 63, no. 13, Jul.
  2015, pp. 3301-3316</journal-ref><doi>10.1109/TSP.2015.2421476</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse signal restoration is usually formulated as the minimization of a
quadratic cost function $\|y-Ax\|_2^2$, where A is a dictionary and x is an
unknown sparse vector. It is well-known that imposing an $\ell_0$ constraint
leads to an NP-hard minimization problem. The convex relaxation approach has
received considerable attention, where the $\ell_0$-norm is replaced by the
$\ell_1$-norm. Among the many efficient $\ell_1$ solvers, the homotopy
algorithm minimizes $\|y-Ax\|_2^2+\lambda\|x\|_1$ with respect to x for a
continuum of $\lambda$'s. It is inspired by the piecewise regularity of the
$\ell_1$-regularization path, also referred to as the homotopy path. In this
paper, we address the minimization problem $\|y-Ax\|_2^2+\lambda\|x\|_0$ for a
continuum of $\lambda$'s and propose two heuristic search algorithms for
$\ell_0$-homotopy. Continuation Single Best Replacement is a forward-backward
greedy strategy extending the Single Best Replacement algorithm, previously
proposed for $\ell_0$-minimization at a given $\lambda$. The adaptive search of
the $\lambda$-values is inspired by $\ell_1$-homotopy. $\ell_0$ Regularization
Path Descent is a more complex algorithm exploiting the structural properties
of the $\ell_0$-regularization path, which is piecewise constant with respect
to $\lambda$. Both algorithms are empirically evaluated for difficult inverse
problems involving ill-conditioned dictionaries. Finally, we show that they can
be easily coupled with usual methods of model order selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4803</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4803</id><created>2014-06-18</created><authors><author><keyname>Vadeyar</keyname><forenames>Deepshree A.</forenames></author><author><keyname>K</keyname><forenames>Yogish H.</forenames></author></authors><title>Reorganization of Links to Improve User Navigation</title><categories>cs.HC cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Website can be easily design but to efficient user navigation is not a easy
task since user behavior is keep changing and developer view is quite different
from what user wants, so to improve navigation one way is reorganization of
website structure. For reorganization here proposed strategy is farthest first
traversal clustering algorithm perform clustering on two numeric parameters and
for finding frequent traversal path of user Apriori algorithm is used. Our aim
is to perform reorganization with fewer changes in website structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4806</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4806</id><created>2014-06-03</created><authors><author><keyname>Ooms</keyname><forenames>Jeroen</forenames></author></authors><title>The OpenCPU System: Towards a Universal Interface for Scientific
  Computing through Separation of Concerns</title><categories>stat.CO cs.MS cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications integrating analysis components require a programmable interface
which defines statistical operations independently of any programming language.
By separating concerns of scientific computing from application and
implementation details we can derive an interoperable API for data analysis.
But what exactly are the concerns of scientific computing? To answer this
question, the paper starts with an exploration of the purpose, problems,
characteristics, struggles, culture, and community of this unique branch of
computing. By mapping out the domain logic, we try to unveil the fundamental
principles and concepts behind statistical software. Along the way we highlight
important problems and bottlenecks that need to be addressed by the system in
order to facilitate reliable and scalable analysis units. Finally, the OpenCPU
software is introduced as an example implementation that builds on HTTP and R
to expose a simple, abstracted interface for scientific computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4812</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4812</id><created>2014-05-27</created><updated>2015-11-27</updated><authors><author><keyname>Zhou</keyname><forenames>Michael X.</forenames></author></authors><title>A benchmark generator for boolean quadratic programming</title><categories>cs.NA</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For boolean quadratic programming (BQP), we will show that there is no
duality gap between the primal and dual problems under some conditions by using
the classical Lagrangian duality. A benchmark generator is given to create
random BQP problems which can be solved in polynomial time. Several numerical
examples are generated to demonstrate the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4822</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4822</id><created>2014-06-18</created><authors><author><keyname>Choudhary</keyname><forenames>Aruni</forenames></author><author><keyname>Kerber</keyname><forenames>Michael</forenames></author></authors><title>Local Doubling Dimension of Point Sets</title><categories>cs.CG math.AT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of t-restricted doubling dimension of a point set in
Euclidean space as the local intrinsic dimension up to scale t. In many
applications information is only relevant for a fixed range of scales. We
present an algorithm to construct a hierarchical net-tree up to scale t which
we denote as the net-forest. We present a method based on Locality Sensitive
Hashing to compute all near neighbours of points within a certain distance. Our
construction of the net-forest is probabilistic, and we guarantee that with
high probability, the net-forest is supplemented with the correct neighbouring
information. We apply our net-forest construction scheme to create an
approximate Cech complex up to a fixed scale; and its complexity depends on the
local intrinsic dimension up to that scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4823</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4823</id><created>2014-05-29</created><authors><author><keyname>Rivas</keyname><forenames>Exequiel</forenames></author><author><keyname>Jaskelioff</keyname><forenames>Mauro</forenames></author></authors><title>Notions of Computation as Monoids</title><categories>cs.LO cs.PL math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are different notions of computation, the most popular being monads,
applicative functors, and arrows. In this article we show that these three
notions can be seen as monoids in a monoidal category. We demonstrate that at
this level of abstraction one can obtain useful results which can be
instantiated to the different notions of computation. In particular, we show
how free constructions and Cayley representations for monoids translate into
useful constructions for monads, applicative functors, and arrows. Moreover,
the uniform presentation of all three notions helps in the analysis of the
relation between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4824</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4824</id><created>2014-06-12</created><updated>2015-12-18</updated><authors><author><keyname>Parshad</keyname><forenames>Rana D.</forenames></author><author><keyname>Chand</keyname><forenames>Vineeta</forenames></author><author><keyname>Sinha</keyname><forenames>Neha</forenames></author><author><keyname>Kumari</keyname><forenames>Nitu</forenames></author></authors><title>What is India speaking: The &quot;Hinglish&quot; invasion</title><categories>cs.CL math.DS</categories><comments>This paper has been withdrawan as the model has now been modified and
  the existing model has some errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While language competition models of diachronic language shift are
increasingly sophisticated, drawing on sociolinguistic components like variable
language prestige, distance from language centers and intermediate bilingual
transitionary populations, in one significant way they fall short. They fail to
consider contact-based outcomes resulting in mixed language practices, e.g.
outcome scenarios such as creoles or unmarked code switching as an emergent
communicative norm. On these lines something very interesting is uncovered in
India, where traditionally there have been monolingual Hindi speakers and
Hindi/English bilinguals, but virtually no monolingual English speakers. While
the Indian census data reports a sharp increase in the proportion of
Hindi/English bilinguals, we argue that the number of Hindi/English bilinguals
in India is inaccurate, given a new class of urban individuals speaking a mixed
lect of Hindi and English, popularly known as &quot;Hinglish&quot;. Based on
predator-prey, sociolinguistic theories, salient local ecological factors and
the rural-urban divide in India, we propose a new mathematical model of
interacting monolingual Hindi speakers, Hindi/English bilinguals and Hinglish
speakers. The model yields globally asymptotic stable states of coexistence, as
well as bilingual extinction. To validate our model, sociolinguistic data from
different Indian classes are contrasted with census reports: We see that
purported urban Hindi/English bilinguals are unable to maintain fluent Hindi
speech and instead produce Hinglish, whereas rural speakers evidence
monolingual Hindi. Thus we present evidence for the first time where an
unrecognized mixed lect involving English but not &quot;English&quot;, has possibly taken
over a sizeable faction of a large global population.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4828</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4828</id><created>2014-05-28</created><updated>2015-09-16</updated><authors><author><keyname>Tan</keyname><forenames>Yong</forenames></author></authors><title>Analyzing Traffic Problem Model With Graph Theory Algorithms</title><categories>cs.DS cs.SI math.CO</categories><comments>7 pages, 5 figures, Science and Information Conference (SAI), 2015</comments><msc-class>90B20, 90C27, 90B50</msc-class><acm-class>I.2.8</acm-class><doi>10.1109/SAI.2015.7237137</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper will contribute to a practical problem, Urban Traffic. We will
investigate those features, try to simplify the complexity and formulize this
dynamic system. These contents mainly contain how to analyze a decision problem
with combinatorial method and graph theory algorithms; how to optimize our
strategy to gain a feasible solution through employing other principles of
Computer Science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4829</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4829</id><created>2014-06-18</created><updated>2014-09-24</updated><authors><author><keyname>Fitzsimmons</keyname><forenames>Zack</forenames></author></authors><title>Single-Peaked Consistency for Weak Orders Is Easy</title><categories>cs.GT cs.CC cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that single-peaked consistency for total orders is in P.
However in practice a preference profile is not always comprised of total
orders. Often voters have indifference between some of the candidates. In a
weak preference order indifference must be transitive. We show that
single-peaked consistency for weak orders is in P for the three different
variants of single-peakedness for weak orders. Specifically, we consider
Black's original definition for weak orders (Black 1948), single-plateaued
preferences (Black 1958), and the existential model recently introduced by
Lackner (2014). We accomplish this by transforming each of these single-peaked
consistency problems to the problem of determining if a 0-1 matrix has the
property of consecutive ones in rows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4837</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4837</id><created>2014-06-18</created><authors><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Dworkin</keyname><forenames>Lili</forenames></author></authors><title>A Computational Study of Feasible Repackings in the FCC Incentive
  Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report the results of a computational study of repacking in the FCC
Incentive Auctions. Our interest lies in the structure and constraints of the
solution space of feasible repackings. Our analyses are &quot;mechanism-free&quot;, in
the sense that they identify constraints that must hold regardless of the
reverse auction mechanism chosen or the prices offered for broadcaster
clearing. We examine topics such as the amount of spectrum that can be cleared
nationwide, the geographic distribution of broadcaster clearings required to
reach a clearing target, and the likelihood of reaching clearing targets under
various models for broadcaster participation. Our study uses FCC interference
data and a satisfiability-checking approach, and elucidates both the
unavoidable mathematical constraints on solutions imposed by interference, as
well as additional constraints imposed by assumptions on the participation
decisions of broadcasters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4840</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4840</id><created>2014-06-18</created><authors><author><keyname>Castells-Rufas</keyname><forenames>David</forenames></author><author><keyname>Carrabina</keyname><forenames>Jordi</forenames></author><author><keyname>Marug&#xe1;n</keyname><forenames>Pablo Gonz&#xe1;lez de Aledo</forenames></author><author><keyname>Espeso</keyname><forenames>Pablo S&#xe1;nchez</forenames></author></authors><title>Fast Trace Generation of Many-Core Embedded Systems with Native
  Simulation</title><categories>cs.DC</categories><comments>Proceedings of HIP3ES Workshop, Vienna, January, 21st 2014</comments><acm-class>B.8.2; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded Software development and optimization are complex tasks. Late
availably of hardware platforms, their usual low visibility and
controllability, and their limiting resource constraints makes early
performance estimation an attractive option instead of using the final
execution platform. With early performance estimation, software development can
progress although the real hardware is not yet available or it is too complex
to interact with. In this paper, we present how the native simulation framework
SCoPE is extended to generate OTF trace files. Those trace files can be later
visualized with trace visualization tools, which recently were only used to
optimize HPC workloads in order to iterate in the development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4842</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4842</id><created>2014-05-16</created><authors><author><keyname>Memon</keyname><forenames>A. A.</forenames></author><author><keyname>Wang</keyname><forenames>C.</forenames></author><author><keyname>Naeem</keyname><forenames>M. R.</forenames></author><author><keyname>Tahir</keyname><forenames>M.</forenames></author><author><keyname>Aamir</keyname><forenames>M.</forenames></author></authors><title>A New Web Based Student Annual Review Information System (SARIS) With
  Student Success Prediction</title><categories>cs.CY</categories><comments>4 pages, 7 figures and 2 Tables</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V10(5):275-278 Apr 2014</journal-ref><doi>10.14445/22312803/IJCTT-V10P149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are proposing new web based Student Annual Review
Information System (SARIS) and prediction method for the success of scholar
students to China Scholarship Council(CSC). The main objective of developing
this system is to save the cost of paper, to reduce the risk of data loss, to
decrease the processing time, to reduce the delay in finding for the successful
students. The proposed system and prediction method is intended to be used by
China Scholarship Council; however SARIS and prediction method are quite
generic and can be used by other scholarship agencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4844</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4844</id><created>2014-05-30</created><authors><author><keyname>Sarma</keyname><forenames>Siddhartha</forenames></author><author><keyname>Agnihotri</keyname><forenames>Samar</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Beam-forming for Secure Communication in Amplify-and-Forward Networks:
  An SNR based approach</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of secure communication in Amplify-and-Forward (AF) relay
networks with multiple eavesdroppers is considered. Assuming that a receiver
(destination or eavesdropper) can decode a message only if the received SNR is
above a predefined threshold, we introduce SNR based optimization formulations
to calculate optimal scaling factors for relay nodes in two scenarios. In the
first scenario, we maximize the achievable rate at the legitimate destination,
subject to the condition that the received SNR at each eavesdropper is below
the target threshold. Due to the non-convex nature of the objective function
and eavesdroppers' constraints, we transform variables and obtain a
Quadratically Constrained Quadratic Program (QCQP) with convex constraints,
which can be solved efficiently. When the constraints are not convex, we
consider a Semi-definite relaxation (SDR). In the second scenario, we minimize
the total power consumed by all relay nodes, subject to the condition that the
received SNR at the legitimate destination is above the threshold and at every
eavesdropper, it is below the corresponding threshold. We propose a
semi-definite relaxation of the problem in this scenario and also provide an
analytical lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4845</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4845</id><created>2014-05-16</created><authors><author><keyname>P&#xe9;rez</keyname><forenames>Diego Sebasti&#xe1;n</forenames></author><author><keyname>Bromberg</keyname><forenames>Facundo</forenames></author><author><keyname>Antivilo</keyname><forenames>Francisco Gonzalez</forenames></author></authors><title>Low Cost, High Precision, Autonomous Measurement of Trunk Diameter based
  on Computer Vision</title><categories>cs.CV</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trunk diameter is a variable of agricultural interest, used mainly in the
prediction of fruit trees production. It is correlated with leaf area and
biomass of trees, and consequently gives a good estimate of potential
production of the plants. This work presents a low cost, high precision method
for autonomous measurement of trunk diameter of fruit trees based on Computer
Vision. Autonomous methods based on Computer Vision or other techniques are
introduced in the literature for they present important simplifications in the
measurement process, requiring little to none human decision making. This
presents different advantages for crop management: the method is amenable to be
operated by unknowledgeable personnel, with lower operational costs; it results
in lower stress levels to knowledgeable personnel, avoiding the deterioration
of the measurement quality over time; or it makes the measurement process
amenable to be embedded in larger autonomous systems, allowing more measurement
to be taken with equivalent costs. In a more personal aspect, the present work
is also a successful proof-of-concept for our laboratories and regional
research institutions in favor of autonomous measurements based on Computer
Vision, opening the door to further investigations in other important agronomic
variables measurable by Computer Vision. To date, all existing autonomous
methods are either of low precision, or have a prohibitive cost for massive
agricultural adoption, leaving the manual Vernier caliper or tape measure as
the only choice in most situations. In this work we present an autonomous
solution that is costly effective for mass adoption, and its precision is
competitive (with slight improvements) over the caliper method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4851</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4851</id><created>2014-06-18</created><authors><author><keyname>Moln&#xe1;r</keyname><forenames>F.</forenames><suffix>Jr.</suffix></author><author><keyname>Derzsy</keyname><forenames>N.</forenames></author><author><keyname>Czabarka</keyname><forenames>&#xc9;.</forenames></author><author><keyname>Sz&#xe9;kely</keyname><forenames>L.</forenames></author><author><keyname>Szymanski</keyname><forenames>B. K.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Dominating Scale-Free Networks Using Generalized Probabilistic Methods</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Scientific Reports 4, 6308 (2014)</journal-ref><doi>10.1038/srep06308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study ensemble-based graph-theoretical methods aiming to approximate the
size of the minimum dominating set (MDS) in scale-free networks. We analyze
both analytical upper bounds of dominating sets and numerical realizations for
applications. We propose two novel probabilistic dominating set selection
strategies that are applicable to heterogeneous networks. One of them obtains
the smallest probabilistic dominating set and also outperforms the
deterministic degree-ranked method. We show that a degree-dependent
probabilistic selection method becomes optimal in its deterministic limit. In
addition, we also find the precise limit where selecting high-degree nodes
exclusively becomes inefficient for network domination. We validate our results
on several real-world networks, and provide highly accurate analytical
estimates for our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4852</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4852</id><created>2014-06-18</created><authors><author><keyname>Duursma</keyname><forenames>Iwan M.</forenames></author></authors><title>Outer bounds for exact repair codes</title><categories>cs.IT math.IT</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the open problem of establishing the rate region for exact-repair
regenerating codes for given parameters (n,k,d). Tian determined the rate
region for a (4,3,3) code and found that it lies strictly within the
functional-repair rate region. Using different methods, Sasidharan, Senthoor
and Kumar proved a non-vanishing gap between the functional-repair outer bound
and the exact-repair outer bound for codes with k&gt;=3. Our main results are two
improved outer bounds for exact-repair regenerating codes. They capture and
then extend essential parts in the proofs by Tian and by Sasidharan, Senthoor
and Kumar. We show that the bounds can be combined for further improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4877</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4877</id><created>2014-06-18</created><authors><author><keyname>Raposo</keyname><forenames>Francisco</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author></authors><title>On the Application of Generic Summarization Algorithms to Music</title><categories>cs.IR cs.LG cs.SD</categories><comments>12 pages, 1 table; Submitted to IEEE Signal Processing Letters</comments><acm-class>H.5.5</acm-class><journal-ref>IEEE Signal Processing Letters, IEEE, vol. 22, n. 1, January 2015</journal-ref><doi>10.1109/LSP.2014.2347582</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several generic summarization algorithms were developed in the past and
successfully applied in fields such as text and speech summarization. In this
paper, we review and apply these algorithms to music. To evaluate this
summarization's performance, we adopt an extrinsic approach: we compare a Fado
Genre Classifier's performance using truncated contiguous clips against the
summaries extracted with those algorithms on 2 different datasets. We show that
Maximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA)
all improve classification performance in both datasets used for testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4879</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4879</id><created>2014-05-29</created><authors><author><keyname>Schipor</keyname><forenames>Ovidiu-Andrei</forenames></author><author><keyname>Nestor</keyname><forenames>Titus-Marian</forenames></author></authors><title>Automat Parsing of Audio Recordings. Testing Children with Dyslalia.
  Theoretical Background</title><categories>cs.CY</categories><comments>6 pages, 3 figures, in Romanian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present our researches regarding automat parsing of audio
recordings. These recordings are obtained from children with dyslalia and are
necessary for an accurate identification of speech problems. We develop the ADM
algorithm and we analyze the complexity of this solution. We utilize a digital
voice recorder in High Quality mode and with VCVA (Variable Control Voice
Actuator) activated. The record format is IMA-ADPCM, 16KHz and 4bits (16 bits
PCM). A microphone was placed at 10 cm from mouth in order to minimize
environment noise. A software set of classes (C#) was created for handling
audio stream (read, conversion between different format, write). We also
propose an original solution for placing markers in audio stream. These markers
are needed for a correct parsing af full recoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4881</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4881</id><created>2014-05-29</created><authors><author><keyname>Schipor</keyname><forenames>Ovidiu-Andrei</forenames></author><author><keyname>Pentiuc</keyname><forenames>Stefan-Gheorghe</forenames></author><author><keyname>Schipor</keyname><forenames>Maria-Doina</forenames></author></authors><title>Architecture of a Fuzzy Expert System Used for Dyslalic Children Therapy</title><categories>cs.AI cs.CY</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present architecture of a fuzzy expert system used for
therapy of dyslalic children. With fuzzy approach we can create a better model
for speech therapist decisions. A software interface was developed for
validation of the system. The main objectives of this task are: personalized
therapy (the therapy must be in according with child's problems level, context
and possibilities), speech therapist assistant (the expert system offer some
suggestion regarding what exercises are better for a specific moment and from a
specific child), (self) teaching (when system's conclusion is different that
speech therapist's conclusion the last one must have the knowledge base change
possibility).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4882</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4882</id><created>2014-05-29</created><authors><author><keyname>Schipor</keyname><forenames>Ovidiu-Andrei</forenames></author><author><keyname>Pentiuc</keyname><forenames>Stefan-Gheorghe</forenames></author><author><keyname>Schipor</keyname><forenames>Doina-Maria</forenames></author></authors><title>Knowledge Base of an Expert System Used for Dyslalic Children Therapy</title><categories>cs.AI</categories><comments>4 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to improve children speech therapy, we develop a Fuzzy Expert System
based on a speech therapy guide. This guide, write in natural language, was
formalized using fuzzy logic paradigm. In this manner we obtain a knowledge
base with over 150 rules and 19 linguistic variables. All these researches,
including expert system validation, are part of TERAPERS project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4905</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4905</id><created>2014-06-18</created><updated>2014-11-03</updated><authors><author><keyname>Frigola</keyname><forenames>Roger</forenames></author><author><keyname>Chen</keyname><forenames>Yutian</forenames></author><author><keyname>Rasmussen</keyname><forenames>Carl E.</forenames></author></authors><title>Variational Gaussian Process State-Space Models</title><categories>cs.LG cs.RO cs.SY stat.ML</categories><journal-ref>R. Frigola, Y. Chen and C. E. Rasmussen. Variational Gaussian
  Process State-Space Models, in Advances in Neural Information Processing
  Systems (NIPS), 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-space models have been successfully used for more than fifty years in
different areas of science and engineering. We present a procedure for
efficient variational Bayesian learning of nonlinear state-space models based
on sparse Gaussian processes. The result of learning is a tractable posterior
over nonlinear dynamical systems. In comparison to conventional parametric
models, we offer the possibility to straightforwardly trade off model capacity
and computational cost whilst avoiding overfitting. Our main algorithm uses a
hybrid inference approach combining variational Bayes and sequential Monte
Carlo. We also present stochastic variational inference and online learning
approaches for fast learning with long time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4917</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4917</id><created>2014-06-18</created><authors><author><keyname>Kim</keyname><forenames>Joongheon</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Max-Weight Scheduling and Quality-Aware Streaming for Device-to-Device
  Video Delivery</title><categories>cs.NI</categories><comments>2 pages, 1 figure, 1 table</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose and analyze centralized and distributed algorithms for
device-to-device video scheduling and streaming. The proposed algorithms
address jointly the problems of device-to-device link scheduling and video
quality adaptation in streaming. Our simulations show that the proposed
algorithms significantly outperform conventional separated approaches that
treat these two problems independently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4923</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4923</id><created>2014-06-18</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames><affiliation>MIT</affiliation></author><author><keyname>Arcand</keyname><forenames>William</forenames><affiliation>MIT</affiliation></author><author><keyname>Bestor</keyname><forenames>David</forenames><affiliation>MIT</affiliation></author><author><keyname>Bergeron</keyname><forenames>Bill</forenames><affiliation>MIT</affiliation></author><author><keyname>Byun</keyname><forenames>Chansup</forenames><affiliation>MIT</affiliation></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames><affiliation>MIT</affiliation></author><author><keyname>Hubbell</keyname><forenames>Matthew</forenames><affiliation>MIT</affiliation></author><author><keyname>Michaleas</keyname><forenames>Peter</forenames><affiliation>MIT</affiliation></author><author><keyname>Mullen</keyname><forenames>Julie</forenames><affiliation>MIT</affiliation></author><author><keyname>Prout</keyname><forenames>Andrew</forenames><affiliation>MIT</affiliation></author><author><keyname>Reuther</keyname><forenames>Albert</forenames><affiliation>MIT</affiliation></author><author><keyname>Rosa</keyname><forenames>Antonio</forenames><affiliation>MIT</affiliation></author><author><keyname>Yee</keyname><forenames>Charles</forenames><affiliation>MIT</affiliation></author></authors><title>Achieving 100,000,000 database inserts per second using Accumulo and D4M</title><categories>cs.DB astro-ph.IM cs.CE cs.DC cs.MS</categories><comments>6 pages; to appear in IEEE High Performance Extreme Computing (HPEC)
  2014</comments><doi>10.1109/HPEC.2014.7040945</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Apache Accumulo database is an open source relaxed consistency database
that is widely used for government applications. Accumulo is designed to
deliver high performance on unstructured data such as graphs of network data.
This paper tests the performance of Accumulo using data from the Graph500
benchmark. The Dynamic Distributed Dimensional Data Model (D4M) software is
used to implement the benchmark on a 216-node cluster running the MIT
SuperCloud software stack. A peak performance of over 100,000,000 database
inserts per second was achieved which is 100x larger than the highest
previously published value for any other database. The performance scales
linearly with the number of ingest clients, number of database servers, and
data size. The performance was achieved by adapting several supercomputing
techniques to this application: distributed arrays, domain decomposition,
adaptive load balancing, and single-program-multiple-data programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4928</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4928</id><created>2014-06-18</created><updated>2014-06-20</updated><authors><author><keyname>Lei</keyname><forenames>Ming</forenames></author><author><keyname>Soleymani</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Diversity Multiplexing Tradeoff of the Half-duplex Slow Fading Multiple
  Access Channel based on Generalized Quantize-and-Forward Scheme</title><categories>cs.IT math.IT</categories><comments>16 pages, 3 tables and 1 figure, in preparation to submit. arXiv
  admin note: text overlap with arXiv:1404.0354</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the Diversity Multiplexing Tradeoff (DMT) of the
generalized quantize-and-forward (GQF) relaying scheme over the slow fading
half-duplex multiple-access relay channel (HD-MARC). The compress-and-forward
(CF) scheme has been shown to achieve the optimal DMT when the channel state
information (CSI) of the relay-destination link is available at the relay.
However, having the CSI of relay-destination link at relay is not always
possible due to the practical considerations of the wireless system. In
contrast, in this work, the DMT of the GQF scheme is derived without
relay-destination link CSI at the relay. It is shown that even without
knowledge of relay-destination CSI, the GQF scheme achieves the same DMT,
achievable by CF scheme with full knowledge of CSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4929</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4929</id><created>2014-06-18</created><updated>2014-06-20</updated><authors><author><keyname>Nakajima</keyname><forenames>Kohei</forenames></author><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Hauser</keyname><forenames>Helmut</forenames></author><author><keyname>Pfeifer</keyname><forenames>Rolf</forenames></author></authors><title>Exploiting short-term memory in soft body dynamics as a computational
  resource</title><categories>physics.comp-ph cs.DC nlin.AO</categories><comments>22 pages, 11 figures; email address corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft materials are not only highly deformable but they also possess rich and
diverse body dynamics. Soft body dynamics exhibit a variety of properties,
including nonlinearity, elasticity, and potentially infinitely many degrees of
freedom. Here we demonstrate that such soft body dynamics can be employed to
conduct certain types of computation. Using body dynamics generated from a soft
silicone arm, we show that they can be exploited to emulate functions that
require memory and to embed robust closed-loop control into the arm. Our
results suggest that soft body dynamics have a short-term memory and can serve
as a computational resource. This finding paves the way toward exploiting
passive body dynamics for control of a large class of underactuated systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4941</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4941</id><created>2014-06-19</created><authors><author><keyname>Bhattasali</keyname><forenames>Tapalina</forenames></author><author><keyname>Chaki</keyname><forenames>Rituparna</forenames></author><author><keyname>Chaki</keyname><forenames>Nabendu</forenames></author></authors><title>Study of Security Issues in Pervasive Environment of Next Generation
  Internet of Things</title><categories>cs.NI cs.CR</categories><comments>12 pages, CISIM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things is a novel concept that semantically implies a world-wide
network of uniquely addressable interconnected smart objects. It is aimed at
establishing any paradigm in computing. This environment is one where the
boundary between virtual and physical world is eliminated. As the network gets
loaded with hitherto unknown applications, security threats also become
rampant. Current security solutions fail as new threats appear to de-struct the
reliability of information. The network has to be transformed to IPv6 enabled
network to address huge number of smart objects. Thus new addressing schemes
come up with new attacks. Real time analysis of information from the
heterogeneous smart objects needs use of cloud services. This can fall prey to
cloud specific security threats. Therefore need arises for a review of security
threats for a new area having huge demand. Here a study of security issues in
this domain is briefly presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4943</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4943</id><created>2014-06-19</created><authors><author><keyname>Pritam</keyname><forenames>Siddharth</forenames></author></authors><title>&quot;Infographics&quot; team: Selecting Control Parameters via Maximal Fisher
  Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Team description paper for RoboCup 2014 Soccer Simulation League 2D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4944</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4944</id><created>2014-06-19</created><authors><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>IT based social media impacts on Indonesian general legislative
  elections 2014</title><categories>cs.CY</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information technology applications in cyberspace (the internet) are
currently dominated by social media. The author investigates and explores the
advantages of social media implementation of any political party in Indonesian
general legislative elections 2014. There are twelve national political parties
participating in the election as contestants plus three local political parties
in Aceh. In this research, author focus on national political parties only. The
author visited, analyzed, and learnt the social media used by the contestants.
Those social media are : 1) Facebook, 2) Twitter, and 3) YouTube. Author also
compares the popularity of political parties on social media with the results
of a real count. Then Author can discuss : 1) the impact of social media on
political parties, 2) social media as a brand of political parties, 3) social
media as political presentation, and 4) social media as virtual society. The
results of this study indicate that Facebook is still a social media
application that received high attention by the voters on a campaign of
political parties. Indonesian's legislative elections won by parties that are
using social media as part of their campaigns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4951</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4951</id><created>2014-06-19</created><updated>2014-07-13</updated><authors><author><keyname>Eryilmaz</keyname><forenames>Sukru Burc</forenames></author><author><keyname>Kuzum</keyname><forenames>Duygu</forenames></author><author><keyname>Jeyasingh</keyname><forenames>Rakesh</forenames></author><author><keyname>Kim</keyname><forenames>SangBum</forenames></author><author><keyname>BrightSky</keyname><forenames>Matthew</forenames></author><author><keyname>Lam</keyname><forenames>Chung</forenames></author><author><keyname>Wong</keyname><forenames>H. -S. Philip</forenames></author></authors><title>Brain-like associative learning using a nanoscale non-volatile phase
  change synaptic device array</title><categories>cs.NE cond-mat.mtrl-sci cs.LG</categories><comments>Original article can be found here:
  http://journal.frontiersin.org/Journal/10.3389/fnins.2014.00205/abstract</comments><journal-ref>Front Neurosci. 8, 205 (2014)</journal-ref><doi>10.3389/fnins.2014.00205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in neuroscience together with nanoscale electronic device
technology have resulted in huge interests in realizing brain-like computing
hardwares using emerging nanoscale memory devices as synaptic elements.
Although there has been experimental work that demonstrated the operation of
nanoscale synaptic element at the single device level, network level studies
have been limited to simulations. In this work, we demonstrate, using
experiments, array level associative learning using phase change synaptic
devices connected in a grid like configuration similar to the organization of
the biological brain. Implementing Hebbian learning with phase change memory
cells, the synaptic grid was able to store presented patterns and recall
missing patterns in an associative brain-like fashion. We found that the system
is robust to device variations, and large variations in cell resistance states
can be accommodated by increasing the number of training epochs. We illustrated
the tradeoff between variation tolerance of the network and the overall energy
consumption, and found that energy consumption is decreased significantly for
lower variation tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4966</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4966</id><created>2014-06-19</created><updated>2014-06-19</updated><authors><author><keyname>Du</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author></authors><title>Inner Product Similarity Search using Compositional Codes</title><categories>cs.CV cs.LG stat.ML</categories><comments>The approach presented in this paper (ECCV14 submission) is closely
  related to multi-stage vector quantization and residual quantization. Thanks
  the reviewers (CVPR14 and ECCV14) for pointing out the relationship to the
  two algorithms. Related paper:
  http://sites.skoltech.ru/app/data/uploads/sites/2/2013/09/CVPR14.pdf, which
  also adopts the summation of vectors for vector approximation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the nearest neighbor search problem under inner product
similarity and introduces a compact code-based approach. The idea is to
approximate a vector using the composition of several elements selected from a
source dictionary and to represent this vector by a short code composed of the
indices of the selected elements. The inner product between a query vector and
a database vector is efficiently estimated from the query vector and the short
code of the database vector. We show the superior performance of the proposed
group $M$-selection algorithm that selects $M$ elements from $M$ source
dictionaries for vector approximation in terms of search accuracy and
efficiency for compact codes of the same length via theoretical and empirical
analysis. Experimental results on large-scale datasets ($1M$ and $1B$ SIFT
features, $1M$ linear models and Netflix) demonstrate the superiority of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4969</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4969</id><created>2014-06-19</created><authors><author><keyname>Viard</keyname><forenames>Jordan</forenames><affiliation>LIP6</affiliation></author><author><keyname>Latapy</keyname><forenames>Matthieu</forenames><affiliation>LIP6</affiliation></author></authors><title>Identifying roles in an IP network with temporal and structural density</title><categories>cs.NI</categories><comments>Sixth IEEE International Workshop on Network Science for
  Communication Networks (NetSciCom 2014), Canada (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Captures of IP traffic contain much information on very different kinds of
activities like file transfers, users interacting with remote systems,
automatic backups, or distributed computations. Identifying such activities is
crucial for an appropriate analysis, modeling and monitoring of the traffic. We
propose here a notion of density that captures both temporal and structural
features of interactions, and generalizes the classical notion of clustering
coefficient. We use it to point out important differences between distinct
parts of the traffic, and to identify interesting nodes and groups of nodes in
terms of roles in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4973</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4973</id><created>2014-06-19</created><authors><author><keyname>Marceau</keyname><forenames>Ga&#xe9;tan</forenames><affiliation>LRI, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>LRI, INRIA Saclay - Ile de France</affiliation></author></authors><title>Racing Multi-Objective Selection Probabilities</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>13th International Conference on Parallel Problem Solving from
  Nature, Ljubljana : France (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of Noisy Multi-Objective Optimization, dealing with
uncertainties requires the decision maker to define some preferences about how
to handle them, through some statistics (e.g., mean, median) to be used to
evaluate the qualities of the solutions, and define the corresponding Pareto
set. Approximating these statistics requires repeated samplings of the
population, drastically increasing the overall computational cost. To tackle
this issue, this paper proposes to directly estimate the probability of each
individual to be selected, using some Hoeffding races to dynamically assign the
estimation budget during the selection step. The proposed racing approach is
validated against static budget approaches with NSGA-II on noisy versions of
the ZDT benchmark functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4974</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4974</id><created>2014-06-19</created><authors><author><keyname>Barker</keyname><forenames>Adam</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Ward</keyname><forenames>Jonathan Stuart</forenames></author><author><keyname>Sommerville</keyname><forenames>Ian</forenames></author></authors><title>Academic Cloud Computing Research: Five Pitfalls and Five Opportunities</title><categories>cs.DC</categories><comments>Accepted and presented at the 6th USENIX Workshop on Hot Topics in
  Cloud Computing (HotCloud'14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This discussion paper argues that there are five fundamental pitfalls, which
can restrict academics from conducting cloud computing research at the
infrastructure level, which is currently where the vast majority of academic
research lies. Instead academics should be conducting higher risk research, in
order to gain understanding and open up entirely new areas.
  We call for a renewed mindset and argue that academic research should focus
less upon physical infrastructure and embrace the abstractions provided by
clouds through five opportunities: user driven research, new programming
models, PaaS environments, and improved tools to support elasticity and
large-scale debugging. The objective of this paper is to foster discussion, and
to define a roadmap forward, which will allow academia to make longer-term
impacts to the cloud computing community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4980</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4980</id><created>2014-06-19</created><updated>2014-10-22</updated><authors><author><keyname>Vehkaper&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>Riihonen</keyname><forenames>Taneli</forenames></author><author><keyname>Girnyk</keyname><forenames>Maksym A.</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author><author><keyname>Wichman</keyname><forenames>Risto</forenames></author></authors><title>Asymptotic Analysis of SU-MIMO Channels With Transmitter Noise and
  Mismatched Joint Decoding</title><categories>cs.IT math.IT</categories><comments>16 pages, 7 figures</comments><doi>10.1109/TCOMM.2014.2385051</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardware impairments in radio-frequency components of a wireless system cause
unavoidable distortions to transmission that are not captured by the
conventional linear channel model. In this paper, a 'binoisy' single-user
multiple-input multiple-output (SU-MIMO) relation is considered where the
additional distortions are modeled via an additive noise term at the transmit
side. Through this extended SU-MIMO channel model, the effects of transceiver
hardware impairments on the achievable rate of multi-antenna point-to-point
systems are studied. Channel input distributions encompassing practical
discrete modulation schemes, such as, QAM and PSK, as well as Gaussian
signaling are covered. In addition, the impact of mismatched detection and
decoding when the receiver has insufficient information about the
non-idealities is investigated. The numerical results show that for realistic
system parameters, the effects of transmit-side noise and mismatched decoding
become significant only at high modulation orders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4986</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4986</id><created>2014-06-19</created><authors><author><keyname>Joseph</keyname><forenames>Thomas</forenames></author></authors><title>Smart city analysis using spatial data and predicting the sustainability</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart city [1] planning is crucial as it should balance among resources and
the needs of the city .It allows to achieve good eco-friendly industries, there
by supporting both the nature and the stake holders. Setting up an industry is
a difficult problem, because it should optimize the resources and allocating it
in an effective manner. Weighted sum approach [2] uses the spatial data for
finding appropriate places to set up the industry based on the weight assigned
to each constraint. The user can predict the possible places in the search
space, where the industry can be set with low time complexity using spatial
data. Diversity being introduced by using multipoint crossover and mutation
operations. It will help to bring exploration in the search space, thereby
bring the diversity factor into the solution space. The prediction approach
will help to avoid the human exploitation on nature for resources. This in turn
helps the investors to maximize the Return on Investment (ROI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4988</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4988</id><created>2014-06-19</created><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Sellwood</keyname><forenames>James</forenames></author></authors><title>Path Conditions and Principal Matching: A New Approach to Access Control</title><categories>cs.CR</categories><comments>Accepted for publication at SACMAT 2014</comments><acm-class>D.4.6; H.2.0</acm-class><doi>10.1145/2613087.2613094</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional authorization policies are user-centric, in the sense that
authorization is defined, ultimately, in terms of user identities. We believe
that this user-centric approach is inappropriate for many applications, and
that what should determine authorization is the relationships that exist
between entities in the system. While recent research has considered the
possibility of specifying authorization policies based on the relationships
that exist between peers in social networks, we are not aware of the
application of these ideas to general computing systems. We develop a formal
access control model that makes use of ideas from relationship-based access
control and a two-stage method for evaluating policies. Our policies are
defined using path conditions, which are similar to regular expressions. We
define semantics for path conditions, which we use to develop a rigorous method
for evaluating policies. We describe the algorithm required to evaluate
policies and establish its complexity. Finally, we illustrate the advantages of
our model using an example and describe a preliminary implementation of our
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.4995</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.4995</id><created>2014-06-19</created><authors><author><keyname>Farhang</keyname><forenames>Arman</forenames></author><author><keyname>Aminjavaheri</keyname><forenames>Amir</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author><author><keyname>Doyle</keyname><forenames>Linda E.</forenames></author><author><keyname>Farhang-Boroujeny</keyname><forenames>Behrouz</forenames></author></authors><title>Pilot Decontamination in CMT-based Massive MIMO Networks</title><categories>cs.IT math.IT</categories><comments>Accepted in ISWCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pilot contamination problem in massive MIMO networks operating in
time-division duplex (TDD) mode can limit their expected capacity to a great
extent. This paper addresses this problem in cosine modulated multitone (CMT)
based massive MIMO networks; taking advantage of their so-called blind
equalization property. We extend and apply the blind equalization technique
from single antenna case to multi-cellular massive MIMO systems and show that
it can remove the channel estimation errors (due to pilot contamination effect)
without any need for cooperation between different cells or transmission of
additional training information. Our numerical results advocate the efficacy of
the proposed blind technique in improving the channel estimation accuracy and
removal of the residual channel estimation errors caused by the users of the
other cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5000</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5000</id><created>2014-06-19</created><authors><author><keyname>Khatwal</keyname><forenames>Ravi</forenames></author><author><keyname>Jain</keyname><forenames>Manoj Kumar</forenames></author></authors><title>Application Specific Cache Simulation Analysis for Application Specific
  Instruction set Processor</title><categories>cs.AR</categories><comments>ASIP simulation</comments><doi>10.5120/15782-4526</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Efficient Simulation of application specific instruction-set processors
(ASIP) is a challenging onus in the area of VLSI design. This paper
reconnoiters the possibility of use of ASIP simulators for ASIP Simulation.
This proposed study allow as the simulation of the cache memory design with
various ASIP simulators like Simple scalar and VEX. In this paper we have
implemented the memory configuration according to desire application. These
simulators performs the cache related results such as cache name, sets, cache
associativity, cache block size, cache replacement policy according to specific
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5020</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5020</id><created>2014-06-19</created><authors><author><keyname>AlCattan</keyname><forenames>Rasha Fouad</forenames></author></authors><title>Integration of Cloud Computing and Web2.0 Collaboration Technologies in
  E-Learning</title><categories>cs.CY cs.DC</categories><comments>10 pages, 5 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V12(1):46-55, June 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing technology is an emerging new computing paradigm for
delivering computing services. Although it still in its early stage, it has
changed the way how many applications are developed and accessed. This
computing approach relies on a number of existing technologies, such as Web2.0,
virtualization, Service oriented architecture SOA, Web services,etc.Cloud
computing is growing rapidly and becoming an adoptable technology for the
organizations especially education institutes, with its dynamic scalability and
usage of virtualized resources as a service through the Internet.Today,
eLearning is also becoming a very popular and powerful trend.However,in
traditional web based eLearning systems,building and maintenance are located
onsite in institutions or enterprises, which cause lot of problems to appear
such as lacking the support of underlying infrastructure, which can dynamically
allocate the needed calculation and storage resources for eLearning systems.As
the need for e learning is increasing continuously and its necessary for
eLearning systems to keep pace with the right technology needed for development
and improvement.However, todays technologies such as Web 2.0, Cloud, etc.enable
to build more successful and effective educational environment,that provide
collaboration and interaction in eLearning environments.The challenge is to use
and integrate these technologies in order to construct tools that allow the
best possible learning results.Cloud computing and Web 2.0 are two areas that
are starting to strongly effect how the development,deployment and usage of
eLearning application.This paper presents the benefits of using cloud computing
with the integration of Web 2.0 collaboration technologies in eLearning
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5035</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5035</id><created>2014-06-19</created><authors><author><keyname>Feige</keyname><forenames>Uriel</forenames></author></authors><title>Why are images smooth?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a well observed phenomenon that natural images are smooth, in the sense
that nearby pixels tend to have similar values. We describe a mathematical
model of images that makes no assumptions on the nature of the environment that
images depict. It only assumes that images can be taken at different scales
(zoom levels). We provide quantitative bounds on the smoothness of a typical
image in our model, as a function of the number of available scales. These
bounds can serve as a baseline against which to compare the observed smoothness
of natural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5036</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5036</id><created>2014-06-19</created><authors><author><keyname>Ried</keyname><forenames>Katja</forenames></author><author><keyname>Agnew</keyname><forenames>Megan</forenames></author><author><keyname>Vermeyden</keyname><forenames>Lydia</forenames></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author><author><keyname>Spekkens</keyname><forenames>Robert W.</forenames></author><author><keyname>Resch</keyname><forenames>Kevin J.</forenames></author></authors><title>Inferring causal structure: a quantum advantage</title><categories>quant-ph cs.LG gr-qc stat.ML</categories><comments>17 pages, 6 figures. Comments welcome</comments><journal-ref>Nat Phys 11, 414-420 (2015)</journal-ref><doi>10.1038/nphys3266</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of using observed correlations to infer causal relations is
relevant to a wide variety of scientific disciplines. Yet given correlations
between just two classical variables, it is impossible to determine whether
they arose from a causal influence of one on the other or a common cause
influencing both, unless one can implement a randomized intervention. We here
consider the problem of causal inference for quantum variables. We introduce
causal tomography, which unifies and generalizes conventional quantum
tomography schemes to provide a complete solution to the causal inference
problem using a quantum analogue of a randomized trial. We furthermore show
that, in contrast to the classical case, observed quantum correlations alone
can sometimes provide a solution. We implement a quantum-optical experiment
that allows us to control the causal relation between two optical modes, and
two measurement schemes -- one with and one without randomization -- that
extract this relation from the observed correlations. Our results show that
entanglement and coherence, known to be central to quantum information
processing, also provide a quantum advantage for causal inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5038</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5038</id><created>2014-06-19</created><authors><author><keyname>Hossain</keyname><forenames>Sakir</forenames></author></authors><title>Rain Attenuation Prediction for Terrestrial Microwave Link in Bangladesh</title><categories>cs.NI</categories><comments>Journal of Electrical and Electronics Engineering (volume: 7, Issue:
  1)
  http://electroinf.uoradea.ro/images/articles/CERCETARE/Reviste/JEEE/JEEE_V7_N1_MAY_2014/Hossain_may2014.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rain attenuation is a major shortcoming of microwave transmission. As a
subtropical country, Bangladesh is one of the highest rainy areas of the world.
Thus, designing a terrestrial microwave link is a serious challenge to the
engineers. In this paper, the annual rain rate and monthly variation of rate
are predicted for different percentage of time of the year from the measured
rainfall data. Using ITU rain model for terrestrial microwave communication,
the rain attenuation is predicted for five major cities of Bangladesh, namely
Dhaka, Chittagong, Rajshahi, Sylhet, and Khulna. It is found that rain
attenuation is the most severe in Sylhet and least in Rajshahi. The attenuation
is estimated for different frequency and polarization. A horizontally polarized
signal encounters 15% more rain attenuation than that of vertically polarized
signal. It is also found that attenuation in Rajshahi is about 20% lesser than
that in Sylhet. Thus, the horizontally polarized transmission in Rajshahi
experiences about 5% less attenuation than the vertically polarized
transmission in Sylhet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5052</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5052</id><created>2014-06-19</created><updated>2014-09-24</updated><authors><author><keyname>Boj&#x101;rs</keyname><forenames>Uldis</forenames></author><author><keyname>Liepi&#x146;&#x161;</keyname><forenames>Ren&#x101;rs</forenames></author></authors><title>The State of Open Data in Latvia: 2014</title><categories>cs.CY</categories><comments>keywords: Open Data, Open Government Data, PSI, Latvia</comments><journal-ref>Baltic J. Modern Computing, Vol. 2 (2014), No. 3, 160-170</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the state of Open Data in Latvia at the middle of 2014.
The study is divided into two parts: (i) a survey of open data situation and
(ii) an overview of available open data sets. The first part examines the
general open data climate in Latvia according to the guidelines of the OKFN
Open Data Index making the results comparable to those of other participants of
this index. The second part examines datasets made available on the Latvia Open
Data community catalogue, the only open data catalogue available in Latvia at
the moment. We conclude that Latvia public sector open data mostly fulfil the
basic criteria (e.g., data is available) of the Open Data Index but fail on
more advanced criteria: the majority of data considered in the study are not
published in machine-readable form, are not available for bulk download and
none of the data sources have open license statements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5059</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5059</id><created>2014-06-19</created><authors><author><keyname>Bhola</keyname><forenames>Abhishek</forenames></author></authors><title>Twitter and Polls: Analyzing and estimating political orientation of
  Twitter users in India General #Elections2014</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This year (2014) in the month of May, the tenure of the 15th Lok Sabha was to
end and the elections to the 543 parliamentary seats were to be held. A
whooping $5 billion were spent on these elections, which made us stand second
only to the US Presidential elections in terms of money spent. Swelling number
of Internet users and Online Social Media (OSM) users could effect 3-4% of
urban population votes as per a report of IAMAI (Internet &amp; Mobile Association
of India). Our count of tweets related to elections from September 2013 to May
2014, was close to 18.21 million. We analyzed the complete dataset and found
that the activity on Twitter peaked during important events. It was evident
from our data that the political behavior of the politicians affected their
followers count. Yet another aim of our work was to find an efficient way to
classify the political orientation of the users on Twitter. We used four
different techniques: two were based on the content of the tweets, one on the
user based features and another based on community detection algorithm on the
retweet and user mention networks. We found that the community detection
algorithm worked best. We built a portal to show the analysis of the tweets of
the last 24 hours. To the best of our knowledge, this is the first academic
pursuit to analyze the elections data and classify the users in the India
General Elections 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5067</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5067</id><created>2014-05-16</created><updated>2015-04-16</updated><authors><author><keyname>Petr</keyname><forenames>Jancar</forenames><affiliation>Techn. Univ. Ostrava</affiliation></author><author><keyname>Karandikar</keyname><forenames>Prateek</forenames><affiliation>Chennai Mathematical Institute</affiliation></author><author><keyname>Schnoebelen</keyname><forenames>Philippe</forenames><affiliation>LSV, CNRS &amp; ENS Cachan</affiliation></author></authors><title>On Reachability for Unidirectional Channel Systems Extended with Regular
  Tests</title><categories>cs.LO cs.FL</categories><comments>An extended abstract of this work first appeared in IFIP-TCS 2012,
  LNCS 7604</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 2 (April 17,
  2015) lmcs:876</journal-ref><doi>10.2168/LMCS-11(2:2)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Unidirectional channel systems&quot; (Chambart &amp; Schnoebelen, CONCUR 2008) are
finite-state systems where one-way communication from a Sender to a Receiver
goes via one reliable and one unreliable unbounded fifo channel. While
reachability is decidable for these systems, equipping them with the
possibility of testing regular properties on the contents of channels makes it
undecidable. Decidability is preserved when only emptiness and nonemptiness
tests are considered: the proof relies on an elaborate reduction to a
generalized version of Post's Embedding Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5073</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5073</id><created>2014-06-16</created><authors><author><keyname>Seker</keyname><forenames>Sadi Evren</forenames></author><author><keyname>Cankir</keyname><forenames>Bilal</forenames></author><author><keyname>Arslan</keyname><forenames>Mehmet Lutfi</forenames></author></authors><title>Information and Communication Technology Reputation for XU030 Quote
  Companies</title><categories>cs.CY</categories><comments>5 pages 2 figure 1 table 21 refs. arXiv admin note: text overlap with
  arXiv:1401.7547</comments><acm-class>H.5.3; H.3.4</acm-class><journal-ref>International Journal of Innovation, Management and Technology,
  Vol. 5, No. 3, June 2014 ISSN: 2010-0248</journal-ref><doi>10.7763/IJIMT.2014.V5.517</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By the increasing spread of information technology and Internet improvements,
most of the large-scale companies are paying special attention to their
reputation on many types of the information and communication technology. The
increasing developments and penetration of new technologies into daily life,
brings out paradigm shift on the perception of reputation and creates new
concepts like esocieties, techno-culture and new media. Contemporary companies
are trying to control their reputation over the new communities who are mostly
interacting with social networks, web pages and electronic communication
technologies. In this study, the reputation of top 30 Turkish companies, quoted
to the Istanbul Stock Market, is studied, based on the information technology
interfaces between company and society, such as social networks, blogs, wikis
and web pages. The web reputation is gathered through 17 different parameters,
collected from Google, Facebook, Twitter, Bing, Alexa, etc. The reputation
index is calculated by z-index and fscoring formulations after the min-max
normalization of each web reputation parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5074</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5074</id><created>2014-06-19</created><authors><author><keyname>Vijendra</keyname><forenames>Singh</forenames></author><author><keyname>Shivani</keyname><forenames>Pathak</forenames></author></authors><title>Robust Outlier Detection Technique in Data Mining: A Univariate Approach</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1402.6859 by other authors
  without attribution</comments><report-no>MT CS 2011</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Outliers are the points which are different from or inconsistent with the
rest of the data. They can be novel, new, abnormal, unusual or noisy
information. Outliers are sometimes more interesting than the majority of the
data. The main challenges of outlier detection with the increasing complexity,
size and variety of datasets, are how to catch similar outliers as a group, and
how to evaluate the outliers. This paper describes an approach which uses
Univariate outlier detection as a pre-processing step to detect the outlier and
then applies K-means algorithm hence to analyse the effects of the outliers on
the cluster analysis of dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5095</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5095</id><created>2014-06-19</created><authors><author><keyname>Reddy</keyname><forenames>Vikas</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Sanin</keyname><forenames>Andres</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>MRF-based Background Initialisation for Improved Foreground Detection in
  Cluttered Surveillance Videos</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.2465</comments><acm-class>I.5.4; I.4.5; I.4.6</acm-class><doi>10.1007/978-3-642-19318-7_43</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust foreground object segmentation via background modelling is a difficult
problem in cluttered environments, where obtaining a clear view of the
background to model is almost impossible. In this paper, we propose a method
capable of robustly estimating the background and detecting regions of interest
in such environments. In particular, we propose to extend the background
initialisation component of a recent patch-based foreground detection algorithm
with an elaborate technique based on Markov Random Fields, where the optimal
labelling solution is computed using iterated conditional modes. Rather than
relying purely on local temporal statistics, the proposed technique takes into
account the spatial continuity of the entire background. Experiments with
several tracking algorithms on the CAVIAR dataset indicate that the proposed
method leads to considerable improvements in object tracking accuracy, when
compared to methods based on Gaussian mixture models and feature histograms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5101</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5101</id><created>2014-06-19</created><updated>2015-02-25</updated><authors><author><keyname>Rao</keyname><forenames>Milind</forenames></author><author><keyname>Lopez-Martinez</keyname><forenames>F. Javier</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>MGF Approach to the Analysis of Generalized Two-Ray Fading Models</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 Figures and 2 Tables. This work has been accepted for
  publication at IEEE Transactions on Wireless Communications. Copyright (c)
  2014 IEEE. Personal use of this material is permitted. However, permission to
  use this material for any other purposes must be obtained from the IEEE by
  sending a request to pubs-permissions@ieee.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a class of Generalized Two-Ray (GTR) fading channels that consist
of two line of sight (LOS) components with random phase plus a diffuse
component. We derive a closed form expression for the moment generating
function (MGF) of the signal-to-noise ratio (SNR) for this model, which greatly
simplifies its analysis. This expression arises from the observation that the
GTR fading model can be expressed in terms of a conditional underlying Rician
distribution. We illustrate the approach to derive simple expressions for
statistics and performance metrics of interest such as the amount of fading,
the level crossing rate, the symbol error rate, and the ergodic capacity in GTR
fading channels. We also show that the effect of considering a more general
distribution for the phase difference between the LOS components has an impact
on the average SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5105</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5105</id><created>2014-06-19</created><updated>2015-02-25</updated><authors><author><keyname>Lopez-Martinez</keyname><forenames>F. Javier</forenames></author><author><keyname>Martos-Naya</keyname><forenames>Eduardo</forenames></author><author><keyname>Paris</keyname><forenames>Jose F.</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Eigenvalue Dynamics of a Central Wishart Matrix with Application to MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>15 pages, 9 figures and 1 table. This work has been accepted for
  publication at IEEE Trans. Inf. Theory. Copyright (c) 2014 IEEE. Personal use
  of this material is permitted. However, permission to use this material for
  any other purposes must be obtained from the IEEE by sending a request to
  pubs-permissions@ieee.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the dynamic behavior of the stationary random process defined
by a central complex Wishart (CW) matrix ${\bf{W}}(t)$ as it varies along a
certain dimension $t$. We characterize the second-order joint cdf of the
largest eigenvalue, and the second-order joint cdf of the smallest eigenvalue
of this matrix. We show that both cdfs can be expressed in exact closed-form in
terms of a finite number of well-known special functions in the context of
communication theory. As a direct application, we investigate the dynamic
behavior of the parallel channels associated with multiple-input
multiple-output (MIMO) systems in the presence of Rayleigh fading. Studying the
complex random matrix that defines the MIMO channel, we characterize the
second-order joint cdf of the signal-to-noise ratio (SNR) for the best and
worst channels. We use these results to study the rate of change of MIMO
parallel channels, using different performance metrics. For a given value of
the MIMO channel correlation coefficient, we observe how the SNR associated
with the best parallel channel changes slower than the SNR of the worst
channel. This different dynamic behavior is much more appreciable when the
number of transmit ($N_T$) and receive ($N_R$) antennas is similar. However, as
$N_T$ is increased while keeping $N_R$ fixed, we see how the best and worst
channels tend to have a similar rate of change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5106</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5106</id><created>2014-06-19</created><authors><author><keyname>Johnson</keyname><forenames>J. Ian</forenames></author><author><keyname>Sergey</keyname><forenames>Ilya</forenames></author><author><keyname>Earl</keyname><forenames>Christopher</forenames></author><author><keyname>Might</keyname><forenames>Matthew</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Pushdown flow analysis with abstract garbage collection</title><categories>cs.PL</categories><acm-class>D.3.4; F.3.2</acm-class><journal-ref>Journal of Functional Programming, Volume 24, Special Issue 2-3,
  May 2014, pp 218-283</journal-ref><doi>10.1017/S0956796814000100 10.1017/S0956796814000100
  10.1017/S0956796814000100 10.1017/S0956796814000100 10.1017/S0956796814000100</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the static analysis of functional programs, pushdown flow analysis and
abstract garbage collection push the boundaries of what we can learn about
programs statically. This work illuminates and poses solutions to theoretical
and practical challenges that stand in the way of combining the power of these
techniques. Pushdown flow analysis grants unbounded yet computable polyvariance
to the analysis of return-flow in higher-order programs. Abstract garbage
collection grants unbounded polyvariance to abstract addresses which become
unreachable between invocations of the abstract contexts in which they were
created. Pushdown analysis solves the problem of precisely analyzing recursion
in higher-order languages; abstract garbage collection is essential in solving
the &quot;stickiness&quot; problem. Alone, our benchmarks demonstrate that each method
can reduce analysis times and boost precision by orders of magnitude. We
combine these methods. The challenge in marrying these techniques is not
subtle: computing the reachable control states of a pushdown system relies on
limiting access during transition to the top of the stack; abstract garbage
collection, on the other hand, needs full access to the entire stack to compute
a root set, just as concrete collection does. Conditional pushdown systems were
developed for just such a conundrum, but existing methods are ill-suited for
the dynamic nature of garbage collection. We show fully precise and approximate
solutions to the feasible paths problem for pushdown garbage-collecting
control-flow analysis. Experiments reveal synergistic interplay between garbage
collection and pushdown techniques, and the fusion demonstrates
&quot;better-than-both-worlds&quot; precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5108</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5108</id><created>2014-06-19</created><authors><author><keyname>Huo</keyname><forenames>Qiang</forenames></author><author><keyname>Ma</keyname><forenames>Meng</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Study on Downlink Spectral Efficiency in Orthogonal Frequency Division
  Multiple Access Systems</title><categories>cs.IT math.IT</categories><comments>23 pages, 8 figures, IET Communications, 2014</comments><journal-ref>IET Communications, Vol. 8, Iss. 11, pp. 1902--1909, 2014</journal-ref><doi>10.1049/iet-com.2013.0972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous studies on the capacity of orthogonal frequency division multiple
access (OFDMA) systems, it is usually assumed that co-channel interference
(CCI) from adjacent cells is a Gaussian-distributed random variable. However,
very-little work shows that the Gaussian assumption does not hold true in OFDMA
systems. In this paper, the statistical property of CCI in downlink OFDMA
systems is studied, and spectral efficiency of downlink OFDMA system is
analyzed based on the derived statistical model. First, the probability density
function (PDF) of CCI in downlink OFDMA cellular systems is studied with the
considerations of path loss, multipath fading and Gaussian-like transmit
signals. Moreover, some closed-form expressions of the PDF are obtained for
special cases. The derived results show that the PDFs of CCI are with a heavy
tail, and significantly deviate from the Gaussian distribution. Then, based on
the derived statistical properties of CCI, the downlink spectral efficiency is
derived. Numerical and simulation results justify the derived statistical CCI
model and spectral efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5143</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5143</id><created>2014-06-19</created><updated>2014-06-21</updated><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>The Sample Complexity of Learning Linear Predictors with the Squared
  Loss</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note, we provide tight sample complexity bounds for learning
linear predictors with respect to the squared loss. Our focus is on an agnostic
setting, where no assumptions are made on the data distribution. This contrasts
with standard results in the literature, which either make distributional
assumptions, refer to specific parameter settings, or use other performance
measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5145</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5145</id><created>2014-06-19</created><updated>2014-07-07</updated><authors><author><keyname>Teitler</keyname><forenames>Zach</forenames></author></authors><title>Geometric lower bounds for generalized ranks</title><categories>math.AG cs.CC</categories><comments>43 pages. v2: minor changes</comments><msc-class>15A21, 15A69, 14N15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit a geometric lower bound for Waring rank of polynomials (symmetric
rank of symmetric tensors) of Landsberg and Teitler and generalize it to a
lower bound for rank with respect to arbitrary varieties, improving the bound
given by the &quot;non-Abelian&quot; catalecticants recently introduced by Landsberg and
Ottaviani. This is applied to give lower bounds for ranks of multihomogeneous
polynomials (partially symmetric tensors); a special case is the simultaneous
Waring decomposition problem for a linear system of polynomials. We generalize
the classical Apolarity Lemma to multihomogeneous polynomials and give some
more general statements. Finally we revisit the lower bound of Ranestad and
Schreyer, and again generalize it to multihomogeneous polynomials and some more
general settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5151</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5151</id><created>2014-06-19</created><authors><author><keyname>Lopez</keyname><forenames>Martha Rosa Cordero</forenames></author><author><keyname>Gonzalez</keyname><forenames>Marco Antonio Dorantes</forenames></author></authors><title>Tollan-Xicocotitlan: A reconstructed City by augmented reality</title><categories>cs.CE cs.CY</categories><comments>15 pages, 12 figures, Fourth International Conference on Advances in
  Computing and Information technology (ACITY 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This project presents the analysis, design, implementation and results of
Reconstruction Xicocotitlan Tollan-through augmented reality, which will
release information about the Toltec culture supplemented by presenting an
overview of the main premises of the Xicocotitlan Tollan city supported
dimensional models based on the augmented reality technique showing the user a
virtual representation of buildings in Tollan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5153</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5153</id><created>2014-06-19</created><authors><author><keyname>D&#xed;az</keyname><forenames>Josep</forenames></author><author><keyname>Giotis</keyname><forenames>Ioannis</forenames></author><author><keyname>Kirousis</keyname><forenames>Lefteris</forenames></author><author><keyname>Mourtos</keyname><forenames>Yiannis</forenames></author><author><keyname>Serna</keyname><forenames>Maria J.</forenames></author></authors><title>Optimizing the Social Cost of Congestion Games by Imposing Variable
  Delays</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new coordination mechanism for non-atomic congestion games that
leads to a (selfish) social cost which is arbitrarily close to the non-selfish
optimal. This mechanism does not incur any additional extra cost, like tolls,
which are usually differentiated from the social cost as expressed in terms of
delays only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5161</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5161</id><created>2014-06-19</created><authors><author><keyname>Narasimhan</keyname><forenames>Jeyanthi</forenames></author><author><keyname>Vishnu</keyname><forenames>Abhinav</forenames></author><author><keyname>Holder</keyname><forenames>Lawrence</forenames></author><author><keyname>Hoisie</keyname><forenames>Adolfy</forenames></author></authors><title>Fast Support Vector Machines Using Parallel Adaptive Shrinking on
  Distributed Systems</title><categories>cs.DC cs.LG</categories><comments>10 pages, 9 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support Vector Machines (SVM), a popular machine learning technique, has been
applied to a wide range of domains such as science, finance, and social
networks for supervised learning. Whether it is identifying high-risk patients
by health-care professionals, or potential high-school students to enroll in
college by school districts, SVMs can play a major role for social good. This
paper undertakes the challenge of designing a scalable parallel SVM training
algorithm for large scale systems, which includes commodity multi-core
machines, tightly connected supercomputers and cloud computing systems.
Intuitive techniques for improving the time-space complexity including adaptive
elimination of samples for faster convergence and sparse format representation
are proposed. Under sample elimination, several heuristics for {\em earliest
possible} to {\em lazy} elimination of non-contributing samples are proposed.
In several cases, where an early sample elimination might result in a false
positive, low overhead mechanisms for reconstruction of key data structures are
proposed. The algorithm and heuristics are implemented and evaluated on various
publicly available datasets. Empirical evaluation shows up to 26x speed
improvement on some datasets against the sequential baseline, when evaluated on
multiple compute nodes, and an improvement in execution time up to 30-60\% is
readily observed on a number of other datasets against our parallel baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5162</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5162</id><created>2014-06-19</created><updated>2016-02-18</updated><authors><author><keyname>Zhang</keyname><forenames>Baichuan</forenames></author><author><keyname>Saha</keyname><forenames>Tanay Kumar</forenames></author><author><keyname>Hasan</keyname><forenames>Mohammad Al</forenames></author></authors><title>Name Disambiguation from link data in a collaboration graph using
  temporal and topological features</title><categories>cs.IR cs.SI</categories><comments>The short version of this paper has been accepted to ASONAM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a social community, multiple persons may share the same name, phone number
or some other identifying attributes. This, along with other phenomena, such as
name abbreviation, name misspelling, and human error leads to erroneous
aggregation of records of multiple persons under a single reference. Such
mistakes affect the performance of document retrieval, web search, database
integration, and more importantly, improper attribution of credit (or blame).
The task of entity disambiguation partitions the records belonging to multiple
persons with the objective that each decomposed partition is composed of
records of a unique person. Existing solutions to this task use either
biographical attributes, or auxiliary features that are collected from external
sources, such as Wikipedia. However, for many scenarios, such auxiliary
features are not available, or they are costly to obtain. Besides, the attempt
of collecting biographical or external data sustains the risk of privacy
violation. In this work, we propose a method for solving entity disambiguation
task from link information obtained from a collaboration network. Our method is
non-intrusive of privacy as it uses only the time-stamped graph topology of an
anonymized network. Experimental results on two real-life academic
collaboration networks show that the proposed method has satisfactory
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5170</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5170</id><created>2014-06-19</created><authors><author><keyname>Chen</keyname><forenames>Xiaojie</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Excessive abundance of common resources deters social responsibility</title><categories>q-bio.PE cs.GT physics.soc-ph</categories><comments>5 two-column pages, 3 figures; accepted for publication in Scientific
  Reports</comments><journal-ref>Sci. Rep. 4 (2014) 4161</journal-ref><doi>10.1038/srep04161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the evolution of cooperation in the collective-risk social dilemma
game, where the risk is determined by a collective target that must be reached
with individual contributions. All players initially receive endowments from
the available amount of common resources. While cooperators contribute part of
their endowment to the collective target, defectors do not. If the target is
not reached, the endowments of all players are lost. In our model, we introduce
a feedback between the amount of common resources and the contributions of
cooperators. We show that cooperation can be sustained only if the common
resources are preserved but never excessively abound. This, however, requires a
delicate balance between the amount of common resources that initially exist,
and the amount cooperators contribute to the collective target. Exceeding
critical thresholds in either of the two amounts leads to loss of cooperation,
and consequently to the depletion of common resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5177</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5177</id><created>2014-06-19</created><authors><author><keyname>Thota</keyname><forenames>Lalitha Saroja</forenames></author><author><keyname>Elsayeed</keyname><forenames>Manal</forenames></author><author><keyname>Shaik</keyname><forenames>Naseema</forenames></author><author><keyname>Ghawa</keyname><forenames>Tayf Abdullah</forenames></author><author><keyname>Rashed</keyname><forenames>Ahlam</forenames></author><author><keyname>Refdan</keyname><forenames>Mona</forenames></author><author><keyname>Mohammed</keyname><forenames>Wejdan</forenames></author><author><keyname>Ali</keyname><forenames>Rawan</forenames></author><author><keyname>Changalasetty</keyname><forenames>Suresh Babu</forenames></author></authors><title>Implementation of Tic-Tac-Toe Game in LabVIEW</title><categories>cs.OH</categories><comments>7 pages, 17 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)&quot;</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V12(2):63-70, June 2014</journal-ref><doi>10.14445/22312803/IJCTT-V12P114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tic-Tac-Toe game can be played by two players where the square block (3 x 3)
can be filled with a cross (X) or a circle (O). The game will toggle between
the players by giving the chance for each player to mark their move. When one
of the players make a combination of 3 same markers in a horizontal, vertical
or diagonal line the program will display which player has won, whether X or O.
In this paper, we implement a 3x3 tic-tac-toe game in LabVIEW. The game is
designed so that two players can play tic-tac-toe using LabVIEW software. The
program will contain a display function and a select function to place the
symbol as well as toggle between the symbols allowing each player a turn to
play the game. The program will update after each player makes their move and
check for the conditions of game as it goes on. Overall program works without
any bugs and is able to use
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5179</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5179</id><created>2014-06-19</created><updated>2014-09-28</updated><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Granqvist</keyname><forenames>Claes-Goran</forenames></author></authors><title>Elimination of a Second-Law-attack, and all cable-resistance-based
  attacks, in the Kirchhoff-law-Johnson-noise (KLJN) secure key exchange system</title><categories>cs.ET</categories><comments>accepted for publication in Entropy (open access)</comments><journal-ref>Entropy 16 (2014) 5223-5231</journal-ref><doi>10.3390/e16105223</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We introduce the so far most efficient attack against the
Kirchhoff-law-Johnson-noise (KLJN) secure key exchange system. This attack
utilizes the lack of exact thermal equilibrium in practical applications and is
based on cable resistance losses and the fact that the Second Law of
Thermodynamics cannot provide full security when such losses are present. The
new attack does not challenge the unconditional security of the KLJN scheme,
but it puts more stringent demands on the security/privacy enhancing protocol
than for any earlier attack. In this paper we present a simple defense protocol
to fully eliminate this new attack by increasing the noise-temperature at the
side of the smaller resistance value over the noise-temperature at the at the
side with the greater resistance. It is shown that this simple protocol totally
removes Eve's information not only for the new attack but also for the old
Bergou-Scheuer-Yariv attack. The presently most efficient attacks against the
KLJN scheme are thereby completely nullified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5181</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5181</id><created>2014-06-19</created><updated>2015-03-04</updated><authors><author><keyname>Williams</keyname><forenames>Jake Ryland</forenames></author><author><keyname>Lessard</keyname><forenames>Paul R.</forenames></author><author><keyname>Desu</keyname><forenames>Suma</forenames></author><author><keyname>Clark</keyname><forenames>Eric</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Zipf's law holds for phrases, not words</title><categories>cs.CL physics.soc-ph</categories><comments>Manuscript: 6 pages, 3 figures; Supplementary Information: 8 pages,
  18 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With Zipf's law being originally and most famously observed for word
frequency, it is surprisingly limited in its applicability to human language,
holding over no more than three to four orders of magnitude before hitting a
clear break in scaling. Here, building on the simple observation that phrases
of one or more words comprise the most coherent units of meaning in language,
we show empirically that Zipf's law for phrases extends over as many as nine
orders of rank magnitude. In doing so, we develop a principled and scalable
statistical mechanical method of random text partitioning, which opens up a
rich frontier of rigorous text analysis via a rank ordering of mixed length
phrases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5197</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5197</id><created>2014-06-19</created><authors><author><keyname>Gu</keyname><forenames>Shi</forenames></author><author><keyname>Pasqualetti</keyname><forenames>Fabio</forenames></author><author><keyname>Cieslak</keyname><forenames>Matthew</forenames></author><author><keyname>Grafton</keyname><forenames>Scott T.</forenames></author><author><keyname>Bassett</keyname><forenames>Danielle S.</forenames></author></authors><title>Controllability of Brain Networks</title><categories>q-bio.NC cs.SY</categories><comments>14 pages, 4 figures, supplementary materials</comments><doi>10.1038/ncomms9414</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive function is driven by dynamic interactions between large-scale
neural circuits or networks, enabling behavior. Fundamental principles
constraining these dynamic network processes have remained elusive. Here we use
network control theory to offer a mechanistic explanation for how the brain
moves between cognitive states drawn from the network organization of white
matter microstructure. Our results suggest that densely connected areas,
particularly in the default mode system, facilitate the movement of the brain
to many easily-reachable states. Weakly connected areas, particularly in
cognitive control systems, facilitate the movement of the brain to
difficult-to-reach states. Areas located on the boundary between network
communities, particularly in attentional control systems, facilitate the
integration or segregation of diverse cognitive systems. Our results suggest
that structural network differences between the cognitive circuits dictate
their distinct roles in controlling dynamic trajectories of brain network
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5212</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5212</id><created>2014-06-19</created><authors><author><keyname>Gkioxari</keyname><forenames>Georgia</forenames></author><author><keyname>Hariharan</keyname><forenames>Bharath</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>R-CNNs for Pose Estimation and Action Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present convolutional neural networks for the tasks of keypoint (pose)
prediction and action classification of people in unconstrained images. Our
approach involves training an R-CNN detector with loss functions depending on
the task being tackled. We evaluate our method on the challenging PASCAL VOC
dataset and compare it to previous leading approaches. Our method gives
state-of-the-art results for keypoint and action prediction. Additionally, we
introduce a new dataset for action detection, the task of simultaneously
localizing people and classifying their actions, and present results using our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5215</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5215</id><created>2014-06-19</created><updated>2014-06-24</updated><authors><author><keyname>Hatna</keyname><forenames>Erez</forenames></author><author><keyname>Benenson</keyname><forenames>Itzhak</forenames></author></authors><title>Combining segregation and integration: Schelling model dynamics for
  heterogeneous population</title><categories>physics.soc-ph cs.SI nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Schelling model is a simple agent based model that demonstrates how
individuals' relocation decisions generate residential segregation in cities.
Agents belong to one of two groups and occupy cells of rectangular space.
Agents react to the fraction of agents of their own group within the
neighborhood around their cell. Agents stay put when this fraction is above a
given tolerance threshold but seek a new location if the fraction is below the
threshold. The model is well known for its tipping point behavior: an initial
random (integrated) pattern remains integrated when the tolerance threshold is
below 1/3 but becomes segregated when the tolerance threshold is above 1/3.
  In this paper, we demonstrate that the variety of the Schelling model steady
patterns is richer than the segregation-integration dichotomy and contains
patterns that consist of segregated patches for each of the two groups
alongside patches where both groups are spatially integrated. We obtain such
patterns by considering a general version of the model in which the mechanisms
of agents' interactions remain the same but the tolerance threshold varies
between agents of both groups.
  We show that the model produces patterns of mixed integration and segregation
when the tolerance threshold of most agents is either below the tipping point
or above 2/3. In these cases, the mixed patterns are relatively insensitive to
the model's parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5258</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5258</id><created>2014-06-19</created><authors><author><keyname>Yang</keyname><forenames>Jianjun</forenames></author><author><keyname>Payne</keyname><forenames>Bryson</forenames></author><author><keyname>Hitz</keyname><forenames>Markus</forenames></author><author><keyname>Fei</keyname><forenames>Zongming</forenames></author><author><keyname>Li</keyname><forenames>Le</forenames></author><author><keyname>Wei</keyname><forenames>Tongquan</forenames></author></authors><title>Location Aided Energy Balancing Strategy in Green Cellular Networks</title><categories>cs.NI</categories><comments>6 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1108.5493 by other authors</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Most cellular network communication strategies are focused on data traffic
scenarios rather than energy balance and efficient utilization. Thus mobile
users in hot cells may suffer from low throughput due to energy loading
imbalance problem. In state of art cellular network technologies, relay
stations extend cell coverage and enhance signal strength for mobile users.
However, busy traffic makes the relay stations in hot area run out of energy
quickly. In this paper, we propose an energy balancing strategy in which the
mobile nodes are able to dynamically select and hand over to the relay station
with the highest potential energy capacity to resume communication. Key to the
strategy is that each relay station merely maintains two parameters that
contains the trend of its previous energy consumption and then predicts its
future quantity of energy, which is defined as the relay station potential
energy capacity. Then each mobile node can select the relay station with the
highest potential energy capacity. Simulations demonstrate that our approach
significantly increase the aggregate throughput and the average life time of
relay stations in cellular network environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5263</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5263</id><created>2014-06-19</created><authors><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author></authors><title>Bounds on Eigenvalues of a Spatial Correlation Matrix</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures, IEEE Communications Letters, accepted for
  publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is critical to understand the properties of spatial correlation matrices
in massive multiple-input multiple-output (MIMO) systems. We derive new bounds
on the extreme eigenvalues of a spatial correlation matrix that is
characterized by the exponential model in this paper. The new upper bound on
the maximum eigenvalue is tighter than the previous known bound. Moreover,
numerical studies show that our new lower bound on the maximum eigenvalue is
close to the true maximum eigenvalue in most cases. We also derive an upper
bound on the minimum eigenvalue that is also tight. These bounds can be
exploited to analyze many wireless communication scenarios including uniform
planar arrays, which are expected to be widely used for massive MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5266</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5266</id><created>2014-06-19</created><updated>2015-04-18</updated><authors><author><keyname>Taigman</keyname><forenames>Yaniv</forenames></author><author><keyname>Yang</keyname><forenames>Ming</forenames></author><author><keyname>Ranzato</keyname><forenames>Marc'Aurelio</forenames></author><author><keyname>Wolf</keyname><forenames>Lior</forenames></author></authors><title>Web-Scale Training for Face Identification</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Scaling machine learning methods to very large datasets has attracted
considerable attention in recent years, thanks to easy access to ubiquitous
sensing and data from the web. We study face recognition and show that three
distinct properties have surprising effects on the transferability of deep
convolutional networks (CNN): (1) The bottleneck of the network serves as an
important transfer learning regularizer, and (2) in contrast to the common
wisdom, performance saturation may exist in CNN's (as the number of training
samples grows); we propose a solution for alleviating this by replacing the
naive random subsampling of the training set with a bootstrapping process.
Moreover, (3) we find a link between the representation norm and the ability to
discriminate in a target domain, which sheds lights on how such networks
represent faces. Based on these discoveries, we are able to improve face
recognition accuracy on the widely used LFW benchmark, both in the verification
(1:1) and identification (1:N) protocols, and directly compare, for the first
time, with the state of the art Commercially-Off-The-Shelf system and show a
sizable leap in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5273</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5273</id><created>2014-06-19</created><updated>2015-02-26</updated><authors><author><keyname>Lin</keyname><forenames>Chia-Hsiang</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Li</keyname><forenames>Wei-Chiang</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author><author><keyname>Ambikapathi</keyname><forenames>ArulMurugan</forenames></author></authors><title>Identifiability of the Simplex Volume Minimization Criterion for Blind
  Hyperspectral Unmixing: The No Pure-Pixel Case</title><categories>stat.ML cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In blind hyperspectral unmixing (HU), the pure-pixel assumption is well-known
to be powerful in enabling simple and effective blind HU solutions. However,
the pure-pixel assumption is not always satisfied in an exact sense, especially
for scenarios where pixels are heavily mixed. In the no pure-pixel case, a good
blind HU approach to consider is the minimum volume enclosing simplex (MVES).
Empirical experience has suggested that MVES algorithms can perform well
without pure pixels, although it was not totally clear why this is true from a
theoretical viewpoint. This paper aims to address the latter issue. We develop
an analysis framework wherein the perfect endmember identifiability of MVES is
studied under the noiseless case. We prove that MVES is indeed robust against
lack of pure pixels, as long as the pixels do not get too heavily mixed and too
asymmetrically spread. The theoretical results are verified by numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5279</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5279</id><created>2014-06-20</created><authors><author><keyname>Gharibian</keyname><forenames>Sevag</forenames></author><author><keyname>Landau</keyname><forenames>Zeph</forenames></author><author><keyname>Shin</keyname><forenames>Seung Woo</forenames></author><author><keyname>Wang</keyname><forenames>Guoming</forenames></author></authors><title>Tensor network non-zero testing</title><categories>quant-ph cond-mat.str-el cs.CC</categories><comments>15 pages, 4 figures</comments><journal-ref>Quantum Information &amp; Computation 15 (9 &amp; 10):885-899, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor networks are a central tool in condensed matter physics. In this
paper, we initiate the study of tensor network non-zero testing (TNZ): Given a
tensor network T, does T represent a non-zero vector? We show that TNZ is not
in the Polynomial-Time Hierarchy unless the hierarchy collapses. We next show
(among other results) that the special cases of TNZ on non-negative and
injective tensor networks are in NP. Using this, we make a simple observation:
The commuting variant of the MA-complete stoquastic k-SAT problem on
D-dimensional qudits is in NP for logarithmic k and constant D. This reveals
the first class of quantum Hamiltonians whose commuting variant is known to be
in NP for all (1) logarithmic k, (2) constant D, and (3) for arbitrary
interaction graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5280</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5280</id><created>2014-06-20</created><authors><author><keyname>Butt</keyname><forenames>M. Majid</forenames></author><author><keyname>Anwar</keyname><forenames>Ahmed</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author></authors><title>Effective Capacity of Cognitive Radio Links: Accessing Primary Feedback
  Erroneously</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in International Symposium on Wireless
  Communication Systems (ISWCS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of a cognitive system modeled by one secondary and
one primary link and operating under statistical quality of service (QoS) delay
constraints. We analyze the effective capacity (EC) to quantify the secondary
user (SU) performance under delay constraints. The SU intends to maximize the
benefit of the feedback messages on the primary link to reduce SU interference
for primary user (PU) and makes opportunistic use of the channel to transmit
his packets. We assume that SU has erroneous access to feedback information of
PU. We propose a three power level scheme and study the tradeoff between
degradation in EC of SU and reliability of PU defined as the success rate of
the transmitted packets. Our analysis shows that increase in error in feedback
access causes more interference to PU and packet success rate decreases
correspondingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5282</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5282</id><created>2014-06-20</created><updated>2014-06-23</updated><authors><author><keyname>Li</keyname><forenames>Mingqiang</forenames></author><author><keyname>Lee</keyname><forenames>Patrick P. C.</forenames></author></authors><title>STAIR Codes: A General Family of Erasure Codes for Tolerating Device and
  Sector Failures</title><categories>cs.IT math.IT</categories><comments>An earlier version of this work was presented at USENIX FAST '14.
  This extended version presents new reliability analysis for STAIR codes and
  is submitted for journal review</comments><acm-class>B.3.2; B.8.1; C.4; D.4.2; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Practical storage systems often adopt erasure codes to tolerate device
failures and sector failures, both of which are prevalent in the field.
However, traditional erasure codes employ device-level redundancy to protect
against sector failures, and hence incur significant space overhead. Recent
sector-disk (SD) codes are available only for limited configurations. By making
a relaxed but practical assumption, we construct a general family of erasure
codes called \emph{STAIR codes}, which efficiently and provably tolerate both
device and sector failures without any restriction on the size of a storage
array and the numbers of tolerable device failures and sector failures. We
propose the \emph{upstairs encoding} and \emph{downstairs encoding} methods,
which provide complementary performance advantages for different
configurations. We conduct extensive experiments on STAIR codes in terms of
space saving, encoding/decoding speed, and update cost. We demonstrate that
STAIR codes not only improve space efficiency over traditional erasure codes,
but also provide better computational efficiency than SD codes based on our
special code construction. Finally, we present analytical models that
characterize the reliability of STAIR codes, and show that the support of a
wider range of configurations by STAIR codes is critical for tolerating sector
failure bursts discovered in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5286</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5286</id><created>2014-06-20</created><authors><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author></authors><title>Enhancing Pure-Pixel Identification Performance via Preconditioning</title><categories>stat.ML cs.LG math.NA math.OC</categories><comments>25 pages, 3 figures</comments><journal-ref>SIAM J. on Imaging Sciences 8 (2), pp. 1161-1186, 2015</journal-ref><doi>10.1137/140994915</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze different preconditionings designed to enhance
robustness of pure-pixel search algorithms, which are used for blind
hyperspectral unmixing and which are equivalent to near-separable nonnegative
matrix factorization algorithms. Our analysis focuses on the successive
projection algorithm (SPA), a simple, efficient and provably robust algorithm
in the pure-pixel algorithm class. Recently, a provably robust preconditioning
was proposed by Gillis and Vavasis (arXiv:1310.2273) which requires the
resolution of a semidefinite program (SDP) to find a data points-enclosing
minimum volume ellipsoid. Since solving the SDP in high precisions can be time
consuming, we generalize the robustness analysis to approximate solutions of
the SDP, that is, solutions whose objective function values are some
multiplicative factors away from the optimal value. It is shown that a high
accuracy solution is not crucial for robustness, which paves the way for faster
preconditionings (e.g., based on first-order optimization methods). This first
contribution also allows us to provide a robustness analysis for two other
preconditionings. The first one is pre-whitening, which can be interpreted as
an optimal solution of the same SDP with additional constraints. We analyze
robustness of pre-whitening which allows us to characterize situations in which
it performs competitively with the SDP-based preconditioning. The second one is
based on SPA itself and can be interpreted as an optimal solution of a
relaxation of the SDP. It is extremely fast while competing with the SDP-based
preconditioning on several synthetic data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5291</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5291</id><created>2014-06-20</created><updated>2015-02-02</updated><authors><author><keyname>Chatterjee</keyname><forenames>Soumyadeep</forenames></author><author><keyname>Chen</keyname><forenames>Sheng</forenames></author><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author></authors><title>Generalized Dantzig Selector: Application to the k-support norm</title><categories>stat.ML cs.LG</categories><comments>Updates to bound</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Generalized Dantzig Selector (GDS) for linear models, in which
any norm encoding the parameter structure can be leveraged for estimation. We
investigate both computational and statistical aspects of the GDS. Based on
conjugate proximal operator, a flexible inexact ADMM framework is designed for
solving GDS, and non-asymptotic high-probability bounds are established on the
estimation error, which rely on Gaussian width of unit norm ball and suitable
set encompassing estimation error. Further, we consider a non-trivial example
of the GDS using $k$-support norm. We derive an efficient method to compute the
proximal operator for $k$-support norm since existing methods are inapplicable
in this setting. For statistical analysis, we provide upper bounds for the
Gaussian widths needed in the GDS analysis, yielding the first statistical
recovery guarantee for estimation with the $k$-support norm. The experimental
results confirm our theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5295</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5295</id><created>2014-06-20</created><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author></authors><title>Rows vs Columns for Linear Systems of Equations - Randomized Kaczmarz or
  Coordinate Descent?</title><categories>math.OC cs.LG cs.NA math.NA stat.ML</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about randomized iterative algorithms for solving a linear
system of equations $X \beta = y$ in different settings. Recent interest in the
topic was reignited when Strohmer and Vershynin (2009) proved the linear
convergence rate of a Randomized Kaczmarz (RK) algorithm that works on the rows
of $X$ (data points). Following that, Leventhal and Lewis (2010) proved the
linear convergence of a Randomized Coordinate Descent (RCD) algorithm that
works on the columns of $X$ (features). The aim of this paper is to simplify
our understanding of these two algorithms, establish the direct relationships
between them (though RK is often compared to Stochastic Gradient Descent), and
examine the algorithmic commonalities or tradeoffs involved with working on
rows or columns. We also discuss Kernel Ridge Regression and present a
Kaczmarz-style algorithm that works on data points and having the advantage of
solving the problem without ever storing or forming the Gram matrix, one of the
recognized problems encountered when scaling kernelized methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5298</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5298</id><created>2014-06-20</created><updated>2014-10-31</updated><authors><author><keyname>Kingma</keyname><forenames>Diederik P.</forenames></author><author><keyname>Rezende</keyname><forenames>Danilo J.</forenames></author><author><keyname>Mohamed</keyname><forenames>Shakir</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Semi-Supervised Learning with Deep Generative Models</title><categories>cs.LG stat.ML</categories><comments>To appear in the proceedings of Neural Information Processing Systems
  (NIPS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ever-increasing size of modern data sets combined with the difficulty of
obtaining label information has made semi-supervised learning one of the
problems of significant practical importance in modern data analysis. We
revisit the approach to semi-supervised learning with generative models and
develop new models that allow for effective generalisation from small labelled
data sets to large unlabelled ones. Generative approaches have thus far been
either inflexible, inefficient or non-scalable. We show that deep generative
models and approximate Bayesian inference exploiting recent advances in
variational methods can be used to provide significant improvements, making
generative approaches highly competitive for semi-supervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5299</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5299</id><created>2014-06-20</created><authors><author><keyname>Binucci</keyname><forenames>Carla</forenames></author><author><keyname>Di Giacomo</keyname><forenames>Emilio</forenames></author><author><keyname>Didimo</keyname><forenames>Walter</forenames></author><author><keyname>Montecchiani</keyname><forenames>Fabrizio</forenames></author><author><keyname>Patrignani</keyname><forenames>Maurizio</forenames></author><author><keyname>Tollis</keyname><forenames>Ioannis G.</forenames></author></authors><title>Properties and Complexity of Fan-Planarity</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a \emph{fan-planar drawing} of a graph an edge can cross only edges with a
common end-vertex. Fan-planar drawings have been recently introduced by
Kaufmann and Ueckerdt, who proved that every $n$-vertex fan-planar drawing has
at most $5n-10$ edges, and that this bound is tight for $n \geq 20$. We extend
their result, both from the combinatorial and the algorithmic point of view. We
prove tight bounds on the density of constrained versions of fan-planar
drawings and study the relationship between fan-planarity and $k$-planarity.
Furthermore, we prove that deciding whether a graph admits a fan-planar drawing
in the variable embedding setting is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5301</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5301</id><created>2014-06-20</created><updated>2016-02-22</updated><authors><author><keyname>Bo&#x161;kovi&#x107;</keyname><forenames>Borko</forenames></author><author><keyname>Brglez</keyname><forenames>Franc</forenames></author><author><keyname>Brest</keyname><forenames>Janez</forenames></author></authors><title>Low-Autocorrelation Binary Sequences: On Improved Merit Factors and
  Runtime Predictions to Achieve Them</title><categories>cs.DS cs.AI</categories><comments>text overlap with http://arxiv.org/abs/1406.5301</comments><msc-class>68T20</msc-class><acm-class>I.2.8; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The search for binary sequences with a high figure of merit, known as the low
autocorrelation binary sequence ($labs$}) problem, represents a formidable
computational challenge. To mitigate the computational constraints of the
problem, we consider solvers that accept odd values of sequence length $L$ and
return solutions for skew-symmetric binary sequences only -- with the
consequence that not all best solutions under this constraint will be optimal
for each $L$. In order to improve both, the search for best merit factor $and$
the asymptotic runtime performance, we instrumented three stochastic solvers,
the first two are state-of-the-art solvers that rely on variants of memetic and
tabu search ($lssMAts$ and $lssRRts$), the third solver ($lssOrel$) organizes
the search as a sequence of independent contiguous self-avoiding walk segments.
By adapting a rigorous statistical methodology to performance testing of all
three combinatorial solvers, experiments show that the solver with the best
asymptotic average-case performance, $lssOrel\_8 = 0.000032*1.1504^L$, has the
best chance of finding solutions that improve, as $L$ increases, figures of
merit reported to date. The same methodology can be applied to engineering new
$labs$ solvers that may return merit factors even closer to the conjectured
asymptotic value of 12.3248.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5306</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5306</id><created>2014-06-20</created><authors><author><keyname>Meunier</keyname><forenames>Pierre-&#xc9;tienne</forenames></author></authors><title>Unraveling simplicity in elementary cellular automata</title><categories>cs.CC cs.DM cs.FL nlin.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a large number of elementary cellular automata are
computationally simple. This work is the first systematic classification of
elementary cellular automata based on a formal notion of computational
complexity. Thanks to the generality of communication complexity, the
perspectives of our method include its application to other natural systems
such as neural networks and gene regulatory networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5309</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5309</id><created>2014-06-20</created><updated>2015-07-06</updated><authors><author><keyname>Ryoo</keyname><forenames>M. S.</forenames><affiliation>JPL</affiliation></author><author><keyname>Fuchs</keyname><forenames>Thomas J.</forenames><affiliation>JPL</affiliation></author><author><keyname>Xia</keyname><forenames>Lu</forenames><affiliation>UT Austin</affiliation></author><author><keyname>Aggarwal</keyname><forenames>J. K.</forenames><affiliation>UT Austin</affiliation></author><author><keyname>Matthies</keyname><forenames>Larry</forenames><affiliation>JPL</affiliation></author></authors><title>Early Recognition of Human Activities from First-Person Videos Using
  Onset Representations</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a methodology for early recognition of human
activities from videos taken with a first-person viewpoint. Early recognition,
which is also known as activity prediction, is an ability to infer an ongoing
activity at its early stage. We present an algorithm to perform recognition of
activities targeted at the camera from streaming videos, making the system to
predict intended activities of the interacting person and avoid harmful events
before they actually happen. We introduce the novel concept of 'onset' that
efficiently summarizes pre-activity observations, and design an approach to
consider event history in addition to ongoing video observation for early
first-person recognition of activities. We propose to represent onset using
cascade histograms of time series gradients, and we describe a novel
algorithmic setup to take advantage of onset for early recognition of
activities. The experimental results clearly illustrate that the proposed
concept of onset enables better/earlier recognition of human activities from
first-person videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5311</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5311</id><created>2014-06-20</created><updated>2016-01-29</updated><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Pe&#xf1;a</keyname><forenames>Javier</forenames></author></authors><title>Towards A Deeper Geometric, Analytic and Algorithmic Understanding of
  Margins</title><categories>math.OC cs.AI cs.LG math.NA stat.ML</categories><comments>18 pages, 3 figures</comments><journal-ref>Optimization Methods and Software, Volume 31, Issue 2, Pages
  377-391, 2016</journal-ref><doi>10.1080/10556788.2015.1099652</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a matrix $A$, a linear feasibility problem (of which linear
classification is a special case) aims to find a solution to a primal problem
$w: A^Tw &gt; \textbf{0}$ or a certificate for the dual problem which is a
probability distribution $p: Ap = \textbf{0}$. Inspired by the continued
importance of &quot;large-margin classifiers&quot; in machine learning, this paper
studies a condition measure of $A$ called its \textit{margin} that determines
the difficulty of both the above problems. To aid geometrical intuition, we
first establish new characterizations of the margin in terms of relevant balls,
cones and hulls. Our second contribution is analytical, where we present
generalizations of Gordan's theorem, and variants of Hoffman's theorems, both
using margins. We end by proving some new results on a classical iterative
scheme, the Perceptron, whose convergence rates famously depends on the margin.
Our results are relevant for a deeper understanding of margin-based learning
and proving convergence rates of iterative schemes, apart from providing a
unifying perspective on this vast topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5316</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5316</id><created>2014-06-20</created><authors><author><keyname>Joy</keyname><forenames>Preetha Theresa</forenames></author><author><keyname>Jacob</keyname><forenames>K. Poulose</forenames></author></authors><title>Cache Discovery Over a Multihop Wireless Ad Hoc Network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multihop ad hoc wireless networks consist of mobile nodes that communicate
with each other without any fixed infrastructure. The nodes in these networks
are power constrained, since they operate in limited battery energy.
Cooperative caching is an attractive solution for reducing network traffic and
bandwidth demands in mobile ad hoc networks. Deploying caches in mobile nodes
can reduce the overall traffic considerably.
  Cache hits eliminate the need to contact the data source frequently, which
avoids additional network overhead. In this paper we propose a cache discovery
policy for cooperative caching, which reduces the power usage, caching overhead
and delay. This is done by power control and transmission range adjustment. A
cache discovery process based on position coordinates of neighboring nodes is
developed for this. The simulation results gives a promising result based on
the metrics of studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5323</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5323</id><created>2014-06-20</created><authors><author><keyname>Popovi&#x107;</keyname><forenames>Marko</forenames></author><author><keyname>&#x160;tefan&#x10d;i&#x107;</keyname><forenames>Hrvoje</forenames></author><author><keyname>Sluban</keyname><forenames>Borut</forenames></author><author><keyname>Novak</keyname><forenames>Petra Kralj</forenames></author><author><keyname>Gr&#x10d;ar</keyname><forenames>Miha</forenames></author><author><keyname>Mozeti&#x10d;</keyname><forenames>Igor</forenames></author><author><keyname>Puliga</keyname><forenames>Michelangelo</forenames></author><author><keyname>Zlati&#x107;</keyname><forenames>Vinko</forenames></author></authors><title>Extraction of Temporal Networks from Term Co-occurrences in Online
  Textual Sources</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>27 pages, 12 figures</comments><journal-ref>PLoS ONE 9(12): e99515 (2014)</journal-ref><doi>10.1371/journal.pone.0099515</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stream of unstructured news can be a valuable source of hidden relations
between different entities, such as financial institutions, countries, or
persons. We present an approach to continuously collect online news, recognize
relevant entities in them, and extract time-varying networks. The nodes of the
network are the entities, and the links are their co-occurrences. We present a
method to estimate the significance of co-occurrences, and a benchmark model
against which their robustness is evaluated. The approach is applied to a large
set of financial news, collected over a period of two years. The entities we
consider are 50 countries which issue sovereign bonds, and which are insured by
Credit Default Swaps (CDS) in turn. We compare the country co-occurrence
networks to the CDS networks constructed from the correlations between the CDS.
The results show relatively small, but significant overlap between the networks
extracted from the news and those from the CDS correlations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5336</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5336</id><created>2014-06-20</created><authors><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Kulkarni</keyname><forenames>Raghav</forenames></author><author><keyname>Qiao</keyname><forenames>Youming</forenames></author><author><keyname>Santha</keyname><forenames>Miklos</forenames></author><author><keyname>Sundaram</keyname><forenames>Aarthi</forenames></author></authors><title>On the Complexity of Trial and Error for Constraint Satisfaction
  Problems</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent work of Bei, Chen and Zhang (STOC 2013), a trial and error model
of computing was introduced, and applied to some constraint satisfaction
problems. In this model the input is hidden by an oracle which, for a candidate
assignment, reveals some information about a violated constraint if the
assignment is not satisfying. In this paper we initiate a systematic study of
constraint satisfaction problems in the trial and error model. To achieve this,
we first adopt a formal framework for CSPs, and based on this framework we
define several types of revealing oracles. Our main contribution is to develop
a transfer theorem for each type of the revealing oracle, under a broad class
of parameters. To any hidden CSP with a specific type of revealing oracle, the
transfer theorem associates another, potentially harder CSP in the normal
setting, such that their complexities are polynomial time equivalent. This in
principle transfers the study of a large class of hidden CSPs, possibly with a
promise on the instances, to the study of CSPs in the normal setting. We then
apply the transfer theorems to get polynomial-time algorithms or hardness
results for hidden CSPs, including satisfaction problems, monotone graph
properties, isomorphism problems, and the exact version of the Unique Games
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5354</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5354</id><created>2014-06-20</created><authors><author><keyname>Xu</keyname><forenames>Shengfeng</forenames></author><author><keyname>Zhu</keyname><forenames>Gang</forenames></author><author><keyname>Shen</keyname><forenames>Chao</forenames></author><author><keyname>Ai</keyname><forenames>Bo</forenames></author></authors><title>A QoS-Aware Scheduling Algorithm for High-Speed Railway Communication
  System</title><categories>cs.NI</categories><comments>6 pages, 3 figures, accepted by IEEE ICC 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of high-speed railway (HSR), how to provide the
passengers with multimedia services has attracted increasing attention. A key
issue is to develop an effective scheduling algorithm for multiple services
with different quality of service (QoS) requirements. In this paper, we
investigate the downlink service scheduling problem in HSR network taking
account of end-to-end deadline constraints and successfully packet delivery
ratio requirements. Firstly, by exploiting the deterministic high-speed train
trajectory, we present a time-distance mapping in order to obtain the highly
dynamic link capacity effectively. Next, a novel service model is developed for
deadline constrained services with delivery ratio requirements, which enables
us to turn the delivery ratio requirement into a single queue stability
problem. Based on the Lyapunov drift, the optimal scheduling problem is
formulated and the corresponding scheduling service algorithm is proposed by
stochastic network optimization approach. Simulation results show that the
proposed algorithm outperforms the conventional schemes in terms of QoS
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5362</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5362</id><created>2014-06-20</created><updated>2014-11-20</updated><authors><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>Predicting the Future Behavior of a Time-Varying Probability
  Distribution</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of predicting the future, though only in the
probabilistic sense of estimating a future state of a time-varying probability
distribution. This is not only an interesting academic problem, but solving
this extrapolation problem also has many practical application, e.g. for
training classifiers that have to operate under time-varying conditions. Our
main contribution is a method for predicting the next step of the time-varying
distribution from a given sequence of sample sets from earlier time steps. For
this we rely on two recent machine learning techniques: embedding probability
distributions into a reproducing kernel Hilbert space, and learning operators
by vector-valued regression. We illustrate the working principles and the
practical usefulness of our method by experiments on synthetic and real data.
We also highlight an exemplary application: training a classifier in a domain
adaptation setting without having access to examples from the test time
distribution at training time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5369</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5369</id><created>2014-06-20</created><authors><author><keyname>Koestler</keyname><forenames>Harald</forenames></author><author><keyname>Schmitt</keyname><forenames>Christian</forenames></author><author><keyname>Kuckuk</keyname><forenames>Sebastian</forenames></author><author><keyname>Hannig</keyname><forenames>Frank</forenames></author><author><keyname>Teich</keyname><forenames>Juergen</forenames></author><author><keyname>Ruede</keyname><forenames>Ulrich</forenames></author></authors><title>A Scala Prototype to Generate Multigrid Solver Implementations for
  Different Problems and Target Multi-Core Platforms</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in computational science and engineering involve partial
differential equations and thus require the numerical solution of large, sparse
(non)linear systems of equations. Multigrid is known to be one of the most
efficient methods for this purpose. However, the concrete multigrid algorithm
and its implementation highly depend on the underlying problem and hardware.
Therefore, changes in the code or many different variants are necessary to
cover all relevant cases. In this article we provide a prototype implementation
in Scala for a framework that allows abstract descriptions of PDEs, their
discretization, and their numerical solution via multigrid algorithms. From
these, one is able to generate data structures and implementations of multigrid
components required to solve elliptic PDEs on structured grids. Two different
test problems showcase our proposed automatic generation of multigrid solvers
for both CPU and GPU target platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5370</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5370</id><created>2014-06-20</created><updated>2016-01-18</updated><authors><author><keyname>Fogel</keyname><forenames>Fajwel</forenames></author><author><keyname>d'Aspremont</keyname><forenames>Alexandre</forenames></author><author><keyname>Vojnovic</keyname><forenames>Milan</forenames></author></authors><title>Spectral Ranking using Seriation</title><categories>cs.LG cs.AI stat.ML</categories><comments>Revised version. Typos, minor errors fixed</comments><msc-class>62F07, 06A07, 90C27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a seriation algorithm for ranking a set of items given pairwise
comparisons between these items. Intuitively, the algorithm assigns similar
rankings to items that compare similarly with all others. It does so by
constructing a similarity matrix from pairwise comparisons, using seriation
methods to reorder this matrix and construct a ranking. We first show that this
spectral seriation algorithm recovers the true ranking when all pairwise
comparisons are observed and consistent with a total order. We then show that
ranking reconstruction is still exact when some pairwise comparisons are
corrupted or missing, and that seriation based spectral ranking is more robust
to noise than classical scoring methods. Finally, we bound the ranking error
when only a random subset of the comparions are observed. An additional benefit
of the seriation formulation is that it allows us to solve semi-supervised
ranking problems. Experiments on both synthetic and real datasets demonstrate
that seriation based spectral ranking achieves competitive and in some cases
superior performance compared to classical ranking methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5376</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5376</id><created>2014-06-20</created><authors><author><keyname>&#xc1;gueda</keyname><forenames>Raquel</forenames></author><author><keyname>Borozan</keyname><forenames>Valentin</forenames></author><author><keyname>Groshaus</keyname><forenames>Marina</forenames></author><author><keyname>Manoussakis</keyname><forenames>Yannis</forenames></author><author><keyname>Mendy</keyname><forenames>Gervais</forenames></author><author><keyname>Montero</keyname><forenames>Leandro</forenames></author></authors><title>Proper Hamiltonian Paths in Edge-Coloured Multigraphs</title><categories>cs.DM math.CO</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a $c$-edge-coloured multigraph, a proper Hamiltonian path is a path
that contains all the vertices of the multigraph such that no two adjacent
edges have the same colour. In this work we establish sufficient conditions for
an edge-coloured multigraph to guarantee the existence of a proper Hamiltonian
path, involving various parameters as the number of edges, the number of
colours, the rainbow degree and the connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5383</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5383</id><created>2014-06-20</created><updated>2015-11-23</updated><authors><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Noise-adaptive Margin-based Active Learning and Lower Bounds under
  Tsybakov Noise Condition</title><categories>stat.ML cs.LG</categories><comments>16 pages, 2 figures. An abridged version to appear in Thirtieth AAAI
  Conference on Artificial Intelligence (AAAI), which is held in Phoenix, AZ
  USA in 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple noise-robust margin-based active learning algorithm to
find homogeneous (passing the origin) linear separators and analyze its error
convergence when labels are corrupted by noise. We show that when the imposed
noise satisfies the Tsybakov low noise condition (Mammen, Tsybakov, and others
1999; Tsybakov 2004) the algorithm is able to adapt to unknown level of noise
and achieves optimal statistical rate up to poly-logarithmic factors. We also
derive lower bounds for margin based active learning algorithms under Tsybakov
noise conditions (TNC) for the membership query synthesis scenario (Angluin
1988). Our result implies lower bounds for the stream based selective sampling
scenario (Cohn 1990) under TNC for some fairly simple data distributions. Quite
surprisingly, we show that the sample complexity cannot be improved even if the
underlying data distribution is as simple as the uniform distribution on the
unit ball. Our proof involves the construction of a well separated hypothesis
set on the d-dimensional unit ball along with carefully designed label
distributions for the Tsybakov noise condition. Our analysis might provide
insights for other forms of lower bounds as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5388</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5388</id><created>2014-06-20</created><updated>2015-02-26</updated><authors><author><keyname>Magoarou</keyname><forenames>Luc Le</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Learning computationally efficient dictionaries and their implementation
  as fast transforms</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionary learning is a branch of signal processing and machine learning
that aims at finding a frame (called dictionary) in which some training data
admits a sparse representation. The sparser the representation, the better the
dictionary. The resulting dictionary is in general a dense matrix, and its
manipulation can be computationally costly both at the learning stage and later
in the usage of this dictionary, for tasks such as sparse coding. Dictionary
learning is thus limited to relatively small-scale problems. In this paper,
inspired by usual fast transforms, we consider a general dictionary structure
that allows cheaper manipulation, and propose an algorithm to learn such
dictionaries --and their fast implementation-- over training data. The approach
is demonstrated experimentally with the factorization of the Hadamard matrix
and with synthetic dictionary learning experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5405</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5405</id><created>2014-06-20</created><updated>2014-10-16</updated><authors><author><keyname>Wu</keyname><forenames>Huai-Ning</forenames></author><author><keyname>Wang</keyname><forenames>Hong-Du</forenames></author></authors><title>Distributed Consensus Observers Based H-infinity Control of Dissipative
  PDE Systems Using Sensor Networks</title><categories>cs.SY math.AP</categories><comments>12 pages,4 figures,A complete proof of Theorem 2, final version for
  IEEE Transactions on Control of Network Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of finite dimensional output feedback
H-infinity control for a class of nonlinear spatially distributed processes
(SDPs) described by highly dissipative partial differential equations (PDEs),
whose state is observed by a sensor network (SN) with a given topology. A
highly dissipative PDE system typically involves a spatial differential
operator with eigenspectrum that can be partitioned into a finite-dimensional
slow one and an infinite-dimensional stable fast complement. Motivated by this
fact, the modal decomposition and singular perturbation techniques are
initially applied to the PDE system to derive a finite dimensional ordinary
differential equation model, which accurately captures the dynamics of the slow
modes of the PDE system. Subsequently, based on the slow system and the
topology of the SN, a set of finite dimensional distributed consensus observers
are constructed to estimate the state of the slow system. Then, a centralized
control scheme, which only uses the available estimates from a specified group
of SN nodes, is proposed for the PDE system. An H-infinity control design
method is developed in terms of bilinear matrix inequality (BMI), such that the
original closed-loop PDE system is exponentially stable and a prescribed level
of disturbance attenuation is satisfied for the slow system. Furthermore, a
suboptimal H-infinity controller is also provided to make the attenuation level
as small as possible, which can be obtained via a local optimization algorithm
that treats the BMI as double linear matrix inequality. Finally, the proposed
method is applied to the control of one dimensional Kuramoto-Sivashinsky
equation (KSE) system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5422</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5422</id><created>2014-06-20</created><updated>2015-07-14</updated><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>Optimum Power Control at Finite Blocklength</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the maximal channel coding rate achievable at a given
blocklength $n$ and error probability $\epsilon$, when the codewords are
subject to a long-term (i.e., averaged-over-all-codeword) power constraint. The
second-order term in the large-$n$ expansion of the maximal channel coding rate
is characterized both for additive white Gaussian noise (AWGN) channels and for
quasi-static fading channels with perfect channel state information available
at both the transmitter and the receiver. It is shown that in both cases the
second-order term is proportional to $\sqrt{n^{-1}\ln n}$. For the quasi-static
fading case, this second-order term is achieved by truncated channel inversion,
namely, by concatenating a dispersion-optimal code for an AWGN channel subject
to a short-term power constraint, with a power controller that inverts the
channel whenever the fading gain is above a certain threshold. Easy-to-evaluate
approximations of the maximal channel coding rate are developed for both the
AWGN and the quasi-static fading case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5429</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5429</id><created>2014-06-20</created><updated>2014-12-03</updated><authors><author><keyname>Komodakis</keyname><forenames>Nikos</forenames></author><author><keyname>Pesquet</keyname><forenames>Jean-Christophe</forenames></author></authors><title>Playing with Duality: An Overview of Recent Primal-Dual Approaches for
  Solving Large-Scale Optimization Problems</title><categories>cs.NA cs.CV cs.LG math.OC</categories><acm-class>G.1.6; I.4; I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization methods are at the core of many problems in signal/image
processing, computer vision, and machine learning. For a long time, it has been
recognized that looking at the dual of an optimization problem may drastically
simplify its solution. Deriving efficient strategies which jointly brings into
play the primal and the dual problems is however a more recent idea which has
generated many important new contributions in the last years. These novel
developments are grounded on recent advances in convex analysis, discrete
optimization, parallel processing, and non-smooth optimization with emphasis on
sparsity issues. In this paper, we aim at presenting the principles of
primal-dual approaches, while giving an overview of numerical methods which
have been proposed in different contexts. We show the benefits which can be
drawn from primal-dual algorithms both for solving large-scale convex
optimization problems and discrete ones, and we provide various application
examples to illustrate their usefulness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5431</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5431</id><created>2014-06-20</created><updated>2014-06-23</updated><authors><author><keyname>Takayama</keyname><forenames>Kenshi</forenames></author><author><keyname>Jacobson</keyname><forenames>Alec</forenames></author><author><keyname>Kavan</keyname><forenames>Ladislav</forenames></author><author><keyname>Sorkine-Hornung</keyname><forenames>Olga</forenames></author></authors><title>Consistently Orienting Facets in Polygon Meshes by Minimizing the
  Dirichlet Energy of Generalized Winding Numbers</title><categories>cs.GR</categories><comments>6 pages, 4 figures</comments><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jacobson et al. [JKSH13] hypothesized that the local coherency of the
generalized winding number function could be used to correctly determine
consistent facet orientations in polygon meshes. We report on an approach to
consistently orienting facets in polygon meshes by minimizing the Dirichlet
energy of generalized winding numbers. While the energy can be concisely
formulated and efficiently computed, we found that this approach is
fundamentally flawed and is unfortunately not applicable for most handmade
meshes shared on popular mesh repositories such as Google 3D Warehouse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5433</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5433</id><created>2014-06-20</created><updated>2014-09-11</updated><authors><author><keyname>Allamigeon</keyname><forenames>Xavier</forenames></author><author><keyname>Benchimol</keyname><forenames>Pascal</forenames></author><author><keyname>Gaubert</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>The tropical shadow-vertex algorithm solves mean payoff games in
  polynomial time on average</title><categories>cs.GT cs.DS math.OC</categories><comments>17 pages, 7 figures, appears in 41st International Colloquium, ICALP
  2014, Copenhagen, Denmark, July 8-11, 2014, Proceedings, Part I</comments><doi>10.1007/978-3-662-43948-7_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an algorithm which solves mean payoff games in polynomial time
on average, assuming the distribution of the games satisfies a flip invariance
property on the set of actions associated with every state. The algorithm is a
tropical analogue of the shadow-vertex simplex algorithm, which solves mean
payoff games via linear feasibility problems over the tropical semiring
$(\mathbb{R} \cup \{-\infty\}, \max, +)$. The key ingredient in our approach is
that the shadow-vertex pivoting rule can be transferred to tropical polyhedra,
and that its computation reduces to optimal assignment problems through
Pl\&quot;ucker relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5440</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5440</id><created>2014-06-20</created><authors><author><keyname>Dinesh</keyname></author><author><keyname>Erlich</keyname></author><author><keyname>Gilfoyle</keyname></author><author><keyname>Jared</keyname></author><author><keyname>Richard</keyname></author><author><keyname>Pouwelse</keyname><forenames>Johan</forenames></author></authors><title>Operational Distributed Regulation for Bitcoin</title><categories>cs.CR cs.CY cs.DC</categories><comments>9 pages report by students who desire to remain anonymous</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On February 2014, $650.000.000 worth of Bitcoins disappeared. Currently it is
unclear whether hackers or MtGox, the largest Bitcoin exchange, are to be
blamed. In either case, the anonymous and unregulated nature of the Bitcoin
system makes it practically impossible for innocent victims to get their money
back. We have investigated the technical possibilities, solutions and
implications of introducing a regulatory framework based on redlisting Bitcoin
accounts. Despite numerous proposals, the Bitcoin community has voiced a strong
opinion against any form of regulation. However, most of the discussions were
based on speculations rather than facts. We strive to contribute a scientific
foundation to these discussions and illuminate the path to crypto-justice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5453</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5453</id><created>2014-06-20</created><updated>2015-02-05</updated><authors><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author></authors><title>Rotation of Sequences: Algorithms and Proofs</title><categories>cs.LO</categories><comments>Added verified implementations for ESC/Java2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence rotation consists of a circular shift of the sequence's elements by
a given number of positions. We present the four classic algorithms to rotate a
sequence; the loop invariants underlying their correctness; detailed
correctness proofs; and fully annotated versions for the verifiers Boogie,
Dafny, and ESC/Java2. The presentation illustrates in detail both how the
algorithms work and what it takes to carry out mechanized proofs of their
correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5457</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5457</id><created>2014-06-19</created><authors><author><keyname>Gawlitza</keyname><forenames>Thomas M.</forenames></author><author><keyname>Schwarz</keyname><forenames>Martin D.</forenames></author><author><keyname>Seidl</keyname><forenames>Helmut</forenames></author></authors><title>Parametric Strategy Iteration</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program behavior may depend on parameters, which are either configured before
compilation time, or provided at run-time, e.g., by sensors or other input
devices. Parametric program analysis explores how different parameter settings
may affect the program behavior.
  In order to infer invariants depending on parameters, we introduce parametric
strategy iteration. This algorithm determines the precise least solution of
systems of integer equations depending on surplus parameters. Conceptually, our
algorithm performs ordinary strategy iteration on the given integer system for
all possible parameter settings in parallel. This is made possible by means of
region trees to represent the occurring piecewise affine functions. We indicate
that each required operation on these trees is polynomial-time if only
constantly many parameters are involved.
  Parametric strategy iteration for systems of integer equations allows to
construct parametric integer interval analysis as well as parametric analysis
of differences of integer variables. It thus provides a general technique to
realize precise parametric program analysis if numerical properties of integer
variables are of concern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5472</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5472</id><created>2014-06-20</created><authors><author><keyname>Pirsiavash</keyname><forenames>Hamed</forenames></author><author><keyname>Vondrick</keyname><forenames>Carl</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Inferring the Why in Images</title><categories>cs.CV</categories><comments>Hamed Pirsiavash and Carl Vondrick contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans have the remarkable capability to infer the motivations of other
people's actions, likely due to cognitive skills known in psychophysics as the
theory of mind. In this paper, we strive to build a computational model that
predicts the motivation behind the actions of people from images. To our
knowledge, this challenging problem has not yet been extensively explored in
computer vision. We present a novel learning based framework that uses
high-level visual recognition to infer why people are performing an actions in
images. However, the information in an image alone may not be sufficient to
automatically solve this task. Since humans can rely on their own experiences
to infer motivation, we propose to give computer vision systems access to some
of these experiences by using recently developed natural language models to
mine knowledge stored in massive amounts of text. While we are still far away
from automatically inferring motivation, our results suggest that transferring
knowledge from language into vision can help machines understand why a person
might be performing an action in an image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5480</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5480</id><created>2014-06-20</created><authors><author><keyname>Barton</keyname><forenames>Carl</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Costas S.</forenames></author><author><keyname>Pissis</keyname><forenames>Solon P.</forenames></author></authors><title>Average-Case Optimal Approximate Circular String Matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate string matching is the problem of finding all factors of a text t
of length n that are at a distance at most k from a pattern x of length m.
Approximate circular string matching is the problem of finding all factors of t
that are at a distance at most k from x or from any of its rotations. In this
article, we present a new algorithm for approximate circular string matching
under the edit distance model with optimal average-case search time O(n(k + log
m)/m). Optimal average-case search time can also be achieved by the algorithms
for multiple approximate string matching (Fredriksson and Navarro, 2004) using
x and its rotations as the set of multiple patterns. Here we reduce the
preprocessing time and space requirements compared to that approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5481</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5481</id><created>2014-06-20</created><authors><author><keyname>Roy</keyname><forenames>Matthieu</forenames><affiliation>LAAS</affiliation></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames><affiliation>LAAS</affiliation></author><author><keyname>Tr&#xe9;dan</keyname><forenames>Gilles</forenames><affiliation>LAAS</affiliation></author></authors><title>Modeling and Measuring Graph Similarity: The Case for Centrality
  Distance</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>FOMC 2014, 10th ACM International Workshop on Foundations of Mobile
  Computing, Philadelphia : United States (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of the topological structure of complex networks has fascinated
researchers for several decades, and today we have a fairly good understanding
of the types and reoccurring characteristics of many different complex
networks. However, surprisingly little is known today about models to compare
complex graphs, and quantitatively measure their similarity. This paper
proposes a natural similarity measure for complex networks: centrality
distance, the difference between two graphs with respect to a given node
centrality. Centrality distances allow to take into account the specific roles
of the different nodes in the network, and have many interesting applications.
As a case study, we consider the closeness centrality in more detail, and show
that closeness centrality distance can be used to effectively distinguish
between randomly generated and actual evolutionary paths of two dynamic social
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5493</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5493</id><created>2014-06-20</created><authors><author><keyname>Lin</keyname><forenames>Trista</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Rivano</keyname><forenames>Herv&#xe9;</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CSE, CITI</affiliation></author></authors><title>Performance Comparison of Contention- and Schedule-based MAC Protocols
  in Urban Parking Sensor Networks</title><categories>cs.NI</categories><comments>ACM International Workshop on Wireless and Mobile Technologies for
  Smart Cities (WiMobCity) (2014)</comments><proxy>ccsd</proxy><doi>10.1145/2633661.2633663</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network traffic model is a critical problem for urban applications, mainly
because of its diversity and node density. As wireless sensor network is highly
concerned with the development of smart cities, careful consideration to
traffic model helps choose appropriate protocols and adapt network parameters
to reach best performances on energy-latency tradeoffs. In this paper, we
compare the performance of two off-the-shelf medium access control protocols on
two different kinds of traffic models, and then evaluate their application-end
information delay and energy consumption while varying traffic parameters and
network density. From the simulation results, we highlight some limits induced
by network density and occurrence frequency of event-driven applications. When
it comes to realtime urban services, a protocol selection shall be taken into
account - even dynamically - with a special attention to energy-delay tradeoff.
To this end, we provide several insights on parking sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5495</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5495</id><created>2014-06-20</created><authors><author><keyname>Muyeba</keyname><forenames>Maybin</forenames></author><author><keyname>Rybakov</keyname><forenames>Vladimir</forenames></author></authors><title>Knowledge Representation in Agent's Logic with Uncertainty and Agent's
  Interaction</title><categories>cs.LO</categories><msc-class>03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies knowledge representation in multi-agent environment. We
investigate technique for computation truth-values of statements based at a new
temporal, agent's-knowledge logic TL. A logical language, mathematical symbolic
models and a temporal logic TL based at these models are suggested. We find an
algorithm which computes theorems of TL and satisfiability of statements, this
implies that TL is decidable (i.e. -- the satisfiability problem for TL is
solvable). Application areas are pointed and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5520</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5520</id><created>2014-06-20</created><authors><author><keyname>Moed</keyname><forenames>Henk F.</forenames></author><author><keyname>Halevi</keyname><forenames>Gali</forenames></author></authors><title>The Multidimensional Assessment of Scholarly Research Impact</title><categories>cs.DL</categories><comments>Author copy version accepted for publication, JASIST (Journal of the
  Association for Information Science and Technology) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces the Multidimensional Research Assessment Matrix of
scientific output. Its base notion holds that the choice of metrics to be
applied in a research assessment process depends upon the unit of assessment,
the research dimension to be assessed, and the purposes and policy context of
the assessment. An indicator may by highly useful within one assessment
process, but less so in another. For instance, publication counts are useful
tools to help discriminating between those staff members who are research
active, and those who are not, but are of little value if active scientists are
to be compared one another according to their research performance. This paper
gives a systematic account of the potential usefulness and limitations of a set
of 10 important metrics including altmetrics, applied at the level of
individual articles, individual researchers, research groups and institutions.
It presents a typology of research impact dimensions, and indicates which
metrics are the most appropriate to measure each dimension. It introduces the
concept of a meta-analysis of the units under assessment in which metrics are
not used as tools to evaluate individual units, but to reach policy inferences
regarding the objectives and general setup of an assessment process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5521</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5521</id><created>2014-06-20</created><authors><author><keyname>Azgin</keyname><forenames>Aytac</forenames></author><author><keyname>Ravindran</keyname><forenames>Ravishankar</forenames></author><author><keyname>Wang</keyname><forenames>Guoqiang</forenames></author></authors><title>Mobility Study for Named Data Networking in Wireless Access Networks</title><categories>cs.NI</categories><comments>to appear in IEEE ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information centric networking (ICN) proposes to redesign the Internet by
replacing its host-centric design with information-centric design.
Communication among entities is established at the naming level, with the
receiver side (referred to as the Consumer) acting as the driving force behind
content delivery, by interacting with the network through Interest message
transmissions. One of the proposed advantages for ICN is its support for
mobility, by de-coupling applications from transport semantics. However, so
far, little research has been conducted to understand the interaction between
ICN and mobility of consuming and producing applications, in protocols purely
based on information-centric principles, particularly in the case of NDN. In
this paper, we present our findings on the mobility-based performance of Named
Data Networking (NDN) in wireless access networks. Through simulations, we show
that the current NDN architecture is not efficient in handling mobility and
architectural enhancements needs to be done to fully support mobility of
Consumers and Producers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5549</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5549</id><created>2014-06-20</created><updated>2014-11-24</updated><authors><author><keyname>Doll&#xe1;r</keyname><forenames>Piotr</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author></authors><title>Fast Edge Detection Using Structured Forests</title><categories>cs.CV</categories><comments>update corresponding to acceptance to PAMI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge detection is a critical component of many vision systems, including
object detectors and image segmentation algorithms. Patches of edges exhibit
well-known forms of local structure, such as straight lines or T-junctions. In
this paper we take advantage of the structure present in local image patches to
learn both an accurate and computationally efficient edge detector. We
formulate the problem of predicting local edge masks in a structured learning
framework applied to random decision forests. Our novel approach to learning
decision trees robustly maps the structured labels to a discrete space on which
standard information gain measures may be evaluated. The result is an approach
that obtains realtime performance that is orders of magnitude faster than many
competing state-of-the-art approaches, while also achieving state-of-the-art
edge detection results on the BSDS500 Segmentation dataset and NYU Depth
dataset. Finally, we show the potential of our approach as a general purpose
edge detector by showing our learned edge models generalize well across
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5550</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5550</id><created>2014-06-20</created><updated>2014-06-27</updated><authors><author><keyname>Gorban</keyname><forenames>Alexander N.</forenames></author><author><keyname>Pitenko</keyname><forenames>Alexander</forenames></author><author><keyname>Zinovyev</keyname><forenames>Andrei</forenames></author></authors><title>ViDaExpert: user-friendly tool for nonlinear visualization and analysis
  of multidimensional vectorial data</title><categories>cs.MS stat.CO</categories><msc-class>68N01, 68W25</msc-class><acm-class>G.3; G.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  ViDaExpert is a tool for visualization and analysis of multidimensional
vectorial data. ViDaExpert is able to work with data tables of &quot;object-feature&quot;
type that might contain numerical feature values as well as textual labels for
rows (objects) and columns (features). ViDaExpert implements several
statistical methods such as standard and weighted Principal Component Analysis
(PCA) and the method of elastic maps (non-linear version of PCA), Linear
Discriminant Analysis (LDA), multilinear regression, K-Means clustering, a
variant of decision tree construction algorithm. Equipped with several
user-friendly dialogs for configuring data point representations (size, shape,
color) and fast 3D viewer, ViDaExpert is a handy tool allowing to construct an
interactive 3D-scene representing a table of data in multidimensional space and
perform its quick and insightfull statistical analysis, from basic to advanced
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5554</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5554</id><created>2014-06-20</created><updated>2014-09-25</updated><authors><author><keyname>Rahmati</keyname><forenames>Zahed</forenames></author><author><keyname>King</keyname><forenames>Valerie</forenames></author><author><keyname>Whitesides</keyname><forenames>Sue</forenames></author></authors><title>Kinetic Reverse $k$-Nearest Neighbor Problem</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides the first solution to the kinetic reverse $k$-nearest
neighbor (\rknn) problem in $\mathbb{R}^d$, which is defined as follows: Given
a set $P$ of $n$ moving points in arbitrary but fixed dimension $d$, an integer
$k$, and a query point $q\notin P$ at any time $t$, report all the points $p\in
P$ for which $q$ is one of the $k$-nearest neighbors of $p$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5555</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5555</id><created>2014-06-20</created><authors><author><keyname>Ahmed</keyname><forenames>Elsayed</forenames></author><author><keyname>Eltawil</keyname><forenames>Ahmed M.</forenames></author></authors><title>All-Digital Self-interference Cancellation Technique for Full-duplex
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-duplex systems are expected to double the spectral efficiency compared
to conventional half-duplex systems if the self-interference signal can be
significantly mitigated. Digital cancellation is one of the lowest complexity
self-interference cancellation techniques in full-duplex systems. However, its
mitigation capability is very limited, mainly due to transmitter and receiver
circuit's impairments. In this paper, we propose a novel digital
self-interference cancellation technique for full-duplex systems. The proposed
technique is shown to significantly mitigate the self-interference signal as
well as the associated transmitter and receiver impairments. In the proposed
technique, an auxiliary receiver chain is used to obtain a digital-domain copy
of the transmitted Radio Frequency (RF) self-interference signal. The
self-interference copy is then used in the digital-domain to cancel out both
the self-interference signal and the associated impairments. Furthermore, to
alleviate the receiver phase noise effect, a common oscillator is shared
between the auxiliary and ordinary receiver chains. A thorough analytical and
numerical analysis for the effect of the transmitter and receiver impairments
on the cancellation capability of the proposed technique is presented. Finally,
the overall performance is numerically investigated showing that using the
proposed technique, the self-interference signal could be mitigated to ~3dB
higher than the receiver noise floor, which results in up to 76% rate
improvement compared to conventional half-duplex systems at 20dBm transmit
power values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5565</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5565</id><created>2014-06-20</created><authors><author><keyname>Morton</keyname><forenames>Kenneth D.</forenames><suffix>Jr.</suffix></author><author><keyname>Torrione</keyname><forenames>Peter</forenames></author><author><keyname>Collins</keyname><forenames>Leslie</forenames></author><author><keyname>Keene</keyname><forenames>Sam</forenames></author></authors><title>An Open Source Pattern Recognition Toolbox for MATLAB</title><categories>stat.ML cs.CV cs.LG cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern recognition and machine learning are becoming integral parts of
algorithms in a wide range of applications. Different algorithms and approaches
for machine learning include different tradeoffs between performance and
computation, so during algorithm development it is often necessary to explore a
variety of different approaches to a given task. A toolbox with a unified
framework across multiple pattern recognition techniques enables algorithm
developers the ability to rapidly evaluate different choices prior to
deployment. MATLAB is a widely used environment for algorithm development and
prototyping, and although several MATLAB toolboxes for pattern recognition are
currently available these are either incomplete, expensive, or restrictively
licensed. In this work we describe a MATLAB toolbox for pattern recognition and
machine learning known as the PRT (Pattern Recognition Toolbox), licensed under
the permissive MIT license. The PRT includes many popular techniques for data
preprocessing, supervised learning, clustering, regression and feature
selection, as well as a methodology for combining these components using a
simple, uniform syntax. The resulting algorithms can be evaluated using
cross-validation and a variety of scoring metrics to ensure robust performance
when the algorithm is deployed. This paper presents an overview of the PRT as
well as an example of usage on Fisher's Iris dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5567</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5567</id><created>2014-06-20</created><authors><author><keyname>Nakajima</keyname><forenames>Kohei</forenames></author><author><keyname>Haruna</keyname><forenames>Taichi</forenames></author></authors><title>Symbolic local information transfer</title><categories>nlin.AO cs.IT math.IT nlin.PS</categories><comments>20 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the permutation-information theoretic approach has been used in a
broad range of research fields. In particular, in the study of highdimensional
dynamical systems, it has been shown that this approach can be effective in
characterizing global properties, including the complexity of their
spatiotemporal dynamics. Here, we show that this approach can also be applied
to reveal local spatiotemporal profiles of distributed computations existing at
each spatiotemporal point in the system. J. T. Lizier et al. have recently
introduced the concept of local information dynamics, which consists of
information storage, transfer, and modification. This concept has been
intensively studied with regard to cellular automata, and has provided
quantitative evidence of several characteristic behaviors observed in the
system. In this paper, by focusing on the local information transfer, we
demonstrate that the application of the permutation-information theoretic
approach, which introduces natural symbolization methods, makes the concept
easily extendible to systems that have continuous states. We propose measures
called symbolic local transfer entropies, and apply these measures to two test
models, the coupled map lattice (CML) system and the Bak-Sneppen model
(BS-model), to show their relevance to spatiotemporal systems that have
continuous states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5569</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5569</id><created>2014-06-20</created><authors><author><keyname>Rahimian</keyname><forenames>Ashkan</forenames></author><author><keyname>Ziarati</keyname><forenames>Raha</forenames></author><author><keyname>Preda</keyname><forenames>Stere</forenames></author><author><keyname>Debbabi</keyname><forenames>Mourad</forenames></author></authors><title>On the Reverse Engineering of the Citadel Botnet</title><categories>cs.CR cs.NI cs.OS cs.SE</categories><comments>10 pages, 17 figures. This is an updated / edited version of a paper
  appeared in FPS 2013</comments><acm-class>D.4.6; K.6.5; E.3; D.2.7</acm-class><journal-ref>LNCS 8352, 2014, pp 408-425</journal-ref><doi>10.1007/978-3-319-05302-8_25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citadel is an advanced information-stealing malware which targets financial
information. This malware poses a real threat against the confidentiality and
integrity of personal and business data. A joint operation was recently
conducted by the FBI and the Microsoft Digital Crimes Unit in order to take
down Citadel command-and-control servers. The operation caused some disruption
in the botnet but has not stopped it completely. Due to the complex structure
and advanced anti-reverse engineering techniques, the Citadel malware analysis
process is both challenging and time-consuming. This allows cyber criminals to
carry on with their attacks while the analysis is still in progress. In this
paper, we present the results of the Citadel reverse engineering and provide
additional insight into the functionality, inner workings, and open source
components of the malware. In order to accelerate the reverse engineering
process, we propose a clone-based analysis methodology. Citadel is an offspring
of a previously analyzed malware called Zeus; thus, using the former as a
reference, we can measure and quantify the similarities and differences of the
new variant. Two types of code analysis techniques are provided in the
methodology, namely assembly to source code matching and binary clone
detection. The methodology can help reduce the number of functions requiring
manual analysis. The analysis results prove that the approach is promising in
Citadel malware analysis. Furthermore, the same approach is applicable to
similar malware analysis scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5572</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5572</id><created>2014-06-20</created><authors><author><keyname>Tosch</keyname><forenames>Emma</forenames></author><author><keyname>Berger</keyname><forenames>Emery D.</forenames></author></authors><title>SurveyMan: Programming and Automatically Debugging Surveys</title><categories>cs.PL cs.HC</categories><comments>Submitted version; accepted to OOPSLA 2014</comments><acm-class>D.3.2; J.4; J.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surveys can be viewed as programs, complete with logic, control flow, and
bugs. Word choice or the order in which questions are asked can unintentionally
bias responses. Vague, confusing, or intrusive questions can cause respondents
to abandon a survey. Surveys can also have runtime errors: inattentive
respondents can taint results. This effect is especially problematic when
deploying surveys in uncontrolled settings, such as on the web or via
crowdsourcing platforms. Because the results of surveys drive business
decisions and inform scientific conclusions, it is crucial to make sure they
are correct.
  We present SurveyMan, a system for designing, deploying, and automatically
debugging surveys. Survey authors write their surveys in a lightweight
domain-specific language aimed at end users. SurveyMan statically analyzes the
survey to provide feedback to survey authors before deployment. It then
compiles the survey into JavaScript and deploys it either to the web or a
crowdsourcing platform. SurveyMan's dynamic analyses automatically find survey
bugs, and control for the quality of responses. We evaluate SurveyMan's
algorithms analytically and empirically, demonstrating its effectiveness with
case studies of social science surveys conducted via Amazon's Mechanical Turk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5581</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5581</id><created>2014-06-21</created><updated>2015-07-17</updated><authors><author><keyname>Nguyen</keyname><forenames>Anh</forenames></author><author><keyname>Banic</keyname><forenames>Amy</forenames></author></authors><title>3DTouch: A wearable 3D input device with an optical sensor and a 9-DOF
  inertial measurement unit</title><categories>cs.HC cs.MM</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present 3DTouch, a novel 3D wearable input device worn on the fingertip
for 3D manipulation tasks. 3DTouch is designed to fill the missing gap of a 3D
input device that is self-contained, mobile, and universally working across
various 3D platforms. This paper presents a low-cost solution to designing and
implementing such a device. Our approach relies on relative positioning
technique using an optical laser sensor and a 9-DOF inertial measurement unit.
  3DTouch is self-contained, and designed to universally work on various 3D
platforms. The device employs touch input for the benefits of passive haptic
feedback, and movement stability. On the other hand, with touch interaction,
3DTouch is conceptually less fatiguing to use over many hours than 3D spatial
input devices. We propose a set of 3D interaction techniques including
selection, translation, and rotation using 3DTouch. An evaluation also
demonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for
subtle touch interaction in 3D space. Modular solutions like 3DTouch opens up a
whole new design space for interaction techniques to further develop on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5582</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5582</id><created>2014-06-21</created><updated>2015-04-25</updated><authors><author><keyname>Gulbahar</keyname><forenames>Burhan</forenames></author></authors><title>Optimal Offline Packet Scheduling in Energy Harvesting 2-user Multiple
  Access Channel with Common Data</title><categories>cs.IT math.IT</categories><comments>29 pages, 11 figures, 5 Tables; All the contents of the paper
  (including the title and abstract) reorganized and clarified, converted to
  one-column version. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lifetime and the sustainability of the wireless sensor networks (WSNs)
can be increased with energy harvesting transmitters utilizing optimum packet
scheduling. On the other hand, WSNs are observed to collect spatially or
temporally correlated data which should be taken into account for the optimum
packet scheduling in an energy harvesting system. However, the solutions
available for 2-user multiple-access channel (MAC) systems with energy
harvesting transmitters do not consider the common data or the correlation
among the data. In this paper, optimal packet scheduling for energy harvesting
2-user Gaussian MAC with common data is achieved by assuming deterministic
knowledge of the data and energy packets, i.e., offline solution. The optimum
departure region is found by using Karush- Kuhn-Tucker (KKT) conditions
generalizing the solutions obtained for the MAC without common data. An
efficient iterative backward water-filling algorithm is defined. The optimum
solution is numerically compared with the case of no scheduling, uniform power
scheduling and the previous solutions defined for the MAC without common data
by showing the improvement obtained with the optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5588</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5588</id><created>2014-06-21</created><authors><author><keyname>Sassi</keyname><forenames>Aymen</forenames></author><author><keyname>Charfi</keyname><forenames>Faiza</forenames></author><author><keyname>Kamoun</keyname><forenames>Lotfi</forenames></author><author><keyname>Elhillali</keyname><forenames>Yassin</forenames></author><author><keyname>Rivenq</keyname><forenames>Atika</forenames></author></authors><title>A Symbol-Based Estimation Technique for Inter-vehicular Communication
  Performance Optimization</title><categories>cs.NI</categories><comments>8 pages, 15 figures, IJCSI International Journal of Computer Science
  Issues, Vol. 10, Issue 2, No 3, March 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to enhance the quality of Orthogonal Frequency
Division Multiplexing OFDM estimation in dedicated vehicular communication
transmission V2X networks. Wireless Access in Vehicular Environment WAVE as
also known IEEE 802.11p represents the standard for these networks. Developing
a reliable inter-vehicular V2X communication has to focus on optimizing its
real performances. In this work, we studied the fact that WAVE transmission
uses the channel characteristics designed for indoor and stationary
communication terminals in IEEE 802.11a. In this paper, we propose an approach
to overcome this mobility problem of terminal communication. The considered
solution consists in using pilot estimation technique to reduce the high bit
error rate. First, we highlight the impact of rearranging the pilot symbol
positions on the quality of transmission QoT. Second, we try to overcome one of
the PHY layer estimation constraints by adding two new pilot symbols. By
considering pilot symbol aided channel estimation at the transmitter, we focus
on Least Square LS and Minimum Mean Square Error MMSE channel estimation on the
receiver. A range of simulations is carried out according to ratio between the
Bit Error Rate BER and the Signal to Noise Ratio SNR. We demonstrate that
rearranging pilot pattern can offer better results than standardized ones.
Furthermore, we prove that adding pilots symbols can provide the best
performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5596</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5596</id><created>2014-06-21</created><authors><author><keyname>Binu</keyname><forenames>V P</forenames></author><author><keyname>Sreekumar</keyname><forenames>A</forenames></author></authors><title>An Epitome of Multi Secret Sharing Schemes for General Access Structure</title><categories>cs.CR</categories><journal-ref>International Journal of Information Processing, 8(2), 13-28, 2014
  ISSN : 0973-8215</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secret sharing schemes are widely used now a days in various applications,
which need more security, trust and reliability. In secret sharing scheme, the
secret is divided among the participants and only authorized set of
participants can recover the secret by combining their shares. The authorized
set of participants are called access structure of the scheme. In Multi-Secret
Sharing Scheme (MSSS), k different secrets are distributed among the
participants, each one according to an access structure. Multi-secret sharing
schemes have been studied extensively by the cryptographic community. Number of
schemes are proposed for the threshold multi-secret sharing and multi-secret
sharing according to generalized access structure with various features. In
this survey we explore the important constructions of multi-secret sharing for
the generalized access structure with their merits and demerits. The features
like whether shares can be reused, participants can be enrolled or dis-enrolled
efficiently, whether shares have to modified in the renewal phase etc., are
considered for the evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5597</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5597</id><created>2014-06-21</created><authors><author><keyname>Chatterjee</keyname><forenames>A. G.</forenames></author><author><keyname>Verma</keyname><forenames>M. K.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>M.</forenames></author></authors><title>Transpose-free Fast Fourier Transform for Turbulence Simulation</title><categories>cs.MS cs.CE cs.DS physics.comp-ph physics.flu-dyn</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pseudo-spectral method is one of the most accurate techniques for simulating
turbulent flows. Fast Fourier transform (FFT) is an integral part of this
method. In this paper, we present a new procedure to compute FFT in which we
save operations during interprocess communications by avoiding transpose of the
array. As a result, our transpose-free FFT is 15\% to 20\% faster than FFTW.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5598</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5598</id><created>2014-06-21</created><authors><author><keyname>Prasad</keyname><forenames>Reshma</forenames></author><author><keyname>Sebastian</keyname><forenames>Mary Priya</forenames></author></authors><title>A survey on phrase structure learning methods for text classification</title><categories>cs.CL</categories><comments>14 pages, 2 figures, 2 tables, International Journal on Natural
  Language Computing (IJNLC) Vol. 3, No.2, April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text classification is a task of automatic classification of text into one of
the predefined categories. The problem of text classification has been widely
studied in different communities like natural language processing, data mining
and information retrieval. Text classification is an important constituent in
many information management tasks like topic identification, spam filtering,
email routing, language identification, genre classification, readability
assessment etc. The performance of text classification improves notably when
phrase patterns are used. The use of phrase patterns helps in capturing
non-local behaviours and thus helps in the improvement of text classification
task. Phrase structure extraction is the first step to continue with the phrase
pattern identification. In this survey, detailed study of phrase structure
learning methods have been carried out. This will enable future work in several
NLP tasks, which uses syntactic information from phrase structure like grammar
checkers, question answering, information extraction, machine translation, text
classification. The paper also provides different levels of classification and
detailed comparison of the phrase structure learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5600</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5600</id><created>2014-06-21</created><authors><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author><author><keyname>Petej</keyname><forenames>Ivan</forenames></author><author><keyname>Fedorova</keyname><forenames>Valentina</forenames></author></authors><title>From conformal to probabilistic prediction</title><categories>cs.LG</categories><comments>12 pages, 2 tables</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new method of probabilistic prediction, which is based
on conformal prediction. The method is applied to the standard USPS data set
and gives encouraging results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5614</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5614</id><created>2014-06-21</created><authors><author><keyname>Sun</keyname><forenames>Shiliang</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author></authors><title>PAC-Bayes Analysis of Multi-view Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents four PAC-Bayes bounds to analyse the generalisation
performance of multi-view classifiers. These bounds adopt data dependent
Gaussian priors which emphasize the classifiers with high view agreements. The
centre of the prior for the first two bounds is the origin, while the centre of
the prior for the last two bounds is given by a data dependent vector. Another
important ingredient to obtain these bounds is two derived logarithmic
determinant inequalities whose difference lies at whether the dimensionality of
data is involved. We evaluate the multi-view PAC-Bayes bounds on benchmark data
with preliminary experimental results indicating their usefulness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5616</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5616</id><created>2014-06-21</created><authors><author><keyname>Roul</keyname><forenames>R. K.</forenames></author><author><keyname>Sahay</keyname><forenames>S. K.</forenames></author></authors><title>An Effective Approach for Web Document Classification using the Concept
  of Association Analysis of Data Mining</title><categories>cs.IR</categories><comments>9 Pages</comments><journal-ref>IJCSET, 2012, Vol. 3, No. 10, p. 483</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exponential growth of the web increased the importance of web document
classification and data mining. To get the exact information, in the form of
knowing what classes a web document belongs to, is expensive. Automatic
classification of web document is of great use to search engines which provides
this information at a low cost. In this paper, we propose an approach for
classifying the web document using the frequent item word sets generated by the
Frequent Pattern (FP) Growth which is an association analysis technique of data
mining. These set of associated words act as feature set. The final
classification obtained after Na\&quot;ive Bayes classifier used on the feature set.
For the experimental work, we use Gensim package, as it is simple and robust.
Results show that our approach can be effectively classifying the web document.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5617</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5617</id><created>2014-06-21</created><authors><author><keyname>Roul</keyname><forenames>R. K.</forenames></author><author><keyname>Devanand</keyname><forenames>O. R.</forenames></author><author><keyname>Sahay</keyname><forenames>S. K.</forenames></author></authors><title>Web Document Clustering and Ranking using Tf-Idf based Apriori Approach</title><categories>cs.IR</categories><comments>5 Pages</comments><journal-ref>IJCA Proceedings on ICACEA, No. 2, p. 34 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamic web has increased exponentially over the past few years with more
than thousands of documents related to a subject available to the user now.
Most of the web documents are unstructured and not in an organized manner and
hence user facing more difficult to find relevant documents. A more useful and
efficient mechanism is combining clustering with ranking, where clustering can
group the similar documents in one place and ranking can be applied to each
cluster for viewing the top documents at the beginning.. Besides the particular
clustering algorithm, the different term weighting functions applied to the
selected features to represent web document is a main aspect in clustering
task. Keeping this approach in mind, here we proposed a new mechanism called
Tf-Idf based Apriori for clustering the web documents. We then rank the
documents in each cluster using Tf-Idf and similarity factor of documents based
on the user query. This approach will helps the user to get all his relevant
documents in one place and can restrict his search to some top documents of his
choice. For experimental purpose, we have taken the Classic3 and Classic4
datasets of Cornell University having more than 10,000 documents and use gensim
toolkit to carry out our work. We have compared our approach with traditional
apriori algorithm and found that our approach is giving better results for
higher minimum support. Our ranking mechanism is also giving a good F-measure
of 78%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5622</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5622</id><created>2014-06-21</created><authors><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author></authors><title>Gain-scheduled synchronization of parameter varying systems via relative
  $H_\infty$ consensus with application to synchronization of uncertain
  bilinear systems</title><categories>cs.SY</categories><comments>Accepted for publication in Automatica</comments><journal-ref>Automatica, 50(11):2880-2887, 2014</journal-ref><doi>10.1016/j.automatica.2014.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers a problem of consensus-based synchronization of uncertain
parameter varying multi-agent systems. We present a method for constructing
consensus-based synchronization protocol schedules for each agent to ensure it
synchronizes with a reference parameter-varying system. The proposed protocol
guarantees a specified level of $H_\infty$ transient consensus between the
agents. The algorithm uses a consensus-preserving interpolation and produces
continuous (in the scheduling parameter) coefficients for the protocol. An
application to synchronization of uncertain bilinear systems is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5633</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5633</id><created>2014-06-21</created><authors><author><keyname>Nugent</keyname><forenames>M. Alexander</forenames></author><author><keyname>Molter</keyname><forenames>Timothy W.</forenames></author></authors><title>Thermodynamic-RAM Technology Stack</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a technology stack or specification describing the multiple
levels of abstraction and specialization needed to implement a neuromorphic
processor based on the theory of AHaH Computing. This specific implementation
is called Thermodynamic-RAM (kT-RAM). Bringing us closer to brain-like neural
computation, kT-RAM will provide a general-purpose adaptive hardware resource
to existing computing platforms enabling fast and low-power machine learning
capabilities that are currently hampered by the separation of memory and
processing. The motivation for defining the technology stack is two-fold.
First, explaining kT-RAM is much easier if it is broken down into smaller, more
manageable pieces. Secondly, groups interested in realizing kT-RAM can choose a
level to contribute to that matches their interest and expertise. The levels of
the Thermodynamic-RAM technology stack include the memristor, Knowm-Synapse,
AHaH Node, kT-RAM, kT-RAM instruction set, sparse spike encoding, kT-RAM
emulator, and SENSE Server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5634</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5634</id><created>2014-06-21</created><authors><author><keyname>Qazi</keyname><forenames>Zafar Ayyub</forenames></author><author><keyname>Sekar</keyname><forenames>Vyas</forenames></author><author><keyname>Das</keyname><forenames>Samir</forenames></author></authors><title>A Framework to Quantify the Benefits of Network Functions Virtualization
  in Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network functions virtualization (NFV) is an appealing vision that promises
to dramatically reduce capital and operating expenses for cellular providers.
However, existing efforts in this space leave open broad issues about how NFV
deployments should be instantiated or how they should be provisioned. In this
paper, we present an initial attempt at a framework that will help network
operators systematically evaluate the potential benefits that different points
in the NFV design space can offer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5647</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5647</id><created>2014-06-21</created><updated>2014-12-11</updated><authors><author><keyname>Amini</keyname><forenames>Arash A.</forenames></author><author><keyname>Levina</keyname><forenames>Elizaveta</forenames></author></authors><title>On semidefinite relaxations for the block model</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic block model (SBM) is a popular tool for community detection in
networks, but fitting it by maximum likelihood (MLE) involves an infeasible
optimization problem. We propose a new semi-definite programming (SDP) solution
to the problem of fitting the SBM, derived as a relaxation of the MLE. Our
relaxation, which we call SDP-1, is tighter than other recently proposed SDP
relaxations, namely what we call SDP-2 and SDP-3, and thus previously
established theoretical guarantees carry over. However, we show that SDP-1 is,
in fact, strongly consistent (i.e., exactly recovers true communities) over a
wider class of SBMs than what current results suggest. In particular, one can
relax the assumption of strong assortativity, implicit in consistency
conditions of current SDPs, to that of (weak) assortativity for SDP-1, thus,
significantly broadening the class of applicable models. Our approach in
deriving strong consistency results is based on a primal-dual witness
construction, and as a by-product we recover current results for SDP-2. Our
approach also suggests that strong assortativity is necessary for the success
of SDP-2 and SDP-3 and is not an artifact of the current proofs. We provide
empirical evidence of this conjecture, in addition to other numerical results
comparing these SDPs, and adjacency-based spectral clustering, on real and
synthetic data. Another feature of our relaxation is the tendency to produce
more balanced (i.e., equal-sized) communities which, as we show with a
real-data example, makes it the ideal tool for fitting network histograms, a
concept gaining popularity in the graphon estimation literature. A general
theme throughout will be to view all these SDPs within a unified framework,
specifically, as relaxations of the MLE over various sub-classes of the SBM.
This also leads to a connection with the well-known problem of sparse PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5653</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5653</id><created>2014-06-21</created><authors><author><keyname>Anirudh</keyname><forenames>Rushil</forenames></author><author><keyname>Turaga</keyname><forenames>Pavan</forenames></author></authors><title>Interactively Test Driving an Object Detector: Estimating Performance on
  Unlabeled Data</title><categories>cs.CV</categories><comments>Published at Winter Conference on Applications of Computer Vision,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of `test-driving' a detector, i.e.
allowing a human user to get a quick sense of how well the detector generalizes
to their specific requirement. To this end, we present the first system that
estimates detector performance interactively without extensive ground truthing
using a human in the loop. We approach this as a problem of estimating
proportions and show that it is possible to make accurate inferences on the
proportion of classes or groups within a large data collection by observing
only $5-10\%$ of samples from the data. In estimating the false detections (for
precision), the samples are chosen carefully such that the overall
characteristics of the data collection are preserved. Next, inspired by its use
in estimating disease propagation we apply pooled testing approaches to
estimate missed detections (for recall) from the dataset. The estimates thus
obtained are close to the ones obtained using ground truth, thus reducing the
need for extensive labeling which is expensive and time consuming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5665</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5665</id><created>2014-06-21</created><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Aravindan</forenames></author></authors><title>Constant Factor Approximation for Balanced Cut in the PIE model</title><categories>cs.DS cs.LG</categories><comments>Full version of the paper at the 46th ACM Symposium on the Theory of
  Computing (STOC 2014). 32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and study a new semi-random semi-adversarial model for Balanced
Cut, a planted model with permutation-invariant random edges (PIE). Our model
is much more general than planted models considered previously. Consider a set
of vertices V partitioned into two clusters $L$ and $R$ of equal size. Let $G$
be an arbitrary graph on $V$ with no edges between $L$ and $R$. Let
$E_{random}$ be a set of edges sampled from an arbitrary permutation-invariant
distribution (a distribution that is invariant under permutation of vertices in
$L$ and in $R$). Then we say that $G + E_{random}$ is a graph with
permutation-invariant random edges.
  We present an approximation algorithm for the Balanced Cut problem that finds
a balanced cut of cost $O(|E_{random}|) + n \text{polylog}(n)$ in this model.
In the regime when $|E_{random}| = \Omega(n \text{polylog}(n))$, this is a
constant factor approximation with respect to the cost of the planted cut.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5667</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5667</id><created>2014-06-21</created><updated>2015-05-12</updated><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Aravindan</forenames></author></authors><title>Correlation Clustering with Noisy Partial Information</title><categories>cs.DS cs.LG</categories><comments>To appear at Conference on Learning Theory (COLT) 2015. Substantial
  changes from previous version, including a new section on recovery of the
  ground truth clustering. 20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and study a semi-random model for the Correlation
Clustering problem on arbitrary graphs G. We give two approximation algorithms
for Correlation Clustering instances from this model. The first algorithm finds
a solution of value $(1+ \delta) optcost + O_{\delta}(n\log^3 n)$ with high
probability, where $optcost$ is the value of the optimal solution (for every
$\delta &gt; 0$). The second algorithm finds the ground truth clustering with an
arbitrarily small classification error $\eta$ (under some additional
assumptions on the instance).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5670</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5670</id><created>2014-06-21</created><updated>2015-04-15</updated><authors><author><keyname>Wu</keyname><forenames>Zhirong</forenames></author><author><keyname>Song</keyname><forenames>Shuran</forenames></author><author><keyname>Khosla</keyname><forenames>Aditya</forenames></author><author><keyname>Yu</keyname><forenames>Fisher</forenames></author><author><keyname>Zhang</keyname><forenames>Linguang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author></authors><title>3D ShapeNets: A Deep Representation for Volumetric Shapes</title><categories>cs.CV</categories><comments>to be appeared in CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D shape is a crucial but heavily underutilized cue in today's computer
vision systems, mostly due to the lack of a good generic shape representation.
With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft
Kinect), it is becoming increasingly important to have a powerful 3D shape
representation in the loop. Apart from category recognition, recovering full 3D
shapes from view-based 2.5D depth maps is also a critical part of visual
understanding. To this end, we propose to represent a geometric 3D shape as a
probability distribution of binary variables on a 3D voxel grid, using a
Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the
distribution of complex 3D shapes across different object categories and
arbitrary poses from raw CAD data, and discovers hierarchical compositional
part representations automatically. It naturally supports joint object
recognition and shape completion from 2.5D depth maps, and it enables active
object recognition through view planning. To train our 3D deep learning model,
we construct ModelNet -- a large-scale 3D CAD model dataset. Extensive
experiments show that our 3D deep representation enables significant
performance improvement over the-state-of-the-arts in a variety of tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5675</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5675</id><created>2014-06-22</created><updated>2015-12-20</updated><authors><author><keyname>Wang</keyname><forenames>Shusen</forenames></author><author><keyname>Luo</keyname><forenames>Luo</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>SPSD Matrix Approximation vis Column Selection: Theories, Algorithms,
  and Extensions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetric positive semidefinite (SPSD) matrix approximation is an important
problem with applications in kernel methods. However, existing SPSD matrix
approximation methods such as the Nystr\&quot;om method only have weak error bounds.
In this paper we conduct in-depth studies of an SPSD matrix approximation model
and establish strong relative-error bounds. We call it the prototype model for
it has more efficient and effective extensions, and some of its extensions have
high scalability. Though the prototype model itself is not suitable for
large-scale data, it is still useful to study its properties, on which the
analysis of its extensions relies.
  This paper offers novel theoretical analysis, efficient algorithms, and a
highly accurate extension. First, we establish a lower error bound for the
prototype model, and we improve the error bound of an existing column selection
algorithm to match the lower bound. In this way, we obtain the first optimal
column selection algorithm for the prototype model. We also prove that the
prototype model is exact under certain conditions. Second, we develop a simple
column selection algorithm with a provable error bound. Third, we propose a
so-called spectral shifting model to make the approximation more accurate when
the eigenvalues of the matrix decays slowly, and the improvement is
theoretically quantified. The spectral shifting method can also be applied to
improve other SPSD matrix approximation models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5676</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5676</id><created>2014-06-22</created><authors><author><keyname>Xu</keyname><forenames>Xiangxiang</forenames></author><author><keyname>Zhang</keyname><forenames>Xiujun</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Zhao</keyname><forenames>Yifei</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author></authors><title>How to Upgrade Wireless Networks: Small Cells or Massive MIMO?</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 3 figures, The paper has been accepted by PIMRC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio network deployment and coverage optimization are critical to
next-generation wireless networks. In this paper, the problem of optimally
deciding on whether to install additional small cells or to upgrade current
macrocell base stations (BSs) with massive antenna arrays is studied. This
integrated deployment problem is cast as a general integer optimization model
by using the facility location framework. The capacity limits of both the radio
access link and the backhaul link are considered. The problem is shown to be an
extension of the modular capacitated location problem (MCLP) which is known to
be NP-hard. To solve the problem, a novel deployment algorithm that uses
Lagrangian relaxation and tabu local search is proposed. The developed tabu
search is shown to have a two-level structure and to be able to search the
solution space thoroughly. Simulation results show how the proposed, optimal
approach to upgrading an existing wireless network infrastructure can make use
of a combination of both small cells and BSs with massive antennas. The results
also show that the proposed algorithm can find the optimal solution effectively
while having a computational time that is up to 30% lower than that of
conventional algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5679</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5679</id><created>2014-06-22</created><authors><author><keyname>Karpathy</keyname><forenames>Andrej</forenames></author><author><keyname>Joulin</keyname><forenames>Armand</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</title><categories>cs.CV cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model for bidirectional retrieval of images and sentences
through a multi-modal embedding of visual and natural language data. Unlike
previous models that directly map images or sentences into a common embedding
space, our model works on a finer level and embeds fragments of images
(objects) and fragments of sentences (typed dependency tree relations) into a
common space. In addition to a ranking objective seen in previous work, this
allows us to add a new fragment alignment objective that learns to directly
associate these fragments across modalities. Extensive experimental evaluation
shows that reasoning on both the global level of images and sentences and the
finer level of their respective fragments significantly improves performance on
image-sentence retrieval tasks. Additionally, our model provides interpretable
predictions since the inferred inter-modal fragment alignment is explicit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5685</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5685</id><created>2014-06-22</created><authors><author><keyname>Modenini</keyname><forenames>Andrea</forenames></author></authors><title>Advanced transceivers for spectrally-efficient communications</title><categories>cs.IT math.IT</categories><comments>PhD Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, we will consider techniques to improve the spectral
efficiency of digital communication systems, operating on the whole transceiver
scheme. First, we will focus on receiver schemes having detection algorithms
with a complexity constraint. We will optimize the parameters of the reduced
detector with the aim of maximizing the achievable information rate. Namely, we
will adopt the channel shortening technique. Then, we will focus on a technique
that is getting very popular in the last years (although presented for the
first time in 1975): faster-than-Nyquist signaling, and its extension which is
time packing. Time packing is a very simple technique that consists in
introducing intersymbol interference on purpose with the aim of increasing the
spectral efficiency of finite order constellations. Finally, in the last
chapters we will combine all the presented techniques, and we will consider
their application to satellite channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5687</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5687</id><created>2014-06-22</created><authors><author><keyname>Arifuzzaman</keyname><forenames>Shaikh</forenames></author><author><keyname>Khan</keyname><forenames>Maleq</forenames></author><author><keyname>Marathe</keyname><forenames>Madhav</forenames></author></authors><title>Parallel Algorithms for Counting Triangles in Networks with Large
  Degrees</title><categories>cs.DC cs.DS cs.SI</categories><acm-class>D.1.3; G.2.2; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the number of triangles in a network is an important problem in the
analysis of complex networks. The number of triangles also has important
applications in data mining. Existing distributed memory parallel algorithms
for counting triangles are either Map-Reduce based or message passing interface
(MPI) based and work with overlapping partitions of the given network. These
algorithms are designed for very sparse networks and do not work well when the
degrees of the nodes are relatively larger. For networks with larger degrees,
Map-Reduce based algorithm generates prohibitively large intermediate data, and
in MPI based algorithms with overlapping partitions, each partition can grow as
large as the original network, wiping out the benefit of partitioning the
network.
  In this paper, we present two efficient MPI-based parallel algorithms for
counting triangles in massive networks with large degrees. The first algorithm
is a space-efficient algorithm for networks that do not fit in the main memory
of a single compute node. This algorithm divides the network into
non-overlapping partitions. The second algorithm is for the case where the main
memory of each node is large enough to contain the entire network. We observe
that for such a case, computation load can be balanced dynamically and present
a dynamic load balancing scheme which improves the performance significantly.
Both of our algorithms scale well to large networks and to a large number of
processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5688</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5688</id><created>2014-06-22</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Information, Meaning, and Intellectual Organization in Networks of
  Inter-Human Communication</title><categories>cs.DL cs.CY</categories><comments>Forthcoming in: Theories of Informetrics: A Festschrift in Honor of
  Blaise Cronin, Cassidy R. Sugimoto (Ed.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shannon-Weaver model of linear information transmission is extended with
two loops potentially generating redundancies: (i) meaning is provided locally
to the information from the perspective of hindsight, and (ii) meanings can be
codified differently and then refer to other horizons of meaning. Thus, three
layers are distinguished: variations in the communications, historical
organization at each moment of time, and evolutionary self-organization of the
codes of communication over time. Furthermore, the codes of communication can
functionally be different and then the system is both horizontally and
vertically differentiated. All these subdynamics operate in parallel and
necessarily generate uncertainty. However, meaningful information can be
considered as the specific selection of a signal from the noise; the codes of
communication are social constructs that can generate redundancy by giving
different meanings to the same information. Reflexively, one can translate
among codes in more elaborate discourses. The second (instantiating) layer can
be operationalized in terms of semantic maps using the vector space model; the
third in terms of mutual redundancy among the latent dimensions of the vector
space. Using Blaise Cronin's {\oe}uvre, the different operations of the three
layers are demonstrated empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5690</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5690</id><created>2014-06-22</created><authors><author><keyname>Gupta</keyname><forenames>Sonali</forenames></author><author><keyname>Bhatia</keyname><forenames>Komal kumar</forenames></author><author><keyname>Manchanda</keyname><forenames>Pikakshi</forenames></author></authors><title>WebParF: A Web partitioning framework for Parallel Crawlers</title><categories>cs.IR</categories><comments>8pages, 7 figures, ISSN : 0975-3397 Vol.5 no.8, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the ever proliferating size and scale of the WWW [1] efficient ways of
exploring content are of increasing importance. How can we efficiently retrieve
information from it through crawling? And in this era of tera and multi-core
processors, we ought to think of multi-threaded processes as a serving
solution. So, even better how can we improve the crawling performance by using
parallel crawlers that work independently? The paper devotes to the fundamental
development in the field of parallel crawlers [4] highlighting the advantages
and challenges arising from its design. The paper also focuses on the aspect of
URL distribution among the various parallel crawling processes or threads and
ordering the URLs within each distributed set of URLs. How to distribute URLs
from the URL frontier to the various concurrently executing crawling process
threads is an orthogonal problem. The paper provides a solution to the problem
by designing a framework WebParF that partitions the URL frontier into a
several URL queues while considering the various design issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5691</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5691</id><created>2014-06-22</created><authors><author><keyname>Camilleri</keyname><forenames>John J.</forenames></author><author><keyname>Paganelli</keyname><forenames>Gabriele</forenames></author><author><keyname>Schneider</keyname><forenames>Gerardo</forenames></author></authors><title>A CNL for Contract-Oriented Diagrams</title><categories>cs.CL cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a first step towards a framework for defining and manipulating
normative documents or contracts described as Contract-Oriented (C-O) Diagrams.
These diagrams provide a visual representation for such texts, giving the
possibility to express a signatory's obligations, permissions and prohibitions,
with or without timing constraints, as well as the penalties resulting from the
non-fulfilment of a contract. This work presents a CNL for verbalising C-O
Diagrams, a web-based tool allowing editing in this CNL, and another for
visualising and manipulating the diagrams interactively. We then show how these
proof-of-concept tools can be used by applying them to a small example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5694</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5694</id><created>2014-06-22</created><updated>2015-11-11</updated><authors><author><keyname>Bentov</keyname><forenames>Iddo</forenames></author><author><keyname>Gabizon</keyname><forenames>Ariel</forenames></author><author><keyname>Mizrahi</keyname><forenames>Alex</forenames></author></authors><title>Cryptocurrencies without Proof of Work</title><categories>cs.CR</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study decentralized cryptocurrency protocols in which the participants do
not deplete physical scarce resources. Such protocols commonly rely on Proof of
Stake, i.e., on mechanisms that extend voting power to the stakeholders of the
system. We offer analysis of existing protocols that have a substantial amount
of popularity. We then present our novel pure Proof of Stake protocols, and
argue that they help in mitigating problems that the existing protocols
exhibit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5706</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5706</id><created>2014-06-22</created><updated>2014-09-21</updated><authors><author><keyname>Carli</keyname><forenames>Francesca Paola</forenames></author></authors><title>On the Maximum Entropy Property of the First-Order Stable Spline Kernel
  and its Implications</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>12 pages. In 2014 IEEE Multi-conference on Systems and Control. IEEE,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new nonparametric approach for system identification has been recently
proposed where the impulse response is seen as the realization of a zero--mean
Gaussian process whose covariance, the so--called stable spline kernel,
guarantees that the impulse response is almost surely stable. Maximum entropy
properties of the stable spline kernel have been pointed out in the literature.
In this paper we provide an independent proof that relies on the theory of
matrix extension problems in the graphical model literature and leads to a
closed form expression for the inverse of the first order stable spline kernel
as well as to a new factorization in the form $UWU^\top$ with $U$ upper
triangular and $W$ diagonal. Interestingly, all first--order stable spline
kernels share the same factor $U$ and $W$ admits a closed form representation
in terms of the kernel hyperparameter, making the factorization computationally
inexpensive. Maximum likelihood properties of the stable spline kernel are also
highlighted. These results can be applied both to improve the stability and to
reduce the computational complexity associated with the computation of stable
spline estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5708</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5708</id><created>2014-06-22</created><authors><author><keyname>Charafeddine</keyname><forenames>Hadil</forenames></author><author><keyname>El-Harake</keyname><forenames>Khalil</forenames></author><author><keyname>Falcone</keyname><forenames>Yli&#xe8;s</forenames></author><author><keyname>Jaber</keyname><forenames>Mohamad</forenames></author></authors><title>Runtime Enforcement for Component-Based Systems</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1109.5505 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Runtime enforcement is an increasingly popular and effective dynamic
validation technique aiming to ensure the correct runtime behavior (w.r.t. a
formal specification) of systems using a so-called enforcement monitor. In this
paper we introduce runtime enforcement of specifications on component-based
systems (CBS) modeled in the BIP (Behavior, Interaction and Priority)
framework. BIP is a powerful and expressive component-based framework for
formal construction of heterogeneous systems. However, because of BIP
expressiveness, it remains difficult to enforce at design-time complex
behavioral properties.
  First we propose a theoretical runtime enforcement framework for CBS where we
delineate a hierarchy of sets of enforceable properties (i.e., properties that
can be enforced) according to the number of observational steps a system is
allowed to deviate from the property (i.e., the notion of k-step
enforceability). To ensure the observational equivalence between the correct
executions of the initial system and the monitored system, we show that i) only
stutter-invariant properties should be enforced on CBS with our monitors, ii)
safety properties are 1-step enforceable. Given an abstract enforcement monitor
(as a finite-state machine) for some 1-step enforceable specification, we
formally instrument (at relevant locations) a given BIP system to integrate the
monitor. At runtime, the monitor observes and automatically avoids any error in
the behavior of the system w.r.t. the specification. Our approach is fully
implemented in an available tool that we used to i) avoid deadlock occurrences
on a dining philosophers benchmark, and ii) ensure the correct placement of
robots on a map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5710</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5710</id><created>2014-06-22</created><authors><author><keyname>Hanumantharaju</keyname><forenames>M. C</forenames></author><author><keyname>Ravishankar</keyname><forenames>M.</forenames></author><author><keyname>Rameshbabu</keyname><forenames>D. R</forenames></author></authors><title>Natural Color Image Enhancement based on Modified Multiscale Retinex
  Algorithm and Performance Evaluation usingWavelet Energy</title><categories>cs.CV</categories><comments>10 pages, 3 figures, Recent Advances in Intelligent Informatics
  Advances in Intelligent Systems and Computing Volume 235, 2014, pp 83-92</comments><msc-class>68U10</msc-class><acm-class>I.4.3</acm-class><doi>10.1007/978-3-319-01778-5_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new color image enhancement technique based on modified
MultiScale Retinex(MSR) algorithm and visual quality of the enhanced images are
evaluated using a new metric, namely, wavelet energy. The color image
enhancement is achieved by down sampling the value component of HSV color space
converted image into three scales (normal, medium and fine) following the
contrast stretching operation. These down sampled value components are enhanced
using the MSR algorithm. The value component is reconstructed by averaging each
pixels of the lower scale image with that of the upper scale image subsequent
to up sampling the lower scale image. This process replaces dark pixel by the
average pixels of both the lower scale and upper scale, while retaining the
bright pixels. The quality of the reconstructed images in the proposed method
is found to be good and far better then the other researchers method. The
performance of the proposed scheme is evaluated using new wavelet domain based
assessment criterion, referred as wavelet energy. This scheme computes the
energy of both original and enhanced image in wavelet domain. The number of
edge details as well as wavelet energy is less in a poor quality image compared
with naturally enhanced image. Experimental results presented confirms that the
proposed wavelet energy based color image quality assessment technique
efficiently characterizes both the local and global details of enhanced image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5715</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5715</id><created>2014-06-22</created><updated>2014-06-24</updated><authors><author><keyname>Kaiser</keyname><forenames>Alexander</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Wahl</keyname><forenames>Thomas</forenames></author></authors><title>Lost in Abstraction: Monotonicity in Multi-Threaded Programs (Extended
  Technical Report)</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monotonicity in concurrent systems stipulates that, in any global state,
extant system actions remain executable when new processes are added to the
state. This concept is not only natural and common in multi-threaded software,
but also useful: if every thread's memory is finite, monotonicity often
guarantees the decidability of safety property verification even when the
number of running threads is unknown. In this paper, we show that the act of
obtaining finite-data thread abstractions for model checking can be at odds
with monotonicity: Predicate-abstracting certain widely used monotone software
results in non-monotone multi-threaded Boolean programs - the monotonicity is
lost in the abstraction. As a result, well-established sound and complete
safety checking algorithms become inapplicable; in fact, safety checking turns
out to be undecidable for the obtained class of unbounded-thread Boolean
programs. We demonstrate how the abstract programs can be modified into
monotone ones, without affecting safety properties of the non-monotone
abstraction. This significantly improves earlier approaches of enforcing
monotonicity via overapproximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5720</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5720</id><created>2014-06-22</created><authors><author><keyname>Alderman</keyname><forenames>James</forenames></author><author><keyname>Cid</keyname><forenames>Carlos</forenames></author><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Janson</keyname><forenames>Christian</forenames></author></authors><title>Publicly Verifiable Outsourced Computation with a Key Distribution
  Centre</title><categories>cs.CR</categories><comments>Conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The combination of software-as-a-service and the increasing use of mobile
devices gives rise to a considerable difference in computational power between
servers and clients. Thus, there is a desire for clients to outsource the
evaluation of complex functions to a server and to be able to verify that the
resulting value is correct. Previous work in this area of Publicly Verifiable
Outsourced Computation (PVC) requires a costly pre-processing stage. However,
in many practical situations multiple clients will be interested in the same
set of core functions and will make use of the same servers. Thus, the
pre-processing phase may be performed many more times than is necessary. In
this paper we introduce a Key Distribution Center (KDC) that handles the
generation and distribution of the keys that are required to support PVC,
thereby eliminating this redundancy. We define a number of new security models
and functionalities that arise with the introduction of the KDC, and present a
construction of such a scheme built upon Key-Policy Attribute-based Encryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5722</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5722</id><created>2014-06-22</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>The price of fairness for a small number of indivisible items</title><categories>cs.GT</categories><comments>5 pages</comments><msc-class>91B32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating fairness criteria in optimization problems comes at a certain
cost, which is measured by the so-called price of fairness. Here we consider
the allocation of indivisible goods. For envy-freeness as fairness criterion it
is known from literature that the price of fairness can increase linearly in
terms of the number of agents. For the constructive lower bound a quadratic
number of items was used. In practice this might be inadequately large. So we
introduce the price of fairness in terms of both the number of agents and
items, i.e., key parameters which generally may be considered as common and
available knowledge. It turned out that the price of fairness increases
sublinear if the number of items is not too much larger than the number of
agents. For the special case of coincide of both counts exact asymptotics could
be determined. Additionally an efficient integer programming formulation is
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5726</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5726</id><created>2014-06-22</created><updated>2014-07-09</updated><authors><author><keyname>Wei</keyname><forenames>Yunchao</forenames></author><author><keyname>Xia</keyname><forenames>Wei</forenames></author><author><keyname>Huang</keyname><forenames>Junshi</forenames></author><author><keyname>Ni</keyname><forenames>Bingbing</forenames></author><author><keyname>Dong</keyname><forenames>Jian</forenames></author><author><keyname>Zhao</keyname><forenames>Yao</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>CNN: Single-label to Multi-label</title><categories>cs.CV</categories><comments>13 pages, 10 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Network (CNN) has demonstrated promising performance in
single-label image classification tasks. However, how CNN best copes with
multi-label images still remains an open problem, mainly due to the complex
underlying object layouts and insufficient multi-label training images. In this
work, we propose a flexible deep CNN infrastructure, called
Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment
hypotheses are taken as the inputs, then a shared CNN is connected with each
hypothesis, and finally the CNN output results from different hypotheses are
aggregated with max pooling to produce the ultimate multi-label predictions.
Some unique characteristics of this flexible deep CNN infrastructure include:
1) no ground truth bounding box information is required for training; 2) the
whole HCP infrastructure is robust to possibly noisy and/or redundant
hypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may
be well pre-trained with a large-scale single-label image dataset, e.g.
ImageNet; and 5) it may naturally output multi-label prediction results.
Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasets
well demonstrate the superiority of the proposed HCP infrastructure over other
state-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3%
after the fusion with our complementary result in [47] based on hand-crafted
features on the VOC2012 dataset, which significantly outperforms the
state-of-the-arts with a large margin of more than 7%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5731</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5731</id><created>2014-06-22</created><authors><author><keyname>Vijay</keyname><forenames>Deepa</forenames></author><author><keyname>Ganapathy</keyname><forenames>Gopinath</forenames></author></authors><title>Guidelines to minimize cost of software quality in agile scrum process</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a case study of Agile Scrum process followed in Retail
Domain project. This paper also reveals the impacts of Cost of Software
Quality, when agile scrum process is not followed efficiently. While analyzing
the case study, the gaps were found and guidelines for process improvements
were also suggested in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5732</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5732</id><created>2014-06-22</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author><author><keyname>Liang</keyname><forenames>Ying-Chang</forenames></author></authors><title>Secrecy Outage and Diversity Analysis of Cognitive Radio Systems</title><categories>cs.IT math.IT</categories><comments>16 pages, 5 figures, accepted to appear, IEEE Journal on Selected
  Areas in Communications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the physical-layer security of a multi-user
multi-eavesdropper cognitive radio system, which is composed of multiple
cognitive users (CUs) transmitting to a common cognitive base station (CBS),
while multiple eavesdroppers may collaborate with each other or perform
independently in intercepting the CUs-CBS transmissions, which are called the
coordinated and uncoordinated eavesdroppers, respectively. Considering multiple
CUs available, we propose the round-robin scheduling as well as the optimal and
suboptimal user scheduling schemes for improving the security of CUs-CBS
transmissions against eavesdropping attacks. Specifically, the optimal user
scheduling is designed by assuming that the channel state information (CSI) of
all links from CUs to CBS, to primary user (PU) and to eavesdroppers are
available. By contrast, the suboptimal user scheduling only requires the CSI of
CUs-CBS links without the PU's and eavesdroppers' CSI. We derive closed-form
expressions of the secrecy outage probability of these three scheduling schemes
in the presence of the coordinated and uncoordinated eavesdroppers. We also
carry out the secrecy diversity analysis and show that the round-robin
scheduling achieves the diversity order of only one, whereas the optimal and
suboptimal scheduling schemes obtain the full secrecy diversity, no matter
whether the eavesdroppers collaborate or not. In addition, numerical secrecy
outage results demonstrate that for both the coordinated and uncoordinated
eavesdroppers, the optimal user scheduling achieves the best security
performance and the round-robin scheduling performs the worst. Finally, upon
increasing the number of CUs, the secrecy outage probabilities of the optimal
and suboptimal user scheduling schemes both improve significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5736</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5736</id><created>2014-06-22</created><authors><author><keyname>Ding</keyname><forenames>Chao</forenames></author><author><keyname>Qi</keyname><forenames>Hou-Duo</forenames></author></authors><title>Convex Optimization Learning of Faithful Euclidean Distance
  Representations in Nonlinear Dimensionality Reduction</title><categories>stat.ML cs.LG math.OC</categories><comments>44 pages, 10 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical multidimensional scaling only works well when the noisy distances
observed in a high dimensional space can be faithfully represented by Euclidean
distances in a low dimensional space. Advanced models such as Maximum Variance
Unfolding (MVU) and Minimum Volume Embedding (MVE) use Semi-Definite
Programming (SDP) to reconstruct such faithful representations. While those SDP
models are capable of producing high quality configuration numerically, they
suffer two major drawbacks. One is that there exist no theoretically guaranteed
bounds on the quality of the configuration. The other is that they are slow in
computation when the data points are beyond moderate size. In this paper, we
propose a convex optimization model of Euclidean distance matrices. We
establish a non-asymptotic error bound for the random graph model with
sub-Gaussian noise, and prove that our model produces a matrix estimator of
high accuracy when the order of the uniform sample size is roughly the degree
of freedom of a low-rank matrix up to a logarithmic factor. Our results
partially explain why MVU and MVE often work well. Moreover, we develop a fast
inexact accelerated proximal gradient method. Numerical experiments show that
the model can produce configurations of high quality on large data points that
the SDP approach would struggle to cope with.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5751</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5751</id><created>2014-06-22</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames><affiliation>MIT</affiliation></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames><affiliation>MIT</affiliation></author><author><keyname>Michaleas</keyname><forenames>Pete</forenames><affiliation>MIT</affiliation></author><author><keyname>Schear</keyname><forenames>Nabil</forenames><affiliation>MIT</affiliation></author><author><keyname>Varia</keyname><forenames>Mayank</forenames><affiliation>MIT</affiliation></author><author><keyname>Yerukhimovich</keyname><forenames>Arkady</forenames><affiliation>MIT</affiliation></author><author><keyname>Cunningham</keyname><forenames>Robert K.</forenames><affiliation>MIT</affiliation></author></authors><title>Computing on Masked Data: a High Performance Method for Improving Big
  Data Veracity</title><categories>cs.CR astro-ph.IM cs.DB cs.IR</categories><comments>to appear in IEEE High Performance Extreme Computing 2014
  (ieee-hpec.org)</comments><doi>10.1109/HPEC.2014.7040946</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing gap between data and users calls for innovative tools that
address the challenges faced by big data volume, velocity and variety. Along
with these standard three V's of big data, an emerging fourth &quot;V&quot; is veracity,
which addresses the confidentiality, integrity, and availability of the data.
Traditional cryptographic techniques that ensure the veracity of data can have
overheads that are too large to apply to big data. This work introduces a new
technique called Computing on Masked Data (CMD), which improves data veracity
by allowing computations to be performed directly on masked data and ensuring
that only authorized recipients can unmask the data. Using the sparse linear
algebra of associative arrays, CMD can be performed with significantly less
overhead than other approaches while still supporting a wide range of linear
algebraic operations on the masked data. Databases with strong support of
sparse operations, such as SciDB or Apache Accumulo, are ideally suited to this
technique. Examples are shown for the application of CMD to a complex DNA
matching algorithm and to database operations over social media data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5752</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5752</id><created>2014-06-22</created><authors><author><keyname>Zhou</keyname><forenames>Tianyi</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author></authors><title>Divide-and-Conquer Learning by Anchoring a Conical Hull</title><categories>stat.ML cs.LG</categories><comments>26 pages, long version, in updating</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We reduce a broad class of machine learning problems, usually addressed by EM
or sampling, to the problem of finding the $k$ extremal rays spanning the
conical hull of a data point set. These $k$ &quot;anchors&quot; lead to a global solution
and a more interpretable model that can even outperform EM and sampling on
generalization error. To find the $k$ anchors, we propose a novel
divide-and-conquer learning scheme &quot;DCA&quot; that distributes the problem to
$\mathcal O(k\log k)$ same-type sub-problems on different low-D random
hyperplanes, each can be solved by any solver. For the 2D sub-problem, we
present a non-iterative solver that only needs to compute an array of cosine
values and its max/min entries. DCA also provides a faster subroutine for other
methods to check whether a point is covered in a conical hull, which improves
algorithm design in multiple dimensions and brings significant speedup to
learning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering,
then show its competitive performance and scalability over other methods on
rich datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5759</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5759</id><created>2014-06-22</created><authors><author><keyname>Moniruzzaman</keyname><forenames>A B M</forenames></author><author><keyname>Nafi</keyname><forenames>Kawser Wazed</forenames></author><author><keyname>Hossain</keyname><forenames>Syed Akther</forenames></author></authors><title>An Experimental Study of Load Balancing of OpenNebula Open-Source Cloud
  Computing Platform</title><categories>cs.DC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing is becoming a viable computing solution for services oriented
computing. Several open-source cloud solutions are available to these supports.
Open-source software stacks offer a huge amount of customizability without huge
licensing fees. As a result, open source software are widely used for designing
cloud, and private clouds are being built increasingly in the open source way.
Numerous contributions have been made by the open-source community related to
private-IaaS-cloud. OpenNebula - a cloud platform is one of the popular private
cloud management software. However, little has been done to systematically
investigate the performance evaluation of this open-source cloud solution in
the existing literature. The performance evaluation aids new and existing
research, industry and international projects when selecting OpenNebula
software to their work. The objective of this paper is to evaluate the
load-balancing performance of the OpenNebula cloud management software. For the
performance evaluation, the OpenNebula cloud management software is installed
and configured as a prototype implementation and tested on the DIU Cloud Lab.
In this paper, two set of experiments are conducted to identify the load
balancing performance of the OpenNebula cloud management platform- (1) Delete
and Add Virtual Machine (VM) from OpenNebula cloud platform; (2) Mapping
Physical Hosts to Virtual Machines (VMs) in the OpenNebula cloud platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5760</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5760</id><created>2014-06-22</created><authors><author><keyname>Moniruzzaman</keyname><forenames>A B M</forenames></author><author><keyname>Nafi</keyname><forenames>Kawser Wazed</forenames></author><author><keyname>Hossain</keyname><forenames>Syed Akther</forenames></author></authors><title>Virtual Memory Streaming Technique for Virtual Machines (VMs) for Rapid
  Scaling and High Performance in Cloud Environment</title><categories>cs.DC</categories><comments>Keywords Virtual memory, Memory Streaming, Virtual machine, Virtual
  memory scaling, VM live migration, VM Cloning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the impact of Virtual Memory Streaming (VMS) technique
in provisioning virtual machines (VMs) in cloud environment. VMS is a scaling
virtualization technology that allows different virtual machines rapid scale,
high performance, and increase hardware utilization. Traditional hypervisors do
not support true no-downtime live migration, and its lack of memory
oversubscription can hurt the economics of a private cloud deployment by
limiting the number of VMs on each host. VMS brings together several advanced
hypervisor memory management techniques including granular page sharing,
dynamic memory footprint management, live migration, read caching, and a unique
virtual machine cloning capability. An architecture model is described,
together with a proof-of-concept implementation, that VMS dynamically scaling
of virtualized infrastructure with true live migration and cloning of VMs. This
paper argues that VMS for Cloud allows requiring significantly reduced server
memory and reducing the time for virtualized resource scaling by instantly
adding more virtual machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5761</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5761</id><created>2014-06-22</created><authors><author><keyname>Moniruzzaman</keyname><forenames>A B M</forenames></author><author><keyname>Hossain</keyname><forenames>Syed Akther</forenames></author></authors><title>A Low Cost Two-Tier Architecture Model For High Availability Clusters
  Application Load Balancing</title><categories>cs.NI cs.DC</categories><comments>Load balancing, high availability cluster, web server clusters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a design and implementation of a low cost two-tier
architecture model for high availability cluster combined with load-balancing
and shared storage technology to achieve desired scale of three-tier
architecture for application load balancing e.g. web servers. The research work
proposes a design that physically omits Network File System (NFS) server nodes
and implements NFS server functionalities within the cluster nodes, through Red
Hat Cluster Suite (RHCS) with High Availability (HA) proxy load balancing
technologies. In order to achieve a low-cost implementation in terms of
investment in hardware and computing solutions, the proposed architecture will
be beneficial. This system intends to provide steady service despite any system
components fails due to uncertainly such as network system, storage and
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5765</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5765</id><created>2014-06-22</created><authors><author><keyname>Jin</keyname><forenames>Ming</forenames></author><author><keyname>Zou</keyname><forenames>Han</forenames></author><author><keyname>Weekly</keyname><forenames>Kevin</forenames></author><author><keyname>Jia</keyname><forenames>Ruoxi</forenames></author><author><keyname>Bayen</keyname><forenames>Alexandre M.</forenames></author><author><keyname>Spanos</keyname><forenames>Costas J.</forenames></author></authors><title>Environmental Sensing by Wearable Device for Indoor Activity and
  Location Estimation</title><categories>cs.HC stat.ML</categories><comments>submitted to the 40th Annual Conference of the IEEE Industrial
  Electronics Society (IECON)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present results from a set of experiments in this pilot study to
investigate the causal influence of user activity on various environmental
parameters monitored by occupant carried multi-purpose sensors. Hypotheses with
respect to each type of measurements are verified, including temperature,
humidity, and light level collected during eight typical activities: sitting in
lab / cubicle, indoor walking / running, resting after physical activity,
climbing stairs, taking elevators, and outdoor walking. Our main contribution
is the development of features for activity and location recognition based on
environmental measurements, which exploit location- and activity-specific
characteristics and capture the trends resulted from the underlying
physiological process. The features are statistically shown to have good
separability and are also information-rich. Fusing environmental sensing
together with acceleration is shown to achieve classification accuracy as high
as 99.13%. For building applications, this study motivates a sensor fusion
paradigm for learning individualized activity, location, and environmental
preferences for energy management and user comfort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5774</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5774</id><created>2014-06-22</created><updated>2015-07-15</updated><authors><author><keyname>Azizpour</keyname><forenames>Hossein</forenames></author><author><keyname>Razavian</keyname><forenames>Ali Sharif</forenames></author><author><keyname>Sullivan</keyname><forenames>Josephine</forenames></author><author><keyname>Maki</keyname><forenames>Atsuto</forenames></author><author><keyname>Carlsson</keyname><forenames>Stefan</forenames></author></authors><title>Factors of Transferability for a Generic ConvNet Representation</title><categories>cs.CV</categories><comments>Extended version of the workshop paper with more experiments and
  updated text and title. Original CVPR15 DeepVision workshop paper title:
  &quot;From Generic to Specific Deep Representations for Visual Recognition&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidence is mounting that Convolutional Networks (ConvNets) are the most
effective representation learning method for visual recognition tasks. In the
common scenario, a ConvNet is trained on a large labeled dataset (source) and
the feed-forward units activation of the trained network, at a certain layer of
the network, is used as a generic representation of an input image for a task
with relatively smaller training set (target). Recent studies have shown this
form of representation transfer to be suitable for a wide range of target
visual recognition tasks. This paper introduces and investigates several
factors affecting the transferability of such representations. It includes
parameters for training of the source ConvNet such as its architecture,
distribution of the training data, etc. and also the parameters of feature
extraction such as layer of the trained ConvNet, dimensionality reduction, etc.
Then, by optimizing these factors, we show that significant improvements can be
achieved on various (17) visual recognition tasks. We further show that these
visual recognition tasks can be categorically ordered based on their distance
from the source task such that a correlation between the performance of tasks
and their distance from the source task w.r.t. the proposed factors is
observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5778</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5778</id><created>2014-06-22</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Roy</keyname><forenames>Subhro</forenames></author></authors><title>Approximating the Maximum Overlap of Polygons under Translation</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ and $Q$ be two simple polygons in the plane of total complexity $n$,
each of which can be decomposed into at most $k$ convex parts. We present an
$(1-\varepsilon)$-approximation algorithm, for finding the translation of $Q$,
which maximizes its area of overlap with $P$. Our algorithm runs in $O(c n)$
time, where $c$ is a constant that depends only on $k$ and $\varepsilon$.
  This suggest that for polygons that are &quot;close&quot; to being convex, the problem
can be solved (approximately), in near linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5786</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5786</id><created>2014-06-22</created><authors><author><keyname>Ferner</keyname><forenames>Ulric J.</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Queued cross-bar network models for replication and coded storage
  systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coding techniques may be useful for data center data survivability as well as
for reducing traffic congestion. We present a queued cross-bar network (QCN)
method that can be used for traffic analysis of both replication/uncoded and
coded storage systems. We develop a framework for generating QCN rate regions
(RRs) by analyzing their conflict graph stable set polytopes (SSPs). In doing
so, we apply recent results from graph theory on the characterization of
particular graph SSPs. We characterize the SSP of QCN conflict graphs under a
variety of traffic patterns, allowing for their efficient RR computation. For
uncoded systems, we show how to compute RRs and find rate optimal scheduling
algorithms. For coded storage, we develop a RR upper bound, for which we
provide an intuitive interpretation. We show that the coded storage RR upper
bound is achievable in certain coded systems in which drives store sufficient
coded information, as well in certain dynamic coding systems. Numerical
illustrations show that coded storage can result in gains in RR volume of
approximately 50%, averaged across traffic patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5791</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5791</id><created>2014-06-22</created><authors><author><keyname>Natarajan</keyname><forenames>Abhiram</forenames></author><author><keyname>Wu</keyname><forenames>Yi</forenames></author></authors><title>Computational Complexity of Certifying Restricted Isometry Property</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a matrix $A$ with $n$ rows, a number $k&lt;n$, and $0&lt;\delta &lt; 1$, $A$ is
$(k,\delta)$-RIP (Restricted Isometry Property) if, for any vector $x \in
\mathbb{R}^n$, with at most $k$ non-zero co-ordinates, $$(1-\delta) \|x\|_2
\leq \|A x\|_2 \leq (1+\delta)\|x\|_2$$ In many applications, such as
compressed sensing and sparse recovery, it is desirable to construct RIP
matrices with a large $k$ and a small $\delta$. Given the efficacy of random
constructions in generating useful RIP matrices, the problem of certifying the
RIP parameters of a matrix has become important.
  In this paper, we prove that it is hard to approximate the RIP parameters of
a matrix assuming the Small-Set-Expansion-Hypothesis. Specifically, we prove
that for any arbitrarily large constant $C&gt;0$ and any arbitrarily small
constant $0&lt;\delta&lt;1$, there exists some $k$ such that given a matrix $M$, it
is SSE-Hard to distinguish the following two cases:
  - (Highly RIP) $M$ is $(k,\delta)$-RIP.
  - (Far away from RIP) $M$ is not $(k/C, 1-\delta)$-RIP.
  Most of the previous results on the topic of hardness of RIP certification
only hold for certification when $\delta=o(1)$. In practice, it is of interest
to understand the complexity of certifying a matrix with $\delta$ being close
to $\sqrt{2}-1$, as it suffices for many real applications to have matrices
with $\delta = \sqrt{2}-1$. Our hardness result holds for any constant
$\delta$. Specifically, our result proves that even if $\delta$ is indeed very
small, i.e. the matrix is in fact \emph{strongly RIP}, certifying that the
matrix exhibits \emph{weak RIP} itself is SSE-Hard.
  In order to prove the hardness result, we prove a variant of the Cheeger's
Inequality for sparse vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5794</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5794</id><created>2014-06-22</created><authors><author><keyname>Tushar</keyname><forenames>Wayes</forenames></author><author><keyname>Chai</keyname><forenames>Bo</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Smith</keyname><forenames>David B.</forenames></author><author><keyname>Wood</keyname><forenames>Kristin L.</forenames></author><author><keyname>Yang</keyname><forenames>Zaiyue</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Three-Party Energy Management With Distributed Energy Resources in Smart
  Grid</title><categories>cs.SY</categories><comments>12 pages, Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the benefits of distributed energy resources (DERs) are
considered in an energy management scheme for a smart community consisting of a
large number of residential units (RUs) and a shared facility controller (SFC).
A non-cooperative Stackelberg game between RUs and the SFC is proposed in order
to explore how both entities can benefit, in terms of achieved utility and
minimizing total cost respectively, from their energy trading with each other
and the grid. From the properties of the game, it is shown that the maximum
benefit to the SFC in terms of reduction in total cost is obtained at the
unique and strategy proof Stackelberg equilibrium (SE). It is further shown
that the SE is guaranteed to be reached by the SFC and RUs by executing the
proposed algorithm in a distributed fashion, where participating RUs comply
with their best strategies in response to the action chosen by the SFC. In
addition, a charging-discharging scheme is introduced for the SFC's storage
device (SD) that can further lower the SFC's total cost if the proposed game is
implemented. Numerical experiments confirm the effectiveness of the proposed
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5797</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5797</id><created>2014-06-22</created><authors><author><keyname>Kasahara</keyname><forenames>Masao</forenames></author><author><keyname>Hirasawa</keyname><forenames>Shigeichi</forenames></author></authors><title>Constructions of A Large Class of Optimum Constant Weight Codes over F_2</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method of constructing optimum constant weight codes over F_2 based on
a generalized $(u, u+v)$ construction is presented. We present a new method of
constructing superimposed code $C_{(s_1,s_2,\cdots,s_I)}^{(h_1, h_2, \cdots,
h_I)}$ bound. and presented a large class of optimum constant weight codes over
F_2 that meet the bound due to Brouwer and Verhoeff, which will be referred to
as BV . We present large classes of optimum constant weight codes over F_2 for
$k=2$ and $k=3$ for $n \leqq 128$. We also present optimum constant weight
codes over F_2 that meet the BV bound for $k=2,3,4,5$ and 6, for $n \leqq 128$.
The authors would like to present the following conjectures :
  $C_{I}$: $C_{(s_1)}^{(h_1)}$ presented in this paper yields the optimum
constant weight codes for the code-length $n=3h_1$, number of information
symbols $k=2$ and minimum distance $d=2h_1$ for any positive integer $h_1$.
  $C_{II}$: $C_{(s_1)}^{(h_1)}$ yields the optimum constant weight codes at
$n=7h_1, k=3$ and $d=4h_1$ for any $h_1$.
  $C_{III}$: Code $C_{(s_1,s_2,\cdots,s_I)}^{(h_1, h_2, \cdots, h_I)}$ yields
the optimum constant weight codes of length $n=2^{k+1}-2$, and minimum distance
$d=2^{k}$ for any number of information symbols $k\geq 3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5807</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5807</id><created>2014-06-23</created><authors><author><keyname>Liu</keyname><forenames>Peilei</forenames></author><author><keyname>Wang</keyname><forenames>Ting</forenames></author></authors><title>A Unified Quantitative Model of Vision and Audition</title><categories>cs.CV q-bio.NC q-bio.QM</categories><comments>7 pages, 3 figures</comments><acm-class>I.5.4; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have put forwards a unified quantitative framework of vision and audition,
based on existing data and theories. According to this model, the retina is a
feedforward network self-adaptive to inputs in a specific period. After fully
grown, cells become specialized detectors based on statistics of stimulus
history. This model has provided explanations for perception mechanisms of
colour, shape, depth and motion. Moreover, based on this ground we have put
forwards a bold conjecture that single ear can detect sound direction. This is
complementary to existing theories and has provided better explanations for
sound localization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5814</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5814</id><created>2014-06-23</created><updated>2015-03-20</updated><authors><author><keyname>Liebig</keyname><forenames>J.</forenames></author><author><keyname>Rao</keyname><forenames>A.</forenames></author></authors><title>Identifying Influential Nodes in Bipartite Networks Using the Clustering
  Coefficient</title><categories>cs.SI physics.soc-ph</categories><doi>10.1109/SITIS.2014.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The identification of influential nodes in complex network can be very
challenging. If the network has a community structure, centrality measures may
fail to identify the complete set of influential nodes, as the hubs and other
central nodes of the network may lie inside only one community. Here we define
a bipartite clustering coefficient that, by taking differently structured
clusters into account, can find important nodes across communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5824</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5824</id><created>2014-06-23</created><authors><author><keyname>Yeung</keyname><forenames>Serena</forenames></author><author><keyname>Fathi</keyname><forenames>Alireza</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>VideoSET: Video Summary Evaluation through Text</title><categories>cs.CV cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present VideoSET, a method for Video Summary Evaluation
through Text that can evaluate how well a video summary is able to retain the
semantic information contained in its original video. We observe that semantics
is most easily expressed in words, and develop a text-based approach for the
evaluation. Given a video summary, a text representation of the video summary
is first generated, and an NLP-based metric is then used to measure its
semantic distance to ground-truth text summaries written by humans. We show
that our technique has higher agreement with human judgment than pixel-based
distance metrics. We also release text annotations and ground-truth text
summaries for a number of publicly available video datasets, for use by the
computer vision community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5826</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5826</id><created>2014-06-23</created><authors><author><keyname>Christofides</keyname><forenames>Demetres</forenames></author></authors><title>The asymptotic complexity of matrix reduction over finite fields</title><categories>cs.DS cs.CC math.CO</categories><msc-class>05A16, 15A09</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an invertible n \times n matrix over some field. The Gauss-Jordan
elimination reduces this matrix to the identity matrix using at most n^2 row
operations and in general that many operations might be needed.
  In [1] the authors considered matrices in GL(n;q), the set of n \times n
invertible matrices in the finite field of q elements, and provided an
algorithm using only row operations which performs asymptotically better than
the Gauss-Jordan elimination. More specifically their `striped elimination
algorithm' has asymptotic complexity \frac{n^2}{\log_q{n}}. Furthermore they
proved that up to a constant factor this algorithm is best possible as almost
all matrices in GL(n;g) need asymptotically at least \frac{n^2}{2\log_q{n}}
operations.
  In this short note we show that the `striped elimination algorithm' is
asymptotically optimal by proving that almost all matrices in GL(n;q) need
asymptotically at least frac{n^2}{\log_q{n}} operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5860</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5860</id><created>2014-06-23</created><updated>2015-06-03</updated><authors><author><keyname>Yu</keyname><forenames>Mingchao</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author></authors><title>On Deterministic Linear Network Coded Broadcast and Its Relation to
  Matroid Theory</title><categories>cs.IT math.IT</categories><comments>Origin work accepted by ITW, 5 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deterministic linear network coding (DLNC) is an important family of network
coding techniques for wireless packet broadcast. In this paper, we show that
DLNC is strongly related to and can be effectively studied using matroid theory
without bridging index coding. We prove the equivalence between the DLNC
solution and matrix matroid. We use this equivalence to study the performance
limits of DLNC in terms of the number of transmissions and its dependence on
the finite field size. Specifically, we derive the sufficient and necessary
condition for the existence of perfect DLNC solutions and prove that such
solutions may not exist over certain finite fields. We then show that
identifying perfect solutions over any finite field is still an open problem in
general. To fill this gap, we develop a heuristic algorithm which employs
graphic matroids to find perfect DLNC solutions over any finite field.
Numerical results show that its performance in terms of minimum number of
transmissions is close to the lower bound, and is better than random linear
network coding when the field size is not so large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5886</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5886</id><created>2014-06-23</created><authors><author><keyname>Richter</keyname><forenames>Johannes</forenames></author><author><keyname>Scheunert</keyname><forenames>Christian</forenames></author><author><keyname>Engelmann</keyname><forenames>Sabrina</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Weak Secrecy in the Multi-Way Untrusted Relay Channel with
  Compute-and-Forward</title><categories>cs.IT math.IT</categories><comments>submitted to JSAC Special Issue on Fundamental Approaches to Network
  Coding in Wireless Communication Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of secure communications in a Gaussian multi-way
relay channel applying the compute-and-forward scheme using nested lattice
codes. All nodes employ half-duplex operation and can exchange confidential
messages only via an untrusted relay. The relay is assumed to be honest but
curious, i.e., an eavesdropper that conforms to the system rules and applies
the intended relaying scheme. We start with the general case of the
single-input multiple-output (SIMO) L-user multi-way relay channel and provide
an achievable secrecy rate region under a weak secrecy criterion. We show that
the securely achievable sum rate is equivalent to the difference between the
computation rate and the multiple access channel (MAC) capacity. Particularly,
we show that all nodes must encode their messages such that the common
computation rate tuple falls outside the MAC capacity region of the relay. We
provide results for the single-input single-output (SISO) and the
multiple-input single-input (MISO) L-user multi-way relay channel as well as
the two-way relay channel. We discuss these results and show the dependency
between channel realization and achievable secrecy rate. We further compare our
result to available results in the literature for different schemes and show
that the proposed scheme operates close to the compute-and-forward rate without
secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5895</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5895</id><created>2014-06-23</created><authors><author><keyname>Carpi</keyname><forenames>Arturo</forenames></author><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Holub</keyname><forenames>Stepan</forenames></author><author><keyname>Oprsal</keyname><forenames>Jakub</forenames></author><author><keyname>Sciortino</keyname><forenames>Marinella</forenames></author></authors><title>Universal Lyndon Words</title><categories>cs.DM cs.FL math.CO</categories><comments>To appear in the proceedings of MFCS 2014</comments><msc-class>68R15</msc-class><journal-ref>Lecture Notes in Computer Science, 8634: 135-146 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A word $w$ over an alphabet $\Sigma$ is a Lyndon word if there exists an
order defined on $\Sigma$ for which $w$ is lexicographically smaller than all
of its conjugates (other than itself). We introduce and study \emph{universal
Lyndon words}, which are words over an $n$-letter alphabet that have length
$n!$ and such that all the conjugates are Lyndon words. We show that universal
Lyndon words exist for every $n$ and exhibit combinatorial and structural
properties of these words. We then define particular prefix codes, which we
call Hamiltonian lex-codes, and show that every Hamiltonian lex-code is in
bijection with the set of the shortest unrepeated prefixes of the conjugates of
a universal Lyndon word. This allows us to give an algorithm for constructing
all the universal Lyndon words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5903</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5903</id><created>2014-06-23</created><updated>2015-03-25</updated><authors><author><keyname>Sch&#xfc;lke</keyname><forenames>Christophe</forenames></author><author><keyname>Caltagirone</keyname><forenames>Francesco</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Blind Sensor Calibration using Approximate Message Passing</title><categories>cs.IT math.IT</categories><comments>27 pages, 9 figures, submitted to IEEE Transactions on Signal
  Processing</comments><doi>10.1088/1742-5468/2015/11/P11013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ubiquity of approximately sparse data has led a variety of com- munities
to great interest in compressed sensing algorithms. Although these are very
successful and well understood for linear measurements with additive noise,
applying them on real data can be problematic if imperfect sensing devices
introduce deviations from this ideal signal ac- quisition process, caused by
sensor decalibration or failure. We propose a message passing algorithm called
calibration approximate message passing (Cal-AMP) that can treat a variety of
such sensor-induced imperfections. In addition to deriving the general form of
the algorithm, we numerically investigate two particular settings. In the
first, a fraction of the sensors is faulty, giving readings unrelated to the
signal. In the second, sensors are decalibrated and each one introduces a
different multiplicative gain to the measures. Cal-AMP shares the scalability
of approximate message passing, allowing to treat big sized instances of these
problems, and ex- perimentally exhibits a phase transition between domains of
success and failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5910</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5910</id><created>2014-06-23</created><authors><author><keyname>Shapovalov</keyname><forenames>Roman</forenames></author><author><keyname>Vetrov</keyname><forenames>Dmitry</forenames></author><author><keyname>Osokin</keyname><forenames>Anton</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author></authors><title>Multi-utility Learning: Structured-output Learning with Multiple
  Annotation-specific Loss Functions</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured-output learning is a challenging problem; particularly so because
of the difficulty in obtaining large datasets of fully labelled instances for
training. In this paper we try to overcome this difficulty by presenting a
multi-utility learning framework for structured prediction that can learn from
training instances with different forms of supervision. We propose a unified
technique for inferring the loss functions most suitable for quantifying the
consistency of solutions with the given weak annotation. We demonstrate the
effectiveness of our framework on the challenging semantic image segmentation
problem for which a wide variety of annotations can be used. For instance, the
popular training datasets for semantic segmentation are composed of images with
hard-to-generate full pixel labellings, as well as images with easy-to-obtain
weak annotations, such as bounding boxes around objects, or image-level labels
that specify which object categories are present in an image. Experimental
evaluation shows that the use of annotation-specific loss functions
dramatically improves segmentation accuracy compared to the baseline system
where only one type of weak annotation is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5917</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5917</id><created>2014-06-23</created><authors><author><keyname>Ferchichi</keyname><forenames>Abdelwaheb</forenames></author><author><keyname>Gouider</keyname><forenames>Mohamed Salah</forenames></author></authors><title>BSTree: an Incremental Indexing Structure for Similarity Search and Real
  Time Monitoring of Data Streams</title><categories>cs.DB</categories><journal-ref>Future Information Technology Lecture Notes in Electrical
  Engineering Volume 276, 2014, pp 185-190</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this work, a new indexing technique of data streams called BSTree is
proposed. This technique uses the method of data discretization, SAX [4], to
reduce online the dimensionality of data streams. It draws on Btree to build
the index and finally uses an LRV (least Recently visited) pruning technique to
rid the index structure from data whose last visit time exceeds a threshold
value and thus minimizes response time for similarity search queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5926</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5926</id><created>2014-06-23</created><authors><author><keyname>Herbert</keyname><forenames>Steven</forenames></author><author><keyname>Wassell</keyname><forenames>Ian</forenames></author><author><keyname>Loh</keyname><forenames>Tian-Hong</forenames></author></authors><title>A Simple Lower Bound on the Noncoherent Capacity of Highly Underspread
  Fading Channels</title><categories>cs.IT math.IT</categories><comments>12 pages, 1 figure, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication channels are said to be underspread if their coherence time is
greater than their delay spread. In such cases it can be shown that in the
infinite bandwidth limit the information capacity tends to that of a channel
with perfect receiver Channel State Information (CSI). This paper presents a
lower bound on the capacity of a channel with finite bandwidth, expressed in a
form which is mathematically elegant, and computationally simple. The bounding
method exploits the fact that most actual channels are highly underspread; and
that typically more is known about their impulse response than the channel time
variation. The capacity is lower bounded by finding an achievable rate for
individual time blocks which are shorter than the channel coherence time, in an
orthogonal frequency division multiplexing system model. A highly underspread
channel of particular interest is the invehicle channel, and a numerical
example is given to verify that the capacity is indeed approximately that of a
channel with perfect receiver CSI. The resulting lower bound is shown to be
tighter than those previously derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5935</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5935</id><created>2014-06-23</created><updated>2014-08-26</updated><authors><author><keyname>Araldo</keyname><forenames>Andrea</forenames></author><author><keyname>Mangili</keyname><forenames>Michele</forenames></author><author><keyname>Martignon</keyname><forenames>Fabio</forenames></author><author><keyname>Rossi</keyname><forenames>Dario</forenames></author></authors><title>Cost-aware caching: optimizing cache provisioning and object placement
  in ICN</title><categories>cs.NI</categories><acm-class>C.2.1; C.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching is frequently used by Internet Service Providers as a viable
technique to reduce the latency perceived by end users, while jointly
offloading network traffic. While the cache hit-ratio is generally considered
in the literature as the dominant performance metric for such type of systems,
in this paper we argue that a critical missing piece has so far been neglected.
Adopting a radically different perspective, in this paper we explicitly account
for the cost of content retrieval, i.e. the cost associated to the external
bandwidth needed by an ISP to retrieve the contents requested by its customers.
Interestingly, we discover that classical cache provisioning techniques that
maximize cache efficiency (i.e., the hit-ratio), lead to suboptimal solutions
with higher overall cost. To show this mismatch, we propose two optimization
models that either minimize the overall costs or maximize the hit-ratio,
jointly providing cache sizing, object placement and path selection. We
formulate a polynomial-time greedy algorithm to solve the two problems and
analytically prove its optimality. We provide numerical results and show that
significant cost savings are attainable via a cost-aware design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5943</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5943</id><created>2014-06-23</created><authors><author><keyname>Harris</keyname><forenames>David G.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author></authors><title>The Moser-Tardos Framework with Partial Resampling</title><categories>math.CO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The resampling algorithm of Moser &amp; Tardos is a powerful approach to develop
constructive versions of the Lov\'{a}sz Local Lemma. We develop a partial
resampling approach motivated by this methodology: when a bad event holds, we
resample an appropriately-random \emph{subset} of the variables that define
this event, rather than the entire set as in Moser &amp; Tardos. This is
particularly useful when the bad events are determined by sums of random
variables. This leads to several improved algorithmic applications in
scheduling, graph transversals, packet routing etc. For instance, we improve
the approximation ratio of a generalized $D$-dimensional scheduling problem
studied by Azar &amp; Epstein from $O(D)$ to $O(\log D / \log\log D)$, and settle a
conjecture of Szab\'{o} &amp; Tardos on graph transversals asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5946</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5946</id><created>2014-06-23</created><authors><author><keyname>P&#xe9;rez-Garc&#xed;a</keyname><forenames>Lorena</forenames></author><author><keyname>Broekaert</keyname><forenames>Jan</forenames></author><author><keyname>Note</keyname><forenames>Nicole</forenames></author></authors><title>Is a `Wirikuta empowerment' of the Huichol measurable on the Internet?</title><categories>cs.SI cs.CY cs.IR physics.soc-ph</categories><comments>14 pages, 1 figure, 4 graphs (submitted to journal)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current social and activist movements find the opportunity in social media to
effectively impact on the agenda of governing bodies and create `global'
perceptions -- it is often claimed. Content related to the social and activist
movements is online, to be accessed, supported or disputed and distributed from
virtually anywhere at any time, in the public sphere of the Internet. This
activity allows the enlargement of social movements and would increase the
empowerment in the concerned communities. The aim of this explorative study is
to assess whether the temporal evolution of the Normalised Web Distance (NWD)
--as defined by Cilibrasi &amp; Vit\'anyi (2007)-- between identifying terms
concerning this activism could be used to measure the progress or decline of
social empowerment through the Internet. The NWD relies on the page count
number of single and joint queries, which in our study have been registered
using a freely available web browser (e.g. Google Search) providing a time
search window for temporal query results. To explore this meta-data technique,
we introduce the case of a perceived Wirikuta online movement, which originated
in Mexico with the aim to protect the Huichols' sacred land and water resources
from open mining projects for silver ore. We conducted a small scale Internet
study relating the key terms `Wirikuta', `Huichol', and `Wixarika' and their
co-occurrence with seven positive qualifiers (e.g. `sacred land'), five
negative qualifiers (e.g. `violence') and one neutral qualifier (`table') over
time, annually from 1994 till 2013. We confirm close semantic clustering over
time of traditional indigeneous identity terms of the Huichol, and observe a
slight convergence of key terms to `mines' and less pronounced to `sacred land'
and a divergence with respect to `ancestors' indicating a complex image of a
tendency of empowerment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5947</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5947</id><created>2014-06-23</created><authors><author><keyname>Miclut</keyname><forenames>Bogdan</forenames></author><author><keyname>Kaester</keyname><forenames>Thomas</forenames></author><author><keyname>Martinetz</keyname><forenames>Thomas</forenames></author><author><keyname>Barth</keyname><forenames>Erhardt</forenames></author></authors><title>Committees of deep feedforward networks trained with few data</title><categories>cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks are known to give good results on image
classification tasks. In this paper we present a method to improve the
classification result by combining multiple such networks in a committee. We
adopt the STL-10 dataset which has very few training examples and show that our
method can achieve results that are better than the state of the art. The
networks are trained layer-wise and no backpropagation is used. We also explore
the effects of dataset augmentation by mirroring, rotation, and scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5949</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5949</id><created>2014-06-23</created><updated>2015-06-19</updated><authors><author><keyname>Papadimitriou</keyname><forenames>Georgios</forenames></author><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Traganitis</keyname><forenames>Apostolos</forenames></author><author><keyname>Angelakis</keyname><forenames>Vangelis</forenames></author></authors><title>Network-Level Performance Evaluation of a Two-Relay Cooperative Random
  Access Wireless System</title><categories>cs.NI</categories><comments>Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless networks relay nodes can be used to assist the users'
transmissions to reach their destination. Work on relay cooperation, from a
physical layer perspective, has up to now yielded well-known results. This
paper takes a different stance focusing on network-level cooperation. Extending
previous results for a single relay, we investigate here the benefits from the
deployment of a second one. We assume that the two relays do not generate
packets of their own and the system employs random access to the medium; we
further consider slotted time and that the users have saturated queues. We
obtain analytical expressions for the arrival and service rates of the queues
of the two relays and the stability conditions. We investigate a model of the
system, in which the users are divided into clusters, each being served by one
relay, and show its advantages in terms of aggregate and throughput per user.
We quantify the above, analytically for the case of the collision channel and
through simulations for the case of Multi-Packet Reception (MPR), and we
provide insight on when the deployment of a second relay in the system can
yield significant advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5954</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5954</id><created>2014-06-23</created><updated>2015-06-10</updated><authors><author><keyname>Jia</keyname><forenames>Yanan</forenames></author><author><keyname>Calder</keyname><forenames>Catherine A.</forenames></author><author><keyname>Browning</keyname><forenames>Christopher R.</forenames></author></authors><title>Bilinear Mixed-Effects Models for Affiliation Networks</title><categories>stat.ME cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An affiliation network is a particular type of two-mode social network that
consists of a set of `actors' and a set of `events' where ties indicate an
actor's participation in an event. Although networks describe a variety of
consequential social structures, statistical methods for studying affiliation
networks are less well developed than methods for studying one-mode, or
actor-actor, networks. One way to analyze affiliation networks is to consider
one-mode network matrices that are derived from an affiliation network, but
this approach may lead to the loss of important structural features of the
data. The most comprehensive approach is to study both actors and events
simultaneously. In this paper, we extend the bilinear mixed-effects model, a
type of latent space model developed for one-mode networks, to the affiliation
network setting by considering the dependence patterns in the interactions
between actors and events and describe a Markov chain Monte Carlo algorithm for
Bayesian inference. We use our model to explore patterns in extracurricular
activity membership of students in a racially-diverse high school in a
Midwestern metropolitan area. Using techniques from spatial point pattern
analysis, we show how our model can provide insight into patterns of racial
segregation in the voluntary extracurricular activity participation profiles of
adolescents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5970</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5970</id><created>2014-06-23</created><authors><author><keyname>Hsieh</keyname><forenames>Samuel C.</forenames></author></authors><title>A Lower Bound for Boolean Satisfiability on Turing Machines</title><categories>cs.CC</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a lower bound for deciding the satisfiability of the conjunction
of any two Boolean formulas from a set called a full representation of Boolean
functions of $n$ variables - a set containing a Boolean formula to represent
each Boolean function of $n$ variables. The contradiction proof first assumes
that there exists a Turing machine with $k$ symbols in its tape alphabet that
correctly decides the satisfiability of the conjunction of any two Boolean
formulas from such a set by making fewer than $2^nlog_k2$ moves. By using
multiple runs of this Turing machine, with one run for each Boolean function of
$n$ variables, the proof derives a contradiction by showing that this Turing
machine is unable to correctly decide the satisfiability of the conjunction of
at least one pair of Boolean formulas from a full representation of
$n$-variable Boolean functions if the machine makes fewer than $2^nlog_k2$
moves. This lower bound holds for any full representation of Boolean functions
of $n$ variables, even if a full representation consists solely of minimized
Boolean formulas derived by a Boolean minimization method. We discuss why the
lower bound fails to hold for satisfiability of certain restricted formulas,
such as 2CNF satisfiability, XOR-SAT, and HORN-SAT. We also relate the lower
bound to 3CNF satisfiability. The lower bound does not depend on sequentiality
of access to the tape squares and will hold even if a machine is capable of
non-sequential access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5975</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5975</id><created>2014-06-23</created><authors><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author><author><keyname>Wickramaarachchi</keyname><forenames>Charith</forenames></author><author><keyname>Kumbhare</keyname><forenames>Alok</forenames></author><author><keyname>Frincu</keyname><forenames>Marc</forenames></author><author><keyname>Nagarkar</keyname><forenames>Soonil</forenames></author><author><keyname>Ravi</keyname><forenames>Santosh</forenames></author><author><keyname>Raghavendra</keyname><forenames>Cauligi</forenames></author><author><keyname>Prasanna</keyname><forenames>Viktor</forenames></author></authors><title>Scalable Analytics over Distributed Time-series Graphs using GoFFish</title><categories>cs.DC</categories><journal-ref>Proceedings of the IEEE International Parallel and Distributed
  Processing Symposium (IPDPS) (2015) pp. 809-818</journal-ref><doi>10.1109/IPDPS.2015.66</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are a key form of Big Data, and performing scalable analytics over
them is invaluable to many domains. As our ability to collect data grows, there
is an emerging class of inter-connected data which accumulates or varies over
time, and on which novel analytics - both over the network structure and across
the time-variant attribute values - is necessary. We introduce the notion of
time-series graph analytics and propose Gopher, a scalable programming
abstraction to develop algorithms and analytics on such datasets. Our
abstraction leverages a sub-graph centric programming model and extends it to
the temporal dimension using an iterative BSP (Bulk Synchronous Parallel)
approach. Gopher is co-designed with GoFS, a distributed storage specialized
for time-series graphs, as part of the GoFFish distributed analytics platform.
We examine storage optimizations for GoFS, design patterns in Gopher to
leverage the distributed data layout, and evaluate the GoFFish platform using
time-series graph data and applications on a commodity cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5977</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5977</id><created>2014-06-23</created><authors><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author><author><keyname>Kumbhare</keyname><forenames>Alok</forenames></author></authors><title>Floe: A Continuous Dataflow Framework for Dynamic Cloud Applications</title><categories>cs.DC</categories><comments>Prepared in Dec, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications in cyber-physical systems are increasingly coupled with online
instruments to perform long running, continuous data processing. Such &quot;always
on&quot; dataflow applications are dynamic, where they need to change the
applications logic and performance at runtime, in response to external
operational needs. Floe is a continuous dataflow framework that is designed to
be adaptive for dynamic applications on Cloud infrastructure. It offers
advanced dataflow patterns like BSP and MapReduce for flexible and holistic
composition of streams and files, and supports dynamic recomposition at runtime
with minimal impact on the execution. Adaptive resource allocation strategies
allow our framework to effectively use elastic Cloud resources to meet varying
data rates. We illustrate the design patterns of Floe by running an integration
pipeline and a tweet clustering application from the Smart Power Grids domain
on a private Eucalyptus Cloud. The responsiveness of our resource adaptation is
validated through simulations for periodic, bursty and random workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5979</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5979</id><created>2014-06-23</created><authors><author><keyname>Ross</keyname><forenames>Stephane</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author></authors><title>Reinforcement and Imitation Learning via Interactive No-Regret Learning</title><categories>cs.LG stat.ML</categories><comments>14 pages. Under review for NIPS 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has demonstrated that problems-- particularly imitation learning
and structured prediction-- where a learner's predictions influence the
input-distribution it is tested on can be naturally addressed by an interactive
approach and analyzed using no-regret online learning. These approaches to
imitation learning, however, neither require nor benefit from information about
the cost of actions. We extend existing results in two directions: first, we
develop an interactive imitation learning approach that leverages cost
information; second, we extend the technique to address reinforcement learning.
The results provide theoretical support to the commonly observed successes of
online approximate policy iteration. Our approach suggests a broad new family
of algorithms and provides a unifying view of existing techniques for imitation
and reinforcement learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.5988</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.5988</id><created>2014-06-23</created><updated>2015-01-05</updated><authors><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris L.</forenames></author><author><keyname>Bjornson</keyname><forenames>Emil</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Large System Analysis of the Energy Consumption Distribution in
  Multi-User MIMO Systems with Mobility</title><categories>cs.IT math.IT</categories><comments>8 figures, 2 tables, to appear on IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the downlink of a single-cell multi-user MIMO
system in which the base station (BS) makes use of $N$ antennas to communicate
with $K$ single-antenna user equipments (UEs). The UEs move around in the cell
according to a random walk mobility model. We aim at determining the energy
consumption distribution when different linear precoding techniques are used at
the BS to guarantee target rates within a finite time interval $T$. The
analysis is conducted in the asymptotic regime where $N$ and $K$ grow large
with fixed ratio under the assumption of perfect channel state information
(CSI). Both recent and standard results from large system analysis are used to
provide concise formulae for the asymptotic transmit powers and beamforming
vectors for all considered schemes. These results are eventually used to
provide a deterministic approximation of the energy consumption and to study
its fluctuations around this value in the form of a central limit theorem.
Closed-form expressions for the asymptotic means and variances are given.
Numerical results are used to validate the accuracy of the theoretical analysis
and to make comparisons. We show how the results can be used to approximate the
probability that a battery-powered BS runs out of energy and also to design the
cell radius for minimizing the energy consumption per unit area. The imperfect
CSI case is also briefly considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6012</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6012</id><created>2014-06-23</created><authors><author><keyname>Kl&#xfc;gel</keyname><forenames>Niklas</forenames></author><author><keyname>Becker</keyname><forenames>Timo</forenames></author><author><keyname>Groh</keyname><forenames>Georg</forenames></author></authors><title>Designing Sound Collaboratively - Perceptually Motivated Audio Synthesis</title><categories>cs.MM cs.HC</categories><comments>Extended version of submission to conference proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, we will discuss a prototype that allows a group of
users to design sound collaboratively in real time using a multi-touch
tabletop. We make use of a machine learning method to generate a mapping from
perceptual audio features to synthesis parameters. This mapping is then used
for visualization and interaction. Finally, we discuss the results of a
comparative evaluation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6020</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6020</id><created>2014-06-23</created><authors><author><keyname>Audiffren</keyname><forenames>Julien</forenames><affiliation>CMLA</affiliation></author><author><keyname>Ralaivola</keyname><forenames>Liva</forenames><affiliation>LIF</affiliation></author></authors><title>Stationary Mixing Bandits</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the bandit problem where arms are associated with stationary
phi-mixing processes and where rewards are therefore dependent: the question
that arises from this setting is that of recovering some independence by
ignoring the value of some rewards. As we shall see, the bandit problem we
tackle requires us to address the exploration/exploitation/independence
trade-off. To do so, we provide a UCB strategy together with a general regret
analysis for the case where the size of the independence blocks (the ignored
rewards) is fixed and we go a step beyond by providing an algorithm that is
able to compute the size of the independence blocks from the data. Finally, we
give an analysis of our bandit problem in the restless case, i.e., in the
situation where the time counters for all mixing processes simultaneously
evolve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6035</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6035</id><created>2014-06-23</created><authors><author><keyname>Preoteasa</keyname><forenames>Viorel</forenames></author><author><keyname>Tripakis</keyname><forenames>Stavros</forenames></author></authors><title>Refinement Calculus of Reactive Systems</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Refinement calculus is a powerful and expressive tool for reasoning about
sequential programs in a compositional manner. In this paper we present an
extension of refinement calculus for reactive systems. Refinement calculus is
based on monotonic predicate transformers, which transform sets of post-states
into sets of pre-states. To model reactive systems, we introduce monotonic
property transformers, which transform sets of output traces into sets of input
traces. We show how to model in this semantics refinement, sequential
composition, demonic choice, and other semantic operations on reactive systems.
We use primarily higher order logic to express our results, but we also show
how property transformers can be defined using other formalisms more amenable
to automation, such as linear temporal logic (suitable for specifications) and
symbolic transition systems (suitable for implementations). Finally, we show
how this framework generalizes previous work on relational interfaces so as to
be able to express systems with infinite behaviors and liveness properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6037</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6037</id><created>2014-06-23</created><authors><author><keyname>Pai</keyname><forenames>Sreepathi</forenames></author><author><keyname>Govindarajan</keyname><forenames>R.</forenames></author><author><keyname>Thazhuthaveetil</keyname><forenames>Matthew J.</forenames></author></authors><title>Preemptive Thread Block Scheduling with Online Structural Runtime
  Prediction for Concurrent GPGPU Kernels</title><categories>cs.AR cs.OS</categories><comments>14 pages, full pre-review version of PACT 2014 poster</comments><acm-class>D.3.3; C.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent NVIDIA Graphics Processing Units (GPUs) can execute multiple kernels
concurrently. On these GPUs, the thread block scheduler (TBS) uses the FIFO
policy to schedule their thread blocks. We show that FIFO leaves performance to
chance, resulting in significant loss of performance and fairness. To improve
performance and fairness, we propose use of the preemptive Shortest Remaining
Time First (SRTF) policy instead. Although SRTF requires an estimate of runtime
of GPU kernels, we show that such an estimate of the runtime can be easily
obtained using online profiling and exploiting a simple observation on GPU
kernels' grid structure. Specifically, we propose a novel Structural Runtime
Predictor. Using a simple Staircase model of GPU kernel execution, we show that
the runtime of a kernel can be predicted by profiling only the first few thread
blocks. We evaluate an online predictor based on this model on benchmarks from
ERCBench, and find that it can estimate the actual runtime reasonably well
after the execution of only a single thread block. Next, we design a thread
block scheduler that is both concurrent kernel-aware and uses this predictor.
We implement the SRTF policy and evaluate it on two-program workloads from
ERCBench. SRTF improves STP by 1.18x and ANTT by 2.25x over FIFO. When compared
to MPMax, a state-of-the-art resource allocation policy for concurrent kernels,
SRTF improves STP by 1.16x and ANTT by 1.3x. To improve fairness, we also
propose SRTF/Adaptive which controls resource usage of concurrently executing
kernels to maximize fairness. SRTF/Adaptive improves STP by 1.12x, ANTT by
2.23x and Fairness by 2.95x compared to FIFO. Overall, our implementation of
SRTF achieves system throughput to within 12.64% of Shortest Job First (SJF, an
oracle optimal scheduling policy), bridging 49% of the gap between FIFO and
SJF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6045</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6045</id><created>2014-06-22</created><authors><author><keyname>Diamant</keyname><forenames>Emanuel</forenames></author></authors><title>Cognitive Surveillance: Why does it never appear among the AVSS
  Conferences topics?</title><categories>cs.AI q-bio.NC</categories><comments>The paper was submitted to the 11th IEEE International Conference on
  Advanced Video Signal-Based Surveillance (AVSS2014, August 26-29, 2014,
  Seoul, Korea) and has not been accepted for presentation at the conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video Surveillance is a fast evolving field of research and development (R&amp;D)
driven by the urgent need for public security and safety (due to the growing
threats of terrorism, vandalism, and anti-social behavior). Traditionally,
surveillance systems are comprised of two components - video cameras
distributed over the guarded area and human observer watching and analyzing the
incoming video. Explosive growth of installed cameras and limited human
operator's ability to process the delivered video content raise an urgent
demand for developing surveillance systems with human like cognitive
capabilities, that is - Cognitive surveillance systems. The growing interest in
this issue is testified by the tens of workshops, symposiums and conferences
held over the world each year. The IEEE International Conference on Advanced
Video and Signal-Based Surveillance (AVSS) is certainly one of them. However,
for unknown reasons, the term Cognitive Surveillance does never appear among
its topics. As to me, the explanation for this is simple - the complexity and
the indefinable nature of the term &quot;Cognition&quot;. In this paper, I am trying to
resolve the problem providing a novel definition of cognition equally suitable
for biological as well as technological applications. I hope my humble efforts
will be helpful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6046</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6046</id><created>2014-06-22</created><updated>2015-03-31</updated><authors><author><keyname>Zhang</keyname><forenames>Changwang</forenames></author><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Chain</keyname><forenames>Benjamin M.</forenames></author></authors><title>Hybrid Epidemics - A Case Study on Computer Worm Conficker</title><categories>cs.CR cs.NI</categories><journal-ref>PLoS ONE. 2015 May 15;10(5):e0127478</journal-ref><doi>10.1371/journal.pone.0127478</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conficker is a computer worm that erupted on the Internet in 2008. It is
unique in combining three different spreading strategies: local probing,
neighbourhood probing, and global probing. We propose a mathematical model that
combines three modes of spreading, local, neighbourhood and global to capture
the worm's spreading behaviour. The parameters of the model are inferred
directly from network data obtained during the first day of the Conifcker
epidemic. The model is then used to explore the trade-off between spreading
modes in determining the worm's effectiveness. Our results show that the
Conficker epidemic is an example of a critically hybrid epidemic, in which the
different modes of spreading in isolation do not lead to successful epidemics.
Such hybrid spreading strategies may be used beneficially to provide the most
effective strategies for promulgating information across a large population.
When used maliciously, however, they can present a dangerous challenge to
current internet security protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6047</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6047</id><created>2014-06-23</created><authors><author><keyname>Sacomoto</keyname><forenames>Gustavo</forenames></author></authors><title>Efficient Algorithms for de novo Assembly of Alternative Splicing Events
  from RNA-seq Data</title><categories>cs.DS cs.CE q-bio.QM</categories><comments>PhD thesis, 139 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, we address the problem of identifying and quantifying
variants (alternative splicing and genomic polymorphism) in RNA-seq data when
no reference genome is available, without assembling the full transcripts.
Based on the fundamental idea that each variant corresponds to a recognizable
pattern, a bubble, in a de Bruijn graph constructed from the RNA-seq reads, we
propose a general model for all variants in such graphs. We then introduce an
exact method, called KisSplice, to extract alternative splicing events.
Finally, we show that it enables to identify more correct events than general
purpose transcriptome assemblers.
  In order to deal with ever-increasing volumes of NGS data, we put an extra
effort to make KisSplice as scalable as possible. First, to improve its running
time, we propose a new polynomial delay algorithm to enumerate bubbles. We show
that it is several orders of magnitude faster than previous approaches. Then,
to reduce its memory consumption, we propose a new compact way to build and
represent a de Bruijn graph. We show that our approach uses 30% to 40% less
memory than the state of the art, with an insignificant impact on the
construction time.
  Additionally, we apply the same techniques developed to list bubbles in two
classical problems: cycle enumeration and the K-shortest paths problem. We give
the first optimal algorithm to list cycles in undirected graphs, improving over
Johnson's algorithm. This is the first improvement to this problem in almost 40
years. We then consider a different parameterization of the classical
K-shortest (simple) paths problem: instead of bounding the number of st-paths,
we bound the weight of the st-paths. We present new algorithms with the same
time complexities but using exponentially less memory than previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6084</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6084</id><created>2014-06-23</created><authors><author><keyname>Lam</keyname><forenames>Henry</forenames></author><author><keyname>Liu</keyname><forenames>Zhenming</forenames></author></authors><title>From Black-Scholes to Online Learning: Dynamic Hedging under Adversarial
  Environments</title><categories>cs.DS cs.LG q-fin.PR</categories><acm-class>F.2; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a non-stochastic online learning approach to price financial
options by modeling the market dynamic as a repeated game between the nature
(adversary) and the investor. We demonstrate that such framework yields
analogous structure as the Black-Scholes model, the widely popular option
pricing model in stochastic finance, for both European and American options
with convex payoffs. In the case of non-convex options, we construct
approximate pricing algorithms, and demonstrate that their efficiency can be
analyzed through the introduction of an artificial probability measure, in
parallel to the so-called risk-neutral measure in the finance literature, even
though our framework is completely adversarial. Continuous-time convergence
results and extensions to incorporate price jumps are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6087</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6087</id><created>2014-06-23</created><authors><author><keyname>Lvov</keyname><forenames>Maxim</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author></authors><title>Initialization Algorithms For Convolutional Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms for initializing a convolutional network coding scheme
in networks that may contain cycles. An initialization process is needed if the
network is unknown or if local encoding kernels are chosen randomly. During the
initialization process every source node transmits basis vectors and every sink
node measures the impulse response of the network. The impulse response is then
used to find a relationship between the transmitted and the received symbols,
which is needed for a decoding algorithm and to find the set of all achievable
rates. Unlike acyclic networks, for which it is enough to transmit basis
vectors one after another, the initialization of cyclic networks is more
involved, as pilot symbols interfere with each other and the impulse response
is of infinite duration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6101</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6101</id><created>2014-06-23</created><authors><author><keyname>Trabelsi</keyname><forenames>Imen</forenames></author><author><keyname>Ayed</keyname><forenames>Dorra Ben</forenames></author><author><keyname>Ellouze</keyname><forenames>Noureddine</forenames></author></authors><title>Improved Frame Level Features and SVM Supervectors Approach for the
  Recogniton of Emotional States from Speech: Application to categorical and
  dimensional states</title><categories>cs.CL cs.LG</categories><journal-ref>I.J. Image, Graphics and Signal Processing, 2013, 9, 8-13</journal-ref><doi>10.5815/ijigsp.2013.09.02</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The purpose of speech emotion recognition system is to classify speakers
utterances into different emotional states such as disgust, boredom, sadness,
neutral and happiness. Speech features that are commonly used in speech emotion
recognition rely on global utterance level prosodic features. In our work, we
evaluate the impact of frame level feature extraction. The speech samples are
from Berlin emotional database and the features extracted from these utterances
are energy, different variant of mel frequency cepstrum coefficients, velocity
and acceleration features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6102</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6102</id><created>2014-06-23</created><authors><author><keyname>Wang</keyname><forenames>Kewen</forenames></author><author><keyname>Wen</keyname><forenames>Lian</forenames></author><author><keyname>Mu</keyname><forenames>Kedian</forenames></author></authors><title>Random Logic Programs: Linear Model</title><categories>cs.AI</categories><comments>33 pages. To appear in: Theory and Practice of Logic Programming</comments><report-no>GUICTWK2014-1</report-no><journal-ref>Theory and Practice of Logic Programming 15 (2014) 818-853</journal-ref><doi>10.1017/S1471068414000611</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a model, the linear model, for randomly generating logic
programs with low density of rules and investigates statistical properties of
such random logic programs. It is mathematically shown that the average number
of answer sets for a random program converges to a constant when the number of
atoms approaches infinity. Several experimental results are also reported,
which justify the suitability of the linear model. It is also experimentally
shown that, under this model, the size distribution of answer sets for random
programs tends to a normal distribution when the number of atoms is
sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6114</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6114</id><created>2014-06-23</created><authors><author><keyname>Sripirakas</keyname><forenames>Sakthithasan</forenames></author><author><keyname>Pears</keyname><forenames>Russel</forenames></author></authors><title>Mining Recurrent Concepts in Data Streams using the Discrete Fourier
  Transform</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research we address the problem of capturing recurring concepts in a
data stream environment. Recurrence capture enables the re-use of previously
learned classifiers without the need for re-learning while providing for better
accuracy during the concept recurrence interval. We capture concepts by
applying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to
obtain highly compressed versions of the trees at concept drift points in the
stream and store such trees in a repository for future use. Our empirical
results on real world and synthetic data exhibiting varying degrees of
recurrence show that the Fourier compressed trees are more robust to noise and
are able to capture recurring concepts with higher precision than a meta
learning approach that chooses to re-use classifiers in their originally
occurring form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6126</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6126</id><created>2014-06-23</created><authors><author><keyname>Moore</keyname><forenames>Ross</forenames></author></authors><title>PDF/A-3u as an archival format for Accessible mathematics</title><categories>cs.IR cs.DL</categories><comments>This is a post-print version of original in volume: S.M. Watt et al.
  (Eds.): CICM 2014, LNAI 8543, pp.184-199, 2014; available at
  http://link.springer.com/search?query=LNAI+8543, along with supplementary
  PDF. This version, with supplement as attachment, is enriched to validate as
  PDF/A-3u modulo an error in white-space handling in the pdfTeX version used
  to generate it</comments><msc-class>68U35</msc-class><acm-class>I.7.2; I.7.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Including LaTeX source of mathematical expressions, within the PDF document
of a text-book or research paper, has definite benefits regarding
`Accessibility' considerations. Here we describe three ways in which this can
be done, fully compatibly with international standards ISO 32000, ISO 19005-3,
and the forthcoming ISO 32000-2 (PDF 2.0). Two methods use embedded files, also
known as `attachments', holding information in either LaTeX or MathML formats,
but use different PDF structures to relate these attachments to regions of the
document window. One uses structure, so is applicable to a fully `Tagged PDF'
context, while the other uses /AF tagging of the relevant content. The third
method requires no tagging at all, instead including the source coding as the
/ActualText replacement of a so-called `fake space'. Information provided this
way is extracted via simple Select/Copy/Paste actions, and is available to
existing screen-reading software and assistive technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6130</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6130</id><created>2014-06-23</created><authors><author><keyname>Reid</keyname><forenames>Mark D.</forenames></author><author><keyname>Frongillo</keyname><forenames>Rafael M.</forenames></author><author><keyname>Williamson</keyname><forenames>Robert C.</forenames></author><author><keyname>Mehta</keyname><forenames>Nishant</forenames></author></authors><title>Generalized Mixability via Entropic Duality</title><categories>cs.LG</categories><comments>20 pages, 1 figure. Supersedes the work in arXiv:1403.2433 [cs.LG]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixability is a property of a loss which characterizes when fast convergence
is possible in the game of prediction with expert advice. We show that a key
property of mixability generalizes, and the exp and log operations present in
the usual theory are not as special as one might have thought. In doing this we
introduce a more general notion of $\Phi$-mixability where $\Phi$ is a general
entropy (\ie, any convex function on probabilities). We show how a property
shared by the convex dual of any such entropy yields a natural algorithm (the
minimizer of a regret bound) which, analogous to the classical aggregating
algorithm, is guaranteed a constant regret when used with $\Phi$-mixable
losses. We characterize precisely which $\Phi$ have $\Phi$-mixable losses and
put forward a number of conjectures about the optimality and relationships
between different choices of entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6140</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6140</id><created>2014-06-24</created><updated>2014-10-27</updated><authors><author><keyname>Kulkarni</keyname><forenames>Sadanand A.</forenames></author><author><keyname>Borde</keyname><forenames>Prashant L.</forenames></author><author><keyname>Manza</keyname><forenames>Ramesh R.</forenames></author><author><keyname>Yannawar</keyname><forenames>Pravin L.</forenames></author></authors><title>Offline Handwritten MODI Character Recognition Using HU, Zernike Moments
  and Zoning</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to the paper was
  rejected by journal with a reson &quot;paper was not suitable for the journal&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HOCR is abbreviated as Handwritten Optical Character Recognition. HOCR is a
process of recognition of different handwritten characters from a digital image
of documents. Handwritten automatic character recognition has attracted many
researchers all over the world to contribute handwritten character recognition
domain. Shape identification and feature extraction is very important part of
any character recognition system and success of method is highly dependent on
selection of features. However feature extraction is the most important step in
defining the shape of the character as precisely and as uniquely as possible.
This is indeed the most important step and complex task as well and achieved
success by using invariance property, irrespective of position and orientation.
Zernike moments describes shape, identify rotation invariant due to its
Orthogonality property. MODI is an ancient script of India had cursive and
complex representation of characters. The work described in this paper presents
efficiency of Zernike moments over Hu 7 moment with zoning for automatic
recognition of handwritten MODI characters. Offline approach is used in this
paper because MODI Script was very popular and widely used for writing purpose
till 19th century before Devanagari was officially adopted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6144</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6144</id><created>2014-06-24</created><updated>2015-10-08</updated><authors><author><keyname>Champarnaud</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author><author><keyname>Nicart</keyname><forenames>Florent</forenames></author></authors><title>Constrained Expressions and their Derivatives</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an extension to classical regular expressions by the
addition of two operators allowing the inclusion of boolean formulae from the
zeroth order logic. These expressions are called constrained expressions. The
associated language is defined thanks to the notion of interpretation and of
realization.
  We show that the language associated when both interpretation and realization
are fixed is stricly regular and can be not regular otherwise.
  Furthermore, we use an extension of Antimirov partial derivatives in order to
solve the membership test in the general case. Finally, we show that once the
interpretation is fixed, the membership test of a word in the language denoted
by a constrained expression can be undecidable whereas it is always decidable
when the interpretation is not fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6145</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6145</id><created>2014-06-24</created><authors><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author><author><keyname>Maunu</keyname><forenames>Tyler</forenames></author></authors><title>Fast algorithm for robust subspace recovery</title><categories>cs.LG cs.CV stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a fast algorithm for robust subspace recovery. The
datasets considered include points drawn around a low-dimensional subspace of a
higher dimensional ambient space, and a possibly large portion of points that
do not lie nearby this subspace. The proposed algorithm, which we refer to as
Fast Median Subspace (FMS), is designed to robustly determine the underlying
subspace of such datasets, while having lower computational complexity than
existing methods. Numerical experiments on synthetic and real data demonstrate
its competitive speed and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6146</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6146</id><created>2014-06-24</created><updated>2014-07-09</updated><authors><author><keyname>Ying</keyname><forenames>Shenggang</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>Reachability Analysis of Quantum Markov Decision Processes</title><categories>quant-ph cs.LO</categories><comments>12 pages, 4 figures. Comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of quantum Markov decision process (qMDP) as a
semantic model of nondeterministic and concurrent quantum programs. It is shown
by examples that qMDPs can be used in analysis of quantum algorithms and
protocols. We study various reachability problems of qMDPs both for the
finite-horizon and for the infinite-horizon. The (un)decidability and
complexity of these problems are settled, or their relationships with certain
long-standing open problems are clarified. We also develop an algorithm for
finding optimal scheduler that attains the supremum reachability probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6147</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6147</id><created>2014-06-24</created><authors><author><keyname>Salamati</keyname><forenames>Neda</forenames></author><author><keyname>Larlus</keyname><forenames>Diane</forenames></author><author><keyname>Csurka</keyname><forenames>Gabriela</forenames></author><author><keyname>S&#xfc;sstrunk</keyname><forenames>Sabine</forenames></author></authors><title>Incorporating Near-Infrared Information into Semantic Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in computational photography has shown that we can acquire
near-infrared (NIR) information in addition to the normal visible (RGB) band,
with only slight modifications to standard digital cameras. Due to the
proximity of the NIR band to visible radiation, NIR images share many
properties with visible images. However, as a result of the material dependent
reflection in the NIR part of the spectrum, such images reveal different
characteristics of the scene. We investigate how to effectively exploit these
differences to improve performance on the semantic image segmentation task.
Based on a state-of-the-art segmentation framework and a novel manually
segmented image database (both indoor and outdoor scenes) that contain
4-channel images (RGB+NIR), we study how to best incorporate the specific
characteristics of the NIR response. We show that adding NIR leads to improved
performance for classes that correspond to a specific type of material in both
outdoor and indoor scenes. We also discuss the results with respect to the
physical properties of the NIR response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6158</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6158</id><created>2014-06-24</created><authors><author><keyname>Watanabe</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Nakasato</keyname><forenames>Naohito</forenames></author></authors><title>GPU accelerated Hybrid Tree Algorithm for Collision-less N-body
  Simulations</title><categories>astro-ph.IM cs.DC physics.comp-ph</categories><comments>Paper presented at Fifth International Symposium on Highly-Efficient
  Accelerators and Reconfigurable Technologies (HEART2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a hybrid tree algorithm for reducing calculation and communication
cost of collision-less N-body simulations. The concept of our algorithm is that
we split interaction force into two parts: hard-force from neighbor particles
and soft-force from distant particles, and applying different time integration
for the forces. For hard-force calculation, we can efficiently reduce the
calculation and communication cost of the parallel tree code because we only
need data of neighbor particles for this part. We implement the algorithm on
GPU clusters to accelerate force calculation for both hard and soft force. As
the result of implementing the algorithm on GPU clusters, we were able to
reduce the communication cost and the total execution time to 40% and 80% of
that of a normal tree algorithm, respectively. In addition, the reduction
factor relative the normal tree algorithm is smaller for large number of
processes, and we expect that the execution time can be ultimately reduced down
to about 70% of the normal tree algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6163</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6163</id><created>2014-06-24</created><authors><author><keyname>Hargreaves</keyname><forenames>Felix P.</forenames></author><author><keyname>Merkle</keyname><forenames>Daniel</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>Group Communication Patterns for High Performance Computing in Scala</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed a Functional object-oriented Parallel framework (FooPar) for
high-level high-performance computing in Scala. Central to this framework are
Distributed Memory Parallel Data structures (DPDs), i.e., collections of data
distributed in a shared nothing system together with parallel operations on
these data. In this paper, we first present FooPar's architecture and the idea
of DPDs and group communications. Then, we show how DPDs can be implemented
elegantly and efficiently in Scala based on the Traversable/Builder pattern,
unifying Functional and Object-Oriented Programming. We prove the correctness
and safety of one communication algorithm and show how specification testing
(via ScalaCheck) can be used to bridge the gap between proof and
implementation. Furthermore, we show that the group communication operations of
FooPar outperform those of the MPJ Express open source MPI-bindings for Java,
both asymptotically and empirically. FooPar has already been shown to be
capable of achieving close-to-optimal performance for dense matrix-matrix
multiplication via JNI. In this article, we present results on a parallel
implementation of the Floyd-Warshall algorithm in FooPar, achieving more than
94 % efficiency compared to the serial version on a cluster using 100 cores for
matrices of dimension 38000 x 38000.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6164</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6164</id><created>2014-06-24</created><updated>2014-09-22</updated><authors><author><keyname>Engblom</keyname><forenames>Stefan</forenames></author><author><keyname>Pender</keyname><forenames>Jamol</forenames></author></authors><title>Approximations for the Moments of Nonstationary and State Dependent
  Birth-Death Queues</title><categories>math.NA cs.NA math.DS math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new method for approximating the nonstationary
moment dynamics of one dimensional Markovian birth-death processes. By
expanding the transition probabilities of the Markov process in terms of
Poisson-Charlier polynomials, we are able to estimate any moment of the Markov
process even though the system of moment equations may not be closed. Using new
weighted discrete Sobolev spaces, we derive explicit error bounds of the
transition probabilities and new weak a priori estimates for approximating the
moments of the Markov processs using a truncated form of the expansion. Using
our error bounds and estimates, we are able to show that our approximations
converge to the true stochastic process as we add more terms to the expansion
and give explicit bounds on the truncation error. As a result, we are the first
paper in the queueing literature to provide error bounds and estimates on the
performance of a moment closure approximation. Lastly, we perform several
numerical experiments for some important models in the queueing theory
literature and show that our expansion techniques are accurate at estimating
the moment dynamics of these Markov process with only a few terms of the
expansion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6169</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6169</id><created>2014-06-24</created><authors><author><keyname>Parter</keyname><forenames>Merav</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author></authors><title>Fault Tolerant Approximate BFS Structures</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of designing a {\em fault-tolerant}
$(\alpha, \beta)$ approximate BFS structure (or {\em FT-ABFS structure} for
short), namely, a subgraph $H$ of the network $G$ such that subsequent to the
failure of some subset $F$ of edges or vertices, the surviving part of $H$
still contains an \emph{approximate} BFS spanning tree for (the surviving part
of) $G$, satisfying $dist(s,v,H\setminus F) \leq \alpha \cdot
dist(s,v,G\setminus F)+\beta$ for every $v \in V$. We first consider {\em
multiplicative} $(\alpha,0)$ FT-ABFS structures resilient to a failure of a
single edge and present an algorithm that given an $n$-vertex unweighted
undirected graph $G$ and a source $s$ constructs a $(3,0)$ FT-ABFS structure
rooted at $s$ with at most $4n$ edges (improving by an $O(\log n)$ factor on
the near-tight result of \cite{BS10} for the special case of edge failures).
Assuming at most $f$ edge failures, for constant integer $f&gt;1$, we prove that
there exists a (poly-time constructible) $(3(f+1), (f+1) \log n)$ FT-ABFS
structure with $O(f n)$ edges.
  We then consider {\em additive} $(1,\beta)$ FT-ABFS structures. In contrast
to the linear size of $(\alpha,0)$ FT-ABFS structures, we show that for every
$\beta \in [1, O(\log n)]$ there exists an $n$-vertex graph $G$ with a source
$s$ for which any $(1,\beta)$ FT-ABFS structure rooted at $s$ has
$\Omega(n^{1+\epsilon(\beta)})$ edges, for some function $\epsilon(\beta) \in
(0,1)$. In particular, $(1,3)$ FT-ABFS structures admit a lower bound of
$\Omega(n^{5/4})$ edges. Our lower bounds are complemented by an upper bound,
showing that there exists a poly-time algorithm that for every $n$-vertex
unweighted undirected graph $G$ and source $s$ constructs a $(1,4)$ FT-ABFS
structure rooted at $s$ with at most $O(n^{4/3})$ edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6170</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6170</id><created>2014-06-24</created><authors><author><keyname>Raviv</keyname><forenames>Netanel</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>Distributed Storage Systems based on Equidistant Subspace Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage systems based on equidistant constant dimension codes are
presented. These equidistant codes are based on the Pl\&quot;{u}cker embedding,
which is essential in the repair and the reconstruction algorithms. These
systems posses several useful properties such as high failure resilience,
minimum bandwidth, low storage, simple algebraic repair and reconstruction
algorithms, good locality, and compatibility with small fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6173</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6173</id><created>2014-06-24</created><authors><author><keyname>Matliwala</keyname><forenames>Kirti</forenames><affiliation>Education department, VNSGU, Surat</affiliation></author></authors><title>Use of computer by secondary school students</title><categories>cs.CY</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on the outcomes of a survey implemented in secondary
schools. The survey identified the types of access and use of computers by
students. It was found that the students had significant access to computers
but they were not skilled in many features of computer use. Computers were used
for a range of activities, some educational and others recreational by some
students. Gender differences in computer use were not seen. The study
highlights the changing scenario about uses of computer technology by students.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6176</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6176</id><created>2014-06-24</created><authors><author><keyname>Yasuda</keyname><forenames>Muneki</forenames></author><author><keyname>Kataoka</keyname><forenames>Shun</forenames></author><author><keyname>Waizumi</keyname><forenames>Yuji</forenames></author><author><keyname>Tanaka</keyname><forenames>Kazuyuki</forenames></author></authors><title>Composite Likelihood Estimation for Restricted Boltzmann machines</title><categories>cs.LG</categories><journal-ref>Proceedings of 21st International Conference on Pattern
  Recognition (ICPR2012), pp. 2234-2237, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning the parameters of graphical models using the maximum likelihood
estimation is generally hard which requires an approximation. Maximum composite
likelihood estimations are statistical approximations of the maximum likelihood
estimation which are higher-order generalizations of the maximum
pseudo-likelihood estimation. In this paper, we propose a composite likelihood
method and investigate its property. Furthermore, we apply our composite
likelihood method to restricted Boltzmann machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6200</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6200</id><created>2014-06-24</created><authors><author><keyname>van Ommen</keyname><forenames>Thijs</forenames></author></authors><title>Combining predictions from linear models when training and test inputs
  differ</title><categories>stat.ME cs.LG stat.ML</categories><comments>12 pages, 2 figures. To appear in Proceedings of the 30th Conference
  on Uncertainty in Artificial Intelligence (UAI2014). This version includes
  the supplementary material (regularity assumptions, proofs)</comments><msc-class>62F07 (Primary) 62C12, 62J05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods for combining predictions from different models in a supervised
learning setting must somehow estimate/predict the quality of a model's
predictions at unknown future inputs. Many of these methods (often implicitly)
make the assumption that the test inputs are identical to the training inputs,
which is seldom reasonable. By failing to take into account that prediction
will generally be harder for test inputs that did not occur in the training
set, this leads to the selection of too complex models. Based on a novel,
unbiased expression for KL divergence, we propose XAIC and its special case
FAIC as versions of AIC intended for prediction that use different degrees of
knowledge of the test inputs. Both methods substantially differ from and may
outperform all the known versions of AIC even when the training and test inputs
are iid, and are especially useful for deterministic inputs and under covariate
shift. Our experiments on linear models suggest that if the test and training
inputs differ substantially, then XAIC and FAIC predictively outperform AIC,
BIC and several other methods including Bayesian model averaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6201</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6201</id><created>2014-06-24</created><authors><author><keyname>Lenz</keyname><forenames>Reiner</forenames></author></authors><title>Saccadic Eye Movements and the Generalized Pareto Distribution</title><categories>cs.CV</categories><acm-class>I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a statistical analysis of the eye tracker measurements in a
database with 15 observers viewing 1003 images under free-viewing conditions.
In contrast to the common approach of investigating the properties of the
fixation points we analyze the properties of the transition phases between
fixations. We introduce hyperbolic geometry as a tool to measure the step
length between consecutive eye positions. We show that the step lengths,
measured in hyperbolic and euclidean geometry, follow a generalized Pareto
distribution. The results based on the hyperbolic distance are more robust than
those based on euclidean geometry. We show how the structure of the space of
generalized Pareto distributions can be used to characterize and identify
individual observers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6210</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6210</id><created>2014-06-24</created><updated>2014-11-10</updated><authors><author><keyname>Zhang</keyname><forenames>Yijin</forenames></author><author><keyname>Lo</keyname><forenames>Yuan-Hsun</forenames></author><author><keyname>Wong</keyname><forenames>Wing Shing</forenames></author></authors><title>Optimal strongly conflict-avoiding codes of even length and weight three</title><categories>cs.IT math.CO math.IT</categories><comments>18 pages, 1 figure. Submitted to Designs, Codes and Cryptography. 1st
  revision</comments><msc-class>94B25, 94C30, 11A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strongly conflict-avoiding codes (SCACs) are employed in a slot-asynchronous
multiple-access collision channel without feedback to guarantee that each
active user can send at least one packet successfully in the worst case within
a fixed period of time. Assume all users are assigned distinct codewords, the
number of codewords in an SCAC is equal to the number of potential users that
can be supported. SCACs have different combinatorial structure compared with
conflict-avoiding codes (CACs) due to additional collisions incurred by
partially overlapped transmissions. In this paper, we establish upper bounds on
the size of SCACs of even length and weight three. Furthermore, it is shown
that some optimal CACs can be used to construct optimal SCACs of weight three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6218</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6218</id><created>2014-06-24</created><updated>2015-04-23</updated><authors><author><keyname>Fechner</keyname><forenames>Uwe</forenames></author><author><keyname>van der Vlugt</keyname><forenames>Rolf</forenames></author><author><keyname>Schreuder</keyname><forenames>Edwin</forenames></author><author><keyname>Schmehl</keyname><forenames>Roland</forenames></author></authors><title>Dynamic Model of a Pumping Kite Power System</title><categories>cs.SY</categories><comments>13 pages, 8 figures. This revision was accepted by the journal
  &quot;Renewable Energy&quot; and takes the comments of the reviewers into account. The
  abstract and the conclusions state more carefully the advantages of the
  suggested models. In addition, the wording of the results section was
  improved. Finally, the changes from passive to active voice of the last
  revision were reverted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Converting the traction power of kites into electricity can be a low cost
solution for wind energy. Reliable control of both trajectory and tether
reeling is crucial. The present study proposes a modelling framework describing
the dynamic behaviour of the interconnected system components, suitable for
design and optimization of the control systems. The wing, bridle, airborne
control unit and tether are represented as a particle system using
spring-damper elements to describe their mechanical properties. Two kite models
are proposed: a point mass model and a four point model. Reeling of the tether
is modelled by varying the lengths of constituent tether elements. Dynamic
behaviour of the ground station is included. The framework is validated by
combining it with the automatic control system used for the operation of a kite
power system demonstrator. The simulation results show that the point mass
model can be adjusted to match the measured behaviour during a pumping cycle.
The four point model can better predict the influence of gravity and inertia on
the steering response and remains stable also at low tether forces. Compared to
simple one point models, the proposed framework is more accurate and robust
while allowing real-time simulations of the complete system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6221</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6221</id><created>2014-06-24</created><authors><author><keyname>Masram</keyname><forenames>Ranjeet</forenames></author><author><keyname>Shahare</keyname><forenames>Vivek</forenames></author><author><keyname>Abraham</keyname><forenames>Jibi</forenames></author><author><keyname>Moona</keyname><forenames>Rajni</forenames></author><author><keyname>Sinha</keyname><forenames>Pradeep</forenames></author><author><keyname>Sunder</keyname><forenames>Gaur</forenames></author><author><keyname>Bendale</keyname><forenames>Prashant</forenames></author><author><keyname>Pophalkar</keyname><forenames>Sayali</forenames></author></authors><title>Dynamic Selection of Symmetric Key Cryptographic Algorithms for Securing
  Data Based on Various Parameters</title><categories>cs.CR</categories><comments>8 pages, 4 figures, Fifth International Conference on Communications
  Security &amp; Information Assurance (CSIA 2014) May 24~25, 2014, Delhi, India</comments><doi>10.5121/csit.2014.4519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the information is in the form of electronic data. A lot of
electronic data exchanged takes place through computer applications. Therefore
information exchange through these applications needs to be secure. Different
cryptographic algorithms are usually used to address these security concerns.
However, along with security there are other factors that need to be considered
for practical implementation of different cryptographic algorithms like
implementation cost and performance. This paper provides comparative analysis
of time taken for encryption by seven symmetric key cryptographic algorithms
(AES, DES, Triple DES, RC2, Skipjack, Blowfish and RC4) with variation of
parameters like different data types, data density, data size and key sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6228</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6228</id><created>2014-06-24</created><updated>2015-12-30</updated><authors><author><keyname>Klav&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Saumell</keyname><forenames>Maria</forenames></author></authors><title>Minimal Obstructions for Partial Representations of Interval Graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interval graphs are intersection graphs of closed intervals. A generalization
of recognition called partial representation extension was introduced recently.
The input gives an interval graph with a partial representation specifying some
pre-drawn intervals. We ask whether the remaining intervals can be added to
create an extending representation. Two linear-time algorithms are known for
solving this problem.
  In this paper, we characterize the minimal obstructions which make partial
representations non-extendible. This generalizes Lekkerkerker and Boland's
characterization of the minimal forbidden induced subgraphs of interval graphs.
Each minimal obstruction consists of a forbidden induced subgraph together with
at most four pre-drawn intervals. A Helly-type result follows: A partial
representation is extendible if and only if every quadruple of pre-drawn
intervals is extendible by itself. Our characterization leads to a linear-time
certifying algorithm for partial representation extension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6247</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6247</id><created>2014-06-24</created><authors><author><keyname>Mnih</keyname><forenames>Volodymyr</forenames></author><author><keyname>Heess</keyname><forenames>Nicolas</forenames></author><author><keyname>Graves</keyname><forenames>Alex</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author></authors><title>Recurrent Models of Visual Attention</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying convolutional neural networks to large images is computationally
expensive because the amount of computation scales linearly with the number of
image pixels. We present a novel recurrent neural network model that is capable
of extracting information from an image or video by adaptively selecting a
sequence of regions or locations and only processing the selected regions at
high resolution. Like convolutional neural networks, the proposed model has a
degree of translation invariance built-in, but the amount of computation it
performs can be controlled independently of the input image size. While the
model is non-differentiable, it can be trained using reinforcement learning
methods to learn task-specific policies. We evaluate our model on several image
classification tasks, where it significantly outperforms a convolutional neural
network baseline on cluttered images, and on a dynamic visual control problem,
where it learns to track a simple object without an explicit training signal
for doing so.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6259</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6259</id><created>2014-06-24</created><updated>2014-08-25</updated><authors><author><keyname>Virtema</keyname><forenames>Jonni</forenames><affiliation>Japan Advanced Institute of Science and Technology and University of Tampere</affiliation></author></authors><title>Complexity of validity for propositional dependence logics</title><categories>cs.LO cs.CC</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 18-31</journal-ref><doi>10.4204/EPTCS.161.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the validity problem for propositional dependence logic, modal
dependence logic and extended modal dependence logic. We show that the validity
problem for propositional dependence logic is NEXPTIME-complete. In addition,
we establish that the corresponding problem for modal dependence logic and
extended modal dependence logic is NEXPTIME-hard and in NEXPTIME^NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6265</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6265</id><created>2014-06-24</created><authors><author><keyname>Tapparello</keyname><forenames>Cristiano</forenames></author><author><keyname>Ayatollahi</keyname><forenames>Hoda</forenames></author><author><keyname>Heinzelman</keyname><forenames>Wendi</forenames></author></authors><title>Extending the Energy Framework for Network Simulator 3 (ns-3)</title><categories>cs.NI</categories><comments>2 pages, 4 figures. Poster presented at WNS3 2014, Atlanta, GA</comments><doi>10.1145/2675683.2675685</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of designing and simulating optimal transmission protocols for
energy harvesting wireless networks has recently received considerable
attention, thus requiring for an accurate modeling of the energy harvesting
process and a consequent redesign of the simulation framework to include it.
While the current ns-3 energy framework allows the definition of new energy
sources that incorporate the contribution of an energy harvester, the
integration of an energy harvester component into an existing energy source is
not straightforward using the existing energy framework. In this poster, we
propose an extension of the energy framework currently released with ns-3 in
order to explicitly introduce the concept of an energy harvester. Starting from
the definition of the general interface, we then provide the implementation of
two simple models for the energy harvester. In addition, we extend the set of
implementations of the current energy framework to include a model for a
supercapacitor energy source and a device energy model for the energy
consumption of a sensor. Finally, we introduce the concept of an energy
predictor, that gathers information from the energy source and harvester and
use this information to predict the amount of energy that will be available in
the future, and we provide an example implementation. As a result of these
efforts, we believe that our contributions to the ns-3 energy framework will
provide a useful tool to enhance the quality of simulations of energy-aware
wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6266</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6266</id><created>2014-06-24</created><authors><author><keyname>Hella</keyname><forenames>Lauri</forenames></author><author><keyname>Luosto</keyname><forenames>Kerkko</forenames></author><author><keyname>Sano</keyname><forenames>Katsuhiko</forenames></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames></author></authors><title>The Expressive Power of Modal Dependence Logic</title><categories>cs.LO math.LO</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the expressive power of various modal logics with team semantics. We
show that exactly the properties of teams that are downward closed and closed
under team k-bisimulation, for some finite k, are definable in modal logic
extended with intuitionistic disjunction. Furthermore, we show that the
expressive power of modal logic with intuitionistic disjunction and extended
modal dependence logic coincide. Finally we establish that any translation from
extended modal dependence logic into modal logic with intuitionistic
disjunction increases the size of some formulas exponentially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6268</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6268</id><created>2014-06-24</created><updated>2014-10-17</updated><authors><author><keyname>Forssell</keyname><forenames>Henrik</forenames></author><author><keyname>Gylterud</keyname><forenames>H&#xe5;kon Robbestad</forenames></author><author><keyname>Spivak</keyname><forenames>David I.</forenames></author></authors><title>Type theoretical databases</title><categories>math.LO cs.DB</categories><msc-class>03B15</msc-class><acm-class>F.4.1; H.2.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a soundness theorem for a dependent type theory with context
constants with respect to an indexed category of (finite, abstract) simplical
complexes. The point of interest for computer science is that this category can
be seen to represent tables in a natural way. Thus the category is a model for
databases, a single mathematical structure in which all database schemas and
instances (of a suitable, but sufficiently general form) are represented. The
type theory then allows for the specification of database schemas and
instances, the manipulation of the same with the usual type-theoretic
operations, and the posing of queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6270</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6270</id><created>2014-06-24</created><updated>2014-07-10</updated><authors><author><keyname>Blaum</keyname><forenames>Mario</forenames></author><author><keyname>Hetzler</keyname><forenames>Steven</forenames></author></authors><title>Generalized Concatenated Types of Codes for Erasure Correction</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized Concatenated (GC), also known as Integrated Interleaved (II)
Codes, are studied from an erasure correction point of view making them useful
for Redundant Arrays of Independent Disks (RAID) types of architectures
combining global and local properties. The fundamental erasure-correcting
properties of the codes are proven and efficient encoding and decoding
algorithms are provided. Although less powerful than the recently developed
PMDS codes, this implementation has the advantage of allowing generalization to
any range of parameters while the size of the field is much smaller than the
one required for PMDS codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6273</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6273</id><created>2014-06-24</created><authors><author><keyname>Habigt</keyname><forenames>Julian</forenames></author><author><keyname>Diepold</keyname><forenames>Klaus</forenames></author></authors><title>Image Completion for View Synthesis Using Markov Random Fields and
  Efficient Belief Propagation</title><categories>cs.CV</categories><comments>Published version:
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6738439</comments><journal-ref>Proc. 20th IEEE International Conference on Image Processing
  (2013) 2131-2134</journal-ref><doi>10.1109/ICIP.2013.6738439</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  View synthesis is a process for generating novel views from a scene which has
been recorded with a 3-D camera setup. It has important applications in 3-D
post-production and 2-D to 3-D conversion. However, a central problem in the
generation of novel views lies in the handling of disocclusions. Background
content, which was occluded in the original view, may become unveiled in the
synthesized view. This leads to missing information in the generated view which
has to be filled in a visually plausible manner. We present an inpainting
algorithm for disocclusion filling in synthesized views based on Markov random
fields and efficient belief propagation. We compare the result to two
state-of-the-art algorithms and demonstrate a significant improvement in image
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6277</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6277</id><created>2014-06-20</created><authors><author><keyname>Gonz&#xe1;lez-Valiente</keyname><forenames>C. L.</forenames></author></authors><title>Midiendo la calidad de la informacion gestionada: algunas reflexiones
  conceptuales-metodologicas</title><categories>cs.OH</categories><comments>in Spanish</comments><journal-ref>Biblios. 2014; 52, 27-35</journal-ref><doi>10.5195/biblios.2014.101</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The study, based on a documental classic analysis, presents conceptual and
methodological guidelines concerning the design of methodologies that help to
measure the quality of information that is managed in organizations. It is
described the process of information management and the importance of
implementing quality principles in it. There are exposed the four dimensions of
information quality as part of an indicators integration which characterize the
informational contents. There are defined each of the phases in the
methodological design to evaluate the information. There also are indicated the
implications of this activity for information professionals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6281</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6281</id><created>2014-06-24</created><authors><author><keyname>Bonne</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Alamir</keyname><forenames>Mazen</forenames></author><author><keyname>Bonnay</keyname><forenames>Patrick</forenames></author></authors><title>Experimental Investigation of Control Updating Period Monitoring In
  Industrial PLC-based Fast MPC: Application to The Constrained Control of a
  Cryogenic Refrigerator</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a complete industrial validation of a recently published
scheme for on-line adaptation of the control updating period in Model
Predictive Control is proposed. The industrial process that serves in the
validation is a cryogenic refrigerator that is used to cool the
supra-conductors involved in particle accelerators or experimental nuclear
reactors. Two recently predicted features are validated: the first states that
it is sometimes better to use less efficient (per iteration) optimizer if the
lack of efficiency is over-compensated by an increase in the updating control
frequency. The second is that for a given solver, it is worth monitoring the
control updating period based on the on-line measured behavior of the cost
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6291</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6291</id><created>2014-06-24</created><authors><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author><author><keyname>Dionne</keyname><forenames>Shelley D.</forenames></author></authors><title>Studying Collective Human Decision Making and Creativity with
  Evolutionary Computation</title><categories>cs.NE cs.MA nlin.AO</categories><comments>20 pages, 7 figures, 1 table (Supplemental materials not included)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report a summary of our interdisciplinary research project &quot;Evolutionary
Perspective on Collective Decision Making&quot; that was conducted through close
collaboration between computational, organizational and social scientists at
Binghamton University. We redefined collective human decision making and
creativity as evolution of ecologies of ideas, where populations of ideas
evolve via continual applications of evolutionary operators such as
reproduction, recombination, mutation, selection, and migration of ideas, each
conducted by participating humans. Based on this evolutionary perspective, we
generated hypotheses about collective human decision making using agent-based
computer simulations. The hypotheses were then tested through several
experiments with real human subjects. Throughout this project, we utilized
evolutionary computation (EC) in non-traditional ways---(1) as a theoretical
framework for reinterpreting the dynamics of idea generation and selection, (2)
as a computational simulation model of collective human decision making
processes, and (3) as a research tool for collecting high-resolution
experimental data of actual collaborative design and decision making from human
subjects. We believe our work demonstrates untapped potential of EC for
interdisciplinary research involving human and social dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6298</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6298</id><created>2014-06-24</created><updated>2015-01-13</updated><authors><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>Huang</keyname><forenames>Shenwei</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Bounding Clique-Width via Perfect Graphs</title><categories>cs.DM math.CO</categories><comments>22 Pages, 2 figures, An extended abstract of this paper will appear
  in the proceedings of LATA 2015 (LNCS vol. 8977)</comments><msc-class>05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two graphs $H_1$ and $H_2$, a graph $G$ is $(H_1,H_2)$-free if it
contains no subgraph isomorphic to $H_1$ or $H_2$. We continue a recent study
into the clique-width of $(H_1,H_2)$-free graphs and present three new classes
of $(H_1,H_2)$-free graphs of bounded clique-width and one of unbounded
clique-width. The four new graph classes have in common that one of their two
forbidden induced subgraphs is the diamond (the graph obtained from a clique on
four vertices by deleting one edge). To prove boundedness of clique-width for
the first three cases we develop a technique based on bounding clique covering
number in combination with reduction to subclasses of perfect graphs. We extend
our proof of unboundedness for the fourth case to show that Graph Isomorphism
is Graph Isomorphism-complete on the same graph class. We also show the
implications of our results for the computational complexity of the Colouring
problem restricted to $(H_1,H_2)$-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6304</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6304</id><created>2014-06-24</created><authors><author><keyname>Ploumidis</keyname><forenames>Manolis</forenames></author><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Traganitis</keyname><forenames>Apostolos</forenames></author></authors><title>TOFRA: Throughput Optimal Flow Rate Allocation with Bounded Delay for
  Random Access Wireless Mesh Networks</title><categories>cs.NI</categories><comments>Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider random access wireless multi-hop mesh networks with
multi-packet reception capabilities where multiple flows are forwarded to the
gateways through node disjoint paths. We address the issue of aggregate
throughput optimal flow rate allocation with bounded packet delay guarantees
for flows exhibiting both intra- and inter-path interference. We propose a
distributed flow rate allocation scheme that formulates flow rate allocation as
an optimization problem and derive the conditions for non-convexity for an
illustrative topology. Furthermore we employ a simple model for the average
aggregate throughput achieved by all flows that captures both intra- and
inter-path interference. The proposed scheme is evaluated through NS-2
simulations of several random wireless scenarios. Simulation results reveal
that our model accurately captures the average aggregate throughput achieved by
all flows for all random scenarios employed. We also compare in terms of
average aggregate throughput our flow allocation scheme with the following
schemes: a scheme that assigns flows on paths on a round-robin fashion, one
that optimally utilizes the best path only and another one that assigns the
maximum possible flow on each path. Simulation results show that our scheme
achieves significantly higher average aggregate throughput from these schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6312</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6312</id><created>2014-06-24</created><updated>2014-11-18</updated><authors><author><keyname>El-Kishky</keyname><forenames>Ahmed</forenames></author><author><keyname>Song</keyname><forenames>Yanglei</forenames></author><author><keyname>Wang</keyname><forenames>Chi</forenames></author><author><keyname>Voss</keyname><forenames>Clare</forenames></author><author><keyname>Han</keyname><forenames>Jiawei</forenames></author></authors><title>Scalable Topical Phrase Mining from Text Corpora</title><categories>cs.CL cs.IR cs.LG</categories><journal-ref>Proceedings of the VLDB Endowment, Vol. 8(3), pp. 305 - 316, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While most topic modeling algorithms model text corpora with unigrams, human
interpretation often relies on inherent grouping of terms into phrases. As
such, we consider the problem of discovering topical phrases of mixed lengths.
Existing work either performs post processing to the inference results of
unigram-based topic models, or utilizes complex n-gram-discovery topic models.
These methods generally produce low-quality topical phrases or suffer from poor
scalability on even moderately-sized datasets. We propose a different approach
that is both computationally efficient and effective. Our solution combines a
novel phrase mining framework to segment a document into single and multi-word
phrases, and a new topic model that operates on the induced document partition.
Our approach discovers high quality topical phrases with negligible extra cost
to the bag-of-words topic model in a variety of datasets including research
publication titles, abstracts, reviews, and news articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6314</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6314</id><created>2014-06-22</created><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Nock</keyname><forenames>Richard</forenames></author></authors><title>Further heuristics for $k$-means: The merge-and-split heuristic and the
  $(k,l)$-means</title><categories>cs.LG cs.CV cs.IR stat.ML</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the optimal $k$-means clustering is NP-hard in general and many
heuristics have been designed for minimizing monotonically the $k$-means
objective. We first show how to extend Lloyd's batched relocation heuristic and
Hartigan's single-point relocation heuristic to take into account empty-cluster
and single-point cluster events, respectively. Those events tend to
increasingly occur when $k$ or $d$ increases, or when performing several
restarts. First, we show that those special events are a blessing because they
allow to partially re-seed some cluster centers while further minimizing the
$k$-means objective function. Second, we describe a novel heuristic,
merge-and-split $k$-means, that consists in merging two clusters and splitting
this merged cluster again with two new centers provided it improves the
$k$-means objective. This novel heuristic can improve Hartigan's $k$-means when
it has converged to a local minimum. We show empirically that this
merge-and-split $k$-means improves over the Hartigan's heuristic which is the
{\em de facto} method of choice. Finally, we propose the $(k,l)$-means
objective that generalizes the $k$-means objective by associating the data
points to their $l$ closest cluster centers, and show how to either directly
convert or iteratively relax the $(k,l)$-means into a $k$-means in order to
reach better local minima.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6321</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6321</id><created>2014-06-24</created><authors><author><keyname>Ashour</keyname><forenames>Mahmoud</forenames></author><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Power-Optimal Feedback-Based Random Spectrum Access for an Energy
  Harvesting Cognitive User</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study and analyze cognitive radio networks in which
secondary users (SUs) are equipped with Energy Harvesting (EH) capability. We
design a random spectrum sensing and access protocol for the SU that exploits
the primary link's feedback and requires less average sensing time. Unlike
previous works proposed earlier in literature, we do not assume perfect
feedback. Instead, we take into account the more practical possibilities of
overhearing unreliable feedback signals and accommodate spectrum sensing
errors. Moreover, we assume an interference-based channel model where the
receivers are equipped with multi-packet reception (MPR) capability.
Furthermore, we perform power allocation at the SU with the objective of
maximizing the secondary throughput under constraints that maintain certain
quality-of-service (QoS) measures for the primary user (PU).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6322</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6322</id><created>2014-06-24</created><updated>2015-10-02</updated><authors><author><keyname>Calv&#xe3;o</keyname><forenames>Angelo Mondaini</forenames></author><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author><author><keyname>Anteneodo</keyname><forenames>Celia</forenames></author></authors><title>Stylized facts in Brazilian vote distributions</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an</categories><comments>8 pages, 8 figures, a version with some modifications was published
  in PLoS ONE</comments><journal-ref>PLoS ONE 10(9): e0137732 (2015)</journal-ref><doi>10.1371/journal.pone.0137732</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elections, specially in countries such as Brazil with an electorate of the
order of 100 million people, yield large-scale data-sets embodying valuable
information on the dynamics through which individuals influence each other and
make choices. In this work we perform an extensive analysis of data sets
available for Brazilian proportional elections of legislators and city
councillors throughout the period 1970-2012, which embraces two distinct
political regimes: a military dictatorship and a democratic phase. Through the
distribution $P(v)$ of the number of candidates receiving $v$ votes, we perform
a comparative analysis of different elections in the same calendar and as a
function of time. The distributions $P(v)$ present a scale-free regime with a
power-law exponent $\alpha$ which is not universal and appears to be
characteristic of the electorate. Moreover, we observe that $\alpha$ typically
increases with time. We propose a multi-species model consisting in a system of
nonlinear differential equations with stochastic parameters that allows to
understand the empirical observations. We conclude that the power-law exponent
$\alpha$ constitutes a measure of the degree of feedback of the electorate
interactions. To know the interactivity of the population is relevant beyond
the context of elections, since a similar feedback may occur in other social
contagion processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6323</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6323</id><created>2014-06-24</created><authors><author><keyname>Tau</keyname><forenames>Moria</forenames></author><author><keyname>Hassner</keyname><forenames>Tal</forenames></author></authors><title>Dense Correspondences Across Scenes and Scales</title><categories>cs.CV</categories><comments>A longer version of this paper is in submission. Please see author
  homepage for an updated version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seek a practical method for establishing dense correspondences between two
images with similar content, but possibly different 3D scenes. One of the
challenges in designing such a system is the local scale differences of objects
appearing in the two images. Previous methods often considered only small
subsets of image pixels; matching only pixels for which stable scales may be
reliably estimated. More recently, others have considered dense
correspondences, but with substantial costs associated with generating, storing
and matching scale invariant descriptors. Our work here is motivated by the
observation that pixels in the image have contexts -- the pixels around them --
which may be exploited in order to estimate local scales reliably and
repeatably. Specifically, we make the following contributions. (i) We show that
scales estimated in sparse interest points may be propagated to neighboring
pixels where this information cannot be reliably determined. Doing so allows
scale invariant descriptors to be extracted anywhere in the image, not just in
detected interest points. (ii) We present three different means for propagating
this information: using only the scales at detected interest points, using the
underlying image information to guide the propagation of this information
across each image, separately, and using both images simultaneously. Finally,
(iii), we provide extensive results, both qualitative and quantitative,
demonstrating that accurate dense correspondences can be obtained even between
very different images, with little computational costs beyond those required by
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6332</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6332</id><created>2014-06-24</created><authors><author><keyname>Ku&#x159;&#xe1;tko</keyname><forenames>Jan</forenames></author><author><keyname>Ratschan</keyname><forenames>Stefan</forenames></author></authors><title>Combined Global and Local Search for the Falsification of Hybrid Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we solve the problem of finding a trajectory that shows that a
given hybrid dynamical system with deterministic evolution leaves a given set
of states considered to be safe. The algorithm combines local with global
search for achieving both efficiency and global convergence. In local search,
it exploits derivatives for efficient computation. Unlike other methods for
falsification of hybrid systems with deterministic evolution, we do not
restrict our search to trajectories of a certain bounded length but search for
error trajectories of arbitrary length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6336</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6336</id><created>2014-06-24</created><authors><author><keyname>Oliva</keyname><forenames>Diego</forenames></author><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Pajares</keyname><forenames>Gonzalo</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Osuna</keyname><forenames>Valentin</forenames></author></authors><title>A multilevel thresholding algorithm using Electromagnetism Optimization</title><categories>cs.CV</categories><comments>The figures have been shortened in order to fulfill the submission
  requirements</comments><journal-ref>Neurocomputing, 139, (2014), 357-381</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmentation is one of the most important tasks in image processing. It
consist in classify the pixels into two or more groups depending on their
intensity levels and a threshold value. The quality of the segmentation depends
on the method applied to select the threshold. The use of the classical
implementations for multilevel thresholding is computationally expensive since
they exhaustively search the best values to optimize the objective function.
Under such conditions, the use of optimization evolutionary approaches has been
extended. The Electromagnetism Like algorithm (EMO) is an evolutionary method
which mimics the attraction repulsion mechanism among charges to evolve the
members of a population. Different to other algorithms, EMO exhibits
interesting search capabilities whereas maintains a low computational overhead.
In this paper, a multilevel thresholding (MT) algorithm based on the EMO is
introduced. The approach combines the good search capabilities of EMO algorithm
with objective functions proposed by the popular MT methods of Otsu and Kapur.
The algorithm takes random samples from a feasible search space inside the
image histogram. Such samples build each particle in the EMO context whereas
its quality is evaluated considering the objective that is function employed by
the Otsu or Kapur method. Guided by these objective values the set of candidate
solutions are evolved through the EMO operators until an optimal solution is
found. The approach generates a multilevel segmentation algorithm which can
effectively identify the threshold values of a digital image in a reduced
number of iterations. Experimental results show performance evidence of the
implementation of EMO for digital image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6341</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6341</id><created>2014-06-24</created><updated>2014-06-28</updated><authors><author><keyname>Barton</keyname><forenames>Carl</forenames></author><author><keyname>Heliou</keyname><forenames>Alice</forenames></author><author><keyname>Mouchard</keyname><forenames>Laurent</forenames></author><author><keyname>Pissis</keyname><forenames>Solon P.</forenames></author></authors><title>Linear-time Computation of Minimal Absent Words Using Suffix Array</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An absent word of a word y of length n is a word that does not occur in y. It
is a minimal absent word if all its proper factors occur in y. Minimal absent
words have been computed in genomes of organisms from all domains of life;
their computation provides a fast alternative for measuring approximation in
sequence comparison. There exists an O(n)-time and O(n)-space algorithm for
computing all minimal absent words on a fixed-sized alphabet based on the
construction of suffix automata (Crochemore et al., 1998). No implementation of
this algorithm is publicly available. There also exists an O(n^2)-time and
O(n)-space algorithm for the same problem based on the construction of suffix
arrays (Pinho et al., 2009). An implementation of this algorithm was also
provided by the authors and is currently the fastest available. In this
article, we bridge this unpleasant gap by presenting an O(n)-time and
O(n)-space algorithm for computing all minimal absent words based on the
construction of suffix arrays. Experimental results using real and synthetic
data show that the respective implementation outperforms the one by Pinho et
al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6353</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6353</id><created>2014-06-24</created><authors><author><keyname>Hsieh</keyname><forenames>Samuel C.</forenames></author></authors><title>A Lower Bound of $2^n$ Conditional Branches for Boolean Satisfiability
  on Post Machines</title><categories>cs.CC</categories><comments>This article draws heavily from arXiv:1406.5970</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a lower bound of $2^n$ conditional branches for deciding the
satisfiability of the conjunction of any two Boolean formulas from a set called
a full representation of Boolean functions of $n$ variables - a set containing
a Boolean formula to represent each Boolean function of $n$ variables. The
contradiction proof first assumes that there exists a Post machine (Post's
Formulation 1) that correctly decides the satisfiability of the conjunction of
any two Boolean formulas from such a set by following an execution path that
includes fewer than $2^n$ conditional branches. By using multiple runs of this
Post machine, with one run for each Boolean function of $n$ variables, the
proof derives a contradiction by showing that this Post machine is unable to
correctly decide the satisfiability of the conjunction of at least one pair of
Boolean formulas from a full representation of $n$-variable Boolean functions
if the machine executes fewer than $2^n$ conditional branches. This lower bound
of $2^n$ conditional branches holds for any full representation of Boolean
functions of $n$ variables, even if a full representation consists solely of
minimized Boolean formulas derived by a Boolean minimization method. We discuss
why the lower bound fails to hold for satisfiability of certain restricted
formulas, such as 2CNF satisfiability, XOR-SAT, and HORN-SAT. We also relate
the lower bound to 3CNF satisfiability. The lower bound does not depend on
sequentiality of access to the boxes in the symbol space and will hold even if
a machine is capable of non-sequential access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6355</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6355</id><created>2014-06-24</created><updated>2015-10-26</updated><authors><author><keyname>Huang</keyname><forenames>Yichen</forenames></author></authors><title>A polynomial-time algorithm for the ground state of one-dimensional
  gapped Hamiltonians</title><categories>cond-mat.str-el cs.DS quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A (deterministic) polynomial-time algorithm is proposed for approximating the
ground state of (general) one-dimensional gapped Hamiltonians. Let
$\epsilon,n,\eta$ be the energy gap, the system size, and the desired
precision, respectively. Neglecting $\epsilon$-dependent subpolynomial (in $n$)
and constant factors, the running time of the algorithm is $n^{O(1)}$ for
$\eta=n^{-O(1)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6390</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6390</id><created>2014-06-24</created><authors><author><keyname>Moon</keyname><forenames>Kevin R.</forenames></author><author><keyname>Li</keyname><forenames>Jimmy J.</forenames></author><author><keyname>Delouille</keyname><forenames>Veronique</forenames></author><author><keyname>Watson</keyname><forenames>Fraser</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Image patch analysis and clustering of sunspots: a dimensionality
  reduction approach</title><categories>cs.CV astro-ph.SR</categories><comments>5 pages, 7 figures, accepted to ICIP 2014</comments><journal-ref>K.R. Moon, J.J. Li, V. Delouille, F. Watson, and A.O. Hero III,
  &quot;Image patch analysis and clustering of sunspots: a dimensionality reduction
  approach,&quot; In Image Processing (ICIP), 2014 IEEE Conference on, pp.
  1623-1627, 2014</journal-ref><doi>10.1109/ICIP.2014.7025325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sunspots, as seen in white light or continuum images, are associated with
regions of high magnetic activity on the Sun, visible on magnetogram images.
Their complexity is correlated with explosive solar activity and so classifying
these active regions is useful for predicting future solar activity. Current
classification of sunspot groups is visually based and suffers from bias.
Supervised learning methods can reduce human bias but fail to optimally
capitalize on the information present in sunspot images. This paper uses two
image modalities (continuum and magnetogram) to characterize the spatial and
modal interactions of sunspot and magnetic active region images and presents a
new approach to cluster the images. Specifically, in the framework of image
patch analysis, we estimate the number of intrinsic parameters required to
describe the spatial and modal dependencies, the correlation between the two
modalities and the corresponding spatial patterns, and examine the phenomena at
different scales within the images. To do this, we use linear and nonlinear
intrinsic dimension estimators, canonical correlation analysis, and
multiresolution analysis of intrinsic dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6393</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6393</id><created>2014-06-24</created><updated>2014-06-26</updated><authors><author><keyname>Ciancia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Latella</keyname><forenames>Diego</forenames></author><author><keyname>Loreti</keyname><forenames>Michele</forenames></author><author><keyname>Massink</keyname><forenames>Mieke</forenames></author></authors><title>Specifying and Verifying Properties of Space - Extended Version</title><categories>cs.LO</categories><comments>Presented at &quot;Theoretical Computer Science&quot; 2014, Rome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interplay between process behaviour and spatial aspects of computation
has become more and more relevant in Computer Science, especially in the field
of collective adaptive systems, but also, more generally, when dealing with
systems distributed in physical space. Traditional verification techniques are
well suited to analyse the temporal evolution of programs; properties of space
are typically not explicitly taken into account. We propose a methodology to
verify properties depending upon physical space. We define an appropriate
logic, stemming from the tradition of topological interpretations of modal
logics, dating back to earlier logicians such as Tarski, where modalities
describe neighbourhood. We lift the topological definitions to a more general
setting, also encompassing discrete, graph-based structures. We further extend
the framework with a spatial until operator, and define an efficient model
checking procedure, implemented in a proof-of-concept tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6398</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6398</id><created>2014-06-24</created><authors><author><keyname>Ackerman</keyname><forenames>Margareta</forenames></author><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author></authors><title>Incremental Clustering: The Case for Extra Clusters</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosion in the amount of data available for analysis often necessitates
a transition from batch to incremental clustering methods, which process one
element at a time and typically store only a small subset of the data. In this
paper, we initiate the formal analysis of incremental clustering methods
focusing on the types of cluster structure that they are able to detect. We
find that the incremental setting is strictly weaker than the batch model,
proving that a fundamental class of cluster structures that can readily be
detected in the batch setting is impossible to identify using any incremental
method. Furthermore, we show how the limitations of incremental clustering can
be overcome by allowing additional clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6413</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6413</id><created>2014-06-24</created><updated>2015-12-27</updated><authors><author><keyname>Bul&#xed;n</keyname><forenames>Jakub</forenames><affiliation>Charles University, Prague</affiliation></author><author><keyname>Delic</keyname><forenames>Dejan</forenames><affiliation>Ryerson University</affiliation></author><author><keyname>Jackson</keyname><forenames>Marcel</forenames><affiliation>La Trobe University</affiliation></author><author><keyname>Niven</keyname><forenames>Todd</forenames><affiliation>La Trobe University</affiliation></author></authors><title>A finer reduction of constraint problems to digraphs</title><categories>cs.CC cs.LO math.CO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1305.2039</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:18) 2015</journal-ref><doi>10.2168/LMCS-11(4:18)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that the constraint satisfaction problem over a general
relational structure A is polynomial time equivalent to the constraint problem
over some associated digraph. We present a variant of this construction and
show that the corresponding constraint satisfaction problem is logspace
equivalent to that over A. Moreover, we show that almost all of the commonly
encountered polymorphism properties are held equivalently on the A and the
constructed digraph. As a consequence, the Algebraic CSP dichotomy conjecture
as well as the conjectures characterizing CSPs solvable in logspace and in
nondeterministic logspace are equivalent to their restriction to digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6417</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6417</id><created>2014-06-24</created><authors><author><keyname>Long</keyname><forenames>Ying</forenames></author><author><keyname>Wu</keyname><forenames>Kang</forenames></author><author><keyname>Wang</keyname><forenames>Jianghao</forenames></author><author><keyname>Shen</keyname><forenames>Zhenjiang</forenames></author></authors><title>Big Models: From Beijing to the whole China</title><categories>cs.OH</categories><comments>22 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper propose the concept of big model as a novel research paradigm for
regional and urban studies. Big models are fine-scale regional/urban simulation
models for a large geographical area, and they overcome the trade-off between
simulated scale and spatial unit by tackling both of them at the same time
enabled by emerging big/open data, increasing computation power and matured
regional/urban modeling methods. The concept, characteristics, and potential
applications of big models have been elaborated. We addresse several case
studies to illustrate the progress of research and utilization on big models,
including mapping urban areas for all Chinese cities, performing parcel-level
urban simulation, and several ongoing research projects. Most of these
applications can be adopted across the country, and all of them are focusing on
a fine-scale level, such as a parcel, a block, or a township (sub-district),
which is not the same with the existing studies using conventional models that
are only suitable for a certain single or two cities or regions, or for a
larger area but have to significantly sacrifice the data resolution. It is
expected that big models will mark a promising new era for the urban and
regional study in the age of big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6425</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6425</id><created>2014-06-24</created><updated>2015-07-14</updated><authors><author><keyname>Sudhakar</keyname><forenames>Prasad</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Dubois</keyname><forenames>Xavier</forenames></author><author><keyname>Antoine</keyname><forenames>Philippe</forenames></author><author><keyname>Joannes</keyname><forenames>Luc</forenames></author></authors><title>Compressive Imaging and Characterization of Sparse Light Deflection Maps</title><categories>cs.CV</categories><comments>35 pages, 17 figures. Accepted for publication in SIAM Journal on
  Imaging Sciences</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Light rays incident on a transparent object of uniform refractive index
undergo deflections, which uniquely characterize the surface geometry of the
object. Associated with each point on the surface is a deflection map (or
spectrum) which describes the pattern of deflections in various directions.
This article presents a novel method to efficiently acquire and reconstruct
sparse deflection spectra induced by smooth object surfaces. To this end, we
leverage the framework of Compressed Sensing (CS) in a particular
implementation of a schlieren deflectometer, i.e., an optical system providing
linear measurements of deflection spectra with programmable spatial light
modulation patterns. We design those modulation patterns on the principle of
spread spectrum CS for reducing the number of observations. The ability of our
device to simultaneously observe the deflection spectra on a dense
discretization of the object surface is related to a Multiple Measurement
Vector (MMV) model. This scheme allows us to estimate both the noise power and
the instrumental point spread function.
  We formulate the spectrum reconstruction task as the solving of a linear
inverse problem regularized by an analysis sparsity prior using a translation
invariant wavelet frame. Our results demonstrate the capability and advantages
of using a CS based approach for deflectometric imaging both on simulated data
and experimental deflectometric data.
  Finally, the paper presents an extension of our method showing how we can
extract the main deflection direction in each point of the object surface from
a few compressive measurements, without needing any costly reconstruction
procedures. This compressive characterization is then confirmed with
experimental results on simple plano-convex and multifocal intra-ocular lenses
studying the evolution of the main deflection as a function of the object point
location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6447</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6447</id><created>2014-06-24</created><authors><author><keyname>Hase</keyname><forenames>M. O.</forenames></author></authors><title>Carrying Capacity in Barab\'asi-Albert Network</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effect of finiteness in degree number is analyzed on a growing network.
The dynamics is governed by a rule where the degree number increases under a
scheme similar to the Malthus-Verhulst model in the context of population
growth. One notices that the second moment of degree is closely related to the
linking probability of this growing network. The degree distribution is
analyzed in both stationary and time-dependent regimes through some exact
results and simulations, and a scaling behaviour is found in asymptotically
large time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6449</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6449</id><created>2014-06-24</created><authors><author><keyname>Zhang</keyname><forenames>Kezun</forenames></author><author><keyname>Xiao</keyname><forenames>Yanghua</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author><author><keyname>Wang</keyname><forenames>Haixun</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>The Links Have It: Infobox Generation by Summarization over Linked
  Entities</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Online encyclopedia such as Wikipedia has become one of the best sources of
knowledge. Much effort has been devoted to expanding and enriching the
structured data by automatic information extraction from unstructured text in
Wikipedia. Although remarkable progresses have been made, their effectiveness
and efficiency is still limited as they try to tackle an extremely difficult
natural language understanding problems and heavily relies on supervised
learning approaches which require large amount effort to label the training
data. In this paper, instead of performing information extraction over
unstructured natural language text directly, we focus on a rich set of
semi-structured data in Wikipedia articles: linked entities. The idea of this
paper is the following: If we can summarize the relationship between the entity
and its linked entities, we immediately harvest some of the most important
information about the entity. To this end, we propose a novel rank aggregation
approach to remove noise, an effective clustering and labeling algorithm to
extract knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6453</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6453</id><created>2014-06-25</created><authors><author><keyname>Liu</keyname><forenames>Peilei</forenames></author><author><keyname>Wang</keyname><forenames>Ting</forenames></author></authors><title>A Quantitative Neural Coding Model of Sensory Memory</title><categories>cs.NE q-bio.NC</categories><comments>9 pages, 3 figures</comments><acm-class>I.2.0; I.2.4; I.2.6; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coding mechanism of sensory memory on the neuron scale is one of the most
important questions in neuroscience. We have put forward a quantitative neural
network model, which is self organized, self similar, and self adaptive, just
like an ecosystem following Darwin theory. According to this model, neural
coding is a mult to one mapping from objects to neurons. And the whole cerebrum
is a real-time statistical Turing Machine, with powerful representing and
learning ability. This model can reconcile some important disputations, such
as: temporal coding versus rate based coding, grandmother cell versus
population coding, and decay theory versus interference theory. And it has also
provided explanations for some key questions such as memory consolidation,
episodic memory, consciousness, and sentiment. Philosophical significance is
indicated at last.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6470</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6470</id><created>2014-06-25</created><updated>2014-09-05</updated><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Wireless Networks with RF Energy Harvesting: A Contemporary Survey</title><categories>cs.NI cs.IT math.IT</categories><comments>Any comment(s) would be highly welcomed. Please send email to
  luxiao@ntu.edu.sg</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio frequency (RF) energy transfer and harvesting techniques have recently
become alternative methods to power the next generation wireless networks. As
this emerging technology enables proactive energy replenishment of wireless
devices, it is advantageous in supporting applications with quality of service
(QoS) requirement. In this paper, we present an extensive literature review on
the research progresses in wireless networks with RF energy harvesting
capability, referred to as RF energy harvesting networks (RF-EHNs). First, we
present an overview of the RF-EHNs including system architecture, RF energy
harvesting techniques and existing applications. Then, we present the
background in circuit design as well as the state-of-the-art circuitry
implementations, and review the communication protocols specially designed for
RF-EHNs. We also explore various key design issues in the development of
RF-EHNs according to the network types, i.e., single-hop network, multi-antenna
network, relay network and cognitive radio network. Finally, we envision some
open research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6473</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6473</id><created>2014-06-25</created><authors><author><keyname>Mathew</keyname><forenames>Lani Rachel</forenames></author><author><keyname>Anselam</keyname><forenames>Ancy S.</forenames></author><author><keyname>Pillai</keyname><forenames>Sakuntala S.</forenames></author></authors><title>Performance Comparison of Linear Prediction based Vocoders in Linux
  Platform</title><categories>cs.MM cs.SD</categories><comments>5 pages, 5 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT),V10(11),554-558 April 2014</journal-ref><doi>10.14445/22315381/IJETT-V10P310</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear predictive coders form an important class of speech coders. This paper
describes the software level implementation of linear prediction based
vocoders, viz. Code Excited Linear Prediction (CELP), Low-Delay CELP (LD-CELP)
and Mixed Excitation Linear Prediction (MELP) at bit rates of 4.8 kb/s, 16 kb/s
and 2.4 kb/s respectively. The C programs of the vocoders have been compiled
and executed in Linux platform. Subjective testing with the help of Mean
Opinion Score test has been performed. Waveform analysis has been done using
Praat and Adobe Audition software. The results show that MELP and CELP produce
comparable quality while the quality of LD-CELP coder is much higher, at the
expense of higher bit rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6474</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6474</id><created>2014-06-25</created><updated>2014-11-05</updated><authors><author><keyname>Nishihara</keyname><forenames>Robert</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>On the Convergence Rate of Decomposable Submodular Function Minimization</title><categories>math.OC cs.DM cs.DS cs.LG cs.NA</categories><comments>17 pages, 3 figures</comments><journal-ref>Neural Information Processing Systems 27, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular functions describe a variety of discrete problems in machine
learning, signal processing, and computer vision. However, minimizing
submodular functions poses a number of algorithmic challenges. Recent work
introduced an easy-to-use, parallelizable algorithm for minimizing submodular
functions that decompose as the sum of &quot;simple&quot; submodular functions.
Empirically, this algorithm performs extremely well, but no theoretical
analysis was given. In this paper, we show that the algorithm converges
linearly, and we provide upper and lower bounds on the rate of convergence. Our
proof relies on the geometry of submodular polyhedra and draws on results from
spectral graph theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6490</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6490</id><created>2014-06-25</created><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author></authors><title>Variance Competitiveness for Monotone Estimation: Tightening the Bounds</title><categories>cs.DB</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random samples are extensively used to summarize massive data sets and
facilitate scalable analytics. Coordinated sampling, where samples of different
data sets &quot;share&quot; the randomization, is a powerful method which facilitates
more accurate estimation of many aggregates and similarity measures. We
recently formulated a model of {\it Monotone Estimation Problems} (MEP), which
can be applied to coordinated sampling, projected on a single item. MEP
estimators can then be used to estimate sum aggregates, such as distances, over
coordinated samples. For MEP, we are interested in estimators that are unbiased
and nonnegative. We proposed {\it variance competitiveness} as a quality
measure of estimators: For each data vector, we consider the minimum variance
attainable on it by an unbiased and nonnegative estimator. We then define the
competitiveness of an estimator as the maximum ratio, over data, of the
expectation of the square to the minimum possible. We also presented a general
construction of the L$^*$ estimator, which is defined for any MEP for which a
nonnegative unbiased estimator exists, and is at most 4-competitive.
  Our aim here is to obtain tighter bounds on the {\em universal ratio}, which
we define to be the smallest competitive ratio that can be obtained for any
MEP. We obtain an upper bound of 3.375, improving over the bound of $4$ of the
L$^*$ estimator. We also establish a lower bound of 1.44. The lower bound is
obtained by constructing the {\it optimally competitive} estimator for
particular MEPs. The construction is of independent interest, as it facilitates
estimation with instance-optimal competitiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6494</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6494</id><created>2014-06-25</created><updated>2014-06-26</updated><authors><author><keyname>Apollonio</keyname><forenames>N.</forenames></author><author><keyname>Galluccio</keyname><forenames>A.</forenames></author></authors><title>Minimally unbalanced diamond-free graphs and Dyck-paths</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $\{0,1\}$-matrix $\mathsf{A}$ is balanced if it does not contain a
submatrix of odd order having exactly two 1's per row and per column. A graph
is balanced if its clique-matrix is balanced. No characterization of minimally
unbalanced graphs is known, and even no conjecture on the structure of such
graphs has been posed, contrarily to what happened for perfect graphs. In this
paper, we provide such a characterization for the class of diamond-free graphs
and establish a connection between minimally unbalanced diamond-free graphs and
Dyck-paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6507</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6507</id><created>2014-06-25</created><authors><author><keyname>Song</keyname><forenames>Hyun Oh</forenames></author><author><keyname>Lee</keyname><forenames>Yong Jae</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Weakly-supervised Discovery of Visual Pattern Configurations</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing prominence of weakly labeled data nurtures a growing demand
for object detection methods that can cope with minimal supervision. We propose
an approach that automatically identifies discriminative configurations of
visual patterns that are characteristic of a given object class. We formulate
the problem as a constrained submodular optimization problem and demonstrate
the benefits of the discovered configurations in remedying mislocalizations and
finding informative positive and negative training examples. Together, these
lead to state-of-the-art weakly-supervised detection results on the challenging
PASCAL VOC dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6514</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6514</id><created>2014-06-25</created><updated>2016-03-04</updated><authors><author><keyname>Li</keyname><forenames>Danning</forenames></author><author><keyname>Zou</keyname><forenames>Hui</forenames></author></authors><title>SURE Information Criteria for Large Covariance Matrix Estimation and
  Their Asymptotic Properties</title><categories>math.ST cs.IT math.IT stat.TH</categories><msc-class>62H12 (Primary), 62F12, 62G20 (Secondary)</msc-class><doi>10.1109/TIT.2016.2530090</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider $n$ independent and identically distributed $p$-dimensional Gaussian
random vectors with covariance matrix $\Sigma.$ The problem of estimating
$\Sigma$ when $p$ is much larger than $n$ has received a lot of attention in
recent years. Yet little is known about the information criterion for
covariance matrix estimation. How to properly define such a criterion and what
are the statistical properties? We attempt to answer these questions in the
present paper by focusing on the estimation of bandable covariance matrices
when $p&gt;n$ but $\log(p)=o(n)$. Motivated by the deep connection between Stein's
unbiased risk estimation (SURE) and AIC in regression models, we propose a
family of generalized SURE ($\text{SURE}_c$) indexed by $c$ for covariance
matrix estimation, where $c$ is some constant. When $c$ is 2, $\text{SURE}_2$
provides an unbiased estimator of the Frobenious risk of the covariance matrix
estimator. Furthermore, we show that by minimizing $\text{SURE}_2$ over all
possible banding covariance matrix estimators we attain the minimax optimal
rate of convergence and the resulting estimator behaves like the covariance
matrix estimator obtained by the so-called oracle tuning. On the other hand, we
also show that $\text{SURE}_2$ is selection inconsistent when the true
covariance matrix is exactly banded. To fix the selection inconsistency, we
consider using SURE with $c=\log(n)$ and prove that by minimizing
$\text{SURE}_{\log(n)}$ we select the true bandwith with probability tending to
one. Therefore, our analysis indicates that $\text{SURE}_2$ and
$\text{SURE}_{\log(n)}$ can be regarded as the AIC and BIC for large covariance
matrix estimation, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6529</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6529</id><created>2014-06-25</created><authors><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author></authors><title>Strong Regularities in Growth and Decline of Popularity of Social Media
  Services</title><categories>cs.SI physics.soc-ph</categories><acm-class>G.3; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze general trends and pattern in time series that characterize the
dynamics of collective attention to social media services and Web-based
businesses. Our study is based on search frequency data available from Google
Trends and considers 175 different services. For each service, we collect data
from 45 different countries as well as global averages. This way, we obtain
more than 8,000 time series which we analyze using diffusion models from the
economic sciences. We find that these models accurately characterize the
empirical data and our analysis reveals that collective attention to social
media grows and subsides in a highly regular and predictable manner.
Regularities persist across regions, cultures, and topics and thus hint at
general mechanisms that govern the adoption of Web-based services. We discuss
several cases in detail to highlight interesting findings. Our methods are of
economic interest as they may inform investment decisions and can help
assessing at what stage of the general life-cycle a Web service is at.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6533</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6533</id><created>2014-06-25</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Di Battista</keyname><forenames>Giuseppe</forenames></author><author><keyname>Frati</keyname><forenames>Fabrizio</forenames></author><author><keyname>Roselli</keyname><forenames>Vincenzo</forenames></author></authors><title>On the Complexity of Clustered-Level Planarity and T-Level Planarity</title><categories>cs.DS cs.CG</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study two problems related to the drawing of level graphs,
that is, T-LEVEL PLANARITY and CLUSTERED-LEVEL PLANARITY. We show that both
problems are NP-complete in the general case and that they become
polynomial-time solvable when restricted to proper instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6538</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6538</id><created>2014-06-25</created><authors><author><keyname>Kiechle</keyname><forenames>Martin</forenames></author><author><keyname>Habigt</keyname><forenames>Tim</forenames></author><author><keyname>Hawe</keyname><forenames>Simon</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>A Bimodal Co-Sparse Analysis Model for Image Processing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of many computer vision tasks lies in the ability to exploit the
interdependency between different image modalities such as intensity and depth.
Fusing corresponding information can be achieved on several levels, and one
promising approach is the integration at a low level. Moreover, sparse signal
models have successfully been used in many vision applications. Within this
area of research, the so called co-sparse analysis model has attracted
considerably less attention than its well-known counterpart, the sparse
synthesis model, although it has been proven to be very useful in various image
processing applications. In this paper, we propose a co-sparse analysis model
that is able to capture the interdependency of two image modalities. It is
based on the assumption that a pair of analysis operators exists, so that the
co-supports of the corresponding bimodal image structures are correlated. We
propose an algorithm that is able to learn such a coupled pair of operators
from registered and noise-free training data. Furthermore, we explain how this
model can be applied to solve linear inverse problems in image processing and
how it can be used for image registration tasks. This paper extends the work of
some of the authors by two major contributions. Firstly, a modification of the
learning process is proposed that a priori guarantees unit norm and zero-mean
of the rows of the operator. This accounts for the intuition that contrast in
image modalities carries the most information. Secondly, the model is used in a
novel bimodal image registration algorithm which estimates the transformation
parameters of unregistered images of different modalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6542</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6542</id><created>2014-06-25</created><updated>2015-11-26</updated><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Shaqfeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Alnuweiri</keyname><forenames>Hussein</forenames></author></authors><title>Robust Layered Transmission in Secure MISO Multiuser Unicast Cognitive
  Radio Systems</title><categories>cs.IT math.IT</categories><comments>Accepted for publication with minor revision, IEEE TVT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies robust resource allocation algorithm design for a
multiuser multiple-input single-output (MISO) cognitive radio (CR) downlink
communication network. We focus on a secondary system which provides unicast
secure wireless layered video information to multiple single-antenna secondary
receivers. The resource allocation algorithm design is formulated as a
non-convex optimization problem for minimization of the total transmit power at
the secondary transmitter. The proposed framework takes into account a quality
of service (QoS) requirement regarding video communication secrecy in the
secondary system, the imperfection of the channel state information (CSI) of
potential eavesdroppers (primary receivers) at the secondary transmitter, and a
limit for the maximum tolerable received interference power at the primary
receivers. Thereby, the proposed problem formulation exploits the
self-protecting architecture of layered transmission and artificial noise
generation to ensure communication secrecy. Semidefinite programming (SDP)
relaxation is employed to derive a resource allocation algorithm which finds
the global optimal solution to the formulated problem. Simulation results
demonstrate significant transmit power savings and robustness against CSI
imperfection for the proposed resource allocation algorithm for layered
transmission compared to baseline schemes with traditional single-layer
transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6550</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6550</id><created>2014-06-25</created><updated>2015-10-18</updated><authors><author><keyname>Suk</keyname><forenames>Andrew</forenames></author></authors><title>Semi-algebraic Ramsey numbers</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite point set $P \subset \mathbb{R}^d$, a $k$-ary semi-algebraic
relation $E$ on $P$ is the set of $k$-tuples of points in $P$, which is
determined by a finite number of polynomial equations and inequalities in $kd$
real variables. The description complexity of such a relation is at most $t$ if
the number of polynomials and their degrees are all bounded by $t$. The Ramsey
number $R^{d,t}_k(s,n)$ is the minimum $N$ such that any $N$-element point set
$P$ in $\mathbb{R}^d$ equipped with a $k$-ary semi-algebraic relation $E$, such
that $E$ has complexity at most $t$, contains $s$ members such that every
$k$-tuple induced by them is in $E$, or $n$ members such that every $k$-tuple
induced by them is not in $E$.
  We give a new upper bound for $R^{d,t}_k(s,n)$ for $k\geq 3$ and $s$ fixed.
In particular, we show that for fixed integers $d,t,s$, $R^{d,t}_3(s,n) \leq
2^{n^{o(1)}},$ establishing a subexponential upper bound on $R^{d,t}_3(s,n)$.
This improves the previous bound of $2^{n^C}$ due to Conlon, Fox, Pach,
Sudakov, and Suk, where $C$ is a very large constant depending on $d,t,$ and
$s$. As an application, we give new estimates for a recently studied
Ramsey-type problem on hyperplane arrangements in $\mathbb{R}^d$. We also study
multi-color Ramsey numbers for triangles in our semi-algebraic setting,
achieving some partial results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6558</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6558</id><created>2014-06-25</created><updated>2014-07-03</updated><authors><author><keyname>Ganin</keyname><forenames>Yaroslav</forenames></author><author><keyname>Lempitsky</keyname><forenames>Victor</forenames></author></authors><title>$ N^4 $-Fields: Neural Network Nearest Neighbor Fields for Image
  Transforms</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new architecture for difficult image processing operations, such
as natural edge detection or thin object segmentation. The architecture is
based on a simple combination of convolutional neural networks with the nearest
neighbor search.
  We focus our attention on the situations when the desired image
transformation is too hard for a neural network to learn explicitly. We show
that in such situations, the use of the nearest neighbor search on top of the
network output allows to improve the results considerably and to account for
the underfitting effect during the neural network training. The approach is
validated on three challenging benchmarks, where the performance of the
proposed architecture matches or exceeds the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6560</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6560</id><created>2014-06-25</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Sencion-Echauri</keyname><forenames>Felipe</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Cisneros</keyname><forenames>Marco Perez</forenames></author></authors><title>Multi Circle Detection on Images Using Artificial Bee Colony (ABC)
  Optimization</title><categories>cs.CV cs.NE</categories><comments>19 Pages</comments><journal-ref>Soft Computing, 16 (2), (2012), pp. 281-296</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hough transform (HT) has been the most common method for circle detection,
exhibiting robustness, but adversely demanding considerable computational
effort and large memory requirements. Alternative approaches include heuristic
methods that employ iterative optimization procedures for detecting multiple
circles. Since only one circle can be marked at each optimization cycle,
multiple executions must be enforced in order to achieve multi detection. This
paper presents an algorithm for automatic detection of multiple circular shapes
that considers the overall process as a multi-modal optimization problem. The
approach is based on the artificial bee colony (ABC) algorithm, a swarm
optimization algorithm inspired by the intelligent foraging behavior of honey
bees. Unlike the original ABC algorithm, the proposed approach presents the
addition of a memory for discarded solutions. Such memory allows holding
important information regarding other local optima which might have emerged
during the optimization process. The detector uses a combination of three
non-collinear edge points as parameters to determine circle candidates. A
matching function (nectar- amount) determines if such circle candidates
(bee-food-sources) are actually present in the image. Guided by the values of
such matching functions, the set of encoded candidate circles are evolved
through the ABC algorithm so that the best candidate (global optimum) can be
fitted into an actual circle within the edge only image. Then, an analysis of
the incorporated memory is executed in order to identify potential local
optima, i.e., other circles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6567</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6567</id><created>2014-06-25</created><updated>2015-03-11</updated><authors><author><keyname>Ito</keyname><forenames>Takehiro</forenames></author><author><keyname>Kami&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Ono</keyname><forenames>Hirotaka</forenames></author></authors><title>Fixed-Parameter Tractability of Token Jumping on Planar Graphs</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that we are given two independent sets $I_0$ and $I_r$ of a graph
such that $|I_0| = |I_r|$, and imagine that a token is placed on each vertex in
$I_0$. The token jumping problem is to determine whether there exists a
sequence of independent sets which transforms $I_0$ into $I_r$ so that each
independent set in the sequence results from the previous one by moving exactly
one token to another vertex. This problem is known to be PSPACE-complete even
for planar graphs of maximum degree three, and W[1]-hard for general graphs
when parameterized by the number of tokens. In this paper, we present a
fixed-parameter algorithm for the token jumping problem on planar graphs, where
the parameter is only the number of tokens. Furthermore, the algorithm can be
modified so that it finds a shortest sequence for a yes-instance. The same
scheme of the algorithms can be applied to a wider class of graphs,
$K_{3,t}$-free graphs for any fixed integer $t \ge 3$, and it yields
fixed-parameter algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6568</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6568</id><created>2014-06-25</created><authors><author><keyname>Miller</keyname><forenames>V. A.</forenames></author><author><keyname>Erlien</keyname><forenames>S.</forenames></author><author><keyname>Piersol</keyname><forenames>J.</forenames></author></authors><title>Support vector machine classification of dimensionally reduced
  structural MRI images for dementia</title><categories>cs.CV cs.LG physics.med-ph</categories><comments>technical note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify very-mild to moderate dementia in patients (CDR ranging from 0 to
2) using a support vector machine classifier acting on dimensionally reduced
feature set derived from MRI brain scans of the 416 subjects available in the
OASIS-Brains dataset. We use image segmentation and principal component
analysis to reduce the dimensionality of the data. Our resulting feature set
contains 11 features for each subject. Performance of the classifiers is
evaluated using 10-fold cross-validation. Using linear and (gaussian) kernels,
we obtain a training classification accuracy of 86.4% (90.1%), test accuracy of
85.0% (85.7%), test precision of 68.7% (68.5%), test recall of 68.0% (74.0%),
and test Matthews correlation coefficient of 0.594 (0.616).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6573</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6573</id><created>2014-06-25</created><updated>2015-07-09</updated><authors><author><keyname>Isaac</keyname><forenames>Tobin</forenames></author><author><keyname>Stadler</keyname><forenames>Georg</forenames></author><author><keyname>Ghattas</keyname><forenames>Omar</forenames></author></authors><title>Solution of nonlinear Stokes equations discretized by high-order finite
  elements on nonconforming and anisotropic meshes, with application to ice
  sheet dynamics</title><categories>cs.NA cs.CE math.NA</categories><comments>31 pages</comments><doi>10.1137/140974407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the need for efficient and accurate simulation of the dynamics
of the polar ice sheets, we design high-order finite element discretizations
and scalable solvers for the solution of nonlinear incompressible Stokes
equations. We focus on power-law, shear thinning rheologies used in modeling
ice dynamics and other geophysical flows. We use nonconforming hexahedral
meshes and the conforming inf-sup stable finite element velocity-pressure
pairings $\mathbb{Q}_k\times \mathbb{Q}^\text{disc}_{k-2}$ or $\mathbb{Q}_k
\times \mathbb{P}^\text{disc}_{k-1}$. To solve the nonlinear equations, we
propose a Newton-Krylov method with a block upper triangular preconditioner for
the linearized Stokes systems. The diagonal blocks of this preconditioner are
sparse approximations of the (1,1)-block and of its Schur complement. The
(1,1)-block is approximated using linear finite elements based on the nodes of
the high-order discretization, and the application of its inverse is
approximated using algebraic multigrid with an incomplete factorization
smoother. This preconditioner is designed to be efficient on anisotropic
meshes, which are necessary to match the high aspect ratio domains typical for
ice sheets. We develop and make available extensions to two libraries---a
hybrid meshing scheme for the p4est parallel AMR library, and a modified
smoothed aggregation scheme for PETSc---to improve their support for solving
PDEs in high aspect ratio domains. In a numerical study, we find that our
solver yields fast convergence that is independent of the element aspect ratio,
the occurrence of nonconforming interfaces, and of mesh refinement, and that
depends only weakly on the polynomial finite element order. We simulate the ice
flow in a realistic description of the Antarctic ice sheet derived from field
data, and study the parallel scalability of our solver for problems with up to
383M unknowns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6576</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6576</id><created>2014-06-25</created><updated>2014-09-01</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Fox-Epstein</keyname><forenames>Eli</forenames></author><author><keyname>Hoang</keyname><forenames>Duc A.</forenames></author><author><keyname>Ito</keyname><forenames>Takehiro</forenames></author><author><keyname>Ono</keyname><forenames>Hirotaka</forenames></author><author><keyname>Otachi</keyname><forenames>Yota</forenames></author><author><keyname>Uehara</keyname><forenames>Ryuhei</forenames></author><author><keyname>Yamada</keyname><forenames>Takeshi</forenames></author></authors><title>Linear-Time Algorithm for Sliding Tokens on Trees</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that we are given two independent sets $I_b$ and $I_r$ of a graph
such that $|I_b|=|I_r|$, and imagine that a token is placed on each vertex in
$I_b$. Then, the sliding token problem is to determine whether there exists a
sequence of independent sets which transforms $I_b$ into $I_r$ so that each
independent set in the sequence results from the previous one by sliding
exactly one token along an edge in the graph. This problem is known to be
PSPACE-complete even for planar graphs, and also for bounded treewidth graphs.
In this paper, we thus study the problem restricted to trees, and give the
following three results: (1) the decision problem is solvable in linear time;
(2) for a yes-instance, we can find in quadratic time an actual sequence of
independent sets between $I_b$ and $I_r$ whose length (i.e., the number of
token-slides) is quadratic; and (3) there exists an infinite family of
instances on paths for which any sequence requires quadratic length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6587</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6587</id><created>2014-06-25</created><authors><author><keyname>M&#xfc;ller</keyname><forenames>Stefan</forenames></author><author><keyname>Regensburger</keyname><forenames>Georg</forenames></author></authors><title>Generalized Mass-Action Systems and Positive Solutions of Polynomial
  Equations with Real and Symbolic Exponents</title><categories>math.DS cs.CE math.AG q-bio.MN</categories><comments>22 pages</comments><msc-class>37C10, 70K42, 80A30, 13P15, 68W30, 52C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamical systems arising from chemical reaction networks with mass action
kinetics are the subject of chemical reaction network theory (CRNT). In
particular, this theory provides statements about uniqueness, existence, and
stability of positive steady states for all rate constants and initial
conditions. In terms of the corresponding polynomial equations, the results
guarantee uniqueness and existence of positive solutions for all positive
parameters.
  We address a recent extension of CRNT, called generalized mass-action
systems, where reaction rates are allowed to be power-laws in the
concentrations. In particular, the (real) kinetic orders can differ from the
(integer) stoichiometric coefficients. As with mass-action kinetics, complex
balancing equilibria are determined by the graph Laplacian of the underlying
network and can be characterized by binomial equations and parametrized by
monomials. In algebraic terms, we focus on a constructive characterization of
positive solutions of polynomial equations with real and symbolic exponents.
  Uniqueness and existence for all rate constants and initial conditions
additionally depend on sign vectors of the stoichiometric and kinetic-order
subspaces. This leads to a generalization of Birch's theorem, which is robust
with respect to certain perturbations in the exponents. In this context, we
discuss the occurrence of multiple complex balancing equilibria. We illustrate
our results by a running example and provide a MAPLE worksheet with
implementations of all algorithmic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6591</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6591</id><created>2014-06-25</created><authors><author><keyname>Wang</keyname><forenames>Shouguang</forenames></author><author><keyname>Yang</keyname><forenames>Jing</forenames></author><author><keyname>Zhou</keyname><forenames>Mengchu</forenames></author></authors><title>Comments on 'Maximally permissive supervisor synthesis based on a new
  constraint transformation method'</title><categories>cs.SY cs.FL</categories><comments>4 papers,comment</comments><journal-ref>Automatica 48 (2012) 1097-1101</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Luo et al. proposed a new method to design the maximally permissive and
efficient supervisor for enforcing linear constraints on an ordinary Petri net
with uncontrollable transitions. In order to develop this method, Theorem 3 is
given. It is claimed that 'a linear constraint can be equivalently transformed
at an uncontrollable transition into a disjunction of weakly admissible ones.'
However, this result is erroneous. In this correspondence paper, a
counterexample contradicting it is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6595</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6595</id><created>2014-06-25</created><authors><author><keyname>Herakleous</keyname><forenames>Kyriakos</forenames></author><author><keyname>Poullis</keyname><forenames>Charalambos</forenames></author></authors><title>3DUNDERWORLD-SLS: An Open-Source Structured-Light Scanning System for
  Rapid Geometry Acquisition</title><categories>cs.CV</categories><comments>27 pages describing the 3DUNDERWORLD-SLS open source software by the
  ICT lab (www.theICTlab.org)</comments><report-no>ICT-TR-2014-01</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recently, there has been an increase in the demand of virtual 3D objects
representing real-life objects. A plethora of methods and systems have already
been proposed for the acquisition of the geometry of real-life objects ranging
from those which employ active sensor technology, passive sensor technology or
a combination of various techniques.
  In this paper we present the development of a 3D scanning system which is
based on the principle of structured-light, without having particular
requirements for specialized equipment. We discuss the intrinsic details and
inherent difficulties of structured-light scanning techniques and present our
solutions. Finally, we introduce our open-source scanning system
&quot;3DUNDERWORLD-SLS&quot; which implements the proposed techniques. We have performed
extensive testing with a wide range of models and report the results.
Furthermore, we present a comprehensive evaluation of the system and a
comparison with a high-end commercial 3D scanner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6597</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6597</id><created>2014-06-25</created><authors><author><keyname>Orman</keyname><forenames>G&#xfc;nce Keziban</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Plantevit</keyname><forenames>Marc</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Boulicaut</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>LIRIS</affiliation></author></authors><title>A Method for Characterizing Communities in Dynamic Attributed Complex
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>IEEE/ACM International Conference on Advances in Social Network
  Analysis and Mining (ASONAM), P\'ekin : China (2014)</comments><proxy>ccsd</proxy><doi>10.1109/ASONAM.2014.6921629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many methods have been proposed to detect communities, not only in plain, but
also in attributed, directed or even dynamic complex networks. In its simplest
form, a community structure takes the form of a partition of the node set. From
the modeling point of view, to be of some utility, this partition must then be
characterized relatively to the properties of the studied system. However, if
most of the existing works focus on defining methods for the detection of
communities, only very few try to tackle this interpretation problem. Moreover,
the existing approaches are limited either in the type of data they handle, or
by the nature of the results they output. In this work, we propose a method to
efficiently support such a characterization task. We first define a
sequence-based representation of networks, combining temporal information,
topological measures, and nodal attributes. We then describe how to identify
the most emerging sequential patterns of this dataset, and use them to
characterize the communities. We also show how to detect unusual behavior in a
community, and highlight outliers. Finally, as an illustration, we apply our
method to a network of scientific collaborations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6599</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6599</id><created>2014-06-25</created><authors><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Suri</keyname><forenames>Subhash</forenames></author><author><keyname>Yildiz</keyname><forenames>Hakan</forenames></author><author><keyname>Zhang</keyname><forenames>Wuzhou</forenames></author></authors><title>Convex Hulls under Uncertainty</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the convex-hull problem in a probabilistic setting, motivated by the
need to handle data uncertainty inherent in many applications, including sensor
databases, location-based services and computer vision. In our framework, the
uncertainty of each input site is described by a probability distribution over
a finite number of possible locations including a \emph{null} location to
account for non-existence of the point. Our results include both exact and
approximation algorithms for computing the probability of a query point lying
inside the convex hull of the input, time-space tradeoffs for the membership
queries, a connection between Tukey depth and membership queries, as well as a
new notion of $\some$-hull that may be a useful representation of uncertain
hulls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6603</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6603</id><created>2014-06-25</created><updated>2015-02-02</updated><authors><author><keyname>Bonettini</keyname><forenames>Silvia</forenames></author><author><keyname>Chiuso</keyname><forenames>Alessandro</forenames></author><author><keyname>Prato</keyname><forenames>Marco</forenames></author></authors><title>A scaled gradient projection method for Bayesian learning in dynamical
  systems</title><categories>math.NA cs.LG stat.ML</categories><msc-class>65K05, 90C30, 90C90, 93B30</msc-class><journal-ref>SIAM Journal on Scientific Computing 37 (2015), A1297-A1318</journal-ref><doi>10.1137/140973529</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crucial task in system identification problems is the selection of the most
appropriate model class, and is classically addressed resorting to
cross-validation or using asymptotic arguments. As recently suggested in the
literature, this can be addressed in a Bayesian framework, where model
complexity is regulated by few hyperparameters, which can be estimated via
marginal likelihood maximization. It is thus of primary importance to design
effective optimization methods to solve the corresponding optimization problem.
If the unknown impulse response is modeled as a Gaussian process with a
suitable kernel, the maximization of the marginal likelihood leads to a
challenging nonconvex optimization problem, which requires a stable and
effective solution strategy. In this paper we address this problem by means of
a scaled gradient projection algorithm, in which the scaling matrix and the
steplength parameter play a crucial role to provide a meaning solution in a
computational time comparable with second order methods. In particular, we
propose both a generalization of the split gradient approach to design the
scaling matrix in the presence of box constraints, and an effective
implementation of the gradient and objective function. The extensive numerical
experiments carried out on several test problems show that our method is very
effective in providing in few tenths of a second solutions of the problems with
accuracy comparable with state-of-the-art approaches. Moreover, the flexibility
of the proposed strategy makes it easily adaptable to a wider range of problems
arising in different areas of machine learning, signal processing and system
identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6605</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6605</id><created>2014-06-24</created><updated>2014-11-21</updated><authors><author><keyname>Brenas</keyname><forenames>Jon Ha&#xeb;l</forenames></author><author><keyname>Echahed</keyname><forenames>Rachid</forenames></author><author><keyname>Strecker</keyname><forenames>Martin</forenames></author></authors><title>SROIQsigma is decidable</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dynamic extension of the description logic $\mathcal{SROIQ}$.
This means that interpretations could evolve thanks to some actions such as
addition and/or deletion of an element (respectively, a pair of elements) of a
concept (respectively, of a role). The obtained logic is called
$\mathcal{SROIQ}$ with explicit substitutions and is written
$\mathcal{SROIQ^\sigma}$. Substitution is not treated as meta-operation that is
carried out immediately, but the operation of substitution may be delayed, so
that sub-formulae of $\mathcal{SROIQ}^\sigma$ are of the form $\Phi\sigma$,
where $\Phi$ is a $\mathcal{SROIQ}$ formula and $\sigma$ is a substitution
which encodes changes of concepts and roles. In this paper, we particularly
prove that the satisfiability problem of $\mathcal{SROIQ}^\sigma$ is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6608</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6608</id><created>2014-06-25</created><updated>2014-08-20</updated><authors><author><keyname>Baccelli</keyname><forenames>Emmanuel</forenames></author><author><keyname>Mehlis</keyname><forenames>Christian</forenames></author><author><keyname>Hahm</keyname><forenames>Oliver</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author></authors><title>Information Centric Networking in the IoT: Experiments with NDN in the
  Wild</title><categories>cs.NI</categories><comments>10 pages, 10 figures and tables, ACM ICN-2014 conference</comments><acm-class>C.2.1; C.2.2; C.2.6; C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the feasibility, advantages, and challenges of an
ICN-based approach in the Internet of Things. We report on the first NDN
experiments in a life-size IoT deployment, spread over tens of rooms on several
floors of a building. Based on the insights gained with these experiments, the
paper analyses the shortcomings of CCN applied to IoT. Several interoperable
CCN enhancements are then proposed and evaluated. We significantly decreased
control traffic (i.e., interest messages) and leverage data path and caching to
match IoT requirements in terms of energy and bandwidth constraints. Our
optimizations increase content availability in case of IoT nodes with
intermittent activity. This paper also provides the first experimental
comparison of CCN with the common IoT standards 6LoWPAN/RPL/UDP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6611</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6611</id><created>2014-06-25</created><authors><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIFO</affiliation></author><author><keyname>Dugu&#xe9;</keyname><forenames>Nicolas</forenames><affiliation>LIFO</affiliation></author><author><keyname>Perez</keyname><forenames>Anthony</forenames><affiliation>LIFO</affiliation></author></authors><title>Identifying the Community Roles of Social Capitalists in the Twitter
  Network</title><categories>cs.SI physics.soc-ph</categories><comments>IEEE/ACM International Conference on Advances in Social Network
  Analysis and Mining (ASONAM), P\'ekin : China (2014)</comments><proxy>ccsd</proxy><doi>10.1109/ASONAM.2014.6921612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of Twitter, social capitalists are specific users trying to
increase their number of followers and interactions by any means. These users
are not healthy for the Twitter network since they flaw notions of influence
and visibility. Indeed, it has recently been observed that they are real and
active users that can help malicious users such as spammers gaining influence.
Studying their behavior and understanding their position in Twitter is thus of
important interest. A recent work provided an efficient way to detect social
capitalists using two simple topological measures. Based on this detection
method, we study how social capitalists are distributed over Twitter's
friend-to-follower network. We are especially interested in analyzing how they
are organized, and how their links spread across the network. Answering these
questions allows to know whether the social capitalism methods increase the
actual visibility on the service. To that aim, we study the position of social
capitalists on Twitter w.r.t. the community structure of the network. We base
our work on the concept of community role of a node, which describes its
position in a network depending on its connectivity at the community level. The
topological measures originally defined to characterize these roles consider
only some aspects of community-related connectivity and rely on a set of
empirically fixed thresholds. We first show the limitations of such measures
and then extend and generalize them by considering new aspects of the
community-related connectivity. Moreover, we use an unsupervised approach to
distinguish the roles, in order to provide more flexibility relatively to the
studied system. We then apply our method to the case of social capitalists and
show that they are highly visible on Twitter, due to the specific roles they
occupy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6614</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6614</id><created>2014-06-25</created><authors><author><keyname>Petersen</keyname><forenames>Hauke</forenames></author><author><keyname>Baccelli</keyname><forenames>Emmanuel</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>Schiller</keyname><forenames>Jochen</forenames></author></authors><title>The Role of the Internet of Things in Network Resilience</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disasters lead to devastating structural damage not only to buildings and
transport infrastructure, but also to other critical infrastructure, such as
the power grid and communication backbones. Following such an event, the
availability of minimal communication services is however crucial to allow
efficient and coordinated disaster response, to enable timely public
information, or to provide individuals in need with a default mechanism to post
emergency messages. The Internet of Things consists in the massive deployment
of heterogeneous devices, most of which battery-powered, and interconnected via
wireless network interfaces. Typical IoT communication architectures enables
such IoT devices to not only connect to the communication backbone (i.e. the
Internet) using an infrastructure-based wireless network paradigm, but also to
communicate with one another autonomously, without the help of any
infrastructure, using a spontaneous wireless network paradigm. In this paper,
we argue that the vast deployment of IoT-enabled devices could bring benefits
in terms of data network resilience in face of disaster. Leveraging their
spontaneous wireless networking capabilities, IoT devices could enable minimal
communication services (e.g. emergency micro-message delivery) while the
conventional communication infrastructure is out of service. We identify the
main challenges that must be addressed in order to realize this potential in
practice. These challenges concern various technical aspects, including
physical connectivity requirements, network protocol stack enhancements, data
traffic prioritization schemes, as well as social and political aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6618</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6618</id><created>2014-06-25</created><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Sivaraman</forenames></author><author><keyname>Bradley</keyname><forenames>Joseph</forenames></author><author><keyname>Parekh</keyname><forenames>Abhay</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin</forenames></author></authors><title>When is it Better to Compare than to Score?</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When eliciting judgements from humans for an unknown quantity, one often has
the choice of making direct-scoring (cardinal) or comparative (ordinal)
measurements. In this paper we study the relative merits of either choice,
providing empirical and theoretical guidelines for the selection of a
measurement scheme. We provide empirical evidence based on experiments on
Amazon Mechanical Turk that in a variety of tasks, (pairwise-comparative)
ordinal measurements have lower per sample noise and are typically faster to
elicit than cardinal ones. Ordinal measurements however typically provide less
information. We then consider the popular Thurstone and Bradley-Terry-Luce
(BTL) models for ordinal measurements and characterize the minimax error rates
for estimating the unknown quantity. We compare these minimax error rates to
those under cardinal measurement models and quantify for what noise levels
ordinal measurements are better. Finally, we revisit the data collected from
our experiments and show that fitting these models confirms this prediction:
for tasks where the noise in ordinal measurements is sufficiently low, the
ordinal approach results in smaller errors in the estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6622</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6622</id><created>2014-06-25</created><updated>2014-07-02</updated><authors><author><keyname>Schneider</keyname><forenames>Steve</forenames></author><author><keyname>Treharne</keyname><forenames>Helen</forenames></author><author><keyname>Wehrheim</keyname><forenames>Heike</forenames></author><author><keyname>Williams</keyname><forenames>David</forenames></author></authors><title>Managing LTL properties in Event-B refinement</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Refinement in Event-B supports the development of systems via proof based
step-wise refinement of events. This refinement approach ensures safety
properties are preserved, but additional reasoning is required in order to
establish liveness and fairness properties.
  In this paper we present results which allow a closer integration of two
formal methods, Event-B and linear temporal logic. In particular we show how a
class of temporal logic properties can carry through a refinement chain of
machines. Refinement steps can include introduction of new events, event
renaming and event splitting. We also identify a general liveness property that
holds for the events of the initial system of a refinement chain. The approach
will aid developers in enabling them to verify linear temporal logic properties
at early stages of a development, knowing they will be preserved at later
stages. We illustrate the results via a simple case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6625</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6625</id><created>2014-06-25</created><updated>2015-03-11</updated><authors><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Computational Lower Bounds for Community Detection on Random Graphs</title><categories>math.ST cs.CC stat.ML stat.TH</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of detecting the presence of a small dense
community planted in a large Erd\H{o}s-R\'enyi random graph $\mathcal{G}(N,q)$,
where the edge probability within the community exceeds $q$ by a constant
factor. Assuming the hardness of the planted clique detection problem, we show
that the computational complexity of detecting the community exhibits the
following phase transition phenomenon: As the graph size $N$ grows and the
graph becomes sparser according to $q=N^{-\alpha}$, there exists a critical
value of $\alpha = \frac{2}{3}$, below which there exists a computationally
intensive procedure that can detect far smaller communities than any
computationally efficient procedure, and above which a linear-time procedure is
statistically optimal. The results also lead to the average-case hardness
results for recovering the dense community and approximating the densest
$K$-subgraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6631</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6631</id><created>2014-06-25</created><updated>2014-07-14</updated><authors><author><keyname>Biboudis</keyname><forenames>Aggelos</forenames></author><author><keyname>Palladinos</keyname><forenames>Nick</forenames></author><author><keyname>Smaragdakis</keyname><forenames>Yannis</forenames></author></authors><title>Clash of the Lambdas</title><categories>cs.PL</categories><comments>In the revised version: 1) we used a fixed heap with 3GB. The GC
  throughput was improved and results are more balanced, 2) we discussed
  briefly why we chose not to use targeted JVM flags, 3) we discussed
  @specialized and miniboxing, 4) we added a new benchmark that tests a
  streaming operation that avoids automatic boxing of our input data and 5) a
  subtitle was added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of lambdas in Java 8 completes the slate of
statically-typed, mainstream languages with both object-oriented and functional
features. The main motivation for lambdas in Java has been to facilitate
stream-based declarative APIs, and, therefore, easier parallelism. In this
paper, we evaluate the performance impact of lambda abstraction employed in
stream processing, for a variety of high-level languages that run on a virtual
machine (C#, F#, Java and Scala) and runtime platforms (JVM on Linux and
Windows, .NET CLR for Windows, Mono for Linux). Furthermore, we evaluate the
performance gain that two optimizing libraries (ScalaBlitz and LinqOptimizer)
can offer for C#, F# and Scala. Our study is based on small-scale
throughput-benchmarking, with significant care to isolate different factors,
consult experts on the systems involved, and identify causes and opportunities.
We find that Java exhibits high implementation maturity, which is a dominant
factor in benchmarks. At the same time, optimizing frameworks can be highly
effective for common query patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6633</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6633</id><created>2014-06-25</created><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Berlind</keyname><forenames>Chris</forenames></author><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Cohen</keyname><forenames>Emma</forenames></author><author><keyname>Patnaik</keyname><forenames>Kaushik</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>Active Learning and Best-Response Dynamics</title><categories>cs.LG cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine an important setting for engineered systems in which low-power
distributed sensors are each making highly noisy measurements of some unknown
target function. A center wants to accurately learn this function by querying a
small number of sensors, which ordinarily would be impossible due to the high
noise rate. The question we address is whether local communication among
sensors, together with natural best-response dynamics in an
appropriately-defined game, can denoise the system without destroying the true
signal and allow the center to succeed from only a small number of active
queries. By using techniques from game theory and empirical processes, we prove
positive (and negative) results on the denoising power of several natural
dynamics. We then show experimentally that when combined with recent agnostic
active learning algorithms, this process can achieve low error from very few
queries, performing substantially better than active or passive learning
without these denoising dynamics as well as passive learning with denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6647</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6647</id><created>2014-06-25</created><updated>2014-07-05</updated><authors><author><keyname>Qiao</keyname><forenames>Yue</forenames></author><author><keyname>Srinivasan</keyname><forenames>Kannan</forenames></author><author><keyname>Arora</keyname><forenames>Anish</forenames></author></authors><title>Extract Secrets from Wireless Channel: A New Shape-based Approach</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing secret key extraction techniques use quantization to map wireless
channel amplitudes to secret bits. This pa- per shows that such techniques are
highly prone to environ- ment and local noise effects: They have very high
mismatch rates between the two nodes that measure the channel be- tween them.
This paper advocates using the shape of the channel instead of the size (or
amplitude) of the channel. It shows that this new paradigm shift is
significantly ro- bust against environmental and local noises. We refer to this
shape-based technique as Puzzle. Implementation in a software-defined radio
(SDR) platform demonstrates that Puzzle has a 63% reduction in bit mismatch
rate than the state-of-art frequency domain approach (CSI-2bit). Exper- iments
also show that unlike the state-of-the-art received signal strength (RSS)-based
methods like ASBG, Puzzle is robust against an attack in which an eavesdropper
can pre- dict the secret bits using planned movements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6651</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6651</id><created>2014-06-25</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Ishanu</forenames></author></authors><title>Causality Networks</title><categories>cs.LG cs.IT math.IT q-fin.ST stat.ML</categories><comments>22 pages, 8 figures</comments><msc-class>68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While correlation measures are used to discern statistical relationships
between observed variables in almost all branches of data-driven scientific
inquiry, what we are really interested in is the existence of causal
dependence. Designing an efficient causality test, that may be carried out in
the absence of restrictive pre-suppositions on the underlying dynamical
structure of the data at hand, is non-trivial. Nevertheless, ability to
computationally infer statistical prima facie evidence of causal dependence may
yield a far more discriminative tool for data analysis compared to the
calculation of simple correlations. In the present work, we present a new
non-parametric test of Granger causality for quantized or symbolic data streams
generated by ergodic stationary sources. In contrast to state-of-art binary
tests, our approach makes precise and computes the degree of causal dependence
between data streams, without making any restrictive assumptions, linearity or
otherwise. Additionally, without any a priori imposition of specific dynamical
structure, we infer explicit generative models of causal cross-dependence,
which may be then used for prediction. These explicit models are represented as
generalized probabilistic automata, referred to crossed automata, and are shown
to be sufficient to capture a fairly general class of causal dependence. The
proposed algorithms are computationally efficient in the PAC sense; $i.e.$, we
find good models of cross-dependence with high probability, with polynomial
run-times and sample complexities. The theoretical results are applied to
weekly search-frequency data from Google Trends API for a chosen set of
socially &quot;charged&quot; keywords. The causality network inferred from this dataset
reveals, quite expectedly, the causal importance of certain keywords. It is
also illustrated that correlation analysis fails to gather such insight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6667</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6667</id><created>2014-06-25</created><updated>2014-07-30</updated><authors><author><keyname>Crotty</keyname><forenames>Andrew</forenames></author><author><keyname>Galakatos</keyname><forenames>Alex</forenames></author><author><keyname>Dursun</keyname><forenames>Kayhan</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author><author><keyname>Cetintemel</keyname><forenames>Ugur</forenames></author><author><keyname>Zdonik</keyname><forenames>Stan</forenames></author></authors><title>Tupleware: Redefining Modern Analytics</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a fundamental discrepancy between the targeted and actual users of
current analytics frameworks. Most systems are designed for the data and
infrastructure of the Googles and Facebooks of the world---petabytes of data
distributed across large cloud deployments consisting of thousands of cheap
commodity machines. Yet, the vast majority of users operate clusters ranging
from a few to a few dozen nodes, analyze relatively small datasets of up to a
few terabytes, and perform primarily compute-intensive operations. Targeting
these users fundamentally changes the way we should build analytics systems.
  This paper describes the design of Tupleware, a new system specifically aimed
at the challenges faced by the typical user. Tupleware's architecture brings
together ideas from the database, compiler, and programming languages
communities to create a powerful end-to-end solution for data analysis. We
propose novel techniques that consider the data, computations, and hardware
together to achieve maximum performance on a case-by-case basis. Our
experimental evaluation quantifies the impact of our novel techniques and shows
orders of magnitude performance improvement over alternative systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6669</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6669</id><created>2014-06-24</created><authors><author><keyname>Tsegkis</keyname><forenames>Christos</forenames></author></authors><title>A note on the causality of singular linear discrete time systems</title><categories>math.RA cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study the causality of non-homogeneous linear singular
discrete time systems whose coefficients are square constant matrices. By
assuming that the input vector changes only at equally space sampling instants
we provide properties for causality between state and inputs and causality
between output and inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6672</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6672</id><created>2014-06-25</created><authors><author><keyname>Azrieli</keyname><forenames>Yaron</forenames></author><author><keyname>Shmaya</keyname><forenames>Eran</forenames></author></authors><title>Rental harmony with roommates</title><categories>cs.GT</categories><comments>forthcoming in Journal of Economic Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove existence of envy-free allocations in markets with heterogenous
indivisible goods and money, when a given quantity is supplied from each of the
goods and agents have unit demands. We depart from most of the previous
literature by allowing agents' preferences over the goods to depend on the
entire vector of prices. Our proof uses Shapley's K-K-M-S theorem and Hall's
marriage lemma. We then show how our theorem may be applied in two related
problems: Existence of envy-free allocations in a version of the cake-cutting
problem, and existence of equilibrium in an exchange economy with indivisible
goods and money.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6683</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6683</id><created>2014-06-25</created><authors><author><keyname>Chakraborty</keyname><forenames>Souymodip</forenames></author><author><keyname>Kataon</keyname><forenames>Joost-Pieter</forenames></author></authors><title>Parametric LTL on Markov Chains</title><categories>cs.LO cs.FL</categories><comments>TCS Track B 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the verification of finite Markov chains against
parametrized LTL (pLTL) formulas. In pLTL, the until-modality is equipped with
a bound that contains variables; e.g., $\Diamond_{\le x}\ \varphi$ asserts that
$\varphi$ holds within $x$ time steps, where $x$ is a variable on natural
numbers. The central problem studied in this paper is to determine the set of
parameter valuations $V_{\prec p} (\varphi)$ for which the probability to
satisfy pLTL-formula $\varphi$ in a Markov chain meets a given threshold $\prec
p$, where $\prec$ is a comparison on reals and $p$ a probability. As for pLTL
determining the emptiness of $V_{&gt; 0}(\varphi)$ is undecidable, we consider
several logic fragments. We consider parametric reachability properties, a
sub-logic of pLTL restricted to next and $\Diamond_{\le x}$, parametric B\&quot;uchi
properties and finally, a maximal subclass of pLTL for which emptiness of $V_{&gt;
0}(\varphi)$ is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6705</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6705</id><created>2014-06-25</created><authors><author><keyname>Mellah</keyname><forenames>Mohamed Adnane</forenames></author><author><keyname>Amine</keyname><forenames>Abdelmalek</forenames></author><author><keyname>Hamou</keyname><forenames>Reda Mohamed</forenames></author><author><keyname>Kumar</keyname><forenames>A. V. Senthil</forenames></author></authors><title>Link Analysis for Communities Detection on Facebook</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 7 figures, International Journal of Data Mining And
  Emerging Technologies, 2014, Volume 4, Issue 1</comments><doi>10.5958/2249-3220.2014.00017.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks have become a part in the daily life of millions of users,
which offer wide range of interests and practices. The main characteristic of
social networks is its ability to gather different individuals around a common
point of view or collective beliefs. Among the current social networking sites,
Facebook is the most popular, which has the highest number of users. However,
in Facebook, the existence of communities (groups)is a critical question; thus,
many researchers focus on potential communities by using techniques like data
mining and web mining. In this work, we present four approaches based on link
analysis techniques to detect prospective groups and their members
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6706</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6706</id><created>2014-06-25</created><updated>2015-08-10</updated><authors><author><keyname>Farmer</keyname><forenames>William M.</forenames></author></authors><title>Simple Type Theory with Undefinedness, Quotation, and Evaluation</title><categories>math.LO cs.LO</categories><comments>This research was supported by NSERC</comments><msc-class>03B15 (Primary), 03B35 (Secondary)</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a version of simple type theory called ${\cal Q}^{\rm
uqe}_{0}$ that is based on ${\cal Q}_0$, the elegant formulation of Church's
type theory created and extensively studied by Peter B. Andrews. ${\cal Q}^{\rm
uqe}_{0}$ directly formalizes the traditional approach to undefinedness in
which undefined expressions are treated as legitimate, nondenoting expressions
that can be components of meaningful statements. ${\cal Q}^{\rm uqe}_{0}$ is
also equipped with a facility for reasoning about the syntax of expressions
based on quotation and evaluation. Quotation is used to refer to a syntactic
value that represents the syntactic structure of an expression, and evaluation
is used to refer to the value of the expression that a syntactic value
represents. With quotation and evaluation it is possible to reason in ${\cal
Q}^{\rm uqe}_{0}$ about the interplay of the syntax and semantics of
expressions and, as a result, to formalize in ${\cal Q}^{\rm uqe}_{0}$
syntax-based mathematical algorithms. The paper gives the syntax and semantics
of ${\cal Q}^{\rm uqe}_{0}$ as well as a proof system for ${\cal Q}^{\rm
uqe}_{0}$. The proof system is shown to be sound for all formulas and complete
for formulas that do not contain evaluations. The paper also illustrates some
applications of ${\cal Q}^{\rm uqe}_{0}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6712</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6712</id><created>2014-06-25</created><updated>2015-06-19</updated><authors><author><keyname>Ch&#xe1;vez-Dom&#xed;nguez</keyname><forenames>Javier Alejandro</forenames></author><author><keyname>Kutzarova</keyname><forenames>Denka</forenames></author></authors><title>Stability of low-rank matrix recovery and its connections to Banach
  space geometry</title><categories>math.FA cs.IT math.IT</categories><comments>16 pages</comments><journal-ref>J. Math. Anal. Appl. 427 (2015), no. 1, 320--335</journal-ref><doi>10.1016/j.jmaa.2015.02.041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are well-known relationships between compressed sensing and the
geometry of the finite-dimensional $\ell_p$ spaces. A result of Kashin and
Temlyakov can be described as a characterization of the stability of the
recovery of sparse vectors via $\ell_1$-minimization in terms of the Gelfand
widths of certain identity mappings between finite-dimensional $\ell_1$ and
$\ell_2$ spaces, whereas a more recent result of Foucart, Pajor, Rauhut and
Ullrich proves an analogous relationship even for $\ell_p$ spaces with $p &lt; 1$.
In this paper we prove what we call matrix or noncommutative versions of these
results: we characterize the stability of low-rank matrix recovery via Schatten
$p$-(quasi-)norm minimization in terms of the Gelfand widths of certain
identity mappings between finite-dimensional Schatten $p$-spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6720</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6720</id><created>2014-06-25</created><authors><author><keyname>Kia</keyname><forenames>Seyed Mostafa</forenames></author></authors><title>Mass-Univariate Hypothesis Testing on MEEG Data using Cross-Validation</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>Master thesis, July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in statistical theory, together with advances in the
computational power of computers, provide alternative methods to do
mass-univariate hypothesis testing in which a large number of univariate tests,
can be properly used to compare MEEG data at a large number of time-frequency
points and scalp locations. One of the major problematic aspects of this kind
of mass-univariate analysis is due to high number of accomplished hypothesis
tests. Hence procedures that remove or alleviate the increased probability of
false discoveries are crucial for this type of analysis. Here, I propose a new
method for mass-univariate analysis of MEEG data based on cross-validation
scheme. In this method, I suggest a hierarchical classification procedure under
k-fold cross-validation to detect which sensors at which time-bin and which
frequency-bin contributes in discriminating between two different stimuli or
tasks. To achieve this goal, a new feature extraction method based on the
discrete cosine transform (DCT) employed to get maximum advantage of all three
data dimensions. Employing cross-validation and hierarchy architecture
alongside the DCT feature space makes this method more reliable and at the same
time enough sensitive to detect the narrow effects in brain activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6730</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6730</id><created>2014-06-25</created><updated>2016-03-08</updated><authors><author><keyname>No</keyname><forenames>Albert</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Rateless Lossy Compression via the Extremes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We begin by presenting a simple lossy compressor operating at near-zero rate:
The encoder merely describes the indices of the few maximal source components,
while the decoder's reconstruction is a natural estimate of the source
components based on this information. This scheme turns out to be near-optimal
for the memoryless Gaussian source in the sense of achieving the zero-rate
slope of its distortion-rate function. Motivated by this finding, we then
propose a scheme comprised of iterating the above lossy compressor on an
appropriately transformed version of the difference between the source and its
reconstruction from the previous iteration. The proposed scheme achieves the
rate distortion function of the Gaussian memoryless source (under squared error
distortion) when employed on any finite-variance ergodic source. It further
possesses desirable properties we respectively refer to as infinitesimal
successive refinability, ratelessness, and complete separability. Its storage
and computation requirements are of order no more than $\frac{n^2}{\log^{\beta}
n}$ per source symbol for $\beta&gt;0$ at both the encoder and decoder. Though the
details of its derivation, construction, and analysis differ considerably, we
discuss similarities between the proposed scheme and the recently introduced
Sparse Regression Codes (SPARC) of Venkataramanan et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6758</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6758</id><created>2014-06-25</created><updated>2014-09-26</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu R.</forenames></author></authors><title>Information Spectrum Approach to Strong Converse Theorems for Degraded
  Wiretap Channels</title><categories>cs.IT cs.CR math.IT</categories><comments>Presented at Allerton Conference 2014; Submitted to the IEEE Journal
  of Selected Topics in Signal Processing; v2 corrected typos and strengthened
  Theorem 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider block codes for degraded wiretap channels in which the legitimate
receiver decodes the message with an asymptotic error probability no larger
than $\varepsilon$ but the leakage to the eavesdropper vanishes. For discrete
memoryless and Gaussian wiretap channels, we show that the maximum rate of
transmission does not depend on $\varepsilon\in [0,1)$, i.e., such channels
possess the partial strong converse property. Furthermore, we derive sufficient
conditions for the partial strong converse property to hold for memoryless but
non-stationary symmetric and degraded wiretap channels. Our proof techniques
leverage the information spectrum method, which allows us to establish a
necessary and sufficient condition for the partial strong converse to hold for
general wiretap channels without any information stability assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6764</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6764</id><created>2014-06-26</created><authors><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>A factorization criterion for acyclic directed mixed graphs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-462-470</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acyclic directed mixed graphs, also known as semi-Markov models represent the
conditional independence structure induced on an observed margin by a DAG model
with latent variables. In this paper we present a factorization criterion for
these models that is equivalent to the global Markov property given by (the
natural extension of) d-separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6772</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6772</id><created>2014-06-26</created><updated>2014-11-09</updated><authors><author><keyname>Chen</keyname><forenames>Yung-Chih</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Khalili</keyname><forenames>Ramin</forenames></author></authors><title>MSPlayer: Multi-Source and multi-Path LeverAged YoutubER</title><categories>cs.NI cs.MM</categories><comments>accepted to ACM CoNEXT'14</comments><acm-class>C.2; C.2.1; C.4</acm-class><doi>10.1145/2674005.2675007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online video streaming through mobile devices has become extremely popular
nowadays. YouTube, for example, reported that the percentage of its traffic
streaming to mobile devices has soared from 6% to more than 40% over the past
two years. Moreover, people are constantly seeking to stream high quality video
for better experience while often suffering from limited bandwidth. Thanks to
the rapid deployment of content delivery networks (CDNs), popular videos are
now replicated at different sites, and users can stream videos from close-by
locations with low latencies. As mobile devices nowadays are equipped with
multiple wireless interfaces (e.g., WiFi and 3G/4G), aggregating bandwidth for
high definition video streaming has become possible.
  We propose a client-based video streaming solution, MSPlayer, that takes
advantage of multiple video sources as well as multiple network paths through
different interfaces. MSPlayer reduces start-up latency and provides high
quality video streaming and robust data transport in mobile scenarios. We
experimentally demonstrate our solution on a testbed and through the YouTube
video service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6773</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6773</id><created>2014-06-26</created><authors><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author></authors><title>Approximately Optimal Mechanism Design: Motivation, Examples, and
  Lessons Learned</title><categories>cs.GT cs.DS</categories><comments>Based on a talk given by the author at the 15th ACM Conference on
  Economics and Computation (EC), June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal mechanism design enjoys a beautiful and well-developed theory, and
also a number of killer applications. Rules of thumb produced by the field
influence everything from how governments sell wireless spectrum licenses to
how the major search engines auction off online advertising. There are,
however, some basic problems for which the traditional optimal mechanism design
approach is ill-suited --- either because it makes overly strong assumptions,
or because it advocates overly complex designs. The thesis of this paper is
that approximately optimal mechanisms allow us to reason about fundamental
questions that seem out of reach of the traditional theory.
  This survey has three main parts. The first part describes the approximately
optimal mechanism design paradigm --- how it works, and what we aim to learn by
applying it. The second and third parts of the survey cover two case studies,
where we instantiate the general design paradigm to investigate two basic
questions. In the first example, we consider revenue maximization in a
single-item auction with heterogeneous bidders. Our goal is to understand if
complexity --- in the sense of detailed distributional knowledge --- is an
essential feature of good auctions for this problem, or alternatively if there
are simpler auctions that are near-optimal. The second example considers
welfare maximization with multiple items. Our goal here is similar in spirit:
when is complexity --- in the form of high-dimensional bid spaces --- an
essential feature of every auction that guarantees reasonable welfare? Are
there interesting cases where low-dimensional bid spaces suffice?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6778</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6778</id><created>2014-06-26</created><authors><author><keyname>Mahobiya</keyname><forenames>Chandrakant</forenames></author><author><keyname>Kumar</keyname><forenames>M.</forenames></author></authors><title>Performance Comparison of Two Streaming Data Clustering Algorithms</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The weighted fuzzy c-mean clustering algorithm and weighted fuzzy
c-mean-adaptive cluster number are extension of traditional fuzzy c-mean
Algorithm to stream data clustering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6783</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6783</id><created>2014-06-26</created><authors><author><keyname>Krishnan</keyname><forenames>M. Nikhil</forenames></author><author><keyname>Prakash</keyname><forenames>N.</forenames></author><author><keyname>Lalitha</keyname><forenames>V.</forenames></author><author><keyname>Sasidharan</keyname><forenames>Birenjith</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author><author><keyname>Narayanamurthy</keyname><forenames>Srinivasan</forenames></author><author><keyname>Kumar</keyname><forenames>Ranjit</forenames></author><author><keyname>Nandi</keyname><forenames>Siddhartha</forenames></author></authors><title>Evaluation of Codes with Inherent Double Replication for Hadoop</title><categories>cs.IT math.IT</categories><comments>in Proceedings of Usenix HotStorage, Philadelphia, PA, June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we evaluate the efficacy, in a Hadoop setting, of two coding
schemes, both possessing an inherent double replication of data. The two coding
schemes belong to the class of regenerating and locally regenerating codes
respectively, and these two classes are representative of recent advances made
in designing codes for the efficient storage of data in a distributed setting.
In comparison with triple replication, double replication permits a significant
reduction in storage overhead, while delivering good MapReduce performance
under moderate work loads. The two coding solutions under evaluation here, add
only moderately to the storage overhead of double replication, while
simultaneously offering reliability levels similar to that of triple
replication.
  One might expect from the property of inherent data duplication that the
performance of these codes in executing a MapReduce job would be comparable to
that of double replication. However, a second feature of this class of code
comes into play here, namely that under both coding schemes analyzed here,
multiple blocks from the same coded stripe are required to be stored on the
same node. This concentration of data belonging to a single stripe negatively
impacts MapReduce execution times. However, much of this effect can be undone
by simply adding a larger number of processors per node. Further improvements
are possible if one tailors the Map task scheduler to the codes under
consideration. We present both experimental and simulation results that
validate these observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6786</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6786</id><created>2014-06-26</created><updated>2014-06-27</updated><authors><author><keyname>Mootz</keyname><forenames>Eric</forenames></author></authors><title>3D Texture Coordinates on Polygon Mesh Sequences</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for creating 3D texture coordinates for a sequence of polygon meshes
with changing topology and vertex motion vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6811</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6811</id><created>2014-06-26</created><updated>2014-09-17</updated><authors><author><keyname>Shen</keyname><forenames>Fumin</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author></authors><title>Face Image Classification by Pooling Raw Features</title><categories>cs.CV</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a very simple, efficient yet surprisingly effective feature
extraction method for face recognition (about 20 lines of Matlab code), which
is mainly inspired by spatial pyramid pooling in generic image classification.
We show that features formed by simply pooling local patches over a multi-level
pyramid, coupled with a linear classifier, can significantly outperform most
recent face recognition methods. The simplicity of our feature extraction
procedure is demonstrated by the fact that no learning is involved (except PCA
whitening). We show that, multi-level spatial pooling and dense extraction of
multi-scale patches play critical roles in face image classification. The
extracted facial features can capture strong structural information of
individual faces with no label information being used. We also find that,
pre-processing on local image patches such as contrast normalization can have
an important impact on the classification accuracy. In particular, on the
challenging face recognition datasets of FERET and LFW-a, our method improves
previous best results by more than 10% and 20%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6812</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6812</id><created>2014-06-26</created><authors><author><keyname>Abbasi-Yadkori</keyname><forenames>Yasin</forenames></author><author><keyname>Neu</keyname><forenames>Gergely</forenames></author></authors><title>Online learning in MDPs with side information</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online learning of finite Markov decision process (MDP) problems
when a side information vector is available. The problem is motivated by
applications such as clinical trials, recommendation systems, etc. Such
applications have an episodic structure, where each episode corresponds to a
patient/customer. Our objective is to compete with the optimal dynamic policy
that can take side information into account.
  We propose a computationally efficient algorithm and show that its regret is
at most $O(\sqrt{T})$, where $T$ is the number of rounds. To best of our
knowledge, this is the first regret bound for this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6818</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6818</id><created>2014-06-26</created><updated>2014-09-17</updated><authors><author><keyname>Shen</keyname><forenames>Fumin</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author></authors><title>Face Identification with Second-Order Pooling</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic face recognition has received significant performance improvement
by developing specialised facial image representations. On the other hand,
generic object recognition has rarely been applied to the face recognition.
Spatial pyramid pooling of features encoded by an over-complete dictionary has
been the key component of many state-of-the-art image classification systems.
Inspired by its success, in this work we develop a new face image
representation method inspired by the second-order pooling in Carreira et al.
[1], which was originally proposed for image segmentation. The proposed method
differs from the previous methods in that, we encode the densely extracted
local patches by a small-size dictionary; and the facial image signatures are
obtained by pooling the second-order statistics of the encoded features. We
show the importance of pooling on encoded features, which is bypassed by the
original second-order pooling method to avoid the high computational cost.
Equipped with a simple linear classifier, the proposed method outperforms the
state-of-the-art face identification performance by large margins. For example,
on the LFW databases, the proposed method performs better than the previous
best by around 13% accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6829</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6829</id><created>2014-06-26</created><authors><author><keyname>Brown</keyname><forenames>Chlo&#xeb;</forenames></author><author><keyname>Efstratiou</keyname><forenames>Christos</forenames></author><author><keyname>Leontiadis</keyname><forenames>Ilias</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author><author><keyname>Scott</keyname><forenames>James</forenames></author><author><keyname>Key</keyname><forenames>Peter</forenames></author></authors><title>The architecture of innovation: Tracking face-to-face interactions with
  ubicomp technologies</title><categories>cs.CY cs.HC cs.SI</categories><comments>14 pages, 9 figures. To appear in ACM International Joint Conference
  on Pervasive and Ubiquitous Computing (Ubicomp 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The layouts of the buildings we live in shape our everyday lives. In office
environments, building spaces affect employees' communication, which is crucial
for productivity and innovation. However, accurate measurement of how spatial
layouts affect interactions is a major challenge and traditional techniques may
not give an objective view.We measure the impact of building spaces on social
interactions using wearable sensing devices. We study a single organization
that moved between two different buildings, affording a unique opportunity to
examine how space alone can affect interactions. The analysis is based on two
large scale deployments of wireless sensing technologies: short-range,
lightweight RFID tags capable of detecting face-to-face interactions. We
analyze the traces to study the impact of the building change on social
behavior, which represents a first example of using ubiquitous sensing
technology to study how the physical design of two workplaces combines with
organizational structure to shape contact patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6832</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6832</id><created>2014-06-26</created><authors><author><keyname>Crampes</keyname><forenames>Michel</forenames></author><author><keyname>Planti&#xe9;</keyname><forenames>Michel</forenames></author></authors><title>Overlapping Community Detection Optimization and Nash Equilibrium</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>Submitted to KDD</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection using both graphs and social networks is the focus of
many algorithms. Recent methods aimed at optimizing the so-called modularity
function proceed by maximizing relations within communities while minimizing
inter-community relations.
  However, given the NP-completeness of the problem, these algorithms are
heuristics that do not guarantee an optimum. In this paper, we introduce a new
algorithm along with a function that takes an approximate solution and modifies
it in order to reach an optimum. This reassignment function is considered a
'potential function' and becomes a necessary condition to asserting that the
computed optimum is indeed a Nash Equilibrium. We also use this function to
simultaneously show partitioning and overlapping communities, two detection and
visualization modes of great value in revealing interesting features of a
social network. Our approach is successfully illustrated through several
experiments on either real unipartite, multipartite or directed graphs of
medium and large-sized datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6834</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6834</id><created>2014-06-26</created><authors><author><keyname>M&#xfc;ller</keyname><forenames>Klaus</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>A Model-Based Approach to Impact Analysis Using Model Differencing</title><categories>cs.SE</categories><comments>16 pages, 5 figures, In: Proceedings of the 8th International
  Workshop on Software Quality and Maintainability (SQM), ECEASST Journal, vol.
  65 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Impact analysis is concerned with the identification of consequences of
changes and is therefore an important activity for software evolution. In
modelbased software development, models are core artifacts, which are often
used to generate essential parts of a software system. Changes to a model can
thus substantially affect different artifacts of a software system. In this
paper, we propose a modelbased approach to impact analysis, in which explicit
impact rules can be specified in a domain specific language (DSL). These impact
rules define consequences of designated UML class diagram changes on software
artifacts and the need of dependent activities such as data evolution. The UML
class diagram changes are identified automatically using model differencing.
The advantage of using explicit impact rules is that they enable the
formalization of knowledge about a product. By explicitly defining this
knowledge, it is possible to create a checklist with hints about development
steps that are (potentially) necessary to manage the evolution. To validate the
feasibility of our approach, we provide results of a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6840</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6840</id><created>2014-06-26</created><authors><author><keyname>Joshi</keyname><forenames>Hardik</forenames></author></authors><title>From Citation count to Argumentation count: a new metric to indicate the
  usefulness of an article</title><categories>cs.IR cs.DL</categories><comments>Technical Conference cum Workshop on Digital Library Using DSpace
  hosted by Gujarat National Law University on 21-23 March, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation count is a quantifiable measure to indicate the number of times an
article is cited by other articles. It is believed that if an article is cited
often then it must be an important or influential article; however, there is no
guarantee that the most cited articles are good in quality. In this paper, the
author suggests argumentation count, a new metric for citation analysis. The
proposed metric, argumentation count is a triplet of quantities for each
concept of an article that helps in providing a quantifiable measure about the
usefulness of an article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6844</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6844</id><created>2014-06-26</created><authors><author><keyname>Gruzitis</keyname><forenames>Normunds</forenames></author><author><keyname>Paikens</keyname><forenames>Peteris</forenames></author><author><keyname>Barzdins</keyname><forenames>Guntis</forenames></author></authors><title>FrameNet Resource Grammar Library for GF</title><categories>cs.CL</categories><journal-ref>Lecture Notes in Computer Science, Vol. 7427, Springer, 2012, pp.
  121-137</journal-ref><doi>10.1007/978-3-642-32612-7_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an ongoing research investigating the possibility
and potential of integrating frame semantics, particularly FrameNet, in the
Grammatical Framework (GF) application grammar development. An important
component of GF is its Resource Grammar Library (RGL) that encapsulates the
low-level linguistic knowledge about morphology and syntax of currently more
than 20 languages facilitating rapid development of multilingual applications.
In the ideal case, porting a GF application grammar to a new language would
only require introducing the domain lexicon - translation equivalents that are
interlinked via common abstract terms. While it is possible for a highly
restricted CNL, developing and porting a less restricted CNL requires above
average linguistic knowledge about the particular language, and above average
GF experience. Specifying a lexicon is mostly straightforward in the case of
nouns (incl. multi-word units), however, verbs are the most complex category
(in terms of both inflectional paradigms and argument structure), and adding
them to a GF application grammar is not a straightforward task. In this paper
we are focusing on verbs, investigating the possibility of creating a
multilingual FrameNet-based GF library. We propose an extension to the current
RGL, allowing GF application developers to define clauses on the semantic
level, thus leaving the language-specific syntactic mapping to this extension.
We demonstrate our approach by reengineering the MOLTO Phrasebook application
grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6852</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6852</id><created>2014-06-26</created><updated>2015-07-09</updated><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Frame Based Precoding in Satellite Communications: A Multicast Approach</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at the IEEE ASMS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present work, a multibeam satellite that employs aggressive frequency
reuse towards increasing the offered throughput is considered. Focusing on the
forward link, the goal is to employ multi-antenna signal processing techniques,
namely linear precoding, to manage the inter-beam interferences. In this
context, fundamental practical limitations, namely the rigid framing structure
of satellite communication standards and the on-board per-antenna power
constraints, are herein considered. Therefore, the concept of optimal frame
based precoding under per-antenna constraints, is discussed. This consists in
precoding the transmit signals without changing the underlying framing
structure of the communication standard. In the present work, the connection of
the frame based precoding problem with the generic signal processing problem of
conveying independent sets of common data to distinct groups of users is
established. This model is known as physical layer multicasting to multiple
co-channel groups. Building on recent results, the weighted fair per-antenna
power constrained multigroup multicast precoders are employed for frame based
precoding. The throughput performance of these solutions is compared to
multicast aware heuristic precoding methods over a realistic multibeam
satellite scenario. Consequently, the gains of the proposed approach are
quantified via extensive numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6854</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6854</id><created>2014-06-26</created><authors><author><keyname>Xu</keyname><forenames>Jinwei</forenames></author><author><keyname>Hu</keyname><forenames>Jiankun</forenames></author><author><keyname>Jia</keyname><forenames>Xiuping</forenames></author></authors><title>A Fully Automated Latent Fingerprint Matcher with Embedded Self-learning
  Segmentation Module</title><categories>cs.CR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent fingerprint has the practical value to identify the suspects who have
unintentionally left a trace of fingerprint in the crime scenes. However,
designing a fully automated latent fingerprint matcher is a very challenging
task as it needs to address many challenging issues including the separation of
overlapping structured patterns over the partial and poor quality latent
fingerprint image, and finding a match against a large background database that
would have different resolutions. Currently there is no fully automated latent
fingerprint matcher available to the public and most literature reports have
utilized a specialized latent fingerprint matcher COTS3 which is not accessible
to the public. This will make it infeasible to assess and compare the relevant
research work which is vital for this research community. In this study, we
target to develop a fully automated latent matcher for adaptive detection of
the region of interest and robust matching of latent prints. Unlike the
manually conducted matching procedure, the proposed latent matcher can run like
a sealed black box without any manual intervention. This matcher consists of
the following two modules: (i) the dictionary learning-based region of interest
(ROI) segmentation scheme; and (ii) the genetic algorithm-based minutiae set
matching unit. Experimental results on NIST SD27 latent fingerprint database
demonstrates that the proposed matcher outperforms the currently public
state-of-art latent fingerprint matcher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6859</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6859</id><created>2014-06-26</created><authors><author><keyname>Bulteau</keyname><forenames>Laurent</forenames></author><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Talmon</keyname><forenames>Nimrod</forenames></author></authors><title>Combinatorial Voter Control in Elections</title><categories>cs.MA cs.GT</categories><comments>An extended abstract appears in MFCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voter control problems model situations such as an external agent trying to
affect the result of an election by adding voters, for example by convincing
some voters to vote who would otherwise not attend the election. Traditionally,
voters are added one at a time, with the goal of making a distinguished
alternative win by adding a minimum number of voters. In this paper, we
initiate the study of combinatorial variants of control by adding voters: In
our setting, when we choose to add a voter~$v$, we also have to add a whole
bundle $\kappa(v)$ of voters associated with $v$. We study the computational
complexity of this problem for two of the most basic voting rules, namely the
Plurality rule and the Condorcet rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6873</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6873</id><created>2014-06-26</created><authors><author><keyname>Francisco</keyname><forenames>Matthew R.</forenames></author><author><keyname>Wood</keyname><forenames>Ian</forenames></author><author><keyname>&#x160;abanovi&#x107;</keyname><forenames>Selma</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Designing a minimalist socially aware robotic agent for the home</title><categories>cs.RO cs.HC</categories><comments>8 pages, 10 figures, To be published in the ALIFE 14 conference
  proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a minimalist social robot that relies on long timeseries of low
resolution data such as mechanical vibration, temperature, lighting, sounds and
collisions. Our goal is to develop an experimental system for growing socially
situated robotic agents whose behavioral repertoire is subsumed by the social
order of the space. To get there we are designing robots that use their simple
sensors and motion feedback routines to recognize different classes of human
activity and then associate to each class a range of appropriate behaviors. We
use the Katie Family of robots, built on the iRobot Create platform, an Arduino
Uno, and a Raspberry Pi. We describe its sensor abilities and exploratory tests
that allow us to develop hypotheses about what objects (sensor data) correspond
to something known and observable by a human subject. We use machine learning
methods to classify three social scenarios from over a hundred experiments,
demonstrating that it is possible to detect social situations with high
accuracy, using the low-resolution sensors from our minimalist robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6889</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6889</id><created>2014-06-26</created><updated>2014-07-10</updated><authors><author><keyname>Meunier</keyname><forenames>Pierre-&#xc9;tienne</forenames></author></authors><title>Noncooperative algorithms in self-assembly</title><categories>cs.CG cs.CC cs.DS cs.FL</categories><comments>A few bug fixes and typo corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the first non-trivial positive algorithmic results (i.e. programs
whose output is larger than their size), in a model of self-assembly that has
so far resisted many attempts of formal analysis or programming: the planar
non-cooperative variant of Winfree's abstract Tile Assembly Model.
  This model has been the center of several open problems and conjectures in
the last fifteen years, and the first fully general results on its
computational power were only proven recently (SODA 2014). These results, as
well as ours, exemplify the intricate connections between computation and
geometry that can occur in self-assembly.
  In this model, tiles can stick to an existing assembly as soon as one of
their sides matches the existing assembly. This feature contrasts with the
general cooperative model, where it can be required that tiles match on
\emph{several} of their sides in order to bind.
  In order to describe our algorithms, we also introduce a generalization of
regular expressions called Baggins expressions. Finally, we compare this model
to other automata-theoretic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6900</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6900</id><created>2014-06-26</created><updated>2014-10-18</updated><authors><author><keyname>Kuehn</keyname><forenames>Christian</forenames></author></authors><title>Efficient Gluing of Numerical Continuation and a Multiple Solution
  Method for Elliptic PDEs</title><categories>math.DS cs.MS math.NA nlin.PS physics.comp-ph</categories><comments>Revised version based upon referee comments, 11 figures, shortened
  online abstract and slightly lower quality figures due to arXiv size
  limitations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical continuation calculations for ordinary differential equations
(ODEs) are, by now, an established tool for bifurcation analysis in dynamical
systems theory as well as across almost all natural and engineering sciences.
Although several excellent standard software packages are available for ODEs,
there are - for good reasons - no standard numerical continuation toolboxes
available for partial differential equations (PDEs), which cover a broad range
of different classes of PDEs automatically. A natural approach to this problem
is to look for efficient gluing computation approaches, with independent
components developed by researchers in numerical analysis, dynamical systems,
scientific computing and mathematical modelling. In this paper, we shall study
several elliptic PDEs (Lane-Emden-Fowler, Lane-Emden-Fowler with microscopic
force, Caginalp) via the numerical continuation software pde2path and develop a
gluing component to determine a set of starting solutions for the continuation
by exploting the variational structures of the PDEs. In particular, we solve
the initialization problem of numerical continuation for PDEs via a minimax
algorithm to find multiple unstable solution. Furthermore, for the Caginalp
system, we illustrate the efficient gluing link of pde2path to the underlying
mesh generation and the FEM MatLab pdetoolbox. Even though the approach works
efficiently due to the high-level programming language and without developing
any new algorithms, we still obtain interesting bifurcation diagrams and
directly applicable conclusions about the three elliptic PDEs we study, in
particular with respect to symmetry-breaking. In particular, we show for a
modified Lane-Emden-Fowler equation with an asymmetric microscopic force, how a
fully connected bifurcation diagram splits up into C-shaped isolas on which
localized pattern deformation appears towards two different regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6901</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6901</id><created>2014-06-26</created><updated>2014-07-24</updated><authors><author><keyname>Redozubov</keyname><forenames>Alexey</forenames></author></authors><title>Pattern-wave model of brain. Mechanisms of information processing,
  memory organization</title><categories>cs.AI q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of the axon-dendrite connections of neurons of the brain
creates a rich spatial structure in which provided various combinations of
signals surrounding neurons. Structure of dendritic trees and shape of
dendritic spines allow repeatedly increase combinatorial component through
cross synapses influence neighboring neurons. In this paper it is shown that
the diffuse spreading of neurotransmitters allows neurons to detect and
remember significant set of environmental activity patterns. As a core element
fixation described extrasynaptic metabotropic receptive clusters. The described
mechanism leads to the appearance of wave processes, based on the propagation
of the front-line areas of spontaneous activity. In the proposed model, any
compact pattern of neural activity is seen as a source emitting a diverging
wave endogenous spikes. It is shown that the spike pattern of the wave front is
strictly unique and uniquely defined pattern that started the wave. The
propagation of waves with a unique pattern allows anywhere in nature undergoing
brain wave patterns there to judge the whole brain processes information. In
these assumptions naturally described mechanism of projection information
between regions of the cortex. Performed computer simulations show the high
effectiveness of such information model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6909</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6909</id><created>2014-06-26</created><updated>2015-06-19</updated><authors><author><keyname>Dosovitskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Fischer</keyname><forenames>Philipp</forenames></author><author><keyname>Springenberg</keyname><forenames>Jost Tobias</forenames></author><author><keyname>Riedmiller</keyname><forenames>Martin</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>Discriminative Unsupervised Feature Learning with Exemplar Convolutional
  Neural Networks</title><categories>cs.LG cs.CV cs.NE</categories><comments>PAMI submission. Includes matching experiments as in
  arXiv:1405.5769v1. Also includes new network architectures, experiments on
  Caltech-256, experiment on combining Exemplar-CNN with clustering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional networks have proven to be very successful in learning
task specific features that allow for unprecedented performance on various
computer vision tasks. Training of such networks follows mostly the supervised
learning paradigm, where sufficiently many input-output pairs are required for
training. Acquisition of large training sets is one of the key challenges, when
approaching a new task. In this paper, we aim for generic feature learning and
present an approach for training a convolutional network using only unlabeled
data. To this end, we train the network to discriminate between a set of
surrogate classes. Each surrogate class is formed by applying a variety of
transformations to a randomly sampled 'seed' image patch. In contrast to
supervised network training, the resulting feature representation is not class
specific. It rather provides robustness to the transformations that have been
applied during training. This generic feature representation allows for
classification results that outperform the state of the art for unsupervised
learning on several popular datasets (STL-10, CIFAR-10, Caltech-101,
Caltech-256). While such generic features cannot compete with class specific
features from supervised training on a classification task, we show that they
are advantageous on geometric matching problems, where they also outperform the
SIFT descriptor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6920</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6920</id><created>2014-06-26</created><authors><author><keyname>Guo</keyname><forenames>Chuan</forenames></author><author><keyname>Stinson</keyname><forenames>Douglas R.</forenames></author><author><keyname>van Trung</keyname><forenames>Tran</forenames></author></authors><title>On tight bounds for binary frameproof codes</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study $w$-frameproof codes, which are equivalent to
$\{1,w\}$-separating hash families. Our main results concern binary codes,
which are defined over an alphabet of two symbols. For all $w \geq 3$, and for
$w+1 \leq N \leq 3w$, we show that an $SHF(N; n,2, \{1,w \})$ exists only if $n
\leq N$, and an $SHF(N; N,2, \{1,w \})$ must be a permutation matrix of degree
$N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6924</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6924</id><created>2014-06-26</created><authors><author><keyname>Alberelli</keyname><forenames>Davide</forenames></author><author><keyname>Lella</keyname><forenames>Paolo</forenames></author></authors><title>Strongly stable ideals and Hilbert polynomials</title><categories>cs.SC cs.MS math.AC math.AG math.CO</categories><comments>Source code available at
  http://www.personalweb.unito.it/paolo.lella/EN/Publications_files/StronglyStableIdeals.m2</comments><msc-class>13P10, 13P99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \textit{StronglyStableIdeals} package for \textit{Macaulay2} provides a
method to compute all the saturated strongly stable ideals in a given
polynomial ring defining schemes with a fixed Hilbert polynomial. A description
of the main method and the auxiliary tools is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6937</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6937</id><created>2014-06-26</created><updated>2014-06-27</updated><authors><author><keyname>Hollmann</keyname><forenames>Diego A.</forenames></author><author><keyname>Cristi&#xe1;</keyname><forenames>Maximiliano</forenames></author><author><keyname>Frydman</keyname><forenames>Claudia</forenames></author></authors><title>A Family of Simulation Criteria to Guide DEVS Models Validation
  Rigorously, Systematically and Semi-Automatically</title><categories>cs.SE</categories><doi>10.1016/j.simpat.2014.07.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most common method to validate a DEVS model against the requirements is
to simulate it several times under different conditions, with some simulation
tool. The behavior of the model is compared with what the system is supposed to
do. The number of different scenarios to simulate is usually infinite,
therefore, selecting them becomes a crucial task. This selection, actually, is
made following the experience or intuition of an engineer. Here we present a
family of criteria to conduct DEVS model simulations in a disciplined way and
covering the most significant simulations to increase the confidence on the
model. This is achieved by analyzing the mathematical representation of the
DEVS model and, thus, part of the validation process can be automatized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6946</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6946</id><created>2014-06-26</created><updated>2014-06-27</updated><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Diaz</keyname><forenames>Margarita</forenames></author><author><keyname>Manzanares</keyname><forenames>Miguel</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author></authors><title>An improved computer vision method for detecting white blood cells</title><categories>cs.CV</categories><comments>20 Pages. arXiv admin note: text overlap with arXiv:1405.5164</comments><journal-ref>Computational and Mathematical Methods in Medicine, 2013, art. no.
  137392</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automatic detection of White Blood Cells (WBC) still remains as an
unsolved issue in medical imaging. The analysis of WBC images has engaged
researchers from fields of medicine and computer vision alike. Since WBC can be
approximated by an ellipsoid form, an ellipse detector algorithm may be
successfully applied in order to recognize them. This paper presents an
algorithm for the automatic detection of WBC embedded into complicated and
cluttered smear images that considers the complete process as a multi-ellipse
detection problem. The approach, based on the Differential Evolution (DE)
algorithm, transforms the detection task into an optimization problem where
individuals emulate candidate ellipses. An objective function evaluates if such
candidate ellipses are really present in the edge image of the smear. Guided by
the values of such function, the set of encoded candidate ellipses
(individuals) are evolved using the DE algorithm so that they can fit into the
WBC enclosed within the edge-only map of the image. Experimental results from
white blood cell images with a varying range of complexity are included to
validate the efficiency of the proposed technique in terms of accuracy and
robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6947</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6947</id><created>2014-06-26</created><authors><author><keyname>Zhu</keyname><forenames>Zhenyao</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Deep Learning Multi-View Representation for Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various factors, such as identities, views (poses), and illuminations, are
coupled in face images. Disentangling the identity and view representations is
a major challenge in face recognition. Existing face recognition systems either
use handcrafted features or learn features discriminatively to improve
recognition accuracy. This is different from the behavior of human brain.
Intriguingly, even without accessing 3D data, human not only can recognize face
identity, but can also imagine face images of a person under different
viewpoints given a single 2D image, making face perception in the brain robust
to view changes. In this sense, human brain has learned and encoded 3D face
models from 2D images. To take into account this instinct, this paper proposes
a novel deep neural net, named multi-view perceptron (MVP), which can untangle
the identity and view features, and infer a full spectrum of multi-view images
in the meanwhile, given a single 2D face image. The identity features of MVP
achieve superior performance on the MultiPIE dataset. MVP is also capable to
interpolate and predict images under viewpoints that are unobserved in the
training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6949</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6949</id><created>2014-06-26</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Subcarrier Domain of Multicarrier Continuous-Variable Quantum Key
  Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>25 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the subcarrier domain of multicarrier continuous-variable (CV)
quantum key distribution (QKD). In a multicarrier CVQKD scheme, the information
is granulated into Gaussian subcarrier CVs and the physical Gaussian link is
divided into Gaussian sub-channels. The sub-channels are dedicated for the
conveying of the subcarrier CVs. The angular domain utilizes the phase-space
angles of the Gaussian subcarrier CVs to construct the physical model of a
Gaussian sub-channel. The subcarrier domain injects physical attributes to the
description of the subcarrier transmission. We prove that the subcarrier domain
is a natural representation of the subcarrier-level transmission in a
multicarrier CVQKD scheme. We also extend the subcarrier domain to a
multiple-access multicarrier CVQKD setting. We demonstrate the results through
the adaptive multicarrier quadrature-division (AMQD) CVQKD scheme and the
AMQD-MQA (multiuser quadrature allocation) multiple-access multicarrier scheme.
The subcarrier domain representation provides a general apparatus that can be
utilized for an arbitrary multicarrier CVQKD scenario. The framework is
particularly convenient for experimental multicarrier CVQKD scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6950</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6950</id><created>2014-06-26</created><updated>2015-05-15</updated><authors><author><keyname>van de Belt</keyname><forenames>Jonathan</forenames></author><author><keyname>Ahmadi</keyname><forenames>Hamed</forenames></author><author><keyname>Doyle</keyname><forenames>Linda E.</forenames></author></authors><title>A Dynamic Embedding Algorithm for Wireless Network Virtualization</title><categories>cs.NI</categories><comments>6 pages, Accepted in VTC Fall 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless network virtualization enables multiple virtual wireless networks to
coexist on shared physical infrastructure. However, one of the main challenges
is the problem of assigning the physical resources to virtual networks in an
efficient manner. Although some work has been done on solving the embedding
problem for wireless networks, few solutions are applicable to dynamic networks
with changing traffic patterns. In this paper we propose a dynamic greedy
embedding algorithm for wireless virtualization. Virtual networks can be
re-embedded dynamically using this algorithm, enabling increased resource usage
and lower rejection rates. We compare the dynamic greedy algorithm to a static
embedding algorithm and also to its dynamic version. We show that the dynamic
algorithms provide increased performance to previous methods using simulated
traffic. In addition we formulate the embedding problem with multiple priority
levels for the static and dynamic case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6956</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6956</id><created>2014-06-26</created><updated>2015-03-10</updated><authors><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Venkat</keyname><forenames>Kartik</forenames></author><author><keyname>Han</keyname><forenames>Yanjun</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Minimax Estimation of Functionals of Discrete Distributions</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>To appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general methodology for the construction and analysis of minimax
estimators for a wide class of functionals of finite dimensional parameters,
and elaborate on the case of discrete distributions, where the alphabet size
$S$ is unknown and may be comparable with the number of observations $n$. We
treat the respective regions where the functional is &quot;nonsmooth&quot; and &quot;smooth&quot;
separately. In the &quot;nonsmooth&quot; regime, we apply an unbiased estimator for the
best polynomial approximation of the functional whereas, in the &quot;smooth&quot;
regime, we apply a bias-corrected Maximum Likelihood Estimator (MLE). We
illustrate the merit of this approach by thoroughly analyzing two important
cases: the entropy $H(P) = \sum_{i = 1}^S -p_i \ln p_i$ and $F_\alpha(P) =
\sum_{i = 1}^S p_i^\alpha,\alpha&gt;0$. We obtain the minimax $L_2$ rates for
estimating these functionals. In particular, we demonstrate that our estimator
achieves the optimal sample complexity $n \asymp S/\ln S$ for entropy
estimation. We also show that the sample complexity for estimating
$F_\alpha(P),0&lt;\alpha&lt;1$ is $n\asymp S^{1/\alpha}/ \ln S$, which can be
achieved by our estimator but not the MLE. For $1&lt;\alpha&lt;3/2$, we show the
minimax $L_2$ rate for estimating $F_\alpha(P)$ is $(n\ln n)^{-2(\alpha-1)}$
regardless of the alphabet size, while the $L_2$ rate for the MLE is
$n^{-2(\alpha-1)}$. For all the above cases, the behavior of the minimax
rate-optimal estimators with $n$ samples is essentially that of the MLE with
$n\ln n$ samples. We highlight the practical advantages of our schemes for
entropy and mutual information estimation. We demonstrate that our approach
reduces running time and boosts the accuracy compared to existing various
approaches. Moreover, we show that the mutual information estimator induced by
our methodology leads to significant performance boosts over the Chow--Liu
algorithm in learning graphical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6959</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6959</id><created>2014-06-26</created><updated>2015-02-22</updated><authors><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Venkat</keyname><forenames>Kartik</forenames></author><author><keyname>Han</keyname><forenames>Yanjun</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Maximum Likelihood Estimation of Functionals of Discrete Distributions</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general framework for analyzing the performance of the MLE
(Maximum Likelihood Estimator) in estimating functionals of discrete
distributions, under the worst-case mean squared error criterion. We show that
existing theory, which was developed to accommodate a fixed alphabet and a
growing number of observations, is insufficient for analyzing the bias of the
MLE, and apply the theory of approximation using positive linear operators to
study this bias. The variance is controlled using the well-known tools from the
literature on concentration inequalities. Our techniques yield a
characterization of the worst-case $L_2$ risk incurred by the MLE in estimating
the Shannon entropy $H(P) = \sum_{i = 1}^S -p_i \ln p_i$, and $F_\alpha(P) =
\sum_{i = 1}^S p_i^\alpha,\alpha&gt;0$ up to a multiplicative constant, for any
alphabet size $S\leq \infty$ and sample size $n$. We show that it is necessary
and sufficient to have $n \gg S$ observations for the MLE to be consistent in
Shannon entropy estimation. The MLE requires $n \gg S^{1/\alpha}$ samples to
consistently estimate $F_\alpha(P), 0&lt;\alpha&lt;1$. The minimax rate-optimal
estimators for both problems require $S/\ln S$ and $S^{1/\alpha}/\ln S$
samples, which implies that the MLE has a strictly sub-optimal sample
complexity. When $1&lt;\alpha&lt;3/2$, we show that the worst-case $L_2$ rate of
convergence for the MLE is $n^{-2(\alpha-1)}$ for infinite alphabet size, while
the minimax $L_2$ rate is $(n\ln n)^{-2(\alpha-1)}$. When $\alpha\geq 3/2$, the
MLE achieves the optimal $L_2$ convergence rate $n^{-1}$ regardless of the
alphabet size. We explicitly establish an equivalence between bias analysis of
plug-in estimators for general functionals under arbitrary statistical models,
and the theory of approximation using positive linear operators. This
equivalence is of relevance and consequence far beyond the specific problem
setting in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6962</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6962</id><created>2014-06-26</created><updated>2014-07-22</updated><authors><author><keyname>Hosang</keyname><forenames>Jan</forenames></author><author><keyname>Benenson</keyname><forenames>Rodrigo</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>How good are detection proposals, really?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current top performing Pascal VOC object detectors employ detection proposals
to guide the search for objects thereby avoiding exhaustive sliding window
search across images. Despite the popularity of detection proposals, it is
unclear which trade-offs are made when using them during object detection. We
provide an in depth analysis of ten object proposal methods along with four
baselines regarding ground truth annotation recall (on Pascal VOC 2007 and
ImageNet 2013), repeatability, and impact on DPM detector performance. Our
findings show common weaknesses of existing methods, and provide insights to
choose the most adequate method for different settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6973</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6973</id><created>2014-06-26</created><authors><author><keyname>Guha</keyname><forenames>R. V.</forenames></author></authors><title>Communicating and resolving entity references</title><categories>cs.AI</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statements about entities occur everywhere, from newspapers and web pages to
structured databases. Correlating references to entities across systems that
use different identifiers or names for them is a widespread problem. In this
paper, we show how shared knowledge between systems can be used to solve this
problem. We present &quot;reference by description&quot;, a formal model for resolving
references. We provide some results on the conditions under which a randomly
chosen entity in one system can, with high probability, be mapped to the same
entity in a different system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6995</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6995</id><created>2014-06-26</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Defection and extortion as unexpected catalysts of unconditional
  cooperation in structured populations</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>7 two-column pages, 5 figures; accepted for publication in Scientific
  Reports [related work available at http://arxiv.org/abs/1401.8294]</comments><journal-ref>Sci. Rep. 4 (2014) 5496</journal-ref><doi>10.1038/srep05496</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the evolution of cooperation in the spatial prisoner's dilemma game,
where besides unconditional cooperation and defection, tit-for-tat,
win-stay-lose-shift and extortion are the five competing strategies. While
pairwise imitation fails to sustain unconditional cooperation and extortion
regardless of game parametrization, myopic updating gives rise to the
coexistence of all five strategies if the temptation to defect is sufficiently
large or if the degree distribution of the interaction network is
heterogeneous. This counterintuitive evolutionary outcome emerges as a result
of an unexpected chain of strategy invasions. Firstly, defectors emerge and
coarsen spontaneously among players adopting win-stay-lose-shift. Secondly,
extortioners and players adopting tit-for-tat emerge and spread via neutral
drift among the emerged defectors. And lastly, among the extortioners,
cooperators become viable too. These recurrent evolutionary invasions yield a
five-strategy phase that is stable irrespective of the system size and the
structure of the interaction network, and they reveal the most unexpected
mechanism that stabilizes extortion and cooperation in an evolutionary setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.6998</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.6998</id><created>2014-06-22</created><authors><author><keyname>Boya</keyname><forenames>Q.</forenames></author><author><keyname>Cai</keyname><forenames>Y.</forenames></author><author><keyname>Champagne</keyname><forenames>B.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Zhao</keyname><forenames>M.</forenames></author></authors><title>Low-Complexity Variable Forgetting Factor Constrained Constant Modulus
  RLS Algorithm for Adaptive Beamforming</title><categories>cs.OH</categories><comments>10 pages, 4 figures, Elsevier Signal Processing, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a recursive least squares (RLS) based blind adaptive
beamforming algorithm that features a new variable forgetting factor (VFF)
mechanism is presented. The beamformer is designed according to the constrained
constant modulus (CCM) criterion, and the proposed adaptive algorithm operates
in the generalized sidelobe canceler (GSC) structure. A detailed study of its
operating properties is carried out, including a convexity analysis and a mean
squared error (MSE) analysis of its steady-state behavior. The results of
numerical experiments demonstrate that the proposed VFF mechanism achieves a
superior learning and tracking performance compared to other VFF mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7000</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7000</id><created>2014-06-26</created><authors><author><keyname>Klimek</keyname><forenames>Radoslaw</forenames></author></authors><title>Towards a Pattern-based Automatic Generation of Logical Specifications
  for Software Models</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work relates to the automatic generation of logical specifications,
considered as sets of temporal logic formulas, extracted directly from
developed software models. The extraction process is based on the assumption
that the whole developed model is structured using only predefined workflow
patterns. A method of automatic transformation of workflow patterns to logical
specifications is proposed. Applying the presented concepts enables bridging
the gap between the benefits of deductive reasoning for the correctness
verification process and the difficulties in obtaining complete logical
specifications for this process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7002</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7002</id><created>2014-06-24</created><authors><author><keyname>Nejati</keyname><forenames>Alireza</forenames></author><author><keyname>Unsworth</keyname><forenames>Charles</forenames></author></authors><title>A Concise Information-Theoretic Derivation of the Baum-Welch algorithm</title><categories>cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the Baum-Welch algorithm for hidden Markov models (HMMs) through an
information-theoretical approach using cross-entropy instead of the Lagrange
multiplier approach which is universal in machine learning literature. The
proposed approach provides a more concise derivation of the Baum-Welch method
and naturally generalizes to multiple observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7025</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7025</id><created>2014-06-26</created><authors><author><keyname>Brazil</keyname><forenames>Emilio Vital</forenames></author></authors><title>DASS: Detail Aware Sketch-Based Surface Modeling</title><categories>cs.GR</categories><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a sketch-based modeling system suitable for detail editing, based
on a multilevel representation for surfaces. The main advantage of this
representation allowing for the control of local (details) and global changes
of the model. We used an adaptive mesh (4-8 mesh) and developed a label theory
to construct a manifold structure, which is responsible for controlling local
editing of the model. The overall shape and global modifications are defined by
a variational implicit surface (Hermite RBF). Our system assembles the manifold
structures to allow the user to add details without changing the overall shape,
as well as edit the overall shape while repositioning details coherently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7036</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7036</id><created>2014-06-26</created><updated>2014-09-20</updated><authors><author><keyname>Yuan</keyname><forenames>Bo</forenames></author><author><keyname>Parhi</keyname><forenames>Keshab K.</forenames></author></authors><title>Low-Latency Successive-Cancellation List Decoders for Polar Codes with
  Multi-bit Decision</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE TVLSI in Feb 2014, accepted in Sep. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes, as the first provable capacity-achieving error-correcting codes,
have received much attention in recent years. However, the decoding performance
of polar codes with traditional successive-cancellation (SC) algorithm cannot
match that of the low-density parity-check (LDPC) or turbo codes. Because SC
list (SCL) decoding algorithm can significantly improve the error-correcting
performance of polar codes, design of SCL decoders is important for polar codes
to be deployed in practical applications. However, because the prior latency
reduction approaches for SC decoders are not applicable for SCL decoders, these
list decoders suffer from the long latency bottleneck. In this paper, we
propose a multi-bit-decision approach that can significantly reduce latency of
SCL decoders. First, we present a reformulated SCL algorithm that can perform
intermediate decoding of 2 bits together. The proposed approach, referred as
2-bit reformulated SCL (2b-rSCL) algorithm, can reduce the latency of SCL
decoder from (3n-2) to (2n-2) clock cycles without any performance loss. Then,
we extend the idea of 2-bit-decision to general case, and propose a general
decoding scheme that can perform intermediate decoding of any 2K bits
simultaneously. This general approach, referred as 2K-bit reformulated SCL
(2Kb-rSCL) algorithm, can reduce the overall decoding latency to as short as
n/2K-2-2 cycles. Furthermore, based on the proposed algorithms, VLSI
architectures for 2b-rSCL and 4b-rSCL decoders are synthesized. Compared with a
prior SCL decoder, the proposed (1024, 512) 2b-rSCL and 4b-rSCL decoders can
achieve 21% and 60% reduction in latency, 1.66 times and 2.77 times increase in
coded throughput with list size 2, and 2.11 times and 3.23 times increase in
coded throughput with list size 4, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7042</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7042</id><created>2014-06-26</created><authors><author><keyname>Li</keyname><forenames>Xihao</forenames></author><author><keyname>Sarris</keyname><forenames>Costas D.</forenames></author><author><keyname>Triverio</keyname><forenames>Piero</forenames></author></authors><title>Structure-Preserving Reduction of Finite-Difference Time-Domain
  Equations with Controllable Stability Beyond the CFL Limit</title><categories>cs.CE physics.comp-ph</categories><doi>10.1109/TMTT.2014.2366140</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The timestep of the Finite-Difference Time-Domain method (FDTD) is
constrained by the stability limit known as the Courant-Friedrichs-Lewy (CFL)
condition. This limit can make FDTD simulations quite time consuming for
structures containing small geometrical details. Several methods have been
proposed in the literature to extend the CFL limit, including implicit FDTD
methods and filtering techniques. In this paper, we propose a novel approach
which combines model order reduction and a perturbation algorithm to accelerate
FDTD simulations beyond the CFL barrier. We compare the proposed algorithm
against existing implicit and explicit CFL extension techniques, demonstrating
increased accuracy and performance on a large number of test cases, including
resonant cavities, a waveguide structure, a focusing metascreen and a
microstrip filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7048</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7048</id><created>2014-06-26</created><authors><author><keyname>Goh</keyname><forenames>Ong Sing</forenames></author></authors><title>Global Internet-based Crisis Communication: A case Study on SARS</title><categories>cs.CY</categories><comments>International Journal of Intelligent Information Technologies,
  Inaugural Issue: January 2005, Idea Group Publishing, ISSN: 1548-3657, USA</comments><report-no>TJ211.G64 2004</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crisis Communication is an effective communication mechanism in the world.
The outbreak of the SARS disease and the way information on it was disseminated
has illustrated the importance of effective and efficient crisis communication
management. Consequently, we would like to highlight the viability of
incorporating an intelligent agent software robot into a crisis communication
portal (CCNet) to send the alert news to subscribed users via email and others
mobile services such as SMS, MMS and GPRS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7049</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7049</id><created>2014-06-26</created><authors><author><keyname>Azgin</keyname><forenames>Aytac</forenames></author><author><keyname>Ravindran</keyname><forenames>Ravishankar</forenames></author><author><keyname>Wang</keyname><forenames>Guoqiang</forenames></author></authors><title>A Scalable Mobility-Centric Architecture for Named Data Networking</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-centric networking (ICN) proposes to redesign the Internet by
replacing its host centric design with an information centric one, by
establishing communication at the naming level, with the receiver side acting
as the driving force behind content delivery. Such design promises great
advantages for the delivery of content to and from mobile hosts. This, however,
is at the expense of increased networking overhead, specifically in the case of
Named-data Networking (NDN) due to use of flooding for path recovery. In this
paper, we propose a mobility centric solution to address the overhead and
scalability problems in NDN by introducing a novel forwarding architecture that
leverages decentralized server-assisted routing over flooding based strategies.
We present an in-depth study of the proposed architecture and provide
demonstrative results on its throughput and overhead performance at different
levels of mobility proving its scalability and effectiveness, when compared to
the current NDN based forwarding strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7054</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7054</id><created>2014-06-26</created><authors><author><keyname>Wu</keyname><forenames>Jiyan</forenames></author><author><keyname>Cheng</keyname><forenames>Bo</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Shang</keyname><forenames>Yanlei</forenames></author><author><keyname>Chen</keyname><forenames>Junliang</forenames></author></authors><title>Distortion-Aware Concurrent Multipath Transfer for Mobile Video
  Streaming in Heterogeneous Wireless Networks</title><categories>cs.NI</categories><comments>This paper has already accepted for publication in IEEE Transactions
  on Mobile Computing on Jun, 23rd, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The massive proliferation of wireless infrastructures with complementary
characteristics prompts the bandwidth aggregation for Concurrent Multipath
Transfer (CMT) over heterogeneous access networks. Stream Control Transmission
Protocol (SCTP) is the standard transport-layer solution to enable CMT in
multihomed communication environments. However, delivering high-quality
streaming video with the existing CMT solutions still remains problematic due
to the stringent QoS (Quality of Service) requirements and path asymmetry in
heterogeneous wireless networks. In this paper, we advance the state of the art
by introducing video distortion into the decision process of multipath data
transfer. The proposed Distortion-Aware Concurrent Multipath Transfer (CMT-DA)
solution includes three phases: 1) per-path status estimation and congestion
control; 2) quality-optimal video flow rate allocation; 3) delay and loss
controlled data retransmission. The term `flow rate allocation' indicates
dynamically picking appropriate access networks and assigning the transmission
rates. We analytically formulate the data distribution over multiple
communication paths to minimize the end-to-end video distortion and derive the
solution based on the utility maximization theory. The performance of the
proposed CMT-DA is evaluated through extensive semi-physical emulations in
Exata involving H.264 video streaming. Experimental results show that CMT-DA
outperforms the reference schemes in terms of video PSNR (Peak Signal-to-Noise
Ratio), goodput, and inter-packet delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7061</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7061</id><created>2014-06-27</created><updated>2015-12-02</updated><authors><author><keyname>Sharma</keyname><forenames>Ashu</forenames></author><author><keyname>Sahay</keyname><forenames>S. K.</forenames></author></authors><title>Evolution and Detection of Polymorphic and Metamorphic Malwares: A
  Survey</title><categories>cs.CR</categories><comments>5 Pages</comments><journal-ref>International Journal of Computer Applications, 2014, Vol. 90, No.
  2, p. 7</journal-ref><doi>10.5120/15544-4098</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Malwares are big threat to digital world and evolving with high complexity.
It can penetrate networks, steal confidential information from computers, bring
down servers and can cripple infrastructures etc. To combat the threat/attacks
from the malwares, anti- malwares have been developed. The existing
anti-malwares are mostly based on the assumption that the malware structure
does not changes appreciably. But the recent advancement in second generation
malwares can create variants and hence posed a challenge to anti-malwares
developers. To combat the threat/attacks from the second generation malwares
with low false alarm we present our survey on malwares and its detection
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7062</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7062</id><created>2014-06-27</created><authors><author><keyname>Liu</keyname><forenames>Ke</forenames></author><author><keyname>Xu</keyname><forenames>Ming</forenames></author><author><keyname>Yu</keyname><forenames>Zeyun</forenames></author></authors><title>Adaptive Mesh Representation and Restoration of Biomedical Images</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The triangulation of images has become an active research area in recent
years for its compressive representation and ease of image processing and
visualization. However, little work has been done on how to faithfully recover
image intensities from a triangulated mesh of an image, a process also known as
image restoration or decoding from meshes. The existing methods such as linear
interpolation, least-square interpolation, or interpolation based on radial
basis functions (RBFs) work to some extent, but often yield blurred features
(edges, corners, etc.). The main reason for this problem is due to the
isotropically-defined Euclidean distance that is taken into consideration in
these methods, without considering the anisotropicity of feature intensities in
an image. Moreover, most existing methods use intensities defined at mesh nodes
whose intensities are often ambiguously defined on or near image edges (or
feature boundaries). In the current paper, a new method of restoring an image
from its triangulation representation is proposed, by utilizing anisotropic
radial basis functions (ARBFs). This method considers not only the geometrical
(Euclidean) distances but also the local feature orientations (anisotropic
intensities). Additionally, this method is based on the intensities of mesh
faces instead of mesh nodes and thus provides a more robust restoration. The
two strategies together guarantee excellent feature-preserving restoration of
an image with arbitrary super-resolutions from its triangulation
representation, as demonstrated by various experiments provided in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7068</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7068</id><created>2014-06-27</created><authors><author><keyname>Razeghi</keyname><forenames>Behrooz</forenames></author><author><keyname>Alizadeh</keyname><forenames>Alireza</forenames></author><author><keyname>Naseri</keyname><forenames>Sima</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author><author><keyname>Seyedin</keyname><forenames>Seyed Alireza</forenames></author></authors><title>Analysis of Coverage Region for MIMO Relay Network with Multiple
  Cooperative DF-Relays</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in International Symposium on Wireless
  Communication Systems (ISWCS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study and analyze coverage region in MIMO communication systems for a
multiple-relay network with decode-and-forward (DF) strategy at the relays.
Assuming that there is a line-of-sight (LOS) propagation environment for
source-relay channels and channel state information is available at receivers
(CSIR), we consider the objective of maximizing coverage region for a given
transmission rate and show numerically the significant effect of propagation
environment on capacity bounds, optimal relay location and coverage region.
Also, we study the situation in which two adjacent relays cooperate in
transmission signals to the destination and show analytically that the coverage
region is extended compared to noncooperative scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7075</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7075</id><created>2014-06-27</created><authors><author><keyname>Ertugrul</keyname><forenames>Omer Faruk</forenames></author></authors><title>Adaptive texture energy measure method</title><categories>cs.CV</categories><journal-ref>International Journal of Intelligent Information Systems. Vol. 3,
  No. 2, 2014, pp. 13-18</journal-ref><doi>10.11648/j.ijiis.20140302.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in image quality, data storage, and computational
capacity have heightened the need for texture analysis in image process. To
date various methods have been developed and introduced for assessing textures
in images. One of the most popular texture analysis methods is the Texture
Energy Measure (TEM) and it has been used for detecting edges, levels, waves,
spots and ripples by employing predefined TEM masks to images. Despite several
success- ful studies, TEM has a number of serious weaknesses in use. The major
drawback is; the masks are predefined therefore they cannot be adapted to
image. A new method, Adaptive Texture Energy Measure Method (aTEM), was offered
to over- come this disadvantage of TEM by using adaptive masks by adjusting the
contrast, sharpening and orientation angle of the mask. To assess the
applicability of aTEM, it is compared with TEM. The accuracy of the
classification of butterfly, flower seed and Brodatz datasets are 0.08, 0.3292
and 0.3343, respectively by TEM and 0.0053, 0.2417 and 0.3153, respectively by
aTEM. The results of this study indicate that aTEM is a successful method for
texture analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7078</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7078</id><created>2014-06-27</created><authors><author><keyname>Fouque</keyname><forenames>Pierre-Alain</forenames></author><author><keyname>Tibouchi</keyname><forenames>Mehdi</forenames></author></authors><title>Close to Uniform Prime Number Generation With Fewer Random Bits</title><categories>cs.CR math.NT</categories><comments>Full version of ICALP 2014 paper. Alternate version of IACR ePrint
  Report 2011/481</comments><doi>10.1007/978-3-662-43948-7_82</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze several variants of a simple method for generating
prime numbers with fewer random bits. To generate a prime $p$ less than $x$,
the basic idea is to fix a constant $q\propto x^{1-\varepsilon}$, pick a
uniformly random $a&lt;q$ coprime to $q$, and choose $p$ of the form $a+t\cdot q$,
where only $t$ is updated if the primality test fails. We prove that variants
of this approach provide prime generation algorithms requiring few random bits
and whose output distribution is close to uniform, under less and less
expensive assumptions: first a relatively strong conjecture by H.L. Montgomery,
made precise by Friedlander and Granville; then the Extended Riemann
Hypothesis; and finally fully unconditionally using the
Barban-Davenport-Halberstam theorem. We argue that this approach has a number
of desirable properties compared to previous algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7091</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7091</id><created>2014-06-27</created><updated>2014-09-10</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Do altmetrics point to the broader impact of research? An overview of
  benefits and disadvantages of altmetrics</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>Accepted for publication in the Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, it is not clear how the impact of research on other areas of society
than science should be measured. While peer review and bibliometrics have
become standard methods for measuring the impact of research in science, there
is not yet an accepted framework within which to measure societal impact.
Alternative metrics (called altmetrics to distinguish them from bibliometrics)
are considered an interesting option for assessing the societal impact of
research, as they offer new ways to measure (public) engagement with research
output. Altmetrics is a term to describe web-based metrics for the impact of
publications and other scholarly material by using data from social media
platforms (e.g. Twitter or Mendeley). This overview of studies explores the
potential of altmetrics for measuring societal impact. It deals with the
definition and classification of altmetrics. Furthermore, their benefits and
disadvantages for measuring impact are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7092</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7092</id><created>2014-06-27</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>On zero-rate error exponents of finite-state channels with
  input-dependent states</title><categories>cs.IT math.IT</categories><comments>22 pages; submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a single-letter formula for the zero-rate reliability (error
exponent) of a finite-state channel whose state variable depends
deterministically (and recursively) on past channel inputs, where the code
complies with a given channel input constraint. Special attention is then
devoted to the important special case of the Gaussian channel with inter-symbol
interference (ISI), where more explicit results are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7093</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7093</id><created>2014-06-27</created><authors><author><keyname>Ma</keyname><forenames>Yinglong</forenames></author><author><keyname>Shi</keyname><forenames>Moyi</forenames></author></authors><title>Using multi-categorization semantic analysis and personalization for
  semantic search</title><categories>cs.IR</categories><comments>15 pages</comments><report-no>SCCE-002</report-no><msc-class>68U35</msc-class><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic search technology has received more attention in the last years.
Compared with the keyword based search, semantic search is used to excavate the
latent semantics information and help users find the information items that
they want indeed. In this paper, we present a novel approach for semantic
search which combines Multi-Categorization Semantic Analysis with
personalization technology. The MCSA approach can classify documents into
multiple categories, which is distinct from the existing approaches of
classifying documents into a single category. Then, the search history and
personal information for users are significantly considered in analysing and
matching the original search result by Term Vector DataBase. A series of
personalization algorithms are proposed to match personal information and
search history. At last, the related experiments are made to validate the
effectiveness and efficiency of our method. The experimental results show that
our method based on MCSA and personalization outperforms some existing methods
with the higher search accuracy and the lower extra time cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7098</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7098</id><created>2014-06-27</created><authors><author><keyname>Qureshi</keyname><forenames>Jalaluddin</forenames></author></authors><title>Caching Piggyback Information for Efficient Index Code Transmission</title><categories>cs.NI cs.IT math.IT</categories><comments>This paper has been accepted for publication in the 39th IEEE
  Conference on Local Computer Networks (LCN) to be held in Edmonton, Canada,
  Sep. 8-11, 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The index coding problem is a fundamental transmission problem arising in
content distribution and wireless networks. Traditional approach to solve this
problem is to find heuristic/ approximation minimum clique partition solution
on an appropriately mapped graph of the index coding problem. In this paper we
study index code for unicast data flow for which we propose updated clique
index coding (UCIC) scheme, UCIC piggybacks additional information in the coded
symbol such that an unsatisfied client can update its cache. We show that UCIC
has higher coding gain than previously proposed index coding schemes, and it is
optimal for those instances where index code of minimum length is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7107</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7107</id><created>2014-06-27</created><authors><author><keyname>van Brink</keyname><forenames>Martijn</forenames></author><author><keyname>van der Zwaan</keyname><forenames>Ruben</forenames></author></authors><title>A branch and price procedure for the container premarshalling problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the loading phase of a vessel, only the containers that are on top of
their stack are directly accessible. If the container that needs to be loaded
next is not the top container, extra moves have to be performed, resulting in
an increased loading time. One way to resolve this issue is via a procedure
called premarshalling. The goal of premarshalling is to reshuffle the
containers into a desired lay-out prior to the arrival of the vessel, in the
minimum number of moves possible. This paper presents an exact algorithm based
on branch and bound, that is evaluated on a large set of instances. The
complexity of the premarshalling problem is also considered, and this paper
shows that the problem at hand is NP-hard, even in the natural case of stacks
with fixed height.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7112</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7112</id><created>2014-06-27</created><authors><author><keyname>Zografos</keyname><forenames>Vasileios</forenames></author></authors><title>3D planar patch extraction from stereo using probabilistic region
  growing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a novel 3D planar patch extraction method using a
probabilistic region growing algorithm. Our method works by simultaneously
initiating multiple planar patches from seed points, the latter determined by
an intensity-based 2D segmentation algorithm in the stereo-pair images. The
patches are grown incrementally and in parallel as 3D scene points are
considered for membership, using a probabilistic distance likelihood measure.
In addition, we have incorporated prior information based on the noise model in
the 2D images and the scene configuration but also include the intensity
information resulting from the initial segmentation. This method works well
across many different data-sets, involving real and synthetic examples of both
regularly and non-regularly sampled data, and is fast enough that may be used
for robot navigation tasks of path detection and obstacle avoidance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7116</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7116</id><created>2014-06-27</created><authors><author><keyname>Qureshi</keyname><forenames>Jalaluddin</forenames></author><author><keyname>Foh</keyname><forenames>Chuan Heng</forenames></author><author><keyname>Cai</keyname><forenames>Jianfei</forenames></author></authors><title>Maximum Multipath Routing Throughput in Multirate Wireless Mesh Networks</title><categories>cs.NI</categories><comments>This paper has been accepted for publication in IEEE 80th Vehicular
  Technology Conference, VTC-Fall 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we consider the problem of finding the maximum routing
throughput between any pair of nodes in an arbitrary multirate wireless mesh
network (WMN) using multiple paths. Multipath routing is an efficient technique
to maximize routing throughput in WMN, however maximizing multipath routing
throughput is a NP-complete problem due to the shared medium for
electromagnetic wave transmission in wireless channel, inducing collision-free
scheduling as part of the optimization problem. In this work, we first provide
problem formulation that incorporates collision-free schedule, and then based
on this formulation we design an algorithm with search pruning that jointly
optimizes paths and transmission schedule. Though suboptimal, compared to the
known optimal single path flow, we demonstrate that an efficient multipath
routing scheme can increase the routing throughput by up to 100% for simple
WMNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7120</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7120</id><created>2014-06-27</created><authors><author><keyname>Acharya</keyname><forenames>Anish</forenames></author></authors><title>Template Matching based Object Detection Using HOG Feature Pyramid</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides a step by step development of designing a Object
Detection scheme using the HOG based Feature Pyramid aligned with the concept
of Template Matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7124</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7124</id><created>2014-06-27</created><authors><author><keyname>Wang</keyname><forenames>Yiyin</forenames></author><author><keyname>Ma</keyname><forenames>Xiaoli</forenames></author><author><keyname>Chen</keyname><forenames>Cailian</forenames></author><author><keyname>Guan</keyname><forenames>Xinping</forenames></author></authors><title>UWB Signal Detection by Cyclic Features</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Ultra-wideband (UWB) impulse radio (IR) systems are well known for low
transmission power, low probability of detection, and overlaying with
narrowband (NB) systems. These merits in fact make UWB signal detection
challenging, since several high-power wireless communication systems coexist
with UWB signals. In the literature, cyclic features are exploited for signal
detection. However, the high computational complexity of conventional cyclic
feature based detectors burdens the receivers. In this paper, we propose
computationally efficient detectors using the specific cyclic features of UWB
signals. The closed-form relationships between the cyclic features and the
system parameters are revealed. Then, some constant false alarm rate detectors
are proposed based on the estimated cyclic autocorrelation functions (CAFs).
The proposed detectors have low complexities compared to the existing ones.
Extensive simulation results indicate that the proposed detectors achieve a
good balance between the detection performance and the computational complexity
in various scenarios, such as multipath environments, colored noise, and NB
interferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7128</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7128</id><created>2014-06-27</created><authors><author><keyname>Galiano</keyname><forenames>Gonzalo</forenames></author><author><keyname>Velasco</keyname><forenames>Juli&#xe1;n</forenames></author></authors><title>On a new formulation of nonlocal image filters involving the relative
  rearrangement</title><categories>cs.CV</categories><msc-class>68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlocal filters are simple and powerful techniques for image denoising. In
this paper we study the reformulation of a broad class of nonlocal filters in
terms of two functional rearrangements: the decreasing and the relative
rearrangements.
  Independently of the dimension of the image, we reformulate these filters as
integral operators defined in a one-dimensional space corresponding to the
level sets measures.
  We prove the equivalency between the original and the rearranged versions of
the filters and propose a discretization in terms of constant-wise
interpolators, which we prove to be convergent to the solution of the
continuous setting.
  For some particular cases, this new formulation allows us to perform a
detailed analysis of the filtering properties. Among others, we prove that the
filtered image is a contrast change of the original image, and that the
filtering procedure behaves asymptotically as a shock filter combined with a
border diffusive term, responsible for the staircaising effect and the loss of
contrast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7132</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7132</id><created>2014-06-27</created><authors><author><keyname>Ebbing</keyname><forenames>Johannes</forenames></author><author><keyname>Hella</keyname><forenames>Lauri</forenames></author><author><keyname>Lohmann</keyname><forenames>Peter</forenames></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames></author></authors><title>Boolean Dependence Logic and Partially-Ordered Connectives</title><categories>math.LO cs.LO</categories><comments>41 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new variant of dependence logic called Boolean dependence
logic. In Boolean dependence logic dependence atoms are of the type
=(x_1,...,x_n,\alpha), where \alpha is a Boolean variable. Intuitively, with
Boolean dependence atoms one can express quantification of relations, while
standard dependence atoms express quantification over functions.
  We compare the expressive power of Boolean dependence logic to dependence
logic and first-order logic enriched by partially-ordered connectives. We show
that the expressive power of Boolean dependence logic and dependence logic
coincide. We define natural syntactic fragments of Boolean dependence logic and
show that they coincide with the corresponding fragments of first-order logic
enriched by partially-ordered connectives with respect to expressive power. We
then show that the fragments form a strict hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7136</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7136</id><created>2014-06-27</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Verifying Component and Connector Models against Crosscutting Structural
  Views</title><categories>cs.SE</categories><comments>11 pages, 8 figures</comments><journal-ref>36th International Conference on Software Engineering (ICSE 2014).
  Pages 95-105. Hyderabad, India, ACM New York, June 2014</journal-ref><doi>10.1145/2568225.2568237</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of component and connector (C&amp;C) models, which are used in many
application domains of software engineering, consists of components at
different containment levels, their typed input and output ports, and the
connectors between them. C&amp;C views, presented in [24], can be used to specify
structural properties of C&amp;C models in an expressive and intuitive way. In this
work we address the verification of a C&amp;C model against a C&amp;C view and present
efficient (polynomial) algorithms to decide satisfaction. A unique feature of
our work, not present in existing approaches to checking structural properties
of C&amp;C models, is the generation of witnesses for satisfaction/non-satisfaction
and of short naturallanguage texts, which serve to explain and formally justify
the verification results and point the engineer to its causes. A prototype tool
and an evaluation over four example systems with multiple views, performance
and scalability experiments, as well as a user study of the usefulness of the
witnesses for engineers, demonstrate the contribution of our work to the
state-of-the-art in component and connector modeling and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7142</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7142</id><created>2014-06-27</created><updated>2014-09-08</updated><authors><author><keyname>Leung</keyname><forenames>Debbie</forenames></author><author><keyname>Matthews</keyname><forenames>William</forenames></author></authors><title>On the power of PPT-preserving and non-signalling codes</title><categories>quant-ph cs.IT math.IT</categories><comments>15 pages, 7 figures</comments><journal-ref>IEEE Transactions of Information Theory, 61(8): 4486-4499, 2015</journal-ref><doi>10.1109/TIT.2015.2439953</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive one-shot upper bounds for quantum noisy channel codes. We do so by
regarding a channel code as a bipartite operation with an encoder belonging to
the sender and a decoder belonging to the receiver, and imposing constraints on
the bipartite operation. We investigate the power of codes whose bipartite
operation is non-signalling from Alice to Bob, positive-partial transpose (PPT)
preserving, or both, and derive a simple semidefinite program for the
achievable entanglement fidelity. Using the semidefinite program, we show that
the non-signalling assisted quantum capacity for memoryless channels is equal
to the entanglement-assisted capacity. We also relate our PPT-preserving codes
and the PPT-preserving entanglement distillation protocols studied by Rains.
Applying these results to a concrete example, the 3-dimensional Werner-Holevo
channel, we find that codes that are non-signalling and PPT-preserving can be
strictly less powerful than codes satisfying either one of the constraints, and
therefore provide a tighter bound for unassisted codes. Furthermore,
PPT-preserving non-signalling codes can send one qubit perfectly over two uses
of the channel, which has no quantum capacity. We discuss whether this can be
interpreted as a form of superactivation of quantum capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7155</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7155</id><created>2014-06-27</created><authors><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Subshifts, MSO Logic, and Collapsing Hierarchies</title><categories>math.DS cs.FL cs.LO math.LO</categories><comments>12 pages, 5 figures. To appear in conference proceedings of TCS 2014,
  published by Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use monadic second-order logic to define two-dimensional subshifts, or
sets of colorings of the infinite plane. We present a natural family of
quantifier alternation hierarchies, and show that they all collapse to the
third level. In particular, this solves an open problem of [Jeandel &amp; Theyssier
2013]. The results are in stark contrast with picture languages, where such
hierarchies are usually infinite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7157</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7157</id><created>2014-06-27</created><updated>2015-06-17</updated><authors><author><keyname>Jain</keyname><forenames>Shweta</forenames></author><author><keyname>Gujar</keyname><forenames>Sujit</forenames></author><author><keyname>Bhat</keyname><forenames>Satyanath</forenames></author><author><keyname>Zoeter</keyname><forenames>Onno</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>An Incentive Compatible Multi-Armed-Bandit Crowdsourcing Mechanism with
  Quality Assurance</title><categories>cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a requester who wishes to crowdsource a series of identical binary
labeling tasks to a pool of workers so as to achieve an assured accuracy for
each task, in a cost optimal way. The workers are heterogeneous with unknown
but fixed qualities and their costs are private. The problem is to select for
each task an optimal subset of workers so that the outcome obtained from the
selected workers guarantees a target accuracy level. The problem is a
challenging one even in a non strategic setting since the accuracy of
aggregated label depends on unknown qualities. We develop a novel multi-armed
bandit (MAB) mechanism for solving this problem. First, we propose a framework,
Assured Accuracy Bandit (AAB), which leads to an MAB algorithm, Constrained
Confidence Bound for a Non Strategic setting (CCB-NS). We derive an upper bound
on the number of time steps the algorithm chooses a sub-optimal set that
depends on the target accuracy level and true qualities. A more challenging
situation arises when the requester not only has to learn the qualities of the
workers but also elicit their true costs. We modify the CCB-NS algorithm to
obtain an adaptive exploration separated algorithm which we call { \em
Constrained Confidence Bound for a Strategic setting (CCB-S)}. CCB-S algorithm
produces an ex-post monotone allocation rule and thus can be transformed into
an ex-post incentive compatible and ex-post individually rational mechanism
that learns the qualities of the workers and guarantees a given target accuracy
level in a cost optimal way. We provide a lower bound on the number of times
any algorithm should select a sub-optimal set and we see that the lower bound
matches our upper bound upto a constant factor. We provide insights on the
practical implementation of this framework through an illustrative example and
we show the efficacy of our algorithms through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7168</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7168</id><created>2014-06-27</created><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>Co-occurrence matrices of time series applied to literary works</title><categories>cs.CY</categories><comments>Literary experiments, Time series, Co-occurrence plots, Harry Potter</comments><journal-ref>ijSciences, 2014, Volume 3, Issue 7, Pages: 12-18</journal-ref><doi>10.18483/ijSci.533</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been proposed to analyse the literary works, plays or
novels, using graphs to display the social network of their interacting
characters. In this approach, the timeline of the literary work is lost,
because the storyline is projected on a planar graph. However, timelines can be
used to build some time series and analyse the work by means of vectors and
matrices. These series can be used to describe the presence and relevance, not
only of words in the text, but also of persons and places portrayed in the
drama or novel. In this framework, we discuss here an approach with
co-occurrence matrices plotted over time, concerning the presence of characters
in the pages of a novel. These matrices are similar to those appearing in
recurrence plots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7179</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7179</id><created>2014-06-27</created><authors><author><keyname>Susemihl</keyname><forenames>Alex</forenames></author><author><keyname>Meir</keyname><forenames>Ron</forenames></author><author><keyname>Opper</keyname><forenames>Manfred</forenames></author></authors><title>Optimal Population Codes for Control and Estimation</title><categories>stat.ML cs.IT math.IT q-bio.NC</categories><comments>9 Pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agents acting in the natural world aim at selecting appropriate actions based
on noisy and partial sensory observations. Many behaviors leading to decision
mak- ing and action selection in a closed loop setting are naturally phrased
within a control theoretic framework. Within the framework of optimal Control
Theory, one is usually given a cost function which is minimized by selecting a
control law based on the observations. While in standard control settings the
sensors are assumed fixed, biological systems often gain from the extra
flexibility of optimiz- ing the sensors themselves. However, this sensory
adaptation is geared towards control rather than perception, as is often
assumed. In this work we show that sen- sory adaptation for control differs
from sensory adaptation for perception, even for simple control setups. This
implies, consistently with recent experimental results, that when studying
sensory adaptation, it is essential to account for the task being performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7196</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7196</id><created>2014-06-27</created><updated>2014-06-30</updated><authors><author><keyname>Lardeux</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Monfroy</keyname><forenames>Eric</forenames></author><author><keyname>Crawford</keyname><forenames>Broderick</forenames></author><author><keyname>Soto</keyname><forenames>Ricardo</forenames></author></authors><title>Set Constraint Model and Automated Encoding into SAT: Application to the
  Social Golfer Problem</title><categories>cs.AI</categories><comments>Submitted to Annals of Operations research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the one hand, Constraint Satisfaction Problems allow one to declaratively
model problems. On the other hand, propositional satisfiability problem (SAT)
solvers can handle huge SAT instances. We thus present a technique to
declaratively model set constraint problems and to encode them automatically
into SAT instances. We apply our technique to the Social Golfer Problem and we
also use it to break symmetries of the problem. Our technique is simpler, more
declarative, and less error-prone than direct and improved hand modeling. The
SAT instances that we automatically generate contain less clauses than improved
hand-written instances such as in [20], and with unit propagation they also
contain less variables. Moreover, they are well-suited for SAT solvers and they
are solved faster as shown when solving difficult instances of the Social
Golfer Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7197</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7197</id><created>2014-06-27</created><authors><author><keyname>Varol</keyname><forenames>Onur</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Ogan</keyname><forenames>Christine L.</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>Evolution of Online User Behavior During a Social Upheaval</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>Best Paper Award at ACM Web Science 2014</comments><journal-ref>Proceedings of the 2014 ACM conference on Web science, Pages 81-90</journal-ref><doi>10.1145/2615569.2615699</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media represent powerful tools of mass communication and information
diffusion. They played a pivotal role during recent social uprisings and
political mobilizations across the world. Here we present a study of the Gezi
Park movement in Turkey through the lens of Twitter. We analyze over 2.3
million tweets produced during the 25 days of protest occurred between May and
June 2013. We first characterize the spatio-temporal nature of the conversation
about the Gezi Park demonstrations, showing that similarity in trends of
discussion mirrors geographic cues. We then describe the characteristics of the
users involved in this conversation and what roles they played. We study how
roles and individual influence evolved during the period of the upheaval. This
analysis reveals that the conversation becomes more democratic as events
unfold, with a redistribution of influence over time in the user population. We
conclude by observing how the online and offline worlds are tightly
intertwined, showing that exogenous events, such as political speeches or
police actions, affect social media conversations and trigger changes in
individual behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7210</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7210</id><created>2014-06-27</created><updated>2014-09-30</updated><authors><author><keyname>Feyzmahdavian</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Charalambous</keyname><forenames>Themistoklis</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Asymptotic Stability and Decay Rates of Homogeneous Positive Systems
  With Bounded and Unbounded Delays</title><categories>cs.SY</categories><comments>SIAM Journal on Control and Optimization</comments><journal-ref>SIAM Journal on Control and Optimization, 52(4), pp. 2623-2650,
  2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There are several results on the stability of nonlinear positive systems in
the presence of time delays. However, most of them assume that the delays are
constant. This paper considers time-varying, possibly unbounded, delays and
establishes asymptotic stability and bounds the decay rate of a significant
class of nonlinear positive systems which includes positive linear systems as a
special case. Specifically, we present a necessary and sufficient condition for
delay-independent stability of continuous-time positive systems whose vector
fields are cooperative and homogeneous. We show that global asymptotic
stability of such systems is independent of the magnitude and variation of the
time delays. For various classes of time delays, we are able to derive explicit
expressions that quantify the decay rates of positive systems. We also provide
the corresponding counterparts for discrete-time positive systems whose vector
fields are non-decreasing and homogeneous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7226</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7226</id><created>2014-06-26</created><authors><author><keyname>Rathi</keyname><forenames>Nilesh</forenames></author><author><keyname>Holi</keyname><forenames>Ganga</forenames></author></authors><title>Securing Medical Images by Watermarking Using DWT DCT and SVD</title><categories>cs.MM</categories><comments>9 pages, 14 figures, 4 tables, Published with International Journal
  of Computer Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V12(2):67-74, June 2014. ISSN:2231-2803. Published by Seventh Sense Research
  Group</journal-ref><doi>10.14445/22312803/IJCTT-V12P113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Telemedicine is well known application where enormous amount of medical data
need to be transferred securely over network and manipulate effectively.
Security of digital data, especially medical images, becomes important for many
reasons such as confidentiality, authentication and integrity. Digital
watermarking has emerged as a advanced technology to enhance the security of
digital images. The insertion of watermark in medical images can authenticate
it and guarantee its integrity. The watermark must be generally hidden does not
affect the quality of the medical image. In this paper, we propose blind
watermarking based on Discrete Wavelet Transform (DWT), Discrete Cosine
Transform (DCT) and Singular Value Decomposition (SVD), we compare the
performance of this technique with watermarking based DWT and SVD. The proposed
method DWT, DCT and SVD comparatively better than DWT and SVD method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7250</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7250</id><created>2014-06-27</created><updated>2015-01-06</updated><authors><author><keyname>Deshwar</keyname><forenames>Amit G.</forenames></author><author><keyname>Vembu</keyname><forenames>Shankar</forenames></author><author><keyname>Yung</keyname><forenames>Christina K.</forenames></author><author><keyname>Jang</keyname><forenames>Gun Ho</forenames></author><author><keyname>Stein</keyname><forenames>Lincoln</forenames></author><author><keyname>Morris</keyname><forenames>Quaid</forenames></author></authors><title>Reconstructing subclonal composition and evolution from whole genome
  sequencing of tumors</title><categories>q-bio.PE cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tumors often contain multiple subpopulations of cancerous cells defined by
distinct somatic mutations. We describe a new method, PhyloWGS, that can be
applied to WGS data from one or more tumor samples to reconstruct complete
genotypes of these subpopulations based on variant allele frequencies (VAFs) of
point mutations and population frequencies of structural variations. We
introduce a principled phylogenic correction for VAFs in loci affected by copy
number alterations and we show that this correction greatly improves subclonal
reconstruction compared to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7259</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7259</id><created>2014-06-27</created><authors><author><keyname>Filho</keyname><forenames>C. I. N. Sampaio</forenames></author><author><keyname>Moreira</keyname><forenames>A. A.</forenames></author><author><keyname>Andrade</keyname><forenames>R. F. S.</forenames></author><author><keyname>Herrmann</keyname><forenames>H. J.</forenames></author><author><keyname>Andrade</keyname><forenames>J. S.</forenames><suffix>Jr</suffix></author></authors><title>Mandala Networks: ultra-robust, ultra-small-world and highly sparse
  graphs</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing demands in security and reliability of infrastructures call
for the optimal design of their embedded complex networks topologies. The
following question then arises: what is the optimal layout to fulfill best all
the demands? Here we present a general solution for this problem with
scale-free networks, like the Internet and airline networks. Precisely, we
disclose a way to systematically construct networks which are 100$\%$ robust
against random failures as well as to malicious attacks. Furthermore, as the
sizes of these networks increase, their shortest paths become asymptotically
invariant and densities of links go to zero, making them ultra-small worlds and
highly sparse, respectively. The first property is ideal for communication and
navigation purposes, while the second is interesting economically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7264</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7264</id><created>2014-06-27</created><authors><author><keyname>Calis</keyname><forenames>Gokhan</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author></authors><title>Repairable Block Failure Resilient Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In large scale distributed storage systems (DSS) deployed in cloud computing,
correlated failures resulting in simultaneous failure (or, unavailability) of
blocks of nodes are common. In such scenarios, the stored data or a content of
a failed node can only be reconstructed from the available live nodes belonging
to available blocks. To analyze the resilience of the system against such block
failures, this work introduces the framework of Block Failure Resilient (BFR)
codes, wherein the data (e.g., file in DSS) can be decoded by reading out from
a same number of codeword symbols (nodes) from each available blocks of the
underlying codeword. Further, repairable BFR codes are introduced, wherein any
codeword symbol in a failed block can be repaired by contacting to remaining
blocks in the system. Motivated from regenerating codes, file size bounds for
repairable BFR codes are derived, trade-off between per node storage and repair
bandwidth is analyzed, and BFR-MSR and BFR-MBR points are derived. Explicit
codes achieving these two operating points for a wide set of parameters are
constructed by utilizing combinatorial designs, wherein the codewords of the
underlying outer codes are distributed to BFR codeword symbols according to
projective planes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7279</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7279</id><created>2014-06-27</created><authors><author><keyname>Deshpande</keyname><forenames>Amit</forenames></author><author><keyname>Venkat</keyname><forenames>Rakesh</forenames></author></authors><title>Guruswami-Sinop Rounding without Higher Level Lasserre</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Guruswami and Sinop give a $O(1/\delta)$ approximation guarantee for the
non-uniform Sparsest Cut problem by solving $O(r)$-level Lasserre semidefinite
constraints, provided that the generalized eigenvalues of the Laplacians of the
cost and demand graphs satisfy a certain spectral condition, namely,
$\lambda_{r+1} \geq \Phi^{*}/(1-\delta)$. Their key idea is a rounding
technique that first maps a vector-valued solution to $[0, 1]$ using
appropriately scaled projections onto Lasserre vectors. In this paper, we show
that similar projections and analysis can be obtained using only $\ell_{2}^{2}$
triangle inequality constraints. This results in a $O(r/\delta^{2})$
approximation guarantee for the non-uniform Sparsest Cut problem by adding only
$\ell_{2}^{2}$ triangle inequality constraints to the usual semidefinite
program, provided that the same spectral condition, $\lambda_{r+1} \geq
\Phi^{*}/(1-\delta)$, holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7282</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7282</id><created>2014-06-27</created><authors><author><keyname>Sun</keyname><forenames>Bo</forenames></author><author><keyname>Leonard</keyname><forenames>Blake</forenames></author><author><keyname>Ronhovde</keyname><forenames>Peter</forenames></author><author><keyname>Nussinov</keyname><forenames>Zohar</forenames></author></authors><title>An interacting replica approach applied to the traveling salesman
  problem</title><categories>cond-mat.stat-mech cs.AI physics.data-an</categories><comments>12 pages,17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a physics inspired heuristic method for solving combinatorial
optimization problems. Our approach is specifically motivated by the desire to
avoid trapping in metastable local minima- a common occurrence in hard problems
with multiple extrema. Our method involves (i) coupling otherwise independent
simulations of a system (&quot;replicas&quot;) via geometrical distances as well as (ii)
probabilistic inference applied to the solutions found by individual replicas.
The {\it ensemble} of replicas evolves as to maximize the inter-replica
correlation while simultaneously minimize the local intra-replica cost function
(e.g., the total path length in the Traveling Salesman Problem within each
replica). We demonstrate how our method improves the performance of rudimentary
local optimization schemes long applied to the NP hard Traveling Salesman
Problem. In particular, we apply our method to the well-known &quot;$k$-opt&quot;
algorithm and examine two particular cases- $k=2$ and $k=3$. With the aid of
geometrical coupling alone, we are able to determine for the optimum tour
length on systems up to $280$ cities (an order of magnitude larger than the
largest systems typically solved by the bare $k=3$ opt). The probabilistic
replica-based inference approach improves $k-opt$ even further and determines
the optimal solution of a problem with $318$ cities and find tours whose total
length is close to that of the optimal solutions for other systems with a
larger number of cities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7285</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7285</id><created>2014-06-27</created><authors><author><keyname>Siahmargooei</keyname><forenames>Yaghoob</forenames></author><author><keyname>Akbari</keyname><forenames>Mohammad Kazem</forenames></author><author><keyname>Golpayegani</keyname><forenames>Seyyed Alireza Hashemi</forenames></author><author><keyname>Sharifian</keyname><forenames>Saeed</forenames></author></authors><title>Near-Optimal Virtual Machine Packing Based on Resource Requirement of
  Service Demands Using Pattern Clustering</title><categories>cs.DC cs.NI cs.PF</categories><journal-ref>IJASCSE journal, Volume 3, Issue 6, JUNE 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Upon the expansion of Cloud Computing and the positive outlook of
organizations with regard to the movements towards using cloud computing and
their expanding utilization of such valuable processing method, as well as the
solutions provided by the cloud infrastructure providers with regard to the
reduction of the costs of processing resources, the problem of organizing
resources in a cloud environment gained a high importance. One of the major
preoccupations of the minds of cloud infrastructure clients is their lack of
knowledge on the quantity of their required processing resources in different
periods of time. The managers and technicians are trying to make the most use
of scalability and the flexibility of the resources in cloud computing. The
main challenge is with calculating the amount of the required processing
resources per moment with regard to the quantity of incoming requests of the
service. Through deduction of the accurate amount of these items, one can have
an accurate estimation of the requests per moment. This paper aims at
introducing a model for automatic scaling of the cloud resources that would
reduce the cost of renting the resources for the clients of cloud
infrastructure. Thus, first we start with a thorough explanation of the
proposal and the major components of the model. Then through calculating the
incomings of the model through clustering and introducing the way that each of
these components work in different phases,...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7288</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7288</id><created>2014-06-27</created><updated>2014-10-16</updated><authors><author><keyname>Gatterbauer</keyname><forenames>Wolfgang</forenames></author><author><keyname>G&#xfc;nnemann</keyname><forenames>Stephan</forenames></author><author><keyname>Koutra</keyname><forenames>Danai</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>Linearized and Single-Pass Belief Propagation</title><categories>cs.DB cs.AI</categories><comments>17 pages, 11 figures, 4 algorithms. Includes following major changes
  since v1: renaming of &quot;turbo BP&quot; to &quot;single-pass BP&quot;, convergence criteria
  now give sufficient *and* necessary conditions, more detailed experiments,
  more detailed comparison with prior BP convergence results, overall improved
  exposition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we tell when accounts are fake or real in a social network? And how
can we tell which accounts belong to liberal, conservative or centrist users?
Often, we can answer such questions and label nodes in a network based on the
labels of their neighbors and appropriate assumptions of homophily (&quot;birds of a
feather flock together&quot;) or heterophily (&quot;opposites attract&quot;). One of the most
widely used methods for this kind of inference is Belief Propagation (BP) which
iteratively propagates the information from a few nodes with explicit labels
throughout a network until convergence. One main problem with BP, however, is
that there are no known exact guarantees of convergence in graphs with loops.
  This paper introduces Linearized Belief Propagation (LinBP), a linearization
of BP that allows a closed-form solution via intuitive matrix equations and,
thus, comes with convergence guarantees. It handles homophily, heterophily, and
more general cases that arise in multi-class settings. Plus, it allows a
compact implementation in SQL. The paper also introduces Single-pass Belief
Propagation (SBP), a &quot;localized&quot; version of LinBP that propagates information
across every edge at most once and for which the final class assignments depend
only on the nearest labeled neighbors. In addition, SBP allows fast incremental
updates in dynamic networks. Our runtime experiments show that LinBP and SBP
are orders of magnitude faster than standard
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7289</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7289</id><created>2014-06-28</created><updated>2014-08-15</updated><authors><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Manasa</keyname><forenames>Lakshmi</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author></authors><title>On The Reachability Problem for Recursive Hybrid Automata with One and
  Two Players</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the success of bounded model checking framework for finite state
machines, Ouaknine and Worrell proposed a time-bounded theory of real-time
verification by claiming that restriction to bounded-time recovers decidability
for several key decision problem related to real-time verification. In support
of this theory, the list of undecidable problems recently shown decidable under
time-bounded restriction is rather impressive: language inclusion for timed
automata, emptiness problem for alternating timed automata, and emptiness
problem for rectangular hybrid automata. The objective of our study was to
recover decidability for general recursive timed automata---and perhaps for
recursive hybrid automata---under time-bounded restriction in order to provide
an appealing verification framework for powerful modeling environments such as
Stateflow/Simulink. Unfortunately, however, we answer this question in negative
by showing that time-bounded reachability problem stays undecidable for
recursive timed automata with five or more clocks. While the bad news continues
even when one considers well-behaved subclasses of recursive hybrid automata,
we recover decidability by considering recursive hybrid automata with bounded
context using a pass-by-reference mechanism, or by restricting the number of
variables to two, with rates in $\{0,1\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7314</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7314</id><created>2014-06-27</created><authors><author><keyname>Trabelsi</keyname><forenames>Imen</forenames></author><author><keyname>Ayed</keyname><forenames>Dorra Ben</forenames></author></authors><title>On the Use of Different Feature Extraction Methods for Linear and Non
  Linear kernels</title><categories>cs.CL cs.LG</categories><comments>8 pages, 3 Figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The speech feature extraction has been a key focus in robust speech
recognition research; it significantly affects the recognition performance. In
this paper, we first study a set of different features extraction methods such
as linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC)
and perceptual linear prediction (PLP) with several features normalization
techniques like rasta filtering and cepstral mean subtraction (CMS). Based on
this, a comparative evaluation of these features is performed on the task of
text independent speaker identification using a combination between gaussian
mixture models (GMM) and linear and non-linear kernels based on support vector
machine (SVM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7315</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7315</id><created>2014-06-27</created><authors><author><keyname>&#xd6;stman</keyname><forenames>Johan</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author></authors><title>Diversity versus Multiplexing at Finite Blocklength</title><categories>cs.IT math.IT</categories><comments>Proc. IEEE Int. Symp. Wirel. Comm. Syst. (ISWCS), Aug. 2014, to
  appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite blocklenth analysis of the diversity-multiplexing tradeoff is
presented, based on nonasymptotic bounds on the maximum channel coding rate of
multiple-antenna block-memoryless Rayleigh-fading channels.The bounds in this
paper allow one to numerically assess for which packet size, number of
antennas, and degree of channel selectivity, diversity-exploiting schemes are
close to optimal, and when instead the available spatial degrees of freedom
should be used to provide spatial multiplexing. This finite blocklength view on
the diversity-multiplexing tradeoff provides insights on the design of
delay-sensitive ultra-reliable communication links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7330</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7330</id><created>2014-06-27</created><authors><author><keyname>Wong</keyname><forenames>Felix Ming Fai</forenames></author><author><keyname>Liu</keyname><forenames>Zhenming</forenames></author><author><keyname>Chiang</keyname><forenames>Mung</forenames></author></authors><title>Stock Market Prediction from WSJ: Text Mining via Sparse Matrix
  Factorization</title><categories>cs.LG q-fin.ST</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of predicting directional movements of stock prices
based on news articles: here our algorithm uses daily articles from The Wall
Street Journal to predict the closing stock prices on the same day. We propose
a unified latent space model to characterize the &quot;co-movements&quot; between stock
prices and news articles. Unlike many existing approaches, our new model is
able to simultaneously leverage the correlations: (a) among stock prices, (b)
among news articles, and (c) between stock prices and news articles. Thus, our
model is able to make daily predictions on more than 500 stocks (most of which
are not even mentioned in any news article) while having low complexity. We
carry out extensive backtesting on trading strategies based on our algorithm.
The result shows that our model has substantially better accuracy rate (55.7%)
compared to many widely used algorithms. The return (56%) and Sharpe ratio due
to a trading strategy based on our model are also much higher than baseline
indices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7338</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7338</id><created>2014-06-27</created><authors><author><keyname>Wei</keyname><forenames>Li-Yi</forenames></author><author><keyname>Levoy</keyname><forenames>Marc</forenames></author></authors><title>Order-Independent Texture Synthesis</title><categories>cs.GR</categories><comments>This is a combination of Stanford Computer Science Department
  Technical Report 2002-01 and a subsequent submission to SIGGRAPH 2003</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search-based texture synthesis algorithms are sensitive to the order in which
texture samples are generated; different synthesis orders yield different
textures. Unfortunately, most polygon rasterizers and ray tracers do not
guarantee the order with which surfaces are sampled. To circumvent this
problem, textures are synthesized beforehand at some maximum resolution and
rendered using texture mapping.
  We describe a search-based texture synthesis algorithm in which samples can
be generated in arbitrary order, yet the resulting texture remains identical.
The key to our algorithm is a pyramidal representation in which each texture
sample depends only on a fixed number of neighboring samples at each level of
the pyramid. The bottom (coarsest) level of the pyramid consists of a noise
image, which is small and predetermined. When a sample is requested by the
renderer, all samples on which it depends are generated at once. Using this
approach, samples can be generated in any order. To make the algorithm
efficient, we propose storing texture samples and their dependents in a
pyramidal cache. Although the first few samples are expensive to generate,
there is substantial reuse, so subsequent samples cost less. Fortunately, most
rendering algorithms exhibit good coherence, so cache reuse is high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7357</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7357</id><created>2014-06-28</created><authors><author><keyname>Salkuyeh</keyname><forenames>Davod Khojasteh</forenames></author></authors><title>On the solution of a class of fuzzy system of linear equations</title><categories>math.NA cs.NA</categories><comments>7 pages</comments><report-no>10.1007</report-no><msc-class>15A06, 90C70</msc-class><doi>10.1007/s12046-014-0313-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the system of linear equations $Ax=b$, where $A\in
\Bbb{R}^{n \times n}$ is a crisp H-matrix and $b$ is a fuzzy $n$-vector. We
then investigate the existence and uniqueness of a fuzzy solution to this
system. The results can also be used for the class of M-matrices and strictly
diagonally dominant matrices. Finally, some numerical examples are given to
illustrate the presented theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7360</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7360</id><created>2014-06-28</created><updated>2014-07-04</updated><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author></authors><title>A framework for improving the performance of verification algorithms
  with a low false positive rate requirement and limited training data</title><categories>cs.CV</categories><comments>IEEE/IAPR International Joint Conference on Biometrics, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of matching patterns in the so-called
verification setting in which a novel, query pattern is verified against a
single training pattern: the decision sought is whether the two match (i.e.
belong to the same class) or not. Unlike previous work which has universally
focused on the development of more discriminative distance functions between
patterns, here we consider the equally important and pervasive task of
selecting a distance threshold which fits a particular operational requirement
- specifically, the target false positive rate (FPR). First, we argue on
theoretical grounds that a data-driven approach is inherently ill-conditioned
when the desired FPR is low, because by the very nature of the challenge only a
small portion of training data affects or is affected by the desired threshold.
This leads us to propose a general, statistical model-based method instead. Our
approach is based on the interpretation of an inter-pattern distance as
implicitly defining a pattern embedding which approximately distributes
patterns according to an isotropic multi-variate normal distribution in some
space. This interpretation is then used to show that the distribution of
training inter-pattern distances is the non-central chi2 distribution,
differently parameterized for each class. Thus, to make the class-specific
threshold choice we propose a novel analysis-by-synthesis iterative algorithm
which estimates the three free parameters of the model (for each class) using
task-specific constraints. The validity of the premises of our work and the
effectiveness of the proposed method are demonstrated by applying the method to
the task of set-based face verification on a large database of pseudo-random
head motion videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7362</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7362</id><created>2014-06-28</created><authors><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Exponentially Increasing the Capacity-to-Computation Ratio for
  Conditional Computation in Deep Learning</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many state-of-the-art results obtained with deep networks are achieved with
the largest models that could be trained, and if more computation power was
available, we might be able to exploit much larger datasets in order to improve
generalization ability. Whereas in learning algorithms such as decision trees
the ratio of capacity (e.g., the number of parameters) to computation is very
favorable (up to exponentially more parameters than computation), the ratio is
essentially 1 for deep neural networks. Conditional computation has been
proposed as a way to increase the capacity of a deep neural network without
increasing the amount of computation required, by activating some parameters
and computation &quot;on-demand&quot;, on a per-example basis. In this note, we propose a
novel parametrization of weight matrices in neural networks which has the
potential to increase up to exponentially the ratio of the number of parameters
to computation. The proposed approach is based on turning on some parameters
(weight matrices) when specific bit patterns of hidden unit activations are
obtained. In order to better control for the overfitting that might result, we
propose a parametrization that is tree-structured, where each node of the tree
corresponds to a prefix of a sequence of sign bits, or gating units, associated
with hidden units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7363</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7363</id><created>2014-06-28</created><updated>2014-09-23</updated><authors><author><keyname>Berlinkov</keyname><forenames>Mikhail V.</forenames></author></authors><title>On the Synchronization Rate for e-machines</title><categories>cs.IT cs.DS cs.FL math.IT</categories><comments>A result about computing prediction rate constant has been added</comments><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known, that an $\epsilon$-machine is either exactly or asymptotically
synchronizing. In the exact case, the observer can infer the current machine
state after observing $L$ generated symbols with probability $1-a^L$ where $0
\leq a&lt;1$ is a so-called synchronization rate constant. In the asymptotic case,
the probability of the correct prediction the current machine state after
observing $L$ generated symbols tends to $1$ exponentially fast as $1-b^L$ for
$0&lt;b&lt;1$ and the infimum of such $b$ is a so-called prediction rate constant.
  Hence the synchronization and prediction rate constants serve as natural
measures of synchronization for $\epsilon$-machines. In the present work we
show how to approximate these constants in polynomial time in terms of the
number of machine states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7367</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7367</id><created>2014-06-28</created><authors><author><keyname>Zhu</keyname><forenames>Qijun</forenames></author><author><keyname>Hu</keyname><forenames>Haibo</forenames></author><author><keyname>Xu</keyname><forenames>Jianliang</forenames></author><author><keyname>Lee</keyname><forenames>Wang-Chien</forenames></author></authors><title>Geo-Social Group Queries with Minimum Acquaintance Constraint</title><categories>cs.DB cs.SI</categories><comments>13 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prosperity of location-based social networking services enables
geo-social group queries for group-based activity planning and marketing. This
paper proposes a new family of geo-social group queries with minimum
acquaintance constraint (GSGQs), which are more appealing than existing
geo-social group queries in terms of producing a cohesive group that guarantees
the worst-case acquaintance level. GSGQs, also specified with various spatial
constraints, are more complex than conventional spatial queries; particularly,
those with a strict $k$NN spatial constraint are proved to be NP-hard. For
efficient processing of general GSGQ queries on large location-based social
networks, we devise two social-aware index structures, namely SaR-tree and
SaR*-tree. The latter features a novel clustering technique that considers both
spatial and social factors. Based on SaR-tree and SaR*-tree, efficient
algorithms are developed to process various GSGQs. Extensive experiments on
real-world Gowalla and Dianping datasets show that our proposed methods
substantially outperform the baseline algorithms based on R-tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7371</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7371</id><created>2014-06-28</created><authors><author><keyname>Tanna</keyname><forenames>Paresh</forenames></author><author><keyname>Ghodasara</keyname><forenames>Yogesh</forenames></author></authors><title>Using Apriori with WEKA for Frequent Pattern Mining</title><categories>cs.DB</categories><comments>5 Pages, 4 Figures, &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V12(3), 127-131, June 2014</journal-ref><doi>10.14445/22315381/IJETT-V12P223</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge exploration from the large set of data,generated as a result of the
various data processing activities due to data mining only. Frequent Pattern
Mining is a very important undertaking in data mining. Apriori approach applied
to generate frequent item set generally espouse candidate generation and
pruning techniques for the satisfaction of the desired objective. This paper
shows how the different approaches achieve the objective of frequent mining
along with the complexities required to perform the job. This paper
demonstrates the use of WEKA tool for association rule mining using Apriori
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7373</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7373</id><created>2014-06-28</created><updated>2014-09-20</updated><authors><author><keyname>Mondelli</keyname><forenames>Marco</forenames></author><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>How to Achieve the Capacity of Asymmetric Channels</title><categories>cs.IT math.IT</categories><comments>20 pages, 3 figures, submitted to IEEE Trans. Inform. Theory and in
  part to Allerton Conference'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss coding techniques that allow reliable transmission up to the
capacity of a discrete memoryless {\em a}symmetric channel. To do so, we take
the point of view of modern coding theory and we discuss how recent advances in
coding for symmetric channels help in providing more efficient solutions also
for the asymmetric case. In more detail, we consider three basic approaches.
  The first one is Gallager's scheme that consists of concatenating a linear
code with a non-linear mapper so that the input distribution can be
appropriately shaped. We explicitly show that both polar codes and spatially
coupled codes can be employed in this scenario. Furthermore, we derive a
scaling law between the gap to capacity, the cardinality of the input and
output alphabets of the channel, and the required size of the mapper.
  The second one is an integrated approach in which the coding scheme is used
both for source coding, in order to create codewords distributed according to
the capacity-achieving input distribution, {\em and} for channel coding, in
order to provide error protection. Such a technique has been recently
introduced by Honda and Yamamoto in the context of polar codes, and we show how
to apply it also to the design of sparse graph codes.
  The third approach is based on an idea due to B\&quot;ocherer and Mathar and
separates completely the two tasks of source coding and channel coding by
&quot;chaining&quot; together several codewords. We prove that we can combine {\em any}
suitable source code with {\em any} suitable channel code in order to provide
optimal schemes for asymmetric channels. In particular, we show that again both
polar codes and spatially coupled codes fulfill the required conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7377</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7377</id><created>2014-06-28</created><authors><author><keyname>Kharaji</keyname><forenames>Morteza Yousefi</forenames></author><author><keyname>Rizi</keyname><forenames>Fatemeh Salehi</forenames></author><author><keyname>Khayyambashi</keyname><forenames>Mohammad Reza</forenames></author></authors><title>A New Approach for Finding Cloned Profiles in Online Social Networks</title><categories>cs.CR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, Online Social Networks such as Facebook, LinkedIn and Twitter are the
most popular platforms on the Internet, on which millions of users register to
share personal information with their friends. A large amount of data, social
links and statistics about users are collected by Online Social Networks
services and they create big digital mines of various statistical data. Leakage
of personal information is a significant concern for social network users.
Besides information propagation, some new attacks on Online Social Networks
such as Identity Clone attack (ICA) have been identified. ICA attempts to
create a fake online identity of a victim to fool their friends into believing
the authenticity of the fake identity to establish social links in order to
reap the private information of the victims friends which is not shared in
their public profiles. There are some identity validation services that perform
users identity validation, but they are passive services and they only protect
users who are informed on privacy concerns and online identity issues. This
paper starts with an explanation of two types of profile cloning attacks are
explained and a new approach for detecting clone identities is proposed by
defining profile similarity and strength of relationship measures. According to
similar attributes and strength of relationship among users which are computed
in detection steps, it will be decided which profile is clone and which one is
genuine by a predetermined threshold. Finally, the experimental results are
presented to demonstrate the effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7386</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7386</id><created>2014-06-28</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Contextual Semantics: From Quantum Mechanics to Logic, Databases,
  Constraints, and Complexity</title><categories>quant-ph cs.LO</categories><comments>26 pages, 7 figures</comments><journal-ref>Bulletin of the European Association for Theoretical Computer
  Science, Number 113, June 2014, pages 137--163</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss quantum non-locality and contextuality, emphasising logical and
structural aspects. We also show how the same mathematical structures arise in
various areas of classical computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7395</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7395</id><created>2014-06-28</created><updated>2014-08-20</updated><authors><author><keyname>Ye</keyname><forenames>Junhong</forenames></author><author><keyname>Zhang</keyname><forenames>Ying Jun</forenames></author></authors><title>A Guard Zone Based Scalable Mode Selection Scheme in D2D Underlaid
  Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to the error in the
  proposition proof and simulation setting. The proof of the proposition has
  some flaws currently and needs to be refined. The simulation also needs to be
  re-run due to the error in parameter setting. For the same reason, the
  numerical results also need to be re-configured and re-calculated</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Underlaying the cellular networks, Device to Device (D2D) communication
brings the possibility to significantly improve the spectral efficiency in
cellular networks and offload the traffic relayed by the base station. However,
it creates new challenge for interference management as well. In this paper, we
consider the multi-cell D2D underlaid cellular network in which many D2D links
reuse the uplink resource simultaneously. The infeasibility of interference
cancellation/alignment motivates us to force the D2D users near the base
stations to work in cellular mode. Based on that, we present a distributed and
scalable mode selection scheme based on guard zone to make a good tradeoff
between decreasing interference penalty to base stations and improving spectrum
utilization. With the help of stochastic geometry, we develop an analytically
tractable framework to analyze the interference and then throughput for the two
kinds of users in the considered scenario. The results obtained by the
framework reveal that enabling D2D communication does improve the per user
throughput and that a proper guard zone can further significantly increase the
average throughput of both kinds of users. Thanks to the tractability of our
model, the optimal guard zone, which is the key parameter in our scheme, can be
efficiently obtained by finding the root of the first order derivative of the
throughput expression. Through extensive numerical analysis, we show insights
of the system and give some guidelines in system design aspect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7398</identifier>
 <datestamp>2014-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7398</id><created>2014-06-28</created><updated>2014-08-05</updated><authors><author><keyname>Gwynne</keyname><forenames>Matthew</forenames></author><author><keyname>Kullmann</keyname><forenames>Oliver</forenames></author></authors><title>A framework for good SAT translations, with applications to CNF
  representations of XOR constraints</title><categories>cs.CC</categories><comments>67 pages; second version with extended discussion of literature.
  Continues arXiv:1309.3060</comments><acm-class>F.4.1; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general framework for good CNF-representations of boolean
constraints, to be used for translating decision problems into SAT problems
(i.e., deciding satisfiability for conjunctive normal forms). We apply it to
the representation of systems of XOR-constraints, also known as systems of
linear equations over the two-element field, or systems of parity constraints.
  The general framework defines the notion of &quot;representation&quot;, and provides
several methods to measure the quality of the representation by the complexity
(&quot;hardness&quot;) needed for making implicit &quot;knowledge&quot; of the representation
explicit (to a SAT-solving mechanism). We obtain general upper and lower
bounds.
  Applied to systems of XOR-constraints, we show a super-polynomial lower bound
on &quot;good&quot; representations under very general circumstances. A corresponding
upper bound shows fixed-parameter tractability in the number of constraints.
  The measurement underlying this upper bound ignores the auxiliary variables
needed for shorter representations of XOR-constraints. Improved upper bounds
(for special cases) take them into account, and a rich picture begins to
emerge, under the various hardness measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7399</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7399</id><created>2014-06-28</created><authors><author><keyname>Samara</keyname><forenames>Ghassan</forenames></author><author><keyname>Alhmiedat</keyname><forenames>Tareq</forenames></author></authors><title>Intelligent Emergency Message Broadcasting in VANET Using PSO</title><categories>cs.NI cs.NE</categories><comments>11 pages</comments><journal-ref>World of Computer Science and Information Technology Journal
  (WCSIT) ISSN: 2221-0741 Vol. 4, No. 7, 90-100, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new type of Mobile Ad hoc Network which is called Vehicular Ad hoc
Networks (VANET) created a fertile environment for research. In this research,
a protocol Particle Swarm Optimization Contention Based Broadcast (PCBB) is
proposed, for fast andeffective dissemination of emergency messages within a
geographical area to distribute the emergency message and achieve the safety
system, this research will help the VANET system to achieve its safety goals in
intelligent and efficient way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7405</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7405</id><created>2014-06-28</created><authors><author><keyname>Ghosh</keyname><forenames>Sutanu</forenames></author></authors><title>Performance evaluation on the basis of Bit error rate for different
  order of Modulation and different length of Subchannels in ofdm system</title><categories>cs.IT math.IT</categories><comments>Journal Paper With 9 Pages (Number of Figures - 06), International
  Journal of Mobile Network Communications &amp; Telematics (IJMNCT), June 2014</comments><doi>10.5121/ijmnct.2014.4304</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Today, we have required to accommodate a large number of users under a single
base station. This can be possible only if we have some flexibility over the
spectrum. Previously we have lots of multiplexing methods to accommodate large
number of signals in time and frequency domain. But now we have required to
accommodate a large number of users in the same bandwidth, without any fading
over the received signal. So, orthogonality can be maintained over the
frequency response. This technology is now more popular in the mobile
communication domain, called Orthogonal Frequency Division Multiplexing (OFDM).
Actually user data can be converted into the parallel form and then they are
modulated using digital modulation techniques. Finally, they have followed by
OFDM Modulator and cyclic prefix can be inserted into the OFDM symbols. Here, I
have worked on the measurement of Bit error rate for different modulation
techniques in OFDM technology. It has been considered that subchannel size is
not constant. According to that I have concluded the overall idea regarding the
performance under OFDM technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7423</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7423</id><created>2014-06-28</created><authors><author><keyname>Kumar</keyname><forenames>Vinit</forenames></author><author><keyname>Agarwal</keyname><forenames>Ajay</forenames></author></authors><title>An Efficient Read Dominant Data Replication Protocol under Serial
  Isolation using Quorum Consensus Approach</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed systems, data replication provides better availability, higher
read capacity, improved access efficiency and lower bandwidth requirements in
the system. In this paper, we propose a significantly efficient approach of the
data replication for serial isolation by using newly proposed Circular quorum
systems. This paper has three major contributions. First, we have proposed the
Circular quorum systems that generalize the various existing quorum systems,
such as Read-one-write-all (ROWA) quorum systems, Majority quorum systems, Grid
quorum systems, Diamond quorum systems, D-Space quorum systems,
Multi-dimensional-grid quorum systems and Generalized-grid quorum systems.
Second, Circular quorum systems not only generalizes but also improves the
performance over existing quorum systems of their category. Third, we proposed
a highly available Circular quorum consensus protocol for data replication
under serial isolation level that uses a suitable Circular quorum system for
read dominant scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7424</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7424</id><created>2014-06-28</created><updated>2015-01-23</updated><authors><author><keyname>Pape</keyname><forenames>Andreas D.</forenames></author><author><keyname>Kurtz</keyname><forenames>Kenneth J.</forenames></author><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author></authors><title>Complexity Measures and Concept Learning</title><categories>cs.IT cs.LG math.IT</categories><comments>27 pages, 7 tables, 1 figure. Accepted for publication in Journal of
  Mathematical Psychology, in press</comments><journal-ref>Journal of Mathematical Psychology, vol. 64-65, pp. 66-75, 2015</journal-ref><doi>10.1016/j.jmp.2015.01.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nature of concept learning is a core question in cognitive science.
Theories must account for the relative difficulty of acquiring different
concepts by supervised learners. For a canonical set of six category types, two
distinct orderings of classification difficulty have been found. One ordering,
which we call paradigm-specific, occurs when adult human learners classify
objects with easily distinguishable characteristics such as size, shape, and
shading. The general order occurs in all other known cases: when adult humans
classify objects with characteristics that are not readily distinguished (e.g.,
brightness, saturation, hue); for children and monkeys; and when categorization
difficulty is extrapolated from errors in identification learning. The
paradigm-specific order was found to be predictable mathematically by measuring
the logical complexity of tasks, i.e., how concisely the solution can be
represented by logical rules.
  However, logical complexity explains only the paradigm-specific order but not
the general order. Here we propose a new difficulty measurement, information
complexity, that calculates the amount of uncertainty remaining when a subset
of the dimensions are specified. This measurement is based on Shannon entropy.
We show that, when the metric extracts minimal uncertainties, this new
measurement predicts the paradigm-specific order for the canonical six category
types, and when the metric extracts average uncertainties, this new measurement
predicts the general order. Moreover, for learning category types beyond the
canonical six, we find that the minimal-uncertainty formulation correctly
predicts the paradigm-specific order as well or better than existing metrics
(Boolean complexity and GIST) in most cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7426</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7426</id><created>2014-06-28</created><authors><author><keyname>Ohlberger</keyname><forenames>Mario</forenames></author><author><keyname>Smetana</keyname><forenames>Kathrin</forenames></author></authors><title>Approximation of skewed interfaces with tensor-based model reduction
  procedures: application to the reduced basis hierarchical model reduction
  approach</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we introduce a procedure which allows to recover the
potentially very good approximation properties of tensor-based model reduction
procedures for the solution of partial differential equations in the presence
of interfaces or strong gradients in the solution which are skewed with respect
to the coordinate axes. The two key ideas are the location of the interface by
solving a lower-dimensional partial differential equation and the subsequent
removal of the interface of the solution by choosing the determined interface
as the lifting function of the Dirichlet boundary conditions. For prescribed
interfaces we demonstrate in numerical experiments for linear elliptic
equations and the reduced basis-hierarchical model reduction approach that the
proposed procedure yields a significantly improved convergence behavior even in
the case when we only consider an approximation of the interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7429</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7429</id><created>2014-06-28</created><authors><author><keyname>Katzman</keyname><forenames>Jonathan</forenames></author><author><keyname>Duros</keyname><forenames>Diane</forenames></author></authors><title>Comparison of SVM Optimization Techniques in the Primal</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the efficacy of different optimization techniques in a
primal formulation of a support vector machine (SVM). Three main techniques are
compared. The dataset used to compare all three techniques was the Sentiment
Analysis on Movie Reviews dataset, from kaggle.com.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7435</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7435</id><created>2014-06-28</created><updated>2015-10-17</updated><authors><author><keyname>Wang</keyname><forenames>Da</forenames></author><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory</forenames></author></authors><title>Compression in the Space of Permutations</title><categories>cs.IT math.IT</categories><comments>accepted to IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate lossy compression (source coding) of data in the form of
permutations. This problem has direct applications in the storage of ordinal
data or rankings, and in the analysis of sorting algorithms. We analyze the
rate-distortion characteristic for the permutation space under the uniform
distribution, and the minimum achievable rate of compression that allows a
bounded distortion after recovery. Our analysis is with respect to different
practical and useful distortion measures, including Kendall-tau distance,
Spearman's footrule, Chebyshev distance and inversion-$\ell_1$ distance. We
establish equivalence of source code designs under certain distortions and show
simple explicit code designs that incur low encoding/decoding complexities and
are asymptotically optimal. Finally, we show that for the Mallows model, a
popular nonuniform ranking model on the permutation space, both the entropy and
the maximum distortion at zero rate are much lower than the uniform
counterparts, which motivates the future design of efficient compression
schemes for this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7437</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7437</id><created>2014-06-28</created><authors><author><keyname>Usman</keyname><forenames>Shuaibu Hassan</forenames><affiliation>Abubakar Tafawa Balewa University</affiliation></author><author><keyname>Oyefolahan</keyname><forenames>Ishaq Oyebisi</forenames><affiliation>International Islamic University Malaysia</affiliation></author></authors><title>Encouraging Knowledge Sharing Using Web 2.0 Technologies In Higher
  Education: A Survey</title><categories>cs.CY</categories><comments>The article has 10 pages and 2 tables, International Journal of
  Managing Information Technology (IJMIT), Vol.6, No.2, May 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the technology continuous to advance new technologies have emerged with
the capability to revolutionize knowledge sharing practices. Web 2.0
exemplifies such new technologies which provides dynamic way of interactions of
people and businesses. In learning environment Web 2.0 technologies support and
enhance teaching and learning of students. Therefore the main aim of this study
focuses on the determining the ways to encourage knowledge sharing through web
2.0 technologies from students' point of views. A total of 287 students
responded to the online questionnaire in International Islamic University
Malaysia. Descriptive statistics was used in data analysis. The results show
that students used web 2.0 technologies in learning and sharing knowledge among
them. In addition the study found eight items on ways to encourage and enhance
knowledge sharing among students in the University. These items include Create
Awareness Provide facilities Internet Accessibility Ease of use Encourage
Teamwork Materials Availability Improved Responses and Motivation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7438</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7438</id><created>2014-06-28</created><authors><author><keyname>Bozdag</keyname><forenames>Engin</forenames></author><author><keyname>Gao</keyname><forenames>Qi</forenames></author><author><keyname>Houben</keyname><forenames>Geert-Jan</forenames></author><author><keyname>Warnier</keyname><forenames>Martijn</forenames></author></authors><title>Does Offline Political Segregation Affect the Filter Bubble? An
  Empirical Analysis of Information Diversity for Dutch and Turkish Twitter
  Users</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Computers in Human Behavior, 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  From a liberal perspective, pluralism and viewpoint diversity are seen as a
necessary condition for a well-functioning democracy. Recently, there have been
claims that viewpoint diversity is diminishing in online social networks,
putting users in a &quot;bubble&quot;, where they receive political information which
they agree with. The contributions from our investigations are fivefold: (1) we
introduce different dimensions of the highly complex value viewpoint diversity
using political theory; (2) we provide an overview of the metrics used in the
literature of viewpoint diversity analysis; (3) we operationalize new metrics
using the theory and provide a framework to analyze viewpoint diversity in
Twitter for different political cultures; (4) we share our results for a case
study on minorities we performed for Turkish and Dutch Twitter users; (5) we
show that minority users cannot reach a large percentage of Turkish Twitter
users. With the last of these contributions, using theory from communication
scholars and philosophers, we show how minority access is missing from the
typical dimensions of viewpoint diversity studied by computer scientists and
the impact it has on viewpoint diversity analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7443</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7443</id><created>2014-06-28</created><updated>2015-06-08</updated><authors><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Ashkan</keyname><forenames>Azin</forenames></author></authors><title>Efficient Learning in Large-Scale Combinatorial Semi-Bandits</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stochastic combinatorial semi-bandit is an online learning problem where at
each step a learning agent chooses a subset of ground items subject to
combinatorial constraints, and then observes stochastic weights of these items
and receives their sum as a payoff. In this paper, we consider efficient
learning in large-scale combinatorial semi-bandits with linear generalization,
and as a solution, propose two learning algorithms called Combinatorial Linear
Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both
algorithms are computationally efficient as long as the offline version of the
combinatorial problem can be solved efficiently. We establish that CombLinTS
and CombLinUCB are also provably statistically efficient under reasonable
assumptions, by developing regret bounds that are independent of the problem
scale (number of items) and sublinear in time. We also evaluate CombLinTS on a
variety of problems with thousands of items. Our experiment results demonstrate
that CombLinTS is scalable, robust to the choice of algorithm parameters, and
significantly outperforms the best of our baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7444</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7444</id><created>2014-06-28</created><authors><author><keyname>Schuler</keyname><forenames>Christian J.</forenames></author><author><keyname>Hirsch</keyname><forenames>Michael</forenames></author><author><keyname>Harmeling</keyname><forenames>Stefan</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Learning to Deblur</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a learning-based approach to blind image deconvolution. It uses a
deep layered architecture, parts of which are borrowed from recent work on
neural network learning, and parts of which incorporate computations that are
specific to image deconvolution. The system is trained end-to-end on a set of
artificially generated training examples, enabling competitive performance in
blind deconvolution, both with respect to quality and runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7445</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7445</id><created>2014-06-28</created><authors><author><keyname>Lao</keyname><forenames>Ni</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author></authors><title>Contrastive Feature Induction for Efficient Structure Learning of
  Conditional Random Fields</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structure learning of Conditional Random Fields (CRFs) can be cast into an
L1-regularized optimization problem. To avoid optimizing over a fully linked
model, gain-based or gradient-based feature selection methods start from an
empty model and incrementally add top ranked features to it. However, for
high-dimensional problems like statistical relational learning, training time
of these incremental methods can be dominated by the cost of evaluating the
gain or gradient of a large collection of candidate features. In this study we
propose a fast feature evaluation algorithm called Contrastive Feature
Induction (CFI), which only evaluates a subset of features that involve both
variables with high signals (deviation from mean) and variables with high
errors (residue). We prove that the gradient of candidate features can be
represented solely as a function of signals and errors, and that CFI is an
efficient approximation of gradient-based evaluation methods. Experiments on
synthetic and real data sets show competitive learning speed and accuracy of
CFI on pairwise CRFs, compared to state-of-the-art structure learning methods
such as full optimization over all features, and Grafting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7447</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7447</id><created>2014-06-28</created><updated>2015-03-06</updated><authors><author><keyname>Combes</keyname><forenames>Richard</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Unimodal Bandits without Smoothness</title><categories>cs.LG</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stochastic bandit problems with a continuous set of arms and
where the expected reward is a continuous and unimodal function of the arm. No
further assumption is made regarding the smoothness and the structure of the
expected reward function. For these problems, we propose the Stochastic
Pentachotomy (SP) algorithm, and derive finite-time upper bounds on its regret
and optimization error. In particular, we show that, for any expected reward
function $\mu$ that behaves as $\mu(x)=\mu(x^\star)-C|x-x^\star|^\xi$ locally
around its maximizer $x^\star$ for some $\xi, C&gt;0$, the SP algorithm is
order-optimal. Namely its regret and optimization error scale as
$O(\sqrt{T\log(T)})$ and $O(\sqrt{\log(T)/T})$, respectively, when the time
horizon $T$ grows large. These scalings are achieved without the knowledge of
$\xi$ and $C$. Our algorithm is based on asymptotically optimal sequential
statistical tests used to successively trim an interval that contains the best
arm with high probability. To our knowledge, the SP algorithm constitutes the
first sequential arm selection rule that achieves a regret and optimization
error scaling as $O(\sqrt{T})$ and $O(1/\sqrt{T})$, respectively, up to a
logarithmic factor for non-smooth expected reward functions, as well as for
smooth functions with unknown smoothness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7459</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7459</id><created>2014-06-29</created><authors><author><keyname>Zhu</keyname><forenames>Ru</forenames></author></authors><title>Speedup of Micromagnetic Simulations with C++ AMP On Graphics Processing
  Units</title><categories>cs.CE cond-mat.mtrl-sci</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite-difference Micromagnetic solver is presented utilizing the C++
Accelerated Massive Parallelism (C++ AMP). The high speed performance of a
single Graphics Processing Unit (GPU) is demonstrated compared to a typical
CPU-based solver. The speed-up of GPU to CPU is shown to be greater than 100
for problems with larger sizes. This solver is based on C++ AMP and can run on
GPUs from various hardware vendors, such as NVIDIA, AMD and Intel, regardless
of whether it is dedicated or integrated graphics processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7473</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7473</id><created>2014-06-29</created><authors><author><keyname>Mazinan</keyname><forenames>Hassan Gholami</forenames></author><author><keyname>Ahmadi</keyname><forenames>Gholam Reza</forenames></author><author><keyname>Khaji</keyname><forenames>Erfan</forenames></author></authors><title>An Efficient Hybrid CS and K-Means Algorithm for the Capacitated PMedian
  Problem</title><categories>math.OC cs.AI</categories><comments>2 Figures, 4 Tables, 17 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capacitated p-median problem (CPMP) is an important variation of facility
location problem in which p capacitated medians are economically selected to
serve a set of demand vertices so that the total assigned demand to each of the
candidate medians must not exceed its capacity. This paper surveys and analyses
the combination of Cuckoo Search and K-Means algorithms to solve the CPMP. In
order to check for quality and validity of the suggestive method, we compared
the final solution produced over the two test problems of Osman and
Christofides, each of which including 10 sample tests. According to the
results, the suggested meta-heuristic algorithm shows superiority over the rest
known algorithms in this field as all the best known solutions in the first
problem set, and several sample sets in the second problem set have been
improved within reasonable periods of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7483</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7483</id><created>2014-06-29</created><authors><author><keyname>Martinez</keyname><forenames>Alicia Gonzalez</forenames></author><author><keyname>Hervas</keyname><forenames>Susana Lopez</forenames></author><author><keyname>Samy</keyname><forenames>Doaa</forenames></author><author><keyname>Arques</keyname><forenames>Carlos G.</forenames></author><author><keyname>Sandoval</keyname><forenames>Antonio Moreno</forenames></author></authors><title>Jabalin: a Comprehensive Computational Model of Modern Standard Arabic
  Verbal Morphology Based on Traditional Arabic Prosody</title><categories>cs.CL</categories><comments>Jabalin implementation is available at
  http://sourceforge.net/projects/jabalin/</comments><journal-ref>Systems and Frameworks for Computational Morphology, Springer
  Berlin Heidelberg, (2013) pp. 35-52</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The computational handling of Modern Standard Arabic is a challenge in the
field of natural language processing due to its highly rich morphology.
However, several authors have pointed out that the Arabic morphological system
is in fact extremely regular. The existing Arabic morphological analyzers have
exploited this regularity to variable extent, yet we believe there is still
some scope for improvement. Taking inspiration in traditional Arabic prosody,
we have designed and implemented a compact and simple morphological system
which in our opinion takes further advantage of the regularities encountered in
the Arabic morphological system. The output of the system is a large-scale
lexicon of inflected forms that has subsequently been used to create an Online
Interface for a morphological analyzer of Arabic verbs. The Jabalin Online
Interface is available at http://elvira.lllf.uam.es/jabalin/, hosted at the
LLI-UAM lab. The generation system is also available under a GNU GPL 3 license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7486</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7486</id><created>2014-06-29</created><authors><author><keyname>Jiang</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Achievable Rates of FDD Massive MIMO Systems with Spatial Channel
  Correlation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that the performance of frequency-division-duplex (FDD)
massive MIMO systems with i.i.d. channels is disappointing compared with that
of time-division-duplex (TDD) systems, due to the prohibitively large overhead
for acquiring channel state information at the transmitter (CSIT). In this
paper, we investigate the achievable rates of FDD massive MIMO systems with
spatially correlated channels, considering the CSIT acquisition dimensionality
loss, the imperfection of CSIT and the regularized-zero-forcing linear
precoder. The achievable rates are optimized by judiciously designing the
downlink channel training sequences and user CSIT feedback codebooks,
exploiting the multiuser spatial channel correlation. We compare our achievable
rates with TDD massive MIMO systems, i.i.d. FDD systems, and the joint spatial
division and multiplexing (JSDM) scheme, by deriving the deterministic
equivalents of the achievable rates, based on popular channel models. It is
shown that, based on the proposed eigenspace channel estimation schemes, the
rate-gap between FDD systems and TDD systems is significantly narrowed, even
approached under moderate number of base station antennas. Compared to the JSDM
scheme, our proposal achieves dimensionality-reduction channel estimation
without channel pre-projection, and higher throughput for moderate number of
antennas and moderate to large channel coherence time, though at higher
computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7487</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7487</id><created>2014-06-29</created><updated>2015-03-22</updated><authors><author><keyname>Marinescu</keyname><forenames>Dan C.</forenames></author><author><keyname>Paya</keyname><forenames>Ashkan</forenames></author><author><keyname>Morrison</keyname><forenames>John P.</forenames></author></authors><title>Coalition Formation and Combinatorial Auctions; Applications to
  Self-organization and Self-management in Utility Computing</title><categories>cs.MA cs.GT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a two-stage protocol for resource management in a
hierarchically organized cloud. The first stage exploits spatial locality for
the formation of coalitions of supply agents; the second stage, a combinatorial
auction, is based on a modified proxy-based clock algorithm and has two phases,
a clock phase and a proxy phase. The clock phase supports price discovery; in
the second phase a proxy conducts multiple rounds of a combinatorial auction
for the package of services requested by each client. The protocol strikes a
balance between low-cost services for cloud clients and a decent profit for the
service providers. We also report the results of an empirical investigation of
the combinatorial auction stage of the protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7492</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7492</id><created>2014-06-29</created><authors><author><keyname>Farmer</keyname><forenames>William M.</forenames></author></authors><title>Andrews' Type Theory with Undefinedness</title><categories>math.LO cs.LO</categories><comments>This research was supported by NSERC. arXiv admin note: text overlap
  with arXiv:1406.6706</comments><msc-class>03B15 (Primary), 03B35 (Secondary)</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ${\cal Q}_0$ is an elegant version of Church's type theory formulated and
extensively studied by Peter B. Andrews. Like other traditional logics, ${\cal
Q}_0$ does not admit undefined terms. The &quot;traditional approach to
undefinedness&quot; in mathematical practice is to treat undefined terms as
legitimate, nondenoting terms that can be components of meaningful statements.
${\cal Q}^{\rm u}_{0}$ is a modification of Andrews' type theory ${\cal Q}_0$
that directly formalizes the traditional approach to undefinedness. This paper
presents ${\cal Q}^{\rm u}_{0}$ and proves that the proof system of ${\cal
Q}^{\rm u}_{0}$ is sound and complete with respect to its semantics which is
based on Henkin-style general models. The paper's development of ${\cal Q}^{\rm
u}_{0}$ closely follows Andrews' development of ${\cal Q}_0$ to clearly
delineate the differences between the two systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7496</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7496</id><created>2014-06-29</created><authors><author><keyname>Yetis</keyname><forenames>Cenk M.</forenames></author><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Anand</keyname><forenames>Kushal</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Gunawan</keyname><forenames>Erry</forenames></author></authors><title>Balancing Weighted Substreams in MIMO Interference Channels</title><categories>cs.IT math.IT</categories><comments>To be published at IEEE Wireless Commun. Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Substreams refer to the streams of each user in a system. Substream
weighting, where the weights determine the prioritization order, can be
important in multiple-input multiple-output interference channels. In this
letter, a distributed algorithm is proposed for the problem of power
minimization subject to weighted SINR constraint. The algorithm is based on two
basic features, the well known distributed power control algorithm by Yates in
1995 and a simple linear search to find feasible SINR targets. The power
control law used in the proposed algorithm is proven to linearly converge to a
unique fixed-point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7497</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7497</id><created>2014-06-29</created><authors><author><keyname>AbdelGawad</keyname><forenames>Moez A.</forenames></author></authors><title>Domain Theory for Modeling OOP: A Summary</title><categories>cs.PL</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain theory is `a mathematical theory that serves as a foundation for the
semantics of programming languages'. Domains form the basis of a theory of
partial information, which extends the familiar notion of partial function to
encompass a whole spectrum of &quot;degrees of definedness&quot;, so as to model
incremental higher-order computation (i.e., computing with infinite data
values, such as functions defined over an infinite domain like the domain of
integers, infinite trees, and such as objects of object-oriented programming).
General considerations from recursion theory dictate that partial functions are
unavoidable in any discussion of computability. Domain theory provides an
appropriately abstract setting in which the notion of a partial function can be
lifted and used to give meaning to higher types, recursive types, etc. NOOP is
a domain-theoretic model of nominally-typed OOP. NOOP was used to prove the
identification of inheritance and subtyping in mainstream nominally-typed OO
programming languages and the validity of this identification. In this report
we first present the definitions of basic domain theoretic notions and domain
constructors used in the construction of NOOP, then we present the construction
of a simple structural model of OOP called COOP as a step towards the
construction of NOOP. Like the construction of NOOP, the construction of COOP
uses earlier presented domain constructors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7498</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7498</id><created>2014-06-29</created><updated>2015-03-31</updated><authors><author><keyname>Gopalan</keyname><forenames>Aditya</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Thompson Sampling for Learning Parameterized Markov Decision Processes</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider reinforcement learning in parameterized Markov Decision Processes
(MDPs), where the parameterization may induce correlation across transition
probabilities or rewards. Consequently, observing a particular state transition
might yield useful information about other, unobserved, parts of the MDP. We
present a version of Thompson sampling for parameterized reinforcement learning
problems, and derive a frequentist regret bound for priors over general
parameter spaces. The result shows that the number of instants where suboptimal
actions are chosen scales logarithmically with time, with high probability. It
holds for prior distributions that put significant probability near the true
model, without any additional, specific closed-form structure such as conjugate
or product-form priors. The constant factor in the logarithmic scaling encodes
the information complexity of learning the MDP in terms of the Kullback-Leibler
geometry of the parameter space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7515</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7515</id><created>2014-06-29</created><authors><author><keyname>Kerimbayev</keyname><forenames>Nurassyl</forenames></author><author><keyname>Akramova</keyname><forenames>Aliya</forenames></author><author><keyname>Azieva</keyname><forenames>Nurgul</forenames></author></authors><title>Evaluation of educational activities ungraded school teachers:
  development resources to improve the quality of teaching in a rural school</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this article are issues related to the assessment of the
teacher, working in small schools. The author analyzes the educational
activities ungraded school teacher, as one of the factors increasing the
quality of teaching in a rural school. We have tried to consider other
resources to successfully solve the problem of increasing the quality of
education in rural schools. Teacher in a rural school is a key figure in the
training, development and education of children. From his profession,
qualification, ability to work with a small but different age student
population is entirely dependent and learning outcomes of students. Therefore,
the educational activities of teachers of small schools should be analyzed and
monitored not only for assessing his work, but with the aim to provide all
possible assistance in the specific conditions of ungraded schools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7524</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7524</id><created>2014-06-29</created><authors><author><keyname>Jungum</keyname><forenames>Nevin Vunka</forenames></author><author><keyname>Mohamudally</keyname><forenames>Nawaz</forenames></author><author><keyname>Nissanke</keyname><forenames>Nimal</forenames></author></authors><title>Towards a Generic Application Partitioning and Retraction Framework for
  Pervasive Environments</title><categories>cs.DC</categories><comments>3rd International Conference on Future Computer and Communication,
  ICFCC 2011</comments><doi>10.1115/1.859711</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current mobile context-aware applications for pervasive environments have
been designed to consume information from computational nodes or devices in
their surroundings or environments. As the hardware industry continues making
much smaller, compact and cheap hardware, the vision of having plenty of very
small powerful digital networking nodes in, for e.g., the living room or
bedroom, is not so far. Designing software that can make optimal use of all
these computational nodes when needed is still challenging; since software will
not only consume information from these nodes but parts of the software can be
hosted on these different nodes. In this paper we propose the BubbleCodes
Framework which is a generic application partitioning and retraction framework
for next generation context-aware applications that will have the capabilities
to partition and retract themselves on multiple computational nodes in a
pervasive environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7525</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7525</id><created>2014-06-29</created><authors><author><keyname>Huang</keyname><forenames>Wenqi</forenames></author><author><keyname>Gong</keyname><forenames>Xiaojin</forenames></author></authors><title>Fusion Based Holistic Road Scene Understanding</title><categories>cs.CV</categories><comments>14 pages,11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of holistic road scene understanding based
on the integration of visual and range data. To achieve the grand goal, we
propose an approach that jointly tackles object-level image segmentation and
semantic region labeling within a conditional random field (CRF) framework.
Specifically, we first generate semantic object hypotheses by clustering 3D
points, learning their prior appearance models, and using a deep learning
method for reasoning their semantic categories. The learned priors, together
with spatial and geometric contexts, are incorporated in CRF. With this
formulation, visual and range data are fused thoroughly, and moreover, the
coupled segmentation and semantic labeling problem can be inferred via Graph
Cuts. Our approach is validated on the challenging KITTI dataset that contains
diverse complicated road scenarios. Both quantitative and qualitative
evaluations demonstrate its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7527</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7527</id><created>2014-06-29</created><authors><author><keyname>Koren&#x10d;iak</keyname><forenames>&#x13d;ubo&#x161;</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Jan</forenames></author><author><keyname>&#x158;eh&#xe1;k</keyname><forenames>Vojt&#x11b;ch</forenames></author></authors><title>Dealing with Zero Density Using Piecewise Phase-type Approximation</title><categories>cs.PF</categories><comments>extended version of paper with same name accepted to 11th European
  Workshop on Performance Engineering (EPEW 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every probability distribution can be approximated up to a given precision by
a phase-type distribution, i.e. a distribution encoded by a continuous time
Markov chain (CTMC). However, an excessive number of states in the
corresponding CTMC is needed for some standard distributions, in particular
most distributions with regions of zero density such as uniform or shifted
distributions. Addressing this class of distributions, we suggest an
alternative representation by CTMC extended with discrete-time transitions.
Using discrete-time transitions we split the density function into multiple
intervals. Within each interval, we then approximate the density with standard
phase-type fitting. We provide an experimental evidence that our method
requires only a moderate number of states to approximate such distributions
with regions of zero density. Furthermore, the usage of CTMC with discrete-time
transitions is supported by a number of techniques for their analysis. Thus,
our results promise an efficient approach to the transient analysis of a class
of non-Markovian models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7535</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7535</id><created>2014-06-29</created><authors><author><keyname>Agrawal</keyname><forenames>Manindra</forenames></author><author><keyname>Gurjar</keyname><forenames>Rohit</forenames></author><author><keyname>Korwar</keyname><forenames>Arpita</forenames></author><author><keyname>Saxena</keyname><forenames>Nitin</forenames></author></authors><title>Hitting-sets for ROABP and Sum of Set-Multilinear circuits</title><categories>cs.CC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.1826</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a $n^{O(\log n)}$-time ($n$ is the input size) blackbox polynomial
identity testing algorithm for unknown-order read-once oblivious algebraic
branching programs (ROABP). The best result known for this class was
$n^{O(\log^2 n)}$ due to Forbes-Saptharishi-Shpilka (STOC 2014), and that too
only for multilinear ROABP. We get rid of their exponential dependence on the
individual degree. With this, we match the time-complexity for the unknown
order ROABP with the known order ROABP (due to Forbes-Shpilka (FOCS 2013)) and
also with the depth-$3$ set-multilinear circuits (due to Agrawal-Saha-Saxena
(STOC 2013)). Our proof is simpler and involves a new technique called basis
isolation.
  The depth-$3$ model has recently gained much importance, as it has become a
stepping-stone to understanding general arithmetic circuits. Its restriction to
multilinearity has known exponential lower bounds but no nontrivial blackbox
identity tests. In this paper, we take a step towards designing such
hitting-sets. We give the first subexponential whitebox PIT for the sum of
constantly many set-multilinear depth-$3$ circuits. To achieve this, we define
notions of distance and base sets. Distance, for a multilinear depth-$3$
circuit, measures how far are the partitions from a mere refinement. We design
a hitting-set in time $n^{O(d \log n)}$ for $d$-distance. Further, we give an
extension of our result to models where the distance is large but it is small
when restricted to certain base sets (of variables).
  We also explore a new model of ROABP where the factor-matrices are invertible
(called invertible-factor ROABP). We design a hitting-set in time
poly($n^{w^2}$) for width-$w$ invertible-factor ROABP. Further, we could do
without the invertibility restriction when $w=2$. Previously, the best result
for width-$2$ ROABP was quasi-polynomial time (Forbes-Saptharishi-Shpilka, STOC
2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7537</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7537</id><created>2014-06-29</created><authors><author><keyname>Arima</keyname><forenames>Yoshiko</forenames></author></authors><title>Effect of Group Means on the Probability of Consensus</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/6</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, groups who could not reach a consensus were investigated using
the group polarization paradigm. The purpose was to explore the conditions
leading to intragroup disagreement and attitude change following disagreement
among 269 participants. Analysis indicated that the probability of consensus
was low when the group means differed from the grand mean of the entire sample.
When small differences among group members were found, depolarization (reverse
direction of polarization) followed disagreement. These results suggested the
groups which deviated most from the population tendency were the most likely to
cause within-group disagreement, while within-group variances determined the
direction of attitude change following disagreement within the group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7538</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7538</id><created>2014-06-29</created><authors><author><keyname>Sela</keyname><forenames>Alon</forenames></author><author><keyname>Oved</keyname><forenames>Hila</forenames></author><author><keyname>Ben-Gal</keyname><forenames>Irad</forenames></author></authors><title>Information Spread in a Connected World</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/7</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the following work, we compare the spread of information by word-of-mouth
(WOM) to the spread of information through search engines. We assume that the
initial acknowledgement of new information derives from social interactions but
that solid opinions are only formed after further evaluation through search
engines. Search engines can be viewed as central hubs that connect information
presented in relevant websites to searchers. Since they construct new
connections between searchers and information in every query performed, the
network structure is less relevant. Although models of viral spread of ideas
have been inspected in many previous works [1], [2], [3], [4], [5], [6], [7],
[8], [9], [10], [11], [12], [13], only few assume the acceptance of a novel
concept to be solely based on the evaluation of the opinions of others [8],
[5]. Following this approach, combined with that of models of information
spread with threshold [1] that claim the propagation in a network to occur only
if a threshold of neighbors hold an opinion, the proposed work adds a new
theoretical perspective that is relevant to the daily use of search engines as
a major information search tool. We continue by presenting some justifications
based on experimentations. Last we discuss possible outcomes of over use of
search engines vs. WOM, and suggest an hypothesis that such overuse might
actually narrow the collective information set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7539</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7539</id><created>2014-06-29</created><authors><author><keyname>Quan</keyname><forenames>Wei</forenames></author><author><keyname>Pimentel</keyname><forenames>Andy D.</forenames></author></authors><title>Exploring Task Mappings on Heterogeneous MPSoCs using a Bias-Elitist
  Genetic Algorithm</title><categories>cs.PF cs.NE</categories><comments>9 pages, 11 figures, uses algorithm2e.sty</comments><acm-class>C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploration of task mappings plays a crucial role in achieving high
performance in heterogeneous multi-processor system-on-chip (MPSoC) platforms.
The problem of optimally mapping a set of tasks onto a set of given
heterogeneous processors for maximal throughput has been known, in general, to
be NP-complete. The problem is further exacerbated when multiple applications
(i.e., bigger task sets) and the communication between tasks are also
considered. Previous research has shown that Genetic Algorithms (GA) typically
are a good choice to solve this problem when the solution space is relatively
small. However, when the size of the problem space increases, classic genetic
algorithms still suffer from the problem of long evolution times. To address
this problem, this paper proposes a novel bias-elitist genetic algorithm that
is guided by domain-specific heuristics to speed up the evolution process.
Experimental results reveal that our proposed algorithm is able to handle large
scale task mapping problems and produces high-quality mapping solutions in only
a short time period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7540</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7540</id><created>2014-06-29</created><authors><author><keyname>Benz</keyname><forenames>Samuel</forenames></author><author><keyname>Marandi</keyname><forenames>Parisa Jalili</forenames></author><author><keyname>Pedone</keyname><forenames>Fernando</forenames></author><author><keyname>Garbinato</keyname><forenames>Beno&#xee;t</forenames></author></authors><title>Building global and scalable systems with Atomic Multicast</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of worldwide Internet-scale services demands large distributed
systems. Indeed, when handling several millions of users, it is common to
operate thousands of servers spread across the globe. Here, replication plays a
central role, as it contributes to improve the user experience by hiding
failures and by providing acceptable latency. In this paper, we claim that
atomic multicast, with strong and well-defined properties, is the appropriate
abstraction to efficiently design and implement globally scalable distributed
systems. We substantiate our claim with the design of two modern online
services atop atomic multicast, a strongly consistent key-value store and a
distributed log. In addition to presenting the design of these services, we
experimentally assess their performance in a geographically distributed
deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7541</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7541</id><created>2014-06-29</created><authors><author><keyname>Levine</keyname><forenames>Sheen</forenames></author><author><keyname>Prietula</keyname><forenames>Michael</forenames></author></authors><title>Open Collaboration for Innovation: Principles and Performance</title><categories>cs.CY</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principles of open collaboration for innovation (and production), once
distinctive to open source software, are now found in many other ventures. Some
of these ventures are internet-based: Wikipedia, online forums and communities.
Others are off-line: in medicine, science, and everyday life. Such ventures
have been affecting traditional firms, and may represent a new organizational
form. Despite the impact of such ventures, questions remain about their
operating principles and performance. Here we define open collaboration (OC),
the underlying set of principles, and propose that it is a robust engine for
innovation and production. First, we review multiple OC ventures and identify
four defining principles. In all instances, participants create goods and
services of economic value, they exchange and reuse each other's work, they
labor purposefully with just loose coordination, and they permit anyone to
contribute and consume. These principles distinguish OC from other
organizational forms, such as firms or cooperatives. Next, we turn to
performance. To understand the performance of OC, we develop a computational
model, combining innovation theory with recent evidence on human cooperation.
We identify and investigate three elements that affect performance: the
cooperativeness of participants, the diversity of their needs, and the degree
to which the goods are rival (subtractable). Through computational experiments,
we find that OC performs well even in seemingly harsh environments: when
cooperators are a minority, free riders are present, diversity is lacking, or
goods are rival. We conclude that OC is viable and likely to expand into new
domains. The findings also inform the discussion on new organizational forms,
collaborative and communal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7542</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7542</id><created>2014-06-29</created><updated>2014-07-16</updated><authors><author><keyname>Lee</keyname><forenames>David</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Aitamurto</keyname><forenames>Tanja</forenames></author><author><keyname>Landemore</keyname><forenames>Helene</forenames></author></authors><title>Crowdsourcing for Participatory Democracies: Efficient Elicitation of
  Social Choice Functions</title><categories>cs.MA cs.CY cs.SI</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/14</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present theoretical and empirical results demonstrating the usefulness of
voting rules for participatory democracies. We first give algorithms which
efficiently elicit \epsilon-approximations to two prominent voting rules: the
Borda rule and the Condorcet winner. This result circumvents previous
prohibitive lower bounds and is surprisingly strong: even if the number of
ideas is as large as the number of participants, each participant will only
have to make a logarithmic number of comparisons, an exponential improvement
over the linear number of comparisons previously needed. We demonstrate the
approach in an experiment in Finland's recent off-road traffic law reform,
observing that the total number of comparisons needed to achieve a fixed
\epsilon approximation is linear in the number of ideas and that the constant
is not large.
  Finally, we note a few other experimental observations which support the use
of voting rules for aggregation. First, we observe that rating, one of the
common alternatives to ranking, manifested effects of bias in our data. Second,
we show that very few of the topics lacked a Condorcet winner, one of the
prominent negative results in voting. Finally, we show data hinting at a
potential future direction: the use of partial rankings as opposed to pairwise
comparisons to further decrease the elicitation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7547</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7547</id><created>2014-06-29</created><authors><author><keyname>Hazy</keyname><forenames>James</forenames></author><author><keyname>Curuklu</keyname><forenames>Baran</forenames></author></authors><title>Influence Process Structural Learning and the Emergence of Collective
  Intelligence</title><categories>cs.SI</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/22</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work [Hazy 2012] has demonstrated computationally that collectives
that are organized into networks which govern the flow of resources can learn
to recognize newly emerging opportunities distributed in the environment. This
paper argues that the system does this through a process analogous to neural
network learning with relative status playing the role of synaptic weights.
Hazy showed computationally that learning of this type can occur even when
resource allocation decision makers have no direct visibility into the
environment, have no direct understanding of the opportunity, and are not
involved in their exploitation except to the extent that they evaluate the
success or failure of funded projects. Effectively, the system of interactions
learns which individuals have the best access to information and other
resources within the ecosystem. Hazy [2012] calls this previously unidentified
emergence phenomenon: Influence Process Structural Learning (IPSL). In the
prior model of IPSL, a three-tiered organizational structure was predetermined
in the model design [Hazy 2012]. These initial conditions delimit the extent to
which the emergence of collective intelligence can be posited because the model
itself assumes a defined structure. This work contributes to the field by
extending the IPSL argument for collective intelligence to a holistic emergence
argument. It begins by briefly reviewing previously published work. It
continues the conversation by adding two additional steps: Firstly, it shows
how a three-tier organizing structure might emerge through known complexity
mechanisms. In this case the mechanism identified is preferential attachment
[Barabasi 2002]. Secondly, the paper shows how collective intelligence can
emerge within a system of agents when the influence structure among these
agents is treated as a the genetic algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7549</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7549</id><created>2014-06-29</created><authors><author><keyname>Skarzauskiene</keyname><forenames>Aelita</forenames></author><author><keyname>Pitrenaite-Zileniene</keyname><forenames>Birute</forenames></author><author><keyname>Leichteris</keyname><forenames>Edgaras</forenames></author><author><keyname>Paunksniene</keyname><forenames>Zaneta</forenames></author><author><keyname>Maciuliene</keyname><forenames>Monika</forenames></author></authors><title>Social Technologies for Developing Collective Intelligence in Networked
  Society</title><categories>cs.SI cs.CY</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/25</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scientific problem in our project is defined as a question: how social
technologies could contribute to the development of smart and inclusive
society? The subject of our research are networked projects (virtual CI
systems) which include collective decision making tools and innovation
mechanisms allowing and encouraging individual and team creativity,
entrepreneurship, online collaboration, new forms of self-regulation and
self-governance, self-configuration of communities by considering these
projects as being catalyst for emergence of CI. The answers to these
theoretical questions could have huge practical implications by influencing
more reasonable and sophisticated application of social technologies in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7550</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7550</id><created>2014-06-29</created><authors><author><keyname>Radford</keyname><forenames>Jason</forenames></author></authors><title>Architectures of Virtual Decision-Making: The Emergence of Gender
  Discrimination on a Crowdfunding Website</title><categories>cs.SI cs.CY physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/26</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing relevance of Internet-based markets requires a sustained
investigation into the relationship between design and user behavior. This
research begins within the sociology of quantification and markets to
investigate the impacts of basic design decisions on user behavior and
individual success on a widely used crowdfunding website. This study looks at
one common design feature, publishing recipients' sex, on the probability of
receiving funding. Following research in the sociology of gender, these effects
are defined along individual, behavioral, and structural dimensions. The
results reveal that before teachers' sex was published, gender discrimination
was weak and inconsistent. However, afterward gender discrimination increases
by an order of magnitude and becomes systematized. Contrary to expectation,
donors did not discriminate by sex category, but by teachers' structural
position and the kinds of language they used. Implications for research on
gender discrimination, priming, and online behavior are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7551</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7551</id><created>2014-06-29</created><authors><author><keyname>Tinati</keyname><forenames>Ramine</forenames></author><author><keyname>Simperl</keyname><forenames>Elena</forenames></author><author><keyname>Luczak-Roesch</keyname><forenames>Markus</forenames></author><author><keyname>Van Kleek</keyname><forenames>Max</forenames></author><author><keyname>Shadbolt</keyname><forenames>Nigel</forenames></author></authors><title>Collective Intelligence in Citizen Science -- A Study of Performers and
  Talkers</title><categories>cs.SI cs.CY physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/28</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent emergence of online citizen science is illustrative of an
efficient and effective means to harness the crowd in order to achieve a range
of scientific discoveries. Fundamentally, citizen science projects draw upon
crowds of non-expert volunteers to complete short Tasks, which can vary in
domain and complexity. However, unlike most human-computational systems,
participants in these systems, the `citizen scientists' are volunteers, whereby
no incentives, financial or otherwise, are offered. Furthermore, encouraged by
citizen science platforms such as Zooniverse, online communities have emerged,
providing them with an environment to discuss, share ideas, and solve problems.
In fact, it is the result of these forums that has enabled a number of
scientific discoveries to be made. In this paper we explore the phenomenon of
collective intelligence via the relationship between the activities of online
citizen science communities and the discovery of scientific knowledge. We
perform a cross-project analysis of ten Zooniverse citizen science projects and
analyse the behaviour of users with regards to their Task completion activity
and participation in discussion and discover collective behaviour amongst
highly active users. Whilst our findings have implications for future citizen
science design, we also consider the wider implications for understanding
collective intelligence research in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7557</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7557</id><created>2014-06-29</created><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Weighted Fair Multicast Multigroup Beamforming under Per-antenna Power
  Constraints</title><categories>cs.IT math.IT</categories><comments>Under review in IEEE Transactions in Signal Processing</comments><doi>10.1109/TSP.2014.2345340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multi-antenna transmitter that conveys independent sets of common data to
distinct groups of users is considered. This model is known as physical layer
multicasting to multiple co-channel groups. In this context, the practical
constraint of a maximum permitted power level radiated by each antenna is
addressed. The per-antenna power constrained system is optimized in a maximum
fairness sense with respect to predetermined quality of service weights. In
other words, the worst scaled user is boosted by maximizing its weighted
signal-to-interference plus noise ratio. A detailed solution to tackle the
weighted max-min fair multigroup multicast problem under per-antenna power
constraints is therefore derived. The implications of the novel constraints are
investigated via prominent applications and paradigms. What is more, robust
per-antenna constrained multigroup multicast beamforming solutions are
proposed. Finally, an extensive performance evaluation quantifies the gains of
the proposed algorithm over existing solutions and exhibits its accuracy over
per-antenna power constrained systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7558</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7558</id><created>2014-06-29</created><authors><author><keyname>Fay</keyname><forenames>Nicolas</forenames></author><author><keyname>Tamariz</keyname><forenames>Monica</forenames></author><author><keyname>Ellison</keyname><forenames>T Mark</forenames></author><author><keyname>Barr</keyname><forenames>Dale</forenames></author></authors><title>Human Communication Systems Evolve by Cultural Selection</title><categories>cs.SI cs.CL physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/29</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human communication systems, such as language, evolve culturally; their
components undergo reproduction and variation. However, a role for selection in
cultural evolutionary dynamics is less clear. Often neutral evolution (also
known as 'drift') models, are used to explain the evolution of human
communication systems, and cultural evolution more generally. Under this
account, cultural change is unbiased: for instance, vocabulary, baby names and
pottery designs have been found to spread through random copying.
  While drift is the null hypothesis for models of cultural evolution it does
not always adequately explain empirical results. Alternative models include
cultural selection, which assumes variant adoption is biased. Theoretical
models of human communication argue that during conversation interlocutors are
biased to adopt the same labels and other aspects of linguistic representation
(including prosody and syntax). This basic alignment mechanism has been
extended by computer simulation to account for the emergence of linguistic
conventions. When agents are biased to match the linguistic behavior of their
interlocutor, a single variant can propagate across an entire population of
interacting computer agents. This behavior-matching account operates at the
level of the individual. We call it the Conformity-biased model. Under a
different selection account, called content-biased selection, functional
selection or replicator selection, variant adoption depends upon the intrinsic
value of the particular variant (e.g., ease of learning or use). This second
alternative account operates at the level of the cultural variant. Following
Boyd and Richerson we call it the Content-biased model. The present paper tests
the drift model and the two biased selection models' ability to explain the
spread of communicative signal variants in an experimental micro-society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7560</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7560</id><created>2014-06-29</created><authors><author><keyname>Toyokawa</keyname><forenames>Wataru</forenames></author><author><keyname>Kim</keyname><forenames>Hye-rin</forenames></author><author><keyname>Kameda</keyname><forenames>Tatsuya</forenames></author></authors><title>Less-is-more in a 5-star rating system: an experimental study of human
  combined decisions in a multi-armed bandit problem</title><categories>cs.SI</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/30</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the rapid proliferation of advanced information technologies, including
the Internet, modern humans can easily access vast amount of socially
transmitted information. Intuitively, this situation is isomorphic to some
eusocial insects that are known to solve the exploration-exploitation dilemma
collectively through information transfer (e.g., honeybees [Seeley et al.,
1991]; and ants [Shaffer, Sasaki &amp; Pratt, 2013]). Yet, in contrast from the
eusocial insects, whose colonies are composed of kin, human collective
performance may be affected by an inherent free-rider problem [Bolton &amp; Harris,
1999; Kameda, Tsukasaki, Hastie &amp; Berg, 2011]. Specifically, in groups
involving non-kin members, it is expected that free-riders, who allow others to
search for better alternatives and then exploit their findings through social
learning (&quot;information scroungers&quot;), will frequently appear, and consequently
undermine the advantage of collective intelligence [Rogers, 1998; Kameda &amp;
Nakanishi, 2003].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7561</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7561</id><created>2014-06-29</created><authors><author><keyname>Khazaei</keyname><forenames>Taraneh</forenames></author><author><keyname>Xiao</keyname><forenames>Lu</forenames></author></authors><title>Collective intelligence in Massive Online Dialogues</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/34</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence and ongoing development of Web 2.0 technologies have enabled
new and advanced forms of collective intelligence at unprecedented scales,
allowing large numbers of individuals to act collectively and create high
quality intellectual artifacts. However, little is known about how and when
they indeed promote collective intelligence. In this manuscript, we provide a
survey of the automated tools developed to analyze discourse-centric collective
intelligence. By conducting a thematic analysis of the current research
direction, a set of gaps and limitations are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7562</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7562</id><created>2014-06-29</created><authors><author><keyname>Meslec</keyname><forenames>Nicoleta</forenames></author><author><keyname>Curseu</keyname><forenames>Petru</forenames></author><author><keyname>Meeus</keyname><forenames>Marius</forenames></author><author><keyname>Fodor</keyname><forenames>Oana</forenames></author></authors><title>When none of us perform better than all of us together: the role of
  analogical decision rules in groups</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/35</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During social interactions, groups develop collective competencies that
(ideally) should assist groups to outperform average standalone individual
members (weak cognitive synergy) or the best performing member in the group
(strong cognitive synergy). In two experimental studies we manipulate the type
of decision rule used in group decision-making (identify the best vs.
collaborative), and the way in which the decision rules are induced (direct vs.
analogical) and we test the effect of these two manipulations on the emergence
of strong and weak cognitive synergy. Our most important results indicate that
an analogically induced decision rule (imitate-the-successful heuristic) in
which groups have to identify the best member and build on his/her performance
(take-the-best heuristic) is the most conducive for strong cognitive synergy.
Our studies bring evidence for the role of analogy-making in groups as well as
the role of fast-and-frugal heuristics for group decision-making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7563</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7563</id><created>2014-06-29</created><authors><author><keyname>Davis-Stober</keyname><forenames>Clintin</forenames></author><author><keyname>Budescu</keyname><forenames>David</forenames></author><author><keyname>Dana</keyname><forenames>Jason</forenames></author><author><keyname>Broomell</keyname><forenames>Stephen</forenames></author></authors><title>When is a crowd wise?</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/37</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous studies and anecdotes demonstrate the &quot;wisdom of the crowd,&quot; the
surprising accuracy of a group's aggregated judgments. Less is known, however,
about the generality of crowd wisdom. For example, are crowds wise even if
their members have systematic judgmental biases, or can influence each other
before members render their judgments? If so, are there situations in which we
can expect a crowd to be less accurate than skilled individuals? We provide a
precise but general definition of crowd wisdom: A crowd is wise if a linear
aggregate, for example a mean, of its members' judgments is closer to the
target value than a randomly, but not necessarily uniformly, sampled member of
the crowd. Building on this definition, we develop a theoretical framework for
examining, a priori, when and to what degree a crowd will be wise. We
systematically investigate the boundary conditions for crowd wisdom within this
framework and determine conditions under which the accuracy advantage for
crowds is maximized. Our results demonstrate that crowd wisdom is highly
robust: Even if judgments are biased and correlated, one would need to nearly
deterministically select only a highly skilled judge before an individual's
judgment could be expected to be more accurate than a simple averaging of the
crowd. Our results also provide an accuracy rationale behind the need for
diversity of judgments among group members. Contrary to folk explanations of
crowd wisdom which hold that judgments should ideally be independent so that
errors cancel out, we find that crowd wisdom is maximized when judgments
systematically differ as much as possible. We re-analyze data from two
published studies that confirm our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7564</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7564</id><created>2014-06-29</created><authors><author><keyname>Rahwan</keyname><forenames>Iyad</forenames></author><author><keyname>Krasnoshtan</keyname><forenames>Dmytro</forenames></author><author><keyname>Shariff</keyname><forenames>Azim</forenames></author><author><keyname>Bonnefon</keyname><forenames>Jean-Francois</forenames></author></authors><title>Analytical reasoning task reveals limits of social learning in networks</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/39</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social learning -by observing and copying others- is a highly successful
cultural mechanism for adaptation, outperforming individual information
acquisition and experience. Here, we investigate social learning in the context
of the uniquely human capacity for reflective, analytical reasoning. A hallmark
of the human mind is our ability to engage analytical reasoning, and suppress
false associative intuitions. Through a set of lab-based network experiments,
we find that social learning fails to propagate this cognitive strategy. When
people make false intuitive conclusions, and are exposed to the analytic output
of their peers, they recognize and adopt this correct output. But they fail to
engage analytical reasoning in similar subsequent tasks. Thus, humans exhibit
an 'unreflective copying bias,' which limits their social learning to the
output, rather than the process, of their peers' reasoning -even when doing so
requires minimal effort and no technical skill. In contrast to much recent work
on observation-based social learning, which emphasizes the propagation of
successful behavior through copying, our findings identify a limit on the power
of social networks in situations that require analytical reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7570</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7570</id><created>2014-06-29</created><updated>2014-08-20</updated><authors><author><keyname>Tsourakakis</keyname><forenames>Charalampos E.</forenames></author></authors><title>Streaming Graph Partitioning in the Planted Partition Model</title><categories>cs.DS cs.DC</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sheer increase in the size of graph data has created a lot of interest
into developing efficient distributed graph processing frameworks. Popular
existing frameworks such as Graphlab and Pregel rely on balanced graph
partitioning in order to minimize communication and achieve work balance.
  In this work we contribute to the recent research line of streaming graph
partitioning \cite{stantonstreaming,stanton,fennel} which computes an
approximately balanced $k$-partitioning of the vertex set of a graph using a
single pass over the graph stream using degree-based criteria. This graph
partitioning framework is well tailored to processing large-scale and dynamic
graphs. In this work we introduce the use of higher length walks for streaming
graph partitioning and show that their use incurs a minor computational cost
which can significantly improve the quality of the graph partition. We perform
an average case analysis of our algorithm using the planted partition model
\cite{condon2001algorithms,mcsherry2001spectral}. We complement the recent
results of Stanton \cite{stantonstreaming} by showing that our proposed method
recovers the true partition with high probability even when the gap of the
model tends to zero as the size of the graph grows. Furthermore, among the wide
number of choices for the length of the walks we show that the proposed length
is optimal. Finally, we conduct experiments which verify the value of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7572</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7572</id><created>2014-06-29</created><authors><author><keyname>Azari</keyname><forenames>Amin</forenames></author><author><keyname>Harsini</keyname><forenames>Jalil Seifali</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author></authors><title>Performance Analysis of Ad-Hoc Routing in Clustered Multi-hop Wireless
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the performance of clustered decode-and-forward multi-hop
relaying (CDFMR) wireless Rayleigh fading networks, and sheds light on their
design principles for energy and spectral efficiency. The focus is on a general
performance analysis (over all SNR range) of heterogeneous wireless networks
with possibly different numbers of relays in clusters of various separations.
For clustered multi-hop relaying systems, ad-hoc routing is known as an
efficient decentralized routing algorithm which selects the best relay node on
a hop-by-hop basis using local channel state information. In this article, we
combine ad-hoc routing and cooperative diversity in CDFMR systems, and we
derive (i) a closed-form expression for the probability distribution of the
end-to-end SNR at the destination node; (ii) the system symbol error rate (SER)
performance for a wide class of modulation schemes; and (iii) exact analytical
expressions for the system ergodic capacity, the outage probability and the
achievable probability of the SNR (power) gain. We also provide simple
analytical asymptotic expressions for SER and the outage probability in high
SNR regime. Simulation results are provided to validate the correctness of the
presented analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7577</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7577</id><created>2014-06-29</created><updated>2016-01-18</updated><authors><author><keyname>Kaminski</keyname><forenames>Jermain</forenames></author></authors><title>Nowcasting the Bitcoin Market with Twitter Signals</title><categories>cs.SI</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/48</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes correlations and causalities between Bitcoin market
indicators and Twitter posts containing emotional signals on Bitcoin. Within a
timeframe of 104 days (November 23rd 2013 - March 7th 2014), about 160,000
Twitter posts containing &quot;bitcoin&quot; and a positive, negative or uncertainty
related term were collected and further analyzed. For instance, the terms
&quot;happy&quot;, &quot;love&quot;, &quot;fun&quot;, &quot;good&quot;, &quot;bad&quot;, &quot;sad&quot; and &quot;unhappy&quot; represent positive
and negative emotional signals, while &quot;hope&quot;, &quot;fear&quot; and &quot;worry&quot; are considered
as indicators of uncertainty. The static (daily) Pearson correlation results
show a significant positive correlation between emotional tweets and the close
price, trading volume and intraday price spread of Bitcoin. However, a dynamic
Granger causality analysis does not confirm a statistically significant effect
of emotional Tweets on Bitcoin market values. To the contrary, the analyzed
data shows that a higher Bitcoin trading volume Granger causes more signals of
uncertainty within a 24 to 72-hour timeframe. This result leads to the
interpretation that emotional sentiments rather mirror the market than that
they make it predictable. Finally, the conclusion of this paper is that the
microblogging platform Twitter is Bitcoin's virtual trading floor, emotionally
reflecting its trading dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7578</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7578</id><created>2014-06-29</created><authors><author><keyname>De Polavieja</keyname><forenames>Gonzalo</forenames></author><author><keyname>Madirolas</keyname><forenames>Gabriel</forenames></author></authors><title>Wisdom of the Confident: Using Social Interactions to Eliminate the Bias
  in Wisdom of the Crowds</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/49</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human groups can perform extraordinary accurate estimations compared to
individuals by simply using the mean, median or geometric mean of the
individual estimations [Galton 1907, Surowiecki 2005, Page 2008]. However, this
is true only for some tasks and in general these collective estimations show
strong biases. The method fails also when allowing for social interactions,
which makes the collective estimation worse as individuals tend to converge to
the biased result [Lorenz et al. 2011]. Here we show that there is a bright
side of this apparently negative impact of social interactions into collective
intelligence. We found that some individuals resist the social influence and,
when using the median of this subgroup, we can eliminate the bias of the wisdom
of the full crowd. To find this subgroup of individuals more confident in their
private estimations than in the social influence, we model individuals as
estimators that combine private and social information with different relative
weights [Perez-Escudero &amp; de Polavieja 2011, Arganda et al. 2012]. We then
computed the geometric mean for increasingly smaller groups by eliminating
those using in their estimations higher values of the social influence weight.
The trend obtained in this procedure gives unbiased results, in contrast to the
simpler method of computing the median of the complete group. Our results show
that, while a simple operation like the mean, median or geometric mean of a
group may not allow groups to make good estimations, a more complex operation
taking into account individuality in the social dynamics can lead to a better
collective intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7579</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7579</id><created>2014-06-29</created><authors><author><keyname>Miller</keyname><forenames>Ian</forenames></author><author><keyname>Cupchik</keyname><forenames>Gerald</forenames></author></authors><title>Meme creation and sharing processes: individuals shaping the masses</title><categories>cs.SI cs.CY</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/139</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The propagation of online memes is initially influenced by meme creators and
secondarily by meme consumers, whose individual sharing decisions accumulate to
determine total meme propagation. We characterize this as a sender/receiver
sequence in which the first sender is also the creator. This sequence consists
of two distinct processes, the creation process and the sharing process. We
investigated these processes separately to determine their individual influence
on sharing outcomes. Our study observed participants creating memes in the lab.
We then tracked the sharing of those memes, derived a model of sharing
behavior, and implemented our sharing model in a contagion simulation.
  Although we assume meme consumers typically have little or no information
about a meme's creator when making a decision about whether to share a meme
(and vice versa), we nevertheless ask whether consumer re-sharing behavior can
be predicted based on features of the creator. Using human participants, web
log monitoring, and statistical model fitting, the resulting Creator Model of
Re-sharing Behavior predicts 11.5% of the variance in the behavior of
consumers. Even when we know nothing about re-sharers of a meme, we can predict
something about their behavior by observing the creation process.
  To investigate the individual re-sharing decisions that, together, constitute
a meme's total consumer response, we built a statistical model from human
observation. Receivers make their decision to share as a function of the meme's
content and their reaction to it, which we model as a consumer's decision to
share. The resulting Consumer Model of Sharing Decisions describes 37.5% of the
variance in this decision making process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7581</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7581</id><created>2014-06-29</created><authors><author><keyname>Rothschild</keyname><forenames>David</forenames></author><author><keyname>Goel</keyname><forenames>Sharad</forenames></author><author><keyname>Gelman</keyname><forenames>Andrew</forenames></author><author><keyname>Rivers</keyname><forenames>Doug</forenames></author></authors><title>The Mythical Swing Voter</title><categories>cs.SI</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/132</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The only acceptable form of polling in the multi-billion dollar survey
research field utilizes representative samples. We argue that with proper
statistical adjustment, non-representative polling can provide accurate
predictions, and often in a much more timely and cost-effective fashion. We
demonstrate this by applying multilevel regression and post-stratification
(MRP) to a 2012 election survey on the Xbox gaming platform. Not only do the
transformed top-line projections from this data closely trend standard
indicators, but we use the unique nature of the data's size and panel to answer
a meaningful political puzzle. We find that reported swings in public opinion
polls are generally not due to actual shifts in vote intention, but rather are
the result of temporary periods of relatively low response rates among
supporters of the reportedly slumping candidate. This work shows great promise
for using non-representative polling to measure public opinion and the first
product of this new polling technique raises the possibility that decades of
large, reported swings in public opinion-including the perennial &quot;convention
bounce&quot;-are mostly artifacts of sampling bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7582</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7582</id><created>2014-06-29</created><authors><author><keyname>Doboli</keyname><forenames>Simona</forenames></author><author><keyname>Zhao</keyname><forenames>Fanshu</forenames></author><author><keyname>Doboli</keyname><forenames>Alex</forenames></author></authors><title>New measures for evaluating creativity in scientific publications</title><categories>cs.SI cs.DL physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/127</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of our research is to understand how ideas propagate, combine and
are created in large social networks. In this work, we look at a sample of
relevant scientific publications in the area of high-frequency analog circuit
design and their citation distribution. A novel aspect of our work is the way
in which we categorize citations based on the reason and place of it in a
publication. We created seven citation categories from general domain
references, references to specific methods used in the same domain problem,
references to an analysis method, references for experimental comparison and so
on. This added information allows us to define two new measures to characterize
the creativity (novelty and usefulness) of a publication based on its pattern
of citations clustered by reason, place and citing scientific group. We
analyzed 30 publications in relevant journals since 2000 and their about 300
citations, all in the area of high-frequency analog circuit design. We observed
that the number of citations a publication receives from different scientific
groups matches a Levy type distribution: with a large number of groups citing a
publication relatively few times, and a very small number of groups citing a
publication a large number of times. We looked at the motifs a publication is
cited differently by different scientific groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7583</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7583</id><created>2014-06-29</created><authors><author><keyname>Sun</keyname><forenames>Wei</forenames></author><author><keyname>Laskey</keyname><forenames>Kathryn</forenames></author><author><keyname>Twardy</keyname><forenames>Charles</forenames></author><author><keyname>Hanson</keyname><forenames>Robin</forenames></author><author><keyname>Goldfedder</keyname><forenames>Brandon</forenames></author></authors><title>Trade-based Asset Model using Dynamic Junction Tree for Combinatorial
  Prediction Markets</title><categories>cs.GT</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/126</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction markets have demonstrated their value for aggregating collective
expertise. Combinatorial prediction markets allow forecasts not only on base
events, but also on conditional and/or Boolean combinations of events. We
describe a trade-based combinatorial prediction market asset management system,
called Dynamic Asset Cluster (DAC), that improves both time and space
efficiency over the method of, which maintains parallel junction trees for
assets and probabilities. The basic data structure is the asset block, which
compactly represents a set of trades made by a user. A user's asset model
consists of a set of asset blocks representing the user's entire trade history.
A junction tree is created dynamically from the asset blocks to compute a
user's minimum and expected assets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7584</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7584</id><created>2014-06-29</created><authors><author><keyname>Olson</keyname><forenames>Kenneth</forenames></author><author><keyname>Laskey</keyname><forenames>Kathryn</forenames></author><author><keyname>Twardy</keyname><forenames>Charles</forenames></author></authors><title>Interval Elicitation of Forecasts in a Prediction Market Reveals Lack of
  Anchoring &quot;Bias&quot;</title><categories>cs.SI</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/125</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an online prediction market, forecasters who could not see the current
state of the market until they made their own separate estimates moved their
estimates closer to the market forecast when the current state of the market
became known. Their first edits to the market forecast were very similar to the
first edits of forecasters who could always see the current state of the
market, and forecasters in both conditions had similar accuracy. These results
suggest that our more elaborate forecast elicitation method might not improve
forecasts and that any anchoring on the state of the market does not constitute
an error in judgment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7585</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7585</id><created>2014-06-29</created><authors><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author></authors><title>Social diffusion and global drift in adaptive social networks</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/123</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social contagion has been studied in various contexts. Many instances of
social contagion can be modeled as an infection process where a specific state
(adoption of product, fad, knowledge, behavior, etc.) spreads from individual
to individual through links between them. In the meantime, other forms of
social contagion may better be understood as a diffusion process where the
state of an individual tends to assimilate with the social norm (i.e., local
average state) within his/her neighborhood.
  Unlike infection scenarios where influence is nonlinear, unidirectional,
fast, and potentially disruptive, social diffusion is linear, bidirectional,
gradual, and converging. The distance between an individual's state and his/her
neighbors' average state always decreases, and thus a homogeneous global state
is guaranteed to be the network's stable equilibrium state in the long run.
This does not sound as intriguing or exciting as infection dynamics, which
might be why there are very few studies on mathematical models of social
diffusion processes.
  Here, this study attempts to shed new light on an unrecognized characteristic
of social diffusion, i.e., non-trivial drift it can cause to the network's
global average state. Although somewhat counterintuitive, such global drift is
indeed possible because, unlike physical diffusion processes, social diffusion
processes are not conservational. In what follows, a mathematical model of
social diffusion will be presented to explain the mechanism of this phenomenon,
and some possible collective actions for influencing the direction of global
drift will be proposed. The relevance of social diffusion to individual and
collective improvement will be discussed briefly, with an emphasis on
educational applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7586</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7586</id><created>2014-06-29</created><authors><author><keyname>Shore</keyname><forenames>Jesse</forenames></author><author><keyname>Bernstein</keyname><forenames>Ethan</forenames></author><author><keyname>Lazer</keyname><forenames>David</forenames></author></authors><title>Facts and Figuring: An Experimental Investigation of Network Structure
  and Performance in Information and Solution Spaces</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/120</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using data from a large laboratory experiment on problem solving in which we
varied the structure of 16-person networks we investigate how an organization's
network structure may be constructed to optimize performance in complex
problem-solving tasks. Problem solving involves both search for information and
search for theories to make sense of that information. We show that the effect
of network structure is opposite for these two equally important forms of
search. Dense clustering encourages members of a network to generate more
diverse information, but it also has the power to discourage the generation of
diverse theories: clustering promotes exploration in information space, but
decreases exploration in solution space. Previous research, tending to focus on
only one of those two spaces, had produced inconsistent conclusions about the
value of network clustering. By adopting an experimental platform on which
information was measured separately from solutions, we were able to reconcile
past contradictions and clarify the effects of network clustering on
performance. The finding both provides a sharper tool for structuring
organizations for knowledge work and reveals the challenges inherent in
manipulating network structure to enhance performance, as the communication
structure that helps one aspect of problem solving may harm the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7588</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7588</id><created>2014-06-29</created><authors><author><keyname>Staffelbach</keyname><forenames>Matthew</forenames></author><author><keyname>Sempolinski</keyname><forenames>Peter</forenames></author><author><keyname>Hachen</keyname><forenames>David</forenames></author><author><keyname>Kareem</keyname><forenames>Ahsan</forenames></author><author><keyname>Kijewski-Correa</keyname><forenames>Tracy</forenames></author><author><keyname>Thain</keyname><forenames>Douglas</forenames></author><author><keyname>Wei</keyname><forenames>Daniel</forenames></author><author><keyname>Madey</keyname><forenames>Greg</forenames></author></authors><title>Lessons Learned from an Experiment in Crowdsourcing Complex Citizen
  Engineering Tasks with Amazon Mechanical Turk</title><categories>cs.SI cs.CY</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/118</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the feasibility of obtaining highly trustworthy results using
crowdsourcing on complex engineering tasks. Crowdsourcing is increasingly seen
as a potentially powerful way of increasing the supply of labor for solving
society's problems. While applications in domains such as citizen-science,
citizen-journalism or knowledge organization (e.g., Wikipedia) have seen many
successful applications, there have been fewer applications focused on solving
engineering problems, especially those involving complex tasks. This may be in
part because of concerns that low quality input into engineering analysis and
design could result in failed structures leading to loss of life. We compared
the quality of work of the anonymous workers of Amazon Mechanical Turk (AMT),
an online crowdsourcing service, with the quality of work of expert engineers
in solving the complex engineering task of evaluating virtual wind tunnel data
graphs. On this representative complex engineering task, our results showed
that there was little difference between expert engineers and crowdworkers in
the quality of their work and explained reasons for these results. Along with
showing that crowdworkers are effective at completing new complex tasks our
paper supplies a number of important lessons that were learned in the process
of collecting this data from AMT, which may be of value to other researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7589</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7589</id><created>2014-06-29</created><authors><author><keyname>Uribe</keyname><forenames>Jose</forenames></author><author><keyname>Wang</keyname><forenames>Dan</forenames></author></authors><title>Learning from Others, Together: Brokerage, Closure and Team Performance</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/114</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scholarship on teams has focused on the relationship between a team's
performance, however defined, and the network structure among team members. For
example, Uzzi and Spiro (2005) find that the creative performance of Broadway
musical teams depends heavily on the internal cohesion of team members and
their past collaborative experience with individuals outside their immediate
teams. In other words, team members' internal cohesion and external ties are
crucial to the team's success. How, then, do they interact to produce positive
performance outcomes? In our work, we separate the proximal causes of tie
formation from the proximal determinants of outcomes to determine the mechanism
behind this interaction. To examine this puzzle, we examine the performance of
national soccer squads over time as a function of changing levels and
configurations of brokerage and closure ties formed by players working for
professional soccer clubs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7608</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7608</id><created>2014-06-30</created><updated>2015-02-11</updated><authors><author><keyname>Bloem</keyname><forenames>Roderick</forenames></author><author><keyname>Jacobs</keyname><forenames>Swen</forenames></author><author><keyname>Khalimov</keyname><forenames>Ayrat</forenames></author></authors><title>Parameterized Synthesis Case Study: AMBA AHB (extended version)</title><categories>cs.SE</categories><comments>Moved to appendix some not very important proofs. To section
  'optimizations: added the model for 0-process. Extended version of the paper
  submitted to SYNT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the AMBA AHB case study that has been used as a benchmark for
several reactive syn- thesis tools. Synthesizing AMBA AHB implementations that
can serve a large number of masters is still a difficult problem. We
demonstrate how to use parameterized synthesis in token rings to obtain an
implementation for a component that serves a single master, and can be arranged
in a ring of arbitrarily many components. We describe new tricks -- property
decompositional synthesis, and direct encoding of simple GR(1) -- that together
with previously described optimizations allowed us to synthesize the model with
14 states in 30 minutes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7611</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7611</id><created>2014-06-30</created><updated>2015-02-16</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Validity of altmetrics data for measuring societal impact: A study using
  data from Altmetric and F1000Prime</title><categories>cs.DL physics.soc-ph stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can altmetric data be validly used for the measurement of societal impact?
The current study seeks to answer this question with a comprehensive dataset
(about 100,000 records) from very disparate sources (F1000, Altmetric, and an
in-house database based on Web of Science). In the F1000 peer review system,
experts attach particular tags to scientific papers which indicate whether a
paper could be of interest for science or rather for other segments of society.
The results show that papers with the tag &quot;good for teaching&quot; do achieve higher
altmetric counts than papers without this tag - if the quality of the papers is
controlled. At the same time, a higher citation count is shown especially by
papers with a tag that is specifically scientifically oriented (&quot;new finding&quot;).
The findings indicate that papers tailored for a readership outside the area of
research should lead to societal impact. If altmetric data is to be used for
the measurement of societal impact, the question arises of its normalization.
In bibliometrics, citations are normalized for the papers' subject area and
publication year. This study has taken a second analytic step involving a
possible normalization of altmetric data. As the results show there are
particular scientific topics which are of especial interest for a wide
audience. Since these more or less interesting topics are not completely
reflected in Thomson Reuters' journal sets, a normalization of altmetric data
should not be based on the level of subject categories, but on the level of
topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7620</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7620</id><created>2014-06-30</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Joint Optimization of Spectrum Sensing and Accessing in Multiuser MISO
  Cognitive Networks</title><categories>cs.IT math.IT</categories><comments>10 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a joint spectrum sensing and accessing optimization framework
for a multiuser cognitive network is proposed to significantly improve spectrum
efficiency. For such a cognitive network, there are two important and limited
resources that should be distributed in a comprehensive manner, namely feedback
bits and time duration. First, regarding the feedback bits, there are two
components: sensing component (used to convey various users' sensing results)
and accessing component (used to feedback channel state information). A large
sensing component can support more users to perform cooperative sensing, which
results in high sensing precision. However, a large accessing component is
preferred as well, as it has a direct impact on the performance in the
multiuser cognitive network when multi-antenna technique, such as zero-forcing
beamforming (ZFBF), is utilized. Second, the tradeoff of sensing and accessing
duration in a transmission interval needs to be determined, so that the sum
transmission rate is optimized while satisfying the interference constraint. In
addition, the above two resources are interrelated and inversive under some
conditions. Specifically, sensing time can be saved by utilizing more sensing
feedback bits for a given performance objective. Hence, the resources should be
allocation in a jointly manner. Based on the joint optimization framework and
the intrinsic relationship between the two resources, we propose two joint
resource allocation schemes by maximizing the average sum transmission rate in
a multiuser multi-antenna cognitive network. Simulation results show that, by
adopting the joint resource allocation schemes, obvious performance gain can be
obtained over the traditional fixed strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7623</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7623</id><created>2014-06-30</created><authors><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author><author><keyname>McKay</keyname><forenames>Matthew R.</forenames></author><author><keyname>Xiao</keyname><forenames>Chengshan</forenames></author></authors><title>Transmit Designs for the MIMO Broadcast Channel with Statistical CSI</title><categories>cs.IT math.IT</categories><comments>Accepted for IEEE Transaction on Signal Processing</comments><doi>10.1109/TSP.2014.2336637</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the multiple-input multiple-output broadcast channel with
statistical channel state information available at the transmitter. The
so-called linear assignment operation is employed, and necessary conditions are
derived for the optimal transmit design under general fading conditions. Based
on this, we introduce an iterative algorithm to maximize the linear assignment
weighted sum-rate by applying a gradient descent method. To reduce complexity,
we derive an upper bound of the linear assignment achievable rate of each
receiver, from which a simplified closed-form expression for a near-optimal
linear assignment matrix is derived. This reveals an interesting construction
analogous to that of dirty-paper coding. In light of this, a low complexity
transmission scheme is provided. Numerical examples illustrate the significant
performance of the proposed low complexity scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7629</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7629</id><created>2014-06-30</created><updated>2014-07-24</updated><authors><author><keyname>Chitraganti</keyname><forenames>Shaikshavali</forenames></author><author><keyname>Aberkane</keyname><forenames>Samir</forenames></author><author><keyname>Aubrun</keyname><forenames>Christophe</forenames></author><author><keyname>Valencia-Palomo</keyname><forenames>Guillermo</forenames></author><author><keyname>Dragan</keyname><forenames>Vasile</forenames></author></authors><title>On control of discrete-time state-dependent jump linear systems with
  probabilistic constraints: A receding horizon approach</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we consider a receding horizon control of discrete-time
state-dependent jump linear systems, particular kind of stochastic switching
systems, subject to possibly unbounded random disturbances and probabilistic
state constraints. Due to a nature of the dynamical system and the constraints,
we consider a one-step receding horizon. Using inverse cumulative distribution
function, we convert the probabilistic state constraints to deterministic
constraints, and obtain a tractable deterministic receding horizon control
problem. We consider the receding control law to have a linear state-feedback
and an admissible offset term. We ensure mean square boundedness of the state
variable via solving linear matrix inequalities off-line, and solve the
receding horizon control problem on-line with control offset terms. We
illustrate the overall approach applied on a macroeconomic system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7630</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7630</id><created>2014-06-30</created><authors><author><keyname>Chitraganti</keyname><forenames>Shaikshavali</forenames></author><author><keyname>Aberkane</keyname><forenames>Samir</forenames></author><author><keyname>Aubrun</keyname><forenames>Christophe</forenames></author></authors><title>Stochastic stability and stabilization of a class of state-dependent
  jump linear systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals a continuous-time state-dependent jump linear system, a
particular kind of stochastic switching system. In particular, we consider a
situation when the transition rate of the random jump process depends on the
state variable, and addressed the problem of stochastic stability and
stabilization analysis for the proposed system. Numerically solvable
?sufficient conditions for the stochastic stability and stabilization of the
proposed system is established in terms of linear matrix inequalities. The
obtained results are illustrated in numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7639</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7639</id><created>2014-06-30</created><updated>2015-06-18</updated><authors><author><keyname>Marecek</keyname><forenames>Jakub</forenames></author><author><keyname>Shorten</keyname><forenames>Robert</forenames></author><author><keyname>Yu</keyname><forenames>Jia Yuan</forenames></author></authors><title>Signalling and obfuscation for congestion control</title><categories>math.OC cs.MA math.DS</categories><comments>in International Journal of Control, 2015</comments><doi>10.1080/00207179.2015.1033758</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim to reduce the social cost of congestion in many smart city
applications. In our model of congestion, agents interact over limited
resources after receiving signals from a central agent that observes the state
of congestion in real time. Under natural models of agent populations, we
develop new signalling schemes and show that by introducing a non-trivial
amount of uncertainty in the signals, we reduce the social cost of congestion,
i.e., improve social welfare. The signalling schemes are efficient in terms of
both communication and computation, and are consistent with past observations
of the congestion. Moreover, the resulting population dynamics converge under
reasonable assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7648</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7648</id><created>2014-06-30</created><updated>2015-06-01</updated><authors><author><keyname>Scutari</keyname><forenames>Marco</forenames></author></authors><title>Bayesian Network Constraint-Based Structure Learning Algorithms:
  Parallel and Optimised Implementations in the bnlearn R Package</title><categories>stat.CO cs.AI cs.MS stat.ME</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known in the literature that the problem of learning the structure
of Bayesian networks is very hard to tackle: its computational complexity is
super-exponential in the number of nodes in the worst case and polynomial in
most real-world scenarios.
  Efficient implementations of score-based structure learning benefit from past
and current research in optimisation theory, which can be adapted to the task
by using the network score as the objective function to maximise. This is not
true for approaches based on conditional independence tests, called
constraint-based learning algorithms. The only optimisation in widespread use,
backtracking, leverages the symmetries implied by the definitions of
neighbourhood and Markov blanket.
  In this paper we illustrate how backtracking is implemented in recent
versions of the bnlearn R package, and how it degrades the stability of
Bayesian network structure learning for little gain in terms of speed. As an
alternative, we describe a software architecture and framework that can be used
to parallelise constraint-based structure learning algorithms (also implemented
in bnlearn) and we demonstrate its performance using four reference networks
and two real-world data sets from genetics and systems biology. We show that on
modern multi-core or multiprocessor hardware parallel implementations are
preferable over backtracking, which was developed when single-processor
machines were the norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7650</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7650</id><created>2014-06-30</created><updated>2015-03-17</updated><authors><author><keyname>Femminella</keyname><forenames>M.</forenames></author><author><keyname>Francescangeli</keyname><forenames>R.</forenames></author><author><keyname>Reali</keyname><forenames>G.</forenames></author><author><keyname>Schulzrinne</keyname><forenames>H.</forenames></author></authors><title>Gossip-based Signaling Dissemination Extension for Next Steps In
  Signaling</title><categories>cs.NI</categories><comments>Accepted for publication at IFIP/IEEE NOMS 2012, Maui, USA</comments><acm-class>C.2.2</acm-class><doi>10.1109/NOMS.2012.6212024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new gossip-based signaling dissemination method
for the Next Steps in Signaling protocol family. In more detail, we propose to
extend the General Internet Signaling Transport (GIST) protocol, so as to
leverage these new dissemination capabilities from all NSIS Signaling Layer
Protocol applications using its transport capabilities. The new GIST extension
consists of two main procedures: a bootstrap procedure, during which new
GIST-enabled nodes discover each other, and a service dissemination procedure,
which is used to effectively disseminate signaling messages within an
Autonomous System. To this aim, we defined three dissemination models, bubble,
balloon, and hose, so as to fulfill requirements of different network and/or
service management scenarios. An experimental campaign carried out on the GENI
testbed shows the effectiveness of the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7658</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7658</id><created>2014-06-30</created><updated>2014-08-17</updated><authors><author><keyname>Allender</keyname><forenames>Eric</forenames><affiliation>Rutgers University</affiliation></author><author><keyname>Buhrman</keyname><forenames>Harry</forenames><affiliation>CWI &amp; University of Amsterdam</affiliation></author><author><keyname>Friedman</keyname><forenames>Luke</forenames><affiliation>Rutgers University</affiliation></author><author><keyname>Loff</keyname><forenames>Bruno</forenames><affiliation>CWI</affiliation></author></authors><title>Reductions to the set of random strings: The resource-bounded case</title><categories>cs.CC cs.LO</categories><comments>Conference version in MFCS 2012</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  19, 2014) lmcs:723</journal-ref><doi>10.2168/LMCS-10(3:5)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is motivated by a conjecture that BPP can be characterized in
terms of polynomial-time nonadaptive reductions to the set of Kolmogorov-random
strings. In this paper we show that an approach laid out in [Allender et al] to
settle this conjecture cannot succeed without significant alteration, but that
it does bear fruit if we consider time-bounded Kolmogorov complexity instead.
We show that if a set A is reducible in polynomial time to the set of
time-t-bounded Kolmogorov random strings (for all large enough time bounds t),
then A is in P/poly, and that if in addition such a reduction exists for any
universal Turing machine one uses in the definition of Kolmogorov complexity,
then A is in PSPACE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7662</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7662</id><created>2014-06-30</created><authors><author><keyname>Zackriya.</keyname><forenames>Mohammed</forenames><suffix>V</suffix></author><author><keyname>Kittur</keyname><forenames>Harish M</forenames></author></authors><title>Selective Match-Line Energizer Content Addressable Memory(SMLE -CAM)</title><categories>cs.AR</categories><comments>6 pages, 12 figures. Accepted for publication, International Journal
  of applied Engineering Research,Vol. 8 No. 19, 2013</comments><acm-class>B.7.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Content Addressable Memory (CAM) is a memory primarily designed for high
speed search operation. Parallel search scheme forms the basis of CAM, thus
power reduction is the challenge associated with a large amount of parallel
active circuits. We are presenting a novel algorithm and architecture described
as Selective Match-Line Energizer Content Addressable Memory (SMLE-CAM) which
energizes only those MLs (Match-Line) whose first three bits are conditionally
matched with corresponding first three search bit using special architecture
which comprises of novel XNOR-CAM cell and novel XOR-CAM cell. The rest of the
CAM chain is followed by NOR-CAM cell. The 256 X 144 bit SMLE-CAM is
implemented in TSMC 90 nm technology and its robustness across PVT variation is
verified. The post-layout simulation result shows, it has energy metric of
0.115 fJ/bit/search with search time 361.6 ps, the best reported so far. The
maximum operating frequency is 1GHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7684</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7684</id><created>2014-06-30</created><updated>2014-08-13</updated><authors><author><keyname>Blumensath</keyname><forenames>Achim</forenames><affiliation>TU Darmstadt</affiliation></author><author><keyname>Otto</keyname><forenames>Martin</forenames><affiliation>TU Darmstadt</affiliation></author><author><keyname>Weyer</keyname><forenames>Mark</forenames><affiliation>none</affiliation></author></authors><title>Decidability Results for the Boundedness Problem</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  15, 2014) lmcs:1225</journal-ref><doi>10.2168/LMCS-10(3:2)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove decidability of the boundedness problem for monadic least
fixed-point recursion based on positive monadic second-order (MSO) formulae
over trees. Given an MSO-formula phi(X,x) that is positive in X, it is
decidable whether the fixed-point recursion based on phi is spurious over the
class of all trees in the sense that there is some uniform finite bound for the
number of iterations phi takes to reach its least fixed point, uniformly across
all trees. We also identify the exact complexity of this problem. The proof
uses automata-theoretic techniques. This key result extends, by means of
model-theoretic interpretations, to show decidability of the boundedness
problem for MSO and guarded second-order logic (GSO) over the classes of
structures of fixed finite tree-width. Further model-theoretic transfer
arguments allow us to derive major known decidability results for boundedness
for fragments of first-order logic as well as new ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7685</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7685</id><created>2014-06-30</created><authors><author><keyname>Aziz</keyname><forenames>Mehwish</forenames></author><author><keyname>Nawaz</keyname><forenames>Shabnam</forenames></author><author><keyname>Batool</keyname><forenames>Pakeeza</forenames></author></authors><title>Efficiency Analysis of Materialized views in DataWarehouse Using
  selfmaintenance</title><categories>cs.DB</categories><comments>Journal based on thesis</comments><acm-class>H.2.0; H.2.2; H.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A data warehouse is a large data repository for the purpose of analysis and
decision making in organizations. To improve the query performance and to get
fast access to the data, data is stored as materialized views (MV) in the data
warehouse. When data at source gets updated, the materialized views also need
to be updated. In this paper, we focus on the problem of maintenance of these
materialized views and address the issue of finding such auxiliary views (AV)
that together with the materialized views make the data self-maintainable and
take minimal space. We propose an algorithm that uses key and referential
constraints which reduces the total number of tuples in auxiliary views and
uses idea of information sharing between these auxiliary views to further
reduce number of auxiliary views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7699</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7699</id><created>2014-06-30</created><updated>2015-04-19</updated><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Multicast Multigroup Precoding and User Scheduling for Frame-Based
  Satellite Communications</title><categories>cs.IT math.IT</categories><comments>Accepted for publication to the IEEE Transactions on Wireless
  Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work focuses on the forward link of a broadband multibeam
satellite system that aggressively reuses the user link frequency resources.
Two fundamental practical challenges, namely the need to frame multiple users
per transmission and the per-antenna transmit power limitations, are addressed.
To this end, the so-called frame-based precoding problem is optimally solved
using the principles of physical layer multicasting to multiple co-channel
groups under per-antenna constraints. In this context, a novel optimization
problem that aims at maximizing the system sum rate under individual power
constraints is proposed. Added to that, the formulation is further extended to
include availability constraints. As a result, the high gains of the sum rate
optimal design are traded off to satisfy the stringent availability
requirements of satellite systems. Moreover, the throughput maximization with a
granular spectral efficiency versus SINR function, is formulated and solved.
Finally, a multicast-aware user scheduling policy, based on the channel state
information, is developed. Thus, substantial multiuser diversity gains are
gleaned. Numerical results over a realistic simulation environment exhibit as
much as 30% gains over conventional systems, even for 7 users per frame,
without modifying the framing structure of legacy communication standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7708</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7708</id><created>2014-06-30</created><authors><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Fritzsche</keyname><forenames>Richard</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Salim</keyname><forenames>Umer</forenames></author></authors><title>Robust Precoding for Network MIMO with Hierarchical CSIT</title><categories>cs.IT math.IT</categories><comments>Extended version of a paper accepted to ISWCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider a wireless network with K cooperating transmitters
(TXs) serving jointly K receivers (RXs). Due to the practical limitations of
the backhaul network, it is relevant to consider a setting where each TX
receives its own imperfect estimate of the multi-user channel state, denoted as
the distributed channel state information (CSI) setting. We focus in this work
on a particular distributed CSI configuration called hierarchical CSI
configuration in which the TXs can be ordered by increasing level of CSI. This
scenario is particularly relevant for future networks with heterogeneous
backhaul where the TXs connected with a weak backhaul link will receive only a
coarse estimate while the TXs with a stronger backhaul will have a more
accurate CSI. In that scenario, we formulate the optimal precoding as a team
decision problem. Solving optimally this problem is extremely challenging such
that we propose a heuristic approach allowing to obtain a simple, yet efficient
and practical, precoding algorithm. The proposed precoding algorithm exploits
the hierarchical structure of the CSI to make the transmission more robust to
the imperfect CSI knowledge at the TXs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7716</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7716</id><created>2014-06-30</created><authors><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author><author><keyname>Lewenstein</keyname><forenames>Moshe</forenames></author><author><keyname>Nicholson</keyname><forenames>Patrick K.</forenames></author></authors><title>Weighted ancestors in suffix trees</title><categories>cs.DS</categories><comments>27 pages, LNCS format. A condensed version will appear in ESA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical, ubiquitous, predecessor problem is to construct a data
structure for a set of integers that supports fast predecessor queries. Its
generalization to weighted trees, a.k.a. the weighted ancestor problem, has
been extensively explored and successfully reduced to the predecessor problem.
It is known that any solution for both problems with an input set from a
polynomially bounded universe that preprocesses a weighted tree in O(n
polylog(n)) space requires \Omega(loglogn) query time. Perhaps the most
important and frequent application of the weighted ancestors problem is for
suffix trees. It has been a long-standing open question whether the weighted
ancestors problem has better bounds for suffix trees. We answer this question
positively: we show that a suffix tree built for a text w[1..n] can be
preprocessed using O(n) extra space, so that queries can be answered in O(1)
time. Thus we improve the running times of several applications. Our
improvement is based on a number of data structure tools and a
periodicity-based insight into the combinatorial structure of a suffix tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7720</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7720</id><created>2014-06-30</created><authors><author><keyname>Lee</keyname><forenames>Edward</forenames></author><author><keyname>Daniels</keyname><forenames>Bryan</forenames></author><author><keyname>Flack</keyname><forenames>Jessica</forenames></author><author><keyname>Krakauer</keyname><forenames>David</forenames></author></authors><title>Capturing collective conflict dynamics with sparse social circuits</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/108</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a set of computational techniques, called Inductive Game Theory,
for extracting strategic decision-making rules from time series data and
constructing probabilistic social circuits. We construct these circuits by
connecting component individuals and groups with strategies in a game and
propose an inductive approach to reconstructing the edges. We demonstrate this
approach with conflict behavior in a society of pigtailed macaques by
identifying significant patterns in decision-making by individuals. With the
constructed circuit, we then capture macroscopic features of the system that
were not specified in the construction of the initial circuit, providing a
mapping between individual level behaviors to collective behaviors over the
scale of the group. We extend on previous work in Inductive Game Theory by more
efficiently searching the space of possible strategies by grouping individuals
into socially relevant sets to produce a more efficient, parsimonious
specification of the underlying interactions between components. We discuss how
we reduce the dimensionality of these circuits using coarse-graining or
compression to build cognitive effective theories for collective behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7727</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7727</id><created>2014-06-30</created><authors><author><keyname>Lacic</keyname><forenames>Emanuel</forenames></author><author><keyname>Kowald</keyname><forenames>Dominik</forenames></author><author><keyname>Seitlinger</keyname><forenames>Paul</forenames></author><author><keyname>Trattner</keyname><forenames>Christoph</forenames></author><author><keyname>Parra</keyname><forenames>Denis</forenames></author></authors><title>Recommending Items in Social Tagging Systems Using Tag and Time
  Information</title><categories>cs.IR</categories><comments>6 pages, 2 tables, 9 figures</comments><acm-class>H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a novel item recommendation approach that aims at
improving Collaborative Filtering (CF) in social tagging systems using the
information about tags and time. Our algorithm follows a two-step approach,
where in the first step a potentially interesting candidate item-set is found
using user-based CF and in the second step this candidate item-set is ranked
using item-based CF. Within this ranking step we integrate the information of
tag usage and time using the Base-Level Learning (BLL) equation coming from
human memory theory that is used to determine the reuse-probability of words
and tags using a power-law forgetting function.
  As the results of our extensive evaluation conducted on data-sets gathered
from three social tagging systems (BibSonomy, CiteULike and MovieLens) show,
the usage of tag-based and time information via the BLL equation also helps to
improve the ranking and recommendation process of items and thus, can be used
to realize an effective item recommender that outperforms two alternative
algorithms which also exploit time and tag-based information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7729</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7729</id><created>2014-06-30</created><authors><author><keyname>Krafft</keyname><forenames>Peter</forenames></author><author><keyname>Zheng</keyname><forenames>Julia</forenames></author><author><keyname>Shmueli</keyname><forenames>Erez</forenames></author><author><keyname>Della Penna</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Josh</forenames></author><author><keyname>Pentland</keyname><forenames>Sandy</forenames></author></authors><title>Popularity and Performance: A Large-Scale Study</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/105</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social scientists have long sought to understand why certain people, items,
or options become more popular than others. One seemingly intuitive theory is
that inherent value drives popularity. An alternative theory claims that
popularity is driven by the rich-get-richer effect of cumulative
advantage---certain options become more popular, not because they are higher
quality, but because they are already relatively popular. Realistically, it
seems likely that popularity is driven by neither one of these forces alone but
rather both together. Recently, researchers have begun using large-scale online
experiments to study the effect of cumulative advantage in realistic scenarios,
but there have been no large-scale studies of the combination of these two
effects. We are interested in studying a case where decision-makers observe
explicit signals of both the popularity and the quality of various options. We
derive a model for change in popularity as a function of past popularity and
past perceived quality. Our model implies that we should expect an interaction
between these two forces---popularity should amplify the effect of quality, so
that the more popular an option is, the faster we expect it to increase in
popularity with better perceived quality. We use a data set from eToro.com, an
online social investment platform, to support this hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7735</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7735</id><created>2014-06-30</created><authors><author><keyname>Zhang</keyname><forenames>Haoqi</forenames></author><author><keyname>Monroy-Hernandez</keyname><forenames>Andes</forenames></author><author><keyname>Shaw</keyname><forenames>Aaron</forenames></author><author><keyname>Munson</keyname><forenames>Sean</forenames></author><author><keyname>Gerber</keyname><forenames>Liz</forenames></author><author><keyname>Hill</keyname><forenames>Benjamin Mako</forenames></author><author><keyname>Kinnaird</keyname><forenames>Peter</forenames></author><author><keyname>Farnham</keyname><forenames>Shelly</forenames></author><author><keyname>Minder</keyname><forenames>Patrick</forenames></author></authors><title>WeDo: Exploring Participatory, End-To-End Collective Action</title><categories>cs.CY</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/95</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many celebrate the Internet's ability to connect individuals and facilitate
collective action toward a common goal. While numerous systems have been
designed to support particular aspects of collective action, few systems
support participatory, end-to-end collective action in which a crowd or
community identifies opportunities, formulates goals, brainstorms ideas,
develops plans, mobilizes, and takes action. To explore the possibilities and
barriers in supporting such interactions, we have developed WeDo, a system
aimed at promoting simple forms of participatory, end-to-end collective action.
Pilot deployments of WeDo illustrate that sociotechnical systems can support
automated transitions through different phases of end-to-end collective action,
but that challenges, such as the elicitation of leadership and the
accommodation of existing group norms, remain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7738</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7738</id><created>2014-06-30</created><authors><author><keyname>Das</keyname><forenames>Sanmay</forenames></author><author><keyname>Lavoie</keyname><forenames>Allen</forenames></author></authors><title>Home Is Where the Up-Votes Are: Behavior Changes in Response to Feedback
  in Social Media</title><categories>cs.SI physics.soc-ph</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/93</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research shows that humans are heavily influenced by online social
interactions: We are more likely to perform actions which, in the past, have
led to positive social feedback. We introduce a quantitative model of behavior
changes in response to such feedback, drawing on inverse reinforcement learning
and studies of human game playing. The model allows us to make predictions,
particularly in the context of social media, about which community a user will
select, and to quantify how future selections change based on the feedback a
user receives. We show that our model predicts real-world changes in behavior
on a dataset gathered from reddit. We also explore how this relatively simple
model of individual behavior can lead to complex collective dynamics when there
is a population of users, each individual learning in response to feedback and
in turn providing feedback to others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7744</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7744</id><created>2014-06-26</created><authors><author><keyname>Ajmera</keyname><forenames>Reema</forenames></author><author><keyname>dharamdasani</keyname><forenames>Dinesh Kumar</forenames></author></authors><title>E-Learning Quality Criteria and Aspects</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As IT grows the impact of new technology reflects in more or less every
field. Education also gets new dimensions with the advancement in IT sector.
Nowadays education is not limited to books and black boards only it gets a new
way i.e. electronic media. Although with e-learning, the education having
broader phenomena, yet it is in budding stage. Quality is a crucial issue for
education as well as e-learning. It is required to serve qualitative and
standardization education. Quality cannot be expressed and set by a simple
definition, since in itself quality is a very abstract notion. The specified
context and the perspectives of users need to be taken into account when
defining quality in e-learning. It is also essential to classify suitable
criteria to address quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7746</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7746</id><created>2014-06-30</created><authors><author><keyname>Maillart</keyname><forenames>Thomas</forenames></author><author><keyname>Sornette</keyname><forenames>Didier</forenames></author></authors><title>Using Prediction Markets to Incentivize and Measure Collective Knowledge
  Production</title><categories>cs.GT</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/85</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a mechanism design, coupling an online collaboration software and
a prediction market, which allows tracking down the very roots of individual
incentives, actions and how these behaviors influence collective intelligence
in terms of knowledge production as a public good. We show that the incentive
mechanism efficiently engages users without further governance structure, and
doesn't crowd out intrinsic motivation. Furthermore, it enables a powerful and
robust creative destruction process, which helps quickly filter out irrelevant
knowledge. While still at an early stage, this mechanism design can not only
bring insights for knowledge production organization design, but also has the
potential to illuminate the fundamental mechanisms underlying the emergence of
collective intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7749</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7749</id><created>2014-06-30</created><authors><author><keyname>Absalom</keyname><forenames>Richard</forenames></author><author><keyname>Luczak-Rosch</keyname><forenames>Marcus</forenames></author><author><keyname>Hartmann</keyname><forenames>Dap</forenames></author><author><keyname>Plaat</keyname><forenames>Aske</forenames></author></authors><title>Crowd-Sourcing Fuzzy and Faceted Classification for Concept Search</title><categories>cs.IR cs.DL</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/82</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching for concepts in science and technology is often a difficult task.
To facilitate concept search, different types of human-generated metadata have
been created to define the content of scientific and technical disclosures.
Classification schemes such as the International Patent Classification (IPC)
and MEDLINE's MeSH are structured and controlled, but require trained experts
and central management to restrict ambiguity (Mork, 2013). While unstructured
tags of folksonomies can be processed to produce a degree of structure
(Kalendar, 2010; Karampinas, 2012; Sarasua, 2012; Bragg, 2013) the freedom
enjoyed by the crowd typically results in less precision (Stock 2007).
  Existing classification schemes suffer from inflexibility and ambiguity.
Since humans understand language, inference, implication, abstraction and hence
concepts better than computers, we propose to harness the collective wisdom of
the crowd. To do so, we propose a novel classification scheme that is
sufficiently intuitive for the crowd to use, yet powerful enough to facilitate
search by analogy, and flexible enough to deal with ambiguity. The system will
enhance existing classification information. Linking up with the semantic web
and computer intelligence, a Citizen Science effort (Good, 2013) would support
innovation by improving the quality of granted patents, reducing duplicitous
research, and stimulating problem-oriented solution design.
  A prototype of our design is in preparation. A crowd-sourced fuzzy and
faceted classification scheme will allow for better concept search and improved
access to prior art in science and technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7751</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7751</id><created>2014-06-30</created><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Interdonato</keyname><forenames>Roberto</forenames></author><author><keyname>Tagarelli</keyname><forenames>Andrea</forenames></author></authors><title>Online Popularity and Topical Interests through the Lens of Instagram</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>11 pages, 11 figures, Proceedings of ACM Hypertext 2014</comments><doi>10.1145/2631775.2631808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online socio-technical systems can be studied as proxy of the real world to
investigate human behavior and social interactions at scale. Here we focus on
Instagram, a media-sharing online platform whose popularity has been rising up
to gathering hundred millions users. Instagram exhibits a mixture of features
including social structure, social tagging and media sharing. The network of
social interactions among users models various dynamics including
follower/followee relations and users' communication by means of
posts/comments. Users can upload and tag media such as photos and pictures, and
they can &quot;like&quot; and comment each piece of information on the platform. In this
work we investigate three major aspects on our Instagram dataset: (i) the
structural characteristics of its network of heterogeneous interactions, to
unveil the emergence of self organization and topically-induced community
structure; (ii) the dynamics of content production and consumption, to
understand how global trends and popular users emerge; (iii) the behavior of
users labeling media with tags, to determine how they devote their attention
and to explore the variety of their topical interests. Our analysis provides
clues to understand human behavior dynamics on socio-technical systems,
specifically users and content popularity, the mechanisms of users'
interactions in online environments and how collective trends emerge from
individuals' topical interests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7753</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7753</id><created>2014-06-30</created><authors><author><keyname>Brise</keyname><forenames>Yves</forenames></author><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author><author><keyname>Eversmann</keyname><forenames>Dustin</forenames></author><author><keyname>Hoffmann</keyname><forenames>Michael</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author></authors><title>Interference Minimization in Asymmetric Sensor Networks</title><categories>cs.CG</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in wireless sensor networks is to connect a given set
of sensors while minimizing the \emph{receiver interference}. This is modeled
as follows: each sensor node corresponds to a point in $\mathbb{R}^d$ and each
\emph{transmission range} corresponds to a ball. The receiver interference of a
sensor node is defined as the number of transmission ranges it lies in. Our
goal is to choose transmission radii that minimize the maximum interference
while maintaining a strongly connected asymmetric communication graph.
  For the two-dimensional case, we show that it is NP-complete to decide
whether one can achieve a receiver interference of at most $5$. In the
one-dimensional case, we prove that there are optimal solutions with nontrivial
structural properties. These properties can be exploited to obtain an exact
algorithm that runs in quasi-polynomial time. This generalizes a result by Tan
et al. to the asymmetric case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7756</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7756</id><created>2014-06-30</created><updated>2016-01-02</updated><authors><author><keyname>Yajnanarayana</keyname><forenames>Vijaya</forenames></author><author><keyname>Magnusson</keyname><forenames>Klas E. G.</forenames></author><author><keyname>Brandt</keyname><forenames>Rasmus</forenames></author><author><keyname>Dwivedi</keyname><forenames>Satyam</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>Optimal Scheduling for Interference Mitigation by Range Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes several algorithms for generating an optimal schedule
for multiple access on a shared channel by utilizing range information in a
fully connected network. We also provide detailed analysis for the proposed
algorithms in terms of their complexity, convergence, and effect of
non-idealities in the network. The performance of the proposed schemes are
compared with non-aided methods to quantify the benefits of using the range
information in the communication. We argue that the proposed techniques yield
significant benefits as the number of nodes in the network increases. We
provide simulation results in support of the claim. The proposed methods
indicate that the throughput can be increased on average by 3-10 times for
typical network configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7758</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7758</id><created>2014-06-30</created><authors><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian
  Process Hyper-Parameters</title><categories>stat.ML cs.LG</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimisation has gained great popularity as a tool for optimising
the parameters of machine learning algorithms and models. Somewhat ironically,
setting up the hyper-parameters of Bayesian optimisation methods is notoriously
hard. While reasonable practical solutions have been advanced, they can often
fail to find the best optima. Surprisingly, there is little theoretical
analysis of this crucial problem in the literature. To address this, we derive
a cumulative regret bound for Bayesian optimisation with Gaussian processes and
unknown kernel hyper-parameters in the stochastic setting. The bound, which
applies to the expected improvement acquisition function and sub-Gaussian
observation noise, provides us with guidelines on how to design hyper-parameter
estimation methods. A simple simulation demonstrates the importance of
following these guidelines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7768</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7768</id><created>2014-06-30</created><authors><author><keyname>Berger</keyname><forenames>Christian</forenames></author></authors><title>From a Competition for Self-Driving Miniature Cars to a Standardized
  Experimental Platform: Concept, Models, Architecture, and Evaluation</title><categories>cs.RO cs.SE</categories><comments>17 pages, 19 figues, 2 tables</comments><acm-class>I.2.9; I.6.8; D.2.11; D.2.13</acm-class><journal-ref>Journal of Software Engineering for Robotics, vol. 5, no. 1, 2014,
  pp. 63-79</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Context: Competitions for self-driving cars facilitated the development and
research in the domain of autonomous vehicles towards potential solutions for
the future mobility.
  Objective: Miniature vehicles can bridge the gap between simulation-based
evaluations of algorithms relying on simplified models, and those
time-consuming vehicle tests on real-scale proving grounds.
  Method: This article combines findings from a systematic literature review,
an in-depth analysis of results and technical concepts from contestants in a
competition for self-driving miniature cars, and experiences of participating
in the 2013 competition for self-driving cars.
  Results: A simulation-based development platform for real-scale vehicles has
been adapted to support the development of a self-driving miniature car.
Furthermore, a standardized platform was designed and realized to enable
research and experiments in the context of future mobility solutions.
  Conclusion: A clear separation between algorithm conceptualization and
validation in a model-based simulation environment enabled efficient and
riskless experiments and validation. The design of a reusable, low-cost, and
energy-efficient hardware architecture utilizing a standardized
software/hardware interface enables experiments, which would otherwise require
resources like a large real-scale test track.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7769</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7769</id><created>2014-06-30</created><updated>2015-09-17</updated><authors><author><keyname>Banerjee</keyname><forenames>Anirban</forenames></author><author><keyname>Mehatari</keyname><forenames>Ranjit</forenames></author></authors><title>Characteristics polynomial of normalized Laplacian for trees</title><categories>math.CO cs.DM</categories><comments>11 pages, 1 table, 2 Figures</comments><msc-class>05C50, 05C05</msc-class><journal-ref>Appl. Math. Comput 271 (2015) 838-844</journal-ref><doi>10.1016/j.amc.2015.09.054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we find the characteristics polynomial of normalized Laplacian of a
tree. The coefficients of this polynomial are expressed by the higher order
general Randi\'c indices for matching, whose values depend on the structure of
the tree. We also find the expression of these indices for starlike tree and a
double-starlike tree, $H_m(p,q)$. Moreover, we show that two cospectral
$H_m(p,q)$ of the same diameter are isomorphic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7770</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7770</id><created>2014-06-30</created><authors><author><keyname>Duggins</keyname><forenames>Peter</forenames></author></authors><title>Modeling Attitude Change from Political Dialogues</title><categories>cs.MA cs.SI physics.soc-ph</categories><comments>28 pages, 13 figures, 2 appendices. Keywords: Agent-Based Model,
  Opinion Dynamics, Attitude Change, Social Influence, Homophily, Conformity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I use an agent-based simulation to model how individual and societal attitude
change arise from the mutual interactions that occur within political
dialogues. Agents converse with members of their social network, express their
political attitudes, and update their beliefs in a manner that reflects their
personal convictions, their tolerance for dissimilarity, and local social
norms. The model extends previous work by endowing agents with multiple,
theoretically motivated heuristics for reevaluating and expressing their
opinions, including homophily, attitude strength, and conformity. I demonstrate
that novel macroscopic phenomenon emerge from the interactions of these
cognitively and socially refined agents. Specifically, I find that (a)
interactions between persuadable and closed-minded agents drive non-monotonic
changes in the societal distribution of opinions; (b) extremists instigate
neighborhood polarization that propagates outward through undecided agents; (c)
preference falsification produces non-representative public norms that obscure
attitude diversity and consequently alter dynamics of attitude change. I
conclude by discussing the political implications of the results, suggesting an
experiment to validate the model's findings, and proposing extensions for
future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7799</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7799</id><created>2014-06-30</created><authors><author><keyname>Mohammadi</keyname><forenames>Pedram</forenames></author><author><keyname>Ebrahimi-Moghadam</keyname><forenames>Abbas</forenames></author><author><keyname>Shirani</keyname><forenames>Shahram</forenames></author></authors><title>Subjective and Objective Quality Assessment of Image: A Survey</title><categories>cs.MM cs.CV</categories><comments>50 pages, 12 figures, and 3 Tables. This work has been submitted to
  Elsevier Journal of Visual Communication and Image Representation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing demand for image-based applications, the efficient and
reliable evaluation of image quality has increased in importance. Measuring the
image quality is of fundamental importance for numerous image processing
applications, where the goal of image quality assessment (IQA) methods is to
automatically evaluate the quality of images in agreement with human quality
judgments. Numerous IQA methods have been proposed over the past years to
fulfill this goal. In this paper, a survey of the quality assessment methods
for conventional image signals, as well as the newly emerged ones, which
includes the high dynamic range (HDR) and 3-D images, is presented. A
comprehensive explanation of the subjective and objective IQA and their
classification is provided. Six widely used subjective quality datasets, and
performance measures are reviewed. Emphasis is given to the full-reference
image quality assessment (FR-IQA) methods, and 9 often-used quality measures
(including mean squared error (MSE), structural similarity index (SSIM),
multi-scale structural similarity index (MS-SSIM), visual information fidelity
(VIF), most apparent distortion (MAD), feature similarity measure (FSIM),
feature similarity measure for color images (FSIMC), dynamic range independent
measure (DRIM), and tone-mapped images quality index (TMQI)) are carefully
described, and their performance and computation time on four subjective
quality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA is
provided and the issues related to this area of research are reviewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7801</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7801</id><created>2014-06-30</created><authors><author><keyname>Bourhis</keyname><forenames>Pierre</forenames></author><author><keyname>Kr&#xf6;tzsch</keyname><forenames>Markus</forenames></author><author><keyname>Rudolph</keyname><forenames>Sebastian</forenames></author></authors><title>Query Containment for Highly Expressive Datalog Fragments</title><categories>cs.DB cs.CC cs.LO</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The containment problem of Datalog queries is well known to be undecidable.
There are, however, several Datalog fragments for which containment is known to
be decidable, most notably monadic Datalog and several &quot;regular&quot; query
languages on graphs. Monadically Defined Queries (MQs) have been introduced
recently as a joint generalization of these query languages. In this paper, we
study a wide range of Datalog fragments with decidable query containment and
determine exact complexity results for this problem. We generalize MQs to
(Frontier-)Guarded Queries (GQs), and show that the containment problem is
3ExpTime-complete in either case, even if we allow arbitrary Datalog in the
sub-query. If we focus on graph query languages, i.e., fragments of linear
Datalog, then this complexity is reduced to 2ExpSpace. We also consider nested
queries, which gain further expressivity by using predicates that are defined
by inner queries. We show that nesting leads to an exponentially increasing
hierarchy for the complexity of query containment, both in the linear and in
the general case. Our results settle open problems for (nested) MQs, and they
paint a comprehensive picture of the state of the art in Datalog query
containment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7806</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7806</id><created>2014-06-30</created><updated>2015-01-20</updated><authors><author><keyname>Maas</keyname><forenames>Andrew L.</forenames></author><author><keyname>Qi</keyname><forenames>Peng</forenames></author><author><keyname>Xie</keyname><forenames>Ziang</forenames></author><author><keyname>Hannun</keyname><forenames>Awni Y.</forenames></author><author><keyname>Lengerich</keyname><forenames>Christopher T.</forenames></author><author><keyname>Jurafsky</keyname><forenames>Daniel</forenames></author><author><keyname>Ng</keyname><forenames>Andrew Y.</forenames></author></authors><title>Building DNN Acoustic Models for Large Vocabulary Speech Recognition</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks (DNNs) are now a central component of nearly all
state-of-the-art speech recognition systems. Building neural network acoustic
models requires several design decisions including network architecture, size,
and training loss function. This paper offers an empirical investigation on
which aspects of DNN acoustic model design are most important for speech
recognition system performance. We report DNN classifier performance and final
speech recognizer word error rates, and compare DNNs using several metrics to
quantify factors influencing differences in task performance. Our first set of
experiments use the standard Switchboard benchmark corpus, which contains
approximately 300 hours of conversational telephone speech. We compare standard
DNNs to convolutional networks, and present the first experiments using
locally-connected, untied neural networks for acoustic modeling. We
additionally build systems on a corpus of 2,100 hours of training data by
combining the Switchboard and Fisher corpora. This larger corpus allows us to
more thoroughly examine performance of large DNN models -- with up to ten times
more parameters than those typically used in speech recognition systems. Our
results suggest that a relatively simple DNN architecture and optimization
technique produces strong results. These findings, along with previous work,
help establish a set of best practices for building DNN hybrid speech
recognition systems with maximum likelihood training. Our experiments in DNN
optimization additionally serve as a case study for training DNNs with
discriminative loss functions for speech tasks, as well as DNN classifiers more
generally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7807</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7807</id><created>2014-06-30</created><updated>2016-01-26</updated><authors><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Universal Compressed Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of developing universal algorithms for compressed
sensing of stochastic processes is studied. First, R\'enyi's notion of
information dimension (ID) is generalized to analog stationary processes. This
provides a measure of complexity for such processes and is connected to the
number of measurements required for their accurate recovery. Then a minimum
entropy pursuit (MEP) optimization approach is proposed, and it is proven that
it can reliably recover any stationary process satisfying some mixing
constraints from sufficient number of randomized linear measurements, without
having any prior information about the distribution of the process. It is
proved that a Lagrangian-type approximation of the MEP optimization problem,
referred to as Lagrangian-MEP problem, is identical to a heuristic
implementable algorithm proposed by Baron et al. It is shown that for the right
choice of parameters the Lagrangian-MEP algorithm, in addition to having the
same asymptotic performance as MEP optimization, is also robust to the
measurement noise. For memoryless sources with a discrete-continuous mixture
distribution, the fundamental limits of the minimum number of required
measurements by a non-universal compressed sensing decoder is characterized by
Wu et al. For such sources, it is proved that there is no loss in universal
coding, and both the MEP and the Lagrangian-MEP asymptotically achieve the
optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7811</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7811</id><created>2014-06-30</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Gonzalez</keyname><forenames>Mauricio</forenames></author></authors><title>An optimization algorithm for multimodal functions inspired by
  collective animal behavior</title><categories>cs.NE</categories><comments>18 Pages. arXiv admin note: text overlap with arXiv:1405.5164</comments><journal-ref>Soft Computing 17 (3) , (2013), pp. 489-502</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interest in multimodal function optimization is expanding rapidly since real
world optimization problems often demand locating multiple optima within a
search space. This article presents a new multimodal optimization algorithm
named as the Collective Animal Behavior (CAB). Animal groups, such as schools
of fish, flocks of birds, swarms of locusts and herds of wildebeest, exhibit a
variety of behaviors including swarming about a food source, milling around a
central location or migrating over large distances in aligned groups. These
collective behaviors are often advantageous to groups, allowing them to
increase their harvesting efficiency to follow better migration routes, to
improve their aerodynamic and to avoid predation. In the proposed algorithm,
searcher agents are a group of animals which interact to each other based on
the biological laws of collective motion. Experimental results demonstrate that
the proposed algorithm is capable of finding global and local optima of
benchmark multimodal optimization problems with a higher efficiency in
comparison to other methods reported in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7824</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7824</id><created>2014-06-30</created><authors><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author></authors><title>First-order definable string transformations</title><categories>cs.LO</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connection between languages defined by computational models and logic
for languages is well-studied. Monadic second-order logic and finite automata
are shown to closely correspond to each-other for the languages of strings,
trees, and partial-orders. Similar connections are shown for first-order logic
and finite automata with certain aperiodicity restriction. Courcelle in 1994
proposed a way to use logic to define functions over structures where the
output structure is defined using logical formulas interpreted over the input
structure. Engelfriet and Hoogeboom discovered the corresponding &quot;automata
connection&quot; by showing that two-way generalised sequential machines capture the
class of monadic-second order definable transformations. Alur and Cerny further
refined the result by proposing a one-way deterministic transducer model with
string variables---called the streaming string transducers---to capture the
same class of transformations. In this paper we establish a transducer-logic
correspondence for Courcelle's first-order definable string transformations. We
propose a new notion of transition monoid for streaming string transducers that
involves structural properties of both underlying input automata and variable
dependencies. By putting an aperiodicity restriction on the transition monoids,
we define a class of streaming string transducers that captures exactly the
class of first-order definable transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7831</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7831</id><created>2014-06-30</created><authors><author><keyname>Adiprasito</keyname><forenames>Karim A.</forenames></author><author><keyname>Padrol</keyname><forenames>Arnau</forenames></author><author><keyname>Theran</keyname><forenames>Louis</forenames></author></authors><title>Universality theorems for inscribed polytopes and Delaunay
  triangulations</title><categories>math.MG cs.CG math.AG math.CO</categories><comments>15 pages, 2 figures</comments><msc-class>52B40, 52B12, 14P10, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every primary basic semialgebraic set is homotopy equivalent to
the set of inscribed realizations (up to M\&quot;obius transformation) of a
polytope. If the semialgebraic set is moreover open, then, in addition, we
prove that (up to homotopy) it is a retract of the realization space of some
inscribed neighborly (and simplicial) polytope. We also show that all algebraic
extensions of $\mathbb{Q}$ are needed to coordinatize inscribed polytopes.
These statements show that inscribed polytopes exhibit the Mn\&quot;ev universality
phenomenon.
  Via stereographic projections, these theorems have a direct translation to
universality theorems for Delaunay subdivisions. In particular, our results
imply that the realizability problem for Delaunay triangulations is
polynomially equivalent to the existential theory of the reals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7838</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7838</id><created>2014-06-30</created><authors><author><keyname>Janota</keyname><forenames>Mikol&#xe1;&#x161;</forenames></author><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames></author></authors><title>On Minimal Corrections in ASP</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a programming paradigm, answer set programming (ASP) brings about the
usual issue of the human error. Hence, it is desirable to provide automated
techniques that could help the programmer to find the error. This paper
addresses the question of computing a subset-minimal correction of a
contradictory ASP program. A contradictory ASP program is often undesirable and
we wish to provide an automated way of fixing it. We consider a minimal
correction set of a contradictory program to be an irreducible set of rules
whose removal makes the program consistent. In contrast to propositional logic,
corrections of ASP programs behave non-monotonically. Nevertheless, we show
that a variety of algorithms for correction set computation in propositional
logic can be ported to ASP. An experimental evaluation was carried showing that
having a portfolio of such algorithms is indeed of benefit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7841</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7841</id><created>2014-06-30</created><authors><author><keyname>Olver</keyname><forenames>Neil</forenames></author></authors><title>A note on hierarchical hubbing for a generalization of the VPN problem</title><categories>cs.DS cs.NI</categories><comments>14 pages, 1 figure</comments><acm-class>F.2.2; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust network design refers to a class of optimization problems that occur
when designing networks to efficiently handle variable demands. The notion of
&quot;hierarchical hubbing&quot; was introduced (in the narrow context of a specific
robust network design question), by Olver and Shepherd [2010]. Hierarchical
hubbing allows for routings with a multiplicity of &quot;hubs&quot; which are connected
to the terminals and to each other in a treelike fashion. Recently, Fr\'echette
et al. [2013] explored this notion much more generally, focusing on its
applicability to an extension of the well-studied hose model that allows for
upper bounds on individual point-to-point demands. In this paper, we consider
hierarchical hubbing in the context of a previously studied (and extremely
natural) generalization of the hose model, and prove that the optimal
hierarchical hubbing solution can be found efficiently. This result is relevant
to a recently proposed generalization of the &quot;VPN Conjecture&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7842</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7842</id><created>2014-06-30</created><updated>2016-02-19</updated><authors><author><keyname>Dong</keyname><forenames>Xiaowen</forenames></author><author><keyname>Thanou</keyname><forenames>Dorina</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Learning Laplacian Matrix in Smooth Graph Signal Representations</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The construction of a meaningful graph plays a crucial role in the success of
many graph-based representations and algorithms for handling structured data,
especially in the emerging field of graph signal processing. However, a
meaningful graph is not always readily available from the data, nor easy to
define depending on the application domain. In particular, it is often
desirable in graph signal processing applications that a graph is chosen such
that the data admit certain regularity or smoothness on the graph. In this
paper, we address the problem of learning graph Laplacians, which is equivalent
to learning graph topologies, such that the input data form graph signals with
smooth variations on the resulting topology. To this end, we adopt a factor
analysis model for the graph signals and impose a Gaussian probabilistic prior
on the latent variables that control these signals. We show that the Gaussian
prior leads to an efficient representation that favors the smoothness property
of the graph signals. We then propose an algorithm for learning graphs that
enforces such property and is based on minimizing the variations of the signals
on the learned graph. Experiments on both synthetic and real world data
demonstrate that the proposed graph learning framework can efficiently infer
meaningful graph topologies from signal observations under the smoothness
prior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7855</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7855</id><created>2014-06-30</created><updated>2015-07-28</updated><authors><author><keyname>Heilman</keyname><forenames>Steven</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Oleszkiewicz</keyname><forenames>Krzysztof</forenames></author></authors><title>Strong Contraction and Influences in Tail Spaces</title><categories>math.PR cs.CC math.FA</categories><comments>20 pages, two new proofs added of the main theorem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study contraction under a Markov semi-group and influence bounds for
functions in $L^2$ tail spaces, i.e. functions all of whose low level Fourier
coefficients vanish. It is natural to expect that certain analytic inequalities
are stronger for such functions than for general functions in $L^2$. In the
positive direction we prove an $L^{p}$ Poincar\'{e} inequality and moment decay
estimates for mean $0$ functions and for all $1&lt;p&lt;\infty$, proving the degree
one case of a conjecture of Mendel and Naor as well as the general degree case
of the conjecture when restricted to Boolean functions. In the negative
direction, we answer negatively two questions of Hatami and Kalai concerning
extensions of the Kahn-Kalai-Linial and Harper Theorems to tail spaces. That
is, we construct a function $f\colon\{-1,1\}^{n}\to\{-1,1\}$ whose Fourier
coefficients vanish up to level $c \log n$, with all influences bounded by $C
\log n/n$ for some constants $0&lt;c,C&lt; \infty$. We also construct a function
$f\colon\{-1,1\}^{n}\to\{0,1\}$ with nonzero mean whose remaining Fourier
coefficients vanish up to level $c' \log n$, with the sum of the influences
bounded by $C'(\mathbb{E}f)\log(1/\mathbb{E}f)$ for some constants
$0&lt;c',C'&lt;\infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7858</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7858</id><created>2014-06-30</created><authors><author><keyname>Rebelatto</keyname><forenames>Jo&#xe3;o Luiz</forenames></author><author><keyname>Souza</keyname><forenames>Richard Demo</forenames></author><author><keyname>Kaido</keyname><forenames>Rodrigo Tsuneyoshi</forenames></author><author><keyname>Rayel</keyname><forenames>Ohara Kerusauskas</forenames></author><author><keyname>Uch&#xf4;a-Filho</keyname><forenames>Bartolomeu F.</forenames></author></authors><title>Secrecy Outage Probability of Network-Coded Cooperative Communication</title><categories>cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluate the secrecy performance of a multiple access cooperative network
where the destination node is wiretapped by a malicious and passive
eavesdropper. We propose the application of the network coding technique as an
alternative to increase the secrecy at the destination node, on the top of
improving the error performance of the legitimate communication, already
demonstrated in the literature. Network coding is leveraged by assuming that
the legitime cooperative nodes are able to perform non-binary linear
combinations of different frames before the transmission. Different scenarios
with and without channel state information (CSI) at the transmitter side are
evaluated. The effectiveness of the proposed schemes is evaluated in terms of
secrecy outage probability through theoretic and numerical analyses. It is
shown that, even when the legitimate transmitters do not have any CSI, the
secrecy can be increased through the use of network coding when compared to the
direct transmission and traditional cooperative techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7865</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7865</id><created>2014-06-30</created><updated>2014-11-18</updated><authors><author><keyname>Sutera</keyname><forenames>Antonio</forenames></author><author><keyname>Joly</keyname><forenames>Arnaud</forenames></author><author><keyname>Fran&#xe7;ois-Lavet</keyname><forenames>Vincent</forenames></author><author><keyname>Qiu</keyname><forenames>Zixiao Aaron</forenames></author><author><keyname>Louppe</keyname><forenames>Gilles</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author><author><keyname>Geurts</keyname><forenames>Pierre</forenames></author></authors><title>Simple connectome inference from partial correlation statistics in
  calcium imaging</title><categories>stat.ML cs.CE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a simple yet effective solution to the problem of
connectome inference in calcium imaging data. The proposed algorithm consists
of two steps. First, processing the raw signals to detect neural peak
activities. Second, inferring the degree of association between neurons from
partial correlation statistics. This paper summarises the methodology that led
us to win the Connectomics Challenge, proposes a simplified version of our
method, and finally compares our results with respect to other inference
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1406.7866</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1406.7866</id><created>2014-06-30</created><authors><author><keyname>Angeleski</keyname><forenames>Marjan</forenames></author><author><keyname>Mitrevski</keyname><forenames>Pece</forenames></author><author><keyname>Rocheska</keyname><forenames>Slavica</forenames></author><author><keyname>Lashkoska</keyname><forenames>Ane</forenames></author></authors><title>Regional Pilot Study to Evaluate e-Readiness and Local e-Government
  Services</title><categories>cs.CY</categories><comments>10 pages, 3 figures, International Journal of Managing Public Sector
  Information and Communication Technologies (IJMPICT), Vol. 5, No. 2, June
  2014</comments><acm-class>K.4.0</acm-class><doi>10.5121/ijmpict.2014.5201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of local e-Government has become a key factor for delivering
services in an efficient, cost effective, transparent and convenient way, in
circumstances where a) citizens do not have enough time available to
communicate with local authorities in order to perform their responsibilities
and needs, and b) information and communication technologies significantly
facilitate administrative procedures and citizens-government interaction. This
paper aims to identify e-services that local authorities provide, and to
investigate their readiness for delivering these services. A pilot research has
been conducted to identify the offer of e-services by local authorities, along
with e-readiness in municipalities of the Pelagonia region in the Republic of
Macedonia. The survey was carried out by means of structured interview
questions based on a modified model proposed by Partnership on Measuring ICT
for Development - web analysis of municipal websites in the region has been
conducted, as well. The study reveals uneven distribution according to the age
group of users, lack of reliability and confidence for processing the needs and
requests electronically by a large part of the population, and improperly
developed set of ICT tools by local governments for providing a variety of
services that can be fully processed electronically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0003</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0003</id><created>2014-06-28</created><authors><author><keyname>Latha</keyname><forenames>R.</forenames></author><author><keyname>Kanthalakshmi</keyname><forenames>S.</forenames></author><author><keyname>Kanagaraj</keyname><forenames>J.</forenames></author></authors><title>Design of Power System Stabilizer using Fuzzy Based Sliding Mode Control
  Technique</title><categories>cs.SY</categories><comments>16 pages,14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power systems are usually large non-linear systems, which are often subjected
to low frequency electromechanical oscillations. Power System Stabilizers are
often used as effective and economic means for damping the generator's
electromechanical oscillations and enhance the overall stability of power
systems. Power system stabilizers have been applied for several decades in
utilities and they can extend power transfer stability limits by adding
modulation signal through excitation control system. Sliding mode control is
one of the main methods employed to overcome the uncertainty of the system.
This controller can be applied very well in presence of both parameter
uncertainties and unknown nonlinear function such as disturbance. To enhance
stability and improve dynamic response of the system operating in faulty
conditions a Fuzzy based Sliding Mode Control Power System Stabiliser is
developed for a multimachine system with two generators and it is compared with
the Conventional Power System Stabiliser, Fuzzy based Power System Stabiliser,
Sliding Mode based Power System Stabiliser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0004</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0004</id><created>2014-06-29</created><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Multicast Multigroup Beamforming under Per-antenna Power Constraints</title><categories>cs.IT math.IT</categories><comments>Presented in IEEE ICC 2014, Sydney, AUS. arXiv admin note:
  substantial text overlap with arXiv:1406.7557</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear precoding exploits the spatial degrees of freedom offered by
multi-antenna transmitters to serve multiple users over the same frequency
resources. The present work focuses on simultaneously serving multiple groups
of users, each with its own channel, by transmitting a stream of common symbols
to each group. This scenario is known as physical layer multicasting to
multiple co-channel groups. Extending the current state of the art in
multigroup multicasting, the practical constraint of a maximum permitted power
level radiated by each antenna is tackled herein. The considered per antenna
power constrained system is optimized in a maximum fairness sense. In other
words, the optimization aims at favoring the worst user by maximizing the
minimum rate. This Max-Min Fair criterion is imperative in multicast systems,
where the performance of all the receivers listening to the same multicast is
dictated by the worst rate in the group. An analytic framework to tackle the
Max-Min Fair multigroup multicasting scenario under per antenna power
constraints is therefore derived. Numerical results display the accuracy of the
proposed solution and provide insights to the performance of a per antenna
power constrained system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0005</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0005</id><created>2014-06-29</created><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Sum Rate Maximizing Multigroup Multicast Beamforming under Per-antenna
  Power Constraints</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE GlobeCom 2014, Austin, TX. arXiv admin note:
  substantial text overlap with arXiv:1406.7699, arXiv:1406.7557</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multi-antenna transmitter that conveys independent sets of common data to
distinct groups of users is herein considered, a model known as physical layer
multicasting to multiple co-channel groups. In the recently proposed context of
per-antenna power constrained multigroup multicasting, the present work focuses
on a novel system design that aims at maximizing the total achievable
throughput. Towards increasing the system sum rate, the available power
resources need to be allocated to well conditioned groups of users. A detailed
solution to tackle the elaborate sum rate maximization multigroup multicast
problem under per-antenna power constraints is therefore derived. Numerical
results are presented to quantify the gains of the proposed algorithm over
heuristic solutions. Besides Rayleigh faded channels, the solution is also
applied to uniform linear array transmitters operating in the far field, where
line-ofsight conditions are realized. In this setting, a sensitivity analysis
with respect to the angular separation of co-group users is included. Finally,
a simple scenario providing important intuitions for the sum rate maximizing
multigroup multicast solutions is elaborated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0006</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0006</id><created>2014-06-29</created><updated>2014-07-17</updated><authors><author><keyname>Ding</keyname><forenames>Weijun</forenames></author><author><keyname>Xu</keyname><forenames>Jim</forenames></author><author><keyname>Dai</keyname><forenames>Jim</forenames></author><author><keyname>Song</keyname><forenames>Yang</forenames></author><author><keyname>Lin</keyname><forenames>Bill</forenames></author></authors><title>Sprinklers: A Randomized Variable-Size Striping Approach to
  Reordering-Free Load-Balanced Switching</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet traffic continues to grow exponentially, calling for switches that
can scale well in both size and speed. While load-balanced switches can achieve
such scalability, they suffer from a fundamental packet reordering problem.
Existing proposals either suffer from poor worst-case packet delays or require
sophisticated matching mechanisms. In this paper, we propose a new family of
stable load-balanced switches called &quot;Sprinklers&quot; that has comparable
implementation cost and performance as the baseline load-balanced switch, but
yet can guarantee packet ordering. The main idea is to force all packets within
the same virtual output queue (VOQ) to traverse the same &quot;fat path&quot; through the
switch, so that packet reordering cannot occur. At the core of Sprinklers are
two key innovations: a randomized way to determine the &quot;fat path&quot; for each VOQ,
and a way to determine its &quot;fatness&quot; roughly in proportion to the rate of the
VOQ. These innovations enable Sprinklers to achieve near-perfect load-balancing
under arbitrary admissible traffic. Proving this property rigorously using
novel worst-case large deviation techniques is another key contribution of this
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0007</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0007</id><created>2014-06-29</created><authors><author><keyname>Sun</keyname><forenames>Yu</forenames></author><author><keyname>Rossi</keyname><forenames>Louis F.</forenames></author><author><keyname>Shen</keyname><forenames>Chien-Chung</forenames></author><author><keyname>Miller</keyname><forenames>Jennifer</forenames></author><author><keyname>Wang</keyname><forenames>X. Rosalind</forenames></author><author><keyname>Lizier</keyname><forenames>Joseph T.</forenames></author><author><keyname>Prokopenko</keyname><forenames>Mikhail</forenames></author><author><keyname>Senanayake</keyname><forenames>Upul</forenames></author></authors><title>Information Transfer in Swarms with Leaders</title><categories>cs.NE</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/141</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm dynamics is the study of collections of agents that interact with one
another without central control. In natural systems, insects, birds, fish and
other large mammals function in larger units to increase the overall fitness of
the individuals. Their behavior is coordinated through local interactions to
enhance mate selection, predator detection, migratory route identification and
so forth [Andersson and Wallander 2003; Buhl et al. 2006; Nagy et al. 2010;
Partridge 1982; Sumpter et al. 2008]. In artificial systems, swarms of
autonomous agents can augment human activities such as search and rescue, and
environmental monitoring by covering large areas with multiple nodes [Alami et
al. 2007; Caruso et al. 2008; Ogren et al. 2004; Paley et al. 2007; Sibley et
al. 2002]. In this paper, we explore the interplay between swarm dynamics,
covert leadership and theoretical information transfer. A leader is a member of
the swarm that acts upon information in addition to what is provided by local
interactions. Depending upon the leadership model, leaders can use their
external information either all the time or in response to local conditions
[Couzin et al. 2005; Sun et al. 2013]. A covert leader is a leader that is
treated no differently than others in the swarm, so leaders and followers
participate equally in whatever interaction model is used [Rossi et al. 2007].
In this study, we use theoretical information transfer as a means of analyzing
swarm interactions to explore whether or not it is possible to distinguish
between followers and leaders based on interactions within the swarm. We find
that covert leaders can be distinguished from followers in a swarm because they
receive less transfer entropy than followers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0008</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0008</id><created>2014-06-29</created><authors><author><keyname>Gao</keyname><forenames>Grace</forenames></author></authors><title>Navigating Robot Swarms Using Collective Intelligence Learned from
  Golden Shiner Fish</title><categories>cs.NE</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/129</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigating networked robot swarms often requires knowing where to go, sensing
the environment, and path-planning based on the destination and barriers in the
environment. Such a process is computationally intensive. Moreover, as the
network scales up, the computational load increases quadratically, or even
exponentially. Unlike these man-made systems, most biological systems scale
linearly in complexity. Furthermore, the scale of a biological swarm can even
enable collective intelligence. One example comes from observations of golden
shiner fish. Golden shiners naturally prefer darkness and school together. Each
individual golden shiner does not know where the darkness is. Neither does it
sense the light gradients in the environment. However, by moving together as a
school, they always end up in the shady area. We apply such collective
intelligence learned from golden shiner fish to navigating robot swarms. Each
individual robot's dynamic is based on the gold shiners' movement strategy---a
random walk with its speed modulated by the light intensity and its direction
affected by its neighbors. The theoretical analysis and simulation results show
that our method 1) promises to navigate a robot swarm with little situational
knowledge, 2) simplifies control and decision-making for each individual robot,
3) requires minimal or even no information exchange within the swarm, and 4) is
highly distributed, adaptive, and robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0009</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0009</id><created>2014-06-30</created><authors><author><keyname>Sumalatha</keyname><forenames>G.</forenames></author><author><keyname>Zareena</keyname><forenames>N.</forenames></author><author><keyname>Raju</keyname><forenames>Ch. Gopi</forenames></author></authors><title>A Review on Failure Node Recovery Algorithms in Wireless Sensor Actor
  Networks</title><categories>cs.NI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless sensor-actor networks, sensors probe their surroundings and
forward their data to actor nodes. Actors collect sensor data and perform
certain tasks in response to various events. Since actors operate on harsh
environment, they may easily get damaged or failed. Failed actor nodes may
partition the network into disjoint subsets. In order to reestablish
connectivity nodes may be relocated to new positions. This paper focus on
review of three (LeDir, RIM, DARA) node recovery algorithms, and their
performance has been analysed in terms network overhead and path length
validation metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0010</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0010</id><created>2014-06-30</created><updated>2015-01-28</updated><authors><author><keyname>Qu</keyname><forenames>Liangqiong</forenames></author><author><keyname>Tian</keyname><forenames>Jiandong</forenames></author><author><keyname>Han</keyname><forenames>Zhi</forenames></author><author><keyname>Tang</keyname><forenames>Yandong</forenames></author></authors><title>Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and
  Shadow-free Image</title><categories>cs.CV</categories><comments>This paper has been published in Optics Express, Vol. 23, Issue 3,
  pp. 2220-2239. The final version is available on
  http://dx.doi.org/10.1364/OE.23.002220. Please refer to that version when
  citing this paper</comments><journal-ref>Optics Express, Vol. 23, Issue 3, pp. 2220-2239 (2015)</journal-ref><doi>10.1364/OE.23.002220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel, effective and fast method to obtain a
color illumination invariant and shadow-free image from a single outdoor image.
Different from state-of-the-art methods for shadow-free image that either need
shadow detection or statistical learning, we set up a linear equation set for
each pixel value vector based on physically-based shadow invariants, deduce a
pixel-wise orthogonal decomposition for its solutions, and then get an
illumination invariant vector for each pixel value vector on an image. The
illumination invariant vector is the unique particular solution of the linear
equation set, which is orthogonal to its free solutions. With this illumination
invariant vector and Lab color space, we propose an algorithm to generate a
shadow-free image which well preserves the texture and color information of the
original image. A series of experiments on a diverse set of outdoor images and
the comparisons with the state-of-the-art methods validate our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0013</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0013</id><created>2014-06-30</created><authors><author><keyname>Sundin</keyname><forenames>Martin</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author></authors><title>Relevance Singular Vector Machine for low-rank matrix sensing</title><categories>cs.NA cs.LG math.ST stat.TH</categories><comments>International Conference on Signal Processing and Communications
  (SPCOM), 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a new Bayesian inference method for low rank matrix
reconstruction. We call the new method the Relevance Singular Vector Machine
(RSVM) where appropriate priors are defined on the singular vectors of the
underlying matrix to promote low rank. To accelerate computations, a
numerically efficient approximation is developed. The proposed algorithms are
applied to matrix completion and matrix reconstruction problems and their
performance is studied numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0014</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0014</id><created>2014-06-30</created><authors><author><keyname>Jeong</keyname><forenames>Donghwa</forenames></author><author><keyname>Lee</keyname><forenames>Kiju</forenames></author></authors><title>Dispersion and Line Formation in Artificial Swarm Intelligence</title><categories>cs.NE</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/103</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major motifs in collective or swarm intelligence is that, even
though individuals follow simple rules, the resulting global behavior can be
complex and intelligent. In artificial swarm systems, such as swarm robots, the
goal is to use systems that are as simple and cheap as possible, deploy many of
them, and coordinate them to conduct complex tasks that each individual cannot
accomplish. Shape formation in artificial intelligence systems is usually
required for specific task-oriented performance, including 1) forming sensing
grids, 2) exploring and mapping in space, underwater, or hazardous
environments, and 3) forming a barricade for surveillance or protecting an area
or a person. This paper presents a dynamic model of an artificial swarm system
based on a virtual spring damper model and algorithms for dispersion without a
leader and line formation with an interim leader using only the distance
estimation among the neighbors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0015</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0015</id><created>2014-06-30</created><authors><author><keyname>Bogdanovi&#x107;</keyname><forenames>Nikola</forenames></author><author><keyname>Plata-Chaves</keyname><forenames>Jorge</forenames></author><author><keyname>Berberidis</keyname><forenames>Kostas</forenames></author></authors><title>Distributed Diffusion-based LMS for Node-Specific Parameter Estimation
  over Adaptive Networks</title><categories>cs.SY cs.MA</categories><comments>5 pages, 2 figures, Published in Proc. IEEE ICASSP, Florence, Italy,
  May 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed adaptive algorithm is proposed to solve a node-specific
parameter estimation problem where nodes are interested in estimating
parameters of local interest and parameters of global interest to the whole
network. To address the different node-specific parameter estimation problems,
this novel algorithm relies on a diffusion-based implementation of different
Least Mean Squares (LMS) algorithms, each associated with the estimation of a
specific set of local or global parameters. Although all the different LMS
algorithms are coupled, the diffusion-based implementation of each LMS
algorithm is exclusively undertaken by the nodes of the network interested in a
specific set of local or global parameters. To illustrate the effectiveness of
the proposed technique we provide simulation results in the context of
cooperative spectrum sensing in cognitive radio networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0039</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0039</id><created>2014-06-26</created><authors><author><keyname>Gnang</keyname><forenames>Edinah K.</forenames></author></authors><title>Integer formula encoding SageTeX package</title><categories>cs.MS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes a SageTeX implementation of an integer encoding
procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0049</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0049</id><created>2014-06-30</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author></authors><title>Low-cost commercial LEGO platform for mobile robotics</title><categories>cs.RO</categories><comments>10 Pages</comments><journal-ref>International Journal of Electrical Engineering Education 47 (2),
  (2010), pp. 132-150</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows the potential of a Lego\c{opyright} based low-cost
commercial robotic platform for learning and testing prototypes in higher
education and research. The overall setup aims to explain mobile robotic issues
strongly related to several fields such as Mechatronics, Robotics, and
Automatic Control theory. The capabilities and limitations of LEGO robots are
studied within two projects. The first one involves a robotic vehicle which is
able to follow several predefined paths. The second project concerns to the
classical problem of position control. Algorithms and additional tools have
been fully designed, applied and documented with results shown throughout the
paper. The platform is found to be suitable for teaching and researching on key
issues related to the aforementioned fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0051</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0051</id><created>2014-06-30</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez-</keyname><forenames>Marco</forenames></author><author><keyname>Ramirez</keyname><forenames>Marte</forenames></author></authors><title>Hands-on experiments on intelligent behavior for mobile robots</title><categories>cs.RO</categories><comments>11 Pages</comments><journal-ref>International Journal of Electrical Engineering Education 48 (1),
  (2011), pp. 66-78</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, Artificial Intelligence techniques have emerged as useful
tools for solving various engineering problems that were not possible or
convenient to handle by traditional methods. AI has directly influenced many
areas of computer science and becomes an important part of the engineering
curriculum. However, determining the important topics for a single semester AI
course is a nontrivial task, given the lack of a general methodology. AI
concepts commonly overlap with many other disciplines involving a wide range of
subjects, including applied approaches to more formal mathematical issues. This
paper presents the use of a simple robotic platform to assist the learning of
basic AI concepts. The study is guided through some simple experiments using
autonomous mobile robots. The central algorithm is the Learning Automata. Using
LA, each robot action is applied to an environment to be evaluated by means of
a fitness value. The response of the environment is used by the automata to
select its next action. This procedure holds until the goal task is reached.
The proposal addresses the AI study by offering in LA a unifying context to
draw together several of the topics of AI and motivating the students to learn
by building some hands on laboratory exercises. The presented material has been
successfully tested as AI teaching aide in the University of Guadalajara
robotics group as it motivates students and increases enrolment and retention
while educating better computer engineers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0061</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0061</id><created>2014-06-30</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez</keyname><forenames>Marco</forenames></author><author><keyname>Sossa</keyname><forenames>Humberto</forenames></author><author><keyname>Osuna</keyname><forenames>Valentin</forenames></author></authors><title>Block matching algorithm for motion estimation based on Artificial Bee
  Colony (ABC)</title><categories>cs.NE</categories><comments>22 Pages. arXiv admin note: substantial text overlap with
  arXiv:1405.4721, arXiv:1406.4484</comments><journal-ref>Applied Soft Computing Journal 13 (6), (2013), pp. 3047-3059</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Block matching (BM) motion estimation plays a very important role in video
coding. In a BM approach, image frames in a video sequence are divided into
blocks. For each block in the current frame, the best matching block is
identified inside a region of the previous frame, aiming to minimize the sum of
absolute differences (SAD). Unfortunately, the SAD evaluation is
computationally expensive and represents the most consuming operation in the BM
process. Therefore, BM motion estimation can be approached as an optimization
problem, where the goal is to find the best matching block within a search
space. The simplest available BM method is the full search algorithm (FSA)
which finds the most accurate motion vector through an exhaustive computation
of SAD values for all elements of the search window. Recently, several fast BM
algorithms have been proposed to reduce the number of SAD operations by
calculating only a fixed subset of search locations at the price of poor
accuracy. In this paper, a new algorithm based on Artificial Bee Colony (ABC)
optimization is proposed to reduce the number of search locations in the BM
process. In our algorithm, the computation of search locations is drastically
reduced by considering a fitness calculation strategy which indicates when it
is feasible to calculate or only estimate new search locations. Since the
proposed algorithm does not consider any fixed search pattern or any other
movement assumption as most of other BM approaches do, a high probability for
finding the true minimum (accurate motion vector) is expected. Conducted
simulations show that the proposed method achieves the best balance over other
fast BM algorithms, in terms of both estimation accuracy and computational
cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0063</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0063</id><created>2014-06-30</created><updated>2014-07-02</updated><authors><author><keyname>Albeladi</keyname><forenames>Abdulrhman</forenames></author><author><keyname>Abdalkareem</keyname><forenames>Rabe</forenames></author><author><keyname>Agwaeten</keyname><forenames>Farhat</forenames></author><author><keyname>Altoum</keyname><forenames>Khalid</forenames></author><author><keyname>Bennis</keyname><forenames>Youssef</forenames></author><author><keyname>Nasereldine</keyname><forenames>Zakaria</forenames></author></authors><title>Toward Software Measurement and Quality Analysis of MARF and GIPSY Case
  Studies a Team 13 SOEN6611-S14 Project Report</title><categories>cs.SE</categories><comments>34 pages</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is no longer a debate that quality is an essential requirement in any
software product, especially in a highly competitive market and a context of
mission critical product. To obtain better product quality, software metrics
are the only reliable indicators provided to assess and measure this attribute
of a software product. Several metrics have been elaborated but none of them
were really convenient in an object oriented ecosystem. However, the MOOD
metrics have proven their efficiency in gauging the software quality at system
level, while CK Metrics measure the quality of software at class level . These
metrics, well suited for Object-Oriented design, allow measuring object
oriented design properties such as coupling, cohesion, encapsulation,
Inheritance and polymorphism. The goal of the present study is using the
mentioned metrics to assess the quality of two different case studies, MARF and
GIPSY. For this purpose, different tools such as McCabe, Logiscope and,
JDeodorant have been used to measure the quality of these projects by
implementing in different manners the metrics composing the CK and MOOD suite
metrics, whilst MARFCAT has been used to detect vulnerable code files in both
case studies. The present study puts the light on the strengths of these tools
to measure the quality of proven and largely researched software products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0067</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0067</id><created>2014-06-30</created><updated>2014-07-01</updated><authors><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author></authors><title>Rates of Convergence for Nearest Neighbor Classification</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nearest neighbor methods are a popular class of nonparametric estimators with
several desirable properties, such as adaptivity to different distance scales
in different regions of space. Prior work on convergence rates for nearest
neighbor classification has not fully reflected these subtle properties. We
analyze the behavior of these estimators in metric spaces and provide
finite-sample, distribution-dependent rates of convergence under minimal
assumptions. As a by-product, we are able to establish the universal
consistency of nearest neighbor in a broader range of data spaces than was
previously known. We illustrate our upper and lower bounds by introducing
smoothness classes that are customized for nearest neighbor classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0070</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0070</id><created>2014-06-30</created><authors><author><keyname>Schaeffer</keyname><forenames>Ben</forenames></author><author><keyname>Perkowski</keyname><forenames>Marek</forenames></author></authors><title>A Cost Minimization Approach to Synthesis of Linear Reversible Circuits</title><categories>cs.ET</categories><comments>16 pages, PDF</comments><acm-class>B.6.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a heuristic cost minimization approach to synthesizing
linear reversible circuits. Two bidirectional linear reversible circuit
synthesis methods are introduced, the Alternating Elimination with Cost
Minimization method (AECM) and the Multiple CNOT Gate method (MCG). Algorithms,
example syntheses, and extensions to these methods are presented. An MCG
variant which incorporates line reordering is introduced. Tests comparing the
new cost minimization methods with the best known method for large circuits are
presented. Results show that of the three methods MCG had the lowest average
CNOT gate counts for linear reversible circuits up to 24 lines, and that AECM
had the lowest counts between 28 and 60 lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0080</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0080</id><created>2014-06-30</created><authors><author><keyname>Wilson</keyname><forenames>Graeme N.</forenames></author><author><keyname>Ramirez-Serrano</keyname><forenames>Alejandro</forenames></author><author><keyname>Mustafa</keyname><forenames>Mahmoud</forenames></author><author><keyname>Davies</keyname><forenames>Krispin A.</forenames></author></authors><title>Velocity Selection for High-Speed UGVs in Rough Unknown Terrains using
  Force Prediction</title><categories>cs.RO</categories><comments>10 pages, 6 figures, Proceedings of 5th International Conference on
  Intelligent Robotics and Applications, Concordia University, October 3-5,
  2012, Montreal, Canada</comments><journal-ref>5th International Conference, ICIRA 2012, Montreal, Canada,
  October 3-5, 2012, Proceedings, Part II. 7507: 387-396</journal-ref><doi>10.1007/978-3-642-33515-0_39</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enabling high speed navigation of Unmanned Ground Vehicles (UGVs) in unknown
rough terrain where limited or no information is available in advance requires
the assessment of terrain in front of the UGV. Attempts have been made to
predict the forces the terrain exerts on the UGV for the purpose of determining
the maximum allowable velocity for a given terrain. However, current methods
produce overly aggressive velocity profiles which could damage the UGV. This
paper presents three novel safer methods of force prediction that produce
effective velocity profiles. Two models, Instantaneous Elevation Change Model
(IECM) and Sinusoidal Base Excitation Model: using Excitation Force (SBEM:EF),
predict the forces exerted by the terrain on the vehicle at the ground contact
point, while another method, Sinusoidal Base Excitation Model: using
Transmitted Force (SBEM:TF), predicts the forces transmitted to the vehicle
frame by the suspension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0085</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0085</id><created>2014-06-30</created><updated>2014-09-08</updated><authors><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author></authors><title>Improved Quantum Algorithm for Triangle Finding via Combinatorial
  Arguments</title><categories>quant-ph cs.CC cs.DS</categories><comments>17 pages, to appear in FOCS'14; v2: minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a quantum algorithm solving the triangle finding
problem in unweighted graphs with query complexity $\tilde O(n^{5/4})$, where
$n$ denotes the number of vertices in the graph. This improves the previous
upper bound $O(n^{9/7})=O(n^{1.285...})$ recently obtained by Lee, Magniez and
Santha. Our result shows, for the first time, that in the quantum query
complexity setting unweighted triangle finding is easier than its edge-weighted
version, since for finding an edge-weighted triangle Belovs and Rosmanis proved
that any quantum algorithm requires $\Omega(n^{9/7}/\sqrt{\log n})$ queries.
Our result also illustrates some limitations of the non-adaptive learning graph
approach used to obtain the previous $O(n^{9/7})$ upper bound since, even over
unweighted graphs, any quantum algorithm for triangle finding obtained using
this approach requires $\Omega(n^{9/7}/\sqrt{\log n})$ queries as well. To
bypass the obstacles characterized by these lower bounds, our quantum algorithm
uses combinatorial ideas exploiting the graph-theoretic properties of triangle
finding, which cannot be used when considering edge-weighted graphs or the
non-adaptive learning graph approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0088</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0088</id><created>2014-06-30</created><authors><author><keyname>Nguyen</keyname><forenames>Nam</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Woolf</keyname><forenames>Tina</forenames></author></authors><title>Linear Convergence of Stochastic Iterative Greedy Algorithms with Sparse
  Constraints</title><categories>math.NA cs.IT math.IT math.OC</categories><msc-class>41A46, 68Q25, 68W20, 90C27, 65B99, 52A99, 60G99, 62L20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by recent work on stochastic gradient descent methods, we develop
two stochastic variants of greedy algorithms for possibly non-convex
optimization problems with sparsity constraints. We prove linear convergence in
expectation to the solution within a specified tolerance. This generalized
framework applies to problems such as sparse signal recovery in compressed
sensing, low-rank matrix recovery, and covariance matrix estimation, giving
methods with provable convergence guarantees that often outperform their
deterministic counterparts. We also analyze the settings where gradients and
projections can only be computed approximately, and prove the methods are
robust to these approximations. We include many numerical experiments which
align with the theoretical analysis and demonstrate these improvements in
several different settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0091</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0091</id><created>2014-06-30</created><authors><author><keyname>Almajadub</keyname><forenames>Fatma</forenames></author><author><keyname>Elleithy</keyname><forenames>Khaled</forenames></author></authors><title>Performance Advancement of Wireless Sensor Networks using Low Power
  Techniques and Efficient Placement of Nodes</title><categories>cs.NI</categories><comments>pages 9, Figures 13,International Journal of Engineering Research and
  Development April,2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present optimization techniques for WSNs. Our main goal is
to minimize the power consumption and latency. We address the problem of
minimizing the energy consumption in WSNs including hardware. ZigBee protocol
is used to design nodes on WSN to achieve a very low power consumption rate.
Furthermore, we propose to use IRS protocol in WSN within a ZigBee technique to
discover information from unaware locations and achieve efficiency of energy
and sacrifices latency. Our main idea is to support WSNs with both ZigBee
technique and IRS protocol. In addition, we address the problem of efficient
node placement for congestion control in WSNs. Thus, we evaluate the
performance of specific routing and some algorithms of congestion control when
wireless sensor nodes are deployed under different placements of network. To
demonstrate the strength of the used algorithms, our simulation in C# proves
that ZigBee-IRS- ESRT-Flooding approaches reduce the power consumption from 10%
to 19% when compared to existing techniques of low Power and node placement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0097</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0097</id><created>2014-07-01</created><authors><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A betweenness structure entropy of complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>18 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure entropy is an important index to illuminate the structure
property of the complex network. Most of the existing structure entropies are
based on the degree distribution of the complex network. But the structure
entropy based on the degree can not illustrate the structure property of the
weighted networks. In order to study the structure property of the weighted
networks, a new structure entropy of the complex networks based on the
betweenness is proposed in this paper. Comparing with the existing structure
entropy, the proposed method is more reasonable to describe the structure
property of the complex weighted networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0107</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0107</id><created>2014-07-01</created><updated>2014-07-26</updated><authors><author><keyname>Wang</keyname><forenames>Huahua</forenames></author><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author></authors><title>Randomized Block Coordinate Descent for Online and Stochastic
  Optimization</title><categories>cs.LG</categories><comments>The errors in the proof of ORBCD with variance reduction have been
  corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two types of low cost-per-iteration gradient descent methods have been
extensively studied in parallel. One is online or stochastic gradient descent
(OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this
paper, we combine the two types of methods together and propose online
randomized block coordinate descent (ORBCD). At each iteration, ORBCD only
computes the partial gradient of one block coordinate of one mini-batch
samples. ORBCD is well suited for the composite minimization problem where one
function is the average of the losses of a large number of samples and the
other is a simple regularizer defined on high dimensional variables. We show
that the iteration complexity of ORBCD has the same order as OGD or SGD. For
strongly convex functions, by reducing the variance of stochastic gradients, we
show that ORBCD can converge at a geometric rate in expectation, matching the
convergence rate of SGD with variance reduction and RBCD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0114</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0114</id><created>2014-07-01</created><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author></authors><title>Suffix Arrays for Spaced-SNP Databases</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single-nucleotide polymorphisms (SNPs) account for most variations between
human genomes. We show how, if the genomes in a database differ only by a
reasonable number of SNPs and the substrings between those SNPs are unique,
then we can store a fast compressed suffix array for that database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0116</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0116</id><created>2014-07-01</created><authors><author><keyname>Naldi</keyname><forenames>Maurizio</forenames></author><author><keyname>D'Acquisto</keyname><forenames>Giuseppe</forenames></author></authors><title>Differential privacy for counting queries: can Bayes estimation help
  uncover the true value?</title><categories>cs.DB cs.CR</categories><acm-class>H.2.8; H.2.4; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is achieved by the introduction of Laplacian noise in
the response to a query, establishing a precise trade-off between the level of
differential privacy and the accuracy of the database response (via the amount
of noise introduced). Multiple queries may improve the accuracy but erode the
privacy budget. We examine the case where we submit just a single counting
query. We show that even in that case a Bayesian approach may be used to
improve the accuracy for the same amount of noise injected, if we know the size
of the database and the probability of a positive response to the query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0118</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0118</id><created>2014-07-01</created><authors><author><keyname>Silva</keyname><forenames>Eduardo~I.</forenames></author><author><keyname>Derpich</keyname><forenames>Milan S.</forenames></author><author><keyname>Ostergaard</keyname><forenames>Jan</forenames></author><author><keyname>Encina</keyname><forenames>Marco A.</forenames></author></authors><title>A Characterization of the Minimal Average Data Rate that Guarantees a
  Given Closed-Loop Performance Level</title><categories>cs.SY cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Automatic Control on December 26,
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies networked control systems closed over noiseless digital
channels. By focusing on noisy LTI plants with scalar-valued control inputs and
sensor outputs, we derive an absolute lower bound on the minimal average data
rate that allows one to achieve a prescribed level of stationary performance
under Gaussianity assumptions. We also present a simple coding scheme that
allows one to achieve average data rates that are at most 1.254 bits away from
the derived lower bound, while satisfying the performance constraint. Our
results are given in terms of the solution to a stationary signal-to-noise
ratio minimization problem and builds upon a recently proposed framework to
deal with average data rate constraints in feedback systems. A numerical
example is presented to illustrate our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0120</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0120</id><created>2014-07-01</created><updated>2014-07-02</updated><authors><author><keyname>Schall</keyname><forenames>Daniel</forenames></author><author><keyname>H&#xe4;rder</keyname><forenames>Theo</forenames></author></authors><title>Dynamic Physiological Partitioning on a Shared-nothing Database Cluster</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional DBMS servers are usually over-provisioned for most of their daily
workloads and, because they do not show good-enough energy proportionality,
waste a lot of energy while underutilized. A cluster of small (wimpy) servers,
where its size can be dynamically adjusted to the current workload, offers
better energy characteristics for these workloads. Yet, data migration,
necessary to balance utilization among the nodes, is a non-trivial and
time-consuming task that may consume the energy saved. For this reason, a
sophisticated and easy to adjust partitioning scheme fostering dynamic
reorganization is needed. In this paper, we adapt a technique originally
created for SMP systems, called physiological partitioning, to distribute data
among nodes, that allows to easily repartition data without interrupting
transactions. We dynamically partition DB tables based on the nodes'
utilization and given energy constraints and compare our approach with physical
partitioning and logical partitioning methods. To quantify possible energy
saving and its conceivable drawback on query runtimes, we evaluate our
implementation on an experimental cluster and compare the results w.r.t.
performance and energy consumption. Depending on the workload, we can
substantially save energy without sacrificing too much performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0122</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0122</id><created>2014-07-01</created><authors><author><keyname>Sakib</keyname><forenames>Kazi</forenames></author><author><keyname>Hasan</keyname><forenames>M. S.</forenames></author><author><keyname>Hossain</keyname><forenames>M. A.</forenames></author></authors><title>Effects of Hard Real-Time Constraints in Implementing the Myopic
  Scheduling Algorithm</title><categories>cs.OS cs.DC</categories><comments>9 pages, Journal of Computer Science (JCS), Bangladesh, Vol. 1, No.
  2, December, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Myopic is a hard real-time process scheduling algorithm that selects a
suitable process based on a heuristic function from a subset (Window)of all
ready processes instead of choosing from all available processes, like original
heuristic scheduling algorithm. Performance of the algorithm significantly
depends on the chosen heuristic function that assigns weight to different
parameters like deadline, earliest starting time, processing time etc. and the
sizeof the Window since it considers only k processes from n processes (where,
k&lt;= n). This research evaluates the performance of the Myopic algorithm for
different parameters to demonstrate the merits and constraints of the
algorithm. A comparative performance of the impact of window size in
implementing the Myopic algorithm is presented and discussed through a set of
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0124</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0124</id><created>2014-07-01</created><authors><author><keyname>Yagi</keyname><forenames>Hideki</forenames></author><author><keyname>Nomura</keyname><forenames>Ryo</forenames></author></authors><title>Single-Letter Characterization of Epsilon-Capacity for Mixed Memoryless
  Channels</title><categories>cs.IT math.IT</categories><comments>This is an extended version of the paper submitted to the 2014 IEEE
  International Symposium on Information Theory (ISIT2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the class of mixed channels decomposed into stationary memoryless
channels, single-letter characterizations of the $\varepsilon$-capacity have
not been known except for restricted classes of channels such as the regular
decomposable channel introduced by Winkelbauer. This paper gives single-letter
characterizations of $\varepsilon$-capacity for mixed channels decomposed into
at most countably many memoryless channels with a finite input alphabet and a
general output alphabet with/without cost constraints. It is shown that a given
characterization reduces to the one for the channel capacity given by Ahlswede
when $\varepsilon$ is zero. In the proof of the coding theorem, the meta
converse bound, originally given by Polyanskiy, Poor and Verd\'u, is
particularized for the mixed channel decomposed into general component
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0142</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0142</id><created>2014-07-01</created><updated>2015-10-21</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Asymmetric Evaluations of Erasure and Undetected Error Probabilities</title><categories>cs.IT math.IT</categories><comments>28 pages, no figures in IEEE Transactions on Information Theory, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of channel coding with the erasure option is revisited for
discrete memoryless channels. The interplay between the code rate, the
undetected and total error probabilities is characterized. Using the
information spectrum method, a sequence of codes of increasing blocklengths $n$
is designed to illustrate this tradeoff. Furthermore, for additive discrete
memoryless channels with uniform input distribution, we establish that our
analysis is tight with respect to the ensemble average. This is done by
analysing the ensemble performance in terms of a tradeoff between the code
rate, the undetected and the total errors. This tradeoff is parametrized by the
threshold in a generalized likelihood ratio test. Two asymptotic regimes are
studied. First, the code rate tends to the capacity of the channel at a rate
slower than $n^{-1/2}$ corresponding to the moderate deviations regime. In this
case, both error probabilities decay subexponentially and asymmetrically. The
precise decay rates are characterized. Second, the code rate tends to capacity
at a rate of $n^{-1/2}$. In this case, the total error probability is
asymptotically a positive constant while the undetected error probability
decays as $\exp(- b n^{ 1/2})$ for some $b&gt;0$. The proof techniques involve
applications of a modified (or &quot;shifted&quot;) version of the G\&quot;artner-Ellis
theorem and the type class enumerator method to characterize the asymptotic
behavior of a sequence of cumulant generating functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0145</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0145</id><created>2014-07-01</created><updated>2014-11-25</updated><authors><author><keyname>Sun</keyname><forenames>Lijun</forenames></author><author><keyname>Jin</keyname><forenames>Jian Gang</forenames></author><author><keyname>Axhausen</keyname><forenames>Kay W.</forenames></author><author><keyname>Lee</keyname><forenames>Der-Horng</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author></authors><title>Quantifying long-term evolution of intra-urban spatial interactions</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 4 figures; J. R. Soc. Interface 20141089</comments><doi>10.1098/rsif.2014.1089</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the long-term impact that changes in a city's transportation
infrastructure have on its spatial interactions remains a challenge. The
difficulty arises from the fact that the real impact may not be revealed in
static or aggregated mobility measures, as these are remarkably robust to
perturbations. More generally, the lack of longitudinal, cross-sectional data
demonstrating the evolution of spatial interactions at a meaningful urban scale
also hinders us from evaluating the sensitivity of movement indicators,
limiting our capacity to understand the evolution of urban mobility in depth.
Using very large mobility records distributed over three years we quantify the
impact of the completion of a metro line extension: the circle line (CCL) in
Singapore. We find that the commonly used movement indicators are almost
identical before and after the project was completed. However, in comparing the
temporal community structure across years, we do observe significant
differences in the spatial reorganization of the affected geographical areas.
The completion of CCL enables travelers to re-identify their desired
destinations collectively with lower transport cost, making the community
structure more consistent. These changes in locality are dynamic, and
characterized over short time-scales, offering us a different approach to
identify and analyze the long-term impact of new infrastructures on cities and
their evolution dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0147</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0147</id><created>2014-07-01</created><authors><author><keyname>Khatwal</keyname><forenames>Ravi</forenames></author><author><keyname>Jain</keyname><forenames>Manoj Kumar</forenames></author></authors><title>Application Specific Hardware Design Simulation for High Performance
  Embedded System</title><categories>cs.AR</categories><comments>ijca,june(2014)</comments><doi>10.5120/16834-6599</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Application specific simulation is challenging task in various real time high
performance embedded devices. In this study specific application is implemented
with the help of Xilinx. Xilinx provides SDK and XPS tools, XPS tools used for
develop complete hardware platform and SDK provides software platform for
application creation and verification. Xilinx XUP-5 board have been used and
implemented various specific Applications with hardware platform. In this study
the base instruction set with customized instructions, supported with specific
hardware resources are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0153</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0153</id><created>2014-07-01</created><authors><author><keyname>Cena</keyname><forenames>Federica</forenames></author><author><keyname>Likavec</keyname><forenames>Silvia</forenames></author><author><keyname>Lombardi</keyname><forenames>Ilaria</forenames></author><author><keyname>Picardi</keyname><forenames>Claudia</forenames></author></authors><title>Should I Stay or Should I Go? Improving Event Recommendation in the
  Social Web</title><categories>cs.HC cs.SI</categories><comments>21 pages. Accepted for publication on Interacting with Computers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the recommendation of events in the Social Web, and
addresses the problem of finding if, and to which extent, certain features,
which are peculiar to events, are relevant in predicting the users' interests
and should thereby be taken into account in recommendation.
  We consider in particular three &quot;additional&quot; features that are usually shown
to users within social networking environments: reachability from the user
location, the reputation of the event in the community, and the participation
of the user's friends. Our study is aimed at evaluating whether adding this
information to the description of the event type and topic, and including in
the user profile the information on the relevance of these factors, can improve
our capability to predict the user's interest.
  We approached the problem by carrying out two surveys with users, who were
asked to express %with a score their interest in a number of events. We then
trained, by means of linear regression, a scoring function defined as a linear
combination of the different factors, whose goal was to predict the user
scores. We repeated this experiment under different hypotheses on the
additional factors, in order to assess their relevance by comparing the
predictive capabilities of the resulting functions.
  The compared results of our experiments show that additional factors, if
properly weighted, can improve the prediction accuracy with an error reduction
of 4.1%. The best results were obtained by combining content-based factors and
additional factors in a proportion of approximately 10:4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0160</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0160</id><created>2014-07-01</created><updated>2014-12-10</updated><authors><author><keyname>C&#xe2;mpeanu</keyname><forenames>Cezar</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author><author><keyname>Reis</keyname><forenames>Rog&#xe9;rio</forenames></author></authors><title>Distinguishability Operations and Closures on Regular Languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a regular language $L$, we study the language of words $\mathsf{D}(L)$,
that distinguish between pairs of different left-quotients of $L$. We
characterize this distinguishability operation, show that its iteration has
always a fixed point, and we generalize this result to operations derived from
closure operators and Boolean operators. We give an upper bound for the state
complexity of the distinguishability operation, and prove its tightness. We
show that the set of minimal words that can be used to distinguish between
different left-quotients of a language $L$ has at most $n-1$ elements, where
$n$ is the state complexity of $L$, and we also study the properties of its
iteration. We generalize the results for the languages of words that
distinguish between pairs of different right-quotients and two-sided quotients
of a language $L$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0165</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0165</id><created>2014-07-01</created><authors><author><keyname>Garc&#xed;a-Jim&#xe9;nez</keyname><forenames>Beatriz</forenames></author><author><keyname>Wilkinson</keyname><forenames>Mark D.</forenames></author></authors><title>Automatic annotation of bioinformatics workflows with biomedical
  ontologies</title><categories>cs.IR q-bio.QM</categories><comments>6th International Symposium on Leveraging Applications (ISoLA 2014
  conference), 15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Legacy scientific workflows, and the services within them, often present
scarce and unstructured (i.e. textual) descriptions. This makes it difficult to
find, share and reuse them, thus dramatically reducing their value to the
community. This paper presents an approach to annotating workflows and their
subcomponents with ontology terms, in an attempt to describe these artifacts in
a structured way. Despite a dearth of even textual descriptions, we
automatically annotated 530 myExperiment bioinformatics-related workflows,
including more than 2600 workflow-associated services, with relevant
ontological terms. Quantitative evaluation of the Information Content of these
terms suggests that, in cases where annotation was possible at all, the
annotation quality was comparable to manually curated bioinformatics resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0166</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0166</id><created>2014-07-01</created><authors><author><keyname>Di</keyname><forenames>Xiaofei</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author><author><keyname>Qiu</keyname><forenames>Zhengding</forenames></author></authors><title>Simultaneous Wireless Information and Power Transfer for Two-hop OFDM
  Relay System</title><categories>cs.IT math.IT</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the simultaneous wireless information and power
transfer (SWIPT) for two-hop orthogonal frequency division multiplexing (OFDM)
decode-and-forward (DF) relay communication system, where a relay harvests
energy from radio frequency signals transmitted by the source and then uses the
harvested energy to assist the information transmission from the source to its
destination. The power splitting receiver is considered at the relay, which
splits the received signal into two power streams to perform information
decoding (ID) and energy harvesting (EH) respectively. For better understanding
the behavior and exploring the performance limit of such a system, resource
allocation is studied to maximize the total achievable transmission rate. An
optimization problem, which jointly takes into account the power allocation,
the subcarrier pairing and the power splitting, is formulated. Due to its
non-convexity, a resource allocation policy with low complexity based on
separation principle is designed. Simulation results show that the system
performance can be significantly improved by using our proposed policy.
Moreover, the system performance behavior to the relay position is also
discussed, and results show that in the two-hop OFDM system with EH relay, the
relay should be deployed near the source, while in that with conventional
non-EH relay, it should be deployed at the middle between the source and the
destination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0167</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0167</id><created>2014-07-01</created><authors><author><keyname>Pagael</keyname><forenames>Robert</forenames></author><author><keyname>Schubotz</keyname><forenames>Moritz</forenames></author></authors><title>Mathematical Language Processing Project</title><categories>cs.DL cs.CL cs.IR</categories><comments>8 pages, one figure, Conferences on Intelligent Computer Mathematics
  (CICM) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In natural language, words and phrases themselves imply the semantics. In
contrast, the meaning of identifiers in mathematical formulae is undefined.
Thus scientists must study the context to decode the meaning. The Mathematical
Language Processing (MLP) project aims to support that process. In this paper,
we compare two approaches to discover identifier-definition tuples. At first we
use a simple pattern matching approach. Second, we present the MLP approach
that uses part-of-speech tag based distances as well as sentence positions to
calculate identifier-definition probabilities. The evaluation of our
prototypical system, applied on the Wikipedia text corpus, shows that our
approach augments the user experience substantially. While hovering the
identifiers in the formula, tool-tips with the most probable definitions occur.
Tests with random samples show that the displayed definitions provide a good
match with the actual meaning of the identifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0169</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0169</id><created>2014-07-01</created><authors><author><keyname>Amorim</keyname><forenames>Ivone</forenames></author><author><keyname>Machiavelo</keyname><forenames>Ant&#xf3;nio</forenames></author><author><keyname>Reis</keyname><forenames>Rog&#xe9;rio</forenames></author></authors><title>Statistical Study On The Number Of Injective Linear Finite Transducers</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of linear finite transducer (LFT) plays a crucial role in some
cryptographic systems. In this paper we present a way to get an approximate
value, by random sampling, for the number of non-equivalent injective LFTs. By
introducing a recurrence relation to count canonical LFTs, we show how to
estimate the percentage of $\tau$-injective LFTs. Several experimental results
are presented, which by themselves constitute an important step towards the
evaluation of the key space of those systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0179</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0179</id><created>2014-07-01</created><authors><author><keyname>Hern&#xe1;ndez-Lobato</keyname><forenames>Daniel</forenames></author><author><keyname>Sharmanska</keyname><forenames>Viktoriia</forenames></author><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author><author><keyname>Quadrianto</keyname><forenames>Novi</forenames></author></authors><title>Mind the Nuisance: Gaussian Process Classification using Privileged
  Noise</title><categories>stat.ML cs.LG</categories><comments>14 pages with figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The learning with privileged information setting has recently attracted a lot
of attention within the machine learning community, as it allows the
integration of additional knowledge into the training process of a classifier,
even when this comes in the form of a data modality that is not available at
test time. Here, we show that privileged information can naturally be treated
as noise in the latent function of a Gaussian Process classifier (GPC). That
is, in contrast to the standard GPC setting, the latent function is not just a
nuisance but a feature: it becomes a natural measure of confidence about the
training data by modulating the slope of the GPC sigmoid likelihood function.
Extensive experiments on public datasets show that the proposed GPC method
using privileged noise, called GPC+, improves over a standard GPC without
privileged knowledge, and also over the current state-of-the-art SVM-based
method, SVM+. Moreover, we show that advanced neural networks and deep learning
methods can be compressed as privileged information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0189</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0189</id><created>2014-07-01</created><authors><author><keyname>Pradhan</keyname><forenames>Rajkumar</forenames></author><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author></authors><title>Convergence of maxgeneralized mean-mingeneralized mean powers of
  intuitionistic fuzzy matrices</title><categories>cs.DM</categories><comments>20 pages</comments><journal-ref>The Journal of Fuzzy Mathematics, 22(2) (2014) 477-492</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intuitionistic fuzzy relations on finite universes can be represent by
intuitionistic fuzzy matrices and the
  limiting behavior of the power matrices depends on the algebraic operation
employed on the matrices. In this
  paper, the power of intuitionistic fuzzy matrices with maxgeneralized
mean-mingeneralized mean operation have
  been studied. Here it is shown that the power of intuitionistic fuzzy
matrices with the said operations are
  always convergent. The convergence of powers for an intuitionistic fuzzy
matrix with convex combination of
  max-min and maxarithmetic mean-minarithmetic mean are also dicussed here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0199</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0199</id><created>2014-07-01</created><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Raan</keyname><forenames>Anthony F. J.</forenames></author><author><keyname>Smart</keyname><forenames>Sue</forenames></author></authors><title>Exploring the relationship between the Engineering and Physical Sciences
  and the Health and Life Sciences by advanced bibliometric methods</title><categories>cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the extent to which advances in the health and life sciences
(HLS) are dependent on research in the engineering and physical sciences (EPS),
particularly physics, chemistry, mathematics, and engineering. The analysis
combines two different bibliometric approaches. The first approach to analyze
the 'EPS-HLS interface' is based on term map visualizations of HLS research
fields. We consider 16 clinical fields and five life science fields. On the
basis of expert judgment, EPS research in these fields is studied by
identifying EPS-related terms in the term maps. In the second approach, a
large-scale citation-based network analysis is applied to publications from all
fields of science. We work with about 22,000 clusters of publications, each
representing a topic in the scientific literature. Citation relations are used
to identify topics at the EPS-HLS interface. The two approaches complement each
other. The advantages of working with textual data compensate for the
limitations of working with citation relations and the other way around. An
important advantage of working with textual data is in the in-depth qualitative
insights it provides. Working with citation relations, on the other hand,
yields many relevant quantitative statistics. We find that EPS research
contributes to HLS developments mainly in the following five ways: new
materials and their properties; chemical methods for analysis and molecular
synthesis; imaging of parts of the body as well as of biomaterial surfaces;
medical engineering mainly related to imaging, radiation therapy, signal
processing technology, and other medical instrumentation; mathematical and
statistical methods for data analysis. In our analysis, about 10% of all EPS
and HLS publications are classified as being at the EPS-HLS interface. This
percentage has remained more or less constant during the past decade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0202</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0202</id><created>2014-07-01</created><updated>2014-12-16</updated><authors><author><keyname>Defazio</keyname><forenames>Aaron</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS, MSR - INRIA</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS, MSR - INRIA</affiliation></author><author><keyname>Lacoste-Julien</keyname><forenames>Simon</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS, MSR - INRIA</affiliation></author></authors><title>SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly
  Convex Composite Objectives</title><categories>cs.LG math.OC stat.ML</categories><comments>Advances In Neural Information Processing Systems, Nov 2014,
  Montreal, Canada</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we introduce a new optimisation method called SAGA in the spirit
of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient
algorithms with fast linear convergence rates. SAGA improves on the theory
behind SAG and SVRG, with better theoretical convergence rates, and has support
for composite objectives where a proximal operator is used on the regulariser.
Unlike SDCA, SAGA supports non-strongly convex problems directly, and is
adaptive to any inherent strong convexity of the problem. We give experimental
results showing the effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0208</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0208</id><created>2014-07-01</created><updated>2014-10-26</updated><authors><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Weiss</keyname><forenames>Roi</forenames></author></authors><title>A Bayes consistent 1-NN classifier</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a simple modification of the 1-nearest neighbor classifier
yields a strongly Bayes consistent learner. Prior to this work, the only
strongly Bayes consistent proximity-based method was the k-nearest neighbor
classifier, for k growing appropriately with sample size. We will argue that a
margin-regularized 1-NN enjoys considerable statistical and algorithmic
advantages over the k-NN classifier. These include user-friendly finite-sample
error bounds, as well as time- and memory-efficient learning and test-point
evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging
empirical results are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0221</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0221</id><created>2014-07-01</created><authors><author><keyname>Lellmann</keyname><forenames>Jan</forenames></author><author><keyname>Lorenz</keyname><forenames>Dirk A.</forenames></author><author><keyname>Sch&#xf6;nlieb</keyname><forenames>Carola</forenames></author><author><keyname>Valkonen</keyname><forenames>Tuomo</forenames></author></authors><title>Imaging with Kantorovich-Rubinstein discrepancy</title><categories>cs.CV math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the use of the Kantorovich-Rubinstein norm from optimal transport
in imaging problems. In particular, we discuss a variational regularisation
model endowed with a Kantorovich-Rubinstein discrepancy term and total
variation regularization in the context of image denoising and cartoon-texture
decomposition. We point out connections of this approach to several other
recently proposed methods such as total generalized variation and norms
capturing oscillating patterns. We also show that the respective optimization
problem can be turned into a convex-concave saddle point problem with simple
constraints and hence, can be solved by standard tools. Numerical examples
exhibit interesting features and favourable performance for denoising and
cartoon-texture decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0224</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0224</id><created>2014-07-01</created><updated>2014-10-02</updated><authors><author><keyname>Silva</keyname><forenames>Filipi N.</forenames></author><author><keyname>Comin</keyname><forenames>Cesar H.</forenames></author><author><keyname>Peron</keyname><forenames>Thomas K. DM.</forenames></author><author><keyname>Rodrigues</keyname><forenames>Francisco A.</forenames></author><author><keyname>Ye</keyname><forenames>Cheng</forenames></author><author><keyname>Wilson</keyname><forenames>Richard C.</forenames></author><author><keyname>Hancock</keyname><forenames>Edwin</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Concentric Network Symmetry</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantification of symmetries in complex networks is typically done globally
in terms of automorphisms. Extending previous methods to locally assess the
symmetry of nodes is not straightforward. Here we present a new framework to
quantify the symmetries around nodes, which we call connectivity patterns. We
develop two topological transformations that allow a concise characterization
of the different types of symmetry appearing on networks and apply these
concepts to six network models, namely the Erd\H{o}s-R\'enyi,
Barab\'asi-Albert, random geometric graph, Waxman, Voronoi and rewired Voronoi.
Real-world networks, namely the scientific areas of Wikipedia, the world-wide
airport network and the street networks of Oldenburg and San Joaquin, are also
analyzed in terms of the proposed symmetry measurements. Several interesting
results emerge from this analysis, including the high symmetry exhibited by the
Erd\H{o}s-R\'enyi model. Additionally, we found that the proposed measurements
present low correlation with other traditional metrics, such as node degree and
betweenness centrality. Principal component analysis is used to combine all the
results, revealing that the concepts presented here have substantial potential
to also characterize networks at a global scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0265</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0265</id><created>2014-07-01</created><authors><author><keyname>Stromatias</keyname><forenames>Evangelos</forenames></author><author><keyname>Marsland</keyname><forenames>John</forenames></author></authors><title>Supervised learning in Spiking Neural Networks with Limited Precision:
  SNN/LP</title><categories>cs.NE</categories><comments>7 pages, originally submitted to IJCNN 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new supervised learning algorithm, SNN/LP, is proposed for Spiking Neural
Networks. This novel algorithm uses limited precision for both synaptic weights
and synaptic delays; 3 bits in each case. Also a genetic algorithm is used for
the supervised training. The results are comparable or better than previously
published work. The results are applicable to the realization of large scale
hardware neural networks. One of the trained networks is implemented in
programmable hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0276</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0276</id><created>2014-06-29</created><authors><author><keyname>Das</keyname><forenames>Indrani</forenames></author><author><keyname>Lobiyal</keyname><forenames>D. K</forenames></author><author><keyname>Katti</keyname><forenames>C. P</forenames></author></authors><title>Effect of node mobility on AOMDV protocol in MANET</title><categories>cs.NI</categories><comments>9 pages, 6 figures, International Journal of Wireless &amp; Mobile
  Networks (IJWMN) Vol. 6, No. 3, June 2014</comments><doi>10.5121/ijwmn.2014.6307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have analyzed the effect of node mobility on the
performance of AOMDV multipath routing protocol. This routing protocol in ad
hoc network has been analyzed with random way point mobility model only. This
is not sufficient to evaluate the behavior of a routing protocol. Therefore, in
this paper, we have considered Random waypoint, Random Direction and
Probabilistic Random Walk mobility Model for performance analysis of AOMDV
protocol. The result reveals that packet delivery ratio decreases with the
increasing node mobility for all mobility models. Also, average end-to-end
delay is also vary with varying node speed, initially upto 20 nodes in all
mobility models delay is minimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0286</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0286</id><created>2014-07-01</created><updated>2014-07-02</updated><authors><author><keyname>Thi</keyname><forenames>Hoai An Le</forenames></author><author><keyname>Dinh</keyname><forenames>Tao Pham</forenames></author><author><keyname>Le</keyname><forenames>Hoai Minh</forenames></author><author><keyname>Vo</keyname><forenames>Xuan Thanh</forenames></author></authors><title>DC approximation approaches for sparse optimization</title><categories>cs.NA cs.LG stat.ML</categories><comments>35 pages</comments><msc-class>90C26, 90C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse optimization refers to an optimization problem involving the zero-norm
in objective or constraints. In this paper, nonconvex approximation approaches
for sparse optimization have been studied with a unifying point of view in DC
(Difference of Convex functions) programming framework. Considering a common DC
approximation of the zero-norm including all standard sparse inducing penalty
functions, we studied the consistency between global minimums (resp. local
minimums) of approximate and original problems. We showed that, in several
cases, some global minimizers (resp. local minimizers) of the approximate
problem are also those of the original problem. Using exact penalty techniques
in DC programming, we proved stronger results for some particular
approximations, namely, the approximate problem, with suitable parameters, is
equivalent to the original problem. The efficiency of several sparse inducing
penalty functions have been fully analyzed. Four DCA (DC Algorithm) schemes
were developed that cover all standard algorithms in nonconvex sparse
approximation approaches as special versions. They can be viewed as, an $\ell
_{1}$-perturbed algorithm / reweighted-$\ell _{1}$ algorithm / reweighted-$\ell
_{1}$ algorithm. We offer a unifying nonconvex approximation approach, with
solid theoretical tools as well as efficient algorithms based on DC programming
and DCA, to tackle the zero-norm and sparse optimization. As an application, we
implemented our methods for the feature selection in SVM (Support Vector
Machine) problem and performed empirical comparative numerical experiments on
the proposed algorithms with various approximation functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0292</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0292</id><created>2014-07-01</created><authors><author><keyname>Kulkarni</keyname><forenames>Ajay</forenames></author><author><keyname>Kulkarni</keyname><forenames>Saurabh</forenames></author></authors><title>An Open Source P2P Encrypted VoIP Application</title><categories>cs.PL cs.CR</categories><comments>International Journal of Advanced Computer Science and
  Applications(IJACSA), 5 pages, 6 figures,
  http://thesai.org/Publications/ViewPaper?Volume=5&amp;Issue=6&amp;Code=IJACSA&amp;SerialNo=1</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), 5(6), 2014</journal-ref><doi>10.14569/IJACSA.2014.050601</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Open source is the future of technology. This community is growing by the
day; developing and improving existing frameworks and software for free. Open
source replacements are coming up for almost all proprietary software nowadays.
This paper proposes an open source application which could replace Skype, a
popular VoIP soft phone. The performance features of the developed software is
analyzed and compared with Skype so that we can conclude that it can be an
efficient replacement. This application is developed in pure Java using various
APIs and package and boasts features like voice calling, chatting, file sharing
etc. The target audience for this software will initially only be organizations
(for internal communication) and later will be released on a larger scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0308</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0308</id><created>2014-07-01</created><authors><author><keyname>Jonsdottir</keyname><forenames>Anna Helga</forenames></author><author><keyname>Stefansson</keyname><forenames>Gunnar</forenames></author></authors><title>Using an Online Learning Environment to Teach an Undergraduate
  Statistics Course: the tutor-web</title><categories>stat.OT cs.CY</categories><comments>Presented at Edulearn 2013, Barcelona</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A learning environment, the tutor-web (http://tutor-web.net), has been
developed and used for educational research. The system is accessible and free
to use for anyone having access to the Web. It is based on open source software
and the teaching material is licensed under the Creative Commons
Attribution-ShareAlike License. The system has been used for computer-assisted
education in statistics and mathematics. It offers a unique way to structure
and link together teaching material and includes interactive quizzes with the
primary purpose of increasing learning rather than mere evaluation.
  The system was used in a course on basic statistics in the University of
Iceland, spring 2013. A randomized trial was conducted to investigate the
difference in learning between students doing regular homework and students
using the system. The difference between the groups was not found to be
significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0312</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0312</id><created>2014-07-01</created><updated>2014-11-18</updated><authors><author><keyname>Li</keyname><forenames>Xingguo</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis</forenames></author></authors><title>Identifying Outliers in Large Matrices via Randomized Adaptive
  Compressive Sampling</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>16 pages, 7 figures, 2 tables, IEEE Transactions on Signal Processing
  (submitted)</comments><doi>10.1109/TSP.2015.2401536</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the problem of locating outlier columns in a large,
otherwise low-rank, matrix. We propose a simple two-step adaptive sensing and
inference approach and establish theoretical guarantees for its performance;
our results show that accurate outlier identification is achievable using very
few linear summaries of the original data matrix -- as few as the squared rank
of the low-rank component plus the number of outliers, times constant and
logarithmic factors. We demonstrate the performance of our approach
experimentally in two stylized applications, one motivated by robust
collaborative filtering tasks, and the other by saliency map estimation tasks
arising in computer vision and automated surveillance, and also investigate
extensions to settings where the data are noisy, or possibly incomplete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0313</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0313</id><created>2014-06-30</created><authors><author><keyname>Mane</keyname><forenames>Mr. Pradip Suresh</forenames></author><author><keyname>Khairnar</keyname><forenames>Vaishali D.</forenames></author></authors><title>Analysis of Bus Tracking System Using GPS on Smartphones</title><categories>cs.CY cs.NI</categories><comments>PP.80-82</comments><journal-ref>IOSR Journal of Computer Engineering (IOSR-JCE)e-ISSN:
  2278-06618727 Volume 16, Issue 2, Ver. XII (Mar-Apr. 2014), PP 80-82</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public transport networks(PTNs)are difficult to use when the user is
unfamiliar with the area they are traveling to.This is true for both infrequent
users(including visitors)and regular users who need to travel to areas with
which they are not acquainted.In these situations,adequate on-trip navigation
information can substantially ease the use of public transportation and be the
driving factor in motivating travelers to prefer it over other modes of
transportation.However,estimating the localization of a user is not
trivial,although it is critical for providing relevant information.I assess
relevant design issues for a modular cost-efficient user-friendly on-trip
Navigation service that uses position sensors.By helping travelers move from
single-occupancy vehicles to public transportation systems, communities can
reduce traffic congestion as well as its environmental impact.Here,I describe
our efforts to increase the satisfaction of current public transportation users
and help motivate more people to ride.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0316</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0316</id><created>2014-07-01</created><updated>2015-01-30</updated><authors><author><keyname>Sugiyama</keyname><forenames>Mahito</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Felipe Llinares</forenames></author><author><keyname>Kasenburg</keyname><forenames>Niklas</forenames></author><author><keyname>Borgwardt</keyname><forenames>Karsten M.</forenames></author></authors><title>Significant Subgraph Mining with Multiple Testing Correction</title><categories>stat.ME cs.LG stat.ML</categories><comments>18 pages, 5 figure, accepted to the 2015 SIAM International
  Conference on Data Mining (SDM15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding itemsets that are statistically significantly enriched
in a class of transactions is complicated by the need to correct for multiple
hypothesis testing. Pruning untestable hypotheses was recently proposed as a
strategy for this task of significant itemset mining. It was shown to lead to
greater statistical power, the discovery of more truly significant itemsets,
than the standard Bonferroni correction on real-world datasets. An open
question, however, is whether this strategy of excluding untestable hypotheses
also leads to greater statistical power in subgraph mining, in which the number
of hypotheses is much larger than in itemset mining. Here we answer this
question by an empirical investigation on eight popular graph benchmark
datasets. We propose a new efficient search strategy, which always returns the
same solution as the state-of-the-art approach and is approximately two orders
of magnitude faster. Moreover, we exploit the dependence between subgraphs by
considering the effective number of tests and thereby further increase the
statistical power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0323</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0323</id><created>2014-06-30</created><authors><author><keyname>Shaw</keyname><forenames>Aaron</forenames></author><author><keyname>Hill</keyname><forenames>Benjamin Mako</forenames></author></authors><title>Laboratories of Oligarchy? How the Iron Law Extends to Peer Production</title><categories>cs.CY cs.SI</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/96</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer production projects like Wikipedia have inspired voluntary associations,
collectives, social movements, and scholars to embrace open online
collaboration as a model of democratic organization. However, many peer
production projects exhibit entrenched leadership and deep inequalities,
suggesting that they may not fulfill democratic ideals. Instead, peer
production projects may conform to Robert Michels' &quot;iron law of oligarchy,&quot;
which proposes that democratic membership organizations become increasingly
oligarchic as they grow. Using exhaustive data of internal processes from a
sample of 683 wikis, we construct empirical measures of participation and test
for increases in oligarchy associated with growth. In contrast to previous
studies, we find support for Michels' iron law and conclude that peer
production entails oligarchic organizational forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0325</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0325</id><created>2014-06-30</created><authors><author><keyname>Prpic</keyname><forenames>John</forenames></author><author><keyname>Jackson</keyname><forenames>Piper</forenames></author><author><keyname>Nguyen</keyname><forenames>Thai</forenames></author></authors><title>A Computational Model of Crowds for Collective Intelligence</title><categories>cs.CY</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/94</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a high-level computational model of IT-mediated
crowds for collective intelligence. We introduce the Crowd Capital perspective
as an organizational-level model of collective intelligence generation from
IT-mediated crowds, and specify a computational system including agents, forms
of IT, and organizational knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0330</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0330</id><created>2014-06-30</created><authors><author><keyname>Maddali</keyname><forenames>Hanuma Teja</forenames></author><author><keyname>Novitzky</keyname><forenames>Michael</forenames></author><author><keyname>Hrolenok</keyname><forenames>Brian</forenames></author><author><keyname>Walker</keyname><forenames>Daniel</forenames></author><author><keyname>Balch</keyname><forenames>Tucker</forenames></author><author><keyname>Wallen</keyname><forenames>Kim</forenames></author></authors><title>Inferring Social Structure and Dominance Relationships Between Rhesus
  macaques using RFID Tracking Data</title><categories>cs.SI</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/100</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of inferring social structure and
dominance relationships in a group of rhesus macaques (a species of monkey)
using only position data captured using RFID tags. Automatic inference of the
social structure in an animal group enables a number of important capabilities,
including: 1) A verifiable measure of how the social structure is affected by
an intervention such as a change in the environment, or the introduction of
another animal, and 2) A potentially significant reduction in person hours
normally used for assessing these changes. Social structure in a group is an
important indicator of its members' relative level of access to resources and
has interesting implications for an individual's health and learning in groups.
There are two main quantitative criteria assessed in order to infer the social
structure; Time spent close to conspecifics, and displacements. An interaction
matrix is used to represent the total duration of events detected as grooming
behavior between any two monkeys. This forms an undirected tie-strength
(closeness of relationships) graph. A directed graph of hierarchy is
constructed by using the well cited assumption of a linear hierarchy for rhesus
macaques. Events that contribute to the adjacency matrix for this graph are
withdrawals or displacements where a lower ranked monkey moves away from a
higher ranked monkey. Displacements are one of the observable behaviors that
can act as a strong indication of tie-strength and dominance. To quantify the
directedness of interaction during these events we construct histograms of the
dot products of motion orientation and relative position. This gives us a
measure of how much time a monkey spends in moving towards or away from other
group members.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0333</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0333</id><created>2014-07-01</created><authors><author><keyname>Courtade</keyname><forenames>Thomas A.</forenames></author><author><keyname>Halford</keyname><forenames>Thomas R.</forenames></author></authors><title>Coded Cooperative Data Exchange for a Secret Key</title><categories>cs.IT cs.CR math.IT</categories><comments>Full version of a paper that appeared at ISIT 2014. 19 pages, 2
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a coded cooperative data exchange problem with the goal of
generating a secret key. Specifically, we investigate the number of public
transmissions required for a set of clients to agree on a secret key with
probability one, subject to the constraint that it remains private from an
eavesdropper.
  Although the problems are closely related, we prove that secret key
generation with fewest number of linear transmissions is NP-hard, while it is
known that the analogous problem in traditional cooperative data exchange can
be solved in polynomial time. In doing this, we completely characterize the
best possible performance of linear coding schemes, and also prove that linear
codes can be strictly suboptimal. Finally, we extend the single-key results to
characterize the minimum number of public transmissions required to generate a
desired integer number of statistically independent secret keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0334</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0334</id><created>2014-07-01</created><authors><author><keyname>Demirci</keyname><forenames>H. G&#xf6;kalp</forenames></author><author><keyname>Hirvensalo</keyname><forenames>Mika</forenames></author><author><keyname>Reinhardt</keyname><forenames>Klaus</forenames></author><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Classical and quantum realtime alternating automata</title><categories>cs.FL cs.CC quant-ph</categories><comments>16 pages. Accepted to NCMA2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present some new results on realtime classical and quantum alternating
models. Firstly, we show that the emptiness problem for alternating one-counter
automata on unary alphabets is undecidable. Then, we define realtime private
alternating finite automata (PAFAs) and show that they can recognize some
non-regular unary languages, and the emptiness problem is undecidable for them.
Moreover, PAFAs augmented with a counter can recognize the unary squares
language, which seems to be difficult even for some classical counter automata
with two-way input. For quantum finite automata (QFAs), we show that the
emptiness problem for universal QFAs on general alphabets and alternating QFAs
with two alternations on unary alphabets are undecidable. On the other hand,
the same problem is decidable for nondeterministic QFAs on general alphabets.
We also show that the unary squares language is recognized by alternating QFAs
with two alternations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0342</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0342</id><created>2014-07-01</created><authors><author><keyname>Xu</keyname><forenames>Jinwei</forenames></author><author><keyname>Hu</keyname><forenames>Jiankun</forenames></author><author><keyname>Jia</keyname><forenames>Xiuping</forenames></author></authors><title>A New Path to Construct Parametric Orientation Field: Sparse FOMFE Model
  and Compressed Sparse FOMFE Model</title><categories>cs.CV cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orientation field, representing the fingerprint ridge structure direction,
plays a crucial role in fingerprint-related image processing tasks. Orientation
field is able to be constructed by either non-parametric or parametric methods.
In this paper, the advantages and disadvantages regarding to the existing
non-parametric and parametric approaches are briefly summarized. With the
further investigation for constructing the orientation field by parametric
technique, two new models - sparse FOMFE model and compressed sparse FOMFE
model are introduced, based on the rapidly developing signal sparse
representation and compressed sensing theories. The experiments on high-quality
fingerprint image dataset (plain and rolled print) and poor-quality fingerprint
image dataset (latent print) demonstrate their feasibilities to construct the
orientation field in a sparse or even compressed sparse mode. The comparisons
among the state-of-art orientation field modeling approaches show that the
proposed two models have the potential availability in big data-oriented
fingerprint indexing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0343</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0343</id><created>2014-06-30</created><updated>2014-08-11</updated><authors><author><keyname>Small</keyname><forenames>Michael</forenames></author></authors><title>Expected degree of finite preferential attachment networks</title><categories>cs.SI nlin.AO</categories><comments>Technical note - barely 4 pages. Correct sign error in final
  approximation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an analytic expression for the quantity described in the title.
Namely, we perform a preferential attachment growth process to generate a
scale-free network. At each stage we add a new node with $m$ new links. Let $k$
denote the degree of a node, and $N$ the number of nodes in the network. The
degree distribution is assumed to converge to a power-law (for $k\geq m$) of
the form $k^{-\gamma}$ and we obtain an exact implicit relationship for
$\gamma$, $m$ and $N$. We verify this with numerical calculations over several
orders of magnitude. Although this expression is exact, it provides only an
implicit expression for $\gamma(m)$. Nonetheless, we provide a reasonable guess
as to the form of this curve and perform curve fitting to estimate the
parameters of that curve --- demonstrating excellent agreement between
numerical fit, theory, and simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0344</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0344</id><created>2014-07-01</created><updated>2014-08-27</updated><authors><author><keyname>Cavalcante</keyname><forenames>R. L. G.</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>S.</forenames></author><author><keyname>Schubert</keyname><forenames>M.</forenames></author><author><keyname>Eisenbl&#xe4;tter</keyname><forenames>A.</forenames></author><author><keyname>T&#xfc;rke</keyname><forenames>U.</forenames></author></authors><title>Toward Energy-Efficient 5G Wireless Communications Technologies</title><categories>cs.NI cs.IT math.IT</categories><comments>accepted for publication. IEEE Signal Processing Magazine, Nov. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The densification and expansion of wireless networks pose new challenges on
energy efficiency. With a drastic increase of infrastructure nodes (e.g.
ultra-dense deployment of small cells), the total energy consumption may easily
exceed an acceptable level. While most studies focus on the energy radiated by
the antennas, the bigger part of the total energy budget is actually consumed
by the hardware (e.g., coolers and circuit energy consumption). The ability to
shutdown infrastructure nodes (or parts of it) or to adapt the transmission
strategy according to the traffic will therefore become an important design
aspect of future wireless architectures. Network infrastructure should be
regarded as a resource that can be occupied or released on demand. However, the
modeling and optimization of such systems are complicated by the potential
interference coupling between active nodes. In this article, we give an
overview on different aspects of this problem. We show how prior knowledge of
traffic patterns can be exploited for optimization. Then, we discuss the
framework of interference functions, which has proved a useful tool for various
types of coupled systems in the literature. Finally, we introduce different
classes of algorithms that have the objective of improving the energy
efficiency by adapting the network configuration to the traffic load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0374</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0374</id><created>2014-06-29</created><authors><author><keyname>Galan-Paez</keyname><forenames>Juan</forenames></author><author><keyname>Borrego-D&#xed;az</keyname><forenames>Joaqu&#xed;n</forenames></author></authors><title>Discovering New Sentiments from the Social Web</title><categories>cs.SI cs.IR</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/135</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A persistent challenge in Complex Systems (CS) research is the
phenomenological reconstruction of systems from raw data. In order to face the
problem, the use of sound features to reason on the system from data processing
is a key step. In the specific case of complex societal systems, sentiment
analysis allows to mirror (part of) the affective dimension. However it is not
reasonable to think that individual sentiment categorization can encompass the
new affective phenomena in digital social networks.
  The present papers addresses the problem of isolating sentiment concepts
which emerge in social networks. In an analogy to Artificial Intelligent
Singularity, we propose the study and analysis of these new complex sentiment
structures and how they are similar to or diverge from classic conceptual
structures associated to sentiment lexicons. The conjecture is that it is
highly probable that hypercomplex sentiment structures -not explained with
human categorizations- emerge from high dynamic social information networks.
Roughly speaking, new sentiment can emerge from the new global nervous systems
as it occurs in humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0375</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0375</id><created>2014-06-29</created><authors><author><keyname>Tsourakakis</keyname><forenames>Charalampos E.</forenames></author></authors><title>Mathematical and Algorithmic Analysis of Network and Biological Data</title><categories>cs.DS cs.DC cs.DM cs.SI q-bio.QM</categories><comments>Doctorial thesis, 306 pages, Carnegie Mellon University 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This dissertation contributes to mathematical and algorithmic problems that
arise in the analysis of network and biological data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0377</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0377</id><created>2014-06-29</created><authors><author><keyname>Dabbish</keyname><forenames>Laura</forenames></author><author><keyname>Stuart</keyname><forenames>Colleen</forenames></author><author><keyname>Tsay</keyname><forenames>Jason</forenames></author><author><keyname>Herbsleb</keyname><forenames>Jim</forenames></author></authors><title>Transparency and Coordination in Peer Production</title><categories>cs.HC</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/128</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines coordination in transparent work environments -
environments where the content of work artifacts, and the actions taken on
these artifacts, are fully visible to organizational members. Our qualitative
study of a community of open source software developers revealed a coordination
system characterized by interest-based, asynchronous interaction and knowledge
transfer. At the core of asynchronous knowledge transfer, lies the concept of
quasi-codification, which occurs when rich process knowledge is implicitly
encoded in work artifacts. Our findings suggest that members are able to more
selectively form dependencies, monitor the trajectory of projects, and make
their work understandable to others which facilitates coordination. We discuss
two important characteristics that enable coordination activities in a
transparent environment: the presence of an imagined audience that dictates the
way artifacts are crafted, and experience within the environment, that allows
individuals to derive knowledge from these artifacts. By showing how
transparency influences coordination, this research challenges previous
conceptions of coordination for complex, collaborative work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0380</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0380</id><created>2014-06-27</created><authors><author><keyname>Trabelsi</keyname><forenames>Imen</forenames></author><author><keyname>Ayed</keyname><forenames>Dorra Ben</forenames></author></authors><title>A Multi Level Data Fusion Approach for Speaker Identification on
  Telephone Speech</title><categories>cs.SD cs.LG</categories><comments>10 pages, 4 figures, International Journal of Signal Processing,
  Image Processing and Pattern Recognition Vol. 6, No. 2, April, 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Several speaker identification systems are giving good performance with clean
speech but are affected by the degradations introduced by noisy audio
conditions. To deal with this problem, we investigate the use of complementary
information at different levels for computing a combined match score for the
unknown speaker. In this work, we observe the effect of two supervised machine
learning approaches including support vectors machines (SVM) and na\&quot;ive bayes
(NB). We define two feature vector sets based on mel frequency cepstral
coefficients (MFCC) and relative spectral perceptual linear predictive
coefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture
Model (GMM). Several ways of combining these information sources give
significant improvements in a text-independent speaker identification task
using a very large telephone degraded NTIMIT database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0381</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0381</id><created>2014-07-01</created><updated>2016-02-17</updated><authors><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Yang</keyname><forenames>Pengkun</forenames></author></authors><title>Minimax rates of entropy estimation on large alphabets via best
  polynomial approximation</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of estimating the Shannon entropy of a distribution over
$k$ elements from $n$ independent samples. We show that the minimax mean-square
error is within universal multiplicative constant factors of $$\Big(\frac{k }{n
\log k}\Big)^2 + \frac{\log^2 k}{n}$$ if $n$ exceeds a constant factor of
$\frac{k}{\log k}$; otherwise there exists no consistent estimator. This
refines the recent result of Valiant-Valiant \cite{VV11} that the minimal
sample size for consistent entropy estimation scales according to
$\Theta(\frac{k}{\log k})$. The apparatus of best polynomial approximation
plays a key role in both the construction of optimal estimators and, via a
duality argument, the minimax lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0385</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0385</id><created>2014-07-01</created><updated>2014-10-17</updated><authors><author><keyname>Horn</keyname><forenames>Alex</forenames></author><author><keyname>Alglave</keyname><forenames>Jade</forenames></author></authors><title>Concurrent Kleene Algebra of Partial Strings</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent Kleene Algebra (CKA) is a recently proposed algebraic structure by
Hoare and collaborators that unifies the laws of concurrent programming. The
unifying power of CKA rests largely on the so-called exchange law that
describes how concurrent and sequential composition operators can be
interchanged. Based on extensive theoretical work on true concurrency in the
past, this paper extends Gischer's pomset model with least fixed point
operators and formalizes the program refinement relation by \'{E}sik's
monotonic bijective morphisms to construct a partial order model of CKA. The
existence of such a model is relevant when we want to prove and disprove
properties about concurrent programs with loops. In particular, it gives a
foundation for the analysis of programs that concurrently access relaxed memory
as shown in subsequent work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0386</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0386</id><created>2014-07-01</created><updated>2014-07-07</updated><authors><author><keyname>Schall</keyname><forenames>Daniel</forenames></author><author><keyname>H&#xe4;rder</keyname><forenames>Theo</forenames></author></authors><title>Energy and Performance-Can a Wimpy-Node Cluster Challenge a Brawny
  Server?</title><categories>cs.DB cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1407.0120</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional DBMS servers are usually over-provisioned for most of their daily
workloads and, because they do not show good energy proportionality, waste a
lot of energy while underutilized. A cluster of small (wimpy) servers, where
the number of nodes can dynamically adjust to the current workload, might offer
better energy characteristics for these workloads. Yet, clusters suffer from
&quot;friction losses&quot; and may not be able to quickly adapt to the workload, whereas
a single, brawny server delivers performance instantaneously. In this paper, we
compare a small cluster of lightweight nodes to a single server in terms of
performance and energy efficiency. We run several benchmarks, consisting of
OLTP and OLAP queries at variable utilization to test the system's ability to
adjust to the workloads. To quantify possible energy saving and its conceivable
drawback on query runtime, we evaluate our implementation on a cluster as well
as on a single, brawny server and compare the results w.r.t. performance and
energy consumption. Our findings confirm that - based on the workload - energy
can be saved without sacrificing too much performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0406</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0406</id><created>2014-07-01</created><updated>2014-09-16</updated><authors><author><keyname>Neurauter</keyname><forenames>Friedrich</forenames><affiliation>University of Innsbruck</affiliation></author><author><keyname>Middeldorp</keyname><forenames>Aart</forenames><affiliation>University of Innsbruck</affiliation></author></authors><title>Polynomial Interpretations over the Natural, Rational and Real Numbers
  Revisited</title><categories>cs.LO</categories><comments>28 pages; special issue of RTA 2010</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  18, 2014) lmcs:853</journal-ref><doi>10.2168/LMCS-10(3:22)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial interpretations are a useful technique for proving termination of
term rewrite systems. They come in various flavors: polynomial interpretations
with real, rational and integer coefficients. As to their relationship with
respect to termination proving power, Lucas managed to prove in 2006 that there
are rewrite systems that can be shown polynomially terminating by polynomial
interpretations with real (algebraic) coefficients, but cannot be shown
polynomially terminating using polynomials with rational coefficients only. He
also proved the corresponding statement regarding the use of rational
coefficients versus integer coefficients. In this article we extend these
results, thereby giving the full picture of the relationship between the
aforementioned variants of polynomial interpretations. In particular, we show
that polynomial interpretations with real or rational coefficients do not
subsume polynomial interpretations with integer coefficients. Our results hold
also for incremental termination proofs with polynomial interpretations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0414</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0414</id><created>2014-07-01</created><authors><author><keyname>Toussaint</keyname><forenames>Marc</forenames></author></authors><title>Newton methods for k-order Markov Constrained Motion Problems</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a documentation of a framework for robot motion optimization that
aims to draw on classical constrained optimization methods. With one exception
the underlying algorithms are classical ones: Gauss-Newton (with adaptive step
size and damping), Augmented Lagrangian, log-barrier, etc. The exception is a
novel any-time version of the Augmented Lagrangian. The contribution of this
framework is to frame motion optimization problems in a way that makes the
application of these methods efficient, especially by defining a very general
class of robot motion problems while at the same time introducing abstractions
that directly reflect the API of the source code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0420</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0420</id><created>2014-07-01</created><updated>2014-07-15</updated><authors><author><keyname>Zick</keyname><forenames>Yair</forenames></author><author><keyname>Chalkiadakis</keyname><forenames>Georgios</forenames></author><author><keyname>Elkind</keyname><forenames>Edith</forenames></author><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author></authors><title>Cooperative Games with Overlapping Coalitions: Charting the Tractability
  Frontier</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many multiagent scenarios, agents distribute resources, such as time or
energy, among several tasks. Having completed their tasks and generated
profits, task payoffs must be divided among the agents in some reasonable
manner. Cooperative games with overlapping coalitions (OCF games) are a recent
framework proposed by Chalkiadakis et al. (2010), generalizing classic
cooperative games to the case where agents may belong to more than one
coalition. Having formed overlapping coalitions and divided profits, some
agents may feel dissatisfied with their share of the profits, and would like to
deviate from the given outcome. However, deviation in OCF games is a
complicated matter: agents may decide to withdraw only some of their weight
from some of the coalitions they belong to; that is, even after deviation, it
is possible that agents will still be involved in tasks with non-deviators.
This means that the desirability of a deviation, and the stability of formed
coalitions, is to a great extent determined by the reaction of non-deviators.
In this work, we explore algorithmic aspects of OCF games, focusing on the core
in OCF games. We study the problem of deciding if the core of an OCF game is
not empty, and whether a core payoff division can be found in polynomial time;
moreover, we identify conditions that ensure that the problem admits polynomial
time algorithms. Finally, we introduce and study a natural class of OCF games,
Linear Bottleneck Games. Interestingly, we show that such games always have a
non-empty core, even assuming a highly lenient reaction to deviations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0423</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0423</id><created>2014-07-01</created><authors><author><keyname>Yanes</keyname><forenames>Adrian</forenames></author></authors><title>Privacy and Anonymity</title><categories>cs.CY cs.CR</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Since the beginning of the digital area, privacy and anonymity have been
impacted drastically (both, positively and negatively), by the different
technologies developed for communications purposes. The broad possibilities
that the Internet offers since its conception, makes it a mandatory target for
those entities that are aiming to know and control the different channels of
communication and the information that flows through. In this paper, we address
the current threats against privacy and anonymity on the Internet, together
with the methods applied against them. In addition, we enumerate the publicly
known entities behind those threats and their motivations. Finally, we analyze
the state of the art concerning the protection of the privacy and anonymity on
the Internet; introducing future lines of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0424</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0424</id><created>2014-07-01</created><authors><author><keyname>Rasekh</keyname><forenames>Amin</forenames></author><author><keyname>Brumbelow</keyname><forenames>Kelly</forenames></author></authors><title>A Dynamic Simulation-Optimization Model for Adaptive Management of Urban
  Water Distribution System Contamination Threats</title><categories>cs.OH cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Urban water distribution systems hold a critical and strategic position in
preserving public health and industrial growth. Despite the ubiquity of these
urban systems, aging infrastructure, and increased risk of terrorism, decision
support models for a timely and adaptive contamination emergency response still
remain at an undeveloped stage. Emergency response is characterized as a
progressive, interactive, and adaptive process that involves parallel
activities of processing streaming information and executing response actions.
This study develops a dynamic decision support model that adaptively simulates
the time-varying emergency environment and tracks changing best health
protection response measures at every stage of an emergency in real-time.
Feedback mechanisms between the contaminated network, emergency managers, and
consumers are incorporated in a dynamic simulation model to capture
time-varying characteristics of an emergency environment. An
evolutionary-computation-based dynamic optimization model is developed to
adaptively identify time-dependant optimal health protection measures during an
emergency. This dynamic simulation-optimization model treats perceived
contaminant source attributes as time-varying parameters to account for
perceived contamination source updates as more data stream in over time.
Performance of the developed dynamic decision support model is analyzed and
demonstrated using a mid-size virtual city that resembles the dynamics and
complexity of real-world urban systems. This adaptive emergency response
optimization model is intended to be a major component of an all-inclusive
cyberinfrastructure for efficient contamination threat management, which is
currently under development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0434</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0434</id><created>2014-07-01</created><updated>2014-07-14</updated><authors><author><keyname>Misra</keyname><forenames>Prasant</forenames></author><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author><author><keyname>Warrior</keyname><forenames>Jay</forenames></author></authors><title>Towards a Practical Architecture for India Centric Internet of Things</title><categories>cs.HC cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An effective architecture for the Internet of Things (IoT), particularly for
an emerging nation like India with limited technology penetration at the
national scale, should be based on tangible technology advances in the present,
practical application scenarios of social and entrepreneurial value, and
ubiquitous capabilities that make the realization of IoT affordable and
sustainable. Humans, data, communication and devices play key roles in the IoT
ecosystem that we perceive. In a push towards this sustainable and practical
IoT Architecture for India, we synthesize ten design paradigms to consider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0439</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0439</id><created>2014-07-01</created><updated>2015-01-13</updated><authors><author><keyname>Liu</keyname><forenames>Haixia</forenames></author><author><keyname>Chan</keyname><forenames>Raymond H.</forenames></author><author><keyname>Yao</keyname><forenames>Yuan</forenames></author></authors><title>Geometric Tight Frame based Stylometry for Art Authentication of van
  Gogh Paintings</title><categories>cs.LG cs.CV</categories><comments>14 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about authenticating genuine van Gogh paintings from forgeries.
The authentication process depends on two key steps: feature extraction and
outlier detection. In this paper, a geometric tight frame and some simple
statistics of the tight frame coefficients are used to extract features from
the paintings. Then a forward stage-wise rank boosting is used to select a
small set of features for more accurate classification so that van Gogh
paintings are highly concentrated towards some center point while forgeries are
spread out as outliers. Numerical results show that our method can achieve
86.08% classification accuracy under the leave-one-out cross-validation
procedure. Our method also identifies five features that are much more
predominant than other features. Using just these five features for
classification, our method can give 88.61% classification accuracy which is the
highest so far reported in literature. Evaluation of the five features is also
performed on two hundred datasets generated by bootstrap sampling with
replacement. The median and the mean are 88.61% and 87.77% respectively. Our
results show that a small set of statistics of the tight frame coefficients
along certain orientations can serve as discriminative features for van Gogh
paintings. It is more important to look at the tail distributions of such
directional coefficients than mean values and standard deviations. It reflects
a highly consistent style in van Gogh's brushstroke movements, where many
forgeries demonstrate a more diverse spread in these features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0440</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0440</id><created>2014-07-01</created><authors><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Almozlino</keyname><forenames>Adam</forenames></author><author><keyname>Inbar</keyname><forenames>Orr</forenames></author><author><keyname>Lo</keyname><forenames>Wei</forenames></author><author><keyname>Provost</keyname><forenames>Shannon</forenames></author></authors><title>Measuring Team Creativity Through Longitudinal Social Signals</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research into human dynamical systems has long sought to identify robust
signals for human behavior. We have discovered a series of social network-based
indicators that are reliable predictors of team creativity and collaborative
innovation. We extract these signals from electronic records of interpersonal
interactions, including e-mail, and face-to-face interaction measured via
sociometric badges. The first of these signals is Rotating Leadership,
measuring the degree to which, over time, actors in a team vary in how central
they are to team's communication network's structure. The second is Rotating
Contribution, which measures the degree to which, over time, actors in a team
vary in the ratio of communications they distribute versus receive. The third
is Prompt Response Time, which measures, over time, the responsiveness of
actors to one another's communications. Finally, we demonstrate the predictive
utility of these signals in a variety of contexts, showing them to be robust to
various methods of evaluating innovation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0442</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0442</id><created>2014-07-01</created><authors><author><keyname>Davtyan</keyname><forenames>Seda</forenames></author><author><keyname>Konwar</keyname><forenames>Kishori M.</forenames></author><author><keyname>Russell</keyname><forenames>Alexander</forenames></author><author><keyname>Shvartsman</keyname><forenames>Alexander A.</forenames></author></authors><title>Technical Report: Dealing with Undependable Workers in Decentralized
  Network Supercomputing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet supercomputing is an approach to solving partitionable,
computation-intensive problems by harnessing the power of a vast number of
interconnected computers. This paper presents a new algorithm for the problem
of using network supercomputing to perform a large collection of independent
tasks, while dealing with undependable processors. The adversary may cause the
processors to return bogus results for tasks with certain probabilities, and
may cause a subset $F$ of the initial set of processors $P$ to crash. The
adversary is constrained in two ways. First, for the set of non-crashed
processors $P-F$, the \emph{average} probability of a processor returning a
bogus result is inferior to $\frac{1}{2}$. Second, the adversary may crash a
subset of processors $F$, provided the size of $P-F$ is bounded from below. We
consider two models: the first bounds the size of $P-F$ by a fractional
polynomial, the second bounds this size by a poly-logarithm. Both models yield
adversaries that are much stronger than previously studied. Our randomized
synchronous algorithm is formulated for $n$ processors and $t$ tasks, with
$n\le t$, where depending on the number of crashes each live processor is able
to terminate dynamically with the knowledge that the problem is solved with
high probability. For the adversary constrained by a fractional polynomial, the
round complexity of the algorithm is
$O(\frac{t}{n^\varepsilon}\log{n}\log{\log{n}})$, its work is $O(t\log{n}
\log{\log{n}})$ and message complexity is $O(n\log{n}\log{\log{n}})$. For the
poly-log constrained adversary, the round complexity is $O(t)$, work is $O(t
n^{\varepsilon})$, %$O(t \, poly \log{n})$, and message complexity is
$O(n^{1+\varepsilon})$ %$O(n \, poly \log{n})$. All bounds are shown to hold
with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0449</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0449</id><created>2014-07-01</created><authors><author><keyname>Farahmand</keyname><forenames>Amir-massoud</forenames></author><author><keyname>Precup</keyname><forenames>Doina</forenames></author><author><keyname>Barreto</keyname><forenames>Andr&#xe9; M. S.</forenames></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames></author></authors><title>Classification-based Approximate Policy Iteration: Experiments and
  Extended Discussions</title><categories>cs.LG cs.SY math.OC stat.ML</categories><msc-class>68T05 (Primary), 93E35, 93E20, 90C40, 49L20 (Secondary)</msc-class><acm-class>I.2.6; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tackling large approximate dynamic programming or reinforcement learning
problems requires methods that can exploit regularities, or intrinsic
structure, of the problem in hand. Most current methods are geared towards
exploiting the regularities of either the value function or the policy. We
introduce a general classification-based approximate policy iteration (CAPI)
framework, which encompasses a large class of algorithms that can exploit
regularities of both the value function and the policy space, depending on what
is advantageous. This framework has two main components: a generic value
function estimator and a classifier that learns a policy based on the estimated
value function. We establish theoretical guarantees for the sample complexity
of CAPI-style algorithms, which allow the policy evaluation step to be
performed by a wide variety of algorithms (including temporal-difference-style
methods), and can handle nonparametric representations of policies. Our bounds
on the estimation error of the performance loss are tighter than existing
results. We also illustrate this approach empirically on several problems,
including a large HIV control task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0454</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0454</id><created>2014-07-02</created><authors><author><keyname>Alsubaiee</keyname><forenames>Sattam</forenames></author><author><keyname>Altowim</keyname><forenames>Yasser</forenames></author><author><keyname>Altwaijry</keyname><forenames>Hotham</forenames></author><author><keyname>Behm</keyname><forenames>Alexander</forenames></author><author><keyname>Borkar</keyname><forenames>Vinayak</forenames></author><author><keyname>Bu</keyname><forenames>Yingyi</forenames></author><author><keyname>Carey</keyname><forenames>Michael</forenames></author><author><keyname>Cetindil</keyname><forenames>Inci</forenames></author><author><keyname>Cheelangi</keyname><forenames>Madhusudan</forenames></author><author><keyname>Faraaz</keyname><forenames>Khurram</forenames></author><author><keyname>Gabrielova</keyname><forenames>Eugenia</forenames></author><author><keyname>Grover</keyname><forenames>Raman</forenames></author><author><keyname>Heilbron</keyname><forenames>Zachary</forenames></author><author><keyname>Kim</keyname><forenames>Young-Seok</forenames></author><author><keyname>Li</keyname><forenames>Chen</forenames></author><author><keyname>Li</keyname><forenames>Guangqiang</forenames></author><author><keyname>Ok</keyname><forenames>Ji Mahn</forenames></author><author><keyname>Onose</keyname><forenames>Nicola</forenames></author><author><keyname>Pirzadeh</keyname><forenames>Pouria</forenames></author><author><keyname>Tsotras</keyname><forenames>Vassilis</forenames></author><author><keyname>Vernica</keyname><forenames>Rares</forenames></author><author><keyname>Wen</keyname><forenames>Jian</forenames></author><author><keyname>Westmann</keyname><forenames>Till</forenames></author></authors><title>AsterixDB: A Scalable, Open Source BDMS</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AsterixDB is a new, full-function BDMS (Big Data Management System) with a
feature set that distinguishes it from other platforms in today's open source
Big Data ecosystem. Its features make it well-suited to applications like web
data warehousing, social data storage and analysis, and other use cases related
to Big Data. AsterixDB has a flexible NoSQL style data model; a query language
that supports a wide range of queries; a scalable runtime; partitioned,
LSM-based data storage and indexing (including B+-tree, R-tree, and text
indexes); support for external as well as natively stored data; a rich set of
built-in types; support for fuzzy, spatial, and temporal types and queries; a
built-in notion of data feeds for ingestion of data; and transaction support
akin to that of a NoSQL store.
  Development of AsterixDB began in 2009 and led to a mid-2013 initial open
source release. This paper is the first complete description of the resulting
open source AsterixDB system. Covered herein are the system's data model, its
query language, and its software architecture. Also included are a summary of
the current status of the project and a first glimpse into how AsterixDB
performs when compared to alternative technologies, including a parallel
relational DBMS, a popular NoSQL store, and a popular Hadoop-based SQL data
analytics platform, for things that both technologies can do. Also included is
a brief description of some initial trials that the system has undergone and
the lessons learned (and plans laid) based on those early &quot;customer&quot;
engagements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0455</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0455</id><created>2014-07-02</created><authors><author><keyname>Bu</keyname><forenames>Yingyi</forenames></author><author><keyname>Borkar</keyname><forenames>Vinayak</forenames></author><author><keyname>Jia</keyname><forenames>Jianfeng</forenames></author><author><keyname>Carey</keyname><forenames>Michael J.</forenames></author><author><keyname>Condie</keyname><forenames>Tyson</forenames></author></authors><title>Pregelix: Big(ger) Graph Analytics on A Dataflow Engine</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing need for distributed graph processing systems that are
capable of gracefully scaling to very large graph datasets. Unfortunately, this
challenge has not been easily met due to the intense memory pressure imposed by
process-centric, message passing designs that many graph processing systems
follow. Pregelix is a new open source distributed graph processing system that
is based on an iterative dataflow design that is better tuned to handle both
in-memory and out-of-core workloads. As such, Pregelix offers improved
performance characteristics and scaling properties over current open source
systems (e.g., we have seen up to 15x speedup compared to Apache Giraph and up
to 35x speedup compared to distributed GraphLab), and makes more effective use
of available machine resources to support Big(ger) Graph Analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0462</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0462</id><created>2014-07-02</created><authors><author><keyname>Balodhi</keyname><forenames>Meenu</forenames></author><author><keyname>Bijalwan</keyname><forenames>Vishwanath</forenames></author><author><keyname>Negi</keyname><forenames>Banit</forenames></author></authors><title>Zigbee &amp; IEEE 802.11b(WLAN)coexistence in ubiquitous network environment</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IEEE 802.15.4 standard is used for low rate, short distance wireless
communication. However due to its low power it is greatly affected by
interference provided by other wireless technology working on same
ISM(industrial, scientific &amp; medical) band, such as IEEE 802.11b/g &amp; Bluetooth.
In ubiquitous network environment we have two different heterogeneous
communication systems coexists in single place. In this paper we use an
analytic model and an experimental set up for the coexistence among Zigbee,
&amp;WLAN. The model focuses on two aspects, namely Power &amp; Timing. These two
jointly impacts on the performance of IEEE 802.15.4 wireless network. Zigbee is
main component of wireless sensor network, so therefore to study about
performance of network should be need in interference environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0466</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0466</id><created>2014-07-02</created><authors><author><keyname>Guan</keyname><forenames>Li</forenames></author><author><keyname>Hao</keyname><forenames>Bibo</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>How did the Suicide Act and Speak Differently Online? Behavioral and
  Linguistic Features of China's Suicide Microblog Users</title><categories>cs.SI</categories><comments>17 pages, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Suicide issue is of great concern in China. Social media provides
an active approach to understanding suicide individuals in terms of their
behavior and language use. Aims: This study investigates how suicide Microblog
users in China act and speak differently on social media from others. Methods:
Hypothesis testing in behavioral and linguistic features was performed between
a target group of 33 Chinese Microblog users who have committed suicide and a
control group of 30 active users without suicidal ideation. Results: Suicide
group significantly outnumbered control group in the extent of openly published
posts and self-reference, and the intensity of using 7 word categories:
negative words/social process words/cognitive process words/emotion process
words/negative emotion words/exclusive words/physiological process words.
Limitations: Information collection and confirmation of suicide users remain
difficult. Conclusions: It is revealed that suicide people vary from others in
certain behavioral and linguistic features in social media. This study fills
the niche of suicide studies by noting specified indicators of suicide ideation
for Chinese individuals online, providing insights of constructing an online
alarm system for early detection and intervention of suicidal individuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0474</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0474</id><created>2014-07-02</created><authors><author><keyname>Bi</keyname><forenames>Suzhi</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Recent Advances in Joint Wireless Energy and Information Transfer</title><categories>cs.NI cs.IT math.IT</categories><comments>Conference submission accepted by ITW 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we provide an overview of the recent advances in
microwave-enabled wireless energy transfer (WET) technologies and their
applications in wireless communications. Specifically, we divide our
discussions into three parts. First, we introduce the state-of-the-art WET
technologies and the signal processing techniques to maximize the energy
transfer efficiency. Then, we discuss an interesting paradigm named
simultaneous wireless information and power transfer (SWIPT), where energy and
information are jointly transmitted using the same radio waveform. At last, we
review the recent progress in wireless powered communication networks (WPCN),
where wireless devices communicate using the power harvested by means of WET.
Extensions and future directions are also discussed in each of these areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0481</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0481</id><created>2014-07-02</created><authors><author><keyname>Kotis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Athanasakis</keyname><forenames>Iraklis</forenames></author><author><keyname>Vouros</keyname><forenames>George</forenames></author></authors><title>Semantic Integration &amp; Single-Site Opening of Multiple Governmental Data
  Sources</title><categories>cs.IR cs.AI cs.DB</categories><comments>21 pages, 7 figures, live demo at
  http://www.samos.gr/apps/s3-ai/eGovTicketApp.xhtml</comments><acm-class>H.2.5; H.3.4; H.3.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In many cases, government data is still &quot;locked&quot; in several &quot;data silos&quot;,
even within the boundaries of a single (inter-)national public organization
with disparate and distributed organizational units and departments spread
across multiple sites. Opening data and enabling its unified querying from a
single site in an efficient and effective way is a semantic application
integration and open government data challenge. This paper describes how NARA
is using Semantic Web technology to implement an application integration
approach within the boundaries of its organization via opening and querying
multiple governmental data sources from a single site. The generic approach
proposed, namely S3-AI, provides support to answering unified,
ontology-mediated, federated queries to data produced and exploited by
disparate applications, while these are being located in different
organizational sites. S3-AI preserves ownership, autonomy and independency of
applications and data. The paper extensively demonstrates S3-AI, using the D2RQ
and Fuseki technologies, for addressing the needs of a governmental &quot;IT
helpdesk support&quot; case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0491</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0491</id><created>2014-07-02</created><authors><author><keyname>Razgon</keyname><forenames>Igor</forenames></author></authors><title>No small nondeterministic read-once branching programs for CNFs of
  bounded treewidth</title><categories>cs.CC cs.DS cs.LO math.CO</categories><comments>Prepared as a 12 pages conference version, thus some proofs are
  postponed to the appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, given a parameter $k$, we demonstrate an infinite class of
{\sc cnf}s of treewidth at most $k$ of their primary graphs such that the
equivalent nondeterministic read-once branching programs ({\sc nrobp}s) are of
size at least $n^{ck}$ for some universal constant $c$. Thus we rule out the
possibility of fixed-parameter space complexity of {\sc nrobp}s parameterized
by the smallest treewidth of the equivalent {\sc cnf}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0506</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0506</id><created>2014-07-02</created><authors><author><keyname>Misra</keyname><forenames>Prasant</forenames></author><author><keyname>Mohini</keyname><forenames>Santoshini Kumari</forenames></author><author><keyname>Mishra</keyname><forenames>Saroj Kumar</forenames></author></authors><title>The Design and Implementation of an ANN-based Non-linearity Compensator
  of LVDT Sensor</title><categories>cs.SY</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear variable differential transformer (LVDT) sensors are used in
engineering applications due to their fine-grained measurements. However, these
sensors exhibit non-linear input-output characteristics, which decrease the
reliability of the sensing system. The contribution of this article is
three-fold. First, it provides an experimental study of the non-linearity
problem of the LVDT. Second, it proposes the design of a functional link
artificial neural network (FLANN) based non-linearity compensator model for
overcoming it. Finally, it validates the feasibility of the solution in
simulation, and presents a proof-of-concept hardware implementation on a
SPARTAN-II (PQ208)FPGA using VHDL in Xilinx. The model has been mathematically
derived, and its simulation study has been presented that achieves nearly 100%
linearity range. The result obtained from the FPGA implementation is in good
agreement with the simulation result, which establishes its actualization as
part of a general manufacturing process for linearity compensated LVDT sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0511</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0511</id><created>2014-07-02</created><authors><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Moloudi</keyname><forenames>Saeedeh</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author></authors><title>Braided Convolutional Codes -- A Class of Spatially Coupled Turbo-Like
  Codes</title><categories>cs.IT math.IT</categories><comments>Invited paper, International Conference on Signal Processing and
  Communications, SPCOM 2014, Bangalore, India, July 22-25, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the impact of spatial coupling on the
thresholds of turbo-like codes. Parallel concatenated and serially concatenated
convolutional codes as well as braided convolutional codes (BCCs) are compared
by means of an exact density evolution (DE) analysis for the binary erasure
channel (BEC). We propose two extensions of the original BCC ensemble to
improve its threshold and demonstrate that their BP thresholds approach the
maximum-a-posteriori (MAP) threshold of the uncoupled ensemble. A comparison of
the different ensembles shows that parallel concatenated ensembles can be
outperformed by both serially concatenated and BCC ensembles, although they
have the best BP thresholds in the uncoupled case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0516</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0516</id><created>2014-07-02</created><authors><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Moloudi</keyname><forenames>Saeedeh</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author></authors><title>Spatially Coupled Turbo Codes: Principles and Finite Length Performance</title><categories>cs.IT math.IT</categories><comments>Invited paper, IEEE Int. Symp. Wireless Communications Systems
  (ISWCS), Aug. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give an overview of spatially coupled turbo codes (SC-TCs),
the spatial coupling of parallel and serially concatenated convolutional codes,
recently introduced by the authors. For presentation purposes, we focus on
spatially coupled serially concatenated codes (SC-SCCs). We review the main
principles of SC-TCs and discuss their exact density evolution (DE) analysis on
the binary erasure channel. We also consider the construction of a family of
rate-compatible SC-SCCs with simple 4-state component encoders. For all
considered code rates, threshold saturation of the belief propagation (BP) to
the maximum a posteriori threshold of the uncoupled ensemble is demonstrated,
and it is shown that the BP threshold approaches the Shannon limit as the
coupling memory increases. Finally we give some simulation results for finite
lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0519</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0519</id><created>2014-07-02</created><authors><author><keyname>Palasek</keyname><forenames>Stan</forenames></author></authors><title>Non-Cooperativity in Bayesian Social Learning</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 8 figures</comments><msc-class>91</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a Bayesian model for social learning of a random variable in
which agents might observe each other over a directed network. The outcomes
produced are compared to those from a model in which observations occur
randomly over a complete graph. In both cases we observe a nontrivial level of
observation which maximizes learning, though individuals have strong incentive
to defect from the societal optimum. The implications of such competition over
information commons are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0521</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0521</id><created>2014-07-02</created><updated>2014-10-31</updated><authors><author><keyname>Pretti</keyname><forenames>Marco</forenames></author></authors><title>Lowering the error floor of Gallager codes: a statistical-mechanical
  view</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>17 pages, 4 figures; minor revisions, results unchanged</comments><journal-ref>J. Stat. Mech. (2014) P10042</journal-ref><doi>10.1088/1742-5468/2014/10/P10042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of error correction for Gallager's low-density parity-check codes
is famously equivalent to that of computing marginal Boltzmann probabilities
for an Ising-like model with multispin interactions in a non-uniform magnetic
field. Since the graph of interactions is locally a tree, the solution is very
well approximated by a generalized mean-field (Bethe-Peierls) approximation.
Belief propagation (BP) and similar iterative algorithms are an efficient way
to perform the calculation, but they sometimes fail to converge, or converge to
non-codewords, giving rise to a non-negligible residual error probability
(error floor). On the other hand, provably-convergent algorithms are far too
complex to be implemented in a real decoder. In this work we consider the
application of the probability-damping technique, which can be regarded either
as a variant of BP, from which it retains the property of low complexity, or as
an approximation of a provably-convergent algorithm, from which it is expected
to inherit better convergence properties. We investigate the algorithm
behaviour on a real instance of Gallager code, and compare the results with
state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0522</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0522</id><created>2014-07-02</created><authors><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Starikovskaya</keyname><forenames>Tatiana</forenames></author><author><keyname>Vildh&#xf8;j</keyname><forenames>Hjalte Wedel</forenames></author></authors><title>Sublinear Space Algorithms for the Longest Common Substring Problem</title><categories>cs.DS</categories><comments>Accepted to 22nd European Symposium on Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $m$ documents of total length $n$, we consider the problem of finding a
longest string common to at least $d \geq 2$ of the documents. This problem is
known as the \emph{longest common substring (LCS) problem} and has a classic
$O(n)$ space and $O(n)$ time solution (Weiner [FOCS'73], Hui [CPM'92]).
However, the use of linear space is impractical in many applications. In this
paper we show that for any trade-off parameter $1 \leq \tau \leq n$, the LCS
problem can be solved in $O(\tau)$ space and $O(n^2/\tau)$ time, thus providing
the first smooth deterministic time-space trade-off from constant to linear
space. The result uses a new and very simple algorithm, which computes a
$\tau$-additive approximation to the LCS in $O(n^2/\tau)$ time and $O(1)$
space. We also show a time-space trade-off lower bound for deterministic
branching programs, which implies that any deterministic RAM algorithm solving
the LCS problem on documents from a sufficiently large alphabet in $O(\tau)$
space must use $\Omega(n\sqrt{\log(n/(\tau\log n))/\log\log(n/(\tau\log n)})$
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0524</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0524</id><created>2014-07-02</created><updated>2016-02-25</updated><authors><author><keyname>Hakkarainen</keyname><forenames>Aki</forenames></author><author><keyname>Werner</keyname><forenames>Janis</forenames></author><author><keyname>Dandekar</keyname><forenames>Kapil R.</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Analysis and Augmented Spatial Processing for Uplink OFDMA MU-MIMO
  Receiver with Transceiver I/Q Imbalance and External Interference</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Wireless Communications, 2016, 18 pages, 13
  figures, 4 tables</comments><doi>10.1109/TWC.2016.2521382</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address receiver (RX) signal processing in MIMO systems under
in-phase/quadrature (I/Q) imbalance, which causes cross-talk of
mirror-subcarriers in OFDM systems. We extend the typically reported
single-user studies to uplink OFDMA-based multiuser MIMO, with simultaneous
user multiplexing in frequency and spatial domains. We also incorporate
multiple external interferers, for modeling challenging conditions in
heterogeneous networks. In the signal processing developments, we exploit the
augmented subcarrier processing, which processes each subcarrier jointly with
its counterpart at the image subcarrier, and jointly across RX antennas.
Furthermore, we derive an augmented LMMSE RX. The novel approach integrates the
I/Q imbalance mitigation, interference suppression and data stream separation
into a single processing stage, thus avoiding a separate transceiver
calibration. Our numerical results show the signal-to-interference-plus-noise
ratio and symbol-error rate of an arbitrary data stream after RX processing as
a function of different system parameters. The per-subcarrier processing is
shown to suffer heavily under I/Q imbalances, and being particularly sensitive
to external interferers, whereas the augmented method provides efficient data
stream separation and interference suppression. Finally, we extend the studies
to massive MIMO framework and show that the per-subcarrier processing still
suffers from performance degradation, whereas the augmented approach can fully
exploit the array gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0536</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0536</id><created>2014-07-02</created><updated>2014-10-21</updated><authors><author><keyname>Smiljkovikj</keyname><forenames>Katerina</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Gavrilovska</keyname><forenames>Liljana</forenames></author></authors><title>Analysis of the Decoupled Access for Downlink and Uplink in Wireless
  Heterogeneous Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>4 pages, 3 figures, submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless cellular networks evolve towards a heterogeneous infrastructure,
featuring multiple types of Base Stations (BSs), such as Femto BSs (FBSs) and
Macro BSs (MBSs). A wireless device observes multiple points (BSs) through
which it can access the infrastructure and it may choose to receive the
downlink (DL) traffic from one BS and send uplink (UL) traffic through another
BS. Such a situation is referred to as decoupled DL/UL access. Using the
framework of stochastic geometry, we derive the association probability for
DL/UL. In order to maximize the average received power, as the relative density
of FBSs initially increases, a large fraction of devices chooses decoupled
access, i.e. receive from a MBS in DL and transmit through a FBS in UL. We
analyze the impact that this type of association has on the average throughput
in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0547</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0547</id><created>2014-07-02</created><authors><author><keyname>Phillips</keyname><forenames>Mark</forenames></author><author><keyname>Ko</keyname><forenames>Lauren</forenames></author></authors><title>Understanding Repository Growth at the University of North Texas: A Case
  Study</title><categories>cs.DL</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Over the past decade the University of North Texas Libraries (UNTL) has
developed a sizable digital library infrastructure for use in carrying out its
core mission to the students, faculty, staff and associated communities of the
university. This repository of content offers countless research possibilities
for end users across the Internet when it is discovered and used in research,
scholarship, entertainment, and lifelong learning. The characteristics of the
repository itself provide insight into the workings of a modern digital library
infrastructure, how it was created, how often it is updated, or how often it is
modified. In that vein, the authors created a dataset comprised of information
extracted from the UNT Libraries' archival repository Coda and analyzed this
dataset in order to demonstrate the value and insights that can be gained from
sharing repository characteristics more broadly. This case study presents the
findings from an analysis of this dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0549</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0549</id><created>2014-07-02</created><authors><author><keyname>Payer</keyname><forenames>Mathias</forenames></author><author><keyname>Barresi</keyname><forenames>Antonio</forenames></author><author><keyname>Gross</keyname><forenames>Thomas R.</forenames></author></authors><title>Lockdown: Dynamic Control-Flow Integrity</title><categories>cs.CR cs.PL</categories><comments>ETH Technical Report</comments><doi>10.3929/ethz-a-010171214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications written in low-level languages without type or memory safety are
especially prone to memory corruption. Attackers gain code execution
capabilities through such applications despite all currently deployed defenses
by exploiting memory corruption vulnerabilities. Control-Flow Integrity (CFI)
is a promising defense mechanism that restricts open control-flow transfers to
a static set of well-known locations. We present Lockdown, an approach to
dynamic CFI that protects legacy, binary-only executables and libraries.
Lockdown adaptively learns the control-flow graph of a running process using
information from a trusted dynamic loader. The sandbox component of Lockdown
restricts interactions between different shared objects to imported and
exported functions by enforcing fine-grained CFI checks. Our prototype
implementation shows that dynamic CFI results in low performance overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0566</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0566</id><created>2014-07-02</created><updated>2014-07-10</updated><authors><author><keyname>Staiano</keyname><forenames>Jacopo</forenames></author><author><keyname>Oliver</keyname><forenames>Nuria</forenames></author><author><keyname>Lepri</keyname><forenames>Bruno</forenames></author><author><keyname>de Oliveira</keyname><forenames>Rodrigo</forenames></author><author><keyname>Caraviello</keyname><forenames>Michele</forenames></author><author><keyname>Sebe</keyname><forenames>Nicu</forenames></author></authors><title>Money Walks: A Human-Centric Study on the Economics of Personal Mobile
  Data</title><categories>cs.HC cs.CY</categories><comments>15 pages, 2 figures. To appear in ACM International Joint Conference
  on Pervasive and Ubiquitous Computing (Ubicomp 2014)</comments><acm-class>K.6.0</acm-class><doi>10.1145/2632048.2632074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of a myriad of mobile apps which collect personally
identifiable information (PII) and a prospective market place of personal data,
we investigate a user-centric monetary valuation of mobile PII. During a 6-week
long user study in a living lab deployment with 60 participants, we collected
their daily valuations of 4 categories of mobile PII (communication, e.g.
phonecalls made/received, applications, e.g. time spent on different apps,
location and media, photos taken) at three levels of complexity (individual
data points, aggregated statistics and processed, i.e. meaningful
interpretations of the data). In order to obtain honest valuations, we employ a
reverse second price auction mechanism. Our findings show that the most
sensitive and valued category of personal information is location. We report
statistically significant associations between actual mobile usage, personal
dispositions, and bidding behavior. Finally, we outline key implications for
the design of mobile services and future markets of personal data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0576</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0576</id><created>2014-07-02</created><authors><author><keyname>Gomes</keyname><forenames>Jorge</forenames></author><author><keyname>Mariano</keyname><forenames>Pedro</forenames></author><author><keyname>Christensen</keyname><forenames>Anders Lyhne</forenames></author></authors><title>Novelty Search in Competitive Coevolution</title><categories>cs.NE cs.MA</categories><comments>To appear in 13th International Conference on Parallel Problem
  Solving from Nature (PPSN 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main motivations for the use of competitive coevolution systems is
their ability to capitalise on arms races between competing species to evolve
increasingly sophisticated solutions. Such arms races can, however, be hard to
sustain, and it has been shown that the competing species often converge
prematurely to certain classes of behaviours. In this paper, we investigate if
and how novelty search, an evolutionary technique driven by behavioural
novelty, can overcome convergence in coevolution. We propose three methods for
applying novelty search to coevolutionary systems with two species: (i) score
both populations according to behavioural novelty; (ii) score one population
according to novelty, and the other according to fitness; and (iii) score both
populations with a combination of novelty and fitness. We evaluate the methods
in a predator-prey pursuit task. Our results show that novelty-based approaches
can evolve a significantly more diverse set of solutions, when compared to
traditional fitness-based coevolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0577</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0577</id><created>2014-07-02</created><authors><author><keyname>Gomes</keyname><forenames>Jorge</forenames></author><author><keyname>Mariano</keyname><forenames>Pedro</forenames></author><author><keyname>Christensen</keyname><forenames>Anders Lyhne</forenames></author></authors><title>Systematic Derivation of Behaviour Characterisations in Evolutionary
  Robotics</title><categories>cs.NE cs.MA cs.RO</categories><comments>To appear in 14th International Conference on the Synthesis and
  Simulation of Living Systems (ALife 14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary techniques driven by behavioural diversity, such as novelty
search, have shown significant potential in evolutionary robotics. These
techniques rely on priorly specified behaviour characterisations to estimate
the similarity between individuals. Characterisations are typically defined in
an ad hoc manner based on the experimenter's intuition and knowledge about the
task. Alternatively, generic characterisations based on the sensor-effector
values of the agents are used. In this paper, we propose a novel approach that
allows for systematic derivation of behaviour characterisations for
evolutionary robotics, based on a formal description of the agents and their
environment. Systematically derived behaviour characterisations (SDBCs) go
beyond generic characterisations in that they can contain task-specific
features related to the internal state of the agents, environmental features,
and relations between them. We evaluate SDBCs with novelty search in three
simulated collective robotics tasks. Our results show that SDBCs yield a
performance comparable to the task-specific characterisations, in terms of both
solution quality and behaviour space exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0597</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0597</id><created>2014-07-02</created><updated>2014-11-07</updated><authors><author><keyname>Dall'Anese</keyname><forenames>Emiliano</forenames></author><author><keyname>Dhople</keyname><forenames>Sairaj V.</forenames></author><author><keyname>Johnson</keyname><forenames>Brian B.</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Optimal Dispatch of Residential Photovoltaic Inverters Under Forecasting
  Uncertainties</title><categories>math.OC cs.SY</categories><comments>To appear in the IEEE Journal of Photovoltaics in December 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efforts to ensure reliable operation of existing low-voltage distribution
systems with high photovoltaic (PV) generation have focused on the possibility
of inverters providing ancillary services such as active power curtailment and
reactive power compensation. Major benefits include the possibility of averting
overvoltages, which may otherwise be experienced when PV generation exceeds the
demand. This paper deals with ancillary service procurement in the face of
solar irradiance forecasting errors. In particular, assuming that the
forecasted PV irradiance can be described by a random variable with known
(empirical) distribution, the proposed uncertainty-aware optimal inverter
dispatch (OID) framework indicates which inverters should provide ancillary
services with a guaranteed a-priori risk level of PV generation surplus. To
capture forecasting errors, and strike a balance between risk of overvoltages
and (re)active power reserves, the concept of conditional value-at-risk is
advocated. Due to AC power balance equations and binary inverter selection
variables, the formulated OID involves the solution of a nonconvex
mixed-integer nonlinear program. However, a computationally-affordable convex
relaxation is derived by leveraging sparsity-promoting regularization
approaches and semidefinite relaxation techniques. The proposed scheme is
tested using real-world PV-generation and load-profile data for an illustrative
low-voltage residential distribution system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0611</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0611</id><created>2014-07-02</created><authors><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need?</title><categories>stat.ML cs.LG cs.NE</categories><proxy>ccsd</proxy><journal-ref>10th International Workshop on Self Organizing Maps, WSSOM 2014,
  Mittweida : Germany (2014)</journal-ref><doi>10.1007/978-3-319-07695-9_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In numerous applicative contexts, data are too rich and too complex to be
represented by numerical vectors. A general approach to extend machine learning
and data mining techniques to such data is to really on a dissimilarity or on a
kernel that measures how different or similar two objects are. This approach
has been used to define several variants of the Self Organizing Map (SOM). This
paper reviews those variants in using a common set of notations in order to
outline differences and similarities between them. It discusses the advantages
and drawbacks of the variants, as well as the actual relevance of the
dissimilarity/kernel SOM for practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0612</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0612</id><created>2014-07-02</created><authors><author><keyname>Boull&#xe9;</keyname><forenames>Marc</forenames><affiliation>SAMM</affiliation></author><author><keyname>Guigour&#xe8;s</keyname><forenames>Romain</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Nonparametric Hierarchical Clustering of Functional Data</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><journal-ref>Advances in Knowledge Discovery and Management, Guillet, Fabrice
  and Pinaud, Bruno and Venturini, Gilles and Zighed, Djamel Abdelkader (Ed.)
  (2014) 15-35</journal-ref><doi>10.1007/978-3-319-02999-3_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we deal with the problem of curves clustering. We propose a
nonparametric method which partitions the curves into clusters and discretizes
the dimensions of the curve points into intervals. The cross-product of these
partitions forms a data-grid which is obtained using a Bayesian model selection
approach while making no assumptions regarding the curves. Finally, a
post-processing technique, aiming at reducing the number of clusters in order
to improve the interpretability of the clustering, is proposed. It consists in
optimally merging the clusters step by step, which corresponds to an
agglomerative hierarchical classification whose dissimilarity measure is the
variation of the criterion. Interestingly this measure is none other than the
sum of the Kullback-Leibler divergences between clusters distributions before
and after the merges. The practical interest of the approach for functional
data exploratory analysis is presented and compared with an alternative
approach on an artificial and a real world data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0613</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0613</id><created>2014-07-02</created><authors><author><keyname>Scholz</keyname><forenames>Christoph</forenames></author><author><keyname>Illig</keyname><forenames>Jens</forenames></author><author><keyname>Atzmueller</keyname><forenames>Martin</forenames></author><author><keyname>Stumme</keyname><forenames>Gerd</forenames></author></authors><title>On the Predictability of Talk Attendance at Academic Conferences</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the prediction of real-world talk attendances at
academic conferences with respect to different influence factors. We study the
predictability of talk attendances using real-world tracked face-to-face
contacts. Furthermore, we investigate and discuss the predictive power of user
interests extracted from the users' previous publications. We apply Hybrid
Rooted PageRank, a state-of-the-art unsupervised machine learning method that
combines information from different sources. Using this method, we analyze and
discuss the predictive power of contact and interest networks separately and in
combination. We find that contact and similarity networks achieve comparable
results, and that combinations of different networks can only to a limited
extend help to improve the prediction quality. For our experiments, we analyze
the predictability of talk attendance at the ACM Conference on Hypertext and
Hypermedia 2011 collected using the conference management system Conferator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0614</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0614</id><created>2014-07-02</created><updated>2015-03-01</updated><authors><author><keyname>Rabanca</keyname><forenames>George</forenames></author><author><keyname>Vigan</keyname><forenames>Ivo</forenames></author></authors><title>Covering the Boundary of a Simple Polygon with Geodesic Unit Disks</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of covering the boundary of a simple polygon on n
vertices using the minimum number of geodesic unit disks. We present an O(n
\log^2 n+k) time 2-approximation algorithm for finding the centers of the
disks, with k denoting the number centers found by the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0622</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0622</id><created>2014-07-01</created><authors><author><keyname>Jahanbakhsh</keyname><forenames>Kazem</forenames></author><author><keyname>Moon</keyname><forenames>Yumi</forenames></author></authors><title>The Predictive Power of Social Media: On the Predictability of U.S.
  Presidential Elections using Twitter</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter as a new form of social media potentially contains useful information
that opens new opportunities for content analysis on tweets. This paper
examines the predictive power of Twitter regarding the US presidential election
of 2012. For this study, we analyzed 32 million tweets regarding the US
presidential election by employing a combination of machine learning
techniques. We devised an advanced classifier for sentiment analysis in order
to increase the accuracy of Twitter content analysis. We carried out our
analysis by comparing Twitter results with traditional opinion polls. In
addition, we used the Latent Dirichlet Allocation model to extract the
underlying topical structure from the selected tweets. Our results show that we
can determine the popularity of candidates by running sentiment analysis. We
can also uncover candidates popularities in the US states by running the
sentiment analysis algorithm on geo-tagged tweets. To the best of our
knowledge, no previous work in the field has presented a systematic analysis of
a considerable number of tweets employing a combination of analysis techniques
by which we conducted this study. Thus, our results aptly suggest that Twitter
as a well-known social medium is a valid source in predicting future events
such as elections. This implies that understanding public opinions and trends
via social media in turn allows us to propose a cost- and time-effective way
not only for spreading and sharing information, but also for predicting future
events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0623</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0623</id><created>2014-07-02</created><updated>2015-05-28</updated><authors><author><keyname>Ballan</keyname><forenames>Lamberto</forenames></author><author><keyname>Bertini</keyname><forenames>Marco</forenames></author><author><keyname>Serra</keyname><forenames>Giuseppe</forenames></author><author><keyname>Del Bimbo</keyname><forenames>Alberto</forenames></author></authors><title>A Data-Driven Approach for Tag Refinement and Localization in Web Videos</title><categories>cs.CV cs.IR cs.MM</categories><comments>Preprint submitted to Computer Vision and Image Understanding (CVIU)</comments><doi>10.1016/j.cviu.2015.05.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tagging of visual content is becoming more and more widespread as web-based
services and social networks have popularized tagging functionalities among
their users. These user-generated tags are used to ease browsing and
exploration of media collections, e.g. using tag clouds, or to retrieve
multimedia content. However, not all media are equally tagged by users. Using
the current systems is easy to tag a single photo, and even tagging a part of a
photo, like a face, has become common in sites like Flickr and Facebook. On the
other hand, tagging a video sequence is more complicated and time consuming, so
that users just tag the overall content of a video. In this paper we present a
method for automatic video annotation that increases the number of tags
originally provided by users, and localizes them temporally, associating tags
to keyframes. Our approach exploits collective knowledge embedded in
user-generated tags and web sources, and visual similarity of keyframes and
images uploaded to social sites like YouTube and Flickr, as well as web sources
like Google and Bing. Given a keyframe, our method is able to select on the fly
from these visual sources the training exemplars that should be the most
relevant for this test sample, and proceeds to transfer labels across similar
images. Compared to existing video tagging approaches that require training
classifiers for each tag, our system has few parameters, is easy to implement
and can deal with an open vocabulary scenario. We demonstrate the approach on
tag refinement and localization on DUT-WEBV, a large dataset of web videos, and
show state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0628</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0628</id><created>2014-07-02</created><authors><author><keyname>Gual&#xe0;</keyname><forenames>Davide Bil&#xf2; Luciano</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Proietti</keyname><forenames>Guido</forenames></author></authors><title>Exact and approximate algorithms for movement problems on (special
  classes of) graphs</title><categories>cs.DS cs.CC</categories><comments>26 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a large collection of objects (e.g., robots, sensors, etc.) has to be
deployed in a given environment, it is often required to plan a coordinated
motion of the objects from their initial position to a final configuration
enjoying some global property. In such a scenario, the problem of minimizing
some function of the distance travelled, and therefore energy consumption, is
of vital importance. In this paper we study several motion planning problems
that arise when the objects must be moved on a graph, in order to reach certain
goals which are of interest for several network applications. Among the others,
these goals include broadcasting messages and forming connected or
interference-free networks. We study these problems with the aim of minimizing
a number of natural measures such as the average/overall distance travelled,
the maximum distance travelled, or the number of objects that need to be moved.
To this respect, we provide several approximability and inapproximability
results, most of which are tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0632</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0632</id><created>2014-07-02</created><authors><author><keyname>Gautam</keyname><forenames>Piyush</forenames></author></authors><title>Development of Tool for Mapping Conventional Circuit to Reversible Logic</title><categories>cs.ET</categories><comments>71 Pages, Dissertation Report. arXiv admin note: text overlap with
  arXiv:1403.2686 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decades, great achievements have been made in the development of
computing machines. However, due to exponential growth of transistor density
and in particular due to tremendously increasing power consumption, researchers
expect that &quot;Conventional Technologies&quot; like Complementary Metal-Oxide
Semiconductor will reach their limits in near future. To further satisfy the
needs for more computational power, speed, less size etc. alternatives are
needed. Reversible Computation is the emerging field and alternative of
conventional technologies. Reversible Computation is emerging as a promising
solution and likely to work on extremely low power technologies and offer high
speed computations. The reversibility retains the capability to retrieve the
input data from output and minimizes heat dissipation. As migration to new
technology leave a lot of work done in current technology will make the
acceptability difficult. One side familiarly with new technology and other side
transformation of old circuit designs to new technology will pose a challenge
to designers. A need for convertibility of irreversible circuit to reversible
circuit was felt that can make a quick start and keep the development on track.
In this dissertation a logic circuit design entry based on binary logic system
has been taken up that can provide the ease of circuit design in binary logic
system and output as reversible circuit. Entire environment is GUI based and
easy to learn user friendly. This tool offers editing, storage and conversion
into reversible facility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0637</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0637</id><created>2014-07-02</created><authors><author><keyname>Bil&#xf2;</keyname><forenames>Davide</forenames></author><author><keyname>Gual&#xe0;</keyname><forenames>Luciano</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Proietti</keyname><forenames>Guido</forenames></author></authors><title>Fault-Tolerant Approximate Shortest-Path Trees</title><categories>cs.DS</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The resiliency of a network is its ability to remain \emph{effectively}
functioning also when any of its nodes or links fails. However, to reduce
operational and set-up costs, a network should be small in size, and this
conflicts with the requirement of being resilient. In this paper we address
this trade-off for the prominent case of the {\em broadcasting} routing scheme,
and we build efficient (i.e., sparse and fast) \emph{fault-tolerant approximate
shortest-path trees}, for both the edge and vertex \emph{single-failure} case.
In particular, for an $n$-vertex non-negatively weighted graph, and for any
constant $\varepsilon &gt;0$, we design two structures of size $O(\frac{n \log
n}{\varepsilon^2})$ which guarantee $(1+\varepsilon)$-stretched paths from the
selected source also in the presence of an edge/vertex failure. This favorably
compares with the currently best known solutions, which are for the
edge-failure case of size $O(n)$ and stretch factor 3, and for the
vertex-failure case of size $O(n \log n)$ and stretch factor 3. Moreover, we
also focus on the unweighted case, and we prove that an ordinary
$(\alpha,\beta)$-spanner can be slightly augmented in order to build efficient
fault-tolerant approximate \emph{breadth-first-search trees}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0640</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0640</id><created>2014-07-02</created><authors><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Devine</keyname><forenames>Conor</forenames></author><author><keyname>Wang</keyname><forenames>Siyi</forenames></author></authors><title>Performance Analysis of Micro Unmanned Airborne Communication Relays for
  Cellular Networks</title><categories>cs.NI</categories><comments>conference</comments><journal-ref>IEEE International Symposium on Communication Systems, Networks
  and Digital Signal Processing, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyses the potential of utilising small unmanned-aerial-vehicles
(SUAV) as wireless relays for assisting cellular network performance. Whilst
high altitude wireless relays have been investigated over the past 2 decades,
the new class of low cost SUAVs offers new possibilities for addressing local
traffic imbalances and providing emergency coverage.We present field-test
results from an SUAV test-bed in both urban and rural environments. The results
show that trough-to-peak throughput improvements can be achieved for users in
poor coverage zones. Furthermore, the paper reinforces the experimental study
with large-scale network analysis using both stochastic geometry and multi-cell
simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0643</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0643</id><created>2014-07-02</created><authors><author><keyname>Bil&#xf2;</keyname><forenames>Davide</forenames></author><author><keyname>Gual&#xe0;</keyname><forenames>Luciano</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Proietti</keyname><forenames>Guido</forenames></author></authors><title>The Max-Distance Network Creation Game on General Host Graphs</title><categories>cs.GT</categories><comments>17 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a generalization of the classic \emph{network creation
game} in the scenario in which the $n$ players sit on a given arbitrary
\emph{host graph}, which constrains the set of edges a player can activate at a
cost of $\alpha \geq 0$ each. This finds its motivations in the physical
limitations one can have in constructing links in practice, and it has been
studied in the past only when the routing cost component of a player is given
by the sum of distances to all the other nodes. Here, we focus on another
popular routing cost, namely that which takes into account for each player its
\emph{maximum} distance to any other player. For this version of the game, we
first analyze some of its computational and dynamic aspects, and then we
address the problem of understanding the structure of associated pure Nash
equilibria. In this respect, we show that the corresponding price of anarchy
(PoA) is fairly bad, even for several basic classes of host graphs. More
precisely, we first exhibit a lower bound of $\Omega (\sqrt{ n / (1+\alpha)})$
for any $\alpha = o(n)$. Notice that this implies a counter-intuitive lower
bound of $\Omega(\sqrt{n})$ for very small values of $\alpha$ (i.e., edges can
be activated almost for free). Then, we show that when the host graph is
restricted to be either $k$-regular (for any constant $k \geq 3$), or a
2-dimensional grid, the PoA is still $\Omega(1+\min\{\alpha,
\frac{n}{\alpha}\})$, which is proven to be tight for
$\alpha=\Omega(\sqrt{n})$. On the positive side, if $\alpha \geq n$, we show
the PoA is $O(1)$. Finally, in the case in which the host graph is very sparse
(i.e., $|E(H)|=n-1+k$, with $k=O(1)$), we prove that the PoA is $O(1)$, for any
$\alpha$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0645</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0645</id><created>2014-07-02</created><updated>2015-12-18</updated><authors><author><keyname>Czerwi&#x144;ski</keyname><forenames>Wojciech</forenames></author><author><keyname>Jan&#x10d;ar</keyname><forenames>Petr</forenames></author></authors><title>Branching Bisimilarity of Normed BPA Processes is in NEXPTIME</title><categories>cs.LO cs.FL</categories><comments>This is the same text as in July 2014, but only with some
  acknowledgment added due to administrative needs</comments><acm-class>F.1.1; F.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Branching bisimilarity on normed BPA processes was recently shown to be
decidable by Yuxi Fu (ICALP 2013) but his proof has not provided any upper
complexity bound. We present a simpler approach based on relative prime
decompositions that leads to a nondeterministic exponential-time algorithm;
this is close to the known exponential-time lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0683</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0683</id><created>2014-07-02</created><authors><author><keyname>Firsching</keyname><forenames>Moritz</forenames></author></authors><title>Computing maximal copies of polytopes contained in a polytope</title><categories>math.MG cs.CG math.OC</categories><comments>13 pages, 7 figures</comments><msc-class>52C17, 51M20, 90C30</msc-class><journal-ref>Experimental Mathematics Vol. 24 (2015), Issue 1, pp.98-105</journal-ref><doi>10.1080/10586458.2014.956374</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kepler (1619) and Croft (1980) have considered largest homothetic copies of
one regular polytope contained in another regular polytope. For arbitrary pairs
of polytopes we propose to model this as a quadratically constrained
optimization problem. These problems can then be solved numerically; in case
the optimal solutions are algebraic, exact optima can be recovered by solving
systems of equations to very high precision and then using integer relation
algorithms. Based on this approach, we complete Croft's solution to the problem
concerning maximal inclusions of regular three-dimensional polyhedra by
describing inclusions for the six remaining cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0696</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0696</id><created>2014-07-01</created><authors><author><keyname>Davtyan</keyname><forenames>Seda</forenames></author><author><keyname>Konwar</keyname><forenames>Kishori M.</forenames></author><author><keyname>Shvartsman</keyname><forenames>Alexander A.</forenames></author></authors><title>Technical Report: Estimating Reliability of Workers for Cooperative
  Distributed Computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet supercomputing is an approach to solving partitionable,
computation-intensive problems by harnessing the power of a vast number of
interconnected computers. For the problem of using network supercomputing to
perform a large collection of independent tasks, prior work introduced a
decentralized approach and provided randomized synchronous algorithms that
perform all tasks correctly with high probability, while dealing with
misbehaving or crash-prone processors. The main weaknesses of existing
algorithms is that they assume either that the \emph{average} probability of a
non-crashed processor returning incorrect results is inferior to $\frac{1}{2}$,
or that the probability of returning incorrect results is known to \emph{each}
processor. Here we present a randomized synchronous distributed algorithm that
tightly estimates the probability of each processor returning correct results.
Starting with the set $P$ of $n$ processors, let $F$ be the set of processors
that crash. Our algorithm estimates the probability $p_i$ of returning a
correct result for each processor $i \in P-F$, making the estimates available
to all these processors. The estimation is based on the $(\epsilon,
\delta)$-approximation, where each estimated probability $\tilde{p_i}$ of $p_i$
obeys the bound ${\sf Pr}[p_i(1-\epsilon) \leq \tilde{p_i} \leq
p_i(1+\epsilon)] &gt; 1 - \delta$, for any constants $\delta &gt;0$ and $\epsilon &gt;0$
chosen by the user. An important aspect of this algorithm is that each
processor terminates without global coordination. We assess the efficiency of
the algorithm in three adversarial models as follows. For the model where the
number of non-crashed processors $|P-F|$ is linearly bounded the time
complexity $T(n)$ of the algorithm is $\Theta(\log{n})$, work complexity $W(n)$
is $\Theta(n\log{n})$, and message complexity $M(n)$ is $\Theta(n\log^2n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0697</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0697</id><created>2014-07-02</created><authors><author><keyname>Rana</keyname><forenames>Anuradha</forenames></author><author><keyname>Sharma</keyname><forenames>Pratima</forenames></author></authors><title>How to Track Online SLA</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SLA (Service level agreement) is defined by an organization to fulfil its
client requirements, the time within which the deliverables should be turned
over to the clients. Tracking of SLA can be done manually by checking the
status, priority of any particular task. Manual SLA tracking takes time as one
has to go over each and every task that needs to be completed. For instance,
you ordered a product from a website and you are not happy with the quality of
the product and want to replace the same on urgent basis, You send mail to the
customer support department, the query/complaint will be submitted in a queue
and will be processed basis of its priority and urgency (The SLA for responding
back to customers concern are listed in the policy). This online SLA tracking
system will ensure that no queries/complaints are missed and are processed in
an organized manner as per their priority and the date by when it should be
handled. The portal will provide the status of the complaints for that
particular day and the ones which have been pending since last week. The
information can be refreshed as per the client need (within what time frame the
complaint should be addressed).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0698</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0698</id><created>2014-07-02</created><authors><author><keyname>Nunes</keyname><forenames>Davide</forenames></author><author><keyname>Antunes</keyname><forenames>Luis</forenames></author></authors><title>Continuous On-line Evolution of Agent Behaviours with Cartesian Genetic
  Programming</title><categories>cs.NE cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary Computation has been successfully used to synthesise controllers
for embodied agents and multi-agent systems in general. Notwithstanding this,
continuous on-line adaptation by the means of evolutionary algorithms is still
under-explored, especially outside the evolutionary robotics domain. In this
paper, we present an on-line evolutionary programming algorithm that searches
in the agent design space for the appropriate behavioural policies to cope with
the underlying environment. We discuss the current problems of continuous agent
adaptation, present our on-line evolution testbed for evolutionary simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0699</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0699</id><created>2014-07-02</created><authors><author><keyname>Mohamed</keyname><forenames>Nasr</forenames></author></authors><title>Enumeration of Spanning Trees Using Edge Exchange with Minimal
  Partitioning</title><categories>cs.DS</categories><comments>Master Thesis</comments><acm-class>G.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, Minimal Partitioning (MP) algorithm, an innovative algorithm
for enumerating all the spanning trees in an undirected graph is presented.
  While MP algorithm uses a computational tree graph to traverse all possible
spanning trees by the edge exchange technique, it has two unique properties
compared to previous algorithms. In the first place, the algorithm maintains a
state of minimal partition size in the spanning tree due to edge deletion. This
is realized by swapping peripheral edges, more precisely leaf edges, in most of
edge exchange operations. Consequently, the main structure of the spanning
trees is preserved during the steps of the enumeration process. This extra
constraint proves to be advantageous in many applications where the partition
size is a factor in the solution cost. Secondly, we introduce, and utilize, the
new concept of edge promotion: the exchanged edges always share one end.
Practically, and as a result of this property, the interface between the two
partitions of the spanning tree during edge exchange has to be maintained from
one side only.
  For a graph $G(V,E)$, MP algorithm requires $O(log V+E/V)$ expected time and
$OV log V)$ worst case time for generating each spanning tree. MP algorithm
requires a total expected space limit of $O(E log V)$ with worst case limit of
$O(EV)$. Like all edge exchange algorithms, MP algorithm retains the advantage
of compacted output of $O(1)$ per spanning tree by listing the relative
differences only.
  Three sample real-world applications of spanning trees enumeration are
explored and the effects of using MP algorithm are studied. Namely:
construction of nets of polyhedra, multi-robots spanning tree routing, and
computing the electric current in edges of a network. We report that MP
algorithm outperforms other algorithm by $O(V)$ time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0713</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0713</id><created>2014-07-02</created><authors><author><keyname>Seferoglu</keyname><forenames>Hulya</forenames></author><author><keyname>Xing</keyname><forenames>Yuxuan</forenames></author></authors><title>Device-Centric Cooperation in Mobile Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing popularity of applications such as video streaming in today's
mobile devices introduces higher demand for throughput, and puts a strain
especially on cellular links. Cooperation among mobile devices by exploiting
both cellular and local area connections is a promising approach to meet the
increasing demand. In this paper, we consider that a group of cooperative
mobile devices, exploiting both cellular and local area links and within
proximity of each other, are interested in the same video content. Traditional
network control algorithms introduce high overhead and delay in this setup as
the network control and cooperation decisions are made in a source-centric
manner. Instead, we develop a device-centric stochastic cooperation scheme. Our
device-centric scheme; DcC allows mobile devices to make control decisions such
as flow control, scheduling, and cooperation without loss of optimality. Thanks
to being device-centric, DcC reduces; (i) overhead; i.e., the number of control
packets that should be transmitted over cellular links, so cellular links are
used more efficiently, and (ii) the amount of delay that each packet
experiences, which improves quality of service. The simulation results
demonstrate the benefits of DcC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0717</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0717</id><created>2014-07-02</created><authors><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author><author><keyname>Yang</keyname><forenames>Fei</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Deep Poselets for Human Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of detecting people in natural scenes using a part
approach based on poselets. We propose a bootstrapping method that allows us to
collect millions of weakly labeled examples for each poselet type. We use these
examples to train a Convolutional Neural Net to discriminate different poselet
types and separate them from the background class. We then use the trained CNN
as a way to represent poselet patches with a Pose Discriminative Feature (PDF)
vector -- a compact 256-dimensional feature vector that is effective at
discriminating pose from appearance. We train the poselet model on top of PDF
features and combine them with object-level CNNs for detection and bounding box
prediction. The resulting model leads to state-of-the-art performance for human
detection on the PASCAL datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0726</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0726</id><created>2014-07-02</created><updated>2014-12-19</updated><authors><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>Fast Algorithm for Low-rank matrix recovery in Poisson noise</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>Presented at IEEE GLOBALSIP2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a fast algorithm for recovering low-rank matrices from
their linear measurements contaminated with Poisson noise: the Poisson noise
Maximum Likelihood Singular Value thresholding (PMLSV) algorithm. We propose a
convex optimization formulation with a cost function consisting of the sum of a
likelihood function and a regularization function which the nuclear norm of the
matrix. Instead of solving the optimization problem directly by semi-definite
program (SDP), we derive an iterative singular value thresholding algorithm by
expanding the likelihood function. We demonstrate the good performance of the
proposed algorithm on recovery of solar flare images with Poisson noise: the
algorithm is more efficient than solving SDP using the interior-point algorithm
and it generates a good approximate solution compared to that solved from SDP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0727</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0727</id><created>2014-07-02</created><updated>2014-07-05</updated><authors><author><keyname>Konstantakopoulos</keyname><forenames>Ioannis C.</forenames></author><author><keyname>Ratliff</keyname><forenames>Lillian J.</forenames></author><author><keyname>Jin</keyname><forenames>Ming</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author><author><keyname>Spanos</keyname><forenames>Costas</forenames></author></authors><title>Social Game for Building Energy Efficiency: Utility Learning,
  Simulation, and Analysis</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a social game that we designed for encouraging energy efficient
behavior amongst building occupants with the aim of reducing overall energy
consumption in the building. Occupants vote for their desired lighting level
and win points which are used in a lottery based on how far their vote is from
the maximum setting. We assume that the occupants are utility maximizers and
that their utility functions capture the tradeoff between winning points and
their comfort level. We model the occupants as non-cooperative agents in a
continuous game and we characterize their play using the Nash equilibrium
concept. Using occupant voting data, we parameterize their utility functions
and use a convex optimization problem to estimate the parameters. We simulate
the game defined by the estimated utility functions and show that the estimated
model for occupant behavior is a good predictor of their actual behavior. In
addition, we show that due to the social game, there is a significant reduction
in energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0729</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0729</id><created>2014-07-02</created><updated>2014-10-17</updated><authors><author><keyname>Egi</keyname><forenames>Satoshi</forenames></author></authors><title>Non-Linear Pattern-Matching against Unfree Data Types with Lexical
  Scoping</title><categories>cs.PL</categories><comments>25 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a pattern-matching system that enables non-linear
pattern-matching against unfree data types. The system allows multiple
occurrences of the same variables in a pattern, multiple results of
pattern-matching and modularization of the way of pattern-matching for each
data type at the same time. It enables us to represent pattern-matching against
not only algebraic data types but also unfree data types such as sets, graphs
and any other data types whose data have no canonical form and multiple ways of
decomposition. I have realized that with a rule that pattern-matching is
executed from the left side of a pattern and a rule that a binding to a
variable in a pattern can be referred to in its right side of the pattern.
Furthermore, I have realized modularization of these patterns with lexical
scoping. In my system, a pattern is not a first class object, but a
pattern-function that obtains only patterns and returns a pattern is a first
class object. This restriction simplifies the non-linear pattern-matching
system with lexical scoping. I have already implemented the pattern-matching
system in the Egison programming language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0731</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0731</id><created>2014-07-02</created><updated>2015-02-02</updated><authors><author><keyname>Braun</keyname><forenames>Gabor</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>Info-Greedy sequential adaptive compressed sensing</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>Preliminary results presented at Allerton Conference 2014. To appear
  in IEEE Journal Selected Topics on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an information-theoretic framework for sequential adaptive
compressed sensing, Info-Greedy Sensing, where measurements are chosen to
maximize the extracted information conditioned on the previous measurements. We
show that the widely used bisection approach is Info-Greedy for a family of
$k$-sparse signals by connecting compressed sensing and blackbox complexity of
sequential query algorithms, and present Info-Greedy algorithms for Gaussian
and Gaussian Mixture Model (GMM) signals, as well as ways to design sparse
Info-Greedy measurements. Numerical examples demonstrate the good performance
of the proposed algorithms using simulated and real data: Info-Greedy Sensing
shows significant improvement over random projection for signals with sparse
and low-rank covariance matrices, and adaptivity brings robustness when there
is a mismatch between the assumed and the true distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0733</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0733</id><created>2014-07-02</created><updated>2014-10-03</updated><authors><author><keyname>Cocci</keyname><forenames>Giacomo</forenames></author><author><keyname>Barbieri</keyname><forenames>Davide</forenames></author><author><keyname>Citti</keyname><forenames>Giovanna</forenames></author><author><keyname>Sarti</keyname><forenames>Alessandro</forenames></author></authors><title>Cortical spatio-temporal dimensionality reduction for visual grouping</title><categories>cs.CV cs.NE q-bio.NC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The visual systems of many mammals, including humans, is able to integrate
the geometric information of visual stimuli and to perform cognitive tasks
already at the first stages of the cortical processing. This is thought to be
the result of a combination of mechanisms, which include feature extraction at
single cell level and geometric processing by means of cells connectivity. We
present a geometric model of such connectivities in the space of detected
features associated to spatio-temporal visual stimuli, and show how they can be
used to obtain low-level object segmentation. The main idea is that of defining
a spectral clustering procedure with anisotropic affinities over datasets
consisting of embeddings of the visual stimuli into higher dimensional spaces.
Neural plausibility of the proposed arguments will be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0742</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0742</id><created>2014-07-02</created><updated>2014-07-13</updated><authors><author><keyname>Boccaletti</keyname><forenames>S.</forenames></author><author><keyname>Bianconi</keyname><forenames>G.</forenames></author><author><keyname>Criado</keyname><forenames>R.</forenames></author><author><keyname>del Genio</keyname><forenames>C. I.</forenames></author><author><keyname>G&#xf3;mez-Garde&#xf1;es</keyname><forenames>J.</forenames></author><author><keyname>Romance</keyname><forenames>M.</forenames></author><author><keyname>Sendi&#xf1;a-Nadal</keyname><forenames>I.</forenames></author><author><keyname>Wang</keyname><forenames>Z.</forenames></author><author><keyname>Zanin</keyname><forenames>M.</forenames></author></authors><title>The structure and dynamics of multilayer networks</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>In Press, Accepted Manuscript, Physics Reports 2014</comments><journal-ref>Physics Reports 544, 1 (2014)</journal-ref><doi>10.1016/j.physrep.2014.07.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past years, network theory has successfully characterized the
interaction among the constituents of a variety of complex systems, ranging
from biological to technological, and social systems. However, up until
recently, attention was almost exclusively given to networks in which all
components were treated on equivalent footing, while neglecting all the extra
information about the temporal- or context-related properties of the
interactions under study. Only in the last years, taking advantage of the
enhanced resolution in real data sets, network scientists have directed their
interest to the multiplex character of real-world systems, and explicitly
considered the time-varying and multilayer nature of networks. We offer here a
comprehensive review on both structural and dynamical organization of graphs
made of diverse relationships (layers) between its constituents, and cover
several relevant issues, from a full redefinition of the basic structural
measures, to understanding how the multilayer nature of the network affects
processes and dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0749</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0749</id><created>2014-07-02</created><updated>2014-10-08</updated><authors><author><keyname>Domke</keyname><forenames>Justin</forenames></author><author><keyname>Liu</keyname><forenames>Xianghang</forenames></author></authors><title>Projecting Ising Model Parameters for Fast Mixing</title><categories>cs.LG stat.ML</categories><comments>Advances in Neural Information Processing Systems 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference in general Ising models is difficult, due to high treewidth making
tree-based algorithms intractable. Moreover, when interactions are strong,
Gibbs sampling may take exponential time to converge to the stationary
distribution. We present an algorithm to project Ising model parameters onto a
parameter set that is guaranteed to be fast mixing, under several divergences.
We find that Gibbs sampling using the projected parameters is more accurate
than with the original parameters when interaction strengths are strong and
when limited time is available for sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0753</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0753</id><created>2014-07-02</created><updated>2015-11-03</updated><authors><author><keyname>Li</keyname><forenames>Guoyin</forenames></author><author><keyname>Pong</keyname><forenames>Ting Kei</forenames></author></authors><title>Global convergence of splitting methods for nonconvex composite
  optimization</title><categories>math.OC cs.LG math.NA stat.ML</categories><comments>To appear in SIOPT</comments><doi>10.1137/140998135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimizing the sum of a smooth function $h$ with a
bounded Hessian, and a nonsmooth function. We assume that the latter function
is a composition of a proper closed function $P$ and a surjective linear map
$\cal M$, with the proximal mappings of $\tau P$, $\tau &gt; 0$, simple to
compute. This problem is nonconvex in general and encompasses many important
applications in engineering and machine learning. In this paper, we examined
two types of splitting methods for solving this nonconvex optimization problem:
alternating direction method of multipliers and proximal gradient algorithm.
For the direct adaptation of the alternating direction method of multipliers,
we show that, if the penalty parameter is chosen sufficiently large and the
sequence generated has a cluster point, then it gives a stationary point of the
nonconvex problem. We also establish convergence of the whole sequence under an
additional assumption that the functions $h$ and $P$ are semi-algebraic.
Furthermore, we give simple sufficient conditions to guarantee boundedness of
the sequence generated. These conditions can be satisfied for a wide range of
applications including the least squares problem with the $\ell_{1/2}$
regularization. Finally, when $\cal M$ is the identity so that the proximal
gradient algorithm can be efficiently applied, we show that any cluster point
is stationary under a slightly more flexible constant step-size rule than what
is known in the literature for a nonconvex $h$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0754</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0754</id><created>2014-07-02</created><authors><author><keyname>Domke</keyname><forenames>Justin</forenames></author></authors><title>Structured Learning via Logistic Regression</title><categories>cs.LG stat.ML</categories><comments>Advances in Neural Information Processing Systems 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A successful approach to structured learning is to write the learning
objective as a joint function of linear parameters and inference messages, and
iterate between updates to each. This paper observes that if the inference
problem is &quot;smoothed&quot; through the addition of entropy terms, for fixed
messages, the learning objective reduces to a traditional (non-structured)
logistic regression problem with respect to parameters. In these logistic
regression problems, each training example has a bias term determined by the
current set of messages. Based on this insight, the structured energy function
can be extended from linear factors to any function class where an &quot;oracle&quot;
exists to minimize a logistic loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0756</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0756</id><created>2014-07-02</created><authors><author><keyname>Kumar</keyname><forenames>Rajesh</forenames></author><author><keyname>Kumar</keyname><forenames>Sushil</forenames></author><author><keyname>Shukla</keyname><forenames>Diksha</forenames></author><author><keyname>Raw</keyname><forenames>Ram Shringar</forenames></author></authors><title>Geometrical Localization Algorithm for 3-D Wireless Sensor Networks</title><categories>cs.NI</categories><comments>Journal of Wireless Personal Communication, Springer : June, 2014,
  The final version of publication is available at link.springer.com Link:
  http://link.springer.com/article/10.1007%2Fs11277-014-1852-6</comments><report-no>10.1007/s11277-014-1852-6</report-no><doi>10.1007/s11277-014-1852-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an efficient range free localization scheme for
large scale three dimensional wireless sensor networks. Our system environment
consists of two type of sensors, randomly deployed static sensors and global
positioning system equipped moving sensors. These moving anchors travels across
the network field and broadcast their current locations on specified intervals.
As soon as the sensors which are deployed in random fashion receives three
beacon messages (known locations broadcasted by anchors), they computes their
locations automatically by using our proposed algorithm. One of our significant
contributions is, we use only three different beacon messages to localize one
sensor, while in the best of our knowledge, all previously proposed methods use
at least four different known locations. The ability of our method to localize
by using only three known locations not only saves computation, time, energy,
but also reduces the number of anchors needed to be deployed and more
importantly reduces the communication overheads. Experimental results
demonstrate that our proposed scheme improves the overall efficiency of
localization process significantly.
  Important Note: Final version of this paper is accepted and published by
Journal of Wireless Personal Communication, Springer : June, 2014 The final
version of publication is available at link.springer.com Link:
http://link.springer.com/article/10.1007\%2Fs11277-014-1852-6
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0765</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0765</id><created>2014-07-02</created><authors><author><keyname>Mansoor</keyname><forenames>Awais</forenames></author><author><keyname>Patsekin</keyname><forenames>Valery</forenames></author><author><keyname>Scherl</keyname><forenames>Dale</forenames></author><author><keyname>Robinson</keyname><forenames>J. Paul</forenames></author><author><keyname>Rajwa</keyname><forenames>Bartlomiej</forenames></author></authors><title>BiofilmQuant: A Computer-Assisted Tool for Dental Biofilm Quantification</title><categories>cs.CV</categories><comments>4 pages, 4 figures, 36th Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society (EMBC 2014)</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Dental biofilm is the deposition of microbial material over a tooth
substratum. Several methods have recently been reported in the literature for
biofilm quantification; however, at best they provide a barely automated
solution requiring significant input needed from the human expert. On the
contrary, state-of-the-art automatic biofilm methods fail to make their way
into clinical practice because of the lack of effective mechanism to
incorporate human input to handle praxis or misclassified regions. Manual
delineation, the current gold standard, is time consuming and subject to expert
bias. In this paper, we introduce a new semi-automated software tool,
BiofilmQuant, for dental biofilm quantification in quantitative light-induced
fluorescence (QLF) images. The software uses a robust statistical modeling
approach to automatically segment the QLF image into three classes (background,
biofilm, and tooth substratum) based on the training data. This initial
segmentation has shown a high degree of consistency and precision on more than
200 test QLF dental scans. Further, the proposed software provides the
clinicians full control to fix any misclassified areas using a single click. In
addition, BiofilmQuant also provides a complete solution for the longitudinal
quantitative analysis of biofilm of the full set of teeth, providing greater
ease of usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0774</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0774</id><created>2014-07-02</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Zhang</keyname><forenames>Hai-Feng</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Do</keyname><forenames>Younghae</forenames></author><author><keyname>Liu</keyname><forenames>Zong-Hua</forenames></author></authors><title>Epidemic spreading on complex networks with general degree and weight
  distributions</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 5 figures</comments><doi>10.1103/PhysRevE.90.042803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of disease on complex networks has attracted widely attention in
the physics community. Recent works have demonstrated that heterogeneous degree
and weight distributions have a significant influence on the epidemic dynamics.
In this study, a novel edge-weight based compartmental approach is developed to
estimate the epidemic threshold and epidemic size (final infected density) on
networks with general degree and weight distributions, and a remarkable
agreement with numerics is obtained. Even in complex network with the strong
heterogeneous degree and weight distributions, this approach is worked. We then
propose an edge-weight based removal strategy with different biases, and find
that such a strategy can effectively control the spread of epidemic when the
highly weighted edges are preferentially removed, especially when the weight
distribution of a network is extremely heterogenous. The theoretical results
from the suggested method can accurately predict the above removal
effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0779</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0779</id><created>2014-07-03</created><updated>2014-09-18</updated><authors><author><keyname>Huang</keyname><forenames>Haiping</forenames></author></authors><title>Code optimization, frozen glassy phase and improved decoding algorithms
  for low-density parity-check codes</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>12 pages, 6 figures, revised</comments><journal-ref>Commun. Theor. Phys. 63, 115 (2015)</journal-ref><doi>10.1088/0253-6102/63/1/18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The statistical physics properties of low-density parity-check codes for the
binary symmetric channel are investigated as a spin glass problem with
multi-spin interactions and quenched random fields by the cavity method. By
evaluating the entropy function at the Nishimori temperature, we find that
irregular constructions with heterogeneous degree distribution of check (bit)
nodes have higher decoding thresholds compared to regular counterparts with
homogeneous degree distribution. We also show that the instability of the
mean-field calculation takes place only after the entropy crisis, suggesting
the presence of a frozen glassy phase at low temperatures. When no prior
knowledge of channel noise is assumed (searching for the ground state), we find
that a reinforced strategy on normal belief propagation will boost the decoding
threshold to a higher value than the normal belief propagation. This value is
close to the dynamical transition where all local search heuristics fail to
identify the true message (codeword or the ferromagnetic state). After the
dynamical transition, the number of metastable states with larger energy
density (than the ferromagnetic state) becomes exponentially numerous. When the
noise level of the transmission channel approaches the static transition point,
there starts to exist exponentially numerous codewords sharing the identical
ferromagnetic energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0786</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0786</id><created>2014-07-03</created><authors><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Strengthening the Effectiveness of Pedestrian Detection with Spatially
  Pooled Features</title><categories>cs.CV</categories><comments>16 pages. Appearing in Proc. European Conf. Computer Vision (ECCV)
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple yet effective approach to the problem of pedestrian
detection which outperforms the current state-of-the-art. Our new features are
built on the basis of low-level visual features and spatial pooling.
Incorporating spatial pooling improves the translational invariance and thus
the robustness of the detection process. We then directly optimise the partial
area under the ROC curve (\pAUC) measure, which concentrates detection
performance in the range of most practical importance. The combination of these
factors leads to a pedestrian detector which outperforms all competitors on all
of the standard benchmark datasets. We advance state-of-the-art results by
lowering the average miss rate from $13\%$ to $11\%$ on the INRIA benchmark,
$41\%$ to $37\%$ on the ETH benchmark, $51\%$ to $42\%$ on the TUD-Brussels
benchmark and $36\%$ to $29\%$ on the Caltech-USA benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0787</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0787</id><created>2014-07-03</created><authors><author><keyname>Svetlova</keyname><forenames>Ekaterina</forenames><affiliation>Universit&#xe4;t Konstanz, Karlshochschule International University</affiliation></author><author><keyname>van Elst</keyname><forenames>Henk</forenames><affiliation>Karlshochschule International University</affiliation></author></authors><title>Decision-theoretic approaches to non-knowledge in economics</title><categories>q-fin.GN cs.AI</categories><comments>13 pages, LaTeX2e, hyperlinked references. To appear in &quot;Routledge
  International Handbook of Ignorance Studies,&quot; edited by Matthias Gro{\ss} and
  Linsey McGoey (London: Routledge), due to be published in February 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review two strands of conceptual approaches to the formal representation
of a decision maker's non-knowledge at the initial stage of a static
one-person, one-shot decision problem in economic theory. One focuses on
representations of non-knowledge in terms of probability measures over sets of
mutually exclusive and exhaustive consequence-relevant states of Nature, the
other deals with unawareness of potentially important events by means of sets
of states that are less complete than the full set of consequence-relevant
states of Nature. We supplement our review with a brief discussion of
unresolved matters in both approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0788</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0788</id><created>2014-07-03</created><updated>2014-07-04</updated><authors><author><keyname>Barford</keyname><forenames>Paul</forenames></author><author><keyname>Canadi</keyname><forenames>Igor</forenames></author><author><keyname>Krushevskaja</keyname><forenames>Darja</forenames></author><author><keyname>Ma</keyname><forenames>Qiang</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author></authors><title>Adscape: Harvesting and Analyzing Online Display Ads</title><categories>cs.CY cs.NI</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, advertising has emerged as the primary source of
revenue for many web sites and apps. In this paper we report a
first-of-its-kind study that seeks to broadly understand the features,
mechanisms and dynamics of display advertising on the web - i.e., the Adscape.
Our study takes the perspective of users who are the targets of display ads
shown on web sites. We develop a scalable crawling capability that enables us
to gather the details of display ads including creatives and landing pages. Our
crawling strategy is focused on maximizing the number of unique ads harvested.
Of critical importance to our study is the recognition that a user's profile
(i.e. browser profile and cookies) can have a significant impact on which ads
are shown. We deploy our crawler over a variety of websites and profiles and
this yields over 175K distinct display ads.
  We find that while targeting is widely used, there remain many instances in
which delivered ads do not depend on user profile; further, ads vary more over
user profiles than over websites. We also assess the population of advertisers
seen and identify over 3.7K distinct entities from a variety of business
segments. Finally, we find that when targeting is used, the specific types of
ads delivered generally correspond with the details of user profiles, and also
on users' patterns of visit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0791</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0791</id><created>2014-07-03</created><authors><author><keyname>Garimella</keyname><forenames>Venkata Rama Kiran</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>Co-Following on Twitter</title><categories>cs.SI physics.soc-ph</categories><comments>full version of a short paper at Hypertext 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an in-depth study of co-following on Twitter based on the
observation that two Twitter users whose followers have similar friends are
also similar, even though they might not share any direct links or a single
mutual follower. We show how this observation contributes to (i) a better
understanding of language-agnostic user classification on Twitter, (ii)
eliciting opportunities for Computational Social Science, and (iii) improving
online marketing by identifying cross-selling opportunities.
  We start with a machine learning problem of predicting a user's preference
among two alternative choices of Twitter friends. We show that co-following
information provides strong signals for diverse classification tasks and that
these signals persist even when (i) the most discriminative features are
removed and (ii) only relatively &quot;sparse&quot; users with fewer than 152 but more
than 43 Twitter friends are considered.
  Going beyond mere classification performance optimization, we present
applications of our methodology to Computational Social Science. Here we
confirm stereotypes such as that the country singer Kenny Chesney
(@kennychesney) is more popular among @GOP followers, whereas Lady Gaga
(@ladygaga) enjoys more support from @TheDemocrats followers.
  In the domain of marketing we give evidence that celebrity endorsement is
reflected in co-following and we demonstrate how our methodology can be used to
reveal the audience similarities between Apple and Puma and, less obviously,
between Nike and Coca-Cola. Concerning a user's popularity we find a
statistically significant connection between having a more &quot;average&quot;
followership and having more followers than direct rivals. Interestingly, a
\emph{larger} audience also seems to be linked to a \emph{less diverse}
audience in terms of their co-following.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0803</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0803</id><created>2014-07-03</created><authors><author><keyname>Zhou</keyname><forenames>Zhe</forenames></author><author><keyname>Diao</keyname><forenames>Wenrui</forenames></author><author><keyname>Liu</keyname><forenames>Xiangyu</forenames></author><author><keyname>Zhang</keyname><forenames>Kehuan</forenames></author></authors><title>Acoustic Fingerprinting Revisited: Generate Stable Device ID Stealthy
  with Inaudible Sound</title><categories>cs.CR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popularity of mobile device has made people's lives more convenient, but
threatened people's privacy at the same time. As end users are becoming more
and more concerned on the protection of their private information, it is even
harder to track a specific user using conventional technologies. For example,
cookies might be cleared by users regularly. Apple has stopped apps accessing
UDIDs, and Android phones use some special permission to protect IMEI code. To
address this challenge, some recent studies have worked on tracing smart phones
using the hardware features resulted from the imperfect manufacturing process.
These works have demonstrated that different devices can be differentiated to
each other. However, it still has a long way to go in order to replace cookie
and be deployed in real world scenarios, especially in terms of properties like
uniqueness, robustness, etc. In this paper, we presented a novel method to
generate stable and unique device ID stealthy for smartphones by exploiting the
frequency response of the speaker. With carefully selected audio frequencies
and special sound wave patterns, we can reduce the impacts of non-linear
effects and noises, and keep our feature extraction process un-noticeable to
users. The extracted feature is not only very stable for a given smart phone
speaker, but also unique to that phone. The feature contains rich information
that is equivalent to around 40 bits of entropy, which is enough to identify
billions of different smart phones of the same model. We have built a prototype
to evaluate our method, and the results show that the generated device ID can
be used as a replacement of cookie.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0814</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0814</id><created>2014-07-03</created><authors><author><keyname>Lau</keyname><forenames>Siow Shyong</forenames></author></authors><title>Impact of Mobile phone in the Air and Random Access Channel RACH with
  Time Division Multiple Access TDMA Noise in Aircraft Avionics</title><categories>cs.SY</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile phone 1800MHz band already allowed to be used in some airlines. Many
studies talked about lower the mobile phone output power to the lowest of 0dBm,
but seldom talk about the Random Access Channel RACH, which will emit at the
highest power of 30dBm at the instant of call making and is not controllable
until the connection between the Base Station and mobile is made.Hence the
impact of RACH and TDMA noise generated in the aircraft.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0822</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0822</id><created>2014-07-03</created><authors><author><keyname>De Myttenaere</keyname><forenames>Arnaud</forenames><affiliation>SAMM</affiliation></author><author><keyname>Grand</keyname><forenames>B&#xe9;n&#xe9;dicte Le</forenames><affiliation>CRI</affiliation></author><author><keyname>Golden</keyname><forenames>Boris</forenames><affiliation>Viadeo</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Reducing Offline Evaluation Bias in Recommendation Systems</title><categories>cs.IR cs.LG stat.ML</categories><comments>23rd annual Belgian-Dutch Conference on Machine Learning (Benelearn
  2014), Bruxelles : Belgium (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation systems have been integrated into the majority of large online
systems. They tailor those systems to individual users by filtering and ranking
information according to user profiles. This adaptation process influences the
way users interact with the system and, as a consequence, increases the
difficulty of evaluating a recommendation algorithm with historical data (via
offline evaluation). This paper analyses this evaluation bias and proposes a
simple item weighting solution that reduces its impact. The efficiency of the
proposed solution is evaluated on real world data extracted from Viadeo
professional social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0843</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0843</id><created>2014-07-03</created><authors><author><keyname>Meder</keyname><forenames>Michael</forenames></author><author><keyname>Jain</keyname><forenames>Brijnesh-Johannes</forenames></author></authors><title>The Gamification Design Problem</title><categories>cs.HC</categories><comments>5 pages, preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under the assumptions that (i) gamification consists of various types of
users that experience game design elements differently; and (ii) gamification
is deployed in order to achieve some goal in the broadest sense, we pose the
gamification problem as that of assigning each user a game design element that
maximizes their expected contribution in order to achieve that goal. We show
that this problem reduces to a statistical learning problem and suggest matrix
factorization as one solution when user interaction data is given. The
hypothesis is that predictive models as intelligent tools for supporting users
in decision-making may also have potential to support the design process in
gamification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0848</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0848</id><created>2014-07-03</created><updated>2015-01-14</updated><authors><author><keyname>Cascudo</keyname><forenames>Ignacio</forenames></author><author><keyname>Cramer</keyname><forenames>Ronald</forenames></author><author><keyname>Mirandola</keyname><forenames>Diego</forenames></author><author><keyname>Z&#xe9;mor</keyname><forenames>Gilles</forenames></author></authors><title>Squares of Random Linear Codes</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory, vol. 61, no. 3, pp.
  1159-1173, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a linear code $C$, one can define the $d$-th power of $C$ as the span
of all componentwise products of $d$ elements of $C$. A power of $C$ may
quickly fill the whole space. Our purpose is to answer the following question:
does the square of a code &quot;typically&quot; fill the whole space? We give a positive
answer, for codes of dimension $k$ and length roughly $\frac{1}{2}k^2$ or
smaller. Moreover, the convergence speed is exponential if the difference
$k(k+1)/2-n$ is at least linear in $k$. The proof uses random coding and
combinatorial arguments, together with algebraic tools involving the precise
computation of the number of quadratic forms of a given rank, and the number of
their zeros.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0880</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0880</id><created>2014-07-03</created><updated>2014-09-16</updated><authors><author><keyname>Rabenoro</keyname><forenames>Tsirizo</forenames><affiliation>SAMM</affiliation></author><author><keyname>Lacaille</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>SAMM</affiliation></author><author><keyname>Cottrell</keyname><forenames>Marie</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Anomaly Detection Based on Aggregation of Indicators</title><categories>stat.ML cs.LG</categories><comments>23rd annual Belgian-Dutch Conference on Machine Learning (Benelearn
  2014), Bruxelles : Belgium (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic anomaly detection is a major issue in various areas. Beyond mere
detection, the identification of the origin of the problem that produced the
anomaly is also essential. This paper introduces a general methodology that can
assist human operators who aim at classifying monitoring signals. The main idea
is to leverage expert knowledge by generating a very large number of
indicators. A feature selection method is used to keep only the most
discriminant indicators which are used as inputs of a Naive Bayes classifier.
The parameters of the classifier have been optimized indirectly by the
selection process. Simulated data designed to reproduce some of the anomaly
types observed in real world engines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0892</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0892</id><created>2014-07-03</created><authors><author><keyname>Antoniadis</keyname><forenames>Antonios</forenames></author><author><keyname>Huang</keyname><forenames>Chien-Chung</forenames></author><author><keyname>Ott</keyname><forenames>Sebastian</forenames></author></authors><title>A Fully Polynomial-Time Approximation Scheme for Speed Scaling with
  Sleep State</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study classical deadline-based preemptive scheduling of tasks in a
computing environment equipped with both dynamic speed scaling and sleep state
capabilities: Each task is specified by a release time, a deadline and a
processing volume, and has to be scheduled on a single, speed-scalable
processor that is supplied with a sleep state. In the sleep state, the
processor consumes no energy, but a constant wake-up cost is required to
transition back to the active state. In contrast to speed scaling alone, the
addition of a sleep state makes it sometimes beneficial to accelerate the
processing of tasks in order to transition the processor to the sleep state for
longer amounts of time and incur further energy savings. The goal is to output
a feasible schedule that minimizes the energy consumption. Since the
introduction of the problem by Irani et al. [16], its exact computational
complexity has been repeatedly posed as an open question (see e.g. [2,8,15]).
The currently best known upper and lower bounds are a 4/3-approximation
algorithm and NP-hardness due to [2] and [2,17], respectively. We close the
aforementioned gap between the upper and lower bound on the computational
complexity of speed scaling with sleep state by presenting a fully
polynomial-time approximation scheme for the problem. The scheme is based on a
transformation to a non-preemptive variant of the problem, and a discretization
that exploits a carefully defined lexicographical ordering among schedules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0898</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0898</id><created>2014-07-03</created><updated>2015-09-30</updated><authors><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author><author><keyname>Hachem</keyname><forenames>Walid</forenames></author><author><keyname>Iutzeler</keyname><forenames>Franck</forenames></author></authors><title>A Coordinate Descent Primal-Dual Algorithm and Application to
  Distributed Asynchronous Optimization</title><categories>math.OC cs.DC cs.NA cs.SY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the idea of randomized coordinate descent of $\alpha$-averaged
operators, a randomized primal-dual optimization algorithm is introduced, where
a random subset of coordinates is updated at each iteration. The algorithm
builds upon a variant of a recent (deterministic) algorithm proposed by V\~u
and Condat that includes the well known ADMM as a particular case. The obtained
algorithm is used to solve asynchronously a distributed optimization problem. A
network of agents, each having a separate cost function containing a
differentiable term, seek to find a consensus on the minimum of the aggregate
objective. The method yields an algorithm where at each iteration, a random
subset of agents wake up, update their local estimates, exchange some data with
their neighbors, and go idle. Numerical results demonstrate the attractive
performance of the method. The general approach can be naturally adapted to
other situations where coordinate descent convex optimization algorithms are
used with a random choice of the coordinates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0901</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0901</id><created>2014-07-03</created><updated>2014-09-17</updated><authors><author><keyname>Perotti</keyname><forenames>Alberto G.</forenames></author><author><keyname>Popovic</keyname><forenames>Branislav M.</forenames></author></authors><title>Enhanced Trellis Coded Multiple Access (ETCMA)</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an enhanced version of trellis coded multiple access (TCMA), an
overloaded multiple access scheme that outperforms the original TCMA in terms
of achieved spectral efficiency. Enhanced TCMA (ETCMA) performs simultaneous
transmission of multiple data streams intended for users experiencing similar
signal-to-noise ratios and can be employed both in the uplink and in the
downlink of wireless systems, thus overcoming one of the main limitations of
TCMA. Thanks to a new receiver algorithm, ETCMA is capable of delivering a
significantly higher spectral efficiency. We show that ETCMA approaches the
capacity of the Additive White Gaussian Noise channel for a wide range of
signal-to-noise ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0904</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0904</id><created>2014-07-03</created><updated>2014-07-04</updated><authors><author><keyname>Luporini</keyname><forenames>Fabio</forenames></author><author><keyname>Varbanescu</keyname><forenames>Ana Lucia</forenames></author><author><keyname>Rathgeber</keyname><forenames>Florian</forenames></author><author><keyname>Bercea</keyname><forenames>Gheorghe-Teodor</forenames></author><author><keyname>Ramanujam</keyname><forenames>J.</forenames></author><author><keyname>Ham</keyname><forenames>David A.</forenames></author><author><keyname>Kelly</keyname><forenames>Paul H. J.</forenames></author></authors><title>COFFEE: an Optimizing Compiler for Finite Element Local Assembly</title><categories>cs.MS cs.CE cs.PF</categories><comments>Remove volume metadata</comments><acm-class>G.1.8; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The numerical solution of partial differential equations using the finite
element method is one of the key applications of high performance computing.
Local assembly is its characteristic operation. This entails the execution of a
problem-specific kernel to numerically evaluate an integral for each element in
the discretized problem domain. Since the domain size can be huge, executing
efficient kernels is fundamental. Their op- timization is, however, a
challenging issue. Even though affine loop nests are generally present, the
short trip counts and the complexity of mathematical expressions make it hard
to determine a single or unique sequence of successful transformations.
Therefore, we present the design and systematic evaluation of COF- FEE, a
domain-specific compiler for local assembly kernels. COFFEE manipulates
abstract syntax trees generated from a high-level domain-specific language for
PDEs by introducing domain-aware composable optimizations aimed at improving
instruction-level parallelism, especially SIMD vectorization, and register
locality. It then generates C code including vector intrinsics. Experiments
using a range of finite-element forms of increasing complexity show that
significant performance improvement is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0913</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0913</id><created>2014-07-03</created><authors><author><keyname>Maso</keyname><forenames>Carlo Dal</forenames></author><author><keyname>Pompa</keyname><forenames>Gabriele</forenames></author><author><keyname>Puliga</keyname><forenames>Michelangelo</forenames></author><author><keyname>Riotta</keyname><forenames>Gianni</forenames></author><author><keyname>Chessa</keyname><forenames>Alessandro</forenames></author></authors><title>Voting Behavior, Coalitions and Government Strength through a Complex
  Network Analysis</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>6 pages, 4 figures</comments><doi>10.1371/journal.pone.0116046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the network of relations between parliament members according to
their voting behavior. In particular, we examine the emergent community
structure with respect to political coalitions and government alliances. We
rely on tools developed in the Complex Network literature to explore the core
of these communities and use their topological features to develop new metrics
for party polarization, internal coalition cohesiveness and government
strength. As a case study, we focus on the Chamber of Deputies of the Italian
Parliament, for which we are able to characterize the heterogeneity of the
ruling coalition as well as parties specific contributions to the stability of
the government over time. We find sharp contrast in the political debate which
surprisingly does not imply a relevant structure based on establised parties.
We take a closer look to changes in the community structure after parties split
up and their effect on the position of single deputies within communities.
Finally, we introduce a way to track the stability of the government coalition
over time that is able to discern the contribution of each member along with
the impact of its possible defection. While our case study relies on the
Italian parliament, whose relevance has come into the international spotlight
in the present economic downturn, the methods developed here are entirely
general and can therefore be applied to a multitude of other scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0915</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0915</id><created>2014-07-03</created><authors><author><keyname>Chorti</keyname><forenames>Arsenia</forenames></author><author><keyname>Molu</keyname><forenames>Mehdi M.</forenames></author><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Burr</keyname><forenames>Alister</forenames></author></authors><title>Strong Secrecy in Wireless Network Coding Systems with M-QAM Modulators</title><categories>cs.CR cs.IT math.IT</categories><comments>submitted to ICCC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the possibility of developing physical layer network coding
(PNC) schemes with embedded strong secrecy based on standard QAM modulators.
The proposed scheme employs a triple binning approach at the QAM front-end of
the wireless PNC encoders. A constructive example of a strong secrecy encoder
is presented when a BPSK and an 8-PAM modulator are employed at the wireless
transmitters and generalized to arbitrary M-QAM modulators, assuming channel
inversion is attainable at the first cycle of the transmission. Our preliminary
investigations demonstrate the potential of using such techniques to increase
the throughput while in parallel not compromise the confidentiality of the
exchanged data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0921</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0921</id><created>2014-07-03</created><authors><author><keyname>Lenzen</keyname><forenames>Frank</forenames></author><author><keyname>Lellmann</keyname><forenames>Jan</forenames></author><author><keyname>Becker</keyname><forenames>Florian</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author></authors><title>Solving QVIs for Image Restoration with Adaptive Constraint Sets</title><categories>math.OC cs.CV math.NA</categories><msc-class>49J40, 49J53, 49M30, 49N45, 52A20, 65K15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of quasi-variational inequalities (QVIs) for adaptive
image restoration, where the adaptivity is described via solution-dependent
constraint sets. In previous work we studied both theoretical and numerical
issues. While we were able to show the existence of solutions for a relatively
broad class of problems, we encountered problems concerning uniqueness of the
solution as well as convergence of existing algorithms for solving QVIs. In
particular, it seemed that with increasing image size the growing condition
number of the involved differential operator poses severe problems. In the
present paper we prove uniqueness for a larger class of problems and in
particular independent of the image size. Moreover, we provide a numerical
algorithm with proved convergence. Experimental results support our theoretical
findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0927</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0927</id><created>2014-07-03</created><authors><author><keyname>M&#xe9;ry</keyname><forenames>Dominique</forenames><affiliation>LORIA</affiliation></author><author><keyname>Singh</keyname><forenames>Neeraj Kumar</forenames><affiliation>McMaster University</affiliation></author></authors><title>Modelling an Aircraft Landing System in Event-B (Full Report)</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The failure of hardware or software in a critical system can lead to loss of
lives. The design errors can be main source of the failures that can be
introduced during system development process. Formal techniques are an
alternative approach to verify the correctness of critical systems, overcoming
limitations of the traditional validation techniques such as simulation and
testing. The increasing complexity and failure rate brings new challenges in
the area of verification and validation of avionic systems. Since the
reliability of the software cannot be quantified, the \textit{correct by
construction} approach can implement a reliable system. Refinement plays a
major role to build a large system incrementally from an abstract specification
to a concrete system. This paper contributes as a stepwise formal development
of the landing system of an aircraft. The formal models include the complex
behaviour, temporal behaviour and sequence of operations of the landing gear
system. The models are formalized in Event-B modelling language, which supports
stepwise refinement. This case study is considered as a benchmark for
techniques and tools dedicated to the verification of behavioural properties of
systems. The report is the full version of a paper published for the ABZ 2014
Case Study. is
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0935</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0935</id><created>2014-07-03</created><authors><author><keyname>Gopalakrishna</keyname><forenames>M. T</forenames></author><author><keyname>Ravishankar</keyname><forenames>M.</forenames></author><author><keyname>Rameshbabu</keyname><forenames>D. R</forenames></author></authors><title>Multiple Moving Object Recognitions in video based on Log Gabor-PCA
  Approach</title><categories>cs.CV</categories><comments>8,26,conference</comments><msc-class>68T45</msc-class><acm-class>I.4.8</acm-class><doi>10.1007/978-3-319-01778-5_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object recognition in the video sequence or images is one of the sub-field of
computer vision. Moving object recognition from a video sequence is an
appealing topic with applications in various areas such as airport safety,
intrusion surveillance, video monitoring, intelligent highway, etc. Moving
object recognition is the most challenging task in intelligent video
surveillance system. In this regard, many techniques have been proposed based
on different methods. Despite of its importance, moving object recognition in
complex environments is still far from being completely solved for low
resolution videos, foggy videos, and also dim video sequences. All in all,
these make it necessary to develop exceedingly robust techniques. This paper
introduces multiple moving object recognition in the video sequence based on
LoG Gabor-PCA approach and Angle based distance Similarity measures techniques
used to recognize the object as a human, vehicle etc. Number of experiments are
conducted for indoor and outdoor video sequences of standard datasets and also
our own collection of video sequences comprising of partial night vision video
sequences. Experimental results show that our proposed approach achieves an
excellent recognition rate. Results obtained are satisfactory and competent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0943</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0943</id><created>2014-07-03</created><authors><author><keyname>Han</keyname><forenames>Shiying</forenames></author><author><keyname>Liang</keyname><forenames>Ying-Chang</forenames></author><author><keyname>Soong</keyname><forenames>Boon-Hee</forenames></author></authors><title>Spectrum Refarming: A New Paradigm of Spectrum Sharing for Cellular
  Networks</title><categories>cs.IT math.IT</categories><comments>29 pages, 10 figures, submitted to IEEE Trans. Wireless Comms.,
  accepted conference version to GC'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum refarming (SR) refers to a radio resource management technique which
allows different generations of cellular networks to operate in the same radio
spectrum. In this paper, an underlay SR model is proposed, in which an
Orthogonal Frequency Division Multiple Access (OFDMA) system refarms the
spectrum of a Code Division Multiple Access (CDMA) system through intelligently
exploiting the interference margin provided by the CDMA system. We investigate
the mutual effect of the two systems by evaluating the asymptotic
signal-to-interference-plus-noise ratio (SINR) of the users, based on which the
interference margin tolerable by the CDMA system is determined. By using the
interference margin together with the transmit power constraints, the uplink
resource allocation problem of OFDMA system is formulated and solved through
dual decomposition method. Simulation results have verified our theoretical
analysis, and validated the effectiveness of the proposed resource allocation
algorithm and its capability to protect the legacy CDMA users. The proposed SR
system requires the least information flow from the CDMA system to the OFDMA
system, and importantly, no upgrading of legacy CDMA system is needed; thus it
can be deployed by telecom operators to maximize the spectral efficiency of
their cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0950</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0950</id><created>2014-07-03</created><updated>2016-01-14</updated><authors><author><keyname>Barton</keyname><forenames>Carl</forenames></author></authors><title>On the Average-case Complexity of Pattern Matching with Wildcards</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern matching with wildcards is the problem of finding all factors of a
text $t$ of length $n$ that match a pattern $x$ of length $m$, where wildcards
(characters that match everything) may be present. In this paper we present a
number of fast average-case algorithms for pattern matching where wildcards are
restricted to either the pattern or the text, however, the results are easily
adapted to the case where wildcards are allowed in both. We analyse the
\textit{average-case} complexity of these algorithms and show the first
non-trivial time bounds. These are the first results on the average-case
complexity of pattern matching with wildcards which, as a by product, provide
with first provable separation in complexity between exact pattern matching and
pattern matching with wildcards in the word RAM model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0952</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0952</id><created>2014-07-03</created><updated>2014-07-08</updated><authors><author><keyname>Podobnik</keyname><forenames>B.</forenames></author><author><keyname>Lipic</keyname><forenames>T.</forenames></author><author><keyname>Horvatic</keyname><forenames>D.</forenames></author><author><keyname>Majdandzic</keyname><forenames>A.</forenames></author><author><keyname>Bishop</keyname><forenames>S.</forenames></author><author><keyname>Stanley</keyname><forenames>H. E.</forenames></author></authors><title>Predicting Lifetime of Dynamical Networks Experiencing Persistent Random
  Attacks</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical estimation of critical points at which complex systems abruptly
flip from one state to another is among the remaining challenges in network
science. However, due to the stochastic nature of critical transitions it is
widely believed that critical points are difficult to estimate, and it is even
more difficult, if not impossible, to predict the time such transitions occur
[1-4]. We analyze a class of decaying dynamical networks experiencing
persistent attacks in which the magnitude of the attack is quantified by the
probability of an internal failure, and there is some chance that an internal
failure will be permanent. When the fraction of active neighbors declines to a
critical threshold, cascading failures trigger a network breakdown. For this
class of network we find both numerically and analytically that the time to the
network breakdown, equivalent to the network lifetime, is inversely dependent
upon the magnitude of the attack and logarithmically dependent on the
threshold. We analyze how permanent attacks affect dynamical network robustness
and use the network lifetime as a measure of dynamical network robustness
offering new methodological insight into system dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0958</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0958</id><created>2014-07-03</created><authors><author><keyname>Faber</keyname><forenames>Vance</forenames></author></authors><title>Transpose on vertex symmetric digraphs</title><categories>math.CO cs.DS</categories><comments>12 pages</comments><msc-class>68M10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss transpose (sometimes called universal exchange or all-to-all) on
vertex symmetric networks. We provide a method to compare the efficiency of
transpose schemes on two different networks with a cost function based on the
number processors and wires needed to complete a given algorithm in a given
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0961</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0961</id><created>2014-07-03</created><authors><author><keyname>Shi</keyname><forenames>Feng</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Wagh</keyname><forenames>Meghanad</forenames></author></authors><title>An Enhanced Multiway Sorting Network Based on n-Sorters</title><categories>cs.DS</categories><comments>13 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Merging-based sorting networks are an important family of sorting networks.
Most merge sorting networks are based on 2-way or multi-way merging algorithms
using 2-sorters as basic building blocks. An alternative is to use n-sorters,
instead of 2-sorters, as the basic building blocks so as to greatly reduce the
number of sorters as well as the latency. Based on a modified Leighton's
columnsort algorithm, an n-way merging algorithm, referred to as SS-Mk, that
uses n-sorters as basic building blocks was proposed. In this work, we first
propose a new multiway merging algorithm with n-sorters as basic building
blocks that merges n sorted lists of m values each in 1 + ceil(m/2) stages (n
&lt;= m). Based on our merging algorithm, we also propose a sorting algorithm,
which requires O(N log2 N) basic sorters to sort N inputs. While the asymptotic
complexity (in terms of the required number of sorters) of our sorting
algorithm is the same as the SS-Mk, for wide ranges of N, our algorithm
requires fewer sorters than the SS-Mk. Finally, we consider a binary sorting
network, where the basic sorter is implemented in threshold logic and scales
linearly with the number of inputs, and compare the complexity in terms of the
required number of gates. For wide ranges of N, our algorithm requires fewer
gates than the SS-Mk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0970</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0970</id><created>2014-07-03</created><updated>2015-03-31</updated><authors><author><keyname>Preda</keyname><forenames>Mila Dalla</forenames></author><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author><author><keyname>Giallorenzo</keyname><forenames>Saverio</forenames></author><author><keyname>Lanese</keyname><forenames>Ivan</forenames></author><author><keyname>Mauro</keyname><forenames>Jacopo</forenames></author></authors><title>Dynamic Choreographies - Safe Runtime Updates of Distributed
  Applications</title><categories>cs.PL</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming distributed applications free from communication deadlocks and
races is complex. Preserving these properties when applications are updated at
runtime is even harder. We present DIOC, a language for programming distributed
applications that are free from deadlocks and races by construction. A DIOC
program describes a whole distributed application as a unique entity
(choreography). DIOC allows the programmer to specify which parts of the
application can be updated. At runtime, these parts may be replaced by new DIOC
fragments from outside the application. DIOC programs are compiled, generating
code for each site, in a lower-level language called DPOC. We formalise both
DIOC and DPOC semantics as labelled transition systems and prove the
correctness of the compilation as a trace equivalence result. As corollaries,
DPOC applications are free from communication deadlocks and races, even in
presence of runtime updates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0975</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0975</id><created>2014-07-03</created><updated>2014-07-10</updated><authors><author><keyname>Preda</keyname><forenames>Mila Dalla</forenames></author><author><keyname>Giallorenzo</keyname><forenames>Saverio</forenames></author><author><keyname>Lanese</keyname><forenames>Ivan</forenames></author><author><keyname>Mauro</keyname><forenames>Jacopo</forenames></author><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author></authors><title>AIOCJ: A Choreographic Framework for Safe Adaptive Distributed
  Applications</title><categories>cs.PL</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present AIOCJ, a framework for programming distributed adaptive
applications. Applications are programmed using AIOC, a choreographic language
suited for expressing patterns of interaction from a global point of view. AIOC
allows the programmer to specify which parts of the application can be adapted.
Adaptation takes place at runtime by means of rules, which can change during
the execution to tackle possibly unforeseen adaptation needs. AIOCJ relies on a
solid theory that ensures applications to be deadlock-free by construction also
after adaptation. We describe the architecture of AIOCJ, the design of the AIOC
language, and an empirical validation of the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0977</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0977</id><created>2014-07-02</created><authors><author><keyname>Nowotniak</keyname><forenames>Robert</forenames></author><author><keyname>Kucharski</keyname><forenames>Jacek</forenames></author></authors><title>Higher-Order Quantum-Inspired Genetic Algorithms</title><categories>cs.NE quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a theory and an empirical evaluation of Higher-Order
Quantum-Inspired Genetic Algorithms. Fundamental notions of the theory have
been introduced, and a novel Order-2 Quantum-Inspired Genetic Algorithm (QIGA2)
has been presented. Contrary to all QIGA algorithms which represent quantum
genes as independent qubits, in higher-order QIGAs quantum registers are used
to represent genes strings which allows modelling of genes relations using
quantum phenomena. Performance comparison has been conducted on a benchmark of
20 deceptive combinatorial optimization problems. It has been presented that
using higher quantum orders is beneficial for genetic algorithm efficiency, and
the new QIGA2 algorithm outperforms the old QIGA algorithm which was tuned in
highly compute intensive metaoptimization process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0978</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0978</id><created>2014-07-01</created><updated>2014-07-06</updated><authors><author><keyname>Millet</keyname><forenames>Laure</forenames><affiliation>LIP6</affiliation></author><author><keyname>Potop-Butucaru</keyname><forenames>Maria</forenames><affiliation>LIP6</affiliation></author><author><keyname>Sznajder</keyname><forenames>Nathalie</forenames><affiliation>LIP6</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, LINCS, IUF</affiliation></author></authors><title>On the Synthesis of Mobile Robots Algorithms: the Case of Ring Gathering</title><categories>cs.DC cs.LO</categories><comments>International Symposium on Stabilization, Safety, and Security of
  Distributed Systems (SSS 2014), Paderborn : Germany (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RecentadvancesinDistributedComputinghighlightmodelsandalgo- rithms for
autonomous swarms of mobile robots that self-organize and cooperate to solve
global objectives. The overwhelming majority of works so far considers handmade
algorithms and correctness proofs. This paper is the first to propose a formal
framework to automatically design dis- tributed algorithms that are dedicated
to autonomous mobile robots evolving in a discrete space. As a case study, we
consider the problem of gathering all robots at a particular location, not
known beforehand. Our contribution is threefold. First, we propose an encoding
of the gathering problem as a reachability game. Then, we automatically
generate an optimal distributed algorithm for three robots evolv- ing on a
fixed size uniform ring. Finally, we prove by induction that the generated
algorithm is also correct for any ring size except when an impossibility result
holds (that is, when the number of robots divides the ring size).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.0981</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.0981</id><created>2014-07-03</created><authors><author><keyname>Dias</keyname><forenames>Gabriel Martins</forenames></author><author><keyname>Oechsner</keyname><forenames>Simon</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>Intelligently using data from external WSNs</title><categories>cs.NI cs.SY</categories><comments>10 pages, simulation results and figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we build on the concept of inter-Wireless Sensor Networks
(WSNs) information exchange. We show the feasibility of an approach that
intelligently utilizes information produced by other WSNs that may or not
belong to the same administrative domain. To illustrate how the behavior of a
WSN can be optimized using data from external WSNs, a specific use-case is
considered, where the operation of a WSN measuring relative humidity is
optimized using the data obtained from a WSN measuring temperature. Evaluated
using a dedicated performance score, the simulation results show that this new
approach can find a trade-off between energy consumption and quality of
measurements. Moreover, we outline the additional challenges that need to be
overcome, and draw conclusions to guide the future work in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1004</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1004</id><created>2014-07-03</created><authors><author><keyname>Stasi</keyname><forenames>Despina</forenames></author><author><keyname>Sadeghi</keyname><forenames>Kayvan</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Petrovi&#x107;</keyname><forenames>Sonja</forenames></author><author><keyname>Fienberg</keyname><forenames>Stephen E.</forenames></author></authors><title>$\beta$ models for random hypergraphs with a given degree sequence</title><categories>math.ST cs.SI stat.TH</categories><comments>9 pages, 2 figures, Proceedings of 21st International Conference on
  Computational Statistics (2014), to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the beta model for random hypergraphs in order to represent the
occurrence of multi-way interactions among agents in a social network. This
model builds upon and generalizes the well-studied beta model for random
graphs, which instead only considers pairwise interactions. We provide two
algorithms for fitting the model parameters, IPS (iterative proportional
scaling) and fixed point algorithm, prove that both algorithms converge if
maximum likelihood estimator (MLE) exists, and provide algorithmic and
geometric ways of dealing the issue of MLE existence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1015</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1015</id><created>2014-07-02</created><authors><author><keyname>Iturrioz</keyname><forenames>Luisa</forenames></author></authors><title>Algebraic and relational models for a system based on a poset of two
  elements</title><categories>cs.LO</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to present a very simple set of conditions,
necessary for the management of knowledge of a poset $T$ of two agents, which
are partially ordered by the capabilities available in the system. We build up
a formal system and we elaborate suitable semantic models in order to derive
information from the poset. The system is related to three-valued Heyting
algebras with Boolean operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1027</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1027</id><created>2014-07-02</created><authors><author><keyname>Cepeda-Gomez</keyname><forenames>Rudy</forenames></author><author><keyname>Olgac</keyname><forenames>Nejat</forenames></author></authors><title>A Consensus Protocol under Directed Communications with Two Time Delays
  and Delay Scheduling</title><categories>math.OC cs.SY</categories><journal-ref>International Journal of Control, volume 87, issue 2, pages 291 to
  300, 2014</journal-ref><doi>10.1080/00207179.2013.829605</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a consensus protocol over a group of agents driven by
second order dynamics. The communication among members of the group is assumed
to be directed and affected by two rationally independent time delays, one in
the position and the other in the velocity information channels. These delays
are unknown but considered to be constant and uniform throughout the system.
The stability of the consensus protocol is studied using a simplifying
factorization procedure and deploying the Cluster Treatment of Characteristic
Roots (CTCR) paradigm. This effort results in a unique depiction of the exact
stability boundaries in the domain of the delays. CTCR requires the knowledge
of the potential stability switching loci exhaustively within this domain. The
creation of these loci is an important contribution of this work. It is done in
a new surrogate coordinate system, called the \emph{Spectral Delay Space
(SDS)}. The relative stability, i.e., the speed to reach consensus of the
system is also investigated for this class of systems. Based on the outcome of
this effort, a paradoxical control design concept is introduced. It is called
the \emph{Delay Scheduling}, which is another key contribution of this paper.
It reveals that the performance of the system may be improved by increasing the
delays. The amount of increase, however, is only revealed by the CTCR. Example
case studies are presented to verify the underlying analytical derivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1031</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1031</id><created>2014-07-03</created><authors><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Schifanella</keyname><forenames>Rossano</forenames></author><author><keyname>Aiello</keyname><forenames>Luca Maria</forenames></author></authors><title>The Shortest Path to Happiness: Recommending Beautiful, Quiet, and Happy
  Routes in the City</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>11 pages, 7 figures, Proceedings of ACM Hypertext 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When providing directions to a place, web and mobile mapping services are all
able to suggest the shortest route. The goal of this work is to automatically
suggest routes that are not only short but also emotionally pleasant. To
quantify the extent to which urban locations are pleasant, we use data from a
crowd-sourcing platform that shows two street scenes in London (out of
hundreds), and a user votes on which one looks more beautiful, quiet, and
happy. We consider votes from more than 3.3K individuals and translate them
into quantitative measures of location perceptions. We arrange those locations
into a graph upon which we learn pleasant routes. Based on a quantitative
validation, we find that, compared to the shortest routes, the recommended ones
add just a few extra walking minutes and are indeed perceived to be more
beautiful, quiet, and happy. To test the generality of our approach, we
consider Flickr metadata of more than 3.7M pictures in London and 1.3M in
Boston, compute proxies for the crowdsourced beauty dimension (the one for
which we have collected the most votes), and evaluate those proxies with 30
participants in London and 54 in Boston. These participants have not only rated
our recommendations but have also carefully motivated their choices, providing
insights for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1034</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1034</id><created>2014-07-03</created><authors><author><keyname>Luzar</keyname><forenames>Borut</forenames></author><author><keyname>Levnajic</keyname><forenames>Zoran</forenames></author><author><keyname>Povh</keyname><forenames>Janez</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Community structure and the evolution of interdisciplinarity in
  Slovenia's scientific collaboration network</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>11 pages, 4 figures; accepted for publication in PLoS ONE [related
  work available at http://arxiv.org/abs/1004.4824 and
  http://www.matjazperc.com/sicris/stats.html]</comments><journal-ref>PLoS ONE 9 (2014) e94429</journal-ref><doi>10.1371/journal.pone.0094429</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interaction among the scientific disciplines is of vital importance in modern
science. Focusing on the case of Slovenia, we study the dynamics of
interdisciplinary sciences from 1960 to 2010. Our approach relies on
quantifying the interdisciplinarity of research communities detected in the
coauthorship network of Slovenian scientists over time. Examining the evolution
of the community structure, we find that the frequency of interdisciplinary
research is only proportional with the overall growth of the network. Although
marginal improvements in favor of interdisciplinarity are inferable during the
70s and 80s, the overall trends during the past 20 years are constant and
indicative of stalemate. We conclude that the flow of knowledge between
different fields of research in Slovenia is in need of further stimulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1041</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1041</id><created>2014-07-03</created><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author></authors><title>n-Valued Refined Neutrosophic Logic and Its Applications to Physics</title><categories>cs.AI</categories><comments>9 pages</comments><msc-class>28E10</msc-class><journal-ref>Progress in Physics, 143-146, Vol. 4, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a short history of logics: from particular cases of
2-symbol or numerical valued logic to the general case of n-symbol or numerical
valued logic. We show generalizations of 2-valued Boolean logic to fuzzy logic,
also from the Kleene and Lukasiewicz 3-symbol valued logics or Belnap 4-symbol
valued logic to the most general n-symbol or numerical valued refined
neutrosophic logic. Two classes of neutrosophic norm (n-norm) and neutrosophic
conorm (n-conorm) are defined. Examples of applications of neutrosophic logic
to physics are listed in the last section. Similar generalizations can be done
for n-Valued Refined Neutrosophic Set, and respectively n- Valued Refined
Neutrosopjhic Probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1056</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1056</id><created>2014-07-03</created><authors><author><keyname>Musial</keyname><forenames>Katarzyna</forenames></author><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Gaworecki</keyname><forenames>Jaros&#x142;aw</forenames></author></authors><title>Extraction of Multi-layered Social Networks from Activity Data</title><categories>cs.SI physics.soc-ph</categories><comments>20 pages, 15 figures</comments><journal-ref>The Scientific World Journal, vol. 2014, Article ID 359868, 13
  pages, 2014</journal-ref><doi>10.1155/2014/359868</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The data gathered in all kind of web-based systems, which enable users to
interact with each other, provides an opportunity to extract social networks
that consist of people and relationships between them. The emerging structures
are very complex due to the number and type of discovered connections. In
webbased systems, the characteristic element of each interaction between users
is that there is always an object that serves as a communication medium. This
can be e.g. an email sent from one user to another or post at the forum
authored by one user and commented by others. Based on these objects and
activities that users perform towards them, different kinds of relationships
can be identified and extracted. Additional challenge arises from the fact that
hierarchies can exist between objects, e.g. a forum consists of one or more
groups of topics, and each of them contains topics that finally include posts.
In this paper, we propose a new method for creation of multi-layered social
network based on the data about users activities towards different types of
objects between which the hierarchy exists. Due to the flattening,
preprocessing procedure new layers and new relationships in the multi-layered
social network can be identified and analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1061</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1061</id><created>2014-07-03</created><authors><author><keyname>Jakeman</keyname><forenames>John D.</forenames></author><author><keyname>Wildey</keyname><forenames>Timothy</forenames></author></authors><title>Enhancing adaptive sparse grid approximations and improving refinement
  strategies using adjoint-based a posteriori error estimates</title><categories>cs.NA math.NA</categories><doi>10.1016/j.jcp.2014.09.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an algorithm for adaptive sparse grid approximations
of quantities of interest computed from discretized partial differential
equations. We use adjoint-based a posteriori error estimates of the physical
discretization error and the interpolation error in the sparse grid to enhance
the sparse grid approximation and to drive adaptivity of the sparse grid.
Utilizing these error estimates provides significantly more accurate functional
values for random samples of the sparse grid approximation. We also demonstrate
that alternative refinement strategies based upon a posteriori error estimates
can lead to further increases in accuracy in the approximation over traditional
hierarchical surplus based strategies. Throughout this paper we also provide
and test a framework for balancing the physical discretization error with the
stochastic interpolation error of the enhanced sparse grid approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1063</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1063</id><created>2014-07-03</created><authors><author><keyname>Erfanian</keyname><forenames>Aida</forenames></author><author><keyname>Darav</keyname><forenames>Nima Karimpour</forenames></author></authors><title>CBM-Of-TRaCE: An Ontology-Driven Framework for the Improvement of
  Business Service Traceability, Consistency Management and Reusability</title><categories>cs.SE</categories><comments>10 pages, 3 figures, International Journal of Soft Computing and
  Software Engineering, no. 7.7., vol. 2, 2012</comments><msc-class>68Q55,</msc-class><acm-class>D.2; K.6.3</acm-class><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE] 2(2012) 69-78</journal-ref><doi>10.7321/jscse.v2.n7.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we represent CBM-Of-TRaCE which is an ontological framework
that integrates two aspects of business components: conceptual and methodology.
In the development of our framework we have taken IBM Actionable Business
Approach (ABA) in to consideration. We evaluate our framework through some
aspects such as support and facilitation for a business from five different
aspects: service-orientation, business process, management integration,
reusability improvement, consistency rules, and traceability. As well, we
demonstrate the compatibility of our CBM-Of-TRaCE with ABA four phases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1065</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1065</id><created>2014-07-03</created><updated>2015-11-24</updated><authors><author><keyname>Candes</keyname><forenames>Emmanuel</forenames></author><author><keyname>Li</keyname><forenames>Xiaodong</forenames></author><author><keyname>Soltanolkotabi</keyname><forenames>Mahdi</forenames></author></authors><title>Phase Retrieval via Wirtinger Flow: Theory and Algorithms</title><categories>cs.IT math.FA math.IT math.NA math.OC math.ST stat.TH</categories><comments>IEEE Transactions on Information Theory, Vol. 64 (4), Feb. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of recovering the phase from magnitude measurements;
specifically, we wish to reconstruct a complex-valued signal x of C^n about
which we have phaseless samples of the form y_r = |&lt; a_r,x &gt;|^2, r = 1,2,...,m
(knowledge of the phase of these samples would yield a linear system). This
paper develops a non-convex formulation of the phase retrieval problem as well
as a concrete solution algorithm. In a nutshell, this algorithm starts with a
careful initialization obtained by means of a spectral method, and then refines
this initial estimate by iteratively applying novel update rules, which have
low computational complexity, much like in a gradient descent scheme. The main
contribution is that this algorithm is shown to rigorously allow the exact
retrieval of phase information from a nearly minimal number of random
measurements. Indeed, the sequence of successive iterates provably converges to
the solution at a geometric rate so that the proposed scheme is efficient both
in terms of computational and data resources. In theory, a variation on this
scheme leads to a near-linear time algorithm for a physically realizable model
based on coded diffraction patterns. We illustrate the effectiveness of our
methods with various experiments on image data. Underlying our analysis are
insights for the analysis of non-convex optimization schemes that may have
implications for computational problems beyond phase retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1066</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1066</id><created>2014-07-03</created><updated>2015-09-07</updated><authors><author><keyname>Seifi</keyname><forenames>Nima</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Coldrey</keyname><forenames>Mikael</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author></authors><title>Adaptive Multicell 3D Beamforming in Multi-Antenna Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transaction on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cellular network with multi-antenna base stations (BSs) and
single-antenna users, multicell cooperation, imperfect channel state
information, and directional antennas each with a vertically adjustable beam.
We investigate the impact of the elevation angle of the BS antenna pattern,
denoted as tilt, on the performance of the considered network when employing
either a conventional single-cell transmission or a fully cooperative multicell
transmission. Using the results of this investigation, we propose a novel
hybrid multicell cooperation technique in which the intercell interference is
controlled via either cooperative beamforming in the horizontal plane or
coordinated beamfroming in the vertical plane of the wireless channel, denoted
as adaptive multicell 3D beamforming. The main idea is to divide the coverage
area into two disjoint vertical regions and adapt the multicell cooperation
strategy at the BSs when serving each region. A fair scheduler is used to share
the time-slots between the vertical regions. It is shown that the proposed
technique can achieve performance comparable to that of a fully cooperative
transmission but with a significantly lower complexity and signaling
requirements. To make the performance analysis computationally efficient,
analytical expressions for the user ergodic rates under different beamforming
strategies are also derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1069</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1069</id><created>2014-07-03</created><authors><author><keyname>Novara</keyname><forenames>C.</forenames></author><author><keyname>Milanese</keyname><forenames>M.</forenames></author></authors><title>Control of nonlinear systems: a model inversion approach</title><categories>cs.SY</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel control design approach for general nonlinear systems is presented in
this paper. The approach is based on the identification of a polynomial model
of the system to control and on the on-line inversion of this model. An
efficient technique is developed to perform the inversion, which allows an
effective control implementation on real-time processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1082</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1082</id><created>2014-07-03</created><authors><author><keyname>Golovin</keyname><forenames>Daniel</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author><author><keyname>Streeter</keyname><forenames>Matthew</forenames></author></authors><title>Online Submodular Maximization under a Matroid Constraint with
  Application to Learning Assignments</title><categories>cs.LG</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Which ads should we display in sponsored search in order to maximize our
revenue? How should we dynamically rank information sources to maximize the
value of the ranking? These applications exhibit strong diminishing returns:
Redundancy decreases the marginal utility of each ad or information source. We
show that these and other problems can be formalized as repeatedly selecting an
assignment of items to positions to maximize a sequence of monotone submodular
functions that arrive one by one. We present an efficient algorithm for this
general problem and analyze it in the no-regret model. Our algorithm possesses
strong theoretical guarantees, such as a performance ratio that converges to
the optimal constant of 1 - 1/e. We empirically evaluate our algorithm on two
real-world online optimization problems on the web: ad allocation with
submodular utilities, and dynamically ranking blogs to detect information
cascades. Finally, we present a second algorithm that handles the more general
case in which the feasible sets are given by a matroid constraint, while still
maintaining a 1 - 1/e asymptotic performance ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1097</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1097</id><created>2014-07-03</created><authors><author><keyname>Tulabandhula</keyname><forenames>Theja</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Robust Optimization using Machine Learning for Uncertainty Sets</title><categories>math.OC cs.LG stat.ML</categories><comments>28 pages, 2 figures; a shorter preliminary version appeared in ISAIM
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to build robust optimization problems for making decisions based
on complex data from the past. In robust optimization (RO) generally, the goal
is to create a policy for decision-making that is robust to our uncertainty
about the future. In particular, we want our policy to best handle the the
worst possible situation that could arise, out of an uncertainty set of
possible situations. Classically, the uncertainty set is simply chosen by the
user, or it might be estimated in overly simplistic ways with strong
assumptions; whereas in this work, we learn the uncertainty set from data
collected in the past. The past data are drawn randomly from an (unknown)
possibly complicated high-dimensional distribution. We propose a new
uncertainty set design and show how tools from statistical learning theory can
be employed to provide probabilistic guarantees on the robustness of the
policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1103</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1103</id><created>2014-07-03</created><updated>2015-03-29</updated><authors><author><keyname>Lyu</keyname><forenames>Hanbaek</forenames></author></authors><title>Synchronization of finite-state pulse-coupled oscillators</title><categories>cs.SY math.OC</categories><comments>23 pages, 17 figures, To appear in Physica D: Nonlinear Phenomena</comments><doi>10.1016/j.physd.2015.03.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel generalized cellular automaton(GCA) model for
discrete-time pulse-coupled oscillators and study the emergence of synchrony.
Given a finite simple graph and an integer $n\ge 3$, each vertex is an
identical oscillator of period $n$ with the following weak coupling along the
edges: each oscillator inhibits its phase update if it has at least one
neighboring oscillator at a particular &quot;blinking&quot; state and if its state is
ahead of this blinking state. We obtain conditions on initial configurations
and on network topologies for which states of all vertices eventually
synchronize. We show that our GCA model synchronizes arbitrary initial
configurations on paths, trees, and with random perturbation, any connected
graph. In particular, our main result is the following local-global principle
for tree networks: for $n\in \{3,4,5,6\}$, any $n$-periodic network on a tree
synchronizes arbitrary initial configuration if and only if the maximum degree
of the tree is less than the period $n$.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="62000" completeListSize="102538">1122234|63001</resumptionToken>
</ListRecords>
</OAI-PMH>
